#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7615115961618314, 'errorList': [], 'lossList': [0.0, -1.416316899061203, 0.0, 34.04166961669922, 0.0, 0.0, 0.0], 'rewardMean': 0.6028671685686746, 'totalEpisodes': 54, 'stepsPerEpisode': 22, 'rewardPerEpisode': 14.045930817942493
'totalSteps': 3840, 'rewardStep': 0.24376022335712494, 'errorList': [], 'lossList': [0.0, -1.4158412671089173, 0.0, 39.97222840309143, 0.0, 0.0, 0.0], 'rewardMean': 0.4831648534981581, 'totalEpisodes': 69, 'stepsPerEpisode': 95, 'rewardPerEpisode': 70.66786970327684
'totalSteps': 5120, 'rewardStep': 0.6881630550608158, 'errorList': [], 'lossList': [0.0, -1.4050467377901077, 0.0, 30.977895011901854, 0.0, 0.0, 0.0], 'rewardMean': 0.5344144038888226, 'totalEpisodes': 78, 'stepsPerEpisode': 193, 'rewardPerEpisode': 146.5557322671595
'totalSteps': 6400, 'rewardStep': 0.8020668528180439, 'errorList': [], 'lossList': [0.0, -1.3971478009223939, 0.0, 114.8520707321167, 0.0, 0.0, 0.0], 'rewardMean': 0.5879448936746667, 'totalEpisodes': 116, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.419470525859384
'totalSteps': 7680, 'rewardStep': 0.8682225867249849, 'errorList': [], 'lossList': [0.0, -1.4035069096088408, 0.0, 65.73086311340332, 0.0, 0.0, 0.0], 'rewardMean': 0.6346578425163865, 'totalEpisodes': 142, 'stepsPerEpisode': 84, 'rewardPerEpisode': 71.35734459307253
'totalSteps': 8960, 'rewardStep': 0.7752474953007319, 'errorList': [], 'lossList': [0.0, -1.4027512246370315, 0.0, 39.5653468465805, 0.0, 0.0, 0.0], 'rewardMean': 0.6547420786284358, 'totalEpisodes': 153, 'stepsPerEpisode': 270, 'rewardPerEpisode': 198.750956028813
'totalSteps': 10240, 'rewardStep': 0.7033474808246579, 'errorList': [], 'lossList': [0.0, -1.3971022075414659, 0.0, 50.98804152488709, 0.0, 0.0, 0.0], 'rewardMean': 0.6608177539029636, 'totalEpisodes': 162, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.666349547694882
'totalSteps': 11520, 'rewardStep': 0.6240527237452607, 'errorList': [], 'lossList': [0.0, -1.3760900121927262, 0.0, 25.496401298046113, 0.0, 0.0, 0.0], 'rewardMean': 0.6567327505521078, 'totalEpisodes': 168, 'stepsPerEpisode': 69, 'rewardPerEpisode': 59.066956215070924
'totalSteps': 12800, 'rewardStep': 0.7884556366416924, 'errorList': [], 'lossList': [0.0, -1.3570279854536056, 0.0, 9.700786054134369, 0.0, 0.0, 0.0], 'rewardMean': 0.6699050391610661, 'totalEpisodes': 169, 'stepsPerEpisode': 826, 'rewardPerEpisode': 641.4082133789219
'totalSteps': 14080, 'rewardStep': 0.603277884038964, 'errorList': [], 'lossList': [0.0, -1.347322746515274, 0.0, 8.313299224078655, 0.0, 0.0, 0.0], 'rewardMean': 0.6858105534674108, 'totalEpisodes': 172, 'stepsPerEpisode': 208, 'rewardPerEpisode': 164.19921934812777
'totalSteps': 15360, 'rewardStep': 0.6274040655880819, 'errorList': [], 'lossList': [0.0, -1.3409962767362595, 0.0, 6.205997208356857, 0.0, 0.0, 0.0], 'rewardMean': 0.6723998004100358, 'totalEpisodes': 176, 'stepsPerEpisode': 131, 'rewardPerEpisode': 102.8520814120569
'totalSteps': 16640, 'rewardStep': 0.7018816913679059, 'errorList': [], 'lossList': [0.0, -1.3232868152856827, 0.0, 7.18407133281231, 0.0, 0.0, 0.0], 'rewardMean': 0.7182119472111139, 'totalEpisodes': 179, 'stepsPerEpisode': 225, 'rewardPerEpisode': 182.32597634132836
'totalSteps': 17920, 'rewardStep': 0.682676987607332, 'errorList': [], 'lossList': [0.0, -1.306577855348587, 0.0, 5.3410549056529995, 0.0, 0.0, 0.0], 'rewardMean': 0.7176633404657655, 'totalEpisodes': 181, 'stepsPerEpisode': 178, 'rewardPerEpisode': 131.53750871835308
'totalSteps': 19200, 'rewardStep': 0.7372780602923519, 'errorList': [], 'lossList': [0.0, -1.2729875659942627, 0.0, 5.744698766469956, 0.0, 0.0, 0.0], 'rewardMean': 0.7111844612131962, 'totalEpisodes': 182, 'stepsPerEpisode': 1035, 'rewardPerEpisode': 745.1539828691596
'totalSteps': 20480, 'rewardStep': 0.6887705750258727, 'errorList': [], 'lossList': [0.0, -1.2457090318202972, 0.0, 5.455067627429962, 0.0, 0.0, 0.0], 'rewardMean': 0.6932392600432851, 'totalEpisodes': 184, 'stepsPerEpisode': 222, 'rewardPerEpisode': 170.5610668930308
'totalSteps': 21760, 'rewardStep': 0.5057009321429156, 'errorList': [], 'lossList': [0.0, -1.2498639929294586, 0.0, 3.8836341935396193, 0.0, 0.0, 0.0], 'rewardMean': 0.6662846037275034, 'totalEpisodes': 185, 'stepsPerEpisode': 1157, 'rewardPerEpisode': 849.5762244215218
'totalSteps': 23040, 'rewardStep': 0.6749619919946123, 'errorList': [], 'lossList': [0.0, -1.2418293607234956, 0.0, 4.619897116422653, 0.0, 0.0, 0.0], 'rewardMean': 0.6634460548444989, 'totalEpisodes': 186, 'stepsPerEpisode': 920, 'rewardPerEpisode': 747.3444611547901
'totalSteps': 24320, 'rewardStep': 0.9698427549625004, 'errorList': [0.6169446862385546, 0.39151152242409704, 0.42169985537081267, 0.535721663073163, 0.4262241176010625, 0.55113972163117, 0.35632251220394195, 0.5148921918207179, 0.5186512386939148, 0.4105534816628693, 0.9495083276629213, 0.5626299263701363, 0.6458688091485445, 0.5196823951710591, 0.5083739415799609, 0.392333068794637, 0.39367738353958537, 0.7144342456157334, 0.47895753353488335, 0.6934946292257442, 0.39734027501612573, 0.33403075607209826, 0.5178744901774325, 0.7427736040803262, 0.7160677303052839, 0.5124922765410286, 0.4279659774577133, 0.5406473142958182, 0.5776742356625759, 0.3660567996879687, 0.3105958116083327, 0.5053554183121448, 0.6649389988359682, 0.46096436371655847, 0.6720549768469218, 0.5752946876241313, 0.5360956177490451, 0.6491214205450291, 0.593899063231948, 0.5290352007062226, 0.45981033263417137, 0.49803806202283735, 0.4627576411562725, 0.6716805328656554, 0.7124513715476043, 0.7544550593236853, 0.422841666440021, 0.4704486706244233, 0.46103136481926577, 0.6858129408794917], 'lossList': [0.0, -1.2375981134176255, 0.0, 5.506734611988068, 0.0, 0.0, 0.0], 'rewardMean': 0.6980250579662228, 'totalEpisodes': 189, 'stepsPerEpisode': 24, 'rewardPerEpisode': 19.757614390606935, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.8475049836467466, 'errorList': [], 'lossList': [0.0, -1.233430230617523, 0.0, 3.3502633541822435, 0.0, 0.0, 0.0], 'rewardMean': 0.7039299926667284, 'totalEpisodes': 190, 'stepsPerEpisode': 416, 'rewardPerEpisode': 337.50855507022044
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=75.93
