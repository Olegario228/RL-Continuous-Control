#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980785764922136, 'errorList': [], 'lossList': [0.0, -1.417504243850708, 0.0, 30.41210502624512, 0.0, 0.0, 0.0], 'rewardMean': 0.7065643546668106, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.34356450654182
'totalSteps': 3840, 'rewardStep': 0.7337342130467079, 'errorList': [], 'lossList': [0.0, -1.4060174733400346, 0.0, 37.440305285453796, 0.0, 0.0, 0.0], 'rewardMean': 0.7156209741267764, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.247863075167686
'totalSteps': 5120, 'rewardStep': 0.8983266968431368, 'errorList': [], 'lossList': [0.0, -1.4023935985565186, 0.0, 28.85670949935913, 0.0, 0.0, 0.0], 'rewardMean': 0.7612974048058665, 'totalEpisodes': 76, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.645272888643724
'totalSteps': 6400, 'rewardStep': 0.9743737806726301, 'errorList': [103.56901185349997, 218.3839925542563, 130.40631793522795, 197.1366102083881, 150.6875934932968, 219.08177717608288, 129.31179615104662, 73.63097499762101, 238.8188183876611, 244.7816363678099, 174.96211961568335, 112.05557962323026, 236.78440702634649, 84.47820739778166, 47.65784268850808, 212.9443891523222, 217.56288101566224, 247.01001412940172, 170.3383383177981, 38.22419534349357, 230.14943936991034, 139.88310333081827, 238.86071253769492, 237.62138892473445, 167.48672788274604, 130.88739179002079, 150.78110389311854, 232.3641241158379, 127.62784887114987, 250.35917196564802, 152.01790735942424, 189.9994321833488, 154.81485779523084, 113.6325222326368, 75.52746860314807, 207.027597168855, 151.67480009906674, 231.08283356057433, 165.3151077479508, 185.239183836291, 130.40263346748225, 153.00886397750233, 105.28683140453975, 182.7018357664341, 195.79554923439977, 160.92441926630062, 98.5395657037476, 96.0444050771612, 213.25990241004683, 217.43666779126255], 'lossList': [0.0, -1.3848909145593644, 0.0, 102.28918766021728, 0.0, 0.0, 0.0], 'rewardMean': 0.8039126799792193, 'totalEpisodes': 104, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.127291567129528, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.452763390814914, 'errorList': [], 'lossList': [0.0, -1.3634087544679643, 0.0, 43.826983766555784, 0.0, 0.0, 0.0], 'rewardMean': 0.745387798451835, 'totalEpisodes': 116, 'stepsPerEpisode': 170, 'rewardPerEpisode': 127.98142221866105
'totalSteps': 8960, 'rewardStep': 0.7260666049714154, 'errorList': [], 'lossList': [0.0, -1.3448563617467881, 0.0, 56.850017833709714, 0.0, 0.0, 0.0], 'rewardMean': 0.7426276279546322, 'totalEpisodes': 128, 'stepsPerEpisode': 111, 'rewardPerEpisode': 90.58962978753503
'totalSteps': 10240, 'rewardStep': 0.8115534673620138, 'errorList': [], 'lossList': [0.0, -1.340978854894638, 0.0, 15.134875798225403, 0.0, 0.0, 0.0], 'rewardMean': 0.751243357880555, 'totalEpisodes': 130, 'stepsPerEpisode': 89, 'rewardPerEpisode': 75.57063274522228
'totalSteps': 11520, 'rewardStep': 0.5609648979514699, 'errorList': [], 'lossList': [0.0, -1.3331396800279618, 0.0, 26.86092742919922, 0.0, 0.0, 0.0], 'rewardMean': 0.7301013067773233, 'totalEpisodes': 133, 'stepsPerEpisode': 752, 'rewardPerEpisode': 557.8491049650845
'totalSteps': 12800, 'rewardStep': 0.5767563029352343, 'errorList': [], 'lossList': [0.0, -1.3118612229824067, 0.0, 14.862472858428955, 0.0, 0.0, 0.0], 'rewardMean': 0.7147668063931144, 'totalEpisodes': 136, 'stepsPerEpisode': 611, 'rewardPerEpisode': 458.8658281744153
'totalSteps': 14080, 'rewardStep': 0.7803994464158984, 'errorList': [], 'lossList': [0.0, -1.274181934595108, 0.0, 6.209449194669723, 0.0, 0.0, 0.0], 'rewardMean': 0.7113017377505634, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1011.1919708400641
'totalSteps': 15360, 'rewardStep': 0.9344160967211051, 'errorList': [0.36858228468237475, 0.33375528537972937, 0.3188868540475924, 0.4226753800251206, 0.3864615183991563, 0.3379914950674883, 0.3923294625571949, 0.3639428230975191, 0.37246696627367665, 0.3784367525628984, 0.38034270233890616, 0.3756295582742744, 0.4085350615896964, 0.44836028112356235, 0.3625019216421177, 0.3368272117640061, 0.3406186076019961, 0.38423730233955994, 0.3920893444532446, 0.3428912980211658, 0.3751597011872266, 0.3806702614497586, 0.33487153862486085, 0.392022938321923, 0.38410052346684487, 0.4288857727994598, 0.3530058074117553, 0.39195187144237903, 0.39457123632908797, 0.3791695320270478, 0.411367660640402, 0.36491928731393913, 0.42089734566241066, 0.38224540939769236, 0.3745561758806238, 0.3970723102134307, 0.41908848393409326, 0.34569409879263313, 0.4236840689837436, 0.3229457633542581, 0.34990667673603076, 0.3540268778441568, 0.3981017390457027, 0.42055686650754975, 0.3612636655131986, 0.3440456431792009, 0.3856692657740121, 0.3589580902668352, 0.3893292549582223, 0.37814023451492135], 'lossList': [0.0, -1.2394050449132918, 0.0, 4.54921941511333, 0.0, 0.0, 0.0], 'rewardMean': 0.7449354897734526, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.4665818754224, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.8283562029642842, 'errorList': [], 'lossList': [0.0, -1.2214047455787658, 0.0, 3.1286462231725456, 0.0, 0.0, 0.0], 'rewardMean': 0.7543976887652102, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.8137344554113
'totalSteps': 17920, 'rewardStep': 0.8611631155109492, 'errorList': [], 'lossList': [0.0, -1.1872328293323517, 0.0, 2.5699738177657125, 0.0, 0.0, 0.0], 'rewardMean': 0.7506813306319915, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.784987791289
'totalSteps': 19200, 'rewardStep': 0.7873108776979855, 'errorList': [], 'lossList': [0.0, -1.157960713505745, 0.0, 2.243931357860565, 0.0, 0.0, 0.0], 'rewardMean': 0.7319750403345269, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.172876588844
'totalSteps': 20480, 'rewardStep': 0.8541202648337873, 'errorList': [], 'lossList': [0.0, -1.1362221437692641, 0.0, 1.3333624186739326, 0.0, 0.0, 0.0], 'rewardMean': 0.7721107277364142, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.7413880214312
'totalSteps': 21760, 'rewardStep': 0.9121107495425745, 'errorList': [], 'lossList': [0.0, -1.0971069294214248, 0.0, 0.8289182272925973, 0.0, 0.0, 0.0], 'rewardMean': 0.7907151421935302, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.0607608093283
'totalSteps': 23040, 'rewardStep': 0.9605428856052527, 'errorList': [0.1251643903492985, 0.09358787892846876, 0.04076563706192254, 0.0784381398279356, 0.12074354121918109, 0.06711508735054939, 0.06076384496148197, 0.07691825161877029, 0.09187658750913313, 0.018774245393305907, 0.028559787123507988, 0.06758572140475624, 0.0465663789072229, 0.030895266361621126, 0.025264641704788995, 0.06945672646707286, 0.05248535531248358, 0.02909572179367736, 0.026580217750577433, 0.053378874981445994, 0.07323477859235536, 0.09067387195341908, 0.06122921309461201, 0.05011518829573535, 0.04226815400386721, 0.04906034725854453, 0.04653138635252766, 0.06628265751536885, 0.04976086255771192, 0.05508767270683248, 0.05577181851636453, 0.08180564789234887, 0.03401091597333445, 0.0668542504181792, 0.04814172209906695, 0.06569419373795848, 0.09318528385829672, 0.06188618554028062, 0.028177352079170725, 0.123655749472245, 0.07739833969978968, 0.033565134076421185, 0.053747435464074705, 0.03454414690425874, 0.044473230731501265, 0.10855257458808916, 0.09172312042370784, 0.06829134110491199, 0.15265536112577904, 0.08913742260891719], 'lossList': [0.0, -1.0583372128009796, 0.0, 0.7275464896857738, 0.0, 0.0, 0.0], 'rewardMean': 0.8056140840178541, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1212.6870356530326, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=114.06
