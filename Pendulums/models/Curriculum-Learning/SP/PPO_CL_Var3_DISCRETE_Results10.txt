#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 10
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9248751944253679, 'errorList': [], 'lossList': [0.0, -1.407356454730034, 0.0, 27.204375113248826, 0.0, 0.0, 0.0], 'rewardMean': 0.8623370774621655, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.1036728716099
'totalSteps': 3840, 'rewardStep': 0.6395554860146846, 'errorList': [], 'lossList': [0.0, -1.3999825465679168, 0.0, 33.922742812633516, 0.0, 0.0, 0.0], 'rewardMean': 0.7880765469796719, 'totalEpisodes': 11, 'stepsPerEpisode': 263, 'rewardPerEpisode': 201.59114429570403
'totalSteps': 5120, 'rewardStep': 0.7105120977953714, 'errorList': [], 'lossList': [0.0, -1.4016036814451218, 0.0, 25.431722968816757, 0.0, 0.0, 0.0], 'rewardMean': 0.7686854346835967, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.9747604482272
'totalSteps': 6400, 'rewardStep': 0.8776264739140326, 'errorList': [], 'lossList': [0.0, -1.396086457967758, 0.0, 294.7174253845215, 0.0, 0.0, 0.0], 'rewardMean': 0.7904736425296839, 'totalEpisodes': 81, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.573345529483984
'totalSteps': 7680, 'rewardStep': 0.7449825840041657, 'errorList': [], 'lossList': [0.0, -1.3861727565526962, 0.0, 136.1167431640625, 0.0, 0.0, 0.0], 'rewardMean': 0.7828917994420976, 'totalEpisodes': 138, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.510348398633916
'totalSteps': 8960, 'rewardStep': 0.7582021853681513, 'errorList': [], 'lossList': [0.0, -1.3640913236141206, 0.0, 66.20997682571411, 0.0, 0.0, 0.0], 'rewardMean': 0.7793647117172482, 'totalEpisodes': 194, 'stepsPerEpisode': 29, 'rewardPerEpisode': 20.125971090478945
'totalSteps': 10240, 'rewardStep': 0.7063026724651013, 'errorList': [], 'lossList': [0.0, -1.359569786787033, 0.0, 55.331638221740725, 0.0, 0.0, 0.0], 'rewardMean': 0.7702319568107298, 'totalEpisodes': 230, 'stepsPerEpisode': 25, 'rewardPerEpisode': 18.00496812635277
'totalSteps': 11520, 'rewardStep': 0.5969614859541265, 'errorList': [], 'lossList': [0.0, -1.3514973372220993, 0.0, 43.012106819152834, 0.0, 0.0, 0.0], 'rewardMean': 0.7509796822711071, 'totalEpisodes': 247, 'stepsPerEpisode': 68, 'rewardPerEpisode': 57.81165062749838
'totalSteps': 12800, 'rewardStep': 0.7728289251118614, 'errorList': [], 'lossList': [0.0, -1.329085457921028, 0.0, 29.035560755729676, 0.0, 0.0, 0.0], 'rewardMean': 0.7531646065551826, 'totalEpisodes': 255, 'stepsPerEpisode': 87, 'rewardPerEpisode': 66.37894430899773
'totalSteps': 14080, 'rewardStep': 0.662759552332232, 'errorList': [], 'lossList': [0.0, -1.2983290243148804, 0.0, 19.478837707042693, 0.0, 0.0, 0.0], 'rewardMean': 0.7394606657385094, 'totalEpisodes': 261, 'stepsPerEpisode': 58, 'rewardPerEpisode': 48.89661008924408
'totalSteps': 15360, 'rewardStep': 0.8875066174737, 'errorList': [], 'lossList': [0.0, -1.274757616519928, 0.0, 33.70719245195389, 0.0, 0.0, 0.0], 'rewardMean': 0.7357238080433427, 'totalEpisodes': 270, 'stepsPerEpisode': 69, 'rewardPerEpisode': 61.53437439417651
'totalSteps': 16640, 'rewardStep': 0.9581039335599133, 'errorList': [7.593028144326311, 28.388251651789055, 65.1472598933625, 121.45021133379312, 0.08295333785856629, 139.31023230595883, 85.11881409909847, 60.837832030070025, 121.45318944337741, 150.94696463237636, 91.19869450564008, 1.7546068242857977, 116.92599513283022, 99.20598327061525, 81.3060530391468, 96.95232471725426, 3.6805735531268544, 62.64122826175451, 83.23557438225491, 127.64052126270458, 112.38941914586333, 81.1078310199342, 64.42507102048526, 48.65742084836852, 69.70004341331558, 90.43115852950272, 34.06617891508548, 126.42456704235089, 106.17971998649675, 128.9459934605872, 12.494639044454436, 0.3982652296161786, 141.0059399531619, 79.13274942972339, 106.46047586199235, 58.69024550746131, 105.72294310327959, 133.30190910829356, 100.1152587338356, 107.65167082757252, 89.84233482657619, 19.036976924321735, 6.090694971302339, 99.34100523092067, 53.76392648217911, 2.473546012767937, 57.38968298008052, 24.2921355576623, 101.86752448975938, 85.96479147910864], 'lossList': [0.0, -1.275839563012123, 0.0, 11.321288182735444, 0.0, 0.0, 0.0], 'rewardMean': 0.7675786527978656, 'totalEpisodes': 276, 'stepsPerEpisode': 139, 'rewardPerEpisode': 117.60866646264371, 'successfulTests': 1
'totalSteps': 17920, 'rewardStep': 0.8858449167935716, 'errorList': [], 'lossList': [0.0, -1.2739405524730683, 0.0, 9.688765584230422, 0.0, 0.0, 0.0], 'rewardMean': 0.7851119346976856, 'totalEpisodes': 280, 'stepsPerEpisode': 254, 'rewardPerEpisode': 217.60119955500022
'totalSteps': 19200, 'rewardStep': 0.32190170922481826, 'errorList': [], 'lossList': [0.0, -1.2685778671503067, 0.0, 26.179855637550354, 0.0, 0.0, 0.0], 'rewardMean': 0.729539458228764, 'totalEpisodes': 286, 'stepsPerEpisode': 157, 'rewardPerEpisode': 110.62691783768052
'totalSteps': 20480, 'rewardStep': 0.8484719425445965, 'errorList': [], 'lossList': [0.0, -1.262872846722603, 0.0, 6.277323766946792, 0.0, 0.0, 0.0], 'rewardMean': 0.7398883940828072, 'totalEpisodes': 293, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.991793952419712
'totalSteps': 21760, 'rewardStep': 0.9458131723638573, 'errorList': [0.9867148256774416, 30.645057218909063, 3.203710080432814, 75.80432610289199, 0.5985497836126378, 1.8622938736552332, 2.696960893937298, 2.2031016867220776, 5.671725111946156, 57.10690675436698, 43.86477719003359, 5.030564159592592, 64.87933888813102, 49.06486266608669, 22.02484602487075, 20.23668097622404, 1.5036733282740253, 49.68087301870324, 1.9832245714458032, 30.363550756069362, 48.17031988705988, 2.9445576545403465, 8.90743419719614, 3.5824806819848654, 0.4066892972395363, 1.308504125991153, 21.22669223847945, 50.87645442101413, 34.898213672066866, 0.5788726480033238, 0.4931194339590918, 3.9970744401751315, 6.285912753083533, 23.208961585803824, 43.714807005839745, 19.411151344162636, 2.4696164894519184, 36.45458173973256, 0.6155858525638597, 10.784662593146049, 61.230991922080015, 10.586653487909969, 2.7176498751440166, 15.375478835790409, 3.225138547131607, 7.2862241346701495, 8.78523971121109, 30.74902897673252, 25.478870606157994, 0.21208249908621288], 'lossList': [0.0, -1.2623920875787735, 0.0, 27.255519901514052, 0.0, 0.0, 0.0], 'rewardMean': 0.7586494927823778, 'totalEpisodes': 297, 'stepsPerEpisode': 60, 'rewardPerEpisode': 53.83914079370916, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.6446303169582521, 'errorList': [], 'lossList': [0.0, -1.2624391210079193, 0.0, 4.621122623085975, 0.0, 0.0, 0.0], 'rewardMean': 0.7524822572316929, 'totalEpisodes': 300, 'stepsPerEpisode': 271, 'rewardPerEpisode': 232.52405316448483
'totalSteps': 24320, 'rewardStep': 0.9369315335661753, 'errorList': [0.09723595729836851, 0.12372226038981035, 0.04646963207191311, 0.035093258939019045, 0.13535670652905937, 0.06896648812825687, 0.12529783350184553, 0.13357392812850077, 0.0554455288776157, 0.1353216062995219, 0.02635483869554573, 0.1212562785383237, 0.043791624273274134, 0.056759690342097546, 0.04035946216528355, 0.042499097609096356, 0.03852752298900049, 0.10734838104575349, 0.04050826003860548, 0.16969719288764723, 0.09102051497208631, 0.0396608201295776, 0.10220991010172875, 0.03715520384258626, 0.028039427087760666, 0.12622468579440738, 0.07461276734523395, 0.04377730623916796, 0.06160858169703798, 0.08201823186751504, 0.06223494219339836, 0.04631710340952963, 0.10507029103923067, 0.03762608453816862, 0.13686583187166784, 0.04201014697747793, 0.1475826920372985, 0.07195279470049908, 0.06303313404482057, 0.023705840108667994, 0.047896481855019116, 0.07512142833172597, 0.13018893326897557, 0.09609780349956853, 0.12123890651428246, 0.027513669268592084, 0.16297018325714777, 0.07028356862674266, 0.10248855715175974, 0.035419066795730016], 'lossList': [0.0, -1.2626737201213836, 0.0, 4.290670785903931, 0.0, 0.0, 0.0], 'rewardMean': 0.7864792619928977, 'totalEpisodes': 302, 'stepsPerEpisode': 65, 'rewardPerEpisode': 56.74330852867222, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=125.73
