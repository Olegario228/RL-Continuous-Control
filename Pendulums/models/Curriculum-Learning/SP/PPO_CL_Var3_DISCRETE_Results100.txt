#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 100
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7615115961618314, 'errorList': [], 'lossList': [0.0, -1.416316899061203, 0.0, 34.04166961669922, 0.0, 0.0, 0.0], 'rewardMean': 0.6028671685686746, 'totalEpisodes': 54, 'stepsPerEpisode': 22, 'rewardPerEpisode': 14.045930817942493
'totalSteps': 3840, 'rewardStep': 0.24376022335712494, 'errorList': [], 'lossList': [0.0, -1.4158412671089173, 0.0, 39.97222840309143, 0.0, 0.0, 0.0], 'rewardMean': 0.4831648534981581, 'totalEpisodes': 69, 'stepsPerEpisode': 95, 'rewardPerEpisode': 70.66786970327684
'totalSteps': 5120, 'rewardStep': 0.6881630550608158, 'errorList': [], 'lossList': [0.0, -1.4050467377901077, 0.0, 30.977895011901854, 0.0, 0.0, 0.0], 'rewardMean': 0.5344144038888226, 'totalEpisodes': 78, 'stepsPerEpisode': 193, 'rewardPerEpisode': 146.5557322671595
'totalSteps': 6400, 'rewardStep': 0.5538731503602887, 'errorList': [], 'lossList': [0.0, -1.4017364168167115, 0.0, 16.829771225452422, 0.0, 0.0, 0.0], 'rewardMean': 0.5383061531831158, 'totalEpisodes': 85, 'stepsPerEpisode': 160, 'rewardPerEpisode': 120.06529699410683
'totalSteps': 7680, 'rewardStep': 0.856045953496996, 'errorList': [], 'lossList': [0.0, -1.3981187486648559, 0.0, 25.058083976507188, 0.0, 0.0, 0.0], 'rewardMean': 0.5912627865687625, 'totalEpisodes': 89, 'stepsPerEpisode': 418, 'rewardPerEpisode': 339.72563791799064
'totalSteps': 8960, 'rewardStep': 0.54459494524558, 'errorList': [], 'lossList': [0.0, -1.3821819698810578, 0.0, 11.590758373737335, 0.0, 0.0, 0.0], 'rewardMean': 0.5845959520940222, 'totalEpisodes': 94, 'stepsPerEpisode': 123, 'rewardPerEpisode': 86.98111651911053
'totalSteps': 10240, 'rewardStep': 0.8630470801906349, 'errorList': [], 'lossList': [0.0, -1.385882369875908, 0.0, 24.941113127470018, 0.0, 0.0, 0.0], 'rewardMean': 0.6194023431060988, 'totalEpisodes': 95, 'stepsPerEpisode': 109, 'rewardPerEpisode': 82.97908141761282
'totalSteps': 11520, 'rewardStep': 0.45727965644497964, 'errorList': [], 'lossList': [0.0, -1.3793936443328858, 0.0, 287.59994926452634, 0.0, 0.0, 0.0], 'rewardMean': 0.6013887112548634, 'totalEpisodes': 127, 'stepsPerEpisode': 57, 'rewardPerEpisode': 41.27393008241271
'totalSteps': 12800, 'rewardStep': 0.6573746998079151, 'errorList': [], 'lossList': [0.0, -1.3801028656959533, 0.0, 113.25550811767579, 0.0, 0.0, 0.0], 'rewardMean': 0.6069873101101685, 'totalEpisodes': 159, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.681693513931494
'totalSteps': 14080, 'rewardStep': 0.8163552930028488, 'errorList': [], 'lossList': [0.0, -1.3837403559684753, 0.0, 41.80593466758728, 0.0, 0.0, 0.0], 'rewardMean': 0.6442005653129016, 'totalEpisodes': 180, 'stepsPerEpisode': 63, 'rewardPerEpisode': 44.89500513409153
'totalSteps': 15360, 'rewardStep': 0.7003943123424357, 'errorList': [], 'lossList': [0.0, -1.381158185005188, 0.0, 21.097836632728576, 0.0, 0.0, 0.0], 'rewardMean': 0.638088836930962, 'totalEpisodes': 195, 'stepsPerEpisode': 70, 'rewardPerEpisode': 51.1721655453093
'totalSteps': 16640, 'rewardStep': 0.7015323495831854, 'errorList': [], 'lossList': [0.0, -1.3734422355890274, 0.0, 15.050666072368621, 0.0, 0.0, 0.0], 'rewardMean': 0.683866049553568, 'totalEpisodes': 201, 'stepsPerEpisode': 102, 'rewardPerEpisode': 83.19375156166807
'totalSteps': 17920, 'rewardStep': 0.7375270243760388, 'errorList': [], 'lossList': [0.0, -1.3635462701320649, 0.0, 9.495355260372161, 0.0, 0.0, 0.0], 'rewardMean': 0.6888024464850904, 'totalEpisodes': 206, 'stepsPerEpisode': 86, 'rewardPerEpisode': 61.31969629803915
'totalSteps': 19200, 'rewardStep': 0.8062027725516808, 'errorList': [], 'lossList': [0.0, -1.3499476891756057, 0.0, 22.366071796417238, 0.0, 0.0, 0.0], 'rewardMean': 0.7140354087042297, 'totalEpisodes': 210, 'stepsPerEpisode': 300, 'rewardPerEpisode': 241.59608201603186
'totalSteps': 20480, 'rewardStep': 0.9619332811699673, 'errorList': [0.33361254155769804, 0.3326332804583666, 0.3361049644652904, 0.33527173015270556, 0.3320885801309454, 0.33509545272853497, 0.3360183451272697, 0.33263132712337434, 0.33748744352117777, 0.33460923434863804, 0.3349856407338944, 0.3943840935697087, 0.3332086295945963, 0.35034774304596633, 0.3351341303675376, 0.3334094441109747, 0.33441669455447853, 0.33744032233746557, 0.3430997814620461, 0.3343219856471725, 0.3351535027141484, 0.33517292382169533, 0.33219634396604275, 0.33559659335322944, 0.33516233214148866, 0.3358703913908961, 0.3355723532737634, 0.334849499201708, 0.3352640871576069, 0.3607168097590143, 0.3333203758807448, 0.3338227953192571, 0.3315831455868188, 0.33343398553423287, 0.34024690945800845, 0.3410137914381556, 0.33277539288664776, 0.33359183027776046, 0.3319166946027681, 0.3331199270822158, 0.33502843069743854, 0.33423432972031936, 0.3346677349476926, 0.33117956776813384, 0.3334546876685358, 0.3366295947190049, 0.33394382448571475, 0.33891271016065794, 0.33464922019768295, 0.33506579796233754], 'lossList': [0.0, -1.3442388516664505, 0.0, 6.548796933889389, 0.0, 0.0, 0.0], 'rewardMean': 0.7246241414715267, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.5694758091963, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8376227284542144, 'errorList': [], 'lossList': [0.0, -1.3209136015176772, 0.0, 4.06293096780777, 0.0, 0.0, 0.0], 'rewardMean': 0.75392691979239, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1010.4848535193198
'totalSteps': 23040, 'rewardStep': 0.7627273998376308, 'errorList': [], 'lossList': [0.0, -1.2823597699403764, 0.0, 3.791240029633045, 0.0, 0.0, 0.0], 'rewardMean': 0.7438949517570896, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.744960395276
'totalSteps': 24320, 'rewardStep': 0.7370605177520709, 'errorList': [], 'lossList': [0.0, -1.2479378032684325, 0.0, 3.0746038311719897, 0.0, 0.0, 0.0], 'rewardMean': 0.7718730378877987, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.2157787912809
'totalSteps': 25600, 'rewardStep': 0.8980052377422455, 'errorList': [], 'lossList': [0.0, -1.2207220691442489, 0.0, 1.650780871361494, 0.0, 0.0, 0.0], 'rewardMean': 0.7959360916812319, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1109.4856105994972
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.18
