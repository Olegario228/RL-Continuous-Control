#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 102
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092971427115871, 'errorList': [], 'lossList': [0.0, -1.4231055521965026, 0.0, 34.86121163368225, 0.0, 0.0, 0.0], 'rewardMean': 0.8624672422476483, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.76661789737042
'totalSteps': 3840, 'rewardStep': 0.5562513755896646, 'errorList': [], 'lossList': [0.0, -1.4137311124801635, 0.0, 33.320800943374635, 0.0, 0.0, 0.0], 'rewardMean': 0.7603952866949871, 'totalEpisodes': 82, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.96326734396544
'totalSteps': 5120, 'rewardStep': 0.41657226176954365, 'errorList': [], 'lossList': [0.0, -1.393478239774704, 0.0, 22.643683416843416, 0.0, 0.0, 0.0], 'rewardMean': 0.6744395304636263, 'totalEpisodes': 91, 'stepsPerEpisode': 209, 'rewardPerEpisode': 111.1692412224747
'totalSteps': 6400, 'rewardStep': 0.5781114103780555, 'errorList': [], 'lossList': [0.0, -1.3759211069345474, 0.0, 55.08653503417969, 0.0, 0.0, 0.0], 'rewardMean': 0.6551739064465121, 'totalEpisodes': 100, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5781114103780555
'totalSteps': 7680, 'rewardStep': 0.6657041553964848, 'errorList': [], 'lossList': [0.0, -1.3629135596752167, 0.0, 43.17267139911652, 0.0, 0.0, 0.0], 'rewardMean': 0.6569289479381742, 'totalEpisodes': 106, 'stepsPerEpisode': 111, 'rewardPerEpisode': 85.581363560485
'totalSteps': 8960, 'rewardStep': 0.8353175770618145, 'errorList': [], 'lossList': [0.0, -1.350562744140625, 0.0, 21.278632204532624, 0.0, 0.0, 0.0], 'rewardMean': 0.6824130378129799, 'totalEpisodes': 110, 'stepsPerEpisode': 214, 'rewardPerEpisode': 168.60997503580717
'totalSteps': 10240, 'rewardStep': 0.6930639894198799, 'errorList': [], 'lossList': [0.0, -1.3427255243062972, 0.0, 19.16885508775711, 0.0, 0.0, 0.0], 'rewardMean': 0.6837444067638424, 'totalEpisodes': 112, 'stepsPerEpisode': 222, 'rewardPerEpisode': 183.9959048300429
'totalSteps': 11520, 'rewardStep': 0.21448526740684293, 'errorList': [], 'lossList': [0.0, -1.3375404411554337, 0.0, 190.58109188079834, 0.0, 0.0, 0.0], 'rewardMean': 0.6316045023908425, 'totalEpisodes': 133, 'stepsPerEpisode': 141, 'rewardPerEpisode': 93.16794472465595
'totalSteps': 12800, 'rewardStep': 0.6833401101286621, 'errorList': [], 'lossList': [0.0, -1.3321603727340698, 0.0, 79.07719271659852, 0.0, 0.0, 0.0], 'rewardMean': 0.6367780631646245, 'totalEpisodes': 148, 'stepsPerEpisode': 128, 'rewardPerEpisode': 103.43541509349753
'totalSteps': 14080, 'rewardStep': 0.8866516324793787, 'errorList': [], 'lossList': [0.0, -1.323202302455902, 0.0, 43.09891215801239, 0.0, 0.0, 0.0], 'rewardMean': 0.6438794922341914, 'totalEpisodes': 159, 'stepsPerEpisode': 61, 'rewardPerEpisode': 47.862264356192384
'totalSteps': 15360, 'rewardStep': 0.7689466693835643, 'errorList': [], 'lossList': [0.0, -1.3147438323497773, 0.0, 35.461636762619015, 0.0, 0.0, 0.0], 'rewardMean': 0.6298444449013891, 'totalEpisodes': 166, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.22740855412615
'totalSteps': 16640, 'rewardStep': 0.6017153530460706, 'errorList': [], 'lossList': [0.0, -1.3098509657382964, 0.0, 19.120077941417694, 0.0, 0.0, 0.0], 'rewardMean': 0.6343908426470296, 'totalEpisodes': 171, 'stepsPerEpisode': 424, 'rewardPerEpisode': 247.99286012076104
'totalSteps': 17920, 'rewardStep': 0.7679245816599435, 'errorList': [], 'lossList': [0.0, -1.2875406384468078, 0.0, 9.368655532598495, 0.0, 0.0, 0.0], 'rewardMean': 0.6695260746360696, 'totalEpisodes': 174, 'stepsPerEpisode': 182, 'rewardPerEpisode': 155.50894302345534
'totalSteps': 19200, 'rewardStep': 0.8495244133478461, 'errorList': [], 'lossList': [0.0, -1.2553067481517792, 0.0, 11.879983340501786, 0.0, 0.0, 0.0], 'rewardMean': 0.6966673749330488, 'totalEpisodes': 177, 'stepsPerEpisode': 113, 'rewardPerEpisode': 101.32089199562049
'totalSteps': 20480, 'rewardStep': 0.8182548311553782, 'errorList': [], 'lossList': [0.0, -1.2256289899349213, 0.0, 4.558114247918129, 0.0, 0.0, 0.0], 'rewardMean': 0.7119224425089381, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.5524940014675
'totalSteps': 21760, 'rewardStep': 0.7964570740020787, 'errorList': [], 'lossList': [0.0, -1.1935521113872527, 0.0, 3.694255991578102, 0.0, 0.0, 0.0], 'rewardMean': 0.7080363922029644, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.644590500667
'totalSteps': 23040, 'rewardStep': 0.9379417065615395, 'errorList': [0.047271383061322204, 0.05081884133176136, 0.03524244100687149, 0.06423096200188387, 0.05828839437127755, 0.05861823676388937, 0.06618034269172923, 0.05080430431647777, 0.053772572953923584, 0.057848876133349846, 0.06508055137660997, 0.02984673287971575, 0.03892223931437471, 0.04768179094146355, 0.05625866416198982, 0.05262703114406368, 0.035596103888959806, 0.07090978036290431, 0.06895991731088512, 0.03266334123005789, 0.06681247373963518, 0.05483508954085526, 0.04461013371346474, 0.0435145941935426, 0.03559243071228655, 0.0406159294383295, 0.03468262376779539, 0.05123781306455677, 0.08276981121526737, 0.06117169361954943, 0.03137147083663661, 0.04319968923102671, 0.04393871798741222, 0.06579608502974137, 0.06648227749136144, 0.046578961549699247, 0.06700703097890795, 0.04172262601017652, 0.032868491755166276, 0.05330432499726928, 0.02963588768157323, 0.038545389410181895, 0.053072716314791685, 0.0816527437305607, 0.08572015581148167, 0.0677865610868196, 0.04715461288504751, 0.06937584030117802, 0.049183452251925594, 0.0570523461018362], 'lossList': [0.0, -1.145368636250496, 0.0, 2.477936489135027, 0.0, 0.0, 0.0], 'rewardMean': 0.7325241639171305, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.4514203785773, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=76.36
