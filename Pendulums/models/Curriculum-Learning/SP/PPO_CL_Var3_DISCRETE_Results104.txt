#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 104
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764237844444184, 'errorList': [], 'lossList': [0.0, -1.4117648071050644, 0.0, 34.56727179527283, 0.0, 0.0, 0.0], 'rewardMean': 0.8487569592979127, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.540084423708798
'totalSteps': 3840, 'rewardStep': 0.8301048750001292, 'errorList': [], 'lossList': [0.0, -1.4150108939409256, 0.0, 39.124620037078856, 0.0, 0.0, 0.0], 'rewardMean': 0.8425395978653182, 'totalEpisodes': 84, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.84030782755015
'totalSteps': 5120, 'rewardStep': 0.6848389580966191, 'errorList': [], 'lossList': [0.0, -1.4063812613487243, 0.0, 28.86642453432083, 0.0, 0.0, 0.0], 'rewardMean': 0.8031144379231434, 'totalEpisodes': 90, 'stepsPerEpisode': 117, 'rewardPerEpisode': 92.77188059566005
'totalSteps': 6400, 'rewardStep': 0.522264938468509, 'errorList': [], 'lossList': [0.0, -1.3970475417375565, 0.0, 23.584555982351304, 0.0, 0.0, 0.0], 'rewardMean': 0.7469445380322165, 'totalEpisodes': 94, 'stepsPerEpisode': 456, 'rewardPerEpisode': 368.87075139144315
'totalSteps': 7680, 'rewardStep': 0.4707934727118634, 'errorList': [], 'lossList': [0.0, -1.3926397609710692, 0.0, 25.67977596640587, 0.0, 0.0, 0.0], 'rewardMean': 0.7009193604788243, 'totalEpisodes': 96, 'stepsPerEpisode': 428, 'rewardPerEpisode': 314.24768187725965
'totalSteps': 8960, 'rewardStep': 0.9312512349808809, 'errorList': [], 'lossList': [0.0, -1.3727877336740493, 0.0, 21.57163408994675, 0.0, 0.0, 0.0], 'rewardMean': 0.7338239139791181, 'totalEpisodes': 97, 'stepsPerEpisode': 1141, 'rewardPerEpisode': 871.4339552471587
'totalSteps': 10240, 'rewardStep': 0.9896051248271795, 'errorList': [213.68122444773854, 230.19043909601083, 239.2965033109278, 232.2722889451731, 240.7866242412664, 230.18122486395885, 231.3345844936665, 220.40834965296798, 208.06767749270276, 237.55251625464564, 206.84975997693888, 249.642586939204, 228.35651511161842, 165.77948377675804, 242.42337231076579, 241.25208423567707, 243.29903507338653, 242.85809763653444, 240.78623337553248, 233.17725722668558, 194.84039868524854, 190.8146038302962, 234.8541212580269, 109.42800010612058, 219.162236162324, 214.20045992041187, 244.18568984551717, 209.63355853654653, 244.18422228727695, 218.42389337363394, 200.64896226671792, 232.74833870775805, 244.18242345610335, 202.02581644137783, 205.64726965837505, 233.50838239937232, 196.80457074969493, 217.45112335700125, 223.24783508664962, 214.3189809199728, 209.53379240214596, 207.8467033737891, 247.79353847775263, 232.04910120246592, 227.7654643290053, 245.94030127484845, 220.240796982067, 212.8347954854314, 236.73735456271888, 198.88675495295487], 'lossList': [0.0, -1.3599976676702499, 0.0, 8.412683138549328, 0.0, 0.0, 0.0], 'rewardMean': 0.7657965653351257, 'totalEpisodes': 97, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1031.4388916758824, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8224402118287365, 'errorList': [], 'lossList': [0.0, -1.347601546049118, 0.0, 325.26703811645507, 0.0, 0.0, 0.0], 'rewardMean': 0.7720903038344158, 'totalEpisodes': 127, 'stepsPerEpisode': 25, 'rewardPerEpisode': 18.663860377223457
'totalSteps': 12800, 'rewardStep': 0.6522883871255503, 'errorList': [], 'lossList': [0.0, -1.3462189120054244, 0.0, 125.56443046569824, 0.0, 0.0, 0.0], 'rewardMean': 0.7601101121635292, 'totalEpisodes': 156, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.832601285680823
'totalSteps': 14080, 'rewardStep': 0.9358017528992106, 'errorList': [276.98615338386435, 245.99295291942062, 103.27485047635233, 244.1990099721399, 251.70353905261177, 229.2651576288735, 267.77307173637104, 233.22746391443727, 242.87543403911698, 282.21727887384174, 259.8518860423567, 249.38048656104183, 268.1054337378741, 167.27937027132006, 246.42855413231047, 267.44316813084885, 215.8535013909807, 220.37785061714587, 234.84746141950598, 254.72982565782553, 272.2454589572207, 70.41580788016807, 268.9919723393531, 247.11425148961862, 262.01182178662447, 153.69235852474725, 191.5786266606782, 230.2770471071239, 270.6542502935652, 147.68693765763084, 215.16406775411608, 220.62142056753723, 115.07218006711929, 263.12228774268186, 198.18445835318963, 247.70942443614996, 110.16830547296242, 250.67373185502197, 279.70587427431894, 263.9486414362626, 139.22654578146992, 244.16455766539374, 217.81508088212505, 250.1051244402102, 194.34664695111604, 196.93881611206803, 166.1519379008737, 254.1212953467614, 238.8457153642723, 112.45888269186432], 'lossList': [0.0, -1.3448362237215041, 0.0, 38.26287315368652, 0.0, 0.0, 0.0], 'rewardMean': 0.7615812740383097, 'totalEpisodes': 178, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.856707812613295, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.8770177262095962, 'errorList': [], 'lossList': [0.0, -1.3428653287887573, 0.0, 21.010933957099915, 0.0, 0.0, 0.0], 'rewardMean': 0.7716406682148275, 'totalEpisodes': 191, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.298138546211963
'totalSteps': 16640, 'rewardStep': 0.7026588264439863, 'errorList': [], 'lossList': [0.0, -1.3290597331523895, 0.0, 13.70965404510498, 0.0, 0.0, 0.0], 'rewardMean': 0.7588960633592132, 'totalEpisodes': 201, 'stepsPerEpisode': 73, 'rewardPerEpisode': 54.55686399012248
'totalSteps': 17920, 'rewardStep': 0.43273992127883154, 'errorList': [], 'lossList': [0.0, -1.322408617734909, 0.0, 12.874284205436707, 0.0, 0.0, 0.0], 'rewardMean': 0.7336861596774346, 'totalEpisodes': 207, 'stepsPerEpisode': 73, 'rewardPerEpisode': 50.133635351543965
'totalSteps': 19200, 'rewardStep': 0.6038166907912659, 'errorList': [], 'lossList': [0.0, -1.3152912706136703, 0.0, 9.503655644655227, 0.0, 0.0, 0.0], 'rewardMean': 0.7418413349097102, 'totalEpisodes': 213, 'stepsPerEpisode': 129, 'rewardPerEpisode': 106.62410937971556
'totalSteps': 20480, 'rewardStep': 0.6269958667894291, 'errorList': [], 'lossList': [0.0, -1.295228744149208, 0.0, 5.77647811293602, 0.0, 0.0, 0.0], 'rewardMean': 0.7574615743174666, 'totalEpisodes': 218, 'stepsPerEpisode': 182, 'rewardPerEpisode': 134.13931500076413
'totalSteps': 21760, 'rewardStep': 0.8344938966740669, 'errorList': [], 'lossList': [0.0, -1.3099184215068818, 0.0, 5.251545300483704, 0.0, 0.0, 0.0], 'rewardMean': 0.7477858404867852, 'totalEpisodes': 222, 'stepsPerEpisode': 40, 'rewardPerEpisode': 35.41116668271246
'totalSteps': 23040, 'rewardStep': 0.9165034244037863, 'errorList': [], 'lossList': [0.0, -1.3205356442928313, 0.0, 5.240351818799972, 0.0, 0.0, 0.0], 'rewardMean': 0.740475670444446, 'totalEpisodes': 225, 'stepsPerEpisode': 309, 'rewardPerEpisode': 269.72293088956314
'totalSteps': 24320, 'rewardStep': 0.878323989649495, 'errorList': [], 'lossList': [0.0, -1.2931671810150147, 0.0, 2.9559324434399605, 0.0, 0.0, 0.0], 'rewardMean': 0.7460640482265217, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.5628166621166
'totalSteps': 25600, 'rewardStep': 0.9544844531025549, 'errorList': [0.04555472458078162, 0.03597868125176145, 0.028585420104953914, 0.0284214492358202, 0.08019633434140615, 0.04083003600608789, 0.06496863254233544, 0.028919804232885225, 0.028517019344672472, 0.028779697654483294, 0.04919476689397827, 0.03785069174581643, 0.03584683718418204, 0.028084738280817423, 0.028476064755870188, 0.07262323007715527, 0.06287680757318152, 0.03279128575359678, 0.030441913550499407, 0.02844461195567218, 0.04447858491365283, 0.028153784358573276, 0.0285442276229941, 0.028336990269945295, 0.0880325793688515, 0.03322192474293105, 0.03237885373197803, 0.06614150800680219, 0.02832151998447924, 0.051506648913401794, 0.02848361570537953, 0.048488512673915914, 0.028557106705766014, 0.09091123048801064, 0.02872762291669766, 0.02855775230950676, 0.0721035300870472, 0.0638683045983074, 0.02850479175572368, 0.028735054508407614, 0.08328586814955327, 0.0696352151996939, 0.03562055510612108, 0.028373922467593796, 0.05006709464809519, 0.03802364696681072, 0.028006452800893904, 0.04739524663800884, 0.02889802390305432, 0.03631034969415131], 'lossList': [0.0, -1.2550290554761887, 0.0, 2.7780974301695824, 0.0, 0.0, 0.0], 'rewardMean': 0.7762836548242221, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.9082640281458, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=126.43
