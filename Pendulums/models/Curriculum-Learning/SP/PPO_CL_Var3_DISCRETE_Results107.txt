#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 107
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.6024665948879426, 'errorList': [], 'lossList': [0.0, -1.4165354841947555, 0.0, 23.93550584554672, 0.0, 0.0, 0.0], 'rewardMean': 0.6981182980340469, 'totalEpisodes': 20, 'stepsPerEpisode': 134, 'rewardPerEpisode': 81.29176411951762
'totalSteps': 3840, 'rewardStep': 0.7476596497586331, 'errorList': [], 'lossList': [0.0, -1.4073269003629685, 0.0, 25.12256549358368, 0.0, 0.0, 0.0], 'rewardMean': 0.7146320819422423, 'totalEpisodes': 23, 'stepsPerEpisode': 491, 'rewardPerEpisode': 360.6189836227023
'totalSteps': 5120, 'rewardStep': 0.8392984180292674, 'errorList': [], 'lossList': [0.0, -1.3948573023080826, 0.0, 19.90495609998703, 0.0, 0.0, 0.0], 'rewardMean': 0.7457986659639986, 'totalEpisodes': 27, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.942805791393013
'totalSteps': 6400, 'rewardStep': 0.16544997743437073, 'errorList': [], 'lossList': [0.0, -1.396834184527397, 0.0, 32.123555212020875, 0.0, 0.0, 0.0], 'rewardMean': 0.6297289282580729, 'totalEpisodes': 29, 'stepsPerEpisode': 558, 'rewardPerEpisode': 389.36792736240625
'totalSteps': 7680, 'rewardStep': 0.6943710165787682, 'errorList': [], 'lossList': [0.0, -1.393035066127777, 0.0, 15.24554249703884, 0.0, 0.0, 0.0], 'rewardMean': 0.6405026096448555, 'totalEpisodes': 29, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.6548360624729
'totalSteps': 8960, 'rewardStep': 0.8917068343024768, 'errorList': [], 'lossList': [0.0, -1.4008522897958755, 0.0, 10.7542341786623, 0.0, 0.0, 0.0], 'rewardMean': 0.6763889274530871, 'totalEpisodes': 29, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.4722960039269
'totalSteps': 10240, 'rewardStep': 0.8669411784193025, 'errorList': [], 'lossList': [0.0, -1.3751342755556106, 0.0, 11.015152317881585, 0.0, 0.0, 0.0], 'rewardMean': 0.7002079588238641, 'totalEpisodes': 29, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.03802815021
'totalSteps': 11520, 'rewardStep': 0.6708153981868579, 'errorList': [], 'lossList': [0.0, -1.346806037425995, 0.0, 484.29175270080566, 0.0, 0.0, 0.0], 'rewardMean': 0.6969421187530856, 'totalEpisodes': 61, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.955405933110114
'totalSteps': 12800, 'rewardStep': 0.940090771996544, 'errorList': [247.04718900325938, 244.5959058737699, 225.1880472778748, 211.7460625349996, 233.1408368932825, 236.29316906206637, 236.1174688140119, 241.29764655620735, 242.51576764745775, 235.6270311407976, 222.73646607450428, 187.3226988759965, 240.6448665673943, 222.18627967663028, 222.87147007134706, 227.31908772771334, 203.9247721534416, 229.13257349500412, 252.29759206334862, 233.1385992355121, 245.01375004689078, 218.24216329825182, 221.06866359954768, 242.11636455634442, 246.4952205560848, 210.26093496897678, 250.50977294248236, 210.78507912554173, 233.74486109728664, 243.85131299598908, 231.0487795072276, 227.13147995524446, 236.15963691759427, 246.12466229445545, 171.3927789087597, 226.65004514532768, 212.44574495438152, 243.19496965192025, 221.49467993535973, 241.9982071642727, 171.2426879132806, 226.14556942023336, 239.33394996870177, 218.88496174774832, 245.0818986873324, 241.2347604899675, 231.92122666446213, 211.02729951325026, 214.77819283534242, 213.69687557093806], 'lossList': [0.0, -1.3470641553401947, 0.0, 132.7577137565613, 0.0, 0.0, 0.0], 'rewardMean': 0.7212569840774314, 'totalEpisodes': 92, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.447140929478682, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.5648054639062645, 'errorList': [], 'lossList': [0.0, -1.3479624772071839, 0.0, 63.39715992927551, 0.0, 0.0, 0.0], 'rewardMean': 0.6983605303500428, 'totalEpisodes': 124, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.179310448050003
'totalSteps': 15360, 'rewardStep': 0.4873689743337145, 'errorList': [], 'lossList': [0.0, -1.348245592713356, 0.0, 28.183259181976318, 0.0, 0.0, 0.0], 'rewardMean': 0.6868507682946199, 'totalEpisodes': 152, 'stepsPerEpisode': 2, 'rewardPerEpisode': 0.9807705614279731
'totalSteps': 16640, 'rewardStep': 0.5889781012957488, 'errorList': [], 'lossList': [0.0, -1.3442804819345475, 0.0, 23.24742042541504, 0.0, 0.0, 0.0], 'rewardMean': 0.6709826134483314, 'totalEpisodes': 172, 'stepsPerEpisode': 16, 'rewardPerEpisode': 9.369127575107191
'totalSteps': 17920, 'rewardStep': 0.7545530232638855, 'errorList': [], 'lossList': [0.0, -1.3374792504310609, 0.0, 13.213778696060182, 0.0, 0.0, 0.0], 'rewardMean': 0.6625080739717933, 'totalEpisodes': 185, 'stepsPerEpisode': 41, 'rewardPerEpisode': 33.80465434316011
'totalSteps': 19200, 'rewardStep': 0.5200601285938331, 'errorList': [], 'lossList': [0.0, -1.3298974132537842, 0.0, 12.400775389671326, 0.0, 0.0, 0.0], 'rewardMean': 0.6979690890877396, 'totalEpisodes': 196, 'stepsPerEpisode': 105, 'rewardPerEpisode': 85.7997910422953
'totalSteps': 20480, 'rewardStep': 0.8418514788905295, 'errorList': [], 'lossList': [0.0, -1.319693727493286, 0.0, 7.648623596429825, 0.0, 0.0, 0.0], 'rewardMean': 0.7127171353189157, 'totalEpisodes': 205, 'stepsPerEpisode': 49, 'rewardPerEpisode': 37.18129735710407
'totalSteps': 21760, 'rewardStep': 0.8669562239639064, 'errorList': [], 'lossList': [0.0, -1.3140020149946212, 0.0, 7.078545998334885, 0.0, 0.0, 0.0], 'rewardMean': 0.7102420742850586, 'totalEpisodes': 212, 'stepsPerEpisode': 47, 'rewardPerEpisode': 38.06217063902432
'totalSteps': 23040, 'rewardStep': 0.8193130203365815, 'errorList': [], 'lossList': [0.0, -1.3058683013916015, 0.0, 5.339502764344215, 0.0, 0.0, 0.0], 'rewardMean': 0.7054792584767865, 'totalEpisodes': 218, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.022555060709536
'totalSteps': 24320, 'rewardStep': 0.8335079960536153, 'errorList': [], 'lossList': [0.0, -1.2939830899238587, 0.0, 6.390321689248085, 0.0, 0.0, 0.0], 'rewardMean': 0.7217485182634623, 'totalEpisodes': 220, 'stepsPerEpisode': 603, 'rewardPerEpisode': 513.009658468214
'totalSteps': 25600, 'rewardStep': 0.6898324553894423, 'errorList': [], 'lossList': [0.0, -1.2714828580617905, 0.0, 5.7241896212100984, 0.0, 0.0, 0.0], 'rewardMean': 0.6967226866027522, 'totalEpisodes': 221, 'stepsPerEpisode': 716, 'rewardPerEpisode': 617.7997105580853
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.02
