#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 11
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7407526851551015, 'errorList': [], 'lossList': [0.0, -1.4381537955999375, 0.0, 33.31262235403061, 0.0, 0.0, 0.0], 'rewardMean': 0.6287732934272772, 'totalEpisodes': 12, 'stepsPerEpisode': 75, 'rewardPerEpisode': 63.584726357374365
'totalSteps': 3840, 'rewardStep': 0.8637334784555419, 'errorList': [], 'lossList': [0.0, -1.4452659833431243, 0.0, 27.569838765263558, 0.0, 0.0, 0.0], 'rewardMean': 0.7070933551033655, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.4813934950321
'totalSteps': 5120, 'rewardStep': 0.5759928308585094, 'errorList': [], 'lossList': [0.0, -1.4195507580041886, 0.0, 26.616081984639166, 0.0, 0.0, 0.0], 'rewardMean': 0.6743182240421515, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.5796509636435
'totalSteps': 6400, 'rewardStep': 0.920832246054322, 'errorList': [], 'lossList': [0.0, -1.4181356072425841, 0.0, 311.6863954925537, 0.0, 0.0, 0.0], 'rewardMean': 0.7236210284445856, 'totalEpisodes': 75, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.355515929099035
'totalSteps': 7680, 'rewardStep': 0.44120585667453693, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6429309793674288, 'totalEpisodes': 123, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.4856612705726353
'totalSteps': 8960, 'rewardStep': 0.47023793269806125, 'errorList': [], 'lossList': [0.0, -1.4193803584575653, 0.0, 124.64065410614013, 0.0, 0.0, 0.0], 'rewardMean': 0.6213443485337579, 'totalEpisodes': 164, 'stepsPerEpisode': 27, 'rewardPerEpisode': 20.635507174503275
'totalSteps': 10240, 'rewardStep': 0.477875496606716, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5926505781483494, 'totalEpisodes': 212, 'stepsPerEpisode': 10, 'rewardPerEpisode': 5.661801386889275
'totalSteps': 11520, 'rewardStep': 0.9669577871555305, 'errorList': [168.3933743233432, 176.74584912085754, 178.1400192062379, 169.66142510360916, 175.65731146450022, 166.64950731102084, 175.9191744203778, 176.39624535065894, 150.65726718177925, 166.9855523172414, 157.86035260305653, 147.3179311404089, 156.99296625220148, 162.96735946156622, 150.31599071432487, 168.91494922989136, 179.84931663153915, 182.50033143436323, 188.70313460840083, 183.1253764107144, 184.4663824689836, 186.41141953507514, 143.00406470881225, 178.6802229070414, 158.92479782084578, 165.24706982412445, 138.619648767309, 188.43738257861597, 181.75507949242464, 163.73787624894103, 169.58669103964462, 158.53002299254325, 158.26466686117953, 174.39616824138977, 162.2537379818709, 136.1292518125352, 176.0735254406048, 177.5547906268618, 155.92273373088184, 166.54462730105135, 166.40371861354745, 163.75288534664003, 174.56080581498497, 179.61822511391833, 138.35175868858897, 177.06617844353954, 190.34399662870476, 176.40959592546602, 168.6517832773387, 173.00805290582906], 'lossList': [0.0, -1.4100625610351563, 0.0, 66.13799798965454, 0.0, 0.0, 0.0], 'rewardMean': 0.6376669666939573, 'totalEpisodes': 248, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.678665235313874, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.8224316019362705, 'errorList': [], 'lossList': [0.0, -1.3892554891109468, 0.0, 59.41651933670044, 0.0, 0.0, 0.0], 'rewardMean': 0.6458348583720742, 'totalEpisodes': 277, 'stepsPerEpisode': 111, 'rewardPerEpisode': 85.74457422492618
'totalSteps': 14080, 'rewardStep': 0.5852770997316887, 'errorList': [], 'lossList': [0.0, -1.3649267226457595, 0.0, 41.47249357223511, 0.0, 0.0, 0.0], 'rewardMean': 0.6179892204996887, 'totalEpisodes': 294, 'stepsPerEpisode': 92, 'rewardPerEpisode': 66.70296029228844
'totalSteps': 15360, 'rewardStep': 0.9343776324294742, 'errorList': [48.12914322755565, 68.21309759565379, 83.11434460027463, 85.82327599906444, 46.99111235804515, 36.27751599371641, 83.54765609973659, 51.45016198973196, 42.76649486158112, 140.0855383999252, 70.89565096065861, 62.5733568800069, 70.0482170289354, 31.129538358674626, 94.45863341435637, 71.75554670820071, 43.640040287387336, 49.66110228708495, 34.71814862289834, 76.9398712567737, 94.11888765287623, 132.2619128729521, 118.55433033255204, 134.7048860098641, 74.59736848937433, 111.46151404842732, 45.72043367742094, 37.32519647050327, 20.40672881782567, 66.72226255582319, 8.655581743125557, 147.86723856909208, 57.919007061790836, 18.331044101760174, 46.498557402155846, 1.9252276679386349, 137.70253259490428, 14.900160816661236, 46.99635207525059, 39.22803911291855, 99.78930775540623, 37.939863936916254, 140.65666127142336, 30.003748856034168, 61.050706861991245, 49.364664175786096, 99.54982348392717, 56.23133099552547, 79.24679339029754, 14.621145991777313], 'lossList': [0.0, -1.3359204429388045, 0.0, 17.144594435691833, 0.0, 0.0, 0.0], 'rewardMean': 0.6538277006567854, 'totalEpisodes': 300, 'stepsPerEpisode': 241, 'rewardPerEpisode': 199.96969186095774, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6870043901639658, 'errorList': [], 'lossList': [0.0, -1.308136920928955, 0.0, 19.761789121627807, 0.0, 0.0, 0.0], 'rewardMean': 0.6304449150677497, 'totalEpisodes': 306, 'stepsPerEpisode': 255, 'rewardPerEpisode': 201.53514738796744
'totalSteps': 17920, 'rewardStep': 0.8479129376371414, 'errorList': [], 'lossList': [0.0, -1.2940871530771256, 0.0, 10.658540979623794, 0.0, 0.0, 0.0], 'rewardMean': 0.6711156231640102, 'totalEpisodes': 312, 'stepsPerEpisode': 167, 'rewardPerEpisode': 147.88720012587407
'totalSteps': 19200, 'rewardStep': 0.9880033791542184, 'errorList': [0.14251153837193656, 1.1136654454003245, 0.18688063081648965, 1.0897901665111458, 1.6471656293774817, 1.7166884597885934, 3.6047282116697206, 0.14418237965722347, 0.20849939476652468, 0.9511705385998093, 1.1751257053051107, 4.05314157141354, 1.1819842170157513, 0.3276759571337112, 1.3315064668072907, 0.1739725872469717, 2.5494678298354985, 0.30682752689075127, 2.493206520759169, 2.812450464477658, 2.570254784759667, 3.183023361674022, 0.6921952551318811, 2.8099010418371115, 2.4646216616322834, 1.592300680474538, 1.3998341193291677, 0.339871280815544, 1.5952809442827571, 0.4732287346646403, 3.1996774022349017, 2.2768672989120766, 1.2164554209549168, 0.0562960841858642, 0.4934905616444596, 0.7502983162957104, 0.43428177063831647, 0.38347220447124275, 7.017539560631121, 0.7369917784656823, 3.7041351626235914, 0.9945442631148556, 0.39184044591068506, 0.9855894531222847, 0.043598221732707255, 6.012497260222716, 1.5430617572035585, 1.1198928692639716, 2.1826941755282365, 0.45210086363873997], 'lossList': [0.0, -1.296306192278862, 0.0, 6.5363380038738255, 0.0, 0.0, 0.0], 'rewardMean': 0.7257953754119784, 'totalEpisodes': 315, 'stepsPerEpisode': 155, 'rewardPerEpisode': 133.2495757710847, 'successfulTests': 6
'totalSteps': 20480, 'rewardStep': 0.44888807782119955, 'errorList': [], 'lossList': [0.0, -1.3114020210504531, 0.0, 6.512005695700646, 0.0, 0.0, 0.0], 'rewardMean': 0.723660389924292, 'totalEpisodes': 317, 'stepsPerEpisode': 191, 'rewardPerEpisode': 151.36549265514887
'totalSteps': 21760, 'rewardStep': 0.6394589082590002, 'errorList': [], 'lossList': [0.0, -1.3043050587177276, 0.0, 8.457725350856782, 0.0, 0.0, 0.0], 'rewardMean': 0.7398187310895206, 'totalEpisodes': 320, 'stepsPerEpisode': 363, 'rewardPerEpisode': 308.1629730322157
'totalSteps': 23040, 'rewardStep': 0.7296940980707749, 'errorList': [], 'lossList': [0.0, -1.2843589198589325, 0.0, 3.2563731983304023, 0.0, 0.0, 0.0], 'rewardMean': 0.7650005912359263, 'totalEpisodes': 320, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.432047150695
'totalSteps': 24320, 'rewardStep': 0.8344645495986132, 'errorList': [], 'lossList': [0.0, -1.2491941392421722, 0.0, 1.2972997081279756, 0.0, 0.0, 0.0], 'rewardMean': 0.7517512674802347, 'totalEpisodes': 320, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.885484385075
'totalSteps': 25600, 'rewardStep': 0.8773963707207475, 'errorList': [], 'lossList': [0.0, -1.1857021898031235, 0.0, 1.3182069608569145, 0.0, 0.0, 0.0], 'rewardMean': 0.7572477443586825, 'totalEpisodes': 320, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.5609093021146
#maxSuccessfulTests=6, maxSuccessfulTestsAtStep=19200, timeSpent=119.78
