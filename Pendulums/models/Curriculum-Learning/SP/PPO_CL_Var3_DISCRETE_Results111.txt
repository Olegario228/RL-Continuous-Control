#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 111
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7407526851551015, 'errorList': [], 'lossList': [0.0, -1.4381537955999375, 0.0, 33.31262235403061, 0.0, 0.0, 0.0], 'rewardMean': 0.6287732934272772, 'totalEpisodes': 12, 'stepsPerEpisode': 75, 'rewardPerEpisode': 63.584726357374365
'totalSteps': 3840, 'rewardStep': 0.8637334784555419, 'errorList': [], 'lossList': [0.0, -1.4452659833431243, 0.0, 27.569838765263558, 0.0, 0.0, 0.0], 'rewardMean': 0.7070933551033655, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.4813934950321
'totalSteps': 5120, 'rewardStep': 0.5759928308585094, 'errorList': [], 'lossList': [0.0, -1.4195507580041886, 0.0, 26.616081984639166, 0.0, 0.0, 0.0], 'rewardMean': 0.6743182240421515, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.5796509636435
'totalSteps': 6400, 'rewardStep': 0.9463638398839381, 'errorList': [], 'lossList': [0.0, -1.4143244582414627, 0.0, 23.917352037727834, 0.0, 0.0, 0.0], 'rewardMean': 0.7287273472105088, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.0192218254592
'totalSteps': 7680, 'rewardStep': 0.8617472247978639, 'errorList': [], 'lossList': [0.0, -1.3968755543231963, 0.0, 19.54419825017452, 0.0, 0.0, 0.0], 'rewardMean': 0.7508973268084014, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.1240871870643
'totalSteps': 8960, 'rewardStep': 0.9714283295614423, 'errorList': [], 'lossList': [0.0, -1.3814146465063095, 0.0, 11.72468193076551, 0.0, 0.0, 0.0], 'rewardMean': 0.7824017557731215, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.3036865049728
'totalSteps': 10240, 'rewardStep': 0.9756889001125563, 'errorList': [124.72778810543542, 122.27871643475738, 118.03082950798132, 120.70715397986662, 110.13748863864892, 122.78427507188547, 107.72378370403418, 122.61451437607622, 119.16155001306865, 123.71652562848809, 109.72193250090112, 125.61789996993319, 124.48113527250472, 116.95912010334497, 121.01594057568842, 125.19891622324803, 108.5057362785508, 116.82446429253798, 124.56574101779225, 122.12016859895179, 113.87231714535108, 114.32441416830133, 118.63667492843798, 121.72192240005499, 112.36145553283181, 122.96875106245145, 123.1987838136843, 126.06049800100782, 124.28900496709667, 123.21437163787338, 108.45676747001454, 111.6712964624484, 112.9547387167163, 124.06750597456977, 113.45678098556574, 114.33802218002998, 124.98334806054113, 123.55897005645693, 105.94755321699694, 127.23412086345418, 114.29353669508204, 122.81716163007424, 125.76148263537662, 123.53284870450375, 116.18215965093736, 116.83140841723124, 118.24444065869481, 119.17962944957286, 122.67742525199688, 123.71361226007559], 'lossList': [0.0, -1.351256411075592, 0.0, 10.117696880400182, 0.0, 0.0, 0.0], 'rewardMean': 0.8065626488155508, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.3408677044822, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.923456143845831, 'errorList': [], 'lossList': [0.0, -1.3141562616825104, 0.0, 838.3637628173828, 0.0, 0.0, 0.0], 'rewardMean': 0.8195508149300265, 'totalEpisodes': 53, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.923456143845831
'totalSteps': 12800, 'rewardStep': 0.8863443728988606, 'errorList': [], 'lossList': [0.0, -1.3134424728155136, 0.0, 604.7025350952149, 0.0, 0.0, 0.0], 'rewardMean': 0.82623017072691, 'totalEpisodes': 90, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.567298328526071
'totalSteps': 14080, 'rewardStep': 0.4993708586413711, 'errorList': [], 'lossList': [0.0, -1.312723280787468, 0.0, 380.2676641082764, 0.0, 0.0, 0.0], 'rewardMean': 0.8244878664211017, 'totalEpisodes': 128, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.264998216289975
'totalSteps': 15360, 'rewardStep': 0.6447942083049242, 'errorList': [], 'lossList': [0.0, -1.3111039477586746, 0.0, 66.10673148155212, 0.0, 0.0, 0.0], 'rewardMean': 0.8148920187360839, 'totalEpisodes': 162, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.796901856869823
'totalSteps': 16640, 'rewardStep': 0.942089176578209, 'errorList': [252.32939054493846, 248.0807768181097, 249.5414692518249, 259.62869263516603, 256.72795782346964, 293.1515779091688, 226.42346114102452, 272.01142467782535, 284.36332686096307, 266.9717238212468, 290.4561238974143, 227.72115342324128, 239.9621401539836, 250.08057572804427, 284.8088746863584, 290.62694830269, 292.97129539976333, 277.4504076054791, 282.0886730442992, 272.48247025582305, 222.48775189048553, 266.5248033010858, 269.6270921597314, 229.93900678740167, 287.5292511895274, 281.20562157291863, 242.03690622367338, 128.2369391069455, 203.91342251476442, 282.3086990950893, 270.34080509372535, 279.4094185923938, 173.62443796689521, 294.5425637560319, 263.6924040271324, 259.5888695316259, 222.1731256156881, 251.90976998067936, 267.5123901683731, 284.7311274319101, 198.4254599991756, 266.89255366681545, 182.41382695949255, 290.37732794201423, 232.53562433701455, 269.549287164617, 281.4488479873177, 290.91876330501077, 260.9666881090531, 280.7385001297039], 'lossList': [0.0, -1.3101983612775803, 0.0, 28.808662695884706, 0.0, 0.0, 0.0], 'rewardMean': 0.8227275885483506, 'totalEpisodes': 188, 'stepsPerEpisode': 30, 'rewardPerEpisode': 27.461829832526263, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.4982803425542005, 'errorList': [], 'lossList': [0.0, -1.3091615271568298, 0.0, 20.25450490474701, 0.0, 0.0, 0.0], 'rewardMean': 0.8149563397179197, 'totalEpisodes': 204, 'stepsPerEpisode': 100, 'rewardPerEpisode': 81.17677347888625
'totalSteps': 19200, 'rewardStep': 0.8384862918309511, 'errorList': [], 'lossList': [0.0, -1.3055904000997542, 0.0, 18.1949675822258, 0.0, 0.0, 0.0], 'rewardMean': 0.804168584912621, 'totalEpisodes': 211, 'stepsPerEpisode': 120, 'rewardPerEpisode': 107.3938846332145
'totalSteps': 20480, 'rewardStep': 0.28489253860530234, 'errorList': [], 'lossList': [0.0, -1.305095179080963, 0.0, 19.649769406318665, 0.0, 0.0, 0.0], 'rewardMean': 0.746483116293365, 'totalEpisodes': 216, 'stepsPerEpisode': 156, 'rewardPerEpisode': 117.194459993966
'totalSteps': 21760, 'rewardStep': 0.5091916143507731, 'errorList': [], 'lossList': [0.0, -1.3056554567813874, 0.0, 24.559246549606325, 0.0, 0.0, 0.0], 'rewardMean': 0.700259444772298, 'totalEpisodes': 223, 'stepsPerEpisode': 176, 'rewardPerEpisode': 137.9796316935936
'totalSteps': 23040, 'rewardStep': 0.7995911387397492, 'errorList': [], 'lossList': [0.0, -1.298432878255844, 0.0, 14.77528578042984, 0.0, 0.0, 0.0], 'rewardMean': 0.6826496686350172, 'totalEpisodes': 227, 'stepsPerEpisode': 254, 'rewardPerEpisode': 200.89901652313137
'totalSteps': 24320, 'rewardStep': 0.5581471460350603, 'errorList': [], 'lossList': [0.0, -1.2886701178550721, 0.0, 14.647075579166412, 0.0, 0.0, 0.0], 'rewardMean': 0.6461187688539402, 'totalEpisodes': 229, 'stepsPerEpisode': 259, 'rewardPerEpisode': 180.78789086012782
'totalSteps': 25600, 'rewardStep': 0.7155182285650878, 'errorList': [], 'lossList': [0.0, -1.2601643711328507, 0.0, 4.5263316333293915, 0.0, 0.0, 0.0], 'rewardMean': 0.6290361544205629, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1030.209239304997
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=104.08
