#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 112
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.817604565351236, 'errorList': [], 'lossList': [0.0, -1.4439547497034073, 0.0, 25.714645471572876, 0.0, 0.0, 0.0], 'rewardMean': 0.627817578778555, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.8221866151944
'totalSteps': 3840, 'rewardStep': 0.8819465627971335, 'errorList': [], 'lossList': [0.0, -1.452749683856964, 0.0, 40.4849648976326, 0.0, 0.0, 0.0], 'rewardMean': 0.7125272401180812, 'totalEpisodes': 14, 'stepsPerEpisode': 487, 'rewardPerEpisode': 393.1458591944858
'totalSteps': 5120, 'rewardStep': 0.6909629918169633, 'errorList': [], 'lossList': [0.0, -1.4402446401119233, 0.0, 19.477284516096116, 0.0, 0.0, 0.0], 'rewardMean': 0.7071361780428018, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 915.6590066339056
'totalSteps': 6400, 'rewardStep': 0.5316313862161144, 'errorList': [], 'lossList': [0.0, -1.424210580587387, 0.0, 30.624091376066207, 0.0, 0.0, 0.0], 'rewardMean': 0.6720352196774643, 'totalEpisodes': 15, 'stepsPerEpisode': 560, 'rewardPerEpisode': 442.0557397243816
'totalSteps': 7680, 'rewardStep': 0.7409094539208414, 'errorList': [], 'lossList': [0.0, -1.414549109339714, 0.0, 15.928519613742829, 0.0, 0.0, 0.0], 'rewardMean': 0.6835142587180272, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.5592715977277
'totalSteps': 8960, 'rewardStep': 0.882100466627852, 'errorList': [], 'lossList': [0.0, -1.413646804690361, 0.0, 14.669114577919245, 0.0, 0.0, 0.0], 'rewardMean': 0.7118837169908593, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.0470906827923
'totalSteps': 10240, 'rewardStep': 0.9346663498313197, 'errorList': [120.62917933153798, 118.2842872646, 120.06545790564624, 121.73519914731276, 116.30582846072973, 121.12303599318314, 118.4108516557587, 123.1609626830684, 121.92913263805724, 121.74669545522221, 122.29559373930512, 115.86083090739082, 118.88458881766121, 118.93152946593409, 119.30274627151572, 121.29276286430323, 113.69871311814249, 107.85464144126279, 118.45289219640817, 119.11045430404715, 118.30692876194192, 103.7136841515013, 118.46020040868596, 116.29977301130202, 109.42260453666978, 118.34089508050604, 110.48901805407408, 116.9302583250858, 119.69810837212455, 124.19159142836673, 107.95899559456329, 89.71950454877535, 117.8631001803561, 119.18897431788447, 111.04680090254423, 121.53785434163436, 97.26817242880593, 122.4449486686771, 110.6264358843151, 121.74497109811112, 122.00565031379097, 120.32943923908415, 115.92349864201469, 113.26648894266079, 111.88491850739166, 113.13362347019176, 102.41207320600827, 115.90839039353334, 121.04593473868985, 107.3348090464305], 'lossList': [0.0, -1.3861601746082306, 0.0, 12.750700650438667, 0.0, 0.0, 0.0], 'rewardMean': 0.7397315460959168, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.665478473258, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.7580510582342672, 'errorList': [], 'lossList': [0.0, -1.3715683460235595, 0.0, 646.1319897460937, 0.0, 0.0, 0.0], 'rewardMean': 0.7417670474446224, 'totalEpisodes': 50, 'stepsPerEpisode': 42, 'rewardPerEpisode': 35.08628087267355
'totalSteps': 12800, 'rewardStep': 0.8767397800647034, 'errorList': [], 'lossList': [0.0, -1.3712681722640991, 0.0, 433.0713912963867, 0.0, 0.0, 0.0], 'rewardMean': 0.7552643207066305, 'totalEpisodes': 82, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.3851740143025
'totalSteps': 14080, 'rewardStep': 0.552892600324062, 'errorList': [], 'lossList': [0.0, -1.3715387600660325, 0.0, 176.5648031616211, 0.0, 0.0, 0.0], 'rewardMean': 0.7667505215184492, 'totalEpisodes': 123, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.6965917206460435
'totalSteps': 15360, 'rewardStep': 0.828245628041655, 'errorList': [], 'lossList': [0.0, -1.376031352877617, 0.0, 60.35629102706909, 0.0, 0.0, 0.0], 'rewardMean': 0.7678146277874912, 'totalEpisodes': 160, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.828245628041655
'totalSteps': 16640, 'rewardStep': 0.4162703844870748, 'errorList': [], 'lossList': [0.0, -1.3785740500688553, 0.0, 29.13188482284546, 0.0, 0.0, 0.0], 'rewardMean': 0.7212470099564853, 'totalEpisodes': 190, 'stepsPerEpisode': 48, 'rewardPerEpisode': 33.86794792486841
'totalSteps': 17920, 'rewardStep': 0.839323872090324, 'errorList': [], 'lossList': [0.0, -1.3753987658023834, 0.0, 15.7159885597229, 0.0, 0.0, 0.0], 'rewardMean': 0.7360830979838214, 'totalEpisodes': 208, 'stepsPerEpisode': 69, 'rewardPerEpisode': 63.29118737917027
'totalSteps': 19200, 'rewardStep': 0.5937834526952485, 'errorList': [], 'lossList': [0.0, -1.3680831164121627, 0.0, 17.98553587436676, 0.0, 0.0, 0.0], 'rewardMean': 0.7422983046317349, 'totalEpisodes': 225, 'stepsPerEpisode': 36, 'rewardPerEpisode': 26.60016645553513
'totalSteps': 20480, 'rewardStep': 0.8387254620386038, 'errorList': [], 'lossList': [0.0, -1.362227133512497, 0.0, 11.326450653076172, 0.0, 0.0, 0.0], 'rewardMean': 0.752079905443511, 'totalEpisodes': 236, 'stepsPerEpisode': 83, 'rewardPerEpisode': 73.69732087287662
'totalSteps': 21760, 'rewardStep': 0.7807743110184111, 'errorList': [], 'lossList': [0.0, -1.3621298277378082, 0.0, 12.867077932357788, 0.0, 0.0, 0.0], 'rewardMean': 0.741947289882567, 'totalEpisodes': 244, 'stepsPerEpisode': 185, 'rewardPerEpisode': 160.58603615115493
'totalSteps': 23040, 'rewardStep': 0.9144883739954781, 'errorList': [], 'lossList': [0.0, -1.3591201943159104, 0.0, 6.589183788299561, 0.0, 0.0, 0.0], 'rewardMean': 0.7399294922989827, 'totalEpisodes': 253, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.637057340777282
'totalSteps': 24320, 'rewardStep': 0.7084706015176133, 'errorList': [], 'lossList': [0.0, -1.3433265203237534, 0.0, 6.243766907453537, 0.0, 0.0, 0.0], 'rewardMean': 0.7349714466273174, 'totalEpisodes': 259, 'stepsPerEpisode': 278, 'rewardPerEpisode': 232.86324076773826
'totalSteps': 25600, 'rewardStep': 0.7012053745683833, 'errorList': [], 'lossList': [0.0, -1.3134153920412064, 0.0, 7.854996966123581, 0.0, 0.0, 0.0], 'rewardMean': 0.7174180060776855, 'totalEpisodes': 264, 'stepsPerEpisode': 300, 'rewardPerEpisode': 259.06667713908547
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=85.95
