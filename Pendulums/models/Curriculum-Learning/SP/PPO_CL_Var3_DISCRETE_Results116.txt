#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 116
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8584804343243699, 'errorList': [], 'lossList': [0.0, -1.444619840979576, 0.0, 36.9149765253067, 0.0, 0.0, 0.0], 'rewardMean': 0.7085640764003625, 'totalEpisodes': 12, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.028341765022766
'totalSteps': 3840, 'rewardStep': 0.8945312046550165, 'errorList': [], 'lossList': [0.0, -1.4570085686445235, 0.0, 30.45998518407345, 0.0, 0.0, 0.0], 'rewardMean': 0.7705531191519137, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.9854716647518
'totalSteps': 5120, 'rewardStep': 0.6426887006587513, 'errorList': [], 'lossList': [0.0, -1.4410074639320374, 0.0, 29.687185053825377, 0.0, 0.0, 0.0], 'rewardMean': 0.7385870145286231, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.2280427395226
'totalSteps': 6400, 'rewardStep': 0.9462534970079105, 'errorList': [], 'lossList': [0.0, -1.4400340634584428, 0.0, 23.752558589577674, 0.0, 0.0, 0.0], 'rewardMean': 0.7801203110244805, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.7706869240055
'totalSteps': 7680, 'rewardStep': 0.8494213103252838, 'errorList': [], 'lossList': [0.0, -1.421427276134491, 0.0, 19.919883989840745, 0.0, 0.0, 0.0], 'rewardMean': 0.7916704775746144, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.0201867950414
'totalSteps': 8960, 'rewardStep': 0.9708906659178227, 'errorList': [], 'lossList': [0.0, -1.4051207172870637, 0.0, 11.891999492123723, 0.0, 0.0, 0.0], 'rewardMean': 0.8172733616236442, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.5452090213143
'totalSteps': 10240, 'rewardStep': 0.982301873499561, 'errorList': [101.72050816219009, 99.55041372996317, 96.27660136756325, 98.5647570611654, 91.5754463116741, 100.33423608101981, 88.77064119861676, 100.16711083530814, 97.49717566663524, 101.02157427875248, 89.956468716045, 102.29760384357179, 101.40852569437246, 95.0928275300066, 98.6409636776499, 102.07914533483986, 88.70290045267316, 96.26124051510416, 101.43550174550833, 99.95610802167533, 93.59552821324236, 93.31548550370343, 97.07662834751319, 99.5638880855312, 91.95826019936919, 100.5036330087002, 100.48969301886665, 102.68501549682622, 101.3159740085706, 100.46888294758519, 90.55647275426459, 91.08735832500477, 92.14333221993061, 101.10610174888227, 92.11827713153436, 93.49856267502717, 101.8839213909398, 100.87524962831505, 87.31935192325703, 103.69036593564658, 92.97529103790424, 100.22606908010661, 102.38576851606747, 100.97852507059021, 95.39405389518917, 95.75340772048456, 96.50954304940247, 97.54890452704754, 100.24426615064677, 100.98117635941409], 'lossList': [0.0, -1.3746557384729385, 0.0, 10.030746794119477, 0.0, 0.0, 0.0], 'rewardMean': 0.8379019256081339, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.609344454906, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9327851169940075, 'errorList': [94.72536937194626, 103.10750327390978, 98.94455647720783, 110.21503651693381, 107.10956696593236, 100.41451015571123, 100.27106607469229, 108.0265977269634, 105.26307622011363, 108.01779497800877, 106.81057656091896, 105.92174461352012, 105.40964612561278, 111.00968289896987, 90.60868517738587, 106.65139257408349, 103.47734162537309, 106.8981611343402, 106.11989640433492, 105.54866376679067, 97.42723540967877, 99.60494925577296, 104.04211085000424, 102.14630501423771, 106.28315404094458, 108.45512301527381, 103.48933495987774, 98.99110905634912, 104.36136339606907, 107.56946028581056, 107.86153579422633, 101.02448998825649, 106.11875480605035, 103.82076400027198, 106.30288300342802, 108.80959190886516, 100.20187030512021, 107.34505088586013, 108.44965480391977, 107.92130392476867, 104.94093555298195, 98.1305162584391, 85.2367349322533, 92.60162487278693, 109.02408659527354, 108.10419762478303, 103.58682100564081, 103.55244029306199, 109.07679297469043, 98.62767344521477], 'lossList': [0.0, -1.337594433426857, 0.0, 938.9583735656738, 0.0, 0.0, 0.0], 'rewardMean': 0.8484445024287866, 'totalEpisodes': 58, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.07487187466538, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.9597476715471602, 'errorList': [108.96001480854996, 106.24201949822336, 111.97635469729244, 115.41205784601682, 112.84166341661395, 112.94704847747984, 112.98873685228912, 117.46140379662826, 101.68117318915225, 114.85618838927621, 104.62945224667513, 103.05271617078297, 117.18850496792389, 113.40147506859243, 113.84185932920833, 113.98963637702536, 112.7624380565336, 111.06288490751777, 113.2273533962366, 110.42578635916995, 104.20142417151908, 114.52889713947185, 112.272485440255, 100.13574573991372, 91.87723748858907, 109.36160178946635, 112.83822023178777, 110.36415211692285, 100.06684244193103, 100.44597264972582, 117.24372505535186, 107.57907858146578, 105.76157196795212, 113.12954553696814, 110.3896549974643, 108.96140396929978, 106.72703004851607, 109.407713143007, 112.5744555866547, 109.59698519901484, 116.31639739028235, 108.52139332714057, 115.5019471190862, 101.79032838676218, 112.69636735402689, 107.81951424158252, 114.13059772278136, 113.59348423362202, 110.3523690286914, 105.02558635591028], 'lossList': [0.0, -1.336677839756012, 0.0, 551.4246629333496, 0.0, 0.0, 0.0], 'rewardMean': 0.8595748193406239, 'totalEpisodes': 90, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.559754805316931, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.6451606190767478, 'errorList': [], 'lossList': [0.0, -1.3361497402191163, 0.0, 386.7346668243408, 0.0, 0.0, 0.0], 'rewardMean': 0.868226109400663, 'totalEpisodes': 124, 'stepsPerEpisode': 41, 'rewardPerEpisode': 32.6601803768714
'totalSteps': 15360, 'rewardStep': 0.9561004952218193, 'errorList': [169.4799718610553, 151.7765044309208, 178.1640905801648, 176.2860766844666, 166.7312122585368, 183.86332797474395, 139.62262891710864, 183.7035298607221, 175.16343377291304, 173.95047956580257, 176.02659822952089, 162.21585809612375, 174.01372656339703, 177.45751078092042, 168.38195244415536, 161.27631229997536, 176.00344084748008, 172.26730546462542, 159.53700380198225, 146.3017451520977, 140.23976378458315, 181.7134439378762, 164.55077055106574, 154.38751045266935, 185.5666660782532, 150.96527235702283, 184.54711176364853, 126.36608810970644, 183.3529082432885, 183.25334411901488, 172.03699615166883, 161.30123015674585, 166.9965888671221, 171.81108256665462, 157.8508911216888, 169.01067411339585, 169.65940520707377, 175.3051758811217, 174.374143613698, 176.94574469222252, 154.90499996654162, 174.69267336595962, 160.53818692153035, 165.21197347864273, 176.61935995957302, 173.98232743341237, 140.87421416271917, 134.2761401307614, 152.21451289203387, 170.9322291049265], 'lossList': [0.0, -1.3335724973678589, 0.0, 81.23381229400634, 0.0, 0.0, 0.0], 'rewardMean': 0.8779881154904082, 'totalEpisodes': 161, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.588028831710374, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7516328340291839, 'errorList': [], 'lossList': [0.0, -1.3268591326475143, 0.0, 36.59139038085937, 0.0, 0.0, 0.0], 'rewardMean': 0.8636982784278248, 'totalEpisodes': 196, 'stepsPerEpisode': 27, 'rewardPerEpisode': 20.560732998597366
'totalSteps': 17920, 'rewardStep': 0.45311378334917507, 'errorList': [], 'lossList': [0.0, -1.3186619812250138, 0.0, 16.46911114692688, 0.0, 0.0, 0.0], 'rewardMean': 0.8447407866968673, 'totalEpisodes': 221, 'stepsPerEpisode': 59, 'rewardPerEpisode': 46.089002600275954
'totalSteps': 19200, 'rewardStep': 0.6000215036212594, 'errorList': [], 'lossList': [0.0, -1.3129184263944627, 0.0, 14.422413172721862, 0.0, 0.0, 0.0], 'rewardMean': 0.810117587358202, 'totalEpisodes': 238, 'stepsPerEpisode': 22, 'rewardPerEpisode': 13.240627047527218
'totalSteps': 20480, 'rewardStep': 0.9224245538084813, 'errorList': [], 'lossList': [0.0, -1.3058307540416718, 0.0, 14.920480616092682, 0.0, 0.0, 0.0], 'rewardMean': 0.8174179117065219, 'totalEpisodes': 246, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.643468745768288
'totalSteps': 21760, 'rewardStep': 0.06631919437863665, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6353624966405108, 'totalEpisodes': 251, 'stepsPerEpisode': 481, 'rewardPerEpisode': 407.57096141121
'totalSteps': 23040, 'rewardStep': 0.6340399296351948, 'errorList': [], 'lossList': [0.0, -1.3010212928056717, 0.0, 12.25623421907425, 0.0, 0.0, 0.0], 'rewardMean': 0.6054879779046295, 'totalEpisodes': 258, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.145924227940216
'totalSteps': 24320, 'rewardStep': 0.7949239496948115, 'errorList': [], 'lossList': [0.0, -1.3009833031892777, 0.0, 8.601765191555023, 0.0, 0.0, 0.0], 'rewardMean': 0.5890056057193946, 'totalEpisodes': 264, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.39046211192594
'totalSteps': 25600, 'rewardStep': 0.8771989505126141, 'errorList': [], 'lossList': [0.0, -1.2906746315956115, 0.0, 7.1898441129922865, 0.0, 0.0, 0.0], 'rewardMean': 0.6122094388629813, 'totalEpisodes': 268, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8771989505126141
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=144.63
