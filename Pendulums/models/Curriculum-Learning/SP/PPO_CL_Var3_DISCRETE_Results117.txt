#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 117
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.855185987524522, 'errorList': [], 'lossList': [0.0, -1.4427587568759919, 0.0, 27.66762019395828, 0.0, 0.0, 0.0], 'rewardMean': 0.66648588137366, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 654.8354046413383
'totalSteps': 3840, 'rewardStep': 0.84452893941359, 'errorList': [], 'lossList': [0.0, -1.4598970460891723, 0.0, 43.213913540840146, 0.0, 0.0, 0.0], 'rewardMean': 0.72583356738697, 'totalEpisodes': 12, 'stepsPerEpisode': 665, 'rewardPerEpisode': 518.9578920646809
'totalSteps': 5120, 'rewardStep': 0.934200902645594, 'errorList': [], 'lossList': [0.0, -1.4645401257276536, 0.0, 28.880606985092165, 0.0, 0.0, 0.0], 'rewardMean': 0.777925401201626, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.8862821958191
'totalSteps': 6400, 'rewardStep': 0.6884515116920435, 'errorList': [], 'lossList': [0.0, -1.4489372456073761, 0.0, 24.948446321487427, 0.0, 0.0, 0.0], 'rewardMean': 0.7600306232997095, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.2426607322516
'totalSteps': 7680, 'rewardStep': 0.8099342163712256, 'errorList': [], 'lossList': [0.0, -1.431065845489502, 0.0, 16.503789306879042, 0.0, 0.0, 0.0], 'rewardMean': 0.768347888811629, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.387686141491
'totalSteps': 8960, 'rewardStep': 0.8824313539780038, 'errorList': [], 'lossList': [0.0, -1.4277180850505828, 0.0, 16.270056975781916, 0.0, 0.0, 0.0], 'rewardMean': 0.7846455266925396, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.9963482209068
'totalSteps': 10240, 'rewardStep': 0.9605541416022123, 'errorList': [95.79916112444283, 93.62784145220233, 95.208504333023, 96.53872513666013, 94.64813395205397, 97.3728599398658, 94.07139464048163, 97.65824191781238, 96.5380599684588, 97.69891778404921, 97.03215748315168, 94.47550802416362, 94.2911334193525, 95.648742132524, 94.88745090668131, 97.42434167965126, 88.63001168203505, 89.99243238250749, 95.72420494024534, 95.93399414798976, 95.48032640239954, 85.67318372799033, 93.70026445759821, 92.91114801181155, 86.55324851348264, 93.74031693567228, 91.70364347262039, 94.63052959218247, 96.14556288268751, 99.4267790790045, 90.71124690253129, 85.84989879014334, 95.1564810217594, 94.36371519128639, 91.05735181532442, 96.03136943264302, 87.05799659350983, 97.0713110343135, 91.35561163629123, 97.88590984335407, 96.5113095093438, 95.41933194004156, 94.14834496549076, 93.00511808052248, 88.56442759585606, 92.25748357134455, 82.94761260244393, 92.1114155604328, 95.78568718441029, 75.7826825142064], 'lossList': [0.0, -1.4147237598896027, 0.0, 10.841518421173095, 0.0, 0.0, 0.0], 'rewardMean': 0.8066341035562486, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.0420891550243, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5460525413923291, 'errorList': [], 'lossList': [0.0, -1.3987512999773026, 0.0, 817.831067199707, 0.0, 0.0, 0.0], 'rewardMean': 0.7776805966491465, 'totalEpisodes': 55, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.430671946642694
'totalSteps': 12800, 'rewardStep': 0.6963692653030826, 'errorList': [], 'lossList': [0.0, -1.398239744901657, 0.0, 682.0872158813477, 0.0, 0.0, 0.0], 'rewardMean': 0.76954946351454, 'totalEpisodes': 99, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.995662553978597
'totalSteps': 14080, 'rewardStep': 0.7279047116012617, 'errorList': [], 'lossList': [0.0, -1.3977120929956437, 0.0, 473.1011360168457, 0.0, 0.0, 0.0], 'rewardMean': 0.7945613571523865, 'totalEpisodes': 141, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.074144352792686
'totalSteps': 15360, 'rewardStep': 0.5360277119668511, 'errorList': [], 'lossList': [0.0, -1.3963489264249802, 0.0, 227.06762603759765, 0.0, 0.0, 0.0], 'rewardMean': 0.7626455295966194, 'totalEpisodes': 183, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.964691706161394
'totalSteps': 16640, 'rewardStep': 0.5908631294518207, 'errorList': [], 'lossList': [0.0, -1.3937731164693832, 0.0, 85.62976713180542, 0.0, 0.0, 0.0], 'rewardMean': 0.7372789486004423, 'totalEpisodes': 224, 'stepsPerEpisode': 59, 'rewardPerEpisode': 49.49305227225067
'totalSteps': 17920, 'rewardStep': 0.9092832311340514, 'errorList': [], 'lossList': [0.0, -1.3930682975053787, 0.0, 41.89547694206238, 0.0, 0.0, 0.0], 'rewardMean': 0.7347871814492881, 'totalEpisodes': 256, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.370038134410004
'totalSteps': 19200, 'rewardStep': 0.5612943476617508, 'errorList': [], 'lossList': [0.0, -1.3954017901420592, 0.0, 31.225511054992676, 0.0, 0.0, 0.0], 'rewardMean': 0.722071465046259, 'totalEpisodes': 277, 'stepsPerEpisode': 43, 'rewardPerEpisode': 31.8816693581664
'totalSteps': 20480, 'rewardStep': 0.7864499709353546, 'errorList': [], 'lossList': [0.0, -1.392766101360321, 0.0, 14.705658671855927, 0.0, 0.0, 0.0], 'rewardMean': 0.7197230405026718, 'totalEpisodes': 289, 'stepsPerEpisode': 64, 'rewardPerEpisode': 51.46947916780674
'totalSteps': 21760, 'rewardStep': 0.9113075672411486, 'errorList': [], 'lossList': [0.0, -1.3863498693704606, 0.0, 24.36586401462555, 0.0, 0.0, 0.0], 'rewardMean': 0.7226106618289864, 'totalEpisodes': 297, 'stepsPerEpisode': 194, 'rewardPerEpisode': 165.5181745986576
'totalSteps': 23040, 'rewardStep': 0.9224250289858998, 'errorList': [], 'lossList': [0.0, -1.399744998216629, 0.0, 11.463975687026977, 0.0, 0.0, 0.0], 'rewardMean': 0.7187977505673551, 'totalEpisodes': 305, 'stepsPerEpisode': 215, 'rewardPerEpisode': 193.98945224613658
'totalSteps': 24320, 'rewardStep': 0.6328752576089892, 'errorList': [], 'lossList': [0.0, -1.4163215017318727, 0.0, 10.513691027164459, 0.0, 0.0, 0.0], 'rewardMean': 0.7274800221890211, 'totalEpisodes': 311, 'stepsPerEpisode': 102, 'rewardPerEpisode': 81.6551738431577
'totalSteps': 25600, 'rewardStep': 0.768672269981994, 'errorList': [], 'lossList': [0.0, -1.4065580266714095, 0.0, 8.530093834400176, 0.0, 0.0, 0.0], 'rewardMean': 0.7347103226569123, 'totalEpisodes': 317, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.831636874041518
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=82.8
