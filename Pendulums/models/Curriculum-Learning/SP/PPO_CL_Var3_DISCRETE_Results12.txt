#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 12
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.817604565351236, 'errorList': [], 'lossList': [0.0, -1.4439547497034073, 0.0, 25.714645471572876, 0.0, 0.0, 0.0], 'rewardMean': 0.627817578778555, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.8221866151944
'totalSteps': 3840, 'rewardStep': 0.8819465627971335, 'errorList': [], 'lossList': [0.0, -1.452749683856964, 0.0, 40.4849648976326, 0.0, 0.0, 0.0], 'rewardMean': 0.7125272401180812, 'totalEpisodes': 14, 'stepsPerEpisode': 487, 'rewardPerEpisode': 393.1458591944858
'totalSteps': 5120, 'rewardStep': 0.6909629918169633, 'errorList': [], 'lossList': [0.0, -1.4402446401119233, 0.0, 19.477284516096116, 0.0, 0.0, 0.0], 'rewardMean': 0.7071361780428018, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 915.6590066339056
'totalSteps': 6400, 'rewardStep': 0.8888659459651395, 'errorList': [], 'lossList': [0.0, -1.4297248476743698, 0.0, 235.00392524719237, 0.0, 0.0, 0.0], 'rewardMean': 0.7434821316272694, 'totalEpisodes': 70, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.437741151377782
'totalSteps': 7680, 'rewardStep': 0.9097146211210974, 'errorList': [], 'lossList': [0.0, -1.4289766180515289, 0.0, 105.30146884918213, 0.0, 0.0, 0.0], 'rewardMean': 0.7711875465429073, 'totalEpisodes': 142, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.806525790371257
'totalSteps': 8960, 'rewardStep': 0.4853698600125247, 'errorList': [], 'lossList': [0.0, -1.4258103579282762, 0.0, 58.68346387863159, 0.0, 0.0, 0.0], 'rewardMean': 0.7303564484671384, 'totalEpisodes': 179, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.764147427525834
'totalSteps': 10240, 'rewardStep': 0.5818636661208783, 'errorList': [], 'lossList': [0.0, -1.431111485362053, 0.0, 63.08503807067871, 0.0, 0.0, 0.0], 'rewardMean': 0.7117948506738558, 'totalEpisodes': 210, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.189603646326198
'totalSteps': 11520, 'rewardStep': 0.7088302530432902, 'errorList': [], 'lossList': [0.0, -1.4313696753978729, 0.0, 53.031540660858155, 0.0, 0.0, 0.0], 'rewardMean': 0.7114654509371263, 'totalEpisodes': 235, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.018039648018583
'totalSteps': 12800, 'rewardStep': 0.5821958281801478, 'errorList': [], 'lossList': [0.0, -1.425664234161377, 0.0, 41.70292646408081, 0.0, 0.0, 0.0], 'rewardMean': 0.6985384886614285, 'totalEpisodes': 254, 'stepsPerEpisode': 105, 'rewardPerEpisode': 77.68875245179996
'totalSteps': 14080, 'rewardStep': 0.7163762847319739, 'errorList': [], 'lossList': [0.0, -1.4093636137247085, 0.0, 33.399720392227174, 0.0, 0.0, 0.0], 'rewardMean': 0.7263730579140384, 'totalEpisodes': 269, 'stepsPerEpisode': 43, 'rewardPerEpisode': 33.3935759622358
'totalSteps': 15360, 'rewardStep': 0.7857809636443704, 'errorList': [], 'lossList': [0.0, -1.3927965706586838, 0.0, 33.68500144958496, 0.0, 0.0, 0.0], 'rewardMean': 0.723190697743352, 'totalEpisodes': 277, 'stepsPerEpisode': 72, 'rewardPerEpisode': 55.7044824888706
'totalSteps': 16640, 'rewardStep': 0.8635776986992166, 'errorList': [], 'lossList': [0.0, -1.399428076148033, 0.0, 11.242851513624192, 0.0, 0.0, 0.0], 'rewardMean': 0.7213538113335602, 'totalEpisodes': 284, 'stepsPerEpisode': 71, 'rewardPerEpisode': 54.40704854568243
'totalSteps': 17920, 'rewardStep': 0.41004685123371043, 'errorList': [], 'lossList': [0.0, -1.403834364414215, 0.0, 9.139832566976548, 0.0, 0.0, 0.0], 'rewardMean': 0.6932621972752349, 'totalEpisodes': 290, 'stepsPerEpisode': 175, 'rewardPerEpisode': 140.52954245547156
'totalSteps': 19200, 'rewardStep': 0.9271530735920315, 'errorList': [], 'lossList': [0.0, -1.3869930696487427, 0.0, 28.35406813979149, 0.0, 0.0, 0.0], 'rewardMean': 0.6970909100379241, 'totalEpisodes': 297, 'stepsPerEpisode': 55, 'rewardPerEpisode': 47.10246304046568
'totalSteps': 20480, 'rewardStep': 0.7408202155685697, 'errorList': [], 'lossList': [0.0, -1.3757873910665512, 0.0, 9.544729863405228, 0.0, 0.0, 0.0], 'rewardMean': 0.6802014694826714, 'totalEpisodes': 300, 'stepsPerEpisode': 444, 'rewardPerEpisode': 381.45516420933114
'totalSteps': 21760, 'rewardStep': 0.8737707885443773, 'errorList': [], 'lossList': [0.0, -1.3698629516363143, 0.0, 4.490690258741378, 0.0, 0.0, 0.0], 'rewardMean': 0.7190415623358566, 'totalEpisodes': 303, 'stepsPerEpisode': 65, 'rewardPerEpisode': 58.44139596455727
'totalSteps': 23040, 'rewardStep': 0.8357008143675914, 'errorList': [], 'lossList': [0.0, -1.350968295931816, 0.0, 3.7812282621860502, 0.0, 0.0, 0.0], 'rewardMean': 0.7444252771605278, 'totalEpisodes': 306, 'stepsPerEpisode': 112, 'rewardPerEpisode': 96.16245471890758
'totalSteps': 24320, 'rewardStep': 0.931885168108334, 'errorList': [0.20450802731307993, 0.11591059764885743, 0.03571146366549843, 0.07596418252385762, 0.047904918607267456, 0.17166903194646377, 0.2804932595265097, 0.6216816685719646, 0.15080842228173952, 0.0691554648134385, 0.09733627281939297, 0.15385631467238944, 0.10117996275391103, 1.329780124485681, 0.18105965861495268, 0.08869674279065691, 0.052143750894216565, 0.47974749260019, 0.04050060487214437, 0.0333152682365643, 0.04816179313873618, 0.06364802469448663, 0.16439630521590412, 0.16036490165677264, 0.04120907602810366, 0.04787672970325997, 0.25087592545448223, 0.09337933592951271, 0.32030976344299167, 0.3694489059024177, 0.19459037642153043, 0.05028977205013268, 0.08394400354527008, 0.08011950857945208, 0.08865517650408655, 0.0498932653943014, 0.5171135504285007, 0.04337667159240294, 0.15888302655123487, 0.07721992472614081, 0.10425168147653308, 0.15209330117977368, 0.34320218352139575, 0.07032598087156747, 0.6592971153516646, 0.033532116560493735, 0.039669550876208236, 0.20150217123436903, 0.10515460500537502, 0.5951183151212147], 'lossList': [0.0, -1.3387542802095413, 0.0, 2.9143396922945977, 0.0, 0.0, 0.0], 'rewardMean': 0.7667307686670323, 'totalEpisodes': 309, 'stepsPerEpisode': 73, 'rewardPerEpisode': 66.51028703888811, 'successfulTests': 37
'totalSteps': 25600, 'rewardStep': 0.8945688386124715, 'errorList': [], 'lossList': [0.0, -1.333496434688568, 0.0, 2.4313201028108598, 0.0, 0.0, 0.0], 'rewardMean': 0.7979680697102646, 'totalEpisodes': 310, 'stepsPerEpisode': 377, 'rewardPerEpisode': 330.15513220185716
#maxSuccessfulTests=37, maxSuccessfulTestsAtStep=24320, timeSpent=77.51
