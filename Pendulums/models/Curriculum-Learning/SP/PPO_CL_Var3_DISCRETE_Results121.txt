#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 121
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8507820611170008, 'errorList': [], 'lossList': [0.0, -1.4478689223527907, 0.0, 39.36603865623474, 0.0, 0.0, 0.0], 'rewardMean': 0.72197994034858, 'totalEpisodes': 12, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.246092380443315
'totalSteps': 3840, 'rewardStep': 0.912045262930967, 'errorList': [], 'lossList': [0.0, -1.4647571444511414, 0.0, 34.22333289682865, 0.0, 0.0, 0.0], 'rewardMean': 0.7853350478760422, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.6693443357922
'totalSteps': 5120, 'rewardStep': 0.6920257340736807, 'errorList': [], 'lossList': [0.0, -1.4508162778615952, 0.0, 29.470210913419724, 0.0, 0.0, 0.0], 'rewardMean': 0.7620077194254519, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.1034262720764
'totalSteps': 6400, 'rewardStep': 0.9350593392633095, 'errorList': [], 'lossList': [0.0, -1.4478138899803161, 0.0, 24.043236311674118, 0.0, 0.0, 0.0], 'rewardMean': 0.7966180433930233, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.0425432279892
'totalSteps': 7680, 'rewardStep': 0.8365402088078523, 'errorList': [], 'lossList': [0.0, -1.4295602226257325, 0.0, 20.90125725865364, 0.0, 0.0, 0.0], 'rewardMean': 0.8032717376288282, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.5239888302845
'totalSteps': 8960, 'rewardStep': 0.9760006109501922, 'errorList': [], 'lossList': [0.0, -1.4123644584417343, 0.0, 12.227584658116102, 0.0, 0.0, 0.0], 'rewardMean': 0.8279472909604516, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1157.5153735749961
'totalSteps': 10240, 'rewardStep': 0.9756987211912614, 'errorList': [100.37192824137976, 97.75266342961953, 94.79053728092347, 97.1320274446655, 90.66382993740255, 99.55851424549645, 88.73536532599653, 99.21458665648763, 96.34127728730866, 100.3032422735836, 87.49395611013959, 101.48194201414367, 100.50572653490367, 93.39864966188426, 96.97493345888746, 101.3499000553014, 88.08574654725366, 95.78694055827553, 99.92843275548535, 99.21487509358289, 92.04120298262431, 91.95603691823331, 95.86376866478797, 98.66101894432606, 89.48653604905697, 99.73093861857699, 99.050052461133, 101.27494445601168, 100.49448594543895, 99.43324624502594, 89.79757582045777, 89.63373970631369, 90.7220410234618, 100.19604559283837, 89.98728989456647, 91.49195108370417, 101.109974889456, 99.59295938233225, 84.36470156753002, 102.90495101635763, 91.18727934177319, 99.21987745490146, 100.91853329841973, 99.81444739440256, 94.59603342198601, 94.7637086839345, 95.19212822314506, 96.37521467387444, 99.34606989272599, 100.17169280174143], 'lossList': [0.0, -1.3796330499649048, 0.0, 10.307312370613218, 0.0, 0.0, 0.0], 'rewardMean': 0.8464162197393029, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.1867579661866, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5570114886474917, 'errorList': [], 'lossList': [0.0, -1.3441955643892287, 0.0, 850.4904109191895, 0.0, 0.0, 0.0], 'rewardMean': 0.8142601385068794, 'totalEpisodes': 56, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.46744285179156
'totalSteps': 12800, 'rewardStep': 0.8217372894804793, 'errorList': [], 'lossList': [0.0, -1.3435464650392532, 0.0, 707.820309753418, 0.0, 0.0, 0.0], 'rewardMean': 0.8150078536042393, 'totalEpisodes': 99, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.901581232340524
'totalSteps': 14080, 'rewardStep': 0.7166644445566672, 'errorList': [], 'lossList': [0.0, -1.3435792684555055, 0.0, 438.56207901000977, 0.0, 0.0, 0.0], 'rewardMean': 0.8273565161018903, 'totalEpisodes': 139, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.30928811141584
'totalSteps': 15360, 'rewardStep': 0.5940640310827798, 'errorList': [], 'lossList': [0.0, -1.3436281949281692, 0.0, 76.7231281375885, 0.0, 0.0, 0.0], 'rewardMean': 0.8016847130984681, 'totalEpisodes': 172, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.244705919084322
'totalSteps': 16640, 'rewardStep': 0.6545673128748464, 'errorList': [], 'lossList': [0.0, -1.3407895612716674, 0.0, 49.86378215789795, 0.0, 0.0, 0.0], 'rewardMean': 0.775936918092856, 'totalEpisodes': 200, 'stepsPerEpisode': 61, 'rewardPerEpisode': 50.449464836883806
'totalSteps': 17920, 'rewardStep': 0.5592195884663315, 'errorList': [], 'lossList': [0.0, -1.332115775346756, 0.0, 17.77825716495514, 0.0, 0.0, 0.0], 'rewardMean': 0.7626563035321211, 'totalEpisodes': 218, 'stepsPerEpisode': 15, 'rewardPerEpisode': 7.427445392468034
'totalSteps': 19200, 'rewardStep': 0.7068032121722292, 'errorList': [], 'lossList': [0.0, -1.3245459550619125, 0.0, 21.110894441604614, 0.0, 0.0, 0.0], 'rewardMean': 0.739830690823013, 'totalEpisodes': 228, 'stepsPerEpisode': 74, 'rewardPerEpisode': 54.83736854383482
'totalSteps': 20480, 'rewardStep': 0.8381369763537077, 'errorList': [], 'lossList': [0.0, -1.3260072672367096, 0.0, 11.838900434970856, 0.0, 0.0, 0.0], 'rewardMean': 0.7399903675775986, 'totalEpisodes': 235, 'stepsPerEpisode': 27, 'rewardPerEpisode': 25.215199181713224
'totalSteps': 21760, 'rewardStep': 0.933304233520425, 'errorList': [66.88641848373416, 41.14573044404181, 96.59258841254503, 92.1548149122614, 59.25385568105656, 43.260775566083026, 2.7570855275961104, 75.62474932248878, 32.490218837502674, 130.29081276640966, 65.49546788661037, 34.455727531768744, 70.54257098794649, 3.721002366620171, 128.23926700829318, 126.08341851506871, 129.6587291166883, 108.96238933237018, 87.08319521304826, 130.5756790469015, 106.59929326111283, 99.16852776322094, 148.25215828233266, 84.62122391466127, 106.9496775805884, 4.659946144241623, 116.33661545543367, 128.99019083281001, 28.223615789722043, 50.85556303780569, 9.239757302604602, 77.32716889081313, 63.87301938844579, 115.86373535432587, 75.70294148299996, 34.683562259372984, 12.007573041394933, 32.08377918191758, 7.967481517821042, 79.43566967877686, 101.57260805646614, 116.04099568611585, 87.55379615097878, 118.59014339192494, 78.32528387474197, 58.60900628087669, 11.838965949411843, 34.817460407817286, 107.37918614376281, 1.3648530682449227], 'lossList': [0.0, -1.328763374686241, 0.0, 15.366258187294006, 0.0, 0.0, 0.0], 'rewardMean': 0.7357207298346219, 'totalEpisodes': 241, 'stepsPerEpisode': 87, 'rewardPerEpisode': 74.25295577421039, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.8692113100990461, 'errorList': [], 'lossList': [0.0, -1.3239245414733887, 0.0, 7.81402019739151, 0.0, 0.0, 0.0], 'rewardMean': 0.7250719887254004, 'totalEpisodes': 246, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.602791436774394
'totalSteps': 24320, 'rewardStep': 0.7954275916196991, 'errorList': [], 'lossList': [0.0, -1.3043310505151748, 0.0, 6.282676535844803, 0.0, 0.0, 0.0], 'rewardMean': 0.748913599022621, 'totalEpisodes': 251, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.793873157251486
'totalSteps': 25600, 'rewardStep': 0.9286609953662633, 'errorList': [], 'lossList': [0.0, -1.2877325803041457, 0.0, 5.4563616228103635, 0.0, 0.0, 0.0], 'rewardMean': 0.7596059696111995, 'totalEpisodes': 254, 'stepsPerEpisode': 62, 'rewardPerEpisode': 56.59823988871372
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=102.46
