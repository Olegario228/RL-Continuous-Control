#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 126
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980785764922136, 'errorList': [], 'lossList': [0.0, -1.417504243850708, 0.0, 30.41210502624512, 0.0, 0.0, 0.0], 'rewardMean': 0.7065643546668106, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.34356450654182
'totalSteps': 3840, 'rewardStep': 0.7337342130467079, 'errorList': [], 'lossList': [0.0, -1.4060174733400346, 0.0, 37.440305285453796, 0.0, 0.0, 0.0], 'rewardMean': 0.7156209741267764, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.247863075167686
'totalSteps': 5120, 'rewardStep': 0.8983266968431368, 'errorList': [], 'lossList': [0.0, -1.4023935985565186, 0.0, 28.85670949935913, 0.0, 0.0, 0.0], 'rewardMean': 0.7612974048058665, 'totalEpisodes': 76, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.645272888643724
'totalSteps': 6400, 'rewardStep': 0.5405434309987291, 'errorList': [], 'lossList': [0.0, -1.3908120518922806, 0.0, 28.452368409633635, 0.0, 0.0, 0.0], 'rewardMean': 0.717146610044439, 'totalEpisodes': 79, 'stepsPerEpisode': 83, 'rewardPerEpisode': 68.01456295733756
'totalSteps': 7680, 'rewardStep': 0.9298070272068737, 'errorList': [], 'lossList': [0.0, -1.3761466151475907, 0.0, 11.433969601392747, 0.0, 0.0, 0.0], 'rewardMean': 0.7525900129048448, 'totalEpisodes': 82, 'stepsPerEpisode': 54, 'rewardPerEpisode': 46.009717960296676
'totalSteps': 8960, 'rewardStep': 0.7478267082714463, 'errorList': [], 'lossList': [0.0, -1.3627811884880066, 0.0, 18.326602531671526, 0.0, 0.0, 0.0], 'rewardMean': 0.7519095408143593, 'totalEpisodes': 83, 'stepsPerEpisode': 444, 'rewardPerEpisode': 365.94615226628525
'totalSteps': 10240, 'rewardStep': 0.7137846179652984, 'errorList': [], 'lossList': [0.0, -1.3387080430984497, 0.0, 8.727822911739349, 0.0, 0.0, 0.0], 'rewardMean': 0.7471439254582267, 'totalEpisodes': 86, 'stepsPerEpisode': 94, 'rewardPerEpisode': 74.64607991480011
'totalSteps': 11520, 'rewardStep': 0.9264644491306497, 'errorList': [], 'lossList': [0.0, -1.3185293346643447, 0.0, 141.57803211212158, 0.0, 0.0, 0.0], 'rewardMean': 0.7670684280884958, 'totalEpisodes': 103, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.85515571474144
'totalSteps': 12800, 'rewardStep': 0.7989978387271619, 'errorList': [], 'lossList': [0.0, -1.3116118317842484, 0.0, 90.93090755462646, 0.0, 0.0, 0.0], 'rewardMean': 0.7702613691523625, 'totalEpisodes': 116, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.48713920299561
'totalSteps': 14080, 'rewardStep': 0.6896849168665486, 'errorList': [], 'lossList': [0.0, -1.2999354165792465, 0.0, 39.142842044830324, 0.0, 0.0, 0.0], 'rewardMean': 0.7577248475548768, 'totalEpisodes': 121, 'stepsPerEpisode': 225, 'rewardPerEpisode': 147.59261588803636
'totalSteps': 15360, 'rewardStep': 0.5834928063930479, 'errorList': [], 'lossList': [0.0, -1.276053808927536, 0.0, 6.023165839910507, 0.0, 0.0, 0.0], 'rewardMean': 0.7562662705449601, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 934.1805619842063
'totalSteps': 16640, 'rewardStep': 0.8000588110146666, 'errorList': [], 'lossList': [0.0, -1.2573500335216523, 0.0, 8.451060072183608, 0.0, 0.0, 0.0], 'rewardMean': 0.762898730341756, 'totalEpisodes': 124, 'stepsPerEpisode': 121, 'rewardPerEpisode': 103.24951315953622
'totalSteps': 17920, 'rewardStep': 0.6301459303417518, 'errorList': [], 'lossList': [0.0, -1.2405049788951874, 0.0, 3.8575895762443544, 0.0, 0.0, 0.0], 'rewardMean': 0.7360806536916175, 'totalEpisodes': 124, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.5990482964878
'totalSteps': 19200, 'rewardStep': 0.9688318006013239, 'errorList': [0.034321453519507245, 0.023813829508583767, 0.02328414781045431, 0.031408757642862196, 0.013396547183391104, 0.016606883794340603, 0.04815540745667488, 0.00916621412690312, 0.04254918378186898, 0.03859997701935549, 0.01687147483427246, 0.01769663883957898, 0.018916871479759133, 0.03141563322449373, 0.012244460687464884, 0.0641505039740747, 0.024449946626933745, 0.02717652746063032, 0.052718167933855614, 0.03193493566676586, 0.03164314170084315, 0.04959194488616223, 0.02942415136990557, 0.03217491470909709, 0.056962938218361334, 0.023938091437044055, 0.04233461196228136, 0.06307311222291813, 0.016754013473449774, 0.029299527481604037, 0.045982268050305405, 0.05528327373219978, 0.07606248931928973, 0.02320198718954412, 0.030890538202217828, 0.018757328774850507, 0.018464615609274766, 0.010468364309949317, 0.05116133725811776, 0.021133209811841712, 0.009229202458048646, 0.05730670657803857, 0.044365948390748235, 0.008430066025223252, 0.037055633627533995, 0.02159493660217389, 0.02040609447433792, 0.010908228893336303, 0.018647706094692143, 0.013458033849053561], 'lossList': [0.0, -1.2197372633218766, 0.0, 3.094582532942295, 0.0, 0.0, 0.0], 'rewardMean': 0.7789094906518769, 'totalEpisodes': 124, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.474189931613, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=67.36
