#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 128
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.43590750673834866, 'errorList': [], 'lossList': [0.0, -1.4341206073760986, 0.0, 31.667020444869994, 0.0, 0.0, 0.0], 'rewardMean': 0.47042336673936674, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.31342545895599
'totalSteps': 3840, 'rewardStep': 0.9553872686153767, 'errorList': [], 'lossList': [0.0, -1.4267620873451232, 0.0, 33.83611287593842, 0.0, 0.0, 0.0], 'rewardMean': 0.6320780006980368, 'totalEpisodes': 72, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.22112575509128
'totalSteps': 5120, 'rewardStep': 0.8388254462094117, 'errorList': [], 'lossList': [0.0, -1.422535389661789, 0.0, 42.977623014450074, 0.0, 0.0, 0.0], 'rewardMean': 0.6837648620758805, 'totalEpisodes': 78, 'stepsPerEpisode': 72, 'rewardPerEpisode': 64.54850374529123
'totalSteps': 6400, 'rewardStep': 0.48326593053442374, 'errorList': [], 'lossList': [0.0, -1.4289336198568343, 0.0, 39.22549348831177, 0.0, 0.0, 0.0], 'rewardMean': 0.6436650757675891, 'totalEpisodes': 83, 'stepsPerEpisode': 143, 'rewardPerEpisode': 101.4031990359977
'totalSteps': 7680, 'rewardStep': 0.9520836709226697, 'errorList': [], 'lossList': [0.0, -1.4247060024738312, 0.0, 45.84367401599884, 0.0, 0.0, 0.0], 'rewardMean': 0.6950681749601025, 'totalEpisodes': 88, 'stepsPerEpisode': 53, 'rewardPerEpisode': 48.64993932157514
'totalSteps': 8960, 'rewardStep': 0.8106282727462086, 'errorList': [], 'lossList': [0.0, -1.4190297430753709, 0.0, 37.19685288906098, 0.0, 0.0, 0.0], 'rewardMean': 0.7115767603581177, 'totalEpisodes': 90, 'stepsPerEpisode': 372, 'rewardPerEpisode': 252.54289202473865
'totalSteps': 10240, 'rewardStep': 0.7321822000360432, 'errorList': [], 'lossList': [0.0, -1.4164469683170318, 0.0, 29.26871017932892, 0.0, 0.0, 0.0], 'rewardMean': 0.7141524403178584, 'totalEpisodes': 93, 'stepsPerEpisode': 138, 'rewardPerEpisode': 116.13937161934875
'totalSteps': 11520, 'rewardStep': 0.6362726294807742, 'errorList': [], 'lossList': [0.0, -1.3993281257152557, 0.0, 190.2193229675293, 0.0, 0.0, 0.0], 'rewardMean': 0.7054991280026268, 'totalEpisodes': 117, 'stepsPerEpisode': 62, 'rewardPerEpisode': 42.16959075965107
'totalSteps': 12800, 'rewardStep': 0.8557559324514998, 'errorList': [], 'lossList': [0.0, -1.3909322094917298, 0.0, 92.45891952514648, 0.0, 0.0, 0.0], 'rewardMean': 0.7205248084475141, 'totalEpisodes': 132, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.453514303530699
'totalSteps': 14080, 'rewardStep': 0.8140079671409028, 'errorList': [], 'lossList': [0.0, -1.3694837599992753, 0.0, 12.68697924375534, 0.0, 0.0, 0.0], 'rewardMean': 0.7514316824875659, 'totalEpisodes': 137, 'stepsPerEpisode': 245, 'rewardPerEpisode': 168.7417867036242
'totalSteps': 15360, 'rewardStep': 0.58286169084575, 'errorList': [], 'lossList': [0.0, -1.3420069140195847, 0.0, 27.93196924686432, 0.0, 0.0, 0.0], 'rewardMean': 0.766127100898306, 'totalEpisodes': 146, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.117899560532016
'totalSteps': 16640, 'rewardStep': 0.8654358086276187, 'errorList': [], 'lossList': [0.0, -1.330606033205986, 0.0, 6.384399052262307, 0.0, 0.0, 0.0], 'rewardMean': 0.7571319548995302, 'totalEpisodes': 149, 'stepsPerEpisode': 94, 'rewardPerEpisode': 77.0157146931833
'totalSteps': 17920, 'rewardStep': 0.4345618700739725, 'errorList': [], 'lossList': [0.0, -1.314589980840683, 0.0, 8.39562096953392, 0.0, 0.0, 0.0], 'rewardMean': 0.7167055972859864, 'totalEpisodes': 152, 'stepsPerEpisode': 283, 'rewardPerEpisode': 207.67113868748305
'totalSteps': 19200, 'rewardStep': 0.5111871076439207, 'errorList': [], 'lossList': [0.0, -1.2986272710561753, 0.0, 6.658593665361405, 0.0, 0.0, 0.0], 'rewardMean': 0.719497714996936, 'totalEpisodes': 156, 'stepsPerEpisode': 260, 'rewardPerEpisode': 196.34969671780726
'totalSteps': 20480, 'rewardStep': 0.9371153673134599, 'errorList': [0.13731389003626482, 0.02665621294462449, 0.045372441472281824, 0.07961921897623349, 0.04737568234073216, 0.09568495513117643, 0.2345044032058045, 0.13009129207529413, 0.12461844522937272, 0.030960957220735955, 0.032831392538181875, 0.06237414979071835, 0.0732230047848936, 0.07118005032203718, 0.11118466616305614, 0.11501554680849674, 0.19249706688700102, 0.03966532612414067, 0.11775099368766596, 0.06616724087681165, 0.12058944439369955, 0.1433074035985479, 0.18298935349197715, 0.156415615804226, 0.18877875488754992, 0.15641767994370837, 0.05507144871771064, 0.06443741121581827, 0.044830001079409716, 0.029513331879548035, 0.19013991514575737, 0.03377445699020012, 0.029011801553828626, 0.08921394239124393, 0.08806103116175197, 0.11572785179156382, 0.16178920593529203, 0.18341779650151038, 0.08228211123156084, 0.1250028097243674, 0.04712151037292538, 0.06241263305132373, 0.030656533531500538, 0.16272566380784878, 0.13282270581566602, 0.17534376462283308, 0.20711073835871704, 0.10185243281694323, 0.1933685821994007, 0.14091854477582996], 'lossList': [0.0, -1.2850125020742416, 0.0, 5.969546597003937, 0.0, 0.0, 0.0], 'rewardMean': 0.7180008846360151, 'totalEpisodes': 159, 'stepsPerEpisode': 158, 'rewardPerEpisode': 131.98525593481264, 'successfulTests': 48
'totalSteps': 21760, 'rewardStep': 0.951953369274203, 'errorList': [0.22915042094294424, 0.1714872422417054, 0.20078127840522253, 0.21436585862458576, 0.1790747438343966, 0.18324191102278745, 0.1955210895203646, 0.17307510624748879, 0.2214956718982107, 0.19835534090029383, 0.19915110904858696, 0.2048720040344491, 0.19298080000579756, 0.2305658722646584, 0.2103151167527346, 0.18754353142121993, 0.21047479816927425, 0.205124118430204, 0.17799820813454897, 0.1690304110321435, 0.22749904353781633, 0.18261357622572072, 0.255744067732045, 0.2281787917943196, 0.20082052855707963, 0.21409395772450066, 0.24283139478198207, 0.20237997984576078, 0.18841014787975238, 0.2262397443281224, 0.20593452686760166, 0.21970606619301922, 0.22155569857534907, 0.1778636681715216, 0.19048053868595843, 0.20922025765483918, 0.21343826539873312, 0.20937174071807454, 0.23135733378870374, 0.20015377941882984, 0.1796969956899595, 0.2236358987633599, 0.20952621266388943, 0.20339601052656525, 0.1844170227406926, 0.22031104016051745, 0.17976289374429294, 0.18851015156347942, 0.1933603758347802, 0.24228441754713512], 'lossList': [0.0, -1.2642451113462447, 0.0, 4.047842948436737, 0.0, 0.0, 0.0], 'rewardMean': 0.7321333942888144, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1001.1442575775349, 'successfulTests': 20
'totalSteps': 23040, 'rewardStep': 0.6367223340718394, 'errorList': [], 'lossList': [0.0, -1.235913534760475, 0.0, 1.8810440480709076, 0.0, 0.0, 0.0], 'rewardMean': 0.7225874076923942, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.836739966852
'totalSteps': 24320, 'rewardStep': 0.6323539967385565, 'errorList': [], 'lossList': [0.0, -1.222021700143814, 0.0, 2.5128807172179224, 0.0, 0.0, 0.0], 'rewardMean': 0.7221955444181724, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.480164026948
'totalSteps': 25600, 'rewardStep': 0.9862341841246793, 'errorList': [0.10301434382026857, 0.10854433986844284, 0.1125612604751618, 0.10091920214567998, 0.07407988787965938, 0.04801036383673489, 0.046130537651993146, 0.04588235390312292, 0.049100819861858006, 0.047610667731522824, 0.07828991021374662, 0.051899436245020115, 0.07366976301667812, 0.05436341417836348, 0.04578609542715451, 0.10149096668386645, 0.05766087906410073, 0.08155621066946704, 0.09736376873988421, 0.065785815184667, 0.05736312529637505, 0.05510648871350491, 0.049922552634009014, 0.04842275662266627, 0.12831436593155865, 0.0892961053381009, 0.06599710810956808, 0.06443636296978285, 0.04501631210892494, 0.052046049189889025, 0.0540343252419995, 0.05727089729563236, 0.05944553056969941, 0.07152090107713674, 0.04984098464393607, 0.10093214596468174, 0.07613065378115087, 0.053702132556214754, 0.09358601045735473, 0.08505480092114384, 0.0467910206853403, 0.0512904364595107, 0.047920991158390096, 0.06125315631866789, 0.053423541807324115, 0.05354874148457218, 0.08461002122400961, 0.048130594138869304, 0.052496441250127285, 0.06075386673759714], 'lossList': [0.0, -1.2036929124593734, 0.0, 1.3060280181467534, 0.0, 0.0, 0.0], 'rewardMean': 0.7352433695854904, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.4074645117935, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=125.86
