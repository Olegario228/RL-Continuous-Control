#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 13
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.657187940170172, 'errorList': [], 'lossList': [0.0, -1.4219418781995774, 0.0, 27.9683056306839, 0.0, 0.0, 0.0], 'rewardMean': 0.711256886558614, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.155583018468704
'totalSteps': 3840, 'rewardStep': 0.9612180606389581, 'errorList': [], 'lossList': [0.0, -1.4156839829683303, 0.0, 27.140998220443727, 0.0, 0.0, 0.0], 'rewardMean': 0.7945772779187287, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 374.2310408960704
'totalSteps': 5120, 'rewardStep': 0.8097549536927952, 'errorList': [], 'lossList': [0.0, -1.4097404754161835, 0.0, 27.785908808112143, 0.0, 0.0, 0.0], 'rewardMean': 0.7983716968622453, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.4483637329215
'totalSteps': 6400, 'rewardStep': 0.5581019723150383, 'errorList': [], 'lossList': [0.0, -1.3944461607933045, 0.0, 304.5220436859131, 0.0, 0.0, 0.0], 'rewardMean': 0.7503177519528039, 'totalEpisodes': 79, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.068576459706925
'totalSteps': 7680, 'rewardStep': 0.7419428632071731, 'errorList': [], 'lossList': [0.0, -1.3962004816532134, 0.0, 137.13483505249025, 0.0, 0.0, 0.0], 'rewardMean': 0.7489219371618655, 'totalEpisodes': 133, 'stepsPerEpisode': 52, 'rewardPerEpisode': 43.226849925884906
'totalSteps': 8960, 'rewardStep': 0.596193930662226, 'errorList': [], 'lossList': [0.0, -1.4052799421548843, 0.0, 64.71153390884399, 0.0, 0.0, 0.0], 'rewardMean': 0.7271036505190598, 'totalEpisodes': 177, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.473352492858645
'totalSteps': 10240, 'rewardStep': 0.4688014911961843, 'errorList': [], 'lossList': [0.0, -1.4089439314603807, 0.0, 39.65388949394226, 0.0, 0.0, 0.0], 'rewardMean': 0.6948158806037004, 'totalEpisodes': 197, 'stepsPerEpisode': 67, 'rewardPerEpisode': 51.96217241652651
'totalSteps': 11520, 'rewardStep': 0.875794325667839, 'errorList': [], 'lossList': [0.0, -1.4047758370637893, 0.0, 26.906731996536255, 0.0, 0.0, 0.0], 'rewardMean': 0.714924596721938, 'totalEpisodes': 211, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.065215936456346
'totalSteps': 12800, 'rewardStep': 0.6815933220749597, 'errorList': [], 'lossList': [0.0, -1.4041867345571517, 0.0, 18.140804872512817, 0.0, 0.0, 0.0], 'rewardMean': 0.7115914692572403, 'totalEpisodes': 219, 'stepsPerEpisode': 185, 'rewardPerEpisode': 147.38107549408826
'totalSteps': 14080, 'rewardStep': 0.7691321295870535, 'errorList': [], 'lossList': [0.0, -1.398018355369568, 0.0, 17.83397306203842, 0.0, 0.0, 0.0], 'rewardMean': 0.71197209892124, 'totalEpisodes': 226, 'stepsPerEpisode': 34, 'rewardPerEpisode': 27.526205328373177
'totalSteps': 15360, 'rewardStep': 0.6665668979009528, 'errorList': [], 'lossList': [0.0, -1.382713942527771, 0.0, 30.786308829784392, 0.0, 0.0, 0.0], 'rewardMean': 0.712909994694318, 'totalEpisodes': 230, 'stepsPerEpisode': 177, 'rewardPerEpisode': 145.24653653995313
'totalSteps': 16640, 'rewardStep': 0.7676922252283755, 'errorList': [], 'lossList': [0.0, -1.376292667388916, 0.0, 7.127875075936317, 0.0, 0.0, 0.0], 'rewardMean': 0.6935574111532599, 'totalEpisodes': 233, 'stepsPerEpisode': 162, 'rewardPerEpisode': 134.73086259545002
'totalSteps': 17920, 'rewardStep': 0.8556460608172981, 'errorList': [], 'lossList': [0.0, -1.3823629271984101, 0.0, 5.404810354113579, 0.0, 0.0, 0.0], 'rewardMean': 0.69814652186571, 'totalEpisodes': 236, 'stepsPerEpisode': 227, 'rewardPerEpisode': 198.48770178165856
'totalSteps': 19200, 'rewardStep': 0.8453980825520019, 'errorList': [], 'lossList': [0.0, -1.3927452450990676, 0.0, 5.597472327947616, 0.0, 0.0, 0.0], 'rewardMean': 0.7268761328894064, 'totalEpisodes': 238, 'stepsPerEpisode': 724, 'rewardPerEpisode': 604.193193905202
'totalSteps': 20480, 'rewardStep': 0.6424466336621633, 'errorList': [], 'lossList': [0.0, -1.3888059288263321, 0.0, 5.9057195079326625, 0.0, 0.0, 0.0], 'rewardMean': 0.7169265099349055, 'totalEpisodes': 239, 'stepsPerEpisode': 872, 'rewardPerEpisode': 678.4715407120361
'totalSteps': 21760, 'rewardStep': 0.7494250515364229, 'errorList': [], 'lossList': [0.0, -1.3531368219852447, 0.0, 3.2440299877524374, 0.0, 0.0, 0.0], 'rewardMean': 0.732249622022325, 'totalEpisodes': 240, 'stepsPerEpisode': 858, 'rewardPerEpisode': 709.0568869186089
'totalSteps': 23040, 'rewardStep': 0.8373977123764711, 'errorList': [], 'lossList': [0.0, -1.295189407467842, 0.0, 2.4279716581106188, 0.0, 0.0, 0.0], 'rewardMean': 0.7691092441403538, 'totalEpisodes': 240, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.5586827590091
'totalSteps': 24320, 'rewardStep': 0.8060076428337877, 'errorList': [], 'lossList': [0.0, -1.2330834436416627, 0.0, 1.5185768677666784, 0.0, 0.0, 0.0], 'rewardMean': 0.7621305758569485, 'totalEpisodes': 240, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.582190622928
'totalSteps': 25600, 'rewardStep': 0.9612330285244587, 'errorList': [0.015811590237408624, 0.007816392314439896, 0.012275907620722348, 0.003927024392652673, 0.01156820409883375, 0.010258699401065196, 0.020810709045809395, 0.007707824894889009, 0.008829502315620933, 0.004629497388221406, 0.014483652575775663, 0.007153496723452449, 0.006913329342695181, 0.020748833514265444, 0.02559953062197087, 0.001518034708425236, 0.012697217787964725, 0.009024355746285075, 0.017109202118758556, 0.014425526612312209, 0.003616397177888004, 0.022630177376899905, 0.005502325634846987, 0.009522538309781664, 0.018497668679051303, 0.011878857614411877, 0.012489183322092662, 0.021663764067345236, 0.012167412608376539, 0.02234582664889907, 0.022801187024622513, 0.007532098254439173, 0.011633508988163588, 0.008886853161793964, 0.011787294286788453, 0.013477892203264518, 0.016529716221731543, 0.007858485136209905, 0.011288979146972475, 0.02466375942270006, 0.003615144196437669, 0.0057209867388852585, 0.016530207154118803, 0.014973759111349724, 0.0036452275829037934, 0.0045923925348004, 0.019224096421122695, 0.01926439181499506, 0.02016801892743629, 0.011986176818390879], 'lossList': [0.0, -1.2038391196727753, 0.0, 1.2523722602054477, 0.0, 0.0, 0.0], 'rewardMean': 0.7900945465018985, 'totalEpisodes': 240, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.9709747518077, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=76.04
