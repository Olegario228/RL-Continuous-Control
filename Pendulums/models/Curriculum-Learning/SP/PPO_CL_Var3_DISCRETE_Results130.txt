#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 130
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8755310625235291, 'errorList': [], 'lossList': [0.0, -1.3985852706432342, 0.0, 28.8172762799263, 0.0, 0.0, 0.0], 'rewardMean': 0.7431005807326898, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.951881461253773
'totalSteps': 3840, 'rewardStep': 0.7240434810803753, 'errorList': [], 'lossList': [0.0, -1.3883588933944702, 0.0, 30.6139417219162, 0.0, 0.0, 0.0], 'rewardMean': 0.7367482141819183, 'totalEpisodes': 26, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.41354584075523
'totalSteps': 5120, 'rewardStep': 0.6585082841710177, 'errorList': [], 'lossList': [0.0, -1.399420103430748, 0.0, 16.227856035232545, 0.0, 0.0, 0.0], 'rewardMean': 0.7171882316791931, 'totalEpisodes': 27, 'stepsPerEpisode': 123, 'rewardPerEpisode': 101.09556843198314
'totalSteps': 6400, 'rewardStep': 0.9488612710731995, 'errorList': [], 'lossList': [0.0, -1.410182209610939, 0.0, 16.873179924488067, 0.0, 0.0, 0.0], 'rewardMean': 0.7635228395579944, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 958.9682218704281
'totalSteps': 7680, 'rewardStep': 0.7028640015072771, 'errorList': [], 'lossList': [0.0, -1.3963335663080216, 0.0, 13.15499938905239, 0.0, 0.0, 0.0], 'rewardMean': 0.7534130332162081, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.9997383320181
'totalSteps': 8960, 'rewardStep': 0.7432763724430519, 'errorList': [], 'lossList': [0.0, -1.3650491100549698, 0.0, 8.251209321022033, 0.0, 0.0, 0.0], 'rewardMean': 0.751964938820043, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 962.8113727972288
'totalSteps': 10240, 'rewardStep': 0.7235762642987223, 'errorList': [], 'lossList': [0.0, -1.3678180432319642, 0.0, 8.652332128286362, 0.0, 0.0, 0.0], 'rewardMean': 0.7484163545048779, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1039.6160281667378
'totalSteps': 11520, 'rewardStep': 0.7859511091416975, 'errorList': [], 'lossList': [0.0, -1.3457131087779999, 0.0, 549.7726599121094, 0.0, 0.0, 0.0], 'rewardMean': 0.7525868827978578, 'totalEpisodes': 62, 'stepsPerEpisode': 55, 'rewardPerEpisode': 50.251603401848136
'totalSteps': 12800, 'rewardStep': 0.6075105010330107, 'errorList': [], 'lossList': [0.0, -1.3450306630134583, 0.0, 370.52246643066405, 0.0, 0.0, 0.0], 'rewardMean': 0.7380792446213731, 'totalEpisodes': 94, 'stepsPerEpisode': 60, 'rewardPerEpisode': 47.729568293410615
'totalSteps': 14080, 'rewardStep': 0.5702332790786593, 'errorList': [], 'lossList': [0.0, -1.3437906754016877, 0.0, 234.26899284362793, 0.0, 0.0, 0.0], 'rewardMean': 0.7340355626350542, 'totalEpisodes': 124, 'stepsPerEpisode': 81, 'rewardPerEpisode': 61.12561569082744
'totalSteps': 15360, 'rewardStep': 0.9512227629929776, 'errorList': [266.65806995528027, 292.94151183034177, 266.5213117881305, 289.09289657137646, 306.4746865861213, 306.0940639544458, 301.6851468036784, 293.0850373423263, 313.7022939286548, 260.53359257336575, 281.37290727171813, 325.25220814738765, 320.56747096392473, 277.52167137085377, 336.6240594485212, 228.44806235308636, 304.50507754247866, 318.19005690288105, 318.93759357009526, 343.9730572179962, 332.89960099605594, 352.87313078643376, 338.212827532032, 301.3584677518202, 322.08049501630126, 335.1110291775638, 308.09479427215564, 321.19636678433955, 341.250972549561, 244.4365844943235, 343.7455099187612, 305.48823266232614, 304.20520080739215, 155.86122100231034, 280.2379774044467, 343.6193604194682, 299.370744525266, 248.41083567659578, 256.27436930598645, 327.2086163052302, 333.1868815950175, 306.3111550079099, 319.99878780844523, 259.7023296605774, 311.5828918870126, 335.6479917618536, 301.53673476092615, 256.3716112846518, 332.6835757529201, 314.65962784674014], 'lossList': [0.0, -1.3370504188537597, 0.0, 118.13072626113892, 0.0, 0.0, 0.0], 'rewardMean': 0.741604732681999, 'totalEpisodes': 152, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9512227629929776, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.790822144819341, 'errorList': [], 'lossList': [0.0, -1.3313719463348388, 0.0, 91.05858070373534, 0.0, 0.0, 0.0], 'rewardMean': 0.7482825990558954, 'totalEpisodes': 169, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.580887335736566
'totalSteps': 17920, 'rewardStep': 0.7511810811363914, 'errorList': [], 'lossList': [0.0, -1.3291094940900803, 0.0, 55.96378364562988, 0.0, 0.0, 0.0], 'rewardMean': 0.7575498787524328, 'totalEpisodes': 185, 'stepsPerEpisode': 32, 'rewardPerEpisode': 25.049118196687935
'totalSteps': 19200, 'rewardStep': 0.7651319258884812, 'errorList': [], 'lossList': [0.0, -1.3232179242372513, 0.0, 54.432731008529665, 0.0, 0.0, 0.0], 'rewardMean': 0.7391769442339611, 'totalEpisodes': 201, 'stepsPerEpisode': 77, 'rewardPerEpisode': 64.0947032220901
'totalSteps': 20480, 'rewardStep': 0.9324507160041055, 'errorList': [32.824904737622646, 71.27499025011058, 62.36694585125765, 1.9832054614609713, 42.3831060865624, 46.347546128231215, 32.513751850612884, 12.805109225322084, 46.532400697582, 11.779693037772999, 48.75869937902888, 40.689560291242685, 54.4375833121045, 31.386270324762997, 55.78763782471158, 14.62141492220824, 48.20959891910469, 46.152245405793394, 12.545607068822836, 1.0270130097481664, 22.838092191806442, 25.822019450917423, 25.398641149101664, 18.338828827401937, 39.679865067594974, 42.25631274230166, 34.36306374037557, 27.240331405204454, 36.80994180293796, 32.45769836006254, 65.41803208536031, 16.875714539997276, 26.95940236487896, 17.504203001202136, 26.47572287047463, 42.814788142461204, 5.8106895541063945, 22.58164511155227, 5.137565255058006, 3.376513431525221, 4.379753774058211, 13.028526468350025, 47.671579873086124, 58.240683691112004, 26.623513870216435, 41.731714561048484, 28.237955381047833, 32.549883796583956, 22.22406783500308, 35.06530336176498], 'lossList': [0.0, -1.3193189126253129, 0.0, 27.358144521713257, 0.0, 0.0, 0.0], 'rewardMean': 0.7621356156836437, 'totalEpisodes': 210, 'stepsPerEpisode': 86, 'rewardPerEpisode': 74.18609461602428, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.9261769488364573, 'errorList': [], 'lossList': [0.0, -1.329489793777466, 0.0, 56.39583992958069, 0.0, 0.0, 0.0], 'rewardMean': 0.7804256733229843, 'totalEpisodes': 217, 'stepsPerEpisode': 61, 'rewardPerEpisode': 56.316323603547005
'totalSteps': 23040, 'rewardStep': 0.784430705517537, 'errorList': [], 'lossList': [0.0, -1.328984917998314, 0.0, 49.60478758811951, 0.0, 0.0, 0.0], 'rewardMean': 0.7865111174448659, 'totalEpisodes': 222, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.762450552536388
'totalSteps': 24320, 'rewardStep': 0.9626319526901521, 'errorList': [21.430290569650744, 25.42919061191017, 25.053864263962023, 6.9104144986593985, 2.9643788023936355, 9.405055699698723, 11.181112765581688, 5.464780036434082, 27.97774424972196, 11.762971042487598, 12.680348384333898, 1.0583481204876988, 6.010898572671965, 6.689959449691567, 22.89076377761503, 26.691478562752, 3.2831298232265778, 2.1809357952122936, 31.6245653105881, 11.530980298925417, 10.729868817832841, 16.102098255948086, 21.012901233680164, 26.22279550843364, 22.13329889441302, 22.8233977693859, 5.214005824170441, 25.305632522413898, 20.93113123524938, 19.186269036450796, 2.7262099372004354, 2.2130093983200503, 16.80526552008467, 9.61185755784069, 12.513625925960191, 4.85953017658536, 23.71903706648031, 8.909293525576532, 18.7253578688584, 28.889380772969037, 4.259685239610555, 19.660229001949304, 24.950648512170286, 19.067691815017902, 0.5535366045175582, 6.39122243744876, 7.484312005777131, 15.183453045906926, 1.9556246377962692, 20.852691202148446], 'lossList': [0.0, -1.3240505945682526, 0.0, 18.982003390789032, 0.0, 0.0, 0.0], 'rewardMean': 0.8041792017997113, 'totalEpisodes': 227, 'stepsPerEpisode': 180, 'rewardPerEpisode': 147.90918018980344, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.6773214812586946, 'errorList': [], 'lossList': [0.0, -1.3158338952064514, 0.0, 10.081185164451599, 0.0, 0.0, 0.0], 'rewardMean': 0.8111602998222797, 'totalEpisodes': 232, 'stepsPerEpisode': 299, 'rewardPerEpisode': 256.6251017557849
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=124.56
