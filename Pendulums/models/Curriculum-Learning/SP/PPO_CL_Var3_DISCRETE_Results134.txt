#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 134
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7930420769534675, 'errorList': [], 'lossList': [0.0, -1.4114374756813048, 0.0, 23.55243757247925, 0.0, 0.0, 0.0], 'rewardMean': 0.578556091364528, 'totalEpisodes': 20, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.512210118521697
'totalSteps': 3840, 'rewardStep': 0.46685752540477443, 'errorList': [], 'lossList': [0.0, -1.422312022447586, 0.0, 22.715906591415404, 0.0, 0.0, 0.0], 'rewardMean': 0.5413232360446102, 'totalEpisodes': 27, 'stepsPerEpisode': 113, 'rewardPerEpisode': 72.55741603431655
'totalSteps': 5120, 'rewardStep': 0.623695989080709, 'errorList': [], 'lossList': [0.0, -1.432346613407135, 0.0, 19.172786490917204, 0.0, 0.0, 0.0], 'rewardMean': 0.5619164243036349, 'totalEpisodes': 31, 'stepsPerEpisode': 208, 'rewardPerEpisode': 168.83040591018786
'totalSteps': 6400, 'rewardStep': 0.6727221060452948, 'errorList': [], 'lossList': [0.0, -1.4396896713972092, 0.0, 29.235621185302733, 0.0, 0.0, 0.0], 'rewardMean': 0.5840775606519669, 'totalEpisodes': 33, 'stepsPerEpisode': 866, 'rewardPerEpisode': 658.5288995931718
'totalSteps': 7680, 'rewardStep': 0.7977640525822484, 'errorList': [], 'lossList': [0.0, -1.4289227557182311, 0.0, 12.979836930632592, 0.0, 0.0, 0.0], 'rewardMean': 0.6196919759736804, 'totalEpisodes': 33, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 990.3523733334271
'totalSteps': 8960, 'rewardStep': 0.7956761107397264, 'errorList': [], 'lossList': [0.0, -1.4045206701755524, 0.0, 23.614951120018958, 0.0, 0.0, 0.0], 'rewardMean': 0.6448325666545441, 'totalEpisodes': 34, 'stepsPerEpisode': 1124, 'rewardPerEpisode': 933.9348670071951
'totalSteps': 10240, 'rewardStep': 0.9749315892470607, 'errorList': [182.02460779344463, 189.51751880969982, 171.27962235033573, 205.0361835091918, 178.67339723328743, 199.60191635510446, 174.10945914322775, 186.04265723469277, 189.96622118435232, 176.9062931107093, 194.36858136708412, 187.62726419436055, 188.30411102976328, 196.20645718766082, 184.64378668142527, 196.21051431757525, 200.26866420059525, 187.7552906295542, 195.45244567500424, 178.52824022376754, 181.50608552469444, 191.0033336445119, 186.1222529022524, 187.53659767393555, 185.49949159246313, 187.15018947209416, 192.62834401190887, 160.79616662647092, 164.01957473447797, 194.5657941549343, 181.34307551935728, 194.3623897735179, 195.1619763190379, 190.51779816295365, 161.75044283514924, 147.31002919835748, 163.89965821268828, 136.0181172176249, 192.6046649413982, 198.9633828862198, 169.28689032582355, 132.4326035622767, 196.5964596564374, 181.12957815236598, 204.22390598960197, 196.4139656750103, 196.54663927818743, 188.9188985069369, 169.39352276440573, 196.45706439134977], 'lossList': [0.0, -1.3877867829799653, 0.0, 8.767804788947105, 0.0, 0.0, 0.0], 'rewardMean': 0.6860949444786087, 'totalEpisodes': 34, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1095.6513249129976, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.748094706354173, 'errorList': [], 'lossList': [0.0, -1.3693012249469758, 0.0, 314.572094039917, 0.0, 0.0, 0.0], 'rewardMean': 0.6929838069092269, 'totalEpisodes': 60, 'stepsPerEpisode': 38, 'rewardPerEpisode': 33.03414436507375
'totalSteps': 12800, 'rewardStep': 0.9055443537072895, 'errorList': [], 'lossList': [0.0, -1.3683661448955535, 0.0, 111.91903394699096, 0.0, 0.0, 0.0], 'rewardMean': 0.7142398615890332, 'totalEpisodes': 88, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.08679871314209
'totalSteps': 14080, 'rewardStep': 0.6796277329720425, 'errorList': [], 'lossList': [0.0, -1.3680399745702743, 0.0, 39.17335327148437, 0.0, 0.0, 0.0], 'rewardMean': 0.7457956243086785, 'totalEpisodes': 118, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.61532765525305
'totalSteps': 15360, 'rewardStep': 0.7723727572947491, 'errorList': [], 'lossList': [0.0, -1.3671167302131653, 0.0, 22.211938943862915, 0.0, 0.0, 0.0], 'rewardMean': 0.7437286923428068, 'totalEpisodes': 140, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.269867243987054
'totalSteps': 16640, 'rewardStep': 0.8121593254889687, 'errorList': [], 'lossList': [0.0, -1.35667727291584, 0.0, 20.299783248901367, 0.0, 0.0, 0.0], 'rewardMean': 0.7782588723512263, 'totalEpisodes': 153, 'stepsPerEpisode': 79, 'rewardPerEpisode': 62.22934137747767
'totalSteps': 17920, 'rewardStep': 0.8498830270166978, 'errorList': [], 'lossList': [0.0, -1.3482444542646408, 0.0, 19.46004296541214, 0.0, 0.0, 0.0], 'rewardMean': 0.8008775761448252, 'totalEpisodes': 169, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.35356347366288
'totalSteps': 19200, 'rewardStep': 0.9245351698356303, 'errorList': [], 'lossList': [0.0, -1.3484253084659577, 0.0, 16.54185911655426, 0.0, 0.0, 0.0], 'rewardMean': 0.8260588825238585, 'totalEpisodes': 177, 'stepsPerEpisode': 425, 'rewardPerEpisode': 376.06385570224325
'totalSteps': 20480, 'rewardStep': 0.8346568201575826, 'errorList': [], 'lossList': [0.0, -1.3377800339460373, 0.0, 11.368681433200836, 0.0, 0.0, 0.0], 'rewardMean': 0.829748159281392, 'totalEpisodes': 185, 'stepsPerEpisode': 45, 'rewardPerEpisode': 34.64786051536094
'totalSteps': 21760, 'rewardStep': 0.36823650123317714, 'errorList': [], 'lossList': [0.0, -1.3217665147781372, 0.0, 7.174323012828827, 0.0, 0.0, 0.0], 'rewardMean': 0.7870041983307371, 'totalEpisodes': 192, 'stepsPerEpisode': 146, 'rewardPerEpisode': 102.60601201773827
'totalSteps': 23040, 'rewardStep': 0.3445920703277014, 'errorList': [], 'lossList': [0.0, -1.3097251617908479, 0.0, 7.835470827817917, 0.0, 0.0, 0.0], 'rewardMean': 0.7239702464388011, 'totalEpisodes': 198, 'stepsPerEpisode': 176, 'rewardPerEpisode': 124.64106997625804
'totalSteps': 24320, 'rewardStep': 0.6865639966985847, 'errorList': [], 'lossList': [0.0, -1.3074432444572448, 0.0, 5.450483008623123, 0.0, 0.0, 0.0], 'rewardMean': 0.7178171754732424, 'totalEpisodes': 203, 'stepsPerEpisode': 275, 'rewardPerEpisode': 227.58479900418018
'totalSteps': 25600, 'rewardStep': 0.9294035117756472, 'errorList': [], 'lossList': [0.0, -1.3042727780342103, 0.0, 3.8073735779523847, 0.0, 0.0, 0.0], 'rewardMean': 0.7202030912800781, 'totalEpisodes': 208, 'stepsPerEpisode': 57, 'rewardPerEpisode': 50.89894533433349
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.19
