#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 135
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9248751944253679, 'errorList': [], 'lossList': [0.0, -1.407356454730034, 0.0, 27.204375113248826, 0.0, 0.0, 0.0], 'rewardMean': 0.8623370774621655, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.1036728716099
'totalSteps': 3840, 'rewardStep': 0.6395554860146846, 'errorList': [], 'lossList': [0.0, -1.3999825465679168, 0.0, 33.922742812633516, 0.0, 0.0, 0.0], 'rewardMean': 0.7880765469796719, 'totalEpisodes': 11, 'stepsPerEpisode': 263, 'rewardPerEpisode': 201.59114429570403
'totalSteps': 5120, 'rewardStep': 0.7105120977953714, 'errorList': [], 'lossList': [0.0, -1.4016036814451218, 0.0, 25.431722968816757, 0.0, 0.0, 0.0], 'rewardMean': 0.7686854346835967, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.9747604482272
'totalSteps': 6400, 'rewardStep': 0.8612668484829329, 'errorList': [], 'lossList': [0.0, -1.3974467796087264, 0.0, 20.9412685623765, 0.0, 0.0, 0.0], 'rewardMean': 0.787201717443464, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.486297484792
'totalSteps': 7680, 'rewardStep': 0.8317803183002082, 'errorList': [], 'lossList': [0.0, -1.3909426349401475, 0.0, 16.817427765727043, 0.0, 0.0, 0.0], 'rewardMean': 0.7946314842529213, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.9878090634184
'totalSteps': 8960, 'rewardStep': 0.908079502010485, 'errorList': [], 'lossList': [0.0, -1.3724733233451842, 0.0, 10.21014777481556, 0.0, 0.0, 0.0], 'rewardMean': 0.8108383439325733, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.5628900534275
'totalSteps': 10240, 'rewardStep': 0.8176510611047618, 'errorList': [], 'lossList': [0.0, -1.3718874710798263, 0.0, 7.599191617071629, 0.0, 0.0, 0.0], 'rewardMean': 0.8116899335790968, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.7703762343806
'totalSteps': 11520, 'rewardStep': 0.8726480740152442, 'errorList': [], 'lossList': [0.0, -1.3554864263534545, 0.0, 570.5465723419189, 0.0, 0.0, 0.0], 'rewardMean': 0.8184630602942243, 'totalEpisodes': 46, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.248292449359138
'totalSteps': 12800, 'rewardStep': 0.7523409871203885, 'errorList': [], 'lossList': [0.0, -1.3552820515632629, 0.0, 197.95624179840087, 0.0, 0.0, 0.0], 'rewardMean': 0.8118508529768407, 'totalEpisodes': 88, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.615162842296414
'totalSteps': 14080, 'rewardStep': 0.7440599347212205, 'errorList': [], 'lossList': [0.0, -1.354980909228325, 0.0, 115.30806285858154, 0.0, 0.0, 0.0], 'rewardMean': 0.8062769503990665, 'totalEpisodes': 124, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.801493370783872
'totalSteps': 15360, 'rewardStep': 0.8873955438072668, 'errorList': [], 'lossList': [0.0, -1.3535460048913956, 0.0, 66.86136791229248, 0.0, 0.0, 0.0], 'rewardMean': 0.8025289853372565, 'totalEpisodes': 149, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.846155653215338
'totalSteps': 16640, 'rewardStep': 0.9164215067489024, 'errorList': [], 'lossList': [0.0, -1.3457781511545182, 0.0, 37.63012202262878, 0.0, 0.0, 0.0], 'rewardMean': 0.8302155874106782, 'totalEpisodes': 172, 'stepsPerEpisode': 45, 'rewardPerEpisode': 38.870841344504434
'totalSteps': 17920, 'rewardStep': 0.6107459486038699, 'errorList': [], 'lossList': [0.0, -1.3349875104427338, 0.0, 29.036486110687257, 0.0, 0.0, 0.0], 'rewardMean': 0.820238972491528, 'totalEpisodes': 190, 'stepsPerEpisode': 134, 'rewardPerEpisode': 100.55867331095878
'totalSteps': 19200, 'rewardStep': 0.9549397014422246, 'errorList': [31.676361914605398, 110.03754440735456, 32.45804471804521, 131.92506683055993, 31.582734337842883, 81.28116230293404, 32.07288225203871, 31.74872375193936, 88.55964274645429, 18.819234557787762, 6.422899705248527, 80.27633885398471, 132.72521846763598, 31.97090816205154, 31.715795237320695, 117.15922905035332, 123.58784965717473, 31.6802471604085, 31.61785886993806, 31.68423461234785, 94.90913624691478, 35.6534733180016, 0.879539265128484, 31.945296077436304, 32.36189979771297, 116.48341032469826, 19.32081770286522, 137.31298972421644, 120.97264274838413, 69.21486671008286, 31.64310388957198, 29.621751742547566, 28.33695620950771, 32.816170197407786, 22.944363463656007, 60.609072301199866, 121.76458653234897, 31.625407124084266, 70.64724892128228, 32.13740151072902, 152.66402011638118, 98.75152765679022, 90.47724231126467, 2.493883620287291, 129.49679902997994, 32.138883057394985, 31.60410193289506, 53.77600437684744, 21.03059061189134, 5.9083146652513765], 'lossList': [0.0, -1.328998545408249, 0.0, 24.41842183113098, 0.0, 0.0, 0.0], 'rewardMean': 0.8296062577874572, 'totalEpisodes': 207, 'stepsPerEpisode': 30, 'rewardPerEpisode': 28.278029005050314, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.7790164334472454, 'errorList': [], 'lossList': [0.0, -1.3290749263763428, 0.0, 15.129724717140197, 0.0, 0.0, 0.0], 'rewardMean': 0.8243298693021609, 'totalEpisodes': 213, 'stepsPerEpisode': 117, 'rewardPerEpisode': 98.02914404909448
'totalSteps': 21760, 'rewardStep': 0.8317368895304144, 'errorList': [], 'lossList': [0.0, -1.3228564959764482, 0.0, 15.74417958021164, 0.0, 0.0, 0.0], 'rewardMean': 0.8166956080541539, 'totalEpisodes': 221, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.50549555209493
'totalSteps': 23040, 'rewardStep': 0.45769973733792124, 'errorList': [], 'lossList': [0.0, -1.3135515451431274, 0.0, 13.142383513450623, 0.0, 0.0, 0.0], 'rewardMean': 0.7807004756774697, 'totalEpisodes': 224, 'stepsPerEpisode': 295, 'rewardPerEpisode': 214.55880756370345
'totalSteps': 24320, 'rewardStep': 0.9348485836753889, 'errorList': [0.348122242469218, 0.34299504274248227, 0.7069800769592989, 0.4856128626615173, 0.44297415356755554, 0.38631154423871783, 0.3659987679244554, 0.41716081531875226, 0.4792987526204377, 0.33485403461195185, 0.46171323554799576, 0.34062818571634, 0.4070217725076424, 0.419151541959417, 0.33161450503504025, 0.45301389139635334, 0.34254667264564614, 0.36320010155202737, 0.29976785446681686, 0.28101964360113724, 0.44721885509429143, 0.3424915770768961, 0.3122157363125943, 0.29042053507032967, 0.4087292123256152, 0.362294793134041, 0.28108247536731457, 0.45780828018357883, 0.3519441066787657, 0.46034573067605356, 0.29852104747967373, 0.27631090147594134, 0.2844888711500495, 0.535116047218498, 0.3485436502547686, 0.3668732933466013, 0.2738667046040384, 0.3017114363127445, 0.5399448120077829, 0.44098730246969653, 0.3879495111047286, 0.36543520034103455, 0.7250367426617874, 0.5763641710028772, 0.40426240767872407, 0.2902057181208395, 0.3024273754048738, 0.42227374079955265, 0.35752910473198174, 0.5274921264375305], 'lossList': [0.0, -1.2945040309429168, 0.0, 16.29578877210617, 0.0, 0.0, 0.0], 'rewardMean': 0.7869205266434842, 'totalEpisodes': 228, 'stepsPerEpisode': 113, 'rewardPerEpisode': 95.06749228217554, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.5425803317276743, 'errorList': [], 'lossList': [0.0, -1.287886860370636, 0.0, 7.5714584064483645, 0.0, 0.0, 0.0], 'rewardMean': 0.7659444611042128, 'totalEpisodes': 229, 'stepsPerEpisode': 318, 'rewardPerEpisode': 237.3885018736928
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=104.06
