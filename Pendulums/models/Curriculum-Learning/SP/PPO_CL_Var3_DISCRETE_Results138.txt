#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 138
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.657187940170172, 'errorList': [], 'lossList': [0.0, -1.4219418781995774, 0.0, 27.9683056306839, 0.0, 0.0, 0.0], 'rewardMean': 0.711256886558614, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.155583018468704
'totalSteps': 3840, 'rewardStep': 0.9612180606389581, 'errorList': [], 'lossList': [0.0, -1.4156839829683303, 0.0, 27.140998220443727, 0.0, 0.0, 0.0], 'rewardMean': 0.7945772779187287, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 374.2310408960704
'totalSteps': 5120, 'rewardStep': 0.8097549536927952, 'errorList': [], 'lossList': [0.0, -1.4097404754161835, 0.0, 27.785908808112143, 0.0, 0.0, 0.0], 'rewardMean': 0.7983716968622453, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.4483637329215
'totalSteps': 6400, 'rewardStep': 0.7022961852205006, 'errorList': [], 'lossList': [0.0, -1.3900028657913208, 0.0, 20.386204572319983, 0.0, 0.0, 0.0], 'rewardMean': 0.7791565945338964, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.0193214434457
'totalSteps': 7680, 'rewardStep': 0.9918523167229493, 'errorList': [], 'lossList': [0.0, -1.3700878071784972, 0.0, 18.063391667604446, 0.0, 0.0, 0.0], 'rewardMean': 0.8146058815654053, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1120.5015039084467
'totalSteps': 8960, 'rewardStep': 0.7988224738231249, 'errorList': [], 'lossList': [0.0, -1.3588334381580354, 0.0, 10.93037881910801, 0.0, 0.0, 0.0], 'rewardMean': 0.8123511090307938, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.110876703839
'totalSteps': 10240, 'rewardStep': 0.957228735208552, 'errorList': [101.68434217652437, 98.0661582971589, 103.77901464025788, 109.95363295964552, 104.89443841570301, 108.12377237348231, 108.89657504548431, 86.194499118191, 95.48754331948464, 104.21136255292633, 101.64304674683832, 96.60158030555937, 110.06561563058128, 107.36791304212818, 99.89223344365372, 102.26832129368022, 108.39585074314691, 110.01353181316136, 106.56781728552566, 100.16812260601452, 107.24851468573038, 105.73677018754547, 110.98345542814637, 105.23047502066223, 99.31048829643815, 100.70255404650864, 106.8282246552895, 96.83532574178282, 97.09775839649802, 104.34427753919468, 107.06731409625137, 97.65187339952084, 104.09882258717748, 103.03305612815778, 95.46949884686603, 102.75908448140873, 104.52370830497419, 112.054171911063, 102.5770454613066, 89.58382691418684, 96.97312367408261, 105.90846220650825, 94.96641622937406, 112.04873016397681, 98.08132838179058, 108.85530298579684, 107.27887084446671, 105.91252295052234, 102.2936921390453, 106.3880615929689], 'lossList': [0.0, -1.3367244148254394, 0.0, 9.932927299663424, 0.0, 0.0, 0.0], 'rewardMean': 0.8304608123030135, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.620273511599, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.663667736082607, 'errorList': [], 'lossList': [0.0, -1.3135488992929458, 0.0, 675.4693682861329, 0.0, 0.0, 0.0], 'rewardMean': 0.8119282482785238, 'totalEpisodes': 50, 'stepsPerEpisode': 34, 'rewardPerEpisode': 27.72549106569122
'totalSteps': 12800, 'rewardStep': 0.8655542472967498, 'errorList': [], 'lossList': [0.0, -1.3133350735902787, 0.0, 665.3522637939453, 0.0, 0.0, 0.0], 'rewardMean': 0.8172908481803465, 'totalEpisodes': 97, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.283267841967167
'totalSteps': 14080, 'rewardStep': 0.7971707551617445, 'errorList': [], 'lossList': [0.0, -1.3125816178321839, 0.0, 211.95525859832765, 0.0, 0.0, 0.0], 'rewardMean': 0.8204753404018152, 'totalEpisodes': 129, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.66662705342978
'totalSteps': 15360, 'rewardStep': 0.916446227321681, 'errorList': [], 'lossList': [0.0, -1.3095559895038604, 0.0, 110.77972135543823, 0.0, 0.0, 0.0], 'rewardMean': 0.8464011691169662, 'totalEpisodes': 172, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.195471156248427
'totalSteps': 16640, 'rewardStep': 0.47118742551229753, 'errorList': [], 'lossList': [0.0, -1.3054512184858322, 0.0, 45.16539987564087, 0.0, 0.0, 0.0], 'rewardMean': 0.7973981056043002, 'totalEpisodes': 201, 'stepsPerEpisode': 51, 'rewardPerEpisode': 37.80949824680064
'totalSteps': 17920, 'rewardStep': 0.45879869371077614, 'errorList': [], 'lossList': [0.0, -1.3082472079992293, 0.0, 18.695677762031554, 0.0, 0.0, 0.0], 'rewardMean': 0.7623024796060983, 'totalEpisodes': 226, 'stepsPerEpisode': 34, 'rewardPerEpisode': 20.618814349145552
'totalSteps': 19200, 'rewardStep': 0.8639845364262346, 'errorList': [], 'lossList': [0.0, -1.3019342720508575, 0.0, 19.558515377044678, 0.0, 0.0, 0.0], 'rewardMean': 0.7784713147266716, 'totalEpisodes': 250, 'stepsPerEpisode': 50, 'rewardPerEpisode': 45.97626389921812
'totalSteps': 20480, 'rewardStep': 0.9057523318510219, 'errorList': [], 'lossList': [0.0, -1.2797536647319794, 0.0, 12.135468754768372, 0.0, 0.0, 0.0], 'rewardMean': 0.769861316239479, 'totalEpisodes': 267, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.077819172217339
'totalSteps': 21760, 'rewardStep': 0.7759917596541014, 'errorList': [], 'lossList': [0.0, -1.2643116748332976, 0.0, 9.797292587757111, 0.0, 0.0, 0.0], 'rewardMean': 0.7675782448225765, 'totalEpisodes': 278, 'stepsPerEpisode': 70, 'rewardPerEpisode': 61.74650252435663
'totalSteps': 23040, 'rewardStep': 0.8747494987867351, 'errorList': [], 'lossList': [0.0, -1.262838528752327, 0.0, 10.766863353252411, 0.0, 0.0, 0.0], 'rewardMean': 0.7593303211803949, 'totalEpisodes': 285, 'stepsPerEpisode': 68, 'rewardPerEpisode': 55.570900422339975
'totalSteps': 24320, 'rewardStep': 0.2940467278701931, 'errorList': [], 'lossList': [0.0, -1.267672397494316, 0.0, 10.775236625671386, 0.0, 0.0, 0.0], 'rewardMean': 0.7223682203591535, 'totalEpisodes': 289, 'stepsPerEpisode': 132, 'rewardPerEpisode': 104.16828235981878
'totalSteps': 25600, 'rewardStep': 0.619947356851525, 'errorList': [], 'lossList': [0.0, -1.2665339928865433, 0.0, 9.429246128797532, 0.0, 0.0, 0.0], 'rewardMean': 0.697807531314631, 'totalEpisodes': 293, 'stepsPerEpisode': 509, 'rewardPerEpisode': 418.52036491208594
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=85.18
