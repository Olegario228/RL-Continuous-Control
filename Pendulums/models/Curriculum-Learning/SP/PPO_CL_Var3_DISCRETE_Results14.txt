#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 14
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8691065996348363, 'errorList': [], 'lossList': [0.0, -1.4240562045574188, 0.0, 30.64595845937729, 0.0, 0.0, 0.0], 'rewardMean': 0.7705110028890898, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 252.45415072774313
'totalSteps': 3840, 'rewardStep': 0.7734360832441656, 'errorList': [], 'lossList': [0.0, -1.4417775994539261, 0.0, 30.45178715825081, 0.0, 0.0, 0.0], 'rewardMean': 0.7714860296741151, 'totalEpisodes': 15, 'stepsPerEpisode': 168, 'rewardPerEpisode': 126.71211759057093
'totalSteps': 5120, 'rewardStep': 0.6133946466983342, 'errorList': [], 'lossList': [0.0, -1.4452192854881287, 0.0, 23.043071581721307, 0.0, 0.0, 0.0], 'rewardMean': 0.7319631839301699, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 957.1308145624366
'totalSteps': 6400, 'rewardStep': 0.809093075656337, 'errorList': [], 'lossList': [0.0, -1.4323048394918443, 0.0, 308.61212852478025, 0.0, 0.0, 0.0], 'rewardMean': 0.7473891622754033, 'totalEpisodes': 84, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.7121955314645
'totalSteps': 7680, 'rewardStep': 0.574746516857528, 'errorList': [], 'lossList': [0.0, -1.430774959921837, 0.0, 138.2854906463623, 0.0, 0.0, 0.0], 'rewardMean': 0.7186153880390908, 'totalEpisodes': 146, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.516061409340548
'totalSteps': 8960, 'rewardStep': 0.5509861004819911, 'errorList': [], 'lossList': [0.0, -1.4239840346574784, 0.0, 65.01005373001098, 0.0, 0.0, 0.0], 'rewardMean': 0.694668346959505, 'totalEpisodes': 199, 'stepsPerEpisode': 27, 'rewardPerEpisode': 18.994305348351478
'totalSteps': 10240, 'rewardStep': 0.26969396018061, 'errorList': [], 'lossList': [0.0, -1.4092819476127625, 0.0, 55.33798837661743, 0.0, 0.0, 0.0], 'rewardMean': 0.6415465486121432, 'totalEpisodes': 221, 'stepsPerEpisode': 108, 'rewardPerEpisode': 65.4843890860747
'totalSteps': 11520, 'rewardStep': 0.8788005616352772, 'errorList': [], 'lossList': [0.0, -1.3950417858362198, 0.0, 46.04799505233765, 0.0, 0.0, 0.0], 'rewardMean': 0.6679081056147136, 'totalEpisodes': 232, 'stepsPerEpisode': 78, 'rewardPerEpisode': 65.78601121604797
'totalSteps': 12800, 'rewardStep': 0.82058780979565, 'errorList': [], 'lossList': [0.0, -1.382657723426819, 0.0, 52.98033708095551, 0.0, 0.0, 0.0], 'rewardMean': 0.6831760760328073, 'totalEpisodes': 241, 'stepsPerEpisode': 65, 'rewardPerEpisode': 58.09204973120249
'totalSteps': 14080, 'rewardStep': 0.6393974766072426, 'errorList': [], 'lossList': [0.0, -1.354565840959549, 0.0, 19.11718895435333, 0.0, 0.0, 0.0], 'rewardMean': 0.6799242830791972, 'totalEpisodes': 245, 'stepsPerEpisode': 179, 'rewardPerEpisode': 136.37187879735106
'totalSteps': 15360, 'rewardStep': 0.7565191034527352, 'errorList': [], 'lossList': [0.0, -1.3265821492671968, 0.0, 53.92043628692627, 0.0, 0.0, 0.0], 'rewardMean': 0.668665533460987, 'totalEpisodes': 254, 'stepsPerEpisode': 195, 'rewardPerEpisode': 172.9369404363905
'totalSteps': 16640, 'rewardStep': 0.8530796562288709, 'errorList': [], 'lossList': [0.0, -1.3161786764860153, 0.0, 37.12646122455597, 0.0, 0.0, 0.0], 'rewardMean': 0.6766298907594577, 'totalEpisodes': 261, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.522124461103964
'totalSteps': 17920, 'rewardStep': 0.4087313753835489, 'errorList': [], 'lossList': [0.0, -1.299968569278717, 0.0, 18.977399265766145, 0.0, 0.0, 0.0], 'rewardMean': 0.6561635636279791, 'totalEpisodes': 267, 'stepsPerEpisode': 133, 'rewardPerEpisode': 95.87419143817672
'totalSteps': 19200, 'rewardStep': 0.8647073781070513, 'errorList': [], 'lossList': [0.0, -1.279490436911583, 0.0, 7.090392981767654, 0.0, 0.0, 0.0], 'rewardMean': 0.6617249938730506, 'totalEpisodes': 273, 'stepsPerEpisode': 93, 'rewardPerEpisode': 78.30825149979125
'totalSteps': 20480, 'rewardStep': 0.8774903333244596, 'errorList': [], 'lossList': [0.0, -1.2689545893669127, 0.0, 4.654999965429306, 0.0, 0.0, 0.0], 'rewardMean': 0.6919993755197437, 'totalEpisodes': 279, 'stepsPerEpisode': 313, 'rewardPerEpisode': 258.51395853259584
'totalSteps': 21760, 'rewardStep': 0.7820349065834706, 'errorList': [], 'lossList': [0.0, -1.268690465092659, 0.0, 5.343540504574776, 0.0, 0.0, 0.0], 'rewardMean': 0.7151042561298917, 'totalEpisodes': 284, 'stepsPerEpisode': 193, 'rewardPerEpisode': 163.72601346099262
'totalSteps': 23040, 'rewardStep': 0.9238800575492684, 'errorList': [], 'lossList': [0.0, -1.2422820657491684, 0.0, 5.617215192317962, 0.0, 0.0, 0.0], 'rewardMean': 0.7805228658667576, 'totalEpisodes': 288, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.529809278525725
'totalSteps': 24320, 'rewardStep': 0.8426773636178978, 'errorList': [], 'lossList': [0.0, -1.2417643213272094, 0.0, 2.9457833215594293, 0.0, 0.0, 0.0], 'rewardMean': 0.7769105460650195, 'totalEpisodes': 292, 'stepsPerEpisode': 76, 'rewardPerEpisode': 68.83674859168218
'totalSteps': 25600, 'rewardStep': 0.9519227099684178, 'errorList': [0.24597647298374434, 0.44623387289507366, 0.2753041886781003, 0.19615095581019318, 0.15997219152195902, 0.3036485459144374, 0.14662558704614168, 0.25757598636218254, 0.20924031269282514, 0.2893270385371601, 0.2825986633308186, 0.19108957809281904, 0.23626036902355813, 0.23768235292115128, 0.2135382668307445, 0.25541435045955496, 0.2737059266898958, 0.2052108978050093, 0.3708902632938166, 0.26755946767974265, 0.27916850590424547, 0.17370533289551432, 0.18564582159120818, 0.22725842234971905, 0.17861845361401738, 0.1841713277371139, 0.19582206580117556, 0.23633007854692925, 0.25439598274341446, 0.18654443701493578, 0.23031509464132982, 0.2581529553815178, 0.3520708082434366, 0.21238885176356845, 0.25443002354653194, 0.1491804982912624, 0.2515052766528436, 0.31366790569022324, 0.18275867787475134, 0.14254471756489004, 0.20905579856594392, 0.2548432663307478, 0.2695846619233483, 0.3571490169663529, 0.16310895870112885, 0.23452980337553728, 0.4475828233233235, 0.185241400058371, 0.3769515161890279, 0.29087808474528276], 'lossList': [0.0, -1.256599959731102, 0.0, 3.100683134496212, 0.0, 0.0, 0.0], 'rewardMean': 0.7900440360822963, 'totalEpisodes': 294, 'stepsPerEpisode': 143, 'rewardPerEpisode': 127.471869768059, 'successfulTests': 15
#maxSuccessfulTests=15, maxSuccessfulTestsAtStep=25600, timeSpent=76.95
