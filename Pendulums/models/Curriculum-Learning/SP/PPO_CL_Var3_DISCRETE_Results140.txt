#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 140
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9360084744127758, 'errorList': [], 'lossList': [0.0, -1.4352493596076965, 0.0, 30.507024736404418, 0.0, 0.0, 0.0], 'rewardMean': 0.9157950328104714, 'totalEpisodes': 8, 'stepsPerEpisode': 529, 'rewardPerEpisode': 377.10779752439424
'totalSteps': 3840, 'rewardStep': 0.7557452886750211, 'errorList': [], 'lossList': [0.0, -1.4207161974906921, 0.0, 32.01956871271133, 0.0, 0.0, 0.0], 'rewardMean': 0.8624451180986545, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 203.96302985448395
'totalSteps': 5120, 'rewardStep': 0.7637633211487607, 'errorList': [], 'lossList': [0.0, -1.4165323793888092, 0.0, 27.78206905066967, 0.0, 0.0, 0.0], 'rewardMean': 0.8377746688611811, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.0989296707711
'totalSteps': 6400, 'rewardStep': 0.9641597693966607, 'errorList': [], 'lossList': [0.0, -1.4044303733110428, 0.0, 23.92006703555584, 0.0, 0.0, 0.0], 'rewardMean': 0.863051688968277, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.5310283831257
'totalSteps': 7680, 'rewardStep': 0.7471233495034122, 'errorList': [], 'lossList': [0.0, -1.3953341794013978, 0.0, 17.48394833922386, 0.0, 0.0, 0.0], 'rewardMean': 0.8437302990574662, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.6654996091224
'totalSteps': 8960, 'rewardStep': 0.9130543384222685, 'errorList': [], 'lossList': [0.0, -1.3636931151151657, 0.0, 10.636468620598317, 0.0, 0.0, 0.0], 'rewardMean': 0.853633733252438, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.709398165792
'totalSteps': 10240, 'rewardStep': 0.8334929940577142, 'errorList': [], 'lossList': [0.0, -1.3581158620119096, 0.0, 8.0233880802989, 0.0, 0.0, 0.0], 'rewardMean': 0.8511161408530975, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.392864867693
'totalSteps': 11520, 'rewardStep': 0.7651790601600502, 'errorList': [], 'lossList': [0.0, -1.3297533708810807, 0.0, 800.6742253112793, 0.0, 0.0, 0.0], 'rewardMean': 0.8415675763316478, 'totalEpisodes': 55, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7651790601600502
'totalSteps': 12800, 'rewardStep': 0.6215577105618714, 'errorList': [], 'lossList': [0.0, -1.3293734395503998, 0.0, 513.8025900268555, 0.0, 0.0, 0.0], 'rewardMean': 0.8195665897546702, 'totalEpisodes': 92, 'stepsPerEpisode': 74, 'rewardPerEpisode': 64.97922433070376
'totalSteps': 14080, 'rewardStep': 0.6906087773688757, 'errorList': [], 'lossList': [0.0, -1.3283163630962371, 0.0, 171.58401622772217, 0.0, 0.0, 0.0], 'rewardMean': 0.7990693083707411, 'totalEpisodes': 142, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.456075814896526
'totalSteps': 15360, 'rewardStep': 0.7434017819207135, 'errorList': [], 'lossList': [0.0, -1.327326271533966, 0.0, 75.5625520324707, 0.0, 0.0, 0.0], 'rewardMean': 0.7798086391215349, 'totalEpisodes': 178, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.39985082769168
'totalSteps': 16640, 'rewardStep': 0.7094009813368034, 'errorList': [], 'lossList': [0.0, -1.326912134885788, 0.0, 45.66432820320129, 0.0, 0.0, 0.0], 'rewardMean': 0.775174208387713, 'totalEpisodes': 206, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.9043050125312715
'totalSteps': 17920, 'rewardStep': 0.9295429847380455, 'errorList': [], 'lossList': [0.0, -1.3283303934335708, 0.0, 22.766742424964903, 0.0, 0.0, 0.0], 'rewardMean': 0.7917521747466415, 'totalEpisodes': 228, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.372883885958531
'totalSteps': 19200, 'rewardStep': 0.577389489041645, 'errorList': [], 'lossList': [0.0, -1.3282959246635437, 0.0, 27.635235133171083, 0.0, 0.0, 0.0], 'rewardMean': 0.7530751467111398, 'totalEpisodes': 246, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.169960058244776
'totalSteps': 20480, 'rewardStep': 0.9384507190905895, 'errorList': [131.21552345486697, 140.78722749440436, 234.94499112851804, 211.79143482336164, 107.37508380946936, 178.28604144695734, 173.22460409854693, 139.44535254088515, 123.37161747367436, 169.815456699235, 151.54225695665477, 134.22718643628792, 108.77722102453967, 89.95070217593997, 125.66710119379059, 101.72867858995482, 131.88627003335145, 171.62386789127612, 178.64014105348608, 183.96639000405452, 161.4881823757038, 173.53162172207027, 157.64829980244392, 125.58406573901169, 123.19066744186642, 66.43189637208637, 110.32367161740125, 172.65943528307076, 180.71882834546506, 123.41068484924682, 78.40262897536813, 162.89312681714338, 117.29997142584429, 4.284601398886933, 157.3587231475533, 149.41498478710935, 121.64068256059326, 182.3651712387883, 140.00910134092794, 227.29258378041683, 5.118253050214492, 177.40525057982563, 156.59891845637665, 214.6187003279479, 153.75292233314175, 133.05516349500837, 156.93464870650863, 130.92715851216323, 144.51318226461743, 116.92256240883151], 'lossList': [0.0, -1.325225183367729, 0.0, 19.00735905647278, 0.0, 0.0, 0.0], 'rewardMean': 0.7722078836698577, 'totalEpisodes': 257, 'stepsPerEpisode': 83, 'rewardPerEpisode': 70.87408167611137, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8651382677759127, 'errorList': [], 'lossList': [0.0, -1.3209269922971725, 0.0, 20.94594473838806, 0.0, 0.0, 0.0], 'rewardMean': 0.767416276605222, 'totalEpisodes': 264, 'stepsPerEpisode': 60, 'rewardPerEpisode': 51.71171373056746
'totalSteps': 23040, 'rewardStep': 0.7534906429403236, 'errorList': [], 'lossList': [0.0, -1.3103695654869079, 0.0, 10.264652469158172, 0.0, 0.0, 0.0], 'rewardMean': 0.759416041493483, 'totalEpisodes': 268, 'stepsPerEpisode': 53, 'rewardPerEpisode': 42.59887717596613
'totalSteps': 24320, 'rewardStep': 0.7668643631362002, 'errorList': [], 'lossList': [0.0, -1.2800648993253707, 0.0, 9.859124902486801, 0.0, 0.0, 0.0], 'rewardMean': 0.7595845717910981, 'totalEpisodes': 271, 'stepsPerEpisode': 90, 'rewardPerEpisode': 68.07346635104162
'totalSteps': 25600, 'rewardStep': 0.8160893059310609, 'errorList': [], 'lossList': [0.0, -1.2625564903020858, 0.0, 6.684048767089844, 0.0, 0.0, 0.0], 'rewardMean': 0.779037731328017, 'totalEpisodes': 272, 'stepsPerEpisode': 1224, 'rewardPerEpisode': 1071.5571676882366
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=76.72
