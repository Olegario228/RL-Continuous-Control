#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 143
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5846711921719314, 'errorList': [], 'lossList': [0.0, -1.421724066734314, 0.0, 30.98340278506279, 0.0, 0.0, 0.0], 'rewardMean': 0.7214416503637355, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.439177292370605
'totalSteps': 3840, 'rewardStep': 0.9713347298877761, 'errorList': [], 'lossList': [0.0, -1.421346955895424, 0.0, 32.09056904911995, 0.0, 0.0, 0.0], 'rewardMean': 0.8047393435384157, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 391.86324735082087
'totalSteps': 5120, 'rewardStep': 0.836866084518641, 'errorList': [], 'lossList': [0.0, -1.4178673738241196, 0.0, 31.220293309092522, 0.0, 0.0, 0.0], 'rewardMean': 0.8127710287834721, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.6268705646223
'totalSteps': 6400, 'rewardStep': 0.7916054620296396, 'errorList': [], 'lossList': [0.0, -1.399567020535469, 0.0, 23.390938937067986, 0.0, 0.0, 0.0], 'rewardMean': 0.8085379154327056, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.9615870559815
'totalSteps': 7680, 'rewardStep': 0.9791091457614167, 'errorList': [], 'lossList': [0.0, -1.383092558979988, 0.0, 18.600734558850526, 0.0, 0.0, 0.0], 'rewardMean': 0.836966453820824, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.921359567801
'totalSteps': 8960, 'rewardStep': 0.8347668082834457, 'errorList': [], 'lossList': [0.0, -1.3708379256725312, 0.0, 10.846677855253219, 0.0, 0.0, 0.0], 'rewardMean': 0.8366522187440557, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.7825040990022
'totalSteps': 10240, 'rewardStep': 0.9794242695001929, 'errorList': [89.33577170649697, 87.8042965376012, 91.19368023296352, 94.5996334141249, 92.59064482021252, 92.98502271170621, 93.60081661524802, 79.77939187414472, 84.82727003805783, 92.10847393036909, 89.53667277703677, 86.40273187735845, 94.60067547468229, 95.02293061937853, 88.63808688388245, 87.14862020813972, 95.72150423914552, 94.80827805028838, 94.05151901021168, 85.60570942083662, 94.81068108204443, 93.28822668232812, 95.22775676732655, 92.94641799151053, 83.67996247451015, 89.7163944307619, 92.02984614356093, 82.0441859715963, 86.8559580245917, 92.66616556073224, 94.4443327505663, 86.79100449528913, 89.57711141026805, 87.92799198880114, 85.37050674445877, 87.46979939024162, 89.66426754567756, 96.31136195146529, 86.3412254658838, 76.8238767395912, 86.5426207799191, 90.47008686659468, 85.9006480151257, 96.30698499303648, 87.9623837114282, 93.39100258267159, 94.7116057692398, 93.38744027732241, 90.88103640927179, 90.95303309189711], 'lossList': [0.0, -1.3529518687725066, 0.0, 10.248424583226443, 0.0, 0.0, 0.0], 'rewardMean': 0.8544987250885729, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.0044108169386, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5449631188378983, 'errorList': [], 'lossList': [0.0, -1.327757592201233, 0.0, 781.5351849365235, 0.0, 0.0, 0.0], 'rewardMean': 0.8201058799496092, 'totalEpisodes': 55, 'stepsPerEpisode': 9, 'rewardPerEpisode': 4.732120802894293
'totalSteps': 12800, 'rewardStep': 0.9292246960987257, 'errorList': [], 'lossList': [0.0, -1.3275911039114, 0.0, 606.7946095275879, 0.0, 0.0, 0.0], 'rewardMean': 0.8310177615645207, 'totalEpisodes': 93, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.752719940323788
'totalSteps': 14080, 'rewardStep': 0.5485477671149107, 'errorList': [], 'lossList': [0.0, -1.32733873128891, 0.0, 375.5142221069336, 0.0, 0.0, 0.0], 'rewardMean': 0.8000513274204579, 'totalEpisodes': 131, 'stepsPerEpisode': 58, 'rewardPerEpisode': 49.258292435557074
'totalSteps': 15360, 'rewardStep': 0.7262055677711019, 'errorList': [], 'lossList': [0.0, -1.3240630036592484, 0.0, 111.24363033294678, 0.0, 0.0, 0.0], 'rewardMean': 0.8142047649803749, 'totalEpisodes': 173, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.885946903914327
'totalSteps': 16640, 'rewardStep': 0.5401457970208314, 'errorList': [], 'lossList': [0.0, -1.323597200512886, 0.0, 41.07778823375702, 0.0, 0.0, 0.0], 'rewardMean': 0.7710858716936804, 'totalEpisodes': 202, 'stepsPerEpisode': 52, 'rewardPerEpisode': 39.9418881358339
'totalSteps': 17920, 'rewardStep': 0.965897640785777, 'errorList': [249.53506340141138, 205.69216351937965, 212.5487266118623, 181.23257284798007, 210.2950599635558, 238.65707537863628, 236.42936390913488, 251.76454399912134, 245.9291393750399, 237.997621855265, 210.04249046564627, 257.2098454027753, 244.92673807484448, 229.33737946977928, 216.64802999311084, 235.74708175604596, 228.9529147447302, 236.2816225359639, 243.91739981962584, 246.29423369782873, 255.12855715818785, 227.27172730342608, 216.85055933693482, 180.4076619076014, 234.0109291237286, 211.74768638357264, 171.99145114129746, 245.51678312784537, 214.69091236370764, 234.88173810639537, 212.47302854148512, 246.58930536543465, 189.95324405144945, 197.3660748855673, 235.6489217418826, 221.6474605993154, 227.0301702622137, 248.83480732263658, 227.5402995162006, 244.34190430955033, 221.4103050044044, 236.6639620530588, 246.74586662151137, 253.92143780948052, 251.7517652674494, 222.95625083313652, 224.3045927879614, 234.6521849817731, 232.2736593763136, 237.32613969240379], 'lossList': [0.0, -1.3244434195756911, 0.0, 25.191493401527406, 0.0, 0.0, 0.0], 'rewardMean': 0.783989027320394, 'totalEpisodes': 230, 'stepsPerEpisode': 34, 'rewardPerEpisode': 31.662249829241954, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.9735782788950916, 'errorList': [222.3864536509302, 160.0181891063424, 244.72960698950274, 170.41693351061997, 123.5528134031843, 206.46950841247175, 212.72147585405787, 210.54314860464953, 224.9700015538777, 202.77623955963116, 216.61096318078657, 158.21159887420094, 223.9252585289905, 238.21491260972022, 243.89787040910446, 203.9009346854343, 232.92527698225078, 238.24078930583693, 199.17767670350636, 73.18452714573988, 229.83716134405398, 232.999052033313, 200.55699791324452, 230.79698509001483, 232.62196291571004, 152.37303325335031, 169.65035460459526, 92.26015491884259, 241.86658661629394, 235.6111079677768, 180.35376334616973, 95.47236113234683, 219.7931600527357, 183.48640011492807, 213.02070660459083, 206.80332092516213, 202.6360747943438, 208.73724368485054, 245.57298275445348, 162.3369632853217, 156.66385632421947, 214.79804013214468, 234.69642920453492, 230.93965429824576, 232.04334159811154, 253.17572702980902, 238.94391074144616, 234.94784539399984, 225.02109262508972, 207.09132059503395], 'lossList': [0.0, -1.3150672113895416, 0.0, 25.341425461769106, 0.0, 0.0, 0.0], 'rewardMean': 0.8021863090069392, 'totalEpisodes': 247, 'stepsPerEpisode': 37, 'rewardPerEpisode': 32.26687214300753, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8162289584190252, 'errorList': [], 'lossList': [0.0, -1.3008379465341569, 0.0, 14.154705183506012, 0.0, 0.0, 0.0], 'rewardMean': 0.7858982902727, 'totalEpisodes': 262, 'stepsPerEpisode': 64, 'rewardPerEpisode': 55.45995745847243
'totalSteps': 21760, 'rewardStep': 0.6410843102297028, 'errorList': [], 'lossList': [0.0, -1.285369239449501, 0.0, 14.560918629169464, 0.0, 0.0, 0.0], 'rewardMean': 0.7665300404673256, 'totalEpisodes': 273, 'stepsPerEpisode': 23, 'rewardPerEpisode': 14.238228085262065
'totalSteps': 23040, 'rewardStep': 0.30834225832378714, 'errorList': [], 'lossList': [0.0, -1.2676537322998047, 0.0, 7.1179559755325315, 0.0, 0.0, 0.0], 'rewardMean': 0.6994218393496852, 'totalEpisodes': 282, 'stepsPerEpisode': 176, 'rewardPerEpisode': 133.97678359655256
'totalSteps': 24320, 'rewardStep': 0.5797800314959767, 'errorList': [], 'lossList': [0.0, -1.257888280749321, 0.0, 9.357272157669067, 0.0, 0.0, 0.0], 'rewardMean': 0.702903530615493, 'totalEpisodes': 290, 'stepsPerEpisode': 16, 'rewardPerEpisode': 9.949382158004505
'totalSteps': 25600, 'rewardStep': 0.3257319104410899, 'errorList': [], 'lossList': [0.0, -1.2616348165273665, 0.0, 6.668814631700516, 0.0, 0.0, 0.0], 'rewardMean': 0.6425542520497294, 'totalEpisodes': 296, 'stepsPerEpisode': 149, 'rewardPerEpisode': 103.12515828780559
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=89.49
