#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 144
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8807831400329279, 'errorList': [], 'lossList': [0.0, -1.4239101493358612, 0.0, 34.1101538413763, 0.0, 0.0, 0.0], 'rewardMean': 0.8020670319561144, 'totalEpisodes': 13, 'stepsPerEpisode': 311, 'rewardPerEpisode': 259.5193165874563
'totalSteps': 3840, 'rewardStep': 0.7149764825536262, 'errorList': [], 'lossList': [0.0, -1.4262116760015489, 0.0, 33.58108193159104, 0.0, 0.0, 0.0], 'rewardMean': 0.7730368488219517, 'totalEpisodes': 16, 'stepsPerEpisode': 174, 'rewardPerEpisode': 131.53359940844769
'totalSteps': 5120, 'rewardStep': 0.7105539823194872, 'errorList': [], 'lossList': [0.0, -1.4302703541517259, 0.0, 20.59622856259346, 0.0, 0.0, 0.0], 'rewardMean': 0.7574161321963355, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 944.9368777886872
'totalSteps': 6400, 'rewardStep': 0.8902491246567459, 'errorList': [], 'lossList': [0.0, -1.4038825798034669, 0.0, 18.17871629357338, 0.0, 0.0, 0.0], 'rewardMean': 0.7839827306884176, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 984.9605268040733
'totalSteps': 7680, 'rewardStep': 0.8549897689917594, 'errorList': [], 'lossList': [0.0, -1.4109313309192657, 0.0, 14.03353094816208, 0.0, 0.0, 0.0], 'rewardMean': 0.795817237072308, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.589886997253
'totalSteps': 8960, 'rewardStep': 0.6357057049120484, 'errorList': [], 'lossList': [0.0, -1.4085927349328995, 0.0, 8.294088984727859, 0.0, 0.0, 0.0], 'rewardMean': 0.7729441610494138, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.5563965650211
'totalSteps': 10240, 'rewardStep': 0.8477749470829798, 'errorList': [], 'lossList': [0.0, -1.412787175178528, 0.0, 8.168015638291836, 0.0, 0.0, 0.0], 'rewardMean': 0.7822980093036095, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.1958011420227
'totalSteps': 11520, 'rewardStep': 0.6693155527161458, 'errorList': [], 'lossList': [0.0, -1.3956931203603744, 0.0, 780.6968536376953, 0.0, 0.0, 0.0], 'rewardMean': 0.7697444030161136, 'totalEpisodes': 67, 'stepsPerEpisode': 57, 'rewardPerEpisode': 45.60277423523034
'totalSteps': 12800, 'rewardStep': 0.553180221117899, 'errorList': [], 'lossList': [0.0, -1.395128510594368, 0.0, 624.0698593139648, 0.0, 0.0, 0.0], 'rewardMean': 0.7480879848262921, 'totalEpisodes': 120, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.317255608604234
'totalSteps': 14080, 'rewardStep': 0.9355020811798341, 'errorList': [79.80881442137189, 80.4726675168702, 80.24228711497696, 76.38084770483394, 73.2365954122963, 81.00131162632084, 79.94835905150019, 78.16335428801825, 82.33152408368487, 79.98314423024428, 77.53568805729932, 79.37124125814452, 72.36246007207858, 81.51371898717422, 77.4360002387677, 74.99787417452058, 79.60212866939499, 80.53757939898927, 80.3887854423256, 81.87395901232713, 81.16661586031601, 65.75654275398459, 80.76347970795372, 81.49417261476147, 77.07110397213036, 81.3014187071916, 79.3721702120961, 71.32116148241435, 81.14520744846212, 79.67759538065135, 81.9736490648292, 81.15030764941858, 75.28662184242302, 76.28850976105115, 79.36567653633004, 80.22696028915169, 79.75293783754834, 76.05400228161892, 73.79148597689712, 77.57187762792012, 79.12631136952795, 81.05442665916904, 80.86401781795406, 80.15727789100804, 72.46222929021397, 74.20406635054462, 75.47135046712431, 69.93440231530833, 64.89626718869135, 79.42092524551003], 'lossList': [0.0, -1.3949235504865647, 0.0, 461.1067961120605, 0.0, 0.0, 0.0], 'rewardMean': 0.7693031005563454, 'totalEpisodes': 174, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.55847202664464, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.48442271680099547, 'errorList': [], 'lossList': [0.0, -1.3948642098903656, 0.0, 224.5763433074951, 0.0, 0.0, 0.0], 'rewardMean': 0.7296670582331521, 'totalEpisodes': 220, 'stepsPerEpisode': 21, 'rewardPerEpisode': 12.467937007300314
'totalSteps': 16640, 'rewardStep': 0.7461289054401486, 'errorList': [], 'lossList': [0.0, -1.394157629609108, 0.0, 107.36266468048096, 0.0, 0.0, 0.0], 'rewardMean': 0.7327823005218044, 'totalEpisodes': 271, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.03351032516532
'totalSteps': 17920, 'rewardStep': 0.8967685805600009, 'errorList': [], 'lossList': [0.0, -1.3915677374601365, 0.0, 55.981240329742434, 0.0, 0.0, 0.0], 'rewardMean': 0.7514037603458557, 'totalEpisodes': 305, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.592918916540203
'totalSteps': 19200, 'rewardStep': 0.6580488477002778, 'errorList': [], 'lossList': [0.0, -1.3878858387470245, 0.0, 44.198607749938965, 0.0, 0.0, 0.0], 'rewardMean': 0.728183732650209, 'totalEpisodes': 334, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.525440459451351
'totalSteps': 20480, 'rewardStep': 0.7613556093162321, 'errorList': [], 'lossList': [0.0, -1.3878906798362731, 0.0, 34.107745237350464, 0.0, 0.0, 0.0], 'rewardMean': 0.7188203166826562, 'totalEpisodes': 350, 'stepsPerEpisode': 91, 'rewardPerEpisode': 69.10980251116352
'totalSteps': 21760, 'rewardStep': 0.603994146840234, 'errorList': [], 'lossList': [0.0, -1.395201824903488, 0.0, 23.834840359687806, 0.0, 0.0, 0.0], 'rewardMean': 0.7156491608754748, 'totalEpisodes': 362, 'stepsPerEpisode': 88, 'rewardPerEpisode': 73.61961223644587
'totalSteps': 23040, 'rewardStep': 0.5508360837486179, 'errorList': [], 'lossList': [0.0, -1.3948739057779311, 0.0, 15.434536764621734, 0.0, 0.0, 0.0], 'rewardMean': 0.6859552745420386, 'totalEpisodes': 370, 'stepsPerEpisode': 144, 'rewardPerEpisode': 92.01459832401208
'totalSteps': 24320, 'rewardStep': 0.5310007300077159, 'errorList': [], 'lossList': [0.0, -1.3864911955595016, 0.0, 14.106060650348663, 0.0, 0.0, 0.0], 'rewardMean': 0.6721237922711957, 'totalEpisodes': 377, 'stepsPerEpisode': 127, 'rewardPerEpisode': 103.0611355192598
'totalSteps': 25600, 'rewardStep': 0.8108390139746465, 'errorList': [], 'lossList': [0.0, -1.3784116107225417, 0.0, 28.032679553031922, 0.0, 0.0, 0.0], 'rewardMean': 0.6978896715568704, 'totalEpisodes': 383, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.02502263683882
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=70.05
