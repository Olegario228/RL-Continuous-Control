#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 147
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8663769990037149, 'errorList': [], 'lossList': [0.0, -1.4424236595630646, 0.0, 30.24666601061821, 0.0, 0.0, 0.0], 'rewardMean': 0.6847389234538633, 'totalEpisodes': 10, 'stepsPerEpisode': 905, 'rewardPerEpisode': 676.1619782270964
'totalSteps': 3840, 'rewardStep': 0.8318612758400938, 'errorList': [], 'lossList': [0.0, -1.4595861428976058, 0.0, 44.8659469127655, 0.0, 0.0, 0.0], 'rewardMean': 0.7337797075826069, 'totalEpisodes': 12, 'stepsPerEpisode': 668, 'rewardPerEpisode': 531.5237637867058
'totalSteps': 5120, 'rewardStep': 0.9420636962973828, 'errorList': [], 'lossList': [0.0, -1.4606796836853027, 0.0, 29.767981889247896, 0.0, 0.0, 0.0], 'rewardMean': 0.7858507047613009, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.4430166496704
'totalSteps': 6400, 'rewardStep': 0.7083415081798111, 'errorList': [], 'lossList': [0.0, -1.4487400782108306, 0.0, 25.63570770084858, 0.0, 0.0, 0.0], 'rewardMean': 0.7703488654450029, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.4453318273113
'totalSteps': 7680, 'rewardStep': 0.8333019530015451, 'errorList': [], 'lossList': [0.0, -1.4341128611564635, 0.0, 16.80647209405899, 0.0, 0.0, 0.0], 'rewardMean': 0.7808410467044267, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.8633334670064
'totalSteps': 8960, 'rewardStep': 0.8839684413984422, 'errorList': [], 'lossList': [0.0, -1.4346774917840959, 0.0, 16.156789700463413, 0.0, 0.0, 0.0], 'rewardMean': 0.7955735316607147, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.637516643312
'totalSteps': 10240, 'rewardStep': 0.9663382649410598, 'errorList': [93.12125225177023, 90.58824304695752, 92.49478137144658, 93.67135872599081, 92.74443411540996, 94.95050116174222, 91.28669219981663, 94.47455165921832, 93.62135088834489, 95.15402235041445, 94.05003224164085, 92.59862785001508, 91.46131372856105, 93.49163780299223, 92.2822285129645, 94.96148427333398, 85.91308797169998, 88.27375550830067, 93.70879063507945, 93.81860683656943, 93.40989383545644, 85.49237470712771, 90.66383474911086, 90.30909719860432, 85.22426470311932, 90.74000235843666, 89.80741293094839, 92.56502469298077, 93.91653154848997, 96.69449668739733, 89.09880263958497, 84.16171977913149, 93.09295271764711, 91.46134398668325, 88.91848179206653, 93.11966141774548, 85.40019352349708, 94.03925994277316, 89.34232966053533, 95.37633654398552, 93.5523803397548, 92.70721438633639, 92.07576135365525, 90.97305889934663, 86.85976749029595, 89.90625642445643, 82.14298183947145, 89.40437247217525, 92.96597724885979, 64.03212556559859], 'lossList': [0.0, -1.4189454430341721, 0.0, 10.837611444704235, 0.0, 0.0, 0.0], 'rewardMean': 0.8169191233207577, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.770659441609, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.545789426827921, 'errorList': [], 'lossList': [0.0, -1.403928970694542, 0.0, 847.3148208618164, 0.0, 0.0, 0.0], 'rewardMean': 0.7867936014882202, 'totalEpisodes': 55, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.135663072527162
'totalSteps': 12800, 'rewardStep': 0.6673457385304093, 'errorList': [], 'lossList': [0.0, -1.403052158355713, 0.0, 682.4217658996582, 0.0, 0.0, 0.0], 'rewardMean': 0.7748488151924391, 'totalEpisodes': 99, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.828338521251661
'totalSteps': 14080, 'rewardStep': 0.9906122573749082, 'errorList': [93.55933523158306, 95.8602890497108, 85.64177455848868, 91.11748964120518, 89.47816275099542, 94.16479779622837, 90.21975054744472, 94.17100636999974, 94.19468283158413, 96.54485543576841, 92.682286261234, 92.07339414111645, 95.20408265595424, 91.74604025398952, 96.11105097360723, 93.48778194413359, 95.67678719848587, 86.5869828689466, 92.0912592469831, 84.30380925475076, 93.9336569253349, 92.56357926836263, 95.74660714911555, 92.92218525430998, 87.68679868387594, 93.60563615314351, 87.8741038577655, 94.54667105482258, 91.59485394468575, 86.95032776885674, 91.6642432334798, 95.03728132510282, 91.57729748915516, 94.57395888426245, 94.35480349316695, 95.9579418635233, 90.61831575138363, 92.29299449524201, 91.08481981974232, 95.58881003718082, 95.93498016397935, 92.51020355440112, 86.59211426443169, 88.55098268103733, 94.59913284035758, 94.16946223412701, 93.49507056721207, 88.62487122701455, 84.73447658202291, 94.28507956723207], 'lossList': [0.0, -1.402696245908737, 0.0, 507.99318099975585, 0.0, 0.0, 0.0], 'rewardMean': 0.8235999561395287, 'totalEpisodes': 145, 'stepsPerEpisode': 35, 'rewardPerEpisode': 30.584499004852358, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.9552408805685284, 'errorList': [116.03322419345552, 111.43384937954872, 116.49625033539647, 111.27711016586863, 116.19631099271413, 116.2107439947013, 115.79365464084415, 104.2248746358942, 114.06100790513842, 114.30583567309266, 104.04316452409775, 109.92585114422486, 118.82853872960266, 107.18057996517273, 100.12716057766268, 113.83107739225426, 116.20079571883686, 114.80665088762674, 88.51530241929649, 117.60893701391451, 115.58968071500368, 109.73383955138021, 109.89251402801656, 113.50241853511886, 109.99044056490858, 110.99806766180664, 103.46889060575737, 115.28769208043813, 104.83901713313591, 115.54836038983096, 112.34235423638307, 108.90393532024652, 104.49051968440614, 99.67600018930383, 113.27752515818618, 89.95374643296233, 95.47744132755608, 106.72482842224612, 113.60669720500336, 112.06936656073965, 110.4950695070006, 103.31245924720243, 106.41727983452277, 98.33728637464226, 107.522301249014, 114.68310335986385, 112.55440016798347, 118.41802713726062, 105.40309897686195, 118.10907184486963], 'lossList': [0.0, -1.4018212783336639, 0.0, 250.6787686920166, 0.0, 0.0, 0.0], 'rewardMean': 0.8324863442960103, 'totalEpisodes': 189, 'stepsPerEpisode': 12, 'rewardPerEpisode': 11.24899366050321, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.4475920884977145, 'errorList': [], 'lossList': [0.0, -1.4007665729522705, 0.0, 88.05136318206787, 0.0, 0.0, 0.0], 'rewardMean': 0.7940594255617721, 'totalEpisodes': 234, 'stepsPerEpisode': 35, 'rewardPerEpisode': 23.16089495087675
'totalSteps': 17920, 'rewardStep': 0.6837445374722393, 'errorList': [], 'lossList': [0.0, -1.398757854104042, 0.0, 46.313255043029784, 0.0, 0.0, 0.0], 'rewardMean': 0.7682275096792579, 'totalEpisodes': 268, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.315179663766576
'totalSteps': 19200, 'rewardStep': 0.8905265409773002, 'errorList': [], 'lossList': [0.0, -1.3923462229967116, 0.0, 23.481390018463134, 0.0, 0.0, 0.0], 'rewardMean': 0.7864460129590067, 'totalEpisodes': 293, 'stepsPerEpisode': 77, 'rewardPerEpisode': 67.8102692752301
'totalSteps': 20480, 'rewardStep': 0.5631210640016879, 'errorList': [], 'lossList': [0.0, -1.3808617049455643, 0.0, 9.991220846176148, 0.0, 0.0, 0.0], 'rewardMean': 0.7594279240590212, 'totalEpisodes': 310, 'stepsPerEpisode': 68, 'rewardPerEpisode': 46.45981582720411
'totalSteps': 21760, 'rewardStep': 0.9408579368228951, 'errorList': [162.72057441785174, 202.98994035285435, 176.77059872266517, 113.36024484779297, 151.09579369407606, 91.46209964072786, 204.62526106629338, 126.3398203080794, 116.64377908477574, 219.568806999495, 200.90694966819024, 185.90714369314128, 150.75215783640488, 182.18642448861095, 78.32261878935789, 155.36900502184258, 193.76753384106797, 187.82244651782048, 206.3521766297196, 186.84049702815025, 178.34245815278322, 13.490967359157853, 188.9087173649203, 178.68012128504355, 227.95080516870897, 176.1122851828905, 132.66381183947908, 225.53666835380366, 189.87821844961908, 143.4024985066725, 143.02487220901588, 173.08807723719056, 190.61662332375377, 144.31558259235746, 194.97956523941602, 208.32348154969253, 189.90516527378105, 215.38523715194927, 140.21472282359997, 197.8329237531066, 125.48572266225992, 184.942950389988, 192.79149414311252, 212.6026782782008, 208.18249148109962, 158.61371256086747, 140.33111260986712, 223.4886829332869, 185.27110807191224, 197.86486543007342], 'lossList': [0.0, -1.3642779684066773, 0.0, 14.403613722324371, 0.0, 0.0, 0.0], 'rewardMean': 0.7651168736014664, 'totalEpisodes': 322, 'stepsPerEpisode': 89, 'rewardPerEpisode': 76.71273391816959, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.6914609189476411, 'errorList': [], 'lossList': [0.0, -1.365434824824333, 0.0, 9.230500930547715, 0.0, 0.0, 0.0], 'rewardMean': 0.7376291390021245, 'totalEpisodes': 330, 'stepsPerEpisode': 22, 'rewardPerEpisode': 13.461369183614314
'totalSteps': 24320, 'rewardStep': 0.7781357233063301, 'errorList': [], 'lossList': [0.0, -1.371338769197464, 0.0, 9.45762973189354, 0.0, 0.0, 0.0], 'rewardMean': 0.7608637686499654, 'totalEpisodes': 337, 'stepsPerEpisode': 65, 'rewardPerEpisode': 50.37210894038916
'totalSteps': 25600, 'rewardStep': 0.7337106444473271, 'errorList': [], 'lossList': [0.0, -1.3657715946435929, 0.0, 9.07466668844223, 0.0, 0.0, 0.0], 'rewardMean': 0.7675002592416572, 'totalEpisodes': 343, 'stepsPerEpisode': 143, 'rewardPerEpisode': 116.72380002760225
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.29
