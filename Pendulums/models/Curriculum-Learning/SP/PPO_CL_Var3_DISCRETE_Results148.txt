#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 148
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.574368653695752, 'errorList': [], 'lossList': [0.0, -1.4227782970666885, 0.0, 32.65724693536758, 0.0, 0.0, 0.0], 'rewardMean': 0.7366495347612462, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.26894055682111
'totalSteps': 3840, 'rewardStep': 0.9642710036560319, 'errorList': [], 'lossList': [0.0, -1.422820115685463, 0.0, 36.074976375103, 0.0, 0.0, 0.0], 'rewardMean': 0.8125233577261747, 'totalEpisodes': 18, 'stepsPerEpisode': 486, 'rewardPerEpisode': 404.51896440838476
'totalSteps': 5120, 'rewardStep': 0.8515721396544952, 'errorList': [], 'lossList': [0.0, -1.4184762996435165, 0.0, 33.07030442774296, 0.0, 0.0, 0.0], 'rewardMean': 0.8222855532082548, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.9682976252727
'totalSteps': 6400, 'rewardStep': 0.8336663349294923, 'errorList': [], 'lossList': [0.0, -1.402276433110237, 0.0, 25.176099026203154, 0.0, 0.0, 0.0], 'rewardMean': 0.8245617095525024, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.9857121463865
'totalSteps': 7680, 'rewardStep': 0.9699351155057788, 'errorList': [], 'lossList': [0.0, -1.392553585767746, 0.0, 19.627057040184738, 0.0, 0.0, 0.0], 'rewardMean': 0.848790610544715, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.0557908709573
'totalSteps': 8960, 'rewardStep': 0.858684737323219, 'errorList': [], 'lossList': [0.0, -1.378914794921875, 0.0, 11.62502719298005, 0.0, 0.0, 0.0], 'rewardMean': 0.8502040572273585, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.0505032913252
'totalSteps': 10240, 'rewardStep': 0.9938113081924694, 'errorList': [83.83404739459768, 83.00777267746615, 85.38995561738155, 89.14295456090335, 86.48255412917457, 87.65128506507072, 88.25391703983003, 77.90261756566417, 80.92470377094477, 86.24063571528481, 84.00768499286877, 81.96507271763475, 89.12210086143362, 88.49413823155722, 83.60851883819656, 81.86256124417312, 88.986055880294, 89.41329644854696, 87.64260866473036, 80.82280190401687, 88.29303924386923, 87.0285303063427, 89.65322326599497, 86.79866876011383, 77.91539379354319, 84.53152312248099, 86.94956851032774, 77.26402316332356, 82.11017605288507, 86.72007172532889, 87.9410997006173, 82.17879920795039, 84.83089489678487, 82.81380660475702, 81.2446012540337, 82.32314007737126, 84.9493701334202, 90.73742378071474, 80.62973228069792, 68.44723319255029, 82.00538582982853, 85.49873697307822, 81.84953892059407, 90.74979045752426, 83.1627875174118, 87.93204641225181, 88.18453751051982, 87.104419650251, 85.39105315315616, 85.84681411115513], 'lossList': [0.0, -1.371407698392868, 0.0, 10.852112981677056, 0.0, 0.0, 0.0], 'rewardMean': 0.8681549635979974, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.6257848256719, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8538110293462299, 'errorList': [], 'lossList': [0.0, -1.347552753686905, 0.0, 886.5578758239747, 0.0, 0.0, 0.0], 'rewardMean': 0.8665611931255788, 'totalEpisodes': 59, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.21493368617176
'totalSteps': 12800, 'rewardStep': 0.6919218517839484, 'errorList': [], 'lossList': [0.0, -1.347844101190567, 0.0, 577.9759143066407, 0.0, 0.0, 0.0], 'rewardMean': 0.8490972589914157, 'totalEpisodes': 94, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.541033865276688
'totalSteps': 14080, 'rewardStep': 0.6127679794301194, 'errorList': [], 'lossList': [0.0, -1.348367286324501, 0.0, 475.45414527893064, 0.0, 0.0, 0.0], 'rewardMean': 0.8204810153517537, 'totalEpisodes': 146, 'stepsPerEpisode': 35, 'rewardPerEpisode': 23.832345932904257
'totalSteps': 15360, 'rewardStep': 0.8610575488885986, 'errorList': [], 'lossList': [0.0, -1.3442149925231934, 0.0, 88.10261362075806, 0.0, 0.0, 0.0], 'rewardMean': 0.8491499048710383, 'totalEpisodes': 189, 'stepsPerEpisode': 43, 'rewardPerEpisode': 37.462370704215424
'totalSteps': 16640, 'rewardStep': 0.8871447655171978, 'errorList': [], 'lossList': [0.0, -1.3398723143339157, 0.0, 71.08851737976075, 0.0, 0.0, 0.0], 'rewardMean': 0.841437281057155, 'totalEpisodes': 223, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.195537850735677
'totalSteps': 17920, 'rewardStep': 0.6263334381071973, 'errorList': [], 'lossList': [0.0, -1.3397167468070983, 0.0, 33.435210785865785, 0.0, 0.0, 0.0], 'rewardMean': 0.818913410902425, 'totalEpisodes': 254, 'stepsPerEpisode': 48, 'rewardPerEpisode': 39.19544416009575
'totalSteps': 19200, 'rewardStep': 0.5993111842968522, 'errorList': [], 'lossList': [0.0, -1.3359827703237535, 0.0, 28.11614179134369, 0.0, 0.0, 0.0], 'rewardMean': 0.7954778958391611, 'totalEpisodes': 279, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.308190214715466
'totalSteps': 20480, 'rewardStep': 0.5047581525789906, 'errorList': [], 'lossList': [0.0, -1.3298586189746857, 0.0, 18.134395785331726, 0.0, 0.0, 0.0], 'rewardMean': 0.7489601995464822, 'totalEpisodes': 298, 'stepsPerEpisode': 22, 'rewardPerEpisode': 11.352007544023982
'totalSteps': 21760, 'rewardStep': 0.70465974456582, 'errorList': [], 'lossList': [0.0, -1.3247626006603241, 0.0, 16.223723468780516, 0.0, 0.0, 0.0], 'rewardMean': 0.7335577002707424, 'totalEpisodes': 314, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.886458547047525
'totalSteps': 23040, 'rewardStep': 0.8232741576198863, 'errorList': [], 'lossList': [0.0, -1.313363954424858, 0.0, 12.397580785751343, 0.0, 0.0, 0.0], 'rewardMean': 0.7165039852134841, 'totalEpisodes': 323, 'stepsPerEpisode': 40, 'rewardPerEpisode': 30.961624432720715
'totalSteps': 24320, 'rewardStep': 0.9054155922474615, 'errorList': [], 'lossList': [0.0, -1.2963218784332275, 0.0, 11.314086587429047, 0.0, 0.0, 0.0], 'rewardMean': 0.7216644415036073, 'totalEpisodes': 331, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.69924632293554
'totalSteps': 25600, 'rewardStep': 0.7877116933460474, 'errorList': [], 'lossList': [0.0, -1.3011759090423585, 0.0, 7.891752043962478, 0.0, 0.0, 0.0], 'rewardMean': 0.731243425659817, 'totalEpisodes': 338, 'stepsPerEpisode': 149, 'rewardPerEpisode': 128.15308041452644
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=52.85
