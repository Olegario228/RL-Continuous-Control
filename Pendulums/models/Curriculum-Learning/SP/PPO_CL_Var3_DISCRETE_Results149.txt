#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 149
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8201616791084557, 'errorList': [], 'lossList': [0.0, -1.4210871738195419, 0.0, 28.444636491537093, 0.0, 0.0, 0.0], 'rewardMean': 0.7847262717038683, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9817592529649
'totalSteps': 3840, 'rewardStep': 0.7653923850432686, 'errorList': [], 'lossList': [0.0, -1.434804788827896, 0.0, 35.22184423208237, 0.0, 0.0, 0.0], 'rewardMean': 0.7782816428170017, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.43908212466643
'totalSteps': 5120, 'rewardStep': 0.7040208628153881, 'errorList': [], 'lossList': [0.0, -1.4309330904483795, 0.0, 26.119540514349936, 0.0, 0.0, 0.0], 'rewardMean': 0.7597164478165983, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.8211163323339
'totalSteps': 6400, 'rewardStep': 0.9060037613770545, 'errorList': [], 'lossList': [0.0, -1.4235157144069672, 0.0, 19.444068129062654, 0.0, 0.0, 0.0], 'rewardMean': 0.7889739105286895, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.771177195542
'totalSteps': 7680, 'rewardStep': 0.8555522224957413, 'errorList': [], 'lossList': [0.0, -1.431319814324379, 0.0, 14.310615525841714, 0.0, 0.0, 0.0], 'rewardMean': 0.8000702958565314, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.3263114753922
'totalSteps': 8960, 'rewardStep': 0.6686171623150308, 'errorList': [], 'lossList': [0.0, -1.427231957912445, 0.0, 9.493658114671707, 0.0, 0.0, 0.0], 'rewardMean': 0.7812912767791742, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.1251569766994
'totalSteps': 10240, 'rewardStep': 0.8670868387886556, 'errorList': [], 'lossList': [0.0, -1.4333680230379104, 0.0, 8.187709158360958, 0.0, 0.0, 0.0], 'rewardMean': 0.7920157220303594, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.794945216494
'totalSteps': 11520, 'rewardStep': 0.4926821554444433, 'errorList': [], 'lossList': [0.0, -1.4093999940156936, 0.0, 877.4374612426758, 0.0, 0.0, 0.0], 'rewardMean': 0.7587564368541465, 'totalEpisodes': 65, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.46054914423339
'totalSteps': 12800, 'rewardStep': 0.8650164273125418, 'errorList': [], 'lossList': [0.0, -1.4088829618692398, 0.0, 641.0801739501953, 0.0, 0.0, 0.0], 'rewardMean': 0.769382435899986, 'totalEpisodes': 117, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.513638899978778
'totalSteps': 14080, 'rewardStep': 0.8248732615795659, 'errorList': [], 'lossList': [0.0, -1.4084715676307678, 0.0, 487.5676498413086, 0.0, 0.0, 0.0], 'rewardMean': 0.7769406756280145, 'totalEpisodes': 171, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.5632867206216603
'totalSteps': 15360, 'rewardStep': 0.5678456329173001, 'errorList': [], 'lossList': [0.0, -1.4061268365383148, 0.0, 223.8136594390869, 0.0, 0.0, 0.0], 'rewardMean': 0.7517090710088989, 'totalEpisodes': 219, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.560686106597863
'totalSteps': 16640, 'rewardStep': 0.9261489740718569, 'errorList': [], 'lossList': [0.0, -1.402672656774521, 0.0, 91.4181714630127, 0.0, 0.0, 0.0], 'rewardMean': 0.7677847299117578, 'totalEpisodes': 263, 'stepsPerEpisode': 30, 'rewardPerEpisode': 19.149345727874174
'totalSteps': 17920, 'rewardStep': 0.9338678418291875, 'errorList': [153.3046402623758, 192.9769900120865, 177.57392235398942, 164.66743983019052, 171.2920639688245, 190.804388441432, 191.93279627012708, 149.8203851098991, 193.03806242879097, 156.37188675755996, 169.36725619598718, 126.79964156978134, 136.5142584399596, 188.39104668305717, 180.91109139969103, 169.46287574640374, 157.40097371635557, 167.49173450738354, 178.3449364941442, 157.4960256917404, 166.07871809633062, 179.7255285677698, 195.91890955032315, 158.46231136153918, 145.74478455021972, 191.90678614235506, 158.55023683976938, 180.93729097914758, 201.87700809854542, 194.91175110219166, 163.16171424739366, 148.66907033744891, 181.9078024814296, 101.25208776000457, 186.53594996131886, 171.25899601321956, 196.94588317087954, 182.3949409690672, 178.6130553150182, 97.69294246669845, 165.35139302083255, 172.71607708705122, 193.15151652270012, 161.22528383775932, 187.03542473209393, 185.84263079854065, 158.6872497848154, 187.7573094348534, 168.72102961269525, 193.64267412430533], 'lossList': [0.0, -1.4027084404230117, 0.0, 56.85274784088135, 0.0, 0.0, 0.0], 'rewardMean': 0.7907694278131376, 'totalEpisodes': 289, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.112131426873194, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.8475197041023044, 'errorList': [], 'lossList': [0.0, -1.4007691645622253, 0.0, 37.15033423423767, 0.0, 0.0, 0.0], 'rewardMean': 0.7849210220856627, 'totalEpisodes': 307, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.616951587545726
'totalSteps': 20480, 'rewardStep': 0.8936277980318312, 'errorList': [], 'lossList': [0.0, -1.394493436217308, 0.0, 28.300734567642213, 0.0, 0.0, 0.0], 'rewardMean': 0.7887285796392718, 'totalEpisodes': 318, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.278805724736785
'totalSteps': 21760, 'rewardStep': 0.5983081495462842, 'errorList': [], 'lossList': [0.0, -1.379086406826973, 0.0, 14.903546411991119, 0.0, 0.0, 0.0], 'rewardMean': 0.7816976783623971, 'totalEpisodes': 325, 'stepsPerEpisode': 74, 'rewardPerEpisode': 62.458142815278144
'totalSteps': 23040, 'rewardStep': 0.12400079732016606, 'errorList': [], 'lossList': [0.0, -1.3609337478876113, 0.0, 32.058795676231384, 0.0, 0.0, 0.0], 'rewardMean': 0.7073890742155482, 'totalEpisodes': 332, 'stepsPerEpisode': 306, 'rewardPerEpisode': 209.94369281643398
'totalSteps': 24320, 'rewardStep': 0.7693884205225386, 'errorList': [], 'lossList': [0.0, -1.3607259076833724, 0.0, 15.490259480476379, 0.0, 0.0, 0.0], 'rewardMean': 0.7350597007233576, 'totalEpisodes': 338, 'stepsPerEpisode': 198, 'rewardPerEpisode': 177.73354976646098
'totalSteps': 25600, 'rewardStep': 0.3654532864558525, 'errorList': [], 'lossList': [0.0, -1.3534507268667222, 0.0, 9.936543543338775, 0.0, 0.0, 0.0], 'rewardMean': 0.6851033866376887, 'totalEpisodes': 341, 'stepsPerEpisode': 299, 'rewardPerEpisode': 223.73609963024438
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=50.76
