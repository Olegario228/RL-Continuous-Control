#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 15
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9360084744127758, 'errorList': [], 'lossList': [0.0, -1.4352493596076965, 0.0, 30.507024736404418, 0.0, 0.0, 0.0], 'rewardMean': 0.9157950328104714, 'totalEpisodes': 8, 'stepsPerEpisode': 529, 'rewardPerEpisode': 377.10779752439424
'totalSteps': 3840, 'rewardStep': 0.7557452886750211, 'errorList': [], 'lossList': [0.0, -1.4207161974906921, 0.0, 32.01956871271133, 0.0, 0.0, 0.0], 'rewardMean': 0.8624451180986545, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 203.96302985448395
'totalSteps': 5120, 'rewardStep': 0.7637633211487607, 'errorList': [], 'lossList': [0.0, -1.4165323793888092, 0.0, 27.78206905066967, 0.0, 0.0, 0.0], 'rewardMean': 0.8377746688611811, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.0989296707711
'totalSteps': 6400, 'rewardStep': 0.9462802441632047, 'errorList': [65.3438532433969, 64.69581438304539, 64.61490714035162, 64.8232400336885, 64.18165086467847, 58.50123220372322, 64.76655632662252, 63.682964229324114, 64.2477701757544, 66.52811507072191, 67.93829464124082, 62.59817356406236, 67.48557021841275, 63.680655654676826, 64.35329755899792, 66.41559641135987, 64.5569841032015, 66.65918070634444, 65.00991654272286, 60.05964012544767, 63.67817647854789, 64.85203465239208, 63.14115431010364, 62.97597452010103, 63.151264632434, 65.73681359756542, 68.41010626059378, 65.78453587809751, 65.27949086035392, 62.8989243835375, 67.37836872898676, 67.00773796455196, 60.644956883418196, 60.750058084506705, 67.83914412032186, 63.83860543823026, 65.97214738081688, 66.32938303921956, 65.70058715791784, 66.12671544858975, 66.10388629841735, 65.81854065201495, 64.23971123475083, 61.966516452157286, 62.50218841886251, 63.23632985875464, 58.343199652226915, 65.26143346596673, 64.86611851249057, 64.56953789166498], 'lossList': [0.0, -1.4027691793441772, 0.0, 309.575341796875, 0.0, 0.0, 0.0], 'rewardMean': 0.8594757839215859, 'totalEpisodes': 78, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.661777773197767, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.6271284617294697, 'errorList': [], 'lossList': [0.0, -1.3980940532684327, 0.0, 156.07796962738038, 0.0, 0.0, 0.0], 'rewardMean': 0.8207512302228999, 'totalEpisodes': 144, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.004839841730538
'totalSteps': 8960, 'rewardStep': 0.6653127361021801, 'errorList': [], 'lossList': [0.0, -1.3892578762769698, 0.0, 72.57749099731446, 0.0, 0.0, 0.0], 'rewardMean': 0.798545731062797, 'totalEpisodes': 198, 'stepsPerEpisode': 46, 'rewardPerEpisode': 39.17863855051125
'totalSteps': 10240, 'rewardStep': 0.746290618104026, 'errorList': [], 'lossList': [0.0, -1.3695859146118163, 0.0, 51.352253952026366, 0.0, 0.0, 0.0], 'rewardMean': 0.7920138419429505, 'totalEpisodes': 237, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.578147540906189
'totalSteps': 11520, 'rewardStep': 0.7002801735528135, 'errorList': [], 'lossList': [0.0, -1.3557390838861465, 0.0, 46.89679794311523, 0.0, 0.0, 0.0], 'rewardMean': 0.7818212121218242, 'totalEpisodes': 258, 'stepsPerEpisode': 67, 'rewardPerEpisode': 59.767763889747535
'totalSteps': 12800, 'rewardStep': 0.8361834398491035, 'errorList': [], 'lossList': [0.0, -1.334750727415085, 0.0, 46.729589986801145, 0.0, 0.0, 0.0], 'rewardMean': 0.7872574348945521, 'totalEpisodes': 271, 'stepsPerEpisode': 121, 'rewardPerEpisode': 98.84224544380092
'totalSteps': 14080, 'rewardStep': 0.6322765976802479, 'errorList': [], 'lossList': [0.0, -1.3220507144927978, 0.0, 30.84715853691101, 0.0, 0.0, 0.0], 'rewardMean': 0.7609269355417603, 'totalEpisodes': 280, 'stepsPerEpisode': 110, 'rewardPerEpisode': 88.34065807319907
'totalSteps': 15360, 'rewardStep': 0.8118005477391222, 'errorList': [], 'lossList': [0.0, -1.3257827389240264, 0.0, 28.145442638397217, 0.0, 0.0, 0.0], 'rewardMean': 0.748506142874395, 'totalEpisodes': 289, 'stepsPerEpisode': 70, 'rewardPerEpisode': 62.710299011408175
'totalSteps': 16640, 'rewardStep': 0.5772221744293902, 'errorList': [], 'lossList': [0.0, -1.3183266371488571, 0.0, 13.501631453037263, 0.0, 0.0, 0.0], 'rewardMean': 0.7306538314498319, 'totalEpisodes': 294, 'stepsPerEpisode': 201, 'rewardPerEpisode': 156.9697720239458
'totalSteps': 17920, 'rewardStep': 0.8724608250134652, 'errorList': [], 'lossList': [0.0, -1.2906456089019775, 0.0, 8.23204914689064, 0.0, 0.0, 0.0], 'rewardMean': 0.7415235818363023, 'totalEpisodes': 300, 'stepsPerEpisode': 155, 'rewardPerEpisode': 121.82247634405824
'totalSteps': 19200, 'rewardStep': 0.888494749372728, 'errorList': [], 'lossList': [0.0, -1.2667817264795302, 0.0, 14.435442736148834, 0.0, 0.0, 0.0], 'rewardMean': 0.7357450323572546, 'totalEpisodes': 305, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.129458663340056
'totalSteps': 20480, 'rewardStep': 0.7601033954088044, 'errorList': [], 'lossList': [0.0, -1.2558920633792878, 0.0, 8.580804970264435, 0.0, 0.0, 0.0], 'rewardMean': 0.7490425257251881, 'totalEpisodes': 308, 'stepsPerEpisode': 41, 'rewardPerEpisode': 29.497118768008892
'totalSteps': 21760, 'rewardStep': 0.9451334304549521, 'errorList': [3.4143453004120174, 0.27864933106877904, 0.3856897902477696, 0.11879581403470217, 0.5949115489535544, 0.2147830837373043, 0.48502619681181103, 1.0951500034021848, 0.19689726708607633, 0.4310353269451178, 0.3670108851999425, 1.1020319842966952, 0.19475484931726403, 0.2701935700462583, 0.3216454764433893, 0.4643287187614044, 2.398992615326755, 0.1488752087732574, 0.08955593940889912, 0.36126904455928216, 0.6813693588896813, 0.0907727213900011, 0.948519139467501, 0.6908283726790231, 0.4656680487035267, 1.320129260083646, 0.2822973540701533, 0.4128919055174897, 0.821296083570622, 0.4780621418205626, 0.6502598460119869, 0.1686018306165302, 0.4021678382813796, 0.5800965478783886, 0.16260416810352185, 0.25999434791454384, 0.16865748557558716, 0.5604908118934042, 0.13761370583437427, 0.3088854487618705, 0.7783954078109409, 0.0807289734966139, 0.1781086827790385, 0.19732187910788396, 0.6835692715991044, 0.184518574185669, 0.24519989335348605, 0.3372956825224104, 0.37224897123635803, 0.17793005170359477], 'lossList': [0.0, -1.234749608039856, 0.0, 6.22158548951149, 0.0, 0.0, 0.0], 'rewardMean': 0.7770245951604653, 'totalEpisodes': 311, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.83491837026083, 'successfulTests': 15
'totalSteps': 23040, 'rewardStep': 0.672643866946516, 'errorList': [], 'lossList': [0.0, -1.2123443365097046, 0.0, 3.514025269150734, 0.0, 0.0, 0.0], 'rewardMean': 0.7696599200447143, 'totalEpisodes': 312, 'stepsPerEpisode': 635, 'rewardPerEpisode': 547.5861249926763
'totalSteps': 24320, 'rewardStep': 0.8348905330807271, 'errorList': [], 'lossList': [0.0, -1.1911755782365798, 0.0, 2.566365412175655, 0.0, 0.0, 0.0], 'rewardMean': 0.7831209559975056, 'totalEpisodes': 313, 'stepsPerEpisode': 327, 'rewardPerEpisode': 287.65334746508734
'totalSteps': 25600, 'rewardStep': 0.8865667088354346, 'errorList': [], 'lossList': [0.0, -1.1649423778057098, 0.0, 5.857836357653141, 0.0, 0.0, 0.0], 'rewardMean': 0.7881592828961388, 'totalEpisodes': 315, 'stepsPerEpisode': 262, 'rewardPerEpisode': 240.28706146898259
#maxSuccessfulTests=15, maxSuccessfulTestsAtStep=21760, timeSpent=101.51
