#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 16
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8584804343243699, 'errorList': [], 'lossList': [0.0, -1.444619840979576, 0.0, 36.9149765253067, 0.0, 0.0, 0.0], 'rewardMean': 0.7085640764003625, 'totalEpisodes': 12, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.028341765022766
'totalSteps': 3840, 'rewardStep': 0.8945312046550165, 'errorList': [], 'lossList': [0.0, -1.4570085686445235, 0.0, 30.45998518407345, 0.0, 0.0, 0.0], 'rewardMean': 0.7705531191519137, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.9854716647518
'totalSteps': 5120, 'rewardStep': 0.6426887006587513, 'errorList': [], 'lossList': [0.0, -1.4410074639320374, 0.0, 29.687185053825377, 0.0, 0.0, 0.0], 'rewardMean': 0.7385870145286231, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.2280427395226
'totalSteps': 6400, 'rewardStep': 0.4488122351192434, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6419954213921631, 'totalEpisodes': 67, 'stepsPerEpisode': 23, 'rewardPerEpisode': 15.48352771356311
'totalSteps': 7680, 'rewardStep': 0.8552313457382706, 'errorList': [], 'lossList': [0.0, -1.4445199930667878, 0.0, 326.87759323120116, 0.0, 0.0, 0.0], 'rewardMean': 0.67245769629875, 'totalEpisodes': 128, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.804706158321048
'totalSteps': 8960, 'rewardStep': 0.5602754652263396, 'errorList': [], 'lossList': [0.0, -1.439169136285782, 0.0, 149.56403282165527, 0.0, 0.0, 0.0], 'rewardMean': 0.6584349174146987, 'totalEpisodes': 173, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.5577486779833474
'totalSteps': 10240, 'rewardStep': 0.48387602660960904, 'errorList': [], 'lossList': [0.0, -1.4306803029775619, 0.0, 70.41832729339599, 0.0, 0.0, 0.0], 'rewardMean': 0.6390394851030221, 'totalEpisodes': 217, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.001517273153835
'totalSteps': 11520, 'rewardStep': 0.9463355757949554, 'errorList': [171.68097142767044, 170.7327292389605, 182.2632454061747, 30.24290049670028, 180.4998694455341, 178.2443054416734, 180.1799315854374, 141.31565727949646, 114.26679073637315, 186.51154397947786, 182.5514495837476, 189.24561710462902, 169.9872139077712, 178.5146571540623, 149.16878360344944, 184.12778464321516, 165.8527307672351, 175.6222900228158, 187.38542793320926, 191.05583515585195, 178.6497572936921, 124.70782615828438, 197.12193877578906, 170.80741943096888, 189.63162601420447, 192.7539378289636, 179.27758415302938, 177.5370730736395, 195.9069972858128, 1.2549593121964007, 169.57934977697278, 186.14989327367144, 183.3961324461661, 119.99930228704781, 183.12125294560698, 175.43152205972007, 144.7637583699495, 158.52413620429928, 172.3468855343937, 190.3585115534832, 179.1565221344326, 145.67067843055833, 159.4453547843927, 190.11678693861475, 184.01533361989482, 175.1793903887892, 191.4758827304825, 180.34222898807707, 173.03620122080724, 184.92796438286706], 'lossList': [0.0, -1.4231981992721559, 0.0, 31.533704137802125, 0.0, 0.0, 0.0], 'rewardMean': 0.6697690941722154, 'totalEpisodes': 241, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.5230896167220376, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.47491720354014777, 'errorList': [], 'lossList': [0.0, -1.413038123846054, 0.0, 28.601951274871826, 0.0, 0.0, 0.0], 'rewardMean': 0.6613960426785946, 'totalEpisodes': 254, 'stepsPerEpisode': 89, 'rewardPerEpisode': 65.96329004638038
'totalSteps': 14080, 'rewardStep': 0.8278802235787268, 'errorList': [], 'lossList': [0.0, -1.40625825881958, 0.0, 12.933381843566895, 0.0, 0.0, 0.0], 'rewardMean': 0.6583360216040304, 'totalEpisodes': 265, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.433108531848516
'totalSteps': 15360, 'rewardStep': 0.4025018516252538, 'errorList': [], 'lossList': [0.0, -1.404039655327797, 0.0, 10.318975896835328, 0.0, 0.0, 0.0], 'rewardMean': 0.609133086301054, 'totalEpisodes': 272, 'stepsPerEpisode': 239, 'rewardPerEpisode': 187.46243012801818
'totalSteps': 16640, 'rewardStep': 0.8724297770841217, 'errorList': [], 'lossList': [0.0, -1.3818103134632111, 0.0, 23.430882272720336, 0.0, 0.0, 0.0], 'rewardMean': 0.6321071939435912, 'totalEpisodes': 280, 'stepsPerEpisode': 102, 'rewardPerEpisode': 82.41955589078809
'totalSteps': 17920, 'rewardStep': 0.794793208682411, 'errorList': [], 'lossList': [0.0, -1.3598904019594193, 0.0, 28.60192910194397, 0.0, 0.0, 0.0], 'rewardMean': 0.666705291299908, 'totalEpisodes': 287, 'stepsPerEpisode': 85, 'rewardPerEpisode': 70.84745771341254
'totalSteps': 19200, 'rewardStep': 0.9408458092133164, 'errorList': [67.83657759272144, 54.951478444546744, 71.25573347612796, 5.820132521758699, 25.6617992709034, 37.883104157424114, 6.887382105464066, 6.40851702253912, 2.3558753473989835, 90.12832340230266, 34.4573434611947, 97.17591578156978, 13.011138679160362, 1.1493378497521327, 52.10196369804786, 9.563637721495676, 41.302253295324874, 47.08249894042647, 47.026823183135065, 23.288418136643998, 21.568221399623404, 57.595266023293725, 21.046138276318647, 29.97924803370637, 66.61580012206863, 22.124784785727947, 31.352090835469316, 23.781470753712703, 11.517764352057547, 58.902143582852254, 51.14301632919862, 58.599982585697035, 71.42416930026481, 55.03476721161021, 18.576853479659153, 9.781844290167632, 3.6340723683514478, 2.0680513424723905, 30.829362361661826, 2.7216253126667094, 82.4576780927649, 41.55095214369554, 14.971507046501056, 78.44244037080446, 1.192118237100261, 6.1389576380701625, 25.423810840965107, 65.06629636279466, 7.764295793057229, 61.835847700591515], 'lossList': [0.0, -1.3402257198095322, 0.0, 8.351062068939209, 0.0, 0.0, 0.0], 'rewardMean': 0.7159086487093153, 'totalEpisodes': 292, 'stepsPerEpisode': 79, 'rewardPerEpisode': 70.08873525078275, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.26084063392352674, 'errorList': [], 'lossList': [0.0, -1.3170705670118332, 0.0, 11.461945362091065, 0.0, 0.0, 0.0], 'rewardMean': 0.6564695775278409, 'totalEpisodes': 297, 'stepsPerEpisode': 220, 'rewardPerEpisode': 177.400609292934
'totalSteps': 21760, 'rewardStep': 0.8037420157502171, 'errorList': [], 'lossList': [0.0, -1.3297321754693985, 0.0, 4.915579214096069, 0.0, 0.0, 0.0], 'rewardMean': 0.6808162325802286, 'totalEpisodes': 302, 'stepsPerEpisode': 42, 'rewardPerEpisode': 34.73155881903522
'totalSteps': 23040, 'rewardStep': 0.9329807998067812, 'errorList': [0.08298349002197429, 5.521316123011978, 0.6499287975099913, 1.436237190862126, 0.9859010019230626, 0.5331733789273787, 0.24183693107334034, 2.386910869115276, 0.09772175610171328, 1.3739731236389887, 1.3503749774640077, 3.2004811089655143, 1.2469977973606374, 0.10015309208846061, 0.2620651522871788, 5.704467079321005, 0.11367845775943515, 0.2882701514219609, 1.0109263540993916, 0.49756944771590506, 11.67540422575407, 3.067185632007742, 16.869933973713945, 3.8175835452349496, 4.813537761518762, 1.1696782650062867, 1.129070875063364, 1.672402189343298, 0.8158688560919434, 0.6329586811362773, 1.8773341465926539, 8.64508987909421, 5.504982950527239, 2.9075026827503843, 1.4173550848980911, 1.314983318273973, 5.167929635396263, 14.11623579087734, 1.6615408618019152, 0.9369993056379299, 2.434875933518491, 2.4450400979387115, 4.0235702700035665, 7.598500928138189, 3.2947966769831734, 19.552808399568704, 0.44363717671113795, 6.291536107462178, 4.3493123544533825, 1.6770348329794211], 'lossList': [0.0, -1.3454882192611695, 0.0, 2.971091493368149, 0.0, 0.0, 0.0], 'rewardMean': 0.7257267098999458, 'totalEpisodes': 306, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.80723470744802, 'successfulTests': 4
'totalSteps': 24320, 'rewardStep': 0.7816479530992384, 'errorList': [], 'lossList': [0.0, -1.351518436074257, 0.0, 2.7064748442173006, 0.0, 0.0, 0.0], 'rewardMean': 0.7092579476303741, 'totalEpisodes': 308, 'stepsPerEpisode': 407, 'rewardPerEpisode': 341.4667045049199
'totalSteps': 25600, 'rewardStep': 0.4922795966105502, 'errorList': [], 'lossList': [0.0, -1.3383236104249954, 0.0, 3.2032013434171676, 0.0, 0.0, 0.0], 'rewardMean': 0.7109941869374143, 'totalEpisodes': 309, 'stepsPerEpisode': 931, 'rewardPerEpisode': 696.6979343321359
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=23040, timeSpent=122.26
