#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 17
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.855185987524522, 'errorList': [], 'lossList': [0.0, -1.4427587568759919, 0.0, 27.66762019395828, 0.0, 0.0, 0.0], 'rewardMean': 0.66648588137366, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 654.8354046413383
'totalSteps': 3840, 'rewardStep': 0.84452893941359, 'errorList': [], 'lossList': [0.0, -1.4598970460891723, 0.0, 43.213913540840146, 0.0, 0.0, 0.0], 'rewardMean': 0.72583356738697, 'totalEpisodes': 12, 'stepsPerEpisode': 665, 'rewardPerEpisode': 518.9578920646809
'totalSteps': 5120, 'rewardStep': 0.934200902645594, 'errorList': [72.9001773855076, 71.21436538449196, 72.91474520961883, 73.99485018488956, 74.02098006518337, 71.88163847911787, 69.58353191915138, 69.73741804295045, 71.13227689805389, 71.28440078999274, 72.8426147360648, 70.22386688141573, 72.93948582485334, 73.50361649639235, 72.39560930220651, 69.57602596890752, 68.54154063084108, 74.21831415489413, 73.03399021817297, 72.88652110387204, 73.064290761889, 67.21626160747502, 72.63980058096524, 72.98726800275529, 70.68909499184612, 63.571127949469506, 70.93891827373106, 72.7097703052703, 71.31794984546329, 74.20171289136672, 72.76038084441589, 74.39366450724219, 73.0107366455576, 70.32857767151619, 71.2223116971973, 73.88852947207526, 71.78874135038208, 72.91252988060107, 70.30677596747213, 72.37196914774073, 67.99339836978403, 71.21258654818764, 72.99428612752394, 71.40203684917482, 65.43281609447922, 71.07229785790744, 74.0390050167413, 70.6828706599926, 70.69949687552284, 69.08875013991342], 'lossList': [0.0, -1.4645401257276536, 0.0, 28.880606985092165, 0.0, 0.0, 0.0], 'rewardMean': 0.777925401201626, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.8862821958191, 'successfulTests': 0
'totalSteps': 6400, 'rewardStep': 0.6288233416407759, 'errorList': [], 'lossList': [0.0, -1.4530562788248063, 0.0, 301.7651863861084, 0.0, 0.0, 0.0], 'rewardMean': 0.748104989289456, 'totalEpisodes': 79, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2191456482924703
'totalSteps': 7680, 'rewardStep': 0.6957612268572319, 'errorList': [], 'lossList': [0.0, -1.451961217522621, 0.0, 165.6520993423462, 0.0, 0.0, 0.0], 'rewardMean': 0.7393810288840853, 'totalEpisodes': 143, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.348542306187253
'totalSteps': 8960, 'rewardStep': 0.8479265475887774, 'errorList': [], 'lossList': [0.0, -1.4520154571533204, 0.0, 78.16812538146972, 0.0, 0.0, 0.0], 'rewardMean': 0.7548875315561842, 'totalEpisodes': 192, 'stepsPerEpisode': 40, 'rewardPerEpisode': 26.717845124471076
'totalSteps': 10240, 'rewardStep': 0.6850743921924238, 'errorList': [], 'lossList': [0.0, -1.4567966532707215, 0.0, 67.02872064590454, 0.0, 0.0, 0.0], 'rewardMean': 0.7461608891357141, 'totalEpisodes': 226, 'stepsPerEpisode': 56, 'rewardPerEpisode': 48.75633612488413
'totalSteps': 11520, 'rewardStep': 0.8192243282586101, 'errorList': [], 'lossList': [0.0, -1.4537998408079147, 0.0, 59.656792850494384, 0.0, 0.0, 0.0], 'rewardMean': 0.754279049038258, 'totalEpisodes': 245, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.418764091550383
'totalSteps': 12800, 'rewardStep': 0.28607539838198576, 'errorList': [], 'lossList': [0.0, -1.4510203129053116, 0.0, 60.227938108444214, 0.0, 0.0, 0.0], 'rewardMean': 0.7074586839726308, 'totalEpisodes': 257, 'stepsPerEpisode': 144, 'rewardPerEpisode': 96.35075794022926
'totalSteps': 14080, 'rewardStep': 0.6997015678897569, 'errorList': [], 'lossList': [0.0, -1.4414457476139069, 0.0, 58.45663354873657, 0.0, 0.0, 0.0], 'rewardMean': 0.7296502632393267, 'totalEpisodes': 264, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.371735236031775
'totalSteps': 15360, 'rewardStep': 0.7007376851315307, 'errorList': [], 'lossList': [0.0, -1.425652801990509, 0.0, 59.64152606010437, 0.0, 0.0, 0.0], 'rewardMean': 0.7142054330000276, 'totalEpisodes': 273, 'stepsPerEpisode': 37, 'rewardPerEpisode': 25.758800268397515
'totalSteps': 16640, 'rewardStep': 0.8139392270740641, 'errorList': [], 'lossList': [0.0, -1.416089230775833, 0.0, 75.77623381614686, 0.0, 0.0, 0.0], 'rewardMean': 0.711146461766075, 'totalEpisodes': 281, 'stepsPerEpisode': 122, 'rewardPerEpisode': 93.73939040031696
'totalSteps': 17920, 'rewardStep': 0.5165486099018675, 'errorList': [], 'lossList': [0.0, -1.4191090649366378, 0.0, 34.74430716753006, 0.0, 0.0, 0.0], 'rewardMean': 0.6693812324917024, 'totalEpisodes': 285, 'stepsPerEpisode': 313, 'rewardPerEpisode': 249.7888989870222
'totalSteps': 19200, 'rewardStep': 0.8774292127678911, 'errorList': [], 'lossList': [0.0, -1.411955973505974, 0.0, 34.01975971221924, 0.0, 0.0, 0.0], 'rewardMean': 0.6942418196044139, 'totalEpisodes': 290, 'stepsPerEpisode': 169, 'rewardPerEpisode': 145.20648140184073
'totalSteps': 20480, 'rewardStep': 0.9172710555120134, 'errorList': [], 'lossList': [0.0, -1.4081774526834487, 0.0, 53.756800699234006, 0.0, 0.0, 0.0], 'rewardMean': 0.716392802469892, 'totalEpisodes': 293, 'stepsPerEpisode': 65, 'rewardPerEpisode': 55.769833436110595
'totalSteps': 21760, 'rewardStep': 0.8811713370057999, 'errorList': [], 'lossList': [0.0, -1.391931530237198, 0.0, 66.04971918582916, 0.0, 0.0, 0.0], 'rewardMean': 0.7197172814115943, 'totalEpisodes': 297, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.393648520067506
'totalSteps': 23040, 'rewardStep': 0.7479482179791049, 'errorList': [], 'lossList': [0.0, -1.3782840776443481, 0.0, 64.72680236816406, 0.0, 0.0, 0.0], 'rewardMean': 0.7260046639902623, 'totalEpisodes': 301, 'stepsPerEpisode': 124, 'rewardPerEpisode': 105.4161116336774
'totalSteps': 24320, 'rewardStep': 0.9189943455615481, 'errorList': [], 'lossList': [0.0, -1.372120844721794, 0.0, 42.586424968242646, 0.0, 0.0, 0.0], 'rewardMean': 0.7359816657205562, 'totalEpisodes': 305, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.631847496278564
'totalSteps': 25600, 'rewardStep': 0.382211501205816, 'errorList': [], 'lossList': [0.0, -1.3662705570459366, 0.0, 17.469015846252443, 0.0, 0.0, 0.0], 'rewardMean': 0.7455952760029392, 'totalEpisodes': 308, 'stepsPerEpisode': 174, 'rewardPerEpisode': 107.03337059325695
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.21
