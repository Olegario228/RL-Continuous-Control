#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 18
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5846711921719314, 'errorList': [], 'lossList': [0.0, -1.421724066734314, 0.0, 30.98340278506279, 0.0, 0.0, 0.0], 'rewardMean': 0.7214416503637355, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.439177292370605
'totalSteps': 3840, 'rewardStep': 0.9713347298877761, 'errorList': [], 'lossList': [0.0, -1.421346955895424, 0.0, 32.09056904911995, 0.0, 0.0, 0.0], 'rewardMean': 0.8047393435384157, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 391.86324735082087
'totalSteps': 5120, 'rewardStep': 0.836866084518641, 'errorList': [], 'lossList': [0.0, -1.4178673738241196, 0.0, 31.220293309092522, 0.0, 0.0, 0.0], 'rewardMean': 0.8127710287834721, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.6268705646223
'totalSteps': 6400, 'rewardStep': 0.6000701491445795, 'errorList': [], 'lossList': [0.0, -1.40405417740345, 0.0, 347.6727299499512, 0.0, 0.0, 0.0], 'rewardMean': 0.7702308528556936, 'totalEpisodes': 85, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.820738073963986
'totalSteps': 7680, 'rewardStep': 0.87295903580672, 'errorList': [], 'lossList': [0.0, -1.3969401133060455, 0.0, 160.5158821105957, 0.0, 0.0, 0.0], 'rewardMean': 0.7873522166808646, 'totalEpisodes': 142, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.498217226026517
'totalSteps': 8960, 'rewardStep': 0.863856543255692, 'errorList': [], 'lossList': [0.0, -1.390033597946167, 0.0, 74.77190933227538, 0.0, 0.0, 0.0], 'rewardMean': 0.7982814061915543, 'totalEpisodes': 187, 'stepsPerEpisode': 38, 'rewardPerEpisode': 32.140235400119074
'totalSteps': 10240, 'rewardStep': 0.8484873178397742, 'errorList': [], 'lossList': [0.0, -1.387698893547058, 0.0, 64.91965921401977, 0.0, 0.0, 0.0], 'rewardMean': 0.8045571451475817, 'totalEpisodes': 215, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.471164234878938
'totalSteps': 11520, 'rewardStep': 0.5496921825263107, 'errorList': [], 'lossList': [0.0, -1.3910238254070282, 0.0, 44.135935430526736, 0.0, 0.0, 0.0], 'rewardMean': 0.7762388159674405, 'totalEpisodes': 238, 'stepsPerEpisode': 13, 'rewardPerEpisode': 7.052551730904174
'totalSteps': 12800, 'rewardStep': 0.73249799512257, 'errorList': [], 'lossList': [0.0, -1.3981410765647888, 0.0, 26.480480608940123, 0.0, 0.0, 0.0], 'rewardMean': 0.7718647338829534, 'totalEpisodes': 253, 'stepsPerEpisode': 51, 'rewardPerEpisode': 44.88741642847794
'totalSteps': 14080, 'rewardStep': 0.8580950990681403, 'errorList': [], 'lossList': [0.0, -1.3848627978563308, 0.0, 14.023605120182037, 0.0, 0.0, 0.0], 'rewardMean': 0.7718530329342135, 'totalEpisodes': 261, 'stepsPerEpisode': 125, 'rewardPerEpisode': 103.54174541429806
'totalSteps': 15360, 'rewardStep': 0.7715824506474876, 'errorList': [], 'lossList': [0.0, -1.3656610560417175, 0.0, 20.67057721376419, 0.0, 0.0, 0.0], 'rewardMean': 0.7905441587817692, 'totalEpisodes': 270, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.048414593232295
'totalSteps': 16640, 'rewardStep': 0.8679986276233586, 'errorList': [], 'lossList': [0.0, -1.3625190794467925, 0.0, 8.566145598888397, 0.0, 0.0, 0.0], 'rewardMean': 0.7802105485553275, 'totalEpisodes': 277, 'stepsPerEpisode': 154, 'rewardPerEpisode': 133.44666849586181
'totalSteps': 17920, 'rewardStep': 0.35423758083328394, 'errorList': [], 'lossList': [0.0, -1.3656030333042144, 0.0, 21.844853439331054, 0.0, 0.0, 0.0], 'rewardMean': 0.7319476981867916, 'totalEpisodes': 281, 'stepsPerEpisode': 207, 'rewardPerEpisode': 149.93874924581365
'totalSteps': 19200, 'rewardStep': 0.6282960449809243, 'errorList': [], 'lossList': [0.0, -1.35797458589077, 0.0, 6.68655389547348, 0.0, 0.0, 0.0], 'rewardMean': 0.7347702877704262, 'totalEpisodes': 285, 'stepsPerEpisode': 227, 'rewardPerEpisode': 188.5742056271911
'totalSteps': 20480, 'rewardStep': 0.5310176609535129, 'errorList': [], 'lossList': [0.0, -1.3369721919298172, 0.0, 4.914091308712959, 0.0, 0.0, 0.0], 'rewardMean': 0.7005761502851054, 'totalEpisodes': 287, 'stepsPerEpisode': 401, 'rewardPerEpisode': 312.8432299099807
'totalSteps': 21760, 'rewardStep': 0.90679081743442, 'errorList': [], 'lossList': [0.0, -1.2969604766368865, 0.0, 4.424952999651432, 0.0, 0.0, 0.0], 'rewardMean': 0.7048695777029781, 'totalEpisodes': 288, 'stepsPerEpisode': 1156, 'rewardPerEpisode': 999.2763873286018
'totalSteps': 23040, 'rewardStep': 0.8721454947785865, 'errorList': [], 'lossList': [0.0, -1.2348344278335572, 0.0, 3.450486724972725, 0.0, 0.0, 0.0], 'rewardMean': 0.7072353953968595, 'totalEpisodes': 288, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.4365857637183
'totalSteps': 24320, 'rewardStep': 0.6650996253602453, 'errorList': [], 'lossList': [0.0, -1.1985305780172348, 0.0, 3.246628702580929, 0.0, 0.0, 0.0], 'rewardMean': 0.718776139680253, 'totalEpisodes': 289, 'stepsPerEpisode': 535, 'rewardPerEpisode': 465.55829791319707
'totalSteps': 25600, 'rewardStep': 0.9502008243756391, 'errorList': [0.010702482930863398, 0.011784251582444895, 0.010848481901623385, 0.012127566740609962, 0.01170505321092852, 0.013965500498447725, 0.01314552855165606, 0.010015453405965134, 0.012503313208976085, 0.015227769691609194, 0.013046487369534326, 0.05383230356480704, 0.012702631219274231, 0.018920619864578027, 0.010619347401031631, 0.009148209816801447, 0.01305247240843511, 0.017396667717115163, 0.012025682309183872, 0.032806599348601105, 0.008638109103867472, 0.008608307303275657, 0.010802038785092704, 0.01720279822177143, 0.008614691250269275, 0.013488321036173642, 0.01109859704955882, 0.011847286711299261, 0.01762520314631481, 0.008540862267606966, 0.01158922416648576, 0.015622580053015223, 0.011554728727076809, 0.015855759586380972, 0.01557985698742466, 0.016378215188071338, 0.01026756452565363, 0.014350447868776196, 0.008809560944966191, 0.011451953222432462, 0.013655001902360745, 0.010356702449368697, 0.012463058889870204, 0.017387758005289237, 0.019292489857643286, 0.016364338629136093, 0.011431301344400945, 0.012944537885842732, 0.01302240486385628, 0.013058780508505296], 'lossList': [0.0, -1.1947590458393096, 0.0, 2.0723328188806773, 0.0, 0.0, 0.0], 'rewardMean': 0.7405464226055598, 'totalEpisodes': 290, 'stepsPerEpisode': 974, 'rewardPerEpisode': 881.3534617367114, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=76.77
