#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 19
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8807831400329279, 'errorList': [], 'lossList': [0.0, -1.4239101493358612, 0.0, 34.1101538413763, 0.0, 0.0, 0.0], 'rewardMean': 0.8020670319561144, 'totalEpisodes': 13, 'stepsPerEpisode': 311, 'rewardPerEpisode': 259.5193165874563
'totalSteps': 3840, 'rewardStep': 0.7149764825536262, 'errorList': [], 'lossList': [0.0, -1.4262116760015489, 0.0, 33.58108193159104, 0.0, 0.0, 0.0], 'rewardMean': 0.7730368488219517, 'totalEpisodes': 16, 'stepsPerEpisode': 174, 'rewardPerEpisode': 131.53359940844769
'totalSteps': 5120, 'rewardStep': 0.7105539823194872, 'errorList': [], 'lossList': [0.0, -1.4302703541517259, 0.0, 20.59622856259346, 0.0, 0.0, 0.0], 'rewardMean': 0.7574161321963355, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 944.9368777886872
'totalSteps': 6400, 'rewardStep': 0.5783206043666256, 'errorList': [], 'lossList': [0.0, -1.4037969559431076, 0.0, 370.936923828125, 0.0, 0.0, 0.0], 'rewardMean': 0.7215970266303936, 'totalEpisodes': 105, 'stepsPerEpisode': 37, 'rewardPerEpisode': 30.993220288823373
'totalSteps': 7680, 'rewardStep': 0.7429700650917742, 'errorList': [], 'lossList': [0.0, -1.4046998244524003, 0.0, 156.50858322143554, 0.0, 0.0, 0.0], 'rewardMean': 0.7251591997072904, 'totalEpisodes': 171, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.35356696456741
'totalSteps': 8960, 'rewardStep': 0.9474007053142643, 'errorList': [199.1981890033784, 191.24982881836294, 192.2270278689526, 189.63093807192502, 201.89324682472233, 170.06781247660913, 185.96071969532383, 189.64200518693517, 190.6947500712301, 189.22852503827534, 157.51716690812137, 202.0965437036143, 199.89713374114976, 150.00710472830937, 167.10751632807785, 188.30076452805508, 197.21412849945452, 190.2914467921489, 160.89535409558488, 183.242988785941, 162.61863965127327, 197.4553850586656, 194.73424925416228, 195.23177723630116, 189.40046323924702, 182.57989310263596, 190.6490601965324, 199.97809470220233, 191.7315301663239, 196.66975014261556, 181.72633776060553, 193.95212974040564, 188.30416538007344, 196.23302305059832, 200.836569954616, 182.22169559477302, 183.30851719205074, 187.12454970042086, 175.38806102308845, 184.9787222081514, 184.9555117931662, 189.47724678105067, 178.79056775252903, 154.60193835067878, 180.86843909521497, 197.8942214238682, 199.08935118631757, 188.39406926404098, 192.03666814233276, 162.169502518047], 'lossList': [0.0, -1.4005040043592454, 0.0, 53.254807205200194, 0.0, 0.0, 0.0], 'rewardMean': 0.7569079862225723, 'totalEpisodes': 230, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.9162918497997, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.9050483466742112, 'errorList': [], 'lossList': [0.0, -1.3849857652187347, 0.0, 43.23810214042663, 0.0, 0.0, 0.0], 'rewardMean': 0.7754255312790271, 'totalEpisodes': 282, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.87574201864462
'totalSteps': 11520, 'rewardStep': 0.6408106667336473, 'errorList': [], 'lossList': [0.0, -1.36575112760067, 0.0, 47.10116374015808, 0.0, 0.0, 0.0], 'rewardMean': 0.7604683241073182, 'totalEpisodes': 307, 'stepsPerEpisode': 84, 'rewardPerEpisode': 62.72074580302922
'totalSteps': 12800, 'rewardStep': 0.4884856625814666, 'errorList': [], 'lossList': [0.0, -1.3570645439624787, 0.0, 38.50738682746887, 0.0, 0.0, 0.0], 'rewardMean': 0.7332700579547331, 'totalEpisodes': 318, 'stepsPerEpisode': 66, 'rewardPerEpisode': 41.39331855401378
'totalSteps': 14080, 'rewardStep': 0.8730389123622632, 'errorList': [], 'lossList': [0.0, -1.3510108667612075, 0.0, 27.532348883152007, 0.0, 0.0, 0.0], 'rewardMean': 0.7482388568030294, 'totalEpisodes': 323, 'stepsPerEpisode': 360, 'rewardPerEpisode': 290.3567840774251
'totalSteps': 15360, 'rewardStep': 0.6949953724506659, 'errorList': [], 'lossList': [0.0, -1.3408784258365631, 0.0, 52.71697976112366, 0.0, 0.0, 0.0], 'rewardMean': 0.7296600800448033, 'totalEpisodes': 330, 'stepsPerEpisode': 424, 'rewardPerEpisode': 344.0935352283876
'totalSteps': 16640, 'rewardStep': 0.47295192390857027, 'errorList': [], 'lossList': [0.0, -1.336683811545372, 0.0, 31.357076559066773, 0.0, 0.0, 0.0], 'rewardMean': 0.7054576241802976, 'totalEpisodes': 336, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.132548166715235
'totalSteps': 17920, 'rewardStep': 0.8096263018468403, 'errorList': [], 'lossList': [0.0, -1.330591311454773, 0.0, 12.582990657091141, 0.0, 0.0, 0.0], 'rewardMean': 0.7153648561330329, 'totalEpisodes': 341, 'stepsPerEpisode': 226, 'rewardPerEpisode': 200.2356150041424
'totalSteps': 19200, 'rewardStep': 0.5947321802735006, 'errorList': [], 'lossList': [0.0, -1.3470001190900802, 0.0, 6.146125797629356, 0.0, 0.0, 0.0], 'rewardMean': 0.7170060137237204, 'totalEpisodes': 344, 'stepsPerEpisode': 492, 'rewardPerEpisode': 367.1040915612124
'totalSteps': 20480, 'rewardStep': 0.933894755088333, 'errorList': [0.2965800938535344, 0.4368241456379854, 0.5116570342392226, 0.30847707825998666, 0.40982558689999105, 0.7385688348746063, 0.6835534628630516, 0.6300393045337408, 0.35290074701146684, 0.5505552227713468, 0.5908296780335456, 0.3658527517729017, 0.44288440559615255, 0.6199621721422418, 0.6249579895807469, 0.5306563038719456, 0.4019679466124344, 0.6905129376906899, 0.625998749988096, 0.4941229432877163, 0.3613158098169696, 0.6176670927503113, 0.49117735411666513, 0.4054264613630373, 0.40025323458940654, 0.6798471305038359, 0.6786718240297598, 0.5346096711725121, 0.6323814647968724, 0.6579683726732275, 0.3005193420106817, 0.47975690281146177, 0.32800116210176583, 0.8002945389704071, 0.4165986148873024, 0.47475443861739525, 0.46384453062170705, 0.8323415552319924, 0.5824587033542178, 0.5427412120584392, 0.6098862910985321, 0.574587301936858, 0.30525272149134874, 0.24935415605984335, 0.6784671921488042, 0.6697307993992384, 0.6951748466343003, 0.3652571657977449, 0.6837230199823077, 0.4415587437188185], 'lossList': [0.0, -1.362850506901741, 0.0, 4.929541473388672, 0.0, 0.0, 0.0], 'rewardMean': 0.7360984827233763, 'totalEpisodes': 348, 'stepsPerEpisode': 72, 'rewardPerEpisode': 61.03132584620894, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.7448764520841187, 'errorList': [], 'lossList': [0.0, -1.378713139295578, 0.0, 4.72912854373455, 0.0, 0.0, 0.0], 'rewardMean': 0.7158460574003616, 'totalEpisodes': 349, 'stepsPerEpisode': 810, 'rewardPerEpisode': 688.3217082190179
'totalSteps': 23040, 'rewardStep': 0.8595742810498859, 'errorList': [], 'lossList': [0.0, -1.3773801523447036, 0.0, 6.563344704508782, 0.0, 0.0, 0.0], 'rewardMean': 0.7112986508379292, 'totalEpisodes': 350, 'stepsPerEpisode': 721, 'rewardPerEpisode': 581.1427589699045
'totalSteps': 24320, 'rewardStep': 0.7754913336838758, 'errorList': [], 'lossList': [0.0, -1.3649641054868697, 0.0, 1.6959449923038483, 0.0, 0.0, 0.0], 'rewardMean': 0.724766717532952, 'totalEpisodes': 350, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.4387014394342
'totalSteps': 25600, 'rewardStep': 0.9214006205445331, 'errorList': [], 'lossList': [0.0, -1.335288873910904, 0.0, 1.6095881870388984, 0.0, 0.0, 0.0], 'rewardMean': 0.7680582133292587, 'totalEpisodes': 350, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.1159529490733
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=98.63
