#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092971427115871, 'errorList': [], 'lossList': [0.0, -1.4231055521965026, 0.0, 34.86121163368225, 0.0, 0.0, 0.0], 'rewardMean': 0.8624672422476483, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.76661789737042
'totalSteps': 3840, 'rewardStep': 0.5562513755896646, 'errorList': [], 'lossList': [0.0, -1.4137311124801635, 0.0, 33.320800943374635, 0.0, 0.0, 0.0], 'rewardMean': 0.7603952866949871, 'totalEpisodes': 82, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.96326734396544
'totalSteps': 5120, 'rewardStep': 0.41657226176954365, 'errorList': [], 'lossList': [0.0, -1.393478239774704, 0.0, 22.643683416843416, 0.0, 0.0, 0.0], 'rewardMean': 0.6744395304636263, 'totalEpisodes': 91, 'stepsPerEpisode': 209, 'rewardPerEpisode': 111.1692412224747
'totalSteps': 6400, 'rewardStep': 0.8315863946173622, 'errorList': [], 'lossList': [0.0, -1.3782419377565385, 0.0, 101.5588293838501, 0.0, 0.0, 0.0], 'rewardMean': 0.7058689032943735, 'totalEpisodes': 135, 'stepsPerEpisode': 26, 'rewardPerEpisode': 18.183182367982713
'totalSteps': 7680, 'rewardStep': 0.5771115899130732, 'errorList': [], 'lossList': [0.0, -1.3699987387657167, 0.0, 57.9579328918457, 0.0, 0.0, 0.0], 'rewardMean': 0.6844093510641568, 'totalEpisodes': 162, 'stepsPerEpisode': 58, 'rewardPerEpisode': 38.70730580820381
'totalSteps': 8960, 'rewardStep': 0.5764086419300175, 'errorList': [], 'lossList': [0.0, -1.353948671221733, 0.0, 52.856606063842776, 0.0, 0.0, 0.0], 'rewardMean': 0.6689806783307083, 'totalEpisodes': 177, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.1105499843258486
'totalSteps': 10240, 'rewardStep': 0.609102376673656, 'errorList': [], 'lossList': [0.0, -1.3342171025276184, 0.0, 30.028029644489287, 0.0, 0.0, 0.0], 'rewardMean': 0.6614958906235767, 'totalEpisodes': 183, 'stepsPerEpisode': 105, 'rewardPerEpisode': 86.056516041938
'totalSteps': 11520, 'rewardStep': 0.7576790615329373, 'errorList': [], 'lossList': [0.0, -1.3290162563323975, 0.0, 21.31360666036606, 0.0, 0.0, 0.0], 'rewardMean': 0.6721829096135057, 'totalEpisodes': 187, 'stepsPerEpisode': 138, 'rewardPerEpisode': 107.88564409420233
'totalSteps': 12800, 'rewardStep': 0.8975314090897298, 'errorList': [], 'lossList': [0.0, -1.3237727600336076, 0.0, 35.49579725265503, 0.0, 0.0, 0.0], 'rewardMean': 0.6947177595611281, 'totalEpisodes': 193, 'stepsPerEpisode': 39, 'rewardPerEpisode': 30.937826567506892
'totalSteps': 14080, 'rewardStep': 0.6690518568344495, 'errorList': [], 'lossList': [0.0, -1.3153587198257446, 0.0, 8.25231622517109, 0.0, 0.0, 0.0], 'rewardMean': 0.680059211066202, 'totalEpisodes': 197, 'stepsPerEpisode': 124, 'rewardPerEpisode': 89.46744186723062
'totalSteps': 15360, 'rewardStep': 0.7774545959832099, 'errorList': [], 'lossList': [0.0, -1.3335393911600113, 0.0, 7.552517021894455, 0.0, 0.0, 0.0], 'rewardMean': 0.6668749563933644, 'totalEpisodes': 198, 'stepsPerEpisode': 115, 'rewardPerEpisode': 92.48033884717344
'totalSteps': 16640, 'rewardStep': 0.8691310731465146, 'errorList': [], 'lossList': [0.0, -1.3307832950353622, 0.0, 5.6191225975751875, 0.0, 0.0, 0.0], 'rewardMean': 0.6981629261490494, 'totalEpisodes': 200, 'stepsPerEpisode': 127, 'rewardPerEpisode': 109.29100929935687
'totalSteps': 17920, 'rewardStep': 0.8230394537803524, 'errorList': [], 'lossList': [0.0, -1.303438469171524, 0.0, 3.9214481073617935, 0.0, 0.0, 0.0], 'rewardMean': 0.7388096453501303, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 958.9399072751361
'totalSteps': 19200, 'rewardStep': 0.8692074067055143, 'errorList': [], 'lossList': [0.0, -1.2558787894248962, 0.0, 2.60274060189724, 0.0, 0.0, 0.0], 'rewardMean': 0.7425717465589454, 'totalEpisodes': 201, 'stepsPerEpisode': 755, 'rewardPerEpisode': 597.4923844558435
'totalSteps': 20480, 'rewardStep': 0.8448210837983278, 'errorList': [], 'lossList': [0.0, -1.2275496369600296, 0.0, 1.8281607469916343, 0.0, 0.0, 0.0], 'rewardMean': 0.769342695947471, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.5583610433187
'totalSteps': 21760, 'rewardStep': 0.8322879130625632, 'errorList': [], 'lossList': [0.0, -1.2026968950033188, 0.0, 1.049813211262226, 0.0, 0.0, 0.0], 'rewardMean': 0.7949306230607255, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.904060556058
'totalSteps': 23040, 'rewardStep': 0.9749182551309035, 'errorList': [0.2706115097546199, 0.29005953625130554, 0.3121997626721764, 0.3255479849197715, 0.2437558901601675, 0.20284243829783796, 0.4202020174942776, 0.2986464118239363, 0.31946782967375575, 0.315186369100372, 0.2348327029571245, 0.3233126302170644, 0.31621009048920024, 0.38930126215540256, 0.2730027658124525, 0.3408190271492551, 0.43351772053038984, 0.17053977099223183, 0.2731984951334574, 0.23646978125030485, 0.2861082159518693, 0.35855952266004637, 0.33717793967853815, 0.23704582893755524, 0.2801487938987293, 0.27578723619332746, 0.37233952506394863, 0.30350543117623713, 0.26221684555535907, 0.20805975666995627, 0.18743188687016488, 0.3998964174823416, 0.24494599268355932, 0.245708909848588, 0.2657979161599246, 0.1850755786913558, 0.3223395030610991, 0.3422665605901307, 0.22936019066568059, 0.3702937897029692, 0.22367056043654218, 0.27984476094455235, 0.4209956592066049, 0.2285345900760706, 0.19257107831217415, 0.404089522884038, 0.37154057749580366, 0.42903483835468637, 0.2828401627099679, 0.35348255666471845], 'lossList': [0.0, -1.1584974789619447, 0.0, 1.355076885074377, 0.0, 0.0, 0.0], 'rewardMean': 0.8315122109064502, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.8913784498825, 'successfulTests': 4
'totalSteps': 24320, 'rewardStep': 0.8254931777067902, 'errorList': [], 'lossList': [0.0, -1.1315290832519531, 0.0, 0.6933192672207952, 0.0, 0.0, 0.0], 'rewardMean': 0.8382936225238355, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.8770722777585
'totalSteps': 25600, 'rewardStep': 0.8578000824381984, 'errorList': [], 'lossList': [0.0, -1.111017199754715, 0.0, 0.3887697460129857, 0.0, 0.0, 0.0], 'rewardMean': 0.8343204898586825, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.8016559152036
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=23040, timeSpent=77.27
