#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 20
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9120293869764686, 'errorList': [], 'lossList': [0.0, -1.4368718886375427, 0.0, 31.965963249802588, 0.0, 0.0, 0.0], 'rewardMean': 0.9134250087786484, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 383.4933527492489
'totalSteps': 3840, 'rewardStep': 0.7201087920380944, 'errorList': [], 'lossList': [0.0, -1.4210810518264771, 0.0, 31.269216170310973, 0.0, 0.0, 0.0], 'rewardMean': 0.8489862698651304, 'totalEpisodes': 12, 'stepsPerEpisode': 268, 'rewardPerEpisode': 211.44214883633225
'totalSteps': 5120, 'rewardStep': 0.7380898310945112, 'errorList': [], 'lossList': [0.0, -1.4190899974107742, 0.0, 27.047922303676604, 0.0, 0.0, 0.0], 'rewardMean': 0.8212621601724757, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.3847194564404
'totalSteps': 6400, 'rewardStep': 0.834113108625508, 'errorList': [], 'lossList': [0.0, -1.4083021706342698, 0.0, 323.42565544128416, 0.0, 0.0, 0.0], 'rewardMean': 0.8238323498630822, 'totalEpisodes': 88, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.674328582040965
'totalSteps': 7680, 'rewardStep': 0.8150840533121307, 'errorList': [], 'lossList': [0.0, -1.4068250250816345, 0.0, 137.6378632736206, 0.0, 0.0, 0.0], 'rewardMean': 0.8223743004379235, 'totalEpisodes': 145, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.221237271581899
'totalSteps': 8960, 'rewardStep': 0.5977787945428733, 'errorList': [], 'lossList': [0.0, -1.398611307144165, 0.0, 56.29373525619507, 0.0, 0.0, 0.0], 'rewardMean': 0.790289228167202, 'totalEpisodes': 211, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.561308043228303
'totalSteps': 10240, 'rewardStep': 0.5571001808652755, 'errorList': [], 'lossList': [0.0, -1.3786470812559128, 0.0, 49.013255748748776, 0.0, 0.0, 0.0], 'rewardMean': 0.7611405972544613, 'totalEpisodes': 248, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.450454403662127
'totalSteps': 11520, 'rewardStep': 0.8110364314717313, 'errorList': [], 'lossList': [0.0, -1.3591614562273024, 0.0, 39.8696280670166, 0.0, 0.0, 0.0], 'rewardMean': 0.766684578834158, 'totalEpisodes': 265, 'stepsPerEpisode': 71, 'rewardPerEpisode': 58.9855404039968
'totalSteps': 12800, 'rewardStep': 0.6215640281617538, 'errorList': [], 'lossList': [0.0, -1.337228302359581, 0.0, 32.733718266487124, 0.0, 0.0, 0.0], 'rewardMean': 0.7521725237669176, 'totalEpisodes': 276, 'stepsPerEpisode': 123, 'rewardPerEpisode': 89.82952764445872
'totalSteps': 14080, 'rewardStep': 0.5931387257041716, 'errorList': [], 'lossList': [0.0, -1.3210855746269226, 0.0, 25.194758343696595, 0.0, 0.0, 0.0], 'rewardMean': 0.7200043332792518, 'totalEpisodes': 285, 'stepsPerEpisode': 216, 'rewardPerEpisode': 150.580671101025
'totalSteps': 15360, 'rewardStep': 0.6865468816907023, 'errorList': [], 'lossList': [0.0, -1.3223721492290497, 0.0, 21.159876873493193, 0.0, 0.0, 0.0], 'rewardMean': 0.6974560827506753, 'totalEpisodes': 291, 'stepsPerEpisode': 69, 'rewardPerEpisode': 60.29798790059509
'totalSteps': 16640, 'rewardStep': 0.8571279954411748, 'errorList': [], 'lossList': [0.0, -1.3147198575735093, 0.0, 10.402382216453553, 0.0, 0.0, 0.0], 'rewardMean': 0.7111580030909833, 'totalEpisodes': 296, 'stepsPerEpisode': 65, 'rewardPerEpisode': 48.02093789395165
'totalSteps': 17920, 'rewardStep': 0.7432989992072032, 'errorList': [], 'lossList': [0.0, -1.3077698892354965, 0.0, 8.14025598526001, 0.0, 0.0, 0.0], 'rewardMean': 0.7116789199022525, 'totalEpisodes': 298, 'stepsPerEpisode': 408, 'rewardPerEpisode': 304.8963287988173
'totalSteps': 19200, 'rewardStep': 0.547102195260191, 'errorList': [], 'lossList': [0.0, -1.2991538965702056, 0.0, 35.91473893404007, 0.0, 0.0, 0.0], 'rewardMean': 0.6829778285657208, 'totalEpisodes': 300, 'stepsPerEpisode': 823, 'rewardPerEpisode': 615.9338450908633
'totalSteps': 20480, 'rewardStep': 0.9461064914786689, 'errorList': [0.09378771943627442, 0.10433474962153409, 0.11069415368594879, 0.09200835150122003, 0.10500695992261369, 0.1146515676015223, 0.10264005359193522, 0.08643522767386559, 0.10200289528256433, 0.1093108765326372, 0.10770436056058824, 0.10281745381489958, 0.09951694626717146, 0.1158908390836722, 0.0899594685164306, 0.10027989382216174, 0.08817618532235252, 0.10191187971602776, 0.10242088874685999, 0.10199505101963628, 0.08809978450971888, 0.09560604574181411, 0.10938496000264351, 0.09867297926191115, 0.10168582649033212, 0.10190858249545758, 0.1102857082941185, 0.10245587797647338, 0.09696575637170757, 0.09498304170003177, 0.09226894209929844, 0.09513721240618095, 0.09404071539526873, 0.10121942415518671, 0.098560096307803, 0.09782127032741826, 0.1051259385133966, 0.10495833863721278, 0.0988331900838286, 0.11307090672422981, 0.09351250234793756, 0.09784466513766431, 0.08940527745733345, 0.10168954790180458, 0.09811521060438069, 0.09237500744025319, 0.10847078119694512, 0.10125540634860036, 0.09288137106603518, 0.11064632178666196], 'lossList': [0.0, -1.3002229225635529, 0.0, 5.003568520843983, 0.0, 0.0, 0.0], 'rewardMean': 0.6960800723823746, 'totalEpisodes': 300, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.0870140386005, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=76.95
