#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 21
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8507820611170008, 'errorList': [], 'lossList': [0.0, -1.4478689223527907, 0.0, 39.36603865623474, 0.0, 0.0, 0.0], 'rewardMean': 0.72197994034858, 'totalEpisodes': 12, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.246092380443315
'totalSteps': 3840, 'rewardStep': 0.912045262930967, 'errorList': [], 'lossList': [0.0, -1.4647571444511414, 0.0, 34.22333289682865, 0.0, 0.0, 0.0], 'rewardMean': 0.7853350478760422, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.6693443357922
'totalSteps': 5120, 'rewardStep': 0.6920257340736807, 'errorList': [], 'lossList': [0.0, -1.4508162778615952, 0.0, 29.470210913419724, 0.0, 0.0, 0.0], 'rewardMean': 0.7620077194254519, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.1034262720764
'totalSteps': 6400, 'rewardStep': 0.8268625767363116, 'errorList': [], 'lossList': [0.0, -1.4537566792964935, 0.0, 345.71231674194337, 0.0, 0.0, 0.0], 'rewardMean': 0.7749786908876238, 'totalEpisodes': 74, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.4997669980696715
'totalSteps': 7680, 'rewardStep': 0.606072179665666, 'errorList': [], 'lossList': [0.0, -1.457118610739708, 0.0, 158.61031425476074, 0.0, 0.0, 0.0], 'rewardMean': 0.7468276056839641, 'totalEpisodes': 121, 'stepsPerEpisode': 42, 'rewardPerEpisode': 34.10184059322036
'totalSteps': 8960, 'rewardStep': 0.6483815776094516, 'errorList': [], 'lossList': [0.0, -1.4688500982522965, 0.0, 99.84870864868164, 0.0, 0.0, 0.0], 'rewardMean': 0.7327638873876052, 'totalEpisodes': 163, 'stepsPerEpisode': 28, 'rewardPerEpisode': 18.738431824314386
'totalSteps': 10240, 'rewardStep': 0.4381336912208277, 'errorList': [], 'lossList': [0.0, -1.4830876916646958, 0.0, 67.90048667907715, 0.0, 0.0, 0.0], 'rewardMean': 0.6959351128667581, 'totalEpisodes': 185, 'stepsPerEpisode': 54, 'rewardPerEpisode': 41.35386360024043
'totalSteps': 11520, 'rewardStep': 0.419148043195962, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.640577698932599, 'totalEpisodes': 204, 'stepsPerEpisode': 17, 'rewardPerEpisode': 9.733477418400458
'totalSteps': 12800, 'rewardStep': 0.6144159660975311, 'errorList': [], 'lossList': [0.0, -1.4795487397909164, 0.0, 57.824691429138184, 0.0, 0.0, 0.0], 'rewardMean': 0.642701513584336, 'totalEpisodes': 225, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6144159660975311
'totalSteps': 14080, 'rewardStep': 0.6355167987428769, 'errorList': [], 'lossList': [0.0, -1.4761807548999786, 0.0, 33.82013339519501, 0.0, 0.0, 0.0], 'rewardMean': 0.6211749873469237, 'totalEpisodes': 234, 'stepsPerEpisode': 94, 'rewardPerEpisode': 76.31278579253008
'totalSteps': 15360, 'rewardStep': 0.7776989033277559, 'errorList': [], 'lossList': [0.0, -1.469934924840927, 0.0, 18.08217094182968, 0.0, 0.0, 0.0], 'rewardMean': 0.6077403513866025, 'totalEpisodes': 241, 'stepsPerEpisode': 70, 'rewardPerEpisode': 56.75940546714402
'totalSteps': 16640, 'rewardStep': 0.5170920632616127, 'errorList': [], 'lossList': [0.0, -1.4483539128303529, 0.0, 14.20206678390503, 0.0, 0.0, 0.0], 'rewardMean': 0.5902469843053957, 'totalEpisodes': 249, 'stepsPerEpisode': 120, 'rewardPerEpisode': 91.19477790901435
'totalSteps': 17920, 'rewardStep': -0.04021233126185153, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4389110424128278, 'totalEpisodes': 256, 'stepsPerEpisode': 162, 'rewardPerEpisode': 107.44800624210846
'totalSteps': 19200, 'rewardStep': 0.9429228670870188, 'errorList': [76.32328606404818, 12.016304287277658, 108.6969844974373, 15.41663890721961, 46.25533110281804, 57.82971796009939, 174.35608174131895, 108.04756336770444, 46.80395061339757, 136.90589257070616, 47.43306251760262, 123.50416774296751, 6.05584727913663, 143.94278424240701, 90.56453676345603, 142.53015072587903, 58.77188069436281, 99.46836768002372, 5.299569028918375, 30.22888138938368, 136.0179396822608, 136.14380214733143, 70.14537600510236, 147.58737151268176, 50.403866908701566, 35.23069489000845, 66.97629657259317, 161.28785806981296, 119.63088477869945, 138.3771545608295, 183.0712457491312, 40.323917231460115, 119.85684934297667, 30.764852759262663, 85.2978276085694, 135.4036424533625, 102.86608877795076, 116.73746333188275, 169.53130497300072, 152.86944695506466, 171.4771073442439, 138.12978736248712, 75.77707916777923, 34.548062942372674, 143.716222461281, 55.1873532157684, 92.70142588710269, 115.48812982982807, 0.5919464715731426, 148.45555823193666], 'lossList': [0.0, -1.4338049346208572, 0.0, 16.160968532562254, 0.0, 0.0, 0.0], 'rewardMean': 0.4683651713605844, 'totalEpisodes': 263, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.984596188589844, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.681540498906269, 'errorList': [], 'lossList': [0.0, -1.4180642402172088, 0.0, 9.186278481483459, 0.0, 0.0, 0.0], 'rewardMean': 0.49270585212912854, 'totalEpisodes': 267, 'stepsPerEpisode': 312, 'rewardPerEpisode': 263.8821056144537
'totalSteps': 21760, 'rewardStep': 0.6803652846047188, 'errorList': [], 'lossList': [0.0, -1.3839961367845535, 0.0, 15.18771372795105, 0.0, 0.0, 0.0], 'rewardMean': 0.5188275762700042, 'totalEpisodes': 273, 'stepsPerEpisode': 139, 'rewardPerEpisode': 116.21087901820947
'totalSteps': 23040, 'rewardStep': 0.7072145612153677, 'errorList': [], 'lossList': [0.0, -1.3864303290843965, 0.0, 5.3672411608695985, 0.0, 0.0, 0.0], 'rewardMean': 0.5476342280719447, 'totalEpisodes': 280, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7072145612153677
'totalSteps': 24320, 'rewardStep': 0.8985003502037062, 'errorList': [], 'lossList': [0.0, -1.4068723732233048, 0.0, 5.13491211771965, 0.0, 0.0, 0.0], 'rewardMean': 0.5760426664825623, 'totalEpisodes': 283, 'stepsPerEpisode': 46, 'rewardPerEpisode': 38.41555983699194
'totalSteps': 25600, 'rewardStep': 0.5477281392067572, 'errorList': [], 'lossList': [0.0, -1.3964028286933898, 0.0, 4.811273279190064, 0.0, 0.0, 0.0], 'rewardMean': 0.5672638005289503, 'totalEpisodes': 284, 'stepsPerEpisode': 452, 'rewardPerEpisode': 360.76216311539395
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.16
