#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 22
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8663769990037149, 'errorList': [], 'lossList': [0.0, -1.4424236595630646, 0.0, 30.24666601061821, 0.0, 0.0, 0.0], 'rewardMean': 0.6847389234538633, 'totalEpisodes': 10, 'stepsPerEpisode': 905, 'rewardPerEpisode': 676.1619782270964
'totalSteps': 3840, 'rewardStep': 0.8318612758400938, 'errorList': [], 'lossList': [0.0, -1.4595861428976058, 0.0, 44.8659469127655, 0.0, 0.0, 0.0], 'rewardMean': 0.7337797075826069, 'totalEpisodes': 12, 'stepsPerEpisode': 668, 'rewardPerEpisode': 531.5237637867058
'totalSteps': 5120, 'rewardStep': 0.9420636962973828, 'errorList': [68.42029834417585, 65.51118116519088, 66.80048528314532, 67.73924973275354, 69.18161412830521, 65.98164029043754, 63.659544000825775, 64.55528107166958, 65.37920851821774, 66.92845291078574, 66.8020856833987, 64.57477513308754, 66.79956254725793, 67.30158389913886, 66.4431571055508, 63.93127335740928, 63.32488960549913, 69.3770777793354, 68.45915010130336, 66.83830811374347, 66.93221593439193, 62.803754227195384, 68.15822738318536, 66.90343028708084, 64.98506452913483, 61.766854693264285, 65.2229502495284, 66.72674330959904, 65.50174728919974, 69.34675959551579, 66.74604512665029, 69.49427158137894, 68.48237156887365, 64.65233443214709, 65.42218286350335, 67.67694812080946, 67.45932805806079, 68.35493505940913, 65.42551288014104, 68.06320546240961, 63.03938613954564, 66.85504295619943, 66.91985829956357, 65.49474559100635, 60.530492513569776, 65.35483662803448, 67.82116450113867, 65.00717336409406, 65.01966160227933, 63.627587493940744], 'lossList': [0.0, -1.4606796836853027, 0.0, 29.767981889247896, 0.0, 0.0, 0.0], 'rewardMean': 0.7858507047613009, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.4430166496704, 'successfulTests': 0
'totalSteps': 6400, 'rewardStep': 0.6933630286799529, 'errorList': [], 'lossList': [0.0, -1.4534988492727279, 0.0, 348.1107621765137, 0.0, 0.0, 0.0], 'rewardMean': 0.7673531695450313, 'totalEpisodes': 90, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.3657563323394246
'totalSteps': 7680, 'rewardStep': 0.49358931605534123, 'errorList': [], 'lossList': [0.0, -1.4507687240839005, 0.0, 177.3032001876831, 0.0, 0.0, 0.0], 'rewardMean': 0.7217258606300829, 'totalEpisodes': 161, 'stepsPerEpisode': 18, 'rewardPerEpisode': 11.497367358049116
'totalSteps': 8960, 'rewardStep': 0.747997867622679, 'errorList': [], 'lossList': [0.0, -1.4304150635004043, 0.0, 81.78714971542358, 0.0, 0.0, 0.0], 'rewardMean': 0.7254790044861681, 'totalEpisodes': 216, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.792039133383144
'totalSteps': 10240, 'rewardStep': 0.5115283261801691, 'errorList': [], 'lossList': [0.0, -1.4081194710731506, 0.0, 60.47060539245606, 0.0, 0.0, 0.0], 'rewardMean': 0.6987351696979182, 'totalEpisodes': 262, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.738824651693946
'totalSteps': 11520, 'rewardStep': 0.5032977897695845, 'errorList': [], 'lossList': [0.0, -1.4143316906690597, 0.0, 56.33390710830688, 0.0, 0.0, 0.0], 'rewardMean': 0.6770199052614366, 'totalEpisodes': 288, 'stepsPerEpisode': 88, 'rewardPerEpisode': 64.46141707377099
'totalSteps': 12800, 'rewardStep': 0.4519752466867578, 'errorList': [], 'lossList': [0.0, -1.4164390754699707, 0.0, 59.45561701774597, 0.0, 0.0, 0.0], 'rewardMean': 0.6545154394039688, 'totalEpisodes': 305, 'stepsPerEpisode': 64, 'rewardPerEpisode': 42.118210621122294
'totalSteps': 14080, 'rewardStep': 0.8352143827332263, 'errorList': [], 'lossList': [0.0, -1.4190349453687667, 0.0, 62.599404392242434, 0.0, 0.0, 0.0], 'rewardMean': 0.6877267928868902, 'totalEpisodes': 317, 'stepsPerEpisode': 44, 'rewardPerEpisode': 33.8892617954145
'totalSteps': 15360, 'rewardStep': 0.40329725390744886, 'errorList': [], 'lossList': [0.0, -1.4091990113258361, 0.0, 54.87904335975647, 0.0, 0.0, 0.0], 'rewardMean': 0.6414188183772637, 'totalEpisodes': 327, 'stepsPerEpisode': 145, 'rewardPerEpisode': 97.49536984446563
'totalSteps': 16640, 'rewardStep': 0.40460095994172496, 'errorList': [], 'lossList': [0.0, -1.4014088302850722, 0.0, 55.558742876052854, 0.0, 0.0, 0.0], 'rewardMean': 0.5986927867874268, 'totalEpisodes': 337, 'stepsPerEpisode': 142, 'rewardPerEpisode': 101.4078124169531
'totalSteps': 17920, 'rewardStep': 0.7974377210209017, 'errorList': [], 'lossList': [0.0, -1.3962991261482238, 0.0, 46.57380528926849, 0.0, 0.0, 0.0], 'rewardMean': 0.5842301892597787, 'totalEpisodes': 344, 'stepsPerEpisode': 314, 'rewardPerEpisode': 271.7139022657096
'totalSteps': 19200, 'rewardStep': 0.7730598855225527, 'errorList': [], 'lossList': [0.0, -1.3844757038354873, 0.0, 68.00093606948853, 0.0, 0.0, 0.0], 'rewardMean': 0.5921998749440387, 'totalEpisodes': 350, 'stepsPerEpisode': 86, 'rewardPerEpisode': 72.12896419767759
'totalSteps': 20480, 'rewardStep': 0.934643720070973, 'errorList': [79.41484936199984, 167.10746914880067, 123.09373910831756, 72.78523498281406, 175.1046687766797, 185.9597128009592, 56.363496741468914, 154.5264079915711, 147.28218146703915, 162.87417488464195, 124.32921593584243, 1.3983551892806254, 161.10647268402738, 145.16467565298205, 189.15102225276328, 84.43972726304163, 142.29996649686947, 125.96815113074939, 139.39037779004576, 107.75651227996116, 113.861065366324, 159.86672704040697, 176.60708176275116, 115.26686562811692, 41.741742607717576, 17.90101827296365, 117.78945218388311, 144.96621098653338, 109.3591053432786, 146.6663634712763, 26.99118584266179, 103.72886453245368, 91.79787435501078, 0.22330453093664965, 193.44873715731336, 169.08034923778638, 57.136528451210474, 61.231084335124564, 136.43825701846873, 71.83376346159287, 0.12153607000952799, 90.08715277815945, 54.23246929420424, 90.29970196207066, 143.5727441519737, 94.17191467162111, 93.4181888886975, 25.604069171910517, 167.34967181670538, 102.69872013645748], 'lossList': [0.0, -1.3909150391817093, 0.0, 76.64763116836548, 0.0, 0.0, 0.0], 'rewardMean': 0.6363053153456019, 'totalEpisodes': 357, 'stepsPerEpisode': 156, 'rewardPerEpisode': 135.00579043363328, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.8455101751183356, 'errorList': [], 'lossList': [0.0, -1.4083252400159836, 0.0, 79.89920272827149, 0.0, 0.0, 0.0], 'rewardMean': 0.6460565460951675, 'totalEpisodes': 364, 'stepsPerEpisode': 33, 'rewardPerEpisode': 25.405318735376234
'totalSteps': 23040, 'rewardStep': 0.5199169745350581, 'errorList': [], 'lossList': [0.0, -1.4363442343473434, 0.0, 70.4831599521637, 0.0, 0.0, 0.0], 'rewardMean': 0.6468954109306564, 'totalEpisodes': 370, 'stepsPerEpisode': 207, 'rewardPerEpisode': 174.32711631955866
'totalSteps': 24320, 'rewardStep': 0.31766290051259166, 'errorList': [], 'lossList': [0.0, -1.4554022401571274, 0.0, 89.28403354644776, 0.0, 0.0, 0.0], 'rewardMean': 0.6283319220049571, 'totalEpisodes': 377, 'stepsPerEpisode': 218, 'rewardPerEpisode': 162.77298030783106
'totalSteps': 25600, 'rewardStep': 0.7273135961963952, 'errorList': [], 'lossList': [0.0, -1.4416063052415848, 0.0, 97.34100415229797, 0.0, 0.0, 0.0], 'rewardMean': 0.6558657569559208, 'totalEpisodes': 385, 'stepsPerEpisode': 196, 'rewardPerEpisode': 153.9860401921087
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=20480, timeSpent=111.78
