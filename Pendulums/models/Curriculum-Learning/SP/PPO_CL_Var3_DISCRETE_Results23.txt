#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 23
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.574368653695752, 'errorList': [], 'lossList': [0.0, -1.4227782970666885, 0.0, 32.65724693536758, 0.0, 0.0, 0.0], 'rewardMean': 0.7366495347612462, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.26894055682111
'totalSteps': 3840, 'rewardStep': 0.9642710036560319, 'errorList': [], 'lossList': [0.0, -1.422820115685463, 0.0, 36.074976375103, 0.0, 0.0, 0.0], 'rewardMean': 0.8125233577261747, 'totalEpisodes': 18, 'stepsPerEpisode': 486, 'rewardPerEpisode': 404.51896440838476
'totalSteps': 5120, 'rewardStep': 0.8515721396544952, 'errorList': [], 'lossList': [0.0, -1.4184762996435165, 0.0, 33.07030442774296, 0.0, 0.0, 0.0], 'rewardMean': 0.8222855532082548, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.9682976252727
'totalSteps': 6400, 'rewardStep': 0.6135607028478627, 'errorList': [], 'lossList': [0.0, -1.4079007142782212, 0.0, 342.7127063751221, 0.0, 0.0, 0.0], 'rewardMean': 0.7805405831361764, 'totalEpisodes': 83, 'stepsPerEpisode': 15, 'rewardPerEpisode': 8.48478744056301
'totalSteps': 7680, 'rewardStep': 0.6579159733216833, 'errorList': [], 'lossList': [0.0, -1.4061043971776963, 0.0, 173.52342216491698, 0.0, 0.0, 0.0], 'rewardMean': 0.7601031481670942, 'totalEpisodes': 147, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.11397538537134
'totalSteps': 8960, 'rewardStep': 0.7089676853015442, 'errorList': [], 'lossList': [0.0, -1.3982074874639512, 0.0, 72.13086153030396, 0.0, 0.0, 0.0], 'rewardMean': 0.7527980820434442, 'totalEpisodes': 192, 'stepsPerEpisode': 72, 'rewardPerEpisode': 60.35655528132381
'totalSteps': 10240, 'rewardStep': 0.7292755484373395, 'errorList': [], 'lossList': [0.0, -1.4021817702054977, 0.0, 54.45241910934448, 0.0, 0.0, 0.0], 'rewardMean': 0.7498577653426812, 'totalEpisodes': 231, 'stepsPerEpisode': 32, 'rewardPerEpisode': 19.8801559141753
'totalSteps': 11520, 'rewardStep': 0.8158115444836487, 'errorList': [], 'lossList': [0.0, -1.4002361166477204, 0.0, 37.51348894119263, 0.0, 0.0, 0.0], 'rewardMean': 0.757185963025011, 'totalEpisodes': 251, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.065792790425624
'totalSteps': 12800, 'rewardStep': 0.6908280510125192, 'errorList': [], 'lossList': [0.0, -1.4026876604557037, 0.0, 25.380748946666717, 0.0, 0.0, 0.0], 'rewardMean': 0.7505501718237617, 'totalEpisodes': 261, 'stepsPerEpisode': 41, 'rewardPerEpisode': 34.59598493289874
'totalSteps': 14080, 'rewardStep': 0.5295683748406208, 'errorList': [], 'lossList': [0.0, -1.4264224165678023, 0.0, 15.390461628437043, 0.0, 0.0, 0.0], 'rewardMean': 0.7136139677251497, 'totalEpisodes': 266, 'stepsPerEpisode': 285, 'rewardPerEpisode': 219.32558616826245
'totalSteps': 15360, 'rewardStep': 0.7359327434205323, 'errorList': [], 'lossList': [0.0, -1.4322050434350968, 0.0, 42.95381067752838, 0.0, 0.0, 0.0], 'rewardMean': 0.7297703766976278, 'totalEpisodes': 273, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.370823565621976
'totalSteps': 16640, 'rewardStep': 0.3510458664755455, 'errorList': [], 'lossList': [0.0, -1.440404076576233, 0.0, 7.65491918683052, 0.0, 0.0, 0.0], 'rewardMean': 0.6684478629795791, 'totalEpisodes': 276, 'stepsPerEpisode': 271, 'rewardPerEpisode': 175.4632576461416
'totalSteps': 17920, 'rewardStep': 0.9687813104286824, 'errorList': [0.10656867719358586, 0.1046270951675948, 0.12367353819164857, 0.10592227562740492, 0.09885670900528011, 0.10250078534959976, 0.12814922019068004, 0.10022093318807755, 0.0973083422812684, 0.10541785051186991, 0.17957432888096844, 0.10409625871760209, 0.1056476745209644, 0.10465430613560991, 0.13010292704601556, 0.1197347066357385, 0.10565711197906553, 0.10922357258775073, 0.10027575975882956, 0.11569783393378717, 0.10654024275840329, 0.09795851094636503, 0.10726353674042663, 0.10744254704843477, 0.12580179328258798, 0.20877894563380162, 0.1070063693243154, 0.17418980025136632, 0.10862925232019606, 0.10192657682774278, 0.1584503765000537, 0.17621689549128788, 0.12358000393002719, 0.09914028764383119, 0.12012367115546638, 0.102035061194274, 0.19780359204681786, 0.10133701663882644, 0.1164650481277369, 0.1042127468760893, 0.17083168993753917, 0.10009584334554907, 0.10176773639777138, 0.11794906604893661, 0.10237005294358018, 0.15396826244431208, 0.10001300934173091, 0.09935317690749824, 0.1076665398052935, 0.0990274628569857], 'lossList': [0.0, -1.4550168269872665, 0.0, 7.427123021483421, 0.0, 0.0, 0.0], 'rewardMean': 0.680168780056998, 'totalEpisodes': 278, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.787908309750637, 'successfulTests': 49
'totalSteps': 19200, 'rewardStep': 0.8481391762096879, 'errorList': [], 'lossList': [0.0, -1.44619054377079, 0.0, 16.33060191631317, 0.0, 0.0, 0.0], 'rewardMean': 0.7036266273931805, 'totalEpisodes': 279, 'stepsPerEpisode': 688, 'rewardPerEpisode': 542.5545581702512
'totalSteps': 20480, 'rewardStep': 0.9619779898016371, 'errorList': [0.1593893107392291, 0.15671297693542077, 0.1678558740465824, 0.16519284867999423, 0.2169775679192282, 0.16299923615243278, 0.16017405892021291, 0.2301599859815931, 0.1544428851137356, 0.17582752718280034, 0.16064921069517657, 0.16109460200101108, 0.23616040314045206, 0.16503608993340418, 0.2365299875029204, 0.16955841831780688, 0.18576058457179528, 0.16000306220846205, 0.16113295842215233, 0.1626199652685901, 0.16246033101576024, 0.15748438679935567, 0.16547332232641151, 0.23804820096731388, 0.16857240383417046, 0.16804577904309298, 0.16465596055813267, 0.15773778679151765, 0.15771352026288674, 0.1608266491841028, 0.1623123179673667, 0.16483888804814237, 0.1586947507140871, 0.16477089912996, 0.15604828764295223, 0.16618248745697425, 0.17837465475125347, 0.1609132707778268, 0.16208587971241756, 0.16617279030247623, 0.18751279870763238, 0.16229430719077095, 0.16316121954383261, 0.17221048686868912, 0.1582663198947239, 0.1588879537412675, 0.22211880538615844, 0.17849194178747194, 0.15964346865662432, 0.16959848185813636], 'lossList': [0.0, -1.4028184431791306, 0.0, 4.717191402763128, 0.0, 0.0, 0.0], 'rewardMean': 0.7340328290411758, 'totalEpisodes': 279, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.5839007738155, 'successfulTests': 44
'totalSteps': 21760, 'rewardStep': 0.8879616745423993, 'errorList': [], 'lossList': [0.0, -1.3582321673631668, 0.0, 3.9623484724760054, 0.0, 0.0, 0.0], 'rewardMean': 0.7519322279652613, 'totalEpisodes': 279, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.1296055280934
'totalSteps': 23040, 'rewardStep': 0.7765194213285863, 'errorList': [], 'lossList': [0.0, -1.3255314582586288, 0.0, 2.5496703991293908, 0.0, 0.0, 0.0], 'rewardMean': 0.756656615254386, 'totalEpisodes': 279, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.7806869012368
'totalSteps': 24320, 'rewardStep': 0.8064403817430372, 'errorList': [], 'lossList': [0.0, -1.2740654462575913, 0.0, 1.8314980005472898, 0.0, 0.0, 0.0], 'rewardMean': 0.7557194989803249, 'totalEpisodes': 279, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.2625913731372
'totalSteps': 25600, 'rewardStep': 0.9609730215611583, 'errorList': [0.06078930376500679, 0.05319971053892495, 0.058193669382515446, 0.055456244967459004, 0.11916019850092423, 0.07862771478111412, 0.07169240104518992, 0.06124220671574289, 0.07087284845787366, 0.05465239690928601, 0.05703475815056868, 0.054808208824654404, 0.09978094222465814, 0.06620275357805976, 0.057773527119905625, 0.08693157992408987, 0.04817332042368354, 0.08874784268539208, 0.048080165232624275, 0.06710477373873604, 0.08319850518457259, 0.05336261296607551, 0.04944877564287624, 0.0525330756152208, 0.06771867266299668, 0.06235548948044998, 0.05891092489206304, 0.05438313608782843, 0.06029850054079949, 0.056319635309967406, 0.06434142967182598, 0.08892521229896864, 0.05090090497862664, 0.07328174944890176, 0.06126258760367975, 0.06745316489792616, 0.051858766100647505, 0.05010437387337235, 0.053781230736537455, 0.05313782157544602, 0.0898874848523738, 0.05769239091582662, 0.0520926949832566, 0.06246899572323079, 0.05095313495833792, 0.054710858787516525, 0.073414830019783, 0.06003576876423097, 0.06226766082642044, 0.05933185441213687], 'lossList': [0.0, -1.2368885886669159, 0.0, 1.4769795957207679, 0.0, 0.0, 0.0], 'rewardMean': 0.7827339960351887, 'totalEpisodes': 279, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.443572678748, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=130.33
