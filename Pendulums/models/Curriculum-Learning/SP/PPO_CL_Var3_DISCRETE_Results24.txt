#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 24
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8201616791084557, 'errorList': [], 'lossList': [0.0, -1.4210871738195419, 0.0, 28.444636491537093, 0.0, 0.0, 0.0], 'rewardMean': 0.7847262717038683, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9817592529649
'totalSteps': 3840, 'rewardStep': 0.7653923850432686, 'errorList': [], 'lossList': [0.0, -1.434804788827896, 0.0, 35.22184423208237, 0.0, 0.0, 0.0], 'rewardMean': 0.7782816428170017, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.43908212466643
'totalSteps': 5120, 'rewardStep': 0.7040208628153881, 'errorList': [], 'lossList': [0.0, -1.4309330904483795, 0.0, 26.119540514349936, 0.0, 0.0, 0.0], 'rewardMean': 0.7597164478165983, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.8211163323339
'totalSteps': 6400, 'rewardStep': 0.7746223173213097, 'errorList': [], 'lossList': [0.0, -1.4231654822826385, 0.0, 366.7222996520996, 0.0, 0.0, 0.0], 'rewardMean': 0.7626976217175405, 'totalEpisodes': 98, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.552952808518008
'totalSteps': 7680, 'rewardStep': 0.7830440953566876, 'errorList': [], 'lossList': [0.0, -1.4250287574529648, 0.0, 156.27488838195802, 0.0, 0.0, 0.0], 'rewardMean': 0.7660887006573983, 'totalEpisodes': 163, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.851075826170914
'totalSteps': 8960, 'rewardStep': 0.7754796200943157, 'errorList': [], 'lossList': [0.0, -1.4136500352621078, 0.0, 63.88520576477051, 0.0, 0.0, 0.0], 'rewardMean': 0.7674302605769581, 'totalEpisodes': 220, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.190526710393337
'totalSteps': 10240, 'rewardStep': 0.9088633649373012, 'errorList': [], 'lossList': [0.0, -1.383453294634819, 0.0, 51.55394067764282, 0.0, 0.0, 0.0], 'rewardMean': 0.7851093986220009, 'totalEpisodes': 257, 'stepsPerEpisode': 40, 'rewardPerEpisode': 33.09967496784373
'totalSteps': 11520, 'rewardStep': 0.615530079514934, 'errorList': [], 'lossList': [0.0, -1.3557148063182831, 0.0, 43.596021223068234, 0.0, 0.0, 0.0], 'rewardMean': 0.766267252054549, 'totalEpisodes': 274, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.244120967261829
'totalSteps': 12800, 'rewardStep': 0.8820647031018658, 'errorList': [], 'lossList': [0.0, -1.3176922130584716, 0.0, 42.376559257507324, 0.0, 0.0, 0.0], 'rewardMean': 0.7778469971592807, 'totalEpisodes': 289, 'stepsPerEpisode': 65, 'rewardPerEpisode': 57.406618185308105
'totalSteps': 14080, 'rewardStep': 0.3426958101783506, 'errorList': [], 'lossList': [0.0, -1.309572196006775, 0.0, 32.383647847175595, 0.0, 0.0, 0.0], 'rewardMean': 0.7371874917471878, 'totalEpisodes': 300, 'stepsPerEpisode': 70, 'rewardPerEpisode': 39.54537396927671
'totalSteps': 15360, 'rewardStep': 0.32878697133678164, 'errorList': [], 'lossList': [0.0, -1.311104593873024, 0.0, 37.323649983406064, 0.0, 0.0, 0.0], 'rewardMean': 0.6880500209700202, 'totalEpisodes': 311, 'stepsPerEpisode': 129, 'rewardPerEpisode': 89.54671263136589
'totalSteps': 16640, 'rewardStep': 0.7679032987721733, 'errorList': [], 'lossList': [0.0, -1.3087217652797698, 0.0, 12.853494064807892, 0.0, 0.0, 0.0], 'rewardMean': 0.6883011123429108, 'totalEpisodes': 319, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.09671941588784
'totalSteps': 17920, 'rewardStep': 0.40699865042335465, 'errorList': [], 'lossList': [0.0, -1.3083988946676255, 0.0, 11.926257895231247, 0.0, 0.0, 0.0], 'rewardMean': 0.6585988911037075, 'totalEpisodes': 325, 'stepsPerEpisode': 109, 'rewardPerEpisode': 80.25101172086715
'totalSteps': 19200, 'rewardStep': 0.9454134846987768, 'errorList': [94.97716097176198, 10.131270357538689, 129.24981947778826, 44.97644531818171, 1.7414860833274854, 39.8497494328935, 103.97120459230618, 0.30174541618209594, 68.29753259224374, 1.0218010650209362, 90.26159593153932, 21.819724987204005, 40.049701225896904, 53.15474997585501, 68.8047466132348, 23.293719448960932, 94.14314167906095, 64.17649058659215, 65.4380410273227, 1.078916316915567, 73.28888796572294, 0.4262807202691223, 0.572774022344648, 115.54943676496099, 0.6223839106681989, 138.1929804854449, 85.83504144507849, 70.80193749963725, 68.0016190943958, 47.06013026944807, 26.025626495047373, 93.74467662802432, 36.48118097469285, 72.63716531133699, 2.0636704066182023, 133.93641192116044, 1.4052555419340373, 3.056636832313306, 18.030633682464007, 111.06139688087613, 1.9209936145487214, 14.998051306084248, 85.29844559710995, 74.08103592462159, 41.971879578655596, 26.115580954748875, 73.38821097639571, 121.69212755283681, 42.421086361046974, 52.82532041437465], 'lossList': [0.0, -1.3021941483020782, 0.0, 8.952447415590287, 0.0, 0.0, 0.0], 'rewardMean': 0.6756780078414542, 'totalEpisodes': 331, 'stepsPerEpisode': 93, 'rewardPerEpisode': 82.84094016236895, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.7196121384533791, 'errorList': [], 'lossList': [0.0, -1.2799530637264251, 0.0, 6.445831178426743, 0.0, 0.0, 0.0], 'rewardMean': 0.6693348121511232, 'totalEpisodes': 336, 'stepsPerEpisode': 253, 'rewardPerEpisode': 189.04080494723303
'totalSteps': 21760, 'rewardStep': 0.7351834450312884, 'errorList': [], 'lossList': [0.0, -1.2583293831348419, 0.0, 4.860948705077171, 0.0, 0.0, 0.0], 'rewardMean': 0.6653051946448205, 'totalEpisodes': 341, 'stepsPerEpisode': 122, 'rewardPerEpisode': 94.53588746284814
'totalSteps': 23040, 'rewardStep': 0.7167256773833439, 'errorList': [], 'lossList': [0.0, -1.2385795652866363, 0.0, 4.2001875102519985, 0.0, 0.0, 0.0], 'rewardMean': 0.6460914258894249, 'totalEpisodes': 345, 'stepsPerEpisode': 202, 'rewardPerEpisode': 161.47788665037893
'totalSteps': 24320, 'rewardStep': 0.8028668283657205, 'errorList': [], 'lossList': [0.0, -1.2249825346469878, 0.0, 3.3318500328063965, 0.0, 0.0, 0.0], 'rewardMean': 0.6648251007745035, 'totalEpisodes': 347, 'stepsPerEpisode': 67, 'rewardPerEpisode': 58.72710145625296
'totalSteps': 25600, 'rewardStep': 0.9295104656423415, 'errorList': [], 'lossList': [0.0, -1.1880999886989594, 0.0, 2.4997065882384777, 0.0, 0.0, 0.0], 'rewardMean': 0.6695696770285511, 'totalEpisodes': 347, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.2134030777486
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=88.5
