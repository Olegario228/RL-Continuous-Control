#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 25
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7615115961618314, 'errorList': [], 'lossList': [0.0, -1.416316899061203, 0.0, 34.04166961669922, 0.0, 0.0, 0.0], 'rewardMean': 0.6028671685686746, 'totalEpisodes': 54, 'stepsPerEpisode': 22, 'rewardPerEpisode': 14.045930817942493
'totalSteps': 3840, 'rewardStep': 0.24376022335712494, 'errorList': [], 'lossList': [0.0, -1.4158412671089173, 0.0, 39.97222840309143, 0.0, 0.0, 0.0], 'rewardMean': 0.4831648534981581, 'totalEpisodes': 69, 'stepsPerEpisode': 95, 'rewardPerEpisode': 70.66786970327684
'totalSteps': 5120, 'rewardStep': 0.6881630550608158, 'errorList': [], 'lossList': [0.0, -1.4050467377901077, 0.0, 30.977895011901854, 0.0, 0.0, 0.0], 'rewardMean': 0.5344144038888226, 'totalEpisodes': 78, 'stepsPerEpisode': 193, 'rewardPerEpisode': 146.5557322671595
'totalSteps': 6400, 'rewardStep': 0.5538731503602887, 'errorList': [], 'lossList': [0.0, -1.4017364168167115, 0.0, 16.829771225452422, 0.0, 0.0, 0.0], 'rewardMean': 0.5383061531831158, 'totalEpisodes': 85, 'stepsPerEpisode': 160, 'rewardPerEpisode': 120.06529699410683
'totalSteps': 7680, 'rewardStep': 0.8237966490539952, 'errorList': [], 'lossList': [0.0, -1.4100164914131164, 0.0, 178.20971271514892, 0.0, 0.0, 0.0], 'rewardMean': 0.5858879024949291, 'totalEpisodes': 128, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.245984641656541
'totalSteps': 8960, 'rewardStep': 0.5862948488655912, 'errorList': [], 'lossList': [0.0, -1.416668663620949, 0.0, 68.17679586410523, 0.0, 0.0, 0.0], 'rewardMean': 0.585946037690738, 'totalEpisodes': 158, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.85080706603541
'totalSteps': 10240, 'rewardStep': 0.6565612046000957, 'errorList': [], 'lossList': [0.0, -1.41519362449646, 0.0, 53.27206657409668, 0.0, 0.0, 0.0], 'rewardMean': 0.5947729335544076, 'totalEpisodes': 180, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.346637403042555
'totalSteps': 11520, 'rewardStep': 0.6893957105247533, 'errorList': [], 'lossList': [0.0, -1.3854549610614777, 0.0, 27.77373064517975, 0.0, 0.0, 0.0], 'rewardMean': 0.6052865754400015, 'totalEpisodes': 189, 'stepsPerEpisode': 65, 'rewardPerEpisode': 49.124973775092236
'totalSteps': 12800, 'rewardStep': 0.5274782387928321, 'errorList': [], 'lossList': [0.0, -1.353033982515335, 0.0, 9.969205360412598, 0.0, 0.0, 0.0], 'rewardMean': 0.5975057417752846, 'totalEpisodes': 192, 'stepsPerEpisode': 349, 'rewardPerEpisode': 247.00199999132627
'totalSteps': 14080, 'rewardStep': 0.5677034809553037, 'errorList': [], 'lossList': [0.0, -1.3416325002908707, 0.0, 9.349247951507568, 0.0, 0.0, 0.0], 'rewardMean': 0.6098538157732633, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 875.1670591120155
'totalSteps': 15360, 'rewardStep': 0.45639650070800303, 'errorList': [], 'lossList': [0.0, -1.3371547144651412, 0.0, 5.974418225884437, 0.0, 0.0, 0.0], 'rewardMean': 0.5793423062278804, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 906.555665164397
'totalSteps': 16640, 'rewardStep': 0.8419527434332064, 'errorList': [], 'lossList': [0.0, -1.3334788852930068, 0.0, 8.29010014489293, 0.0, 0.0, 0.0], 'rewardMean': 0.6391615582354885, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.0673820112536
'totalSteps': 17920, 'rewardStep': 0.9007429648258528, 'errorList': [], 'lossList': [0.0, -1.3109953153133391, 0.0, 6.150346533209086, 0.0, 0.0, 0.0], 'rewardMean': 0.6604195492119922, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1120.7131816196472
'totalSteps': 19200, 'rewardStep': 0.9236977109585841, 'errorList': [], 'lossList': [0.0, -1.2669923549890518, 0.0, 4.0453306736052035, 0.0, 0.0, 0.0], 'rewardMean': 0.6974020052718218, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.7607386165969
'totalSteps': 20480, 'rewardStep': 0.9225164298341441, 'errorList': [], 'lossList': [0.0, -1.2437435239553452, 0.0, 3.485832101479173, 0.0, 0.0, 0.0], 'rewardMean': 0.7072739833498367, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.0105690320338
'totalSteps': 21760, 'rewardStep': 0.9601929578052399, 'errorList': [0.045150304504077934, 0.04407752437755105, 0.09221130380471142, 0.07062507820932938, 0.1363703526981975, 0.04317308839133097, 0.0659074175564981, 0.03932877484989299, 0.03977296828731757, 0.05970357965900545, 0.10611385325914789, 0.056615787791242225, 0.041057678154918106, 0.1595150010393588, 0.2224025954473553, 0.046484516032012875, 0.04244690019336436, 0.13273610705791197, 0.13414576940695258, 0.15670426852823524, 0.06808986497300362, 0.04237814460373055, 0.11100510703746487, 0.074152787068534, 0.13132772570869747, 0.04031149657832906, 0.08631889800128442, 0.04248747103433923, 0.12946323197942272, 0.08681260312041544, 0.04847352977996322, 0.0373534826099571, 0.07024369318262563, 0.05823638758501557, 0.03826063120352897, 0.08757341502776984, 0.03884162360039637, 0.03886327662587707, 0.036875813225143356, 0.06679934907230142, 0.04692188712323788, 0.03700955894704556, 0.056229018130382176, 0.043398272575901314, 0.04507554300261157, 0.04260093852397572, 0.0422191787806832, 0.12552848762642643, 0.11507527596897467, 0.14597767841392958], 'lossList': [0.0, -1.2262273961305619, 0.0, 2.065098568499088, 0.0, 0.0, 0.0], 'rewardMean': 0.7446637942438015, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.090941518914, 'successfulTests': 49
'totalSteps': 23040, 'rewardStep': 0.9123640950151202, 'errorList': [], 'lossList': [0.0, -1.2063283872604371, 0.0, 1.7007747789379208, 0.0, 0.0, 0.0], 'rewardMean': 0.7702440832853039, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1200.401011575404
'totalSteps': 24320, 'rewardStep': 0.815758699073759, 'errorList': [], 'lossList': [0.0, -1.1756349462270737, 0.0, 1.0878305085189641, 0.0, 0.0, 0.0], 'rewardMean': 0.7828803821402046, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1204.0375605572117
'totalSteps': 25600, 'rewardStep': 0.9680392507353374, 'errorList': [0.10533062405113774, 0.05500464494183066, 0.07824497930617415, 0.069639623391644, 0.08579020437455216, 0.09286483063411925, 0.05142154930311219, 0.0730395050156337, 0.07022573380682452, 0.07910161930702968, 0.08094861183292389, 0.06786509027178197, 0.08332425029339192, 0.03712352079630805, 0.03542817236037478, 0.04346279343909805, 0.12836079216456422, 0.04136490396075298, 0.06913196415567831, 0.06287102999388368, 0.06740594986365063, 0.045987403368521834, 0.054119808513599425, 0.06924696249334068, 0.11667850456395003, 0.07603446765489738, 0.11991649491775555, 0.08059319650375127, 0.1259436463697471, 0.05562452895878405, 0.04947519367208271, 0.06151153535079364, 0.051095789665309053, 0.07382160479745356, 0.09434627487277625, 0.03071445103461325, 0.08530428651886321, 0.06322056917465946, 0.059463355618996065, 0.047268074329751934, 0.03946136631656074, 0.05688575391151855, 0.07704070051766246, 0.034392546608898546, 0.06821133661989613, 0.05496622961509355, 0.05070405452162444, 0.04732430494028009, 0.08126000500550931, 0.06296177993990364], 'lossList': [0.0, -1.1394493061304092, 0.0, 0.7082888904772698, 0.0, 0.0, 0.0], 'rewardMean': 0.826936483334455, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1206.8969005170286, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=111.62
