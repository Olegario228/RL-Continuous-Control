#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 26
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980785764922136, 'errorList': [], 'lossList': [0.0, -1.417504243850708, 0.0, 30.41210502624512, 0.0, 0.0, 0.0], 'rewardMean': 0.7065643546668106, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.34356450654182
'totalSteps': 3840, 'rewardStep': 0.7337342130467079, 'errorList': [], 'lossList': [0.0, -1.4060174733400346, 0.0, 37.440305285453796, 0.0, 0.0, 0.0], 'rewardMean': 0.7156209741267764, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.247863075167686
'totalSteps': 5120, 'rewardStep': 0.8983266968431368, 'errorList': [], 'lossList': [0.0, -1.4023935985565186, 0.0, 28.85670949935913, 0.0, 0.0, 0.0], 'rewardMean': 0.7612974048058665, 'totalEpisodes': 76, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.645272888643724
'totalSteps': 6400, 'rewardStep': 0.5405434309987291, 'errorList': [], 'lossList': [0.0, -1.3908120518922806, 0.0, 28.452368409633635, 0.0, 0.0, 0.0], 'rewardMean': 0.717146610044439, 'totalEpisodes': 79, 'stepsPerEpisode': 83, 'rewardPerEpisode': 68.01456295733756
'totalSteps': 7680, 'rewardStep': 0.5206182467260537, 'errorList': [], 'lossList': [0.0, -1.3821633017063142, 0.0, 114.91610286712647, 0.0, 0.0, 0.0], 'rewardMean': 0.6843918828247082, 'totalEpisodes': 97, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.7154844488731298
'totalSteps': 8960, 'rewardStep': 0.6061464715925695, 'errorList': [], 'lossList': [0.0, -1.3766613763570785, 0.0, 87.06520137786865, 0.0, 0.0, 0.0], 'rewardMean': 0.6732139669344026, 'totalEpisodes': 115, 'stepsPerEpisode': 207, 'rewardPerEpisode': 133.52552592405365
'totalSteps': 10240, 'rewardStep': 0.8179106459064518, 'errorList': [], 'lossList': [0.0, -1.3606198561191558, 0.0, 20.2072016620636, 0.0, 0.0, 0.0], 'rewardMean': 0.6913010518059087, 'totalEpisodes': 123, 'stepsPerEpisode': 25, 'rewardPerEpisode': 22.307763448100168
'totalSteps': 11520, 'rewardStep': 0.7154875297232064, 'errorList': [], 'lossList': [0.0, -1.3534985566139222, 0.0, 32.43881309986114, 0.0, 0.0, 0.0], 'rewardMean': 0.693988438241164, 'totalEpisodes': 131, 'stepsPerEpisode': 59, 'rewardPerEpisode': 49.278129457805164
'totalSteps': 12800, 'rewardStep': 0.6597528052047, 'errorList': [], 'lossList': [0.0, -1.3277687519788741, 0.0, 18.008542673587797, 0.0, 0.0, 0.0], 'rewardMean': 0.6905648749375176, 'totalEpisodes': 138, 'stepsPerEpisode': 76, 'rewardPerEpisode': 63.639056524454865
'totalSteps': 14080, 'rewardStep': 0.7394741035497349, 'errorList': [], 'lossList': [0.0, -1.2941139805316926, 0.0, 17.330043992996217, 0.0, 0.0, 0.0], 'rewardMean': 0.6830072720083504, 'totalEpisodes': 142, 'stepsPerEpisode': 230, 'rewardPerEpisode': 190.54865205618182
'totalSteps': 15360, 'rewardStep': 0.9677620250846852, 'errorList': [0.7141348196147635, 0.23978792195161538, 0.7184938317535129, 0.12234639001672538, 0.3947782912946586, 0.33150386932044457, 0.25725262698299817, 0.2443924849813139, 0.2663845462983219, 1.1706565930873734, 0.930545213352468, 0.45112925351212524, 0.11601563204789721, 0.5093708094058672, 0.21982297828836883, 0.4916933218103508, 0.7644285717639396, 0.4713730901443659, 0.7097667468850795, 0.11396055298758079, 0.7420551405950031, 0.5875655077263542, 0.9577667620174656, 0.1806589178442556, 0.955156703500674, 0.17461428883915747, 0.5788410420749687, 0.47380122017949705, 0.12365779776190101, 0.9744516490269819, 0.5348535222510327, 0.6969739583405784, 1.2215259358394495, 0.15232210491094111, 0.6440263152606568, 0.5817257878453465, 0.15018003007019623, 0.24578414039272678, 0.6127843926960179, 0.4815507641748564, 0.6149965169455123, 0.8408660314738432, 0.4003103133769942, 0.09461884927174358, 0.36000620731250516, 0.10314208917447083, 0.3583576844274694, 0.32293002294349943, 0.11303975082081087, 0.802581628368312], 'lossList': [0.0, -1.2770764684677125, 0.0, 5.392480079531669, 0.0, 0.0, 0.0], 'rewardMean': 0.7199756168675975, 'totalEpisodes': 144, 'stepsPerEpisode': 15, 'rewardPerEpisode': 14.133891503638003, 'successfulTests': 11
'totalSteps': 16640, 'rewardStep': 0.6888855785700194, 'errorList': [], 'lossList': [0.0, -1.251214410662651, 0.0, 5.120459489822387, 0.0, 0.0, 0.0], 'rewardMean': 0.7154907534199287, 'totalEpisodes': 146, 'stepsPerEpisode': 198, 'rewardPerEpisode': 148.31964150285725
'totalSteps': 17920, 'rewardStep': 0.6392721822375987, 'errorList': [], 'lossList': [0.0, -1.2171197307109833, 0.0, 3.408771950006485, 0.0, 0.0, 0.0], 'rewardMean': 0.6895853019593748, 'totalEpisodes': 147, 'stepsPerEpisode': 226, 'rewardPerEpisode': 179.29544677568433
'totalSteps': 19200, 'rewardStep': 0.7853223946959191, 'errorList': [], 'lossList': [0.0, -1.1667853313684464, 0.0, 2.7079975059628487, 0.0, 0.0, 0.0], 'rewardMean': 0.7140631983290938, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 909.531213379657
'totalSteps': 20480, 'rewardStep': 0.7433798287466268, 'errorList': [], 'lossList': [0.0, -1.1247238540649414, 0.0, 2.3199199286103247, 0.0, 0.0, 0.0], 'rewardMean': 0.7363393565311512, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.3327135979061
'totalSteps': 21760, 'rewardStep': 0.8072387890023327, 'errorList': [], 'lossList': [0.0, -1.0705414724349975, 0.0, 1.0327738843858243, 0.0, 0.0, 0.0], 'rewardMean': 0.7564485882721275, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1095.7662314895463
'totalSteps': 23040, 'rewardStep': 0.940524619009964, 'errorList': [0.07053846025561337, 0.06764722185431091, 0.07667621552558786, 0.07072279250742354, 0.0785309650028904, 0.08533526145851869, 0.06703360694578034, 0.08013685578981043, 0.07473868506234409, 0.0796165996996938, 0.08461056991719874, 0.0924583230755981, 0.07069364964739037, 0.09174656568369847, 0.09253424565285467, 0.06273629459728462, 0.07726063347125658, 0.060314127909883296, 0.07801574405831047, 0.08915296517109055, 0.10031823486512594, 0.07531010527206985, 0.12707390153871753, 0.08754915731293776, 0.07590969864178809, 0.08855039579598314, 0.08637431098642223, 0.06333979140722293, 0.08505630303788508, 0.08691101856493971, 0.09009525814620807, 0.08881662253789228, 0.07146535948099368, 0.10094704280254346, 0.087980135379647, 0.08632863079982496, 0.08150417642854481, 0.07321907528529731, 0.0941735422933601, 0.09604092192490514, 0.06105846818304572, 0.07417842302734945, 0.09949389267130014, 0.09949279360238132, 0.08027498362236814, 0.05983620196767254, 0.07947895488641223, 0.06439309941135908, 0.0641320560568675, 0.07835512605229426], 'lossList': [0.0, -1.0455546134710312, 0.0, 0.9551675655692816, 0.0, 0.0, 0.0], 'rewardMean': 0.7687099855824787, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.565039479415, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=104.79
