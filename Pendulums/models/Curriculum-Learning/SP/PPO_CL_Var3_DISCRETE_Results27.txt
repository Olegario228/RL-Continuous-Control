#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 27
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092971427115871, 'errorList': [], 'lossList': [0.0, -1.4231055521965026, 0.0, 34.86121163368225, 0.0, 0.0, 0.0], 'rewardMean': 0.8624672422476483, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.76661789737042
'totalSteps': 3840, 'rewardStep': 0.5562513755896646, 'errorList': [], 'lossList': [0.0, -1.4137311124801635, 0.0, 33.320800943374635, 0.0, 0.0, 0.0], 'rewardMean': 0.7603952866949871, 'totalEpisodes': 82, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.96326734396544
'totalSteps': 5120, 'rewardStep': 0.41657226176954365, 'errorList': [], 'lossList': [0.0, -1.393478239774704, 0.0, 22.643683416843416, 0.0, 0.0, 0.0], 'rewardMean': 0.6744395304636263, 'totalEpisodes': 91, 'stepsPerEpisode': 209, 'rewardPerEpisode': 111.1692412224747
'totalSteps': 6400, 'rewardStep': 0.5781114103780555, 'errorList': [], 'lossList': [0.0, -1.3759211069345474, 0.0, 55.08653503417969, 0.0, 0.0, 0.0], 'rewardMean': 0.6551739064465121, 'totalEpisodes': 100, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5781114103780555
'totalSteps': 7680, 'rewardStep': 0.9078460379959515, 'errorList': [], 'lossList': [0.0, -1.3660125142335893, 0.0, 155.23885803222657, 0.0, 0.0, 0.0], 'rewardMean': 0.6972859283714187, 'totalEpisodes': 146, 'stepsPerEpisode': 12, 'rewardPerEpisode': 11.077901063519072
'totalSteps': 8960, 'rewardStep': 0.526182442370617, 'errorList': [], 'lossList': [0.0, -1.3520324456691741, 0.0, 64.23455345153809, 0.0, 0.0, 0.0], 'rewardMean': 0.672842573228447, 'totalEpisodes': 167, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.942036366049935
'totalSteps': 10240, 'rewardStep': 0.44419013249856243, 'errorList': [], 'lossList': [0.0, -1.3413776129484176, 0.0, 34.9141099691391, 0.0, 0.0, 0.0], 'rewardMean': 0.6442610181372114, 'totalEpisodes': 174, 'stepsPerEpisode': 112, 'rewardPerEpisode': 86.29457672188288
'totalSteps': 11520, 'rewardStep': 0.42865053452630275, 'errorList': [], 'lossList': [0.0, -1.341700050830841, 0.0, 33.39587563276291, 0.0, 0.0, 0.0], 'rewardMean': 0.6203042977359993, 'totalEpisodes': 181, 'stepsPerEpisode': 235, 'rewardPerEpisode': 146.0031544672589
'totalSteps': 12800, 'rewardStep': 0.8669753227820314, 'errorList': [], 'lossList': [0.0, -1.3467882132530213, 0.0, 19.387329428195955, 0.0, 0.0, 0.0], 'rewardMean': 0.6449714002406025, 'totalEpisodes': 187, 'stepsPerEpisode': 184, 'rewardPerEpisode': 146.77268147579346
'totalSteps': 14080, 'rewardStep': 0.7289129342949955, 'errorList': [], 'lossList': [0.0, -1.3682112139463425, 0.0, 6.745481672286988, 0.0, 0.0, 0.0], 'rewardMean': 0.6362989594917311, 'totalEpisodes': 191, 'stepsPerEpisode': 395, 'rewardPerEpisode': 268.3126820642504
'totalSteps': 15360, 'rewardStep': 0.5993393322023717, 'errorList': [], 'lossList': [0.0, -1.3699526107311248, 0.0, 35.9682315659523, 0.0, 0.0, 0.0], 'rewardMean': 0.6053031784408096, 'totalEpisodes': 194, 'stepsPerEpisode': 314, 'rewardPerEpisode': 210.40979179225292
'totalSteps': 16640, 'rewardStep': 0.5910553888979857, 'errorList': [], 'lossList': [0.0, -1.3525767773389816, 0.0, 5.951731472909451, 0.0, 0.0, 0.0], 'rewardMean': 0.6087835797716418, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 944.2228884066736
'totalSteps': 17920, 'rewardStep': 0.8433985909553776, 'errorList': [], 'lossList': [0.0, -1.337405242919922, 0.0, 9.322515107393265, 0.0, 0.0, 0.0], 'rewardMean': 0.6514662126902251, 'totalEpisodes': 196, 'stepsPerEpisode': 408, 'rewardPerEpisode': 355.534619805856
'totalSteps': 19200, 'rewardStep': 0.8708403108474895, 'errorList': [], 'lossList': [0.0, -1.332730506658554, 0.0, 10.214910939335823, 0.0, 0.0, 0.0], 'rewardMean': 0.6807391027371684, 'totalEpisodes': 197, 'stepsPerEpisode': 758, 'rewardPerEpisode': 641.7872376114735
'totalSteps': 20480, 'rewardStep': 0.4722633587439513, 'errorList': [], 'lossList': [0.0, -1.309883527159691, 0.0, 4.807101371884346, 0.0, 0.0, 0.0], 'rewardMean': 0.6371808348119685, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 776.9123289223181
'totalSteps': 21760, 'rewardStep': 0.6860134249890011, 'errorList': [], 'lossList': [0.0, -1.3012867641448975, 0.0, 3.1861786144971846, 0.0, 0.0, 0.0], 'rewardMean': 0.653163933073807, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 918.5075947749083
'totalSteps': 23040, 'rewardStep': 0.6429757666667116, 'errorList': [], 'lossList': [0.0, -1.2868685740232468, 0.0, 6.264313228130341, 0.0, 0.0, 0.0], 'rewardMean': 0.6730424964906218, 'totalEpisodes': 199, 'stepsPerEpisode': 282, 'rewardPerEpisode': 219.0389791457753
'totalSteps': 24320, 'rewardStep': 0.9341328173488725, 'errorList': [0.71409257347964, 0.9561343754377494, 0.6097857915725392, 0.39937960777833365, 0.7280009987018075, 0.39689230796221814, 0.48411915176425246, 0.6650661450951505, 0.46588423137101453, 0.3933566714827584, 0.509226277804922, 0.3379550711768357, 0.5857939388800444, 0.6775059422743843, 0.4246440314683558, 0.7241700115949505, 0.33254205467456466, 0.5744929039491182, 0.7782824141378305, 0.8421486444611058, 0.35145641086934315, 0.6744417904306749, 0.4292282930401577, 0.633648590492195, 0.44604484817532386, 0.7725943523879264, 0.7411196108724128, 0.8355415462776324, 0.861528067374638, 0.4593525912232705, 0.34883655024162563, 0.6881407861971227, 0.4120000224283797, 0.48122078487421166, 0.5925301271476264, 0.6661706070527781, 0.5438285267668148, 0.4862334721115339, 0.647151442357876, 0.4215789120344612, 0.8838191647249308, 0.62521855015331, 0.6976201520353627, 0.5572899218230705, 0.6826822333026857, 0.5379415741121417, 0.8075463993427772, 0.3616389564826039, 0.4526406246971559, 0.7044413211038456], 'lossList': [0.0, -1.2766726207733154, 0.0, 4.294951825141907, 0.0, 0.0, 0.0], 'rewardMean': 0.7235907247728788, 'totalEpisodes': 202, 'stepsPerEpisode': 88, 'rewardPerEpisode': 82.01424275815978, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.845989604344893, 'errorList': [], 'lossList': [0.0, -1.2667184841632844, 0.0, 1.98929011374712, 0.0, 0.0, 0.0], 'rewardMean': 0.721492152929165, 'totalEpisodes': 203, 'stepsPerEpisode': 359, 'rewardPerEpisode': 309.8299517154343
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=92.07
