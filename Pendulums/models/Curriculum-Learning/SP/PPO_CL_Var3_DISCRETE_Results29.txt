#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 29
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764237844444184, 'errorList': [], 'lossList': [0.0, -1.4117648071050644, 0.0, 34.56727179527283, 0.0, 0.0, 0.0], 'rewardMean': 0.8487569592979127, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.540084423708798
'totalSteps': 3840, 'rewardStep': 0.8301048750001292, 'errorList': [], 'lossList': [0.0, -1.4150108939409256, 0.0, 39.124620037078856, 0.0, 0.0, 0.0], 'rewardMean': 0.8425395978653182, 'totalEpisodes': 84, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.84030782755015
'totalSteps': 5120, 'rewardStep': 0.6848389580966191, 'errorList': [], 'lossList': [0.0, -1.4063812613487243, 0.0, 28.86642453432083, 0.0, 0.0, 0.0], 'rewardMean': 0.8031144379231434, 'totalEpisodes': 90, 'stepsPerEpisode': 117, 'rewardPerEpisode': 92.77188059566005
'totalSteps': 6400, 'rewardStep': 0.522264938468509, 'errorList': [], 'lossList': [0.0, -1.3970475417375565, 0.0, 23.584555982351304, 0.0, 0.0, 0.0], 'rewardMean': 0.7469445380322165, 'totalEpisodes': 94, 'stepsPerEpisode': 456, 'rewardPerEpisode': 368.87075139144315
'totalSteps': 7680, 'rewardStep': 0.9162620359835317, 'errorList': [], 'lossList': [0.0, -1.4023456376791001, 0.0, 170.39741188049317, 0.0, 0.0, 0.0], 'rewardMean': 0.7751641210241024, 'totalEpisodes': 132, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.90146430095997
'totalSteps': 8960, 'rewardStep': 0.5468626736438866, 'errorList': [], 'lossList': [0.0, -1.4080060547590256, 0.0, 110.60074138641357, 0.0, 0.0, 0.0], 'rewardMean': 0.7425496285412143, 'totalEpisodes': 172, 'stepsPerEpisode': 101, 'rewardPerEpisode': 79.89775503190698
'totalSteps': 10240, 'rewardStep': 0.32753390061483145, 'errorList': [], 'lossList': [0.0, -1.4061068123579026, 0.0, 38.58367527961731, 0.0, 0.0, 0.0], 'rewardMean': 0.6906726625504166, 'totalEpisodes': 188, 'stepsPerEpisode': 79, 'rewardPerEpisode': 51.35402216323725
'totalSteps': 11520, 'rewardStep': 0.21063592577881296, 'errorList': [], 'lossList': [0.0, -1.388268386721611, 0.0, 20.033088200092315, 0.0, 0.0, 0.0], 'rewardMean': 0.6373352473535717, 'totalEpisodes': 197, 'stepsPerEpisode': 113, 'rewardPerEpisode': 67.20032605009774
'totalSteps': 12800, 'rewardStep': 0.3774388842428371, 'errorList': [], 'lossList': [0.0, -1.3662585455179215, 0.0, 31.750275506973267, 0.0, 0.0, 0.0], 'rewardMean': 0.6113456110424982, 'totalEpisodes': 207, 'stepsPerEpisode': 63, 'rewardPerEpisode': 34.09357053434351
'totalSteps': 14080, 'rewardStep': 0.7475903170365178, 'errorList': [], 'lossList': [0.0, -1.3474309623241425, 0.0, 15.15437190771103, 0.0, 0.0, 0.0], 'rewardMean': 0.5939956293310094, 'totalEpisodes': 210, 'stepsPerEpisode': 1011, 'rewardPerEpisode': 755.9966188559723
'totalSteps': 15360, 'rewardStep': 0.6723671306550745, 'errorList': [], 'lossList': [0.0, -1.3314335602521896, 0.0, 37.94079146385193, 0.0, 0.0, 0.0], 'rewardMean': 0.583589963952075, 'totalEpisodes': 216, 'stepsPerEpisode': 319, 'rewardPerEpisode': 207.98952043593434
'totalSteps': 16640, 'rewardStep': 0.39116151804004595, 'errorList': [], 'lossList': [0.0, -1.314862100481987, 0.0, 7.758277816772461, 0.0, 0.0, 0.0], 'rewardMean': 0.5396956282560665, 'totalEpisodes': 218, 'stepsPerEpisode': 93, 'rewardPerEpisode': 68.44902397005737
'totalSteps': 17920, 'rewardStep': 0.6593227798421961, 'errorList': [], 'lossList': [0.0, -1.2912460213899613, 0.0, 7.032811245322227, 0.0, 0.0, 0.0], 'rewardMean': 0.5371440104306243, 'totalEpisodes': 219, 'stepsPerEpisode': 537, 'rewardPerEpisode': 435.5908987638074
'totalSteps': 19200, 'rewardStep': 0.5895408146352944, 'errorList': [], 'lossList': [0.0, -1.2963253009319304, 0.0, 3.5981199127435683, 0.0, 0.0, 0.0], 'rewardMean': 0.5438715980473028, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 887.1157814117873
'totalSteps': 20480, 'rewardStep': 0.7996938296379487, 'errorList': [], 'lossList': [0.0, -1.2707822912931441, 0.0, 3.1402705259621144, 0.0, 0.0, 0.0], 'rewardMean': 0.5322147774127446, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 994.3171199361427
'totalSteps': 21760, 'rewardStep': 0.892971889537694, 'errorList': [], 'lossList': [0.0, -1.2373445612192153, 0.0, 3.2733518551290035, 0.0, 0.0, 0.0], 'rewardMean': 0.5668256990021254, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.7258879397818
'totalSteps': 23040, 'rewardStep': 0.9301326033510332, 'errorList': [0.02206531884856191, 0.017748338061611524, 0.019715982495077843, 0.010640191940157124, 0.009323853338606612, 0.015378266786557331, 0.010712551491320348, 0.015242016443927116, 0.041947587070754064, 0.029946540868637295, 0.02857388398355613, 0.016769935124479184, 0.01101647571415323, 0.011791868931875, 0.010061769038233788, 0.008123228985963425, 0.00992346676839228, 0.010875330434380204, 0.011084205249918579, 0.023913935842966586, 0.02469652903171324, 0.016409546575259306, 0.015560624633087982, 0.014557638637793089, 0.021484351531581108, 0.04464433745535424, 0.03994716515562695, 0.017746374700337745, 0.007741768711565718, 0.012525130114632682, 0.008141415756981447, 0.020498163911875554, 0.013274010051493717, 0.011209775104521638, 0.013565849455878713, 0.008681761261136357, 0.01231907794247316, 0.013891679286315855, 0.025057567609782887, 0.031810348303224364, 0.04886674849226429, 0.008084826383520285, 0.007885293867343775, 0.018436108477060557, 0.044803555662132136, 0.026629033724325606, 0.020420387169118724, 0.008729307072140815, 0.009242464905122073, 0.05510527816907692], 'lossList': [0.0, -1.2270537942647934, 0.0, 2.4881120343506336, 0.0, 0.0, 0.0], 'rewardMean': 0.6270855692757455, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1116.0381591831213, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=81.78
