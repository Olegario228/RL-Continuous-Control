#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.43590750673834866, 'errorList': [], 'lossList': [0.0, -1.4341206073760986, 0.0, 31.667020444869994, 0.0, 0.0, 0.0], 'rewardMean': 0.47042336673936674, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.31342545895599
'totalSteps': 3840, 'rewardStep': 0.9553872686153767, 'errorList': [], 'lossList': [0.0, -1.4267620873451232, 0.0, 33.83611287593842, 0.0, 0.0, 0.0], 'rewardMean': 0.6320780006980368, 'totalEpisodes': 72, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.22112575509128
'totalSteps': 5120, 'rewardStep': 0.8388254462094117, 'errorList': [], 'lossList': [0.0, -1.422535389661789, 0.0, 42.977623014450074, 0.0, 0.0, 0.0], 'rewardMean': 0.6837648620758805, 'totalEpisodes': 78, 'stepsPerEpisode': 72, 'rewardPerEpisode': 64.54850374529123
'totalSteps': 6400, 'rewardStep': 0.6762116360782087, 'errorList': [], 'lossList': [0.0, -1.4289389210939407, 0.0, 111.78684688568116, 0.0, 0.0, 0.0], 'rewardMean': 0.6822542168763461, 'totalEpisodes': 117, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.82210495514659
'totalSteps': 7680, 'rewardStep': 0.8258841389545243, 'errorList': [], 'lossList': [0.0, -1.4165178054571153, 0.0, 54.25073664665222, 0.0, 0.0, 0.0], 'rewardMean': 0.7061925372227091, 'totalEpisodes': 137, 'stepsPerEpisode': 77, 'rewardPerEpisode': 59.66026445201361
'totalSteps': 8960, 'rewardStep': 0.9511779500393394, 'errorList': [2.2534305159255137, 1.826776635016164, 2.7161704930828674, 2.4064298856148865, 2.236740236686145, 3.1731558393443797, 3.1061148281363904, 2.0467502267563717, 3.0027783825432013, 3.09778717726309, 2.631577830505422, 2.3989932979248882, 2.4017090360342643, 3.284678223952126, 2.233380422678444, 2.4463099176340934, 1.9505322921555617, 1.8321438158355883, 3.298105754669893, 2.3788429072674595, 2.7259085795490114, 2.144107114586906, 2.054827574073166, 2.245370384543873, 1.8368890250646603, 3.337126852691192, 2.293617843803684, 2.3991022106484583, 2.350614349839597, 2.824117022265981, 3.14684154359263, 2.539642054013183, 3.2795774149131773, 3.3565066554330967, 2.46661830663665, 2.397095421621231, 2.652435432204837, 2.7927664325432904, 2.908633045806511, 2.9397173349637784, 3.1458742018444426, 3.215574712289533, 2.1510471744972914, 2.9562487471718537, 2.41446802406714, 3.093374669229777, 3.2480153708191786, 3.1505342608846885, 3.6857387435032773, 2.606092250570434], 'lossList': [0.0, -1.404138051867485, 0.0, 56.80231689453125, 0.0, 0.0, 0.0], 'rewardMean': 0.7411904533393706, 'totalEpisodes': 149, 'stepsPerEpisode': 104, 'rewardPerEpisode': 75.5374927876792, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.7886277390716031, 'errorList': [], 'lossList': [0.0, -1.3944033360481263, 0.0, 35.977369322776795, 0.0, 0.0, 0.0], 'rewardMean': 0.7471201140558996, 'totalEpisodes': 154, 'stepsPerEpisode': 219, 'rewardPerEpisode': 150.03041125611054
'totalSteps': 11520, 'rewardStep': 0.9488959535950665, 'errorList': [16.360958616510764, 4.807370842986503, 4.98005032493989, 12.08616922116607, 0.30140634768759356, 8.333407125135002, 0.17553974414772427, 10.589369892155743, 0.29357822391239, 7.868712146091713, 14.030393895049825, 2.9442325574987724, 9.902546921223454, 18.56631714093391, 8.800902264182472, 27.441310532770007, 15.150615788486844, 4.910233865659357, 1.7873749527982083, 17.664722235171933, 19.417665751284066, 2.9787680837647317, 19.367254804421343, 13.55861053852647, 2.2088685079560975, 5.772901767165192, 0.721195137807804, 9.336013561575257, 0.7173326281097624, 1.7701378890401405, 12.487196727819127, 1.9056809953129827, 8.594390753295187, 9.172771121289209, 5.599110489347044, 5.217703120600744, 16.59620433532872, 2.6297922969508964, 1.9900778416980947, 7.15134226135337, 1.6210072381406904, 2.7898376853499647, 4.35558831653121, 0.19612725193459674, 21.671709837926617, 0.18791309413094537, 1.8541750089795777, 9.726758094527097, 11.425559427955276, 16.84664594283311], 'lossList': [0.0, -1.3709726774692534, 0.0, 16.931606048345564, 0.0, 0.0, 0.0], 'rewardMean': 0.7695396517824737, 'totalEpisodes': 159, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.597918621250608, 'successfulTests': 3
'totalSteps': 12800, 'rewardStep': 0.7931993128005277, 'errorList': [], 'lossList': [0.0, -1.377736012339592, 0.0, 25.859702750444413, 0.0, 0.0, 0.0], 'rewardMean': 0.7719056178842791, 'totalEpisodes': 162, 'stepsPerEpisode': 156, 'rewardPerEpisode': 134.50626539196068
'totalSteps': 14080, 'rewardStep': 0.8388732437876313, 'errorList': [], 'lossList': [0.0, -1.386068913936615, 0.0, 13.614140693545341, 0.0, 0.0, 0.0], 'rewardMean': 0.8052990195890037, 'totalEpisodes': 163, 'stepsPerEpisode': 222, 'rewardPerEpisode': 184.69087414937815
'totalSteps': 15360, 'rewardStep': 0.7366174260033115, 'errorList': [], 'lossList': [0.0, -1.3741521042585374, 0.0, 6.395285994410515, 0.0, 0.0, 0.0], 'rewardMean': 0.8353700115155, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 959.8399544818058
'totalSteps': 16640, 'rewardStep': 0.8064661344877438, 'errorList': [], 'lossList': [0.0, -1.3842332077026367, 0.0, 5.311425771415234, 0.0, 0.0, 0.0], 'rewardMean': 0.8204778981027369, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.0528753915169
'totalSteps': 17920, 'rewardStep': 0.8922389753600467, 'errorList': [], 'lossList': [0.0, -1.3870963668823242, 0.0, 3.6478022895008326, 0.0, 0.0, 0.0], 'rewardMean': 0.8258192510178002, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.702127512931
'totalSteps': 19200, 'rewardStep': 0.7595826381474325, 'errorList': [], 'lossList': [0.0, -1.371475603580475, 0.0, 2.1225408594310284, 0.0, 0.0, 0.0], 'rewardMean': 0.8341563512247226, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.76068514881
'totalSteps': 20480, 'rewardStep': 0.9528305479630652, 'errorList': [0.0922886606384468, 0.06589789778521002, 0.058358031826174724, 0.071926317530889, 0.060483123558666196, 0.05737656125438311, 0.12646270650824135, 0.05560031847430238, 0.04453091545227441, 0.05179835972652583, 0.047082304685485514, 0.04622850427541208, 0.08866209678698621, 0.07119336966666154, 0.055798794374967715, 0.06472388651585158, 0.08307424636887469, 0.07267497080826057, 0.08530415477036073, 0.04400815083226434, 0.06899375925812824, 0.04861344893806696, 0.09588825982906564, 0.08016567409115462, 0.08277741389388227, 0.08246185765185424, 0.04865630598131656, 0.06548880019190895, 0.04261532213809348, 0.06803657076479756, 0.09358799947593549, 0.06666993030670264, 0.04589213015151167, 0.08368238090165461, 0.04143400410418603, 0.0646736330437911, 0.05018527245756474, 0.07226877137272543, 0.06864455761173173, 0.05370666269808372, 0.06585501810412717, 0.06348032016150555, 0.06601203203180685, 0.10481993427871666, 0.04805143423681457, 0.04922437904977168, 0.10531170242655923, 0.05862173613314719, 0.0507527050810865, 0.047634484550234696], 'lossList': [0.0, -1.3561880886554718, 0.0, 1.831748476177454, 0.0, 0.0, 0.0], 'rewardMean': 0.8468509921255768, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.0381678671404, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=108.54
