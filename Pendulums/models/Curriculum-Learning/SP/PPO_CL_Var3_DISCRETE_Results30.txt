#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 30
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8755310625235291, 'errorList': [], 'lossList': [0.0, -1.3985852706432342, 0.0, 28.8172762799263, 0.0, 0.0, 0.0], 'rewardMean': 0.7431005807326898, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.951881461253773
'totalSteps': 3840, 'rewardStep': 0.7240434810803753, 'errorList': [], 'lossList': [0.0, -1.3883588933944702, 0.0, 30.6139417219162, 0.0, 0.0, 0.0], 'rewardMean': 0.7367482141819183, 'totalEpisodes': 26, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.41354584075523
'totalSteps': 5120, 'rewardStep': 0.6585082841710177, 'errorList': [], 'lossList': [0.0, -1.399420103430748, 0.0, 16.227856035232545, 0.0, 0.0, 0.0], 'rewardMean': 0.7171882316791931, 'totalEpisodes': 27, 'stepsPerEpisode': 123, 'rewardPerEpisode': 101.09556843198314
'totalSteps': 6400, 'rewardStep': 0.9488612710731995, 'errorList': [102.95741627075397, 97.01201432372685, 92.46025351330186, 96.99022974243184, 99.04007658982533, 101.90446723534045, 101.84563527693871, 94.14628616705217, 101.0349458161643, 96.871653503968, 102.83445105221257, 102.96342008794673, 102.17864585070298, 100.8692497224357, 102.52483224347947, 91.73510648407697, 96.68241216131734, 102.10253428411207, 101.96616555520164, 100.43174882687718, 100.53428433831135, 94.40089217974983, 100.02874968718982, 96.07357825684169, 89.1720859908698, 88.80135819227458, 99.77340231209237, 95.5013258845417, 103.40265276544297, 95.47575671689546, 103.5620406622551, 102.59252945170422, 91.71046749364956, 95.66877089258712, 99.28076388275275, 99.65982117352446, 104.1055082918356, 99.48820405472246, 97.21429009367169, 101.68862741890402, 101.66983064523387, 98.45638351211366, 102.7604351102602, 96.43681971471186, 95.25224031493707, 101.30456415313773, 100.53196546824167, 102.04478728431542, 102.10546809310604, 98.14181950931379], 'lossList': [0.0, -1.410182209610939, 0.0, 16.873179924488067, 0.0, 0.0, 0.0], 'rewardMean': 0.7635228395579944, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 958.9682218704281, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.7076252272994096, 'errorList': [], 'lossList': [0.0, -1.4087771248817444, 0.0, 288.6007498931885, 0.0, 0.0, 0.0], 'rewardMean': 0.7542065708482303, 'totalEpisodes': 72, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.533384859944166
'totalSteps': 8960, 'rewardStep': 0.8584851367955951, 'errorList': [], 'lossList': [0.0, -1.4054527378082275, 0.0, 151.50557342529297, 0.0, 0.0, 0.0], 'rewardMean': 0.769103508840711, 'totalEpisodes': 120, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.470888084463088
'totalSteps': 10240, 'rewardStep': 0.4967757588200804, 'errorList': [], 'lossList': [0.0, -1.4040674144029617, 0.0, 64.78785968780518, 0.0, 0.0, 0.0], 'rewardMean': 0.7350625400881321, 'totalEpisodes': 152, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.222300832976819
'totalSteps': 11520, 'rewardStep': 0.8485718860150749, 'errorList': [], 'lossList': [0.0, -1.4005012148618698, 0.0, 57.938168649673464, 0.0, 0.0, 0.0], 'rewardMean': 0.7476746896355703, 'totalEpisodes': 174, 'stepsPerEpisode': 56, 'rewardPerEpisode': 49.700991517822786
'totalSteps': 12800, 'rewardStep': 0.8064327369485083, 'errorList': [], 'lossList': [0.0, -1.3641386556625366, 0.0, 45.67080722808838, 0.0, 0.0, 0.0], 'rewardMean': 0.753550494366864, 'totalEpisodes': 186, 'stepsPerEpisode': 121, 'rewardPerEpisode': 105.0810890925664
'totalSteps': 14080, 'rewardStep': 0.6552241949455466, 'errorList': [], 'lossList': [0.0, -1.3352036452293397, 0.0, 50.18857364177704, 0.0, 0.0, 0.0], 'rewardMean': 0.7580059039672337, 'totalEpisodes': 195, 'stepsPerEpisode': 215, 'rewardPerEpisode': 157.98123665274096
'totalSteps': 15360, 'rewardStep': 0.6485582138048627, 'errorList': [], 'lossList': [0.0, -1.3222954082489013, 0.0, 42.8390086889267, 0.0, 0.0, 0.0], 'rewardMean': 0.735308619095367, 'totalEpisodes': 205, 'stepsPerEpisode': 69, 'rewardPerEpisode': 57.626989360351374
'totalSteps': 16640, 'rewardStep': 0.3649731834179218, 'errorList': [], 'lossList': [0.0, -1.3167521983385087, 0.0, 15.22967149734497, 0.0, 0.0, 0.0], 'rewardMean': 0.6994015893291217, 'totalEpisodes': 210, 'stepsPerEpisode': 378, 'rewardPerEpisode': 295.5656650242035
'totalSteps': 17920, 'rewardStep': 0.7326285217586355, 'errorList': [], 'lossList': [0.0, -1.3039645290374755, 0.0, 8.581747654676438, 0.0, 0.0, 0.0], 'rewardMean': 0.7068136130878834, 'totalEpisodes': 216, 'stepsPerEpisode': 140, 'rewardPerEpisode': 99.6015644421871
'totalSteps': 19200, 'rewardStep': 0.4531470416088959, 'errorList': [], 'lossList': [0.0, -1.282139424085617, 0.0, 5.9602900469303135, 0.0, 0.0, 0.0], 'rewardMean': 0.6572421901414531, 'totalEpisodes': 221, 'stepsPerEpisode': 279, 'rewardPerEpisode': 201.00008368389925
'totalSteps': 20480, 'rewardStep': 0.26994169700824566, 'errorList': [], 'lossList': [0.0, -1.2850924521684646, 0.0, 5.3929504519701, 0.0, 0.0, 0.0], 'rewardMean': 0.6134738371123368, 'totalEpisodes': 225, 'stepsPerEpisode': 262, 'rewardPerEpisode': 194.41865213736267
'totalSteps': 21760, 'rewardStep': 0.9280604860534424, 'errorList': [], 'lossList': [0.0, -1.282715961933136, 0.0, 3.761648184657097, 0.0, 0.0, 0.0], 'rewardMean': 0.6204313720381214, 'totalEpisodes': 229, 'stepsPerEpisode': 234, 'rewardPerEpisode': 205.98552581989537
'totalSteps': 23040, 'rewardStep': 0.7144759095854714, 'errorList': [], 'lossList': [0.0, -1.260660868883133, 0.0, 2.9157989172637464, 0.0, 0.0, 0.0], 'rewardMean': 0.6422013871146605, 'totalEpisodes': 230, 'stepsPerEpisode': 894, 'rewardPerEpisode': 787.3987534472315
'totalSteps': 24320, 'rewardStep': 0.8943390818141506, 'errorList': [], 'lossList': [0.0, -1.2504946863651276, 0.0, 2.4136697852611544, 0.0, 0.0, 0.0], 'rewardMean': 0.646778106694568, 'totalEpisodes': 231, 'stepsPerEpisode': 366, 'rewardPerEpisode': 318.2769355848087
'totalSteps': 25600, 'rewardStep': 0.926557863910279, 'errorList': [], 'lossList': [0.0, -1.2345923632383347, 0.0, 2.9565477174520494, 0.0, 0.0, 0.0], 'rewardMean': 0.6587906193907452, 'totalEpisodes': 233, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.547064270436348
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=87.18
