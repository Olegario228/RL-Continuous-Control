#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 31
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6324348664108064, 'errorList': [], 'lossList': [0.0, -1.4222406631708144, 0.0, 26.879743020534516, 0.0, 0.0, 0.0], 'rewardMean': 0.7978622091607581, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.81111973182212
'totalSteps': 3840, 'rewardStep': 0.8403693505209163, 'errorList': [], 'lossList': [0.0, -1.4242901980876923, 0.0, 22.455256407260894, 0.0, 0.0, 0.0], 'rewardMean': 0.8120312562808109, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 255.20613735561352
'totalSteps': 5120, 'rewardStep': 0.7010127482662514, 'errorList': [], 'lossList': [0.0, -1.4249479389190673, 0.0, 16.55255002617836, 0.0, 0.0, 0.0], 'rewardMean': 0.784276629277171, 'totalEpisodes': 23, 'stepsPerEpisode': 72, 'rewardPerEpisode': 50.69641083423738
'totalSteps': 6400, 'rewardStep': 0.7043212459844532, 'errorList': [], 'lossList': [0.0, -1.415982192158699, 0.0, 20.11273698925972, 0.0, 0.0, 0.0], 'rewardMean': 0.7682855526186275, 'totalEpisodes': 24, 'stepsPerEpisode': 824, 'rewardPerEpisode': 585.8677893159934
'totalSteps': 7680, 'rewardStep': 0.8885697101942642, 'errorList': [], 'lossList': [0.0, -1.396452539563179, 0.0, 296.04002433776856, 0.0, 0.0, 0.0], 'rewardMean': 0.7883329122145669, 'totalEpisodes': 73, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.3162051154834415
'totalSteps': 8960, 'rewardStep': 0.6283120816057011, 'errorList': [], 'lossList': [0.0, -1.3906084311008453, 0.0, 118.54553913116455, 0.0, 0.0, 0.0], 'rewardMean': 0.7654727935561575, 'totalEpisodes': 121, 'stepsPerEpisode': 38, 'rewardPerEpisode': 33.44480919713403
'totalSteps': 10240, 'rewardStep': 0.7405704857727705, 'errorList': [], 'lossList': [0.0, -1.3812698233127594, 0.0, 64.66652002334595, 0.0, 0.0, 0.0], 'rewardMean': 0.7623600050832341, 'totalEpisodes': 151, 'stepsPerEpisode': 59, 'rewardPerEpisode': 41.83525395949338
'totalSteps': 11520, 'rewardStep': 0.8575254334937354, 'errorList': [], 'lossList': [0.0, -1.370794420838356, 0.0, 37.89910032272339, 0.0, 0.0, 0.0], 'rewardMean': 0.7729339415732898, 'totalEpisodes': 169, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.387534965670705
'totalSteps': 12800, 'rewardStep': 0.6054144274086671, 'errorList': [], 'lossList': [0.0, -1.3462764990329743, 0.0, 53.90199746131897, 0.0, 0.0, 0.0], 'rewardMean': 0.7561819901568275, 'totalEpisodes': 182, 'stepsPerEpisode': 122, 'rewardPerEpisode': 94.68167714299197
'totalSteps': 14080, 'rewardStep': 0.7794622552776335, 'errorList': [], 'lossList': [0.0, -1.322026827931404, 0.0, 35.75566734790802, 0.0, 0.0, 0.0], 'rewardMean': 0.73779926049352, 'totalEpisodes': 191, 'stepsPerEpisode': 158, 'rewardPerEpisode': 131.91547343024976
'totalSteps': 15360, 'rewardStep': 0.8857414366848958, 'errorList': [], 'lossList': [0.0, -1.3201489531993866, 0.0, 13.062055259943008, 0.0, 0.0, 0.0], 'rewardMean': 0.7631299175209288, 'totalEpisodes': 194, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.83361562080076
'totalSteps': 16640, 'rewardStep': 0.8211485449362949, 'errorList': [], 'lossList': [0.0, -1.3101929265260697, 0.0, 8.129534298181534, 0.0, 0.0, 0.0], 'rewardMean': 0.7612078369624667, 'totalEpisodes': 198, 'stepsPerEpisode': 143, 'rewardPerEpisode': 123.57528963023039
'totalSteps': 17920, 'rewardStep': 0.8313639794140418, 'errorList': [], 'lossList': [0.0, -1.288923283815384, 0.0, 9.089908570051193, 0.0, 0.0, 0.0], 'rewardMean': 0.7742429600772458, 'totalEpisodes': 200, 'stepsPerEpisode': 336, 'rewardPerEpisode': 280.2377840094006
'totalSteps': 19200, 'rewardStep': 0.7936300700649803, 'errorList': [], 'lossList': [0.0, -1.2853080278635025, 0.0, 8.196693058013915, 0.0, 0.0, 0.0], 'rewardMean': 0.7831738424852984, 'totalEpisodes': 202, 'stepsPerEpisode': 452, 'rewardPerEpisode': 312.0423460454149
'totalSteps': 20480, 'rewardStep': 0.9131233245693199, 'errorList': [], 'lossList': [0.0, -1.2771463131904601, 0.0, 2.7497629928588867, 0.0, 0.0, 0.0], 'rewardMean': 0.7856292039228039, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.0634311165481
'totalSteps': 21760, 'rewardStep': 0.6996331646315328, 'errorList': [], 'lossList': [0.0, -1.2484347242116929, 0.0, 2.6314223909378054, 0.0, 0.0, 0.0], 'rewardMean': 0.7927613122253871, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.3703042918307
'totalSteps': 23040, 'rewardStep': 0.932506407024272, 'errorList': [0.1379758090480012, 0.1507032549017321, 0.0830735713286767, 0.16851687758245423, 0.19322583342262428, 0.14363812515951757, 0.09449512967453579, 0.14332966955048834, 0.12586933137254938, 0.04428569822515823, 0.04200284566292495, 0.13561683246248193, 0.048331376978851176, 0.07199149046891871, 0.10180303811312556, 0.14017164345414326, 0.07890367168282138, 0.052393954147973966, 0.09218243132443085, 0.07830291329333458, 0.07356627529598123, 0.10976999290902244, 0.09982822774101702, 0.11857538149253621, 0.05441227049867551, 0.09300000786545037, 0.05567914946047318, 0.06518684850176619, 0.08736431854579743, 0.08380685398328713, 0.07533470042622682, 0.0979160256236268, 0.05024097740453992, 0.07820217230705144, 0.06433743758827212, 0.09371053160054561, 0.088621504055537, 0.14350577158496666, 0.06321207253518828, 0.1359534900586816, 0.13606033024774097, 0.048219166708764485, 0.07291891968401316, 0.05412768058462704, 0.07711740553769512, 0.1118971422011947, 0.16950380326929118, 0.08955257057177395, 0.22647348752944427, 0.10483744550724873], 'lossList': [0.0, -1.2269440466165542, 0.0, 2.1187656366825105, 0.0, 0.0, 0.0], 'rewardMean': 0.8119549043505373, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.7405432917983, 'successfulTests': 49
'totalSteps': 24320, 'rewardStep': 0.7473566397210881, 'errorList': [], 'lossList': [0.0, -1.194047525525093, 0.0, 1.3510299816727638, 0.0, 0.0, 0.0], 'rewardMean': 0.8009380249732727, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.17607314856
'totalSteps': 25600, 'rewardStep': 0.9438970189866152, 'errorList': [0.17522824919217025, 0.11324971406674232, 0.13195041300544819, 0.16504930388720557, 0.018708454036933296, 0.18957702853510458, 0.17564412206780605, 0.10685362553415242, 0.10957718112275552, 0.17896644052676894, 0.04369171993057505, 0.03593095975860859, 0.033097313204154266, 0.14003484969224014, 0.03857707369549244, 0.1050944491927951, 0.02346372573424415, 0.0586777857040396, 0.05354639224185034, 0.2156999371887525, 0.050713696312211065, 0.04376047830947686, 0.2008115053594351, 0.11342399490634059, 0.021425718100600056, 0.06887843271652636, 0.12427936121887939, 0.246569542483593, 0.13716773195039308, 0.10705611315876516, 0.05855831243769049, 0.1599121775201788, 0.14652209695293175, 0.17666677351796942, 0.06581187070443234, 0.06467250678384942, 0.036233398392031206, 0.00836955781822882, 0.0926499089067689, 0.22703591737827594, 0.0704936761837357, 0.04731197563999013, 0.08004358641987533, 0.11189280853462626, 0.10429306026492408, 0.09702525702766154, 0.02145583572990443, 0.13918279355768778, 0.1390631367863229, 0.22008069693719265], 'lossList': [0.0, -1.1509229707717896, 0.0, 1.2124814592301845, 0.0, 0.0, 0.0], 'rewardMean': 0.8347862841310676, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.3408969455031, 'successfulTests': 45
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=23040, timeSpent=111.03
