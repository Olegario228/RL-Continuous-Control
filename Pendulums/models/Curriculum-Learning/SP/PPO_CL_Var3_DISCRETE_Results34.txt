#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 34
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7930420769534675, 'errorList': [], 'lossList': [0.0, -1.4114374756813048, 0.0, 23.55243757247925, 0.0, 0.0, 0.0], 'rewardMean': 0.578556091364528, 'totalEpisodes': 20, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.512210118521697
'totalSteps': 3840, 'rewardStep': 0.46685752540477443, 'errorList': [], 'lossList': [0.0, -1.422312022447586, 0.0, 22.715906591415404, 0.0, 0.0, 0.0], 'rewardMean': 0.5413232360446102, 'totalEpisodes': 27, 'stepsPerEpisode': 113, 'rewardPerEpisode': 72.55741603431655
'totalSteps': 5120, 'rewardStep': 0.623695989080709, 'errorList': [], 'lossList': [0.0, -1.432346613407135, 0.0, 19.172786490917204, 0.0, 0.0, 0.0], 'rewardMean': 0.5619164243036349, 'totalEpisodes': 31, 'stepsPerEpisode': 208, 'rewardPerEpisode': 168.83040591018786
'totalSteps': 6400, 'rewardStep': 0.6727221060452948, 'errorList': [], 'lossList': [0.0, -1.4396896713972092, 0.0, 29.235621185302733, 0.0, 0.0, 0.0], 'rewardMean': 0.5840775606519669, 'totalEpisodes': 33, 'stepsPerEpisode': 866, 'rewardPerEpisode': 658.5288995931718
'totalSteps': 7680, 'rewardStep': 0.4737246936034704, 'errorList': [], 'lossList': [0.0, -1.4386319094896316, 0.0, 270.04358055114744, 0.0, 0.0, 0.0], 'rewardMean': 0.5656854161438841, 'totalEpisodes': 85, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.464380210235482
'totalSteps': 8960, 'rewardStep': 0.9655879609289028, 'errorList': [239.95754370768591, 238.71267354130498, 235.03558921876183, 124.20698555995297, 249.7328271500799, 193.12515148708124, 252.4732204155808, 258.70139060814046, 219.52101475805196, 238.18267859358647, 256.66396042707925, 249.6078281966151, 179.62913779495008, 193.2430228085013, 229.48756241322923, 163.67200619957362, 230.06244981419738, 72.81069551787832, 215.80602211488846, 229.59994133303078, 141.5629912971424, 216.3921272082574, 196.49082756651467, 199.03747308463107, 238.64048651952527, 252.68910962584602, 197.57938970562302, 237.45508396335237, 217.26612687535382, 184.29979394079933, 211.18395315892204, 232.55485753975915, 249.33428549295482, 257.19571310972896, 241.50992241786028, 250.01919503810515, 254.0349216480802, 239.43684519817052, 124.6734120083563, 176.07920009925127, 215.76405632635525, 264.98774709027555, 142.25304177416655, 243.7031018249687, 235.3138749221535, 236.60656537168623, 243.0852965306016, 215.20893691859143, 218.80364480073166, 241.56139874026337], 'lossList': [0.0, -1.4375909852981568, 0.0, 82.61410339355469, 0.0, 0.0, 0.0], 'rewardMean': 0.6228143511131725, 'totalEpisodes': 119, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.687997154565136, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.845533527677406, 'errorList': [], 'lossList': [0.0, -1.4274881291389465, 0.0, 44.981114616394045, 0.0, 0.0, 0.0], 'rewardMean': 0.6506542481837017, 'totalEpisodes': 141, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.720083503925571
'totalSteps': 11520, 'rewardStep': 0.9644493392488751, 'errorList': [83.15279511248241, 13.51256554577515, 74.33771562874671, 27.106361174730832, 95.23965075275632, 12.70222388535185, 77.49089177494193, 46.19106133024076, 65.25719876921568, 21.551258052173985, 52.906124817321526, 67.3949500136792, 48.31501127985024, 17.894206928605797, 27.5716663693799, 21.832955627886953, 96.77997912483282, 57.29325321636744, 68.01622741869151, 8.517833028404736, 74.23304299721674, 63.320341807211435, 2.6851468852496376, 103.57639717406597, 3.8189653528176652, 0.3029693509429203, 93.9097912154772, 82.29080198269018, 49.63325048879132, 3.750430017671503, 43.162563080694966, 7.872249657494084, 9.559021859789183, 34.798851890517845, 27.01200463055032, 62.609058459488836, 100.5179561110898, 71.12310209821155, 81.89567013431429, 5.354656187800246, 42.03663954789394, 27.27686999046871, 18.274675108547413, 10.581673505334741, 31.47331705760761, 51.87406003720225, 26.857940373296497, 12.548765502350767, 28.603770987234356, 1.9531042831162568], 'lossList': [0.0, -1.4195024263858795, 0.0, 16.322131292819975, 0.0, 0.0, 0.0], 'rewardMean': 0.6855203694131654, 'totalEpisodes': 151, 'stepsPerEpisode': 112, 'rewardPerEpisode': 89.94859612948376, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.5518106166936239, 'errorList': [], 'lossList': [0.0, -1.4127182698249816, 0.0, 40.6812128162384, 0.0, 0.0, 0.0], 'rewardMean': 0.6721493941412112, 'totalEpisodes': 160, 'stepsPerEpisode': 65, 'rewardPerEpisode': 42.663576928377104
'totalSteps': 14080, 'rewardStep': 0.7862929699956753, 'errorList': [], 'lossList': [0.0, -1.3849600559473039, 0.0, 12.918462111353874, 0.0, 0.0, 0.0], 'rewardMean': 0.7143716805632199, 'totalEpisodes': 164, 'stepsPerEpisode': 175, 'rewardPerEpisode': 150.22930352565228
'totalSteps': 15360, 'rewardStep': 0.9697134618168656, 'errorList': [31.544127724742072, 50.23473660955263, 14.039447124394094, 9.147258783837087, 18.88614627484285, 29.14476821005945, 15.056003334043675, 25.013086060151753, 52.79634679531039, 38.54775417559142, 27.668827810457284, 9.639815793507264, 8.219707129881154, 6.099300693915185, 40.42548238974705, 7.344000059012667, 47.71983858633195, 4.80120897395755, 5.816223685563805, 4.276617052826523, 20.189609087662262, 9.719133170778461, 2.6171001745185034, 38.99868313887976, 12.88309111594899, 0.614586039645493, 2.570943907719498, 8.961042423052351, 2.2935733899361335, 53.10615390512055, 11.909628428831335, 63.639182168005775, 5.528191360664794, 16.02151436887578, 60.28961441081894, 55.940740625946255, 12.330044889195891, 8.158084193171675, 26.645143541248473, 9.47969372341108, 0.9979335475736113, 6.991569374571017, 7.227643176038457, 12.558699960983857, 13.212111988392007, 18.003266619502195, 22.798153501876445, 73.09804058748031, 6.117275881484926, 11.68060085788956], 'lossList': [0.0, -1.3540269231796265, 0.0, 12.125761938095092, 0.0, 0.0, 0.0], 'rewardMean': 0.7320388190495597, 'totalEpisodes': 171, 'stepsPerEpisode': 89, 'rewardPerEpisode': 79.49497278188832, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5831500976650563, 'errorList': [], 'lossList': [0.0, -1.3469624549150467, 0.0, 6.604228432774544, 0.0, 0.0, 0.0], 'rewardMean': 0.7436680762755878, 'totalEpisodes': 175, 'stepsPerEpisode': 70, 'rewardPerEpisode': 54.999185935299614
'totalSteps': 17920, 'rewardStep': 0.6300322651306967, 'errorList': [], 'lossList': [0.0, -1.3398708397150039, 0.0, 5.095904869437217, 0.0, 0.0, 0.0], 'rewardMean': 0.7443017038805867, 'totalEpisodes': 179, 'stepsPerEpisode': 231, 'rewardPerEpisode': 191.5145514231875
'totalSteps': 19200, 'rewardStep': 0.4337753226243118, 'errorList': [], 'lossList': [0.0, -1.3437999773025513, 0.0, 3.885767834186554, 0.0, 0.0, 0.0], 'rewardMean': 0.7204070255384883, 'totalEpisodes': 183, 'stepsPerEpisode': 296, 'rewardPerEpisode': 221.04899904294052
'totalSteps': 20480, 'rewardStep': 0.7208255164664616, 'errorList': [], 'lossList': [0.0, -1.3320262759923935, 0.0, 3.342363001704216, 0.0, 0.0, 0.0], 'rewardMean': 0.7451171078247875, 'totalEpisodes': 185, 'stepsPerEpisode': 647, 'rewardPerEpisode': 564.9567559454904
'totalSteps': 21760, 'rewardStep': 0.5937063412495756, 'errorList': [], 'lossList': [0.0, -1.3508294743299485, 0.0, 2.32416949942708, 0.0, 0.0, 0.0], 'rewardMean': 0.7079289458568547, 'totalEpisodes': 187, 'stepsPerEpisode': 311, 'rewardPerEpisode': 253.1570383109387
'totalSteps': 23040, 'rewardStep': 0.9310906740913332, 'errorList': [0.3514394638524073, 0.071570496442284, 0.05969194071326277, 0.36777516898495327, 0.18766873228233233, 0.05476106080547222, 0.22805958043491117, 0.3536145874458874, 0.11089642476768727, 0.28971105358435967, 0.12884639663720307, 0.12609077045085476, 0.1381588726911384, 0.22327241763935635, 0.15013923501891482, 0.36664547230772493, 0.5990093112794322, 0.2439505571587786, 0.3456581081885034, 0.07112557248497968, 0.09527808980721275, 0.060705546156085324, 0.18854513017349805, 0.3054187364598727, 0.0897839702304516, 0.08362060346682548, 0.33756918565105287, 0.08332899403043056, 0.5310832486707602, 0.2213595453105126, 0.7187832078648466, 0.12932019402510714, 0.09465597699259022, 0.2569699438789667, 0.1997259040366247, 0.06309290113795774, 0.24662040438174052, 0.07497912288933367, 0.23325466912990225, 0.1827947719290069, 0.12767452610812396, 0.15901787234618814, 0.12301392254385762, 0.06774780541448518, 0.07046497161753958, 0.17117787066526824, 0.1588384995913475, 0.2682918329245177, 0.27410990481220243, 0.2888442482858872], 'lossList': [0.0, -1.3728856533765792, 0.0, 1.5909044399857522, 0.0, 0.0, 0.0], 'rewardMean': 0.7164846604982474, 'totalEpisodes': 189, 'stepsPerEpisode': 703, 'rewardPerEpisode': 608.0266173151397, 'successfulTests': 29
'totalSteps': 24320, 'rewardStep': 0.5227510084376075, 'errorList': [], 'lossList': [0.0, -1.34476447224617, 0.0, 1.6362138926982879, 0.0, 0.0, 0.0], 'rewardMean': 0.6723148274171207, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.9734921233303
'totalSteps': 25600, 'rewardStep': 0.8819945414249718, 'errorList': [], 'lossList': [0.0, -1.3221302741765977, 0.0, 2.5756265565752985, 0.0, 0.0, 0.0], 'rewardMean': 0.7053332198902555, 'totalEpisodes': 191, 'stepsPerEpisode': 266, 'rewardPerEpisode': 230.8193080427928
#maxSuccessfulTests=29, maxSuccessfulTestsAtStep=23040, timeSpent=153.85
