#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 35
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9248751944253679, 'errorList': [], 'lossList': [0.0, -1.407356454730034, 0.0, 27.204375113248826, 0.0, 0.0, 0.0], 'rewardMean': 0.8623370774621655, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.1036728716099
'totalSteps': 3840, 'rewardStep': 0.6395554860146846, 'errorList': [], 'lossList': [0.0, -1.3999825465679168, 0.0, 33.922742812633516, 0.0, 0.0, 0.0], 'rewardMean': 0.7880765469796719, 'totalEpisodes': 11, 'stepsPerEpisode': 263, 'rewardPerEpisode': 201.59114429570403
'totalSteps': 5120, 'rewardStep': 0.7105120977953714, 'errorList': [], 'lossList': [0.0, -1.4016036814451218, 0.0, 25.431722968816757, 0.0, 0.0, 0.0], 'rewardMean': 0.7686854346835967, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.9747604482272
'totalSteps': 6400, 'rewardStep': 0.8612668484829329, 'errorList': [], 'lossList': [0.0, -1.3974467796087264, 0.0, 20.9412685623765, 0.0, 0.0, 0.0], 'rewardMean': 0.787201717443464, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.486297484792
'totalSteps': 7680, 'rewardStep': 0.9588237649799338, 'errorList': [83.64033218300017, 79.75044756806493, 87.4615747403852, 76.84738303116698, 81.72905435743277, 78.62943030363627, 89.44561142623147, 86.58201259633906, 83.77301560233374, 77.42560142763193, 79.95826612762455, 82.25125942256535, 88.05113094944228, 88.53917625823809, 83.17030224274212, 85.46470181636299, 84.40898981814232, 76.7087581010859, 85.83563259160096, 78.66332282468784, 86.11771955838547, 80.24731084500198, 89.07622061626469, 86.5219079668327, 85.91904129395903, 88.28561697649073, 81.09347961750933, 88.43291557239334, 87.40945445394208, 89.68750163281447, 80.70673393637621, 85.0953028482977, 86.56121686955578, 86.65271728447517, 86.47417641278052, 86.78545287707715, 85.34384442236676, 89.06511939328793, 83.62739483603315, 86.21848089366065, 86.7103524570525, 82.30349770680033, 82.55197476013441, 76.06814572604009, 87.42557361885481, 82.47505416022928, 85.79655916487673, 89.05679876006828, 75.73429871925595, 87.78984342884418], 'lossList': [0.0, -1.3942852145433426, 0.0, 385.8677212524414, 0.0, 0.0, 0.0], 'rewardMean': 0.8158053920328756, 'totalEpisodes': 61, 'stepsPerEpisode': 45, 'rewardPerEpisode': 38.00303222797117, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.772448005644003, 'errorList': [], 'lossList': [0.0, -1.3927399253845214, 0.0, 211.73580955505372, 0.0, 0.0, 0.0], 'rewardMean': 0.8096114796916082, 'totalEpisodes': 108, 'stepsPerEpisode': 69, 'rewardPerEpisode': 61.95236697841147
'totalSteps': 10240, 'rewardStep': 0.5499943174802444, 'errorList': [], 'lossList': [0.0, -1.3883223366737365, 0.0, 100.15213956832886, 0.0, 0.0, 0.0], 'rewardMean': 0.7771593344151877, 'totalEpisodes': 163, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.510543753118043
'totalSteps': 11520, 'rewardStep': 0.5256949468085949, 'errorList': [], 'lossList': [0.0, -1.37884521484375, 0.0, 59.28389181137085, 0.0, 0.0, 0.0], 'rewardMean': 0.7492188469033441, 'totalEpisodes': 191, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.70084577983445
'totalSteps': 12800, 'rewardStep': 0.7765088775256396, 'errorList': [], 'lossList': [0.0, -1.3711553078889847, 0.0, 51.0815334892273, 0.0, 0.0, 0.0], 'rewardMean': 0.7519478499655736, 'totalEpisodes': 211, 'stepsPerEpisode': 51, 'rewardPerEpisode': 38.18107374430976
'totalSteps': 14080, 'rewardStep': 0.5019039788870716, 'errorList': [], 'lossList': [0.0, -1.3652650254964829, 0.0, 33.52157990932464, 0.0, 0.0, 0.0], 'rewardMean': 0.7221583518043844, 'totalEpisodes': 222, 'stepsPerEpisode': 58, 'rewardPerEpisode': 37.98225640073633
'totalSteps': 15360, 'rewardStep': 0.7013554099600187, 'errorList': [], 'lossList': [0.0, -1.3515488958358766, 0.0, 31.51112790584564, 0.0, 0.0, 0.0], 'rewardMean': 0.6998063733578495, 'totalEpisodes': 232, 'stepsPerEpisode': 69, 'rewardPerEpisode': 57.332829041703775
'totalSteps': 16640, 'rewardStep': 0.5169326660276563, 'errorList': [], 'lossList': [0.0, -1.3478988373279572, 0.0, 16.41872892141342, 0.0, 0.0, 0.0], 'rewardMean': 0.6875440913591466, 'totalEpisodes': 238, 'stepsPerEpisode': 4, 'rewardPerEpisode': 1.9917043728632646
'totalSteps': 17920, 'rewardStep': 0.8079356063509038, 'errorList': [], 'lossList': [0.0, -1.3455654722452164, 0.0, 11.165866841077804, 0.0, 0.0, 0.0], 'rewardMean': 0.6972864422146999, 'totalEpisodes': 241, 'stepsPerEpisode': 162, 'rewardPerEpisode': 126.55490417969708
'totalSteps': 19200, 'rewardStep': 0.8385811523597066, 'errorList': [], 'lossList': [0.0, -1.3255485671758651, 0.0, 21.736935725212096, 0.0, 0.0, 0.0], 'rewardMean': 0.6950178726023774, 'totalEpisodes': 246, 'stepsPerEpisode': 76, 'rewardPerEpisode': 63.95064031967701
'totalSteps': 20480, 'rewardStep': 0.8759068877261061, 'errorList': [], 'lossList': [0.0, -1.306990920305252, 0.0, 7.163373777270317, 0.0, 0.0, 0.0], 'rewardMean': 0.6867261848769945, 'totalEpisodes': 250, 'stepsPerEpisode': 102, 'rewardPerEpisode': 84.45416746049376
'totalSteps': 21760, 'rewardStep': 0.9013503394098396, 'errorList': [], 'lossList': [0.0, -1.2772362130880355, 0.0, 15.662248841524125, 0.0, 0.0, 0.0], 'rewardMean': 0.6996164182535782, 'totalEpisodes': 254, 'stepsPerEpisode': 61, 'rewardPerEpisode': 50.57177344839474
'totalSteps': 23040, 'rewardStep': 0.5690909081014438, 'errorList': [], 'lossList': [0.0, -1.2556603932380677, 0.0, 6.169078981280327, 0.0, 0.0, 0.0], 'rewardMean': 0.7015260773156982, 'totalEpisodes': 255, 'stepsPerEpisode': 304, 'rewardPerEpisode': 246.3865042749269
'totalSteps': 24320, 'rewardStep': 0.7178575746335225, 'errorList': [], 'lossList': [0.0, -1.2417893832921982, 0.0, 3.312894349992275, 0.0, 0.0, 0.0], 'rewardMean': 0.7207423400981908, 'totalEpisodes': 255, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.0871619357586
'totalSteps': 25600, 'rewardStep': 0.9117997696526069, 'errorList': [], 'lossList': [0.0, -1.2446859365701675, 0.0, 3.8211400906741617, 0.0, 0.0, 0.0], 'rewardMean': 0.7342714293108876, 'totalEpisodes': 255, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.1178251858062
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=90.26
