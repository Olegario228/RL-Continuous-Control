#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 36
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7407526851551015, 'errorList': [], 'lossList': [0.0, -1.4381537955999375, 0.0, 33.31262235403061, 0.0, 0.0, 0.0], 'rewardMean': 0.6287732934272772, 'totalEpisodes': 12, 'stepsPerEpisode': 75, 'rewardPerEpisode': 63.584726357374365
'totalSteps': 3840, 'rewardStep': 0.8637334784555419, 'errorList': [], 'lossList': [0.0, -1.4452659833431243, 0.0, 27.569838765263558, 0.0, 0.0, 0.0], 'rewardMean': 0.7070933551033655, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.4813934950321
'totalSteps': 5120, 'rewardStep': 0.5759928308585094, 'errorList': [], 'lossList': [0.0, -1.4195507580041886, 0.0, 26.616081984639166, 0.0, 0.0, 0.0], 'rewardMean': 0.6743182240421515, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.5796509636435
'totalSteps': 6400, 'rewardStep': 0.9463638398839381, 'errorList': [96.49076986590744, 95.25510191478796, 95.12200908451746, 93.83496453868325, 93.17856684651171, 96.46447092726149, 90.41582946247428, 94.60444676380973, 92.25144414072481, 87.32634377984705, 89.88424424337313, 94.85565158254565, 90.35044521769136, 92.359965849465, 95.15715502496471, 91.59623661016028, 95.37784103356518, 92.18171629412687, 95.60144959222734, 97.45493392231141, 92.33014776517645, 95.42117092483589, 92.63885078103931, 94.56608462388628, 87.36385722872323, 94.95501165052602, 95.24725847409043, 97.08271236948855, 96.71748914583463, 95.98467585372669, 91.21205795198652, 87.41561121065025, 94.18545092930965, 92.40695062836294, 93.64027195760713, 95.26933869765351, 96.77879675163574, 96.91621823091424, 81.48405369172053, 76.90204778178952, 85.93479196590515, 93.228668685827, 92.87010130921075, 90.30192286094835, 91.21296147250217, 96.12033252650313, 86.68476741624363, 89.62690159940718, 87.88830996143147, 96.41662649514085], 'lossList': [0.0, -1.4143244582414627, 0.0, 23.917352037727834, 0.0, 0.0, 0.0], 'rewardMean': 0.7287273472105088, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.0192218254592, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.5743374558131223, 'errorList': [], 'lossList': [0.0, -1.398862166404724, 0.0, 468.35583877563477, 0.0, 0.0, 0.0], 'rewardMean': 0.7029956986442777, 'totalEpisodes': 69, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.11903618013804
'totalSteps': 8960, 'rewardStep': 0.7416148612854013, 'errorList': [], 'lossList': [0.0, -1.3993352574110032, 0.0, 238.97610252380372, 0.0, 0.0, 0.0], 'rewardMean': 0.7085127218787239, 'totalEpisodes': 112, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.12585857280869
'totalSteps': 10240, 'rewardStep': 0.9234295513855003, 'errorList': [], 'lossList': [0.0, -1.3964388167858124, 0.0, 83.29764764785767, 0.0, 0.0, 0.0], 'rewardMean': 0.735377325567071, 'totalEpisodes': 152, 'stepsPerEpisode': 24, 'rewardPerEpisode': 22.611259952340486
'totalSteps': 11520, 'rewardStep': 0.9000702396495444, 'errorList': [], 'lossList': [0.0, -1.3900491058826447, 0.0, 37.834654321670534, 0.0, 0.0, 0.0], 'rewardMean': 0.7536765382429014, 'totalEpisodes': 178, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.345819597408408
'totalSteps': 12800, 'rewardStep': 0.9011278747151477, 'errorList': [], 'lossList': [0.0, -1.3751337075233458, 0.0, 42.890494709014895, 0.0, 0.0, 0.0], 'rewardMean': 0.768421671890126, 'totalEpisodes': 195, 'stepsPerEpisode': 101, 'rewardPerEpisode': 82.99769906321993
'totalSteps': 14080, 'rewardStep': 0.41419176899677873, 'errorList': [], 'lossList': [0.0, -1.3622172957658767, 0.0, 21.866128730773926, 0.0, 0.0, 0.0], 'rewardMean': 0.7581614586198586, 'totalEpisodes': 204, 'stepsPerEpisode': 92, 'rewardPerEpisode': 57.440799728580714
'totalSteps': 15360, 'rewardStep': 0.967092225299399, 'errorList': [25.94958635626178, 120.59139391857356, 51.742599722312455, 46.882838825082636, 106.9478918705768, 105.52366898258303, 1.371169536114148, 146.66143739892163, 11.564245953691463, 7.762366774656839, 68.28617846203574, 16.9400315236006, 54.605048277822206, 49.46833154476204, 7.342875989594009, 61.767543285910634, 61.77792416887057, 29.667495047044152, 53.84229240073127, 12.292321278624378, 87.56802895812218, 61.931990323430334, 35.88408371710944, 104.92279054359807, 1.0311005301663292, 113.93510663894753, 87.20571113847004, 79.38222378852923, 105.38970877917605, 65.79911122517024, 92.95448857122337, 109.60352051964591, 8.306317486790004, 14.293887300663144, 76.0417251588259, 61.935272497285844, 49.61309035998469, 11.52777841441119, 16.313671423436794, 144.27420811135005, 25.788216837251504, 2.214762648768599, 115.38029785150246, 22.0864209747723, 167.05086857464175, 17.863393521979845, 119.43636789551181, 85.56412049538018, 36.412205243455354, 23.752694155315147], 'lossList': [0.0, -1.3475388008356095, 0.0, 13.56611988544464, 0.0, 0.0, 0.0], 'rewardMean': 0.7807954126342883, 'totalEpisodes': 209, 'stepsPerEpisode': 60, 'rewardPerEpisode': 55.537259447191445, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6982040678910268, 'errorList': [], 'lossList': [0.0, -1.3167257690429688, 0.0, 23.512996542453767, 0.0, 0.0, 0.0], 'rewardMean': 0.7642424715778368, 'totalEpisodes': 213, 'stepsPerEpisode': 188, 'rewardPerEpisode': 150.289063316224
'totalSteps': 17920, 'rewardStep': 0.5688985911947417, 'errorList': [], 'lossList': [0.0, -1.3078587996959685, 0.0, 13.007164742946625, 0.0, 0.0, 0.0], 'rewardMean': 0.7635330476114601, 'totalEpisodes': 216, 'stepsPerEpisode': 402, 'rewardPerEpisode': 298.92308350467283
'totalSteps': 19200, 'rewardStep': 0.7709622276096958, 'errorList': [], 'lossList': [0.0, -1.287392302751541, 0.0, 7.645912133455276, 0.0, 0.0, 0.0], 'rewardMean': 0.7459928863840358, 'totalEpisodes': 217, 'stepsPerEpisode': 1257, 'rewardPerEpisode': 1007.0510850645678
'totalSteps': 20480, 'rewardStep': 0.5826559081456886, 'errorList': [], 'lossList': [0.0, -1.2612398016452788, 0.0, 7.935671781301498, 0.0, 0.0, 0.0], 'rewardMean': 0.7468247316172923, 'totalEpisodes': 221, 'stepsPerEpisode': 106, 'rewardPerEpisode': 76.9842931504331
'totalSteps': 21760, 'rewardStep': 0.6353153937163758, 'errorList': [], 'lossList': [0.0, -1.2463191092014312, 0.0, 13.291132813692093, 0.0, 0.0, 0.0], 'rewardMean': 0.7361947848603899, 'totalEpisodes': 224, 'stepsPerEpisode': 323, 'rewardPerEpisode': 255.19161784060316
'totalSteps': 23040, 'rewardStep': 0.8730775022799658, 'errorList': [], 'lossList': [0.0, -1.22463205575943, 0.0, 3.160315768569708, 0.0, 0.0, 0.0], 'rewardMean': 0.7311595799498363, 'totalEpisodes': 224, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.4938650567074
'totalSteps': 24320, 'rewardStep': 0.639677483351704, 'errorList': [], 'lossList': [0.0, -1.228425600528717, 0.0, 2.551082263290882, 0.0, 0.0, 0.0], 'rewardMean': 0.7051203043200525, 'totalEpisodes': 225, 'stepsPerEpisode': 214, 'rewardPerEpisode': 168.4015680365511
'totalSteps': 25600, 'rewardStep': 0.935413845518286, 'errorList': [0.15277729609074756, 0.10463334895774776, 0.3072709121264133, 0.17811505611041892, 0.07624617127409808, 0.08094633540070162, 0.1570423820782863, 0.13074185785093476, 0.07291164604926305, 0.15850732567985684, 0.07856393101587572, 0.06851623018906862, 0.09295541654631378, 0.07663634990070346, 0.11525576691514648, 0.0739979387611253, 0.09333933492137918, 0.2501583925159846, 0.2221567371326117, 0.09201540985904885, 0.24641729928331146, 0.07499855064327798, 0.07701390805466121, 0.23939502046775102, 0.09207834600443641, 0.14423675055691074, 0.1404166479436757, 0.10314490520408794, 0.08572000403885321, 0.06986791123948234, 0.06980761284593757, 0.05516075200436977, 0.12406775745607751, 0.1293338420028783, 0.07369940916674084, 0.10493204184099135, 0.05859711598127207, 0.2066001090549416, 0.0552503548117696, 0.1254700439687922, 0.10688896947046472, 0.046194727831342465, 0.09328804235280459, 0.09889611264189446, 0.0985924937867123, 0.084760873231943, 0.05422291461946576, 0.20190119599708536, 0.0868620078371291, 0.21117722246924023], 'lossList': [0.0, -1.1967504608631134, 0.0, 2.1262910328805447, 0.0, 0.0, 0.0], 'rewardMean': 0.7085489014003662, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.1561548972422, 'successfulTests': 42
#maxSuccessfulTests=42, maxSuccessfulTestsAtStep=25600, timeSpent=132.51
