#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 37
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.817604565351236, 'errorList': [], 'lossList': [0.0, -1.4439547497034073, 0.0, 25.714645471572876, 0.0, 0.0, 0.0], 'rewardMean': 0.627817578778555, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.8221866151944
'totalSteps': 3840, 'rewardStep': 0.8819465627971335, 'errorList': [], 'lossList': [0.0, -1.452749683856964, 0.0, 40.4849648976326, 0.0, 0.0, 0.0], 'rewardMean': 0.7125272401180812, 'totalEpisodes': 14, 'stepsPerEpisode': 487, 'rewardPerEpisode': 393.1458591944858
'totalSteps': 5120, 'rewardStep': 0.6909629918169633, 'errorList': [], 'lossList': [0.0, -1.4402446401119233, 0.0, 19.477284516096116, 0.0, 0.0, 0.0], 'rewardMean': 0.7071361780428018, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 915.6590066339056
'totalSteps': 6400, 'rewardStep': 0.5316313862161144, 'errorList': [], 'lossList': [0.0, -1.424210580587387, 0.0, 30.624091376066207, 0.0, 0.0, 0.0], 'rewardMean': 0.6720352196774643, 'totalEpisodes': 15, 'stepsPerEpisode': 560, 'rewardPerEpisode': 442.0557397243816
'totalSteps': 7680, 'rewardStep': 0.9360842405694068, 'errorList': [81.4044300788416, 84.9910075412122, 87.57471733015906, 85.61225866320964, 84.83909816063283, 84.08978693236, 86.07901293621525, 86.19220907863306, 85.92481597799431, 85.20739959599995, 76.57602556348338, 85.13621080027488, 86.09567925083844, 84.05040863363456, 82.19089047920458, 76.48328803197367, 85.33693384349851, 82.38482136722993, 83.93770183908207, 84.09820396338851, 76.67787276889844, 83.45612718975458, 75.41682880345492, 82.75695000455171, 87.00575300995358, 84.68600516748054, 84.49692626548655, 79.38906514320794, 86.4091188959035, 80.00384887388026, 76.28707565672974, 70.09509171995337, 84.98226568865557, 83.17904651027948, 84.65321362653425, 78.87992582993013, 81.41690136637807, 88.11768356543745, 85.07279191631453, 82.8355513286184, 84.9736550503183, 85.41417882044405, 79.43189212060115, 86.31574534082914, 86.12234188419393, 86.4517361099768, 86.61505101927882, 79.27676982538716, 74.16235556468484, 86.03599952455924], 'lossList': [0.0, -1.4120950651168824, 0.0, 363.481990814209, 0.0, 0.0, 0.0], 'rewardMean': 0.7160433898261213, 'totalEpisodes': 75, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.080024071184686, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.9104093263498652, 'errorList': [], 'lossList': [0.0, -1.4030453336238862, 0.0, 140.77717636108397, 0.0, 0.0, 0.0], 'rewardMean': 0.7438099521866561, 'totalEpisodes': 129, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.041068490811028
'totalSteps': 10240, 'rewardStep': 0.6777108857487658, 'errorList': [], 'lossList': [0.0, -1.3765686464309692, 0.0, 69.25470783233642, 0.0, 0.0, 0.0], 'rewardMean': 0.73554756888192, 'totalEpisodes': 168, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.527708070875358
'totalSteps': 11520, 'rewardStep': 0.7992088462571989, 'errorList': [], 'lossList': [0.0, -1.3716723144054412, 0.0, 45.5716290473938, 0.0, 0.0, 0.0], 'rewardMean': 0.7426210441458398, 'totalEpisodes': 198, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.420479302278093
'totalSteps': 12800, 'rewardStep': 0.4888577784681149, 'errorList': [], 'lossList': [0.0, -1.3832237094640731, 0.0, 24.96318380355835, 0.0, 0.0, 0.0], 'rewardMean': 0.7172447175780674, 'totalEpisodes': 214, 'stepsPerEpisode': 68, 'rewardPerEpisode': 45.96789467133569
'totalSteps': 14080, 'rewardStep': 0.8780941951049005, 'errorList': [], 'lossList': [0.0, -1.3940820974111556, 0.0, 19.720311069488524, 0.0, 0.0, 0.0], 'rewardMean': 0.76125107786797, 'totalEpisodes': 224, 'stepsPerEpisode': 113, 'rewardPerEpisode': 85.13185887928546
'totalSteps': 15360, 'rewardStep': 0.9074706686167529, 'errorList': [], 'lossList': [0.0, -1.3985620522499085, 0.0, 33.21679789543152, 0.0, 0.0, 0.0], 'rewardMean': 0.7702376881945217, 'totalEpisodes': 235, 'stepsPerEpisode': 36, 'rewardPerEpisode': 34.01934580266763
'totalSteps': 16640, 'rewardStep': 0.6124767794982989, 'errorList': [], 'lossList': [0.0, -1.3935235369205474, 0.0, 13.528475670814514, 0.0, 0.0, 0.0], 'rewardMean': 0.7432907098646382, 'totalEpisodes': 240, 'stepsPerEpisode': 257, 'rewardPerEpisode': 209.59284390073358
'totalSteps': 17920, 'rewardStep': 0.771560418368297, 'errorList': [], 'lossList': [0.0, -1.3715906929969788, 0.0, 8.0152911901474, 0.0, 0.0, 0.0], 'rewardMean': 0.7513504525197716, 'totalEpisodes': 247, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.445910418910208
'totalSteps': 19200, 'rewardStep': 0.5893073750967933, 'errorList': [], 'lossList': [0.0, -1.3412440365552902, 0.0, 14.0390771484375, 0.0, 0.0, 0.0], 'rewardMean': 0.7571180514078395, 'totalEpisodes': 251, 'stepsPerEpisode': 513, 'rewardPerEpisode': 426.57734141640935
'totalSteps': 20480, 'rewardStep': 0.6198347987682691, 'errorList': [], 'lossList': [0.0, -1.342737216949463, 0.0, 7.717438905835151, 0.0, 0.0, 0.0], 'rewardMean': 0.7254931072277258, 'totalEpisodes': 254, 'stepsPerEpisode': 444, 'rewardPerEpisode': 371.4676294503555
'totalSteps': 21760, 'rewardStep': 0.8032144251828114, 'errorList': [], 'lossList': [0.0, -1.3433142668008804, 0.0, 5.401778671145439, 0.0, 0.0, 0.0], 'rewardMean': 0.7147736171110204, 'totalEpisodes': 257, 'stepsPerEpisode': 260, 'rewardPerEpisode': 229.2340004306182
'totalSteps': 23040, 'rewardStep': 0.9385969533980552, 'errorList': [0.25895293916655326, 0.35676052169056865, 0.35883654344113786, 0.27644523797895637, 0.4107236921316274, 0.43092615830152303, 0.2925571714885902, 0.26125136820066336, 0.2929833149130583, 0.32270904698121783, 0.3388289902833846, 0.34881620226729637, 0.28544043093864746, 0.28578611437514057, 0.31096973939294315, 0.37932050466465156, 0.4452097444111234, 0.3373560852749992, 0.2773167347134023, 0.304651182456989, 0.38006978416525905, 0.27331816112623897, 0.29798409451845, 0.2564781706018817, 0.2983900846339406, 0.3106190606710061, 0.3076260443357656, 0.30670471121940945, 0.30566740286128047, 0.3482798821369484, 0.4064006007713983, 0.43381897461964825, 0.38047003960038134, 0.3463214813678253, 0.3678195584983655, 0.2819688575110853, 0.26730008545447437, 0.33567800690381994, 0.31715788302319253, 0.26547291955363633, 0.39301678236000337, 0.39901567272460264, 0.2907917497911594, 0.3237872629602449, 0.2897325890509086, 0.2733487887289423, 0.4691965257179744, 0.3603375037105509, 0.35012485664582216, 0.29259059553919126], 'lossList': [0.0, -1.3363621711730957, 0.0, 3.9608896416425705, 0.0, 0.0, 0.0], 'rewardMean': 0.7408622238759491, 'totalEpisodes': 258, 'stepsPerEpisode': 188, 'rewardPerEpisode': 165.72129417839622, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.6378914303482943, 'errorList': [], 'lossList': [0.0, -1.3136344581842423, 0.0, 2.7909398663043974, 0.0, 0.0, 0.0], 'rewardMean': 0.7247304822850588, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1026.4218185768025
'totalSteps': 25600, 'rewardStep': 0.5097412617199186, 'errorList': [], 'lossList': [0.0, -1.3010262215137482, 0.0, 3.611934966444969, 0.0, 0.0, 0.0], 'rewardMean': 0.7268188306102391, 'totalEpisodes': 259, 'stepsPerEpisode': 1143, 'rewardPerEpisode': 903.6689675226422
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=110.18
