#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 38
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.657187940170172, 'errorList': [], 'lossList': [0.0, -1.4219418781995774, 0.0, 27.9683056306839, 0.0, 0.0, 0.0], 'rewardMean': 0.711256886558614, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.155583018468704
'totalSteps': 3840, 'rewardStep': 0.9612180606389581, 'errorList': [], 'lossList': [0.0, -1.4156839829683303, 0.0, 27.140998220443727, 0.0, 0.0, 0.0], 'rewardMean': 0.7945772779187287, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 374.2310408960704
'totalSteps': 5120, 'rewardStep': 0.8097549536927952, 'errorList': [], 'lossList': [0.0, -1.4097404754161835, 0.0, 27.785908808112143, 0.0, 0.0, 0.0], 'rewardMean': 0.7983716968622453, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.4483637329215
'totalSteps': 6400, 'rewardStep': 0.7022961852205006, 'errorList': [], 'lossList': [0.0, -1.3900028657913208, 0.0, 20.386204572319983, 0.0, 0.0, 0.0], 'rewardMean': 0.7791565945338964, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.0193214434457
'totalSteps': 7680, 'rewardStep': 0.862452262321396, 'errorList': [], 'lossList': [0.0, -1.3680953699350358, 0.0, 414.07465995788573, 0.0, 0.0, 0.0], 'rewardMean': 0.7930392058318131, 'totalEpisodes': 68, 'stepsPerEpisode': 58, 'rewardPerEpisode': 50.27432605915599
'totalSteps': 8960, 'rewardStep': 0.7760178938159641, 'errorList': [], 'lossList': [0.0, -1.3640576159954072, 0.0, 271.25215740203856, 0.0, 0.0, 0.0], 'rewardMean': 0.7906075898295489, 'totalEpisodes': 130, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.166589679790016
'totalSteps': 10240, 'rewardStep': 0.9198057637222378, 'errorList': [], 'lossList': [0.0, -1.36310506939888, 0.0, 70.79037624359131, 0.0, 0.0, 0.0], 'rewardMean': 0.806757361566135, 'totalEpisodes': 175, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.815592023957723
'totalSteps': 11520, 'rewardStep': 0.6810398391655388, 'errorList': [], 'lossList': [0.0, -1.3554219526052476, 0.0, 31.743727083206178, 0.0, 0.0, 0.0], 'rewardMean': 0.7927887479660688, 'totalEpisodes': 213, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.395526319957035
'totalSteps': 12800, 'rewardStep': 0.7248436845887604, 'errorList': [], 'lossList': [0.0, -1.3459724795818329, 0.0, 23.166561884880068, 0.0, 0.0, 0.0], 'rewardMean': 0.785994241628338, 'totalEpisodes': 233, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.742602840818847
'totalSteps': 14080, 'rewardStep': 0.4070516387001069, 'errorList': [], 'lossList': [0.0, -1.3234252309799195, 0.0, 23.29464174747467, 0.0, 0.0, 0.0], 'rewardMean': 0.7501668222036431, 'totalEpisodes': 245, 'stepsPerEpisode': 141, 'rewardPerEpisode': 103.59392407488387
'totalSteps': 15360, 'rewardStep': 0.6120404164985891, 'errorList': [], 'lossList': [0.0, -1.2930662697553634, 0.0, 18.15524269580841, 0.0, 0.0, 0.0], 'rewardMean': 0.7456520698364847, 'totalEpisodes': 254, 'stepsPerEpisode': 98, 'rewardPerEpisode': 73.01805688546678
'totalSteps': 16640, 'rewardStep': 0.517008056420097, 'errorList': [], 'lossList': [0.0, -1.282299313545227, 0.0, 14.584963414669037, 0.0, 0.0, 0.0], 'rewardMean': 0.7012310694145986, 'totalEpisodes': 260, 'stepsPerEpisode': 145, 'rewardPerEpisode': 102.81170800314501
'totalSteps': 17920, 'rewardStep': 0.881325130094165, 'errorList': [], 'lossList': [0.0, -1.2795560073852539, 0.0, 9.280074188113213, 0.0, 0.0, 0.0], 'rewardMean': 0.7083880870547355, 'totalEpisodes': 263, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.881325130094165
'totalSteps': 19200, 'rewardStep': 0.8191719886038702, 'errorList': [], 'lossList': [0.0, -1.2760118734836579, 0.0, 8.914535558223724, 0.0, 0.0, 0.0], 'rewardMean': 0.7200756673930725, 'totalEpisodes': 264, 'stepsPerEpisode': 687, 'rewardPerEpisode': 553.5041506661493
'totalSteps': 20480, 'rewardStep': 0.8601084304955361, 'errorList': [], 'lossList': [0.0, -1.2627238351106644, 0.0, 7.26080023676157, 0.0, 0.0, 0.0], 'rewardMean': 0.7198412842104865, 'totalEpisodes': 264, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.1517309899987
'totalSteps': 21760, 'rewardStep': 0.8703509498930769, 'errorList': [], 'lossList': [0.0, -1.246659682393074, 0.0, 4.47869656920433, 0.0, 0.0, 0.0], 'rewardMean': 0.7292745898181979, 'totalEpisodes': 264, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1042.8926037500355
'totalSteps': 23040, 'rewardStep': 0.7870519175455416, 'errorList': [], 'lossList': [0.0, -1.2112517619132996, 0.0, 3.6089620970189573, 0.0, 0.0, 0.0], 'rewardMean': 0.7159992052005282, 'totalEpisodes': 264, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.2669678602122
'totalSteps': 24320, 'rewardStep': 0.8298404186854448, 'errorList': [], 'lossList': [0.0, -1.1496751582622529, 0.0, 2.482168290019035, 0.0, 0.0, 0.0], 'rewardMean': 0.7308792631525188, 'totalEpisodes': 264, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.0113666525265
'totalSteps': 25600, 'rewardStep': 0.9694271564738496, 'errorList': [0.04349553353927516, 0.038858113323748805, 0.06782446692540534, 0.03798805823840658, 0.04437494582574619, 0.058591842044955975, 0.037099418663931974, 0.05489341629502598, 0.03270145648228168, 0.036172511782029786, 0.07723981180483984, 0.04896207891777074, 0.0661490053681151, 0.070102392294481, 0.05675431546457563, 0.056557784536480006, 0.061021447803574144, 0.03497544617022294, 0.03640834773939407, 0.04947265983192628, 0.03821069713009826, 0.03320903400984288, 0.041951578714215544, 0.03962886834667905, 0.03402158949016498, 0.03441312391389563, 0.05912398086892026, 0.04560700688202986, 0.0750882815775956, 0.03678169269413797, 0.04662672064840418, 0.06434814879991606, 0.03372147414643637, 0.06337486112771129, 0.031008053353124558, 0.03270752619122391, 0.04733879312443922, 0.06842228607386923, 0.06667135814990428, 0.04994610848977229, 0.06508773213960502, 0.044473457023821654, 0.03356753312178436, 0.06228302948979427, 0.05199273202584089, 0.03194517879066555, 0.047608411639585284, 0.06691416611629213, 0.04923341704268297, 0.05565254465854167], 'lossList': [0.0, -1.112577002644539, 0.0, 1.9400454709120094, 0.0, 0.0, 0.0], 'rewardMean': 0.7553376103410276, 'totalEpisodes': 264, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1191.830989615455, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=88.48
