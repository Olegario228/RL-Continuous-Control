#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764237844444184, 'errorList': [], 'lossList': [0.0, -1.4117648071050644, 0.0, 34.56727179527283, 0.0, 0.0, 0.0], 'rewardMean': 0.8487569592979127, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.540084423708798
'totalSteps': 3840, 'rewardStep': 0.8301048750001292, 'errorList': [], 'lossList': [0.0, -1.4150108939409256, 0.0, 39.124620037078856, 0.0, 0.0, 0.0], 'rewardMean': 0.8425395978653182, 'totalEpisodes': 84, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.84030782755015
'totalSteps': 5120, 'rewardStep': 0.6848389580966191, 'errorList': [], 'lossList': [0.0, -1.4063812613487243, 0.0, 28.86642453432083, 0.0, 0.0, 0.0], 'rewardMean': 0.8031144379231434, 'totalEpisodes': 90, 'stepsPerEpisode': 117, 'rewardPerEpisode': 92.77188059566005
'totalSteps': 6400, 'rewardStep': 0.8870965981085961, 'errorList': [], 'lossList': [0.0, -1.3919325250387191, 0.0, 114.4594779586792, 0.0, 0.0, 0.0], 'rewardMean': 0.819910869960234, 'totalEpisodes': 134, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.352382703287045
'totalSteps': 7680, 'rewardStep': 0.487801898546015, 'errorList': [], 'lossList': [0.0, -1.3752729338407517, 0.0, 63.58239751815796, 0.0, 0.0, 0.0], 'rewardMean': 0.7645593747245307, 'totalEpisodes': 159, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.00320679994297
'totalSteps': 8960, 'rewardStep': 0.7655578394659738, 'errorList': [], 'lossList': [0.0, -1.3586180621385575, 0.0, 59.97598644256592, 0.0, 0.0, 0.0], 'rewardMean': 0.7647020125447368, 'totalEpisodes': 177, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.355649492233681
'totalSteps': 10240, 'rewardStep': 0.8748934465013779, 'errorList': [], 'lossList': [0.0, -1.3463210350275039, 0.0, 25.595444014072417, 0.0, 0.0, 0.0], 'rewardMean': 0.778475941789317, 'totalEpisodes': 184, 'stepsPerEpisode': 100, 'rewardPerEpisode': 82.41616341817813
'totalSteps': 11520, 'rewardStep': 0.5364675257205272, 'errorList': [], 'lossList': [0.0, -1.331646182537079, 0.0, 12.758138732910156, 0.0, 0.0, 0.0], 'rewardMean': 0.7515861177816737, 'totalEpisodes': 189, 'stepsPerEpisode': 225, 'rewardPerEpisode': 167.23217787899537
'totalSteps': 12800, 'rewardStep': 0.6997820788860202, 'errorList': [], 'lossList': [0.0, -1.3160284692049027, 0.0, 13.343202221393586, 0.0, 0.0, 0.0], 'rewardMean': 0.7464057138921084, 'totalEpisodes': 195, 'stepsPerEpisode': 144, 'rewardPerEpisode': 112.54146481263173
'totalSteps': 14080, 'rewardStep': 0.9433751360503961, 'errorList': [16.387000627355892, 3.1260202361125096, 23.225675627544465, 17.81531378223458, 14.788127741598647, 1.5961486875708166, 4.172565203362813, 3.994445377673846, 2.5016293687141613, 17.50571884103107, 7.326357254394081, 0.6272890950071841, 10.522190813617035, 2.360301092287646, 5.795068197801251, 6.5481865802737556, 19.581633908488296, 7.7532558050981955, 12.698069152133625, 11.476401307090086, 8.063034780668742, 4.93037968776146, 15.103115590476627, 22.80042389366423, 7.773034449226306, 5.63010927485116, 6.359370307667335, 9.984111183002561, 1.8945124536145228, 3.4514429409552787, 5.953718960393757, 22.967389964429895, 30.93152411961368, 22.222364284695292, 2.863632552684649, 15.547988333521436, 2.875655333092853, 10.372210347908279, 5.130996043814153, 6.577411317396208, 28.038653962192473, 10.21167725985525, 6.9292810393601, 4.398734702686817, 1.7432245671730833, 15.798574814907383, 2.997920204168451, 12.601638434376337, 6.129984953843992, 16.129990719696437], 'lossList': [0.0, -1.3319053423404694, 0.0, 7.8034785985946655, 0.0, 0.0, 0.0], 'rewardMean': 0.7486342140820074, 'totalEpisodes': 199, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.332455903935315, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7112695360316155, 'errorList': [], 'lossList': [0.0, -1.348679711818695, 0.0, 8.144203310012818, 0.0, 0.0, 0.0], 'rewardMean': 0.742118789240727, 'totalEpisodes': 204, 'stepsPerEpisode': 183, 'rewardPerEpisode': 153.89737642689616
'totalSteps': 16640, 'rewardStep': 0.12006623932595156, 'errorList': [], 'lossList': [0.0, -1.338888209462166, 0.0, 11.223052167296409, 0.0, 0.0, 0.0], 'rewardMean': 0.6711149256733092, 'totalEpisodes': 208, 'stepsPerEpisode': 479, 'rewardPerEpisode': 324.41210525297765
'totalSteps': 17920, 'rewardStep': 0.6155984982521345, 'errorList': [], 'lossList': [0.0, -1.3286660313606262, 0.0, 6.079810009598732, 0.0, 0.0, 0.0], 'rewardMean': 0.6641908796888607, 'totalEpisodes': 211, 'stepsPerEpisode': 298, 'rewardPerEpisode': 246.2199921167455
'totalSteps': 19200, 'rewardStep': 0.6134448613719848, 'errorList': [], 'lossList': [0.0, -1.311199128627777, 0.0, 3.123101298213005, 0.0, 0.0, 0.0], 'rewardMean': 0.6368257060151997, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 848.0033414820792
'totalSteps': 20480, 'rewardStep': 0.8998541905884386, 'errorList': [], 'lossList': [0.0, -1.2824363452196121, 0.0, 2.0066900627315043, 0.0, 0.0, 0.0], 'rewardMean': 0.678030935219442, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.8387321489174
'totalSteps': 21760, 'rewardStep': 0.8590766354855703, 'errorList': [], 'lossList': [0.0, -1.2525294536352158, 0.0, 1.6739253461360932, 0.0, 0.0, 0.0], 'rewardMean': 0.6873828148214016, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.0772029599343
'totalSteps': 23040, 'rewardStep': 0.8915111834512206, 'errorList': [], 'lossList': [0.0, -1.2434293580055238, 0.0, 1.095459447130561, 0.0, 0.0, 0.0], 'rewardMean': 0.689044588516386, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.6815719996137
'totalSteps': 24320, 'rewardStep': 0.9336660815273948, 'errorList': [0.013406267521717044, 0.018911369187212132, 0.026973931856041723, 0.02266638109413678, 0.038227705890064016, 0.024436609521470087, 0.029370171726298664, 0.0067135214042727195, 0.029288122910422192, 0.016073575371213737, 0.02341627681919637, 0.02487062682321776, 0.0272884254167595, 0.04026156674788557, 0.023838512027827014, 0.019733648286205475, 0.0200069503909432, 0.02363031436162471, 0.010334162187798923, 0.025646464868918414, 0.0073610909580084875, 0.03486189425359987, 0.021480401769259434, 0.020725564683612887, 0.020809190754980524, 0.024905380423029606, 0.03500173355634365, 0.023083831207524535, 0.05682136553891269, 0.01955314790289989, 0.016479545679076807, 0.02994413924801073, 0.036307168256385504, 0.018101056632538786, 0.006890112587433134, 0.007008348109872637, 0.009222787349829691, 0.03662065377147881, 0.014249253260604673, 0.022483440141499545, 0.008654840106940983, 0.015140724877257003, 0.0231037824679916, 0.025449054309314295, 0.04061995751530619, 0.02522701567449278, 0.0420655006224537, 0.028172258382593584, 0.01402963157910825, 0.016289029873631972], 'lossList': [0.0, -1.2304248750209807, 0.0, 1.0795692777074875, 0.0, 0.0, 0.0], 'rewardMean': 0.7287644440970727, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.9309484486935, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=97.14
