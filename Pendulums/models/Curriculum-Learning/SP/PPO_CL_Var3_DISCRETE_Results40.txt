#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 40
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9360084744127758, 'errorList': [], 'lossList': [0.0, -1.4352493596076965, 0.0, 30.507024736404418, 0.0, 0.0, 0.0], 'rewardMean': 0.9157950328104714, 'totalEpisodes': 8, 'stepsPerEpisode': 529, 'rewardPerEpisode': 377.10779752439424
'totalSteps': 3840, 'rewardStep': 0.7557452886750211, 'errorList': [], 'lossList': [0.0, -1.4207161974906921, 0.0, 32.01956871271133, 0.0, 0.0, 0.0], 'rewardMean': 0.8624451180986545, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 203.96302985448395
'totalSteps': 5120, 'rewardStep': 0.7637633211487607, 'errorList': [], 'lossList': [0.0, -1.4165323793888092, 0.0, 27.78206905066967, 0.0, 0.0, 0.0], 'rewardMean': 0.8377746688611811, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.0989296707711
'totalSteps': 6400, 'rewardStep': 0.9641597693966607, 'errorList': [67.05767594957, 63.4553266476269, 58.932382389361564, 64.00115850398664, 64.41293764137451, 66.35425769595803, 66.62684494493884, 63.03864083313444, 66.18774883370834, 63.10808724998023, 67.02491983214134, 66.84578259435395, 66.81898802177282, 65.07809805130448, 65.2855252336893, 62.83136186121646, 63.23980596155001, 66.31870614322902, 66.7822590507021, 63.95160341262676, 65.37773737482173, 61.43733394483756, 65.22160827981033, 61.19032042444045, 59.33898809278824, 58.601633658340184, 65.71402408724498, 62.6728617936004, 67.30647085786732, 62.72523819448084, 65.5929582748041, 64.75906040057347, 54.9721361754216, 63.21930200849565, 63.61571700644406, 63.777159999133914, 67.82131485936353, 63.783320006962356, 64.42254039290903, 64.45220361358638, 64.9396730009905, 63.67470715499297, 65.23930772422092, 62.703678070352446, 62.09580309561989, 66.50432994445778, 63.646120947477726, 66.00937904920032, 67.00917148775234, 63.5072195921863], 'lossList': [0.0, -1.4044303733110428, 0.0, 23.92006703555584, 0.0, 0.0, 0.0], 'rewardMean': 0.863051688968277, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.5310283831257, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.838704642021643, 'errorList': [], 'lossList': [0.0, -1.406995005607605, 0.0, 455.13022544860837, 0.0, 0.0, 0.0], 'rewardMean': 0.8589938478105047, 'totalEpisodes': 68, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.2778337364319
'totalSteps': 8960, 'rewardStep': 0.5571759852658654, 'errorList': [], 'lossList': [0.0, -1.4048634040355683, 0.0, 285.49471603393556, 0.0, 0.0, 0.0], 'rewardMean': 0.8158770103041276, 'totalEpisodes': 124, 'stepsPerEpisode': 28, 'rewardPerEpisode': 18.765730627476152
'totalSteps': 10240, 'rewardStep': 0.8963235092583194, 'errorList': [], 'lossList': [0.0, -1.3937172037363053, 0.0, 134.22161041259767, 0.0, 0.0, 0.0], 'rewardMean': 0.8259328226734016, 'totalEpisodes': 174, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.782549440880821
'totalSteps': 11520, 'rewardStep': 0.671465975180383, 'errorList': [], 'lossList': [0.0, -1.3688237488269805, 0.0, 70.26160442352295, 0.0, 0.0, 0.0], 'rewardMean': 0.8087698396186218, 'totalEpisodes': 205, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.418418743852445
'totalSteps': 12800, 'rewardStep': 0.6820859230825399, 'errorList': [], 'lossList': [0.0, -1.3456411254405976, 0.0, 75.57749959945679, 0.0, 0.0, 0.0], 'rewardMean': 0.7961014479650136, 'totalEpisodes': 235, 'stepsPerEpisode': 29, 'rewardPerEpisode': 22.891937268648622
'totalSteps': 14080, 'rewardStep': 0.5188622394631944, 'errorList': [], 'lossList': [0.0, -1.3452651393413544, 0.0, 41.04931161880493, 0.0, 0.0, 0.0], 'rewardMean': 0.7584295127905164, 'totalEpisodes': 250, 'stepsPerEpisode': 106, 'rewardPerEpisode': 86.75416632252092
'totalSteps': 15360, 'rewardStep': 0.6231468904335763, 'errorList': [], 'lossList': [0.0, -1.3542764919996262, 0.0, 47.53842723846436, 0.0, 0.0, 0.0], 'rewardMean': 0.7271433543925964, 'totalEpisodes': 261, 'stepsPerEpisode': 70, 'rewardPerEpisode': 60.323167755010225
'totalSteps': 16640, 'rewardStep': 0.9372877272786896, 'errorList': [138.86320563412787, 107.3119228640673, 44.008426338986894, 140.7212682066808, 79.36955714680025, 45.49597297033487, 10.437772347097077, 103.70509572143504, 45.3366838242131, 124.35738133464653, 27.78465846518437, 4.103116672859747, 110.46819809756083, 119.07499195807155, 44.56942705708074, 54.201185675351404, 124.38214832752342, 90.48825080925806, 74.16895328713127, 105.33970542373743, 75.43086302028635, 85.9074721453218, 58.79030296272265, 123.49745931995022, 84.26019859367219, 5.593178003922241, 13.371737914711268, 7.645715394387679, 49.25459259951981, 28.911709041108512, 6.572436775352693, 99.0886814033848, 117.98987624098854, 19.9317331483582, 106.21964100178621, 13.378966923268981, 59.98501294267739, 27.68044530135058, 106.79705680505103, 85.31000037477324, 56.60403968769429, 114.96752312478286, 9.80071911683402, 32.908639538936505, 63.55644794097999, 2.7965184115431563, 15.480620420145392, 105.88196140684062, 25.56484813329228, 79.37488362765484], 'lossList': [0.0, -1.3638096737861634, 0.0, 27.867811226844786, 0.0, 0.0, 0.0], 'rewardMean': 0.7452975982529632, 'totalEpisodes': 268, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.039201353130714, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7500301874614863, 'errorList': [], 'lossList': [0.0, -1.368087176680565, 0.0, 13.982992079257965, 0.0, 0.0, 0.0], 'rewardMean': 0.7439242848842358, 'totalEpisodes': 273, 'stepsPerEpisode': 75, 'rewardPerEpisode': 56.549794091192965
'totalSteps': 19200, 'rewardStep': 0.5433565988256603, 'errorList': [], 'lossList': [0.0, -1.3701738268136978, 0.0, 35.00888194799423, 0.0, 0.0, 0.0], 'rewardMean': 0.7018439678271358, 'totalEpisodes': 279, 'stepsPerEpisode': 378, 'rewardPerEpisode': 292.48133855175195
'totalSteps': 20480, 'rewardStep': 0.6331594936031153, 'errorList': [], 'lossList': [0.0, -1.3726733916997909, 0.0, 16.681353594064714, 0.0, 0.0, 0.0], 'rewardMean': 0.681289452985283, 'totalEpisodes': 286, 'stepsPerEpisode': 164, 'rewardPerEpisode': 138.04522500551226
'totalSteps': 21760, 'rewardStep': 0.8877750103242679, 'errorList': [], 'lossList': [0.0, -1.3565951240062715, 0.0, 13.063230769634247, 0.0, 0.0, 0.0], 'rewardMean': 0.7143493554911232, 'totalEpisodes': 290, 'stepsPerEpisode': 63, 'rewardPerEpisode': 52.636952882440156
'totalSteps': 23040, 'rewardStep': 0.6889236041615082, 'errorList': [], 'lossList': [0.0, -1.366081777215004, 0.0, 5.467870090007782, 0.0, 0.0, 0.0], 'rewardMean': 0.6936093649814421, 'totalEpisodes': 294, 'stepsPerEpisode': 144, 'rewardPerEpisode': 119.09683818507828
'totalSteps': 24320, 'rewardStep': 0.8634567896935508, 'errorList': [], 'lossList': [0.0, -1.3760213708877564, 0.0, 4.480471972227097, 0.0, 0.0, 0.0], 'rewardMean': 0.7128084464327589, 'totalEpisodes': 297, 'stepsPerEpisode': 155, 'rewardPerEpisode': 133.8557747800228
'totalSteps': 25600, 'rewardStep': 0.7243651657822983, 'errorList': [], 'lossList': [0.0, -1.3773778450489045, 0.0, 4.289430240392685, 0.0, 0.0, 0.0], 'rewardMean': 0.7170363707027347, 'totalEpisodes': 300, 'stepsPerEpisode': 344, 'rewardPerEpisode': 278.11160148599345
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=110.61
