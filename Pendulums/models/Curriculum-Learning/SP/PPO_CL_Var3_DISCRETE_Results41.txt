#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 41
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8584804343243699, 'errorList': [], 'lossList': [0.0, -1.444619840979576, 0.0, 36.9149765253067, 0.0, 0.0, 0.0], 'rewardMean': 0.7085640764003625, 'totalEpisodes': 12, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.028341765022766
'totalSteps': 3840, 'rewardStep': 0.8945312046550165, 'errorList': [], 'lossList': [0.0, -1.4570085686445235, 0.0, 30.45998518407345, 0.0, 0.0, 0.0], 'rewardMean': 0.7705531191519137, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.9854716647518
'totalSteps': 5120, 'rewardStep': 0.6426887006587513, 'errorList': [], 'lossList': [0.0, -1.4410074639320374, 0.0, 29.687185053825377, 0.0, 0.0, 0.0], 'rewardMean': 0.7385870145286231, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.2280427395226
'totalSteps': 6400, 'rewardStep': 0.9462534970079105, 'errorList': [94.19534787159452, 92.99499728835454, 93.3836927230608, 91.92115407186446, 90.77466925318349, 94.09589795588748, 88.8435790253714, 92.4810810288803, 89.98038106179749, 85.45463613330233, 87.81868917349867, 92.74991420722147, 88.20279969550538, 90.39415038996943, 93.04103938292972, 89.2174810020535, 93.16819451858801, 89.80254615958533, 93.85784521556391, 94.89886451483267, 90.39661544248098, 93.6363610933857, 90.24968688523509, 92.76168014841417, 85.51512499632757, 93.18800462670352, 93.49387147506908, 95.07339705978738, 94.75468412290078, 94.16279689094448, 88.98545026666159, 86.07578187062717, 92.44339134190386, 90.28001330074743, 91.41733608620008, 93.07298746545642, 94.37662781525299, 94.9015891290112, 79.98749745766632, 78.78438897235208, 84.12724757251434, 90.88632531998626, 90.40222423096445, 88.49877012889166, 89.05681472156094, 94.2817735661283, 84.46608242790929, 87.71978235250315, 86.2078804463658, 94.09432062498021], 'lossList': [0.0, -1.4400340634584428, 0.0, 23.752558589577674, 0.0, 0.0, 0.0], 'rewardMean': 0.7801203110244805, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.7706869240055, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.7555704060604538, 'errorList': [], 'lossList': [0.0, -1.4233750212192535, 0.0, 439.8484605407715, 0.0, 0.0, 0.0], 'rewardMean': 0.7760286601971428, 'totalEpisodes': 62, 'stepsPerEpisode': 66, 'rewardPerEpisode': 57.58097491835213
'totalSteps': 8960, 'rewardStep': 0.8858457809439307, 'errorList': [], 'lossList': [0.0, -1.422860285639763, 0.0, 266.8516585540772, 0.0, 0.0, 0.0], 'rewardMean': 0.7917168203038267, 'totalEpisodes': 107, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.03345492221686
'totalSteps': 10240, 'rewardStep': 0.5310177976465269, 'errorList': [], 'lossList': [0.0, -1.4191926616430282, 0.0, 105.79857669830322, 0.0, 0.0, 0.0], 'rewardMean': 0.7591294424716644, 'totalEpisodes': 143, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.19850895113333
'totalSteps': 11520, 'rewardStep': 0.64671158971935, 'errorList': [], 'lossList': [0.0, -1.4143706810474397, 0.0, 58.72195165634155, 0.0, 0.0, 0.0], 'rewardMean': 0.7466385699436295, 'totalEpisodes': 166, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.84986673435078
'totalSteps': 12800, 'rewardStep': 0.8840034226223612, 'errorList': [], 'lossList': [0.0, -1.4025786989927291, 0.0, 36.92291023254395, 0.0, 0.0, 0.0], 'rewardMean': 0.7603750552115026, 'totalEpisodes': 182, 'stepsPerEpisode': 36, 'rewardPerEpisode': 31.307048097730565
'totalSteps': 14080, 'rewardStep': 0.646349816284941, 'errorList': [], 'lossList': [0.0, -1.3807627195119858, 0.0, 29.487171182632448, 0.0, 0.0, 0.0], 'rewardMean': 0.7691452649923611, 'totalEpisodes': 191, 'stepsPerEpisode': 92, 'rewardPerEpisode': 60.26625466126579
'totalSteps': 15360, 'rewardStep': 0.6260305433853754, 'errorList': [], 'lossList': [0.0, -1.365661271214485, 0.0, 17.227743873596193, 0.0, 0.0, 0.0], 'rewardMean': 0.7459002758984618, 'totalEpisodes': 197, 'stepsPerEpisode': 123, 'rewardPerEpisode': 87.92881450974622
'totalSteps': 16640, 'rewardStep': 0.8717715508279126, 'errorList': [], 'lossList': [0.0, -1.3586763536930084, 0.0, 14.233989119529724, 0.0, 0.0, 0.0], 'rewardMean': 0.7436243105157513, 'totalEpisodes': 202, 'stepsPerEpisode': 84, 'rewardPerEpisode': 66.46814556574029
'totalSteps': 17920, 'rewardStep': 0.8561941269187747, 'errorList': [], 'lossList': [0.0, -1.3578215390443802, 0.0, 22.66057985305786, 0.0, 0.0, 0.0], 'rewardMean': 0.7649748531417537, 'totalEpisodes': 206, 'stepsPerEpisode': 144, 'rewardPerEpisode': 121.31274511403868
'totalSteps': 19200, 'rewardStep': 0.8008531772620001, 'errorList': [], 'lossList': [0.0, -1.3245162552595138, 0.0, 6.683364983201027, 0.0, 0.0, 0.0], 'rewardMean': 0.7504348211671625, 'totalEpisodes': 208, 'stepsPerEpisode': 390, 'rewardPerEpisode': 307.61107313436264
'totalSteps': 20480, 'rewardStep': 0.6463671788202442, 'errorList': [], 'lossList': [0.0, -1.3058228874206543, 0.0, 8.557119656801223, 0.0, 0.0, 0.0], 'rewardMean': 0.7395144984431418, 'totalEpisodes': 210, 'stepsPerEpisode': 441, 'rewardPerEpisode': 386.1542935787208
'totalSteps': 21760, 'rewardStep': 0.6959981129189815, 'errorList': [], 'lossList': [0.0, -1.2702826005220413, 0.0, 2.7203605711460113, 0.0, 0.0, 0.0], 'rewardMean': 0.7205297316406467, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1000.0048469220477
'totalSteps': 23040, 'rewardStep': 0.8329741632157603, 'errorList': [], 'lossList': [0.0, -1.237401141524315, 0.0, 4.183061671257019, 0.0, 0.0, 0.0], 'rewardMean': 0.7507253681975701, 'totalEpisodes': 212, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.63780204623029
'totalSteps': 24320, 'rewardStep': 0.8747180425470815, 'errorList': [], 'lossList': [0.0, -1.2325332194566727, 0.0, 1.96737197175622, 0.0, 0.0, 0.0], 'rewardMean': 0.7735260134803432, 'totalEpisodes': 213, 'stepsPerEpisode': 227, 'rewardPerEpisode': 205.66590512629145
'totalSteps': 25600, 'rewardStep': 0.9605406500309209, 'errorList': [0.16014690352667382, 3.2067705986946993, 0.10208083358512521, 0.5593330943897019, 1.6934820622038307, 1.4996180581301948, 0.6648393466871843, 0.5533074196906994, 0.41301384804120694, 0.6042302398492263, 23.633478631189927, 2.112239098845607, 0.5619948558471575, 4.197806537555076, 0.11618802946902235, 0.46668530691700155, 0.21511182184570332, 0.44843682342909513, 0.4472118309753681, 1.3702065923784335, 9.45427875433968, 1.102122619224493, 0.10258937868238853, 0.35642623211926194, 0.591356372987631, 0.5998071230824886, 0.24483835740148135, 5.289463063505844, 1.3129797723394556, 0.2805000163752069, 4.899003014255904, 4.640098640743104, 1.003960848599095, 0.3884242693750734, 0.15919459853077533, 0.5584160145339567, 26.120072401379826, 0.23266694970273952, 0.6971603828714161, 11.509274459376266, 18.57233374713158, 1.438399595898634, 0.6411618832520144, 0.18179196330130848, 2.851889572211704, 25.160057834101263, 0.6118019227787136, 4.23532527563179, 0.25701174921397196, 0.6308958564418045], 'lossList': [0.0, -1.2098752254247664, 0.0, 1.5007193402200938, 0.0, 0.0, 0.0], 'rewardMean': 0.7811797362211993, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1160.2572401043622, 'successfulTests': 6
#maxSuccessfulTests=6, maxSuccessfulTestsAtStep=25600, timeSpent=108.61
