#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 43
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5846711921719314, 'errorList': [], 'lossList': [0.0, -1.421724066734314, 0.0, 30.98340278506279, 0.0, 0.0, 0.0], 'rewardMean': 0.7214416503637355, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.439177292370605
'totalSteps': 3840, 'rewardStep': 0.9713347298877761, 'errorList': [], 'lossList': [0.0, -1.421346955895424, 0.0, 32.09056904911995, 0.0, 0.0, 0.0], 'rewardMean': 0.8047393435384157, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 391.86324735082087
'totalSteps': 5120, 'rewardStep': 0.836866084518641, 'errorList': [], 'lossList': [0.0, -1.4178673738241196, 0.0, 31.220293309092522, 0.0, 0.0, 0.0], 'rewardMean': 0.8127710287834721, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.6268705646223
'totalSteps': 6400, 'rewardStep': 0.7916054620296396, 'errorList': [], 'lossList': [0.0, -1.399567020535469, 0.0, 23.390938937067986, 0.0, 0.0, 0.0], 'rewardMean': 0.8085379154327056, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.9615870559815
'totalSteps': 7680, 'rewardStep': 0.7473253392618927, 'errorList': [], 'lossList': [0.0, -1.381202347278595, 0.0, 482.99852172851564, 0.0, 0.0, 0.0], 'rewardMean': 0.7983358194042367, 'totalEpisodes': 74, 'stepsPerEpisode': 54, 'rewardPerEpisode': 47.302525410186064
'totalSteps': 8960, 'rewardStep': 0.4392360093932046, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7085608669014787, 'totalEpisodes': 119, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.972964577314712
'totalSteps': 10240, 'rewardStep': 0.7704119179168759, 'errorList': [], 'lossList': [0.0, -1.377266963124275, 0.0, 302.0356926727295, 0.0, 0.0, 0.0], 'rewardMean': 0.7154332059031895, 'totalEpisodes': 175, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.710187035491423
'totalSteps': 11520, 'rewardStep': 0.9543282585356341, 'errorList': [132.42843968697596, 138.662815554498, 139.21896736355157, 149.50840801093025, 136.658071648581, 152.05080851022967, 141.2477008735832, 141.56516782289876, 140.49831737786613, 129.9924033724084, 150.28886650382609, 150.09792647857174, 137.44005604544253, 141.64758012445432, 150.59209321455447, 147.67673404186777, 142.89004638336118, 140.35973809791653, 129.2697890503407, 142.44912698057007, 136.5866792823463, 142.12006480161037, 136.50322821802953, 143.96680452588774, 140.3024746195121, 148.0564645126134, 136.52895397182576, 123.47829991141204, 139.27785213981596, 137.5185654496939, 139.93722914224594, 139.98151836635355, 135.26545098212486, 134.2366792322337, 141.75815283168305, 129.5894084929783, 120.85255684640676, 126.65146094060599, 140.46924008882542, 137.44891904692855, 127.56124045407987, 140.36204367514384, 129.24488323234775, 134.86140308258163, 144.0749117704622, 135.41915192469, 139.3221882495423, 141.21829253029392, 134.05598383029155, 136.7427169634015], 'lossList': [0.0, -1.3690547060966491, 0.0, 121.76000389099121, 0.0, 0.0, 0.0], 'rewardMean': 0.739322711166434, 'totalEpisodes': 230, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.660374926740822, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.645166146188001, 'errorList': [], 'lossList': [0.0, -1.3671457451581954, 0.0, 47.10085317611694, 0.0, 0.0, 0.0], 'rewardMean': 0.71801811492968, 'totalEpisodes': 276, 'stepsPerEpisode': 79, 'rewardPerEpisode': 69.67556682247103
'totalSteps': 14080, 'rewardStep': 0.8711069710274464, 'errorList': [], 'lossList': [0.0, -1.365724681019783, 0.0, 32.56317215919495, 0.0, 0.0, 0.0], 'rewardMean': 0.7466616928152316, 'totalEpisodes': 294, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.648545264087883
'totalSteps': 15360, 'rewardStep': 0.6165660108199204, 'errorList': [], 'lossList': [0.0, -1.3459745413064956, 0.0, 26.513766248226165, 0.0, 0.0, 0.0], 'rewardMean': 0.7111848209084461, 'totalEpisodes': 302, 'stepsPerEpisode': 93, 'rewardPerEpisode': 78.47026328545327
'totalSteps': 16640, 'rewardStep': 0.9216433354554473, 'errorList': [], 'lossList': [0.0, -1.3353876763582229, 0.0, 10.962792748212815, 0.0, 0.0, 0.0], 'rewardMean': 0.7196625460021266, 'totalEpisodes': 307, 'stepsPerEpisode': 93, 'rewardPerEpisode': 76.96847705294522
'totalSteps': 17920, 'rewardStep': 0.8752123466859253, 'errorList': [], 'lossList': [0.0, -1.3174505788087845, 0.0, 11.601519590616226, 0.0, 0.0, 0.0], 'rewardMean': 0.7280232344677553, 'totalEpisodes': 310, 'stepsPerEpisode': 76, 'rewardPerEpisode': 68.93242949211225
'totalSteps': 19200, 'rewardStep': 0.653847569128205, 'errorList': [], 'lossList': [0.0, -1.306135323047638, 0.0, 8.62142343878746, 0.0, 0.0, 0.0], 'rewardMean': 0.7186754574543865, 'totalEpisodes': 312, 'stepsPerEpisode': 151, 'rewardPerEpisode': 107.5859149183805
'totalSteps': 20480, 'rewardStep': 0.7902297518244725, 'errorList': [], 'lossList': [0.0, -1.2978523689508439, 0.0, 5.55647509098053, 0.0, 0.0, 0.0], 'rewardMean': 0.7537748316975132, 'totalEpisodes': 313, 'stepsPerEpisode': 1236, 'rewardPerEpisode': 936.6419845856325
'totalSteps': 21760, 'rewardStep': 0.8892675562057641, 'errorList': [], 'lossList': [0.0, -1.272987015247345, 0.0, 5.977375566065311, 0.0, 0.0, 0.0], 'rewardMean': 0.7987779863787694, 'totalEpisodes': 313, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.440190255093
'totalSteps': 23040, 'rewardStep': 0.7687627420298425, 'errorList': [], 'lossList': [0.0, -1.2385329926013946, 0.0, 4.916557243987918, 0.0, 0.0, 0.0], 'rewardMean': 0.7986130687900659, 'totalEpisodes': 313, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.5607040404382
'totalSteps': 24320, 'rewardStep': 0.7586740271218231, 'errorList': [], 'lossList': [0.0, -1.1946514177322387, 0.0, 2.8093131127208473, 0.0, 0.0, 0.0], 'rewardMean': 0.7790476456486848, 'totalEpisodes': 313, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.4742629490659
'totalSteps': 25600, 'rewardStep': 0.9894344779309271, 'errorList': [0.01992250209514788, 0.023140011797991387, 0.047486005053016096, 0.03252961607010371, 0.024666887852899708, 0.026010366582343968, 0.03288652517925227, 0.02504813254854723, 0.05872392009551983, 0.034813872307272634, 0.023290311177802104, 0.026999331455090682, 0.020268239517702447, 0.0458056470948276, 0.019402490027011762, 0.04708657235487401, 0.023322643029319107, 0.03970381654304656, 0.017467115256351022, 0.03090170670807033, 0.03940170029387769, 0.034574962383917524, 0.04066521203660918, 0.04432592451417403, 0.048445235482970264, 0.020195113665819216, 0.04366095808947194, 0.018580754460911616, 0.044798015627333196, 0.043577408512573745, 0.020425609887193122, 0.033566925668975386, 0.021126450808981442, 0.033437710074469496, 0.04402203599801201, 0.040932033559994106, 0.020127269086444674, 0.036422862690765154, 0.03720577897846014, 0.03677437619558493, 0.033708367132068415, 0.045788073260512076, 0.0492078456753359, 0.03544603194734175, 0.028086734631984037, 0.028824916288297187, 0.05167365386460141, 0.07512261464430135, 0.020991600483235142, 0.037819392962948874], 'lossList': [0.0, -1.1613961088657379, 0.0, 2.3003754330426456, 0.0, 0.0, 0.0], 'rewardMean': 0.8134744788229774, 'totalEpisodes': 313, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.894735344409, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=104.5
