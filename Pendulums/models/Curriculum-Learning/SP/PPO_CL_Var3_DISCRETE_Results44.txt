#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 44
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8807831400329279, 'errorList': [], 'lossList': [0.0, -1.4239101493358612, 0.0, 34.1101538413763, 0.0, 0.0, 0.0], 'rewardMean': 0.8020670319561144, 'totalEpisodes': 13, 'stepsPerEpisode': 311, 'rewardPerEpisode': 259.5193165874563
'totalSteps': 3840, 'rewardStep': 0.7149764825536262, 'errorList': [], 'lossList': [0.0, -1.4262116760015489, 0.0, 33.58108193159104, 0.0, 0.0, 0.0], 'rewardMean': 0.7730368488219517, 'totalEpisodes': 16, 'stepsPerEpisode': 174, 'rewardPerEpisode': 131.53359940844769
'totalSteps': 5120, 'rewardStep': 0.7105539823194872, 'errorList': [], 'lossList': [0.0, -1.4302703541517259, 0.0, 20.59622856259346, 0.0, 0.0, 0.0], 'rewardMean': 0.7574161321963355, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 944.9368777886872
'totalSteps': 6400, 'rewardStep': 0.8902491246567459, 'errorList': [], 'lossList': [0.0, -1.4038825798034669, 0.0, 18.17871629357338, 0.0, 0.0, 0.0], 'rewardMean': 0.7839827306884176, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 984.9605268040733
'totalSteps': 7680, 'rewardStep': 0.5732573110574583, 'errorList': [], 'lossList': [0.0, -1.4111668872833252, 0.0, 528.2581059265136, 0.0, 0.0, 0.0], 'rewardMean': 0.748861827416591, 'totalEpisodes': 93, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.449071728576018
'totalSteps': 8960, 'rewardStep': 0.6009584577584626, 'errorList': [], 'lossList': [0.0, -1.410799379348755, 0.0, 360.24259407043456, 0.0, 0.0, 0.0], 'rewardMean': 0.727732774608287, 'totalEpisodes': 180, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.01676329948273
'totalSteps': 10240, 'rewardStep': 0.5609828413778393, 'errorList': [], 'lossList': [0.0, -1.4084423702955247, 0.0, 130.20493856430053, 0.0, 0.0, 0.0], 'rewardMean': 0.706889032954481, 'totalEpisodes': 254, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.541347193859098
'totalSteps': 11520, 'rewardStep': 0.6282134710675138, 'errorList': [], 'lossList': [0.0, -1.400120415687561, 0.0, 51.089193658828734, 0.0, 0.0, 0.0], 'rewardMean': 0.698147303855929, 'totalEpisodes': 315, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.711489245114268
'totalSteps': 12800, 'rewardStep': 0.7360952942522652, 'errorList': [], 'lossList': [0.0, -1.3863669425249099, 0.0, 46.22315024375916, 0.0, 0.0, 0.0], 'rewardMean': 0.7019421028955627, 'totalEpisodes': 348, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.37593862534768
'totalSteps': 14080, 'rewardStep': 0.9303523714456569, 'errorList': [108.21816072876582, 115.69222295569733, 146.5273751205186, 88.97419106668015, 37.251603073785475, 143.27094143911663, 152.93056597419957, 160.65105193258728, 121.28480512400833, 160.33304016438572, 101.21084463698116, 126.91431734444869, 111.2954930242594, 114.79236661239102, 175.24625708756327, 77.05405118508283, 173.04970886422797, 152.51634943645053, 139.22967122865217, 97.76845705246406, 72.87653839232166, 157.63973677092946, 174.4880834510229, 132.2947360376681, 7.237299901518279, 101.32737973443952, 90.05701678761369, 25.704254381309564, 91.2479414763843, 141.3372475431052, 121.80961983837854, 108.11882622550804, 115.53314066327748, 144.53473454968886, 166.06530036128297, 158.89055371848423, 167.50896357995882, 168.10691139457396, 135.59066975241828, 110.97458874274241, 79.83744425168828, 161.51739194515744, 132.0200214364932, 55.46599673282832, 66.94243513325988, 167.37989695748303, 81.90713626858343, 129.54837002587428, 130.1417286136252, 104.26010736463623], 'lossList': [0.0, -1.3781271600723266, 0.0, 34.83763582706452, 0.0, 0.0, 0.0], 'rewardMean': 0.7226422476521982, 'totalEpisodes': 363, 'stepsPerEpisode': 100, 'rewardPerEpisode': 73.68110077672333, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.43501182762164214, 'errorList': [], 'lossList': [0.0, -1.3712761014699937, 0.0, 44.26976676940918, 0.0, 0.0, 0.0], 'rewardMean': 0.6780651164110697, 'totalEpisodes': 375, 'stepsPerEpisode': 92, 'rewardPerEpisode': 65.38398020156282
'totalSteps': 16640, 'rewardStep': 0.46036715080255075, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6275855000842686, 'totalEpisodes': 387, 'stepsPerEpisode': 51, 'rewardPerEpisode': 41.41775155487824
'totalSteps': 17920, 'rewardStep': 0.6655295386388325, 'errorList': [], 'lossList': [0.0, -1.3569272989034653, 0.0, 33.15453783035278, 0.0, 0.0, 0.0], 'rewardMean': 0.6051135414824772, 'totalEpisodes': 394, 'stepsPerEpisode': 151, 'rewardPerEpisode': 119.56198702345266
'totalSteps': 19200, 'rewardStep': 0.8674618747854186, 'errorList': [], 'lossList': [0.0, -1.3495694559812546, 0.0, 12.2439912545681, 0.0, 0.0, 0.0], 'rewardMean': 0.6345339978552732, 'totalEpisodes': 400, 'stepsPerEpisode': 94, 'rewardPerEpisode': 81.42314139394668
'totalSteps': 20480, 'rewardStep': 0.8848905014645962, 'errorList': [], 'lossList': [0.0, -1.3555161935091018, 0.0, 20.702210462093355, 0.0, 0.0, 0.0], 'rewardMean': 0.6629272022258866, 'totalEpisodes': 405, 'stepsPerEpisode': 89, 'rewardPerEpisode': 69.0755582742477
'totalSteps': 21760, 'rewardStep': 0.8077076420498932, 'errorList': [], 'lossList': [0.0, -1.3694705861806868, 0.0, 8.463211913108825, 0.0, 0.0, 0.0], 'rewardMean': 0.6875996822930921, 'totalEpisodes': 409, 'stepsPerEpisode': 75, 'rewardPerEpisode': 61.92945753863626
'totalSteps': 23040, 'rewardStep': 0.6169325419682836, 'errorList': [], 'lossList': [0.0, -1.385139104127884, 0.0, 14.141931569576263, 0.0, 0.0, 0.0], 'rewardMean': 0.686471589383169, 'totalEpisodes': 412, 'stepsPerEpisode': 291, 'rewardPerEpisode': 220.00712132705104
'totalSteps': 24320, 'rewardStep': 0.7909593330297986, 'errorList': [], 'lossList': [0.0, -1.3659793990850448, 0.0, 6.15423488676548, 0.0, 0.0, 0.0], 'rewardMean': 0.6919579932609223, 'totalEpisodes': 414, 'stepsPerEpisode': 26, 'rewardPerEpisode': 17.941893802681054
'totalSteps': 25600, 'rewardStep': 0.8663810051302351, 'errorList': [], 'lossList': [0.0, -1.3522520250082015, 0.0, 4.530217877626419, 0.0, 0.0, 0.0], 'rewardMean': 0.6855608566293802, 'totalEpisodes': 415, 'stepsPerEpisode': 445, 'rewardPerEpisode': 376.1478557114384
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.78
