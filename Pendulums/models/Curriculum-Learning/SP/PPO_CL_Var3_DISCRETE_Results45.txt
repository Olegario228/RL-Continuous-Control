#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 45
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9120293869764686, 'errorList': [], 'lossList': [0.0, -1.4368718886375427, 0.0, 31.965963249802588, 0.0, 0.0, 0.0], 'rewardMean': 0.9134250087786484, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 383.4933527492489
'totalSteps': 3840, 'rewardStep': 0.7201087920380944, 'errorList': [], 'lossList': [0.0, -1.4210810518264771, 0.0, 31.269216170310973, 0.0, 0.0, 0.0], 'rewardMean': 0.8489862698651304, 'totalEpisodes': 12, 'stepsPerEpisode': 268, 'rewardPerEpisode': 211.44214883633225
'totalSteps': 5120, 'rewardStep': 0.7380898310945112, 'errorList': [], 'lossList': [0.0, -1.4190899974107742, 0.0, 27.047922303676604, 0.0, 0.0, 0.0], 'rewardMean': 0.8212621601724757, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.3847194564404
'totalSteps': 6400, 'rewardStep': 0.9046049391236792, 'errorList': [], 'lossList': [0.0, -1.4086470800638198, 0.0, 25.48635731458664, 0.0, 0.0, 0.0], 'rewardMean': 0.8379307159627164, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.3090301323646
'totalSteps': 7680, 'rewardStep': 0.4638736823559986, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7310572777893685, 'totalEpisodes': 79, 'stepsPerEpisode': 45, 'rewardPerEpisode': 35.907067963038095
'totalSteps': 8960, 'rewardStep': 0.965241995039602, 'errorList': [56.258064791617656, 56.711466031223424, 58.19886218888217, 58.393936095305015, 57.69194263432761, 56.90009333997282, 58.91185247807307, 59.74174592615255, 55.94257988239408, 53.40414199876782, 57.84785712010431, 59.109757168020316, 59.07519002558653, 58.56531684388574, 57.94690409861627, 57.687490709708435, 59.05869671928052, 57.54763360367565, 59.06732563792896, 57.82182501568698, 55.29033938385576, 56.86934030003571, 51.44510214791177, 53.407750908949225, 58.609754747426756, 56.03879898630628, 57.40475089173584, 59.88733285287205, 58.5729302690088, 55.70268733573372, 53.47470911600197, 60.63001173064815, 60.828492785172834, 56.54819226321244, 58.979223710273054, 58.708671052944965, 59.390266139661456, 53.19781784393118, 60.44301828806397, 56.94098184352835, 55.11238911570078, 58.95387616899503, 56.22502577831931, 60.04182529179957, 55.35706214010673, 59.23178094638919, 57.521110043119734, 57.94543971164961, 56.843249695985875, 57.23549633678566], 'lossList': [0.0, -1.4086060082912446, 0.0, 495.49304763793947, 0.0, 0.0, 0.0], 'rewardMean': 0.7603303674456476, 'totalEpisodes': 148, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.293683562520442, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.8408911861556513, 'errorList': [], 'lossList': [0.0, -1.4066757225990296, 0.0, 340.68547271728517, 0.0, 0.0, 0.0], 'rewardMean': 0.7692815695245369, 'totalEpisodes': 224, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7132114579649393
'totalSteps': 11520, 'rewardStep': 0.5170083036088703, 'errorList': [], 'lossList': [0.0, -1.4043519341945647, 0.0, 125.69586154937744, 0.0, 0.0, 0.0], 'rewardMean': 0.7440542429329702, 'totalEpisodes': 290, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.983410830363608
'totalSteps': 12800, 'rewardStep': 0.6841439015517781, 'errorList': [], 'lossList': [0.0, -1.3771690654754638, 0.0, 59.191594581604, 0.0, 0.0, 0.0], 'rewardMean': 0.7209865700300652, 'totalEpisodes': 331, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.59731568486195
'totalSteps': 14080, 'rewardStep': 0.7029962608517056, 'errorList': [], 'lossList': [0.0, -1.3498758393526078, 0.0, 46.36419047355652, 0.0, 0.0, 0.0], 'rewardMean': 0.7000832574175888, 'totalEpisodes': 361, 'stepsPerEpisode': 59, 'rewardPerEpisode': 39.757366649123526
'totalSteps': 15360, 'rewardStep': 0.7176332838688183, 'errorList': [], 'lossList': [0.0, -1.3454637402296066, 0.0, 22.093698358535768, 0.0, 0.0, 0.0], 'rewardMean': 0.6998357066006613, 'totalEpisodes': 375, 'stepsPerEpisode': 71, 'rewardPerEpisode': 52.14798787012321
'totalSteps': 16640, 'rewardStep': 0.7911266154448339, 'errorList': [], 'lossList': [0.0, -1.3570512306690217, 0.0, 22.608249588012697, 0.0, 0.0, 0.0], 'rewardMean': 0.7051393850356936, 'totalEpisodes': 385, 'stepsPerEpisode': 97, 'rewardPerEpisode': 81.48635680126895
'totalSteps': 17920, 'rewardStep': 0.6121469382906957, 'errorList': [], 'lossList': [0.0, -1.3709877794981002, 0.0, 19.302380880117415, 0.0, 0.0, 0.0], 'rewardMean': 0.6758935849523953, 'totalEpisodes': 391, 'stepsPerEpisode': 136, 'rewardPerEpisode': 99.40624560169657
'totalSteps': 19200, 'rewardStep': 0.7365657513280385, 'errorList': [], 'lossList': [0.0, -1.376669679880142, 0.0, 9.463949564695358, 0.0, 0.0, 0.0], 'rewardMean': 0.7031627918495993, 'totalEpisodes': 396, 'stepsPerEpisode': 107, 'rewardPerEpisode': 79.85973362727404
'totalSteps': 20480, 'rewardStep': 0.5136419813363994, 'errorList': [], 'lossList': [0.0, -1.3738058364391328, 0.0, 7.045531527996063, 0.0, 0.0, 0.0], 'rewardMean': 0.7081396217476393, 'totalEpisodes': 401, 'stepsPerEpisode': 177, 'rewardPerEpisode': 125.80547643692825
'totalSteps': 21760, 'rewardStep': 0.8275522119428838, 'errorList': [], 'lossList': [0.0, -1.3475518780946731, 0.0, 10.785385012626648, 0.0, 0.0, 0.0], 'rewardMean': 0.6943706434379674, 'totalEpisodes': 405, 'stepsPerEpisode': 62, 'rewardPerEpisode': 49.52966258528065
'totalSteps': 23040, 'rewardStep': 0.6177697348938196, 'errorList': [], 'lossList': [0.0, -1.3363538086414337, 0.0, 5.485825392007828, 0.0, 0.0, 0.0], 'rewardMean': 0.6720584983117843, 'totalEpisodes': 408, 'stepsPerEpisode': 130, 'rewardPerEpisode': 106.5034626416151
'totalSteps': 24320, 'rewardStep': 0.8821033012970168, 'errorList': [], 'lossList': [0.0, -1.3135419803857804, 0.0, 5.194987081289291, 0.0, 0.0, 0.0], 'rewardMean': 0.7085679980805988, 'totalEpisodes': 410, 'stepsPerEpisode': 107, 'rewardPerEpisode': 87.2266185142948
'totalSteps': 25600, 'rewardStep': 0.7456061615422692, 'errorList': [], 'lossList': [0.0, -1.2953210246562958, 0.0, 3.227944250702858, 0.0, 0.0, 0.0], 'rewardMean': 0.714714224079648, 'totalEpisodes': 410, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.1357136584568
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.64
