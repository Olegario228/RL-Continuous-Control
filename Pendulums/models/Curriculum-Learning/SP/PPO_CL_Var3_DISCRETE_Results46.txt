#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 46
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8507820611170008, 'errorList': [], 'lossList': [0.0, -1.4478689223527907, 0.0, 39.36603865623474, 0.0, 0.0, 0.0], 'rewardMean': 0.72197994034858, 'totalEpisodes': 12, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.246092380443315
'totalSteps': 3840, 'rewardStep': 0.912045262930967, 'errorList': [], 'lossList': [0.0, -1.4647571444511414, 0.0, 34.22333289682865, 0.0, 0.0, 0.0], 'rewardMean': 0.7853350478760422, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.6693443357922
'totalSteps': 5120, 'rewardStep': 0.6920257340736807, 'errorList': [], 'lossList': [0.0, -1.4508162778615952, 0.0, 29.470210913419724, 0.0, 0.0, 0.0], 'rewardMean': 0.7620077194254519, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.1034262720764
'totalSteps': 6400, 'rewardStep': 0.9350593392633095, 'errorList': [92.78986178770904, 91.34844438117045, 91.13603559003501, 89.38844190671146, 89.31227031814396, 92.55732120639388, 86.91553374454628, 90.92835626604163, 88.82042822846783, 85.23943776730155, 86.96430233326682, 91.27553221186147, 87.18811173069244, 88.35051257825559, 91.61293610667835, 87.98788693826056, 91.66932408024036, 88.66098190080584, 91.75050674991184, 93.37937966402674, 88.20692836062553, 91.45166861206732, 88.94711601999518, 90.3758199308979, 85.01195213237706, 90.84150016358203, 91.28504815249877, 93.0773472969451, 92.69894501614007, 92.11955516586758, 87.85317437616604, 83.76712193416081, 90.13374018041519, 87.96736315774473, 90.07127051400941, 91.56321096295105, 92.88058619336073, 92.80557083289604, 81.3860509710517, 76.04469501177792, 80.81480206391613, 89.53259435766532, 88.99151148631297, 86.28282988666682, 88.22525231558207, 92.22556585967087, 83.83082721866364, 86.78380074007549, 82.93891846122628, 92.68249140300631], 'lossList': [0.0, -1.4478138899803161, 0.0, 24.043236311674118, 0.0, 0.0, 0.0], 'rewardMean': 0.7966180433930233, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.0425432279892, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.6094443348284562, 'errorList': [], 'lossList': [0.0, -1.4305645489692689, 0.0, 479.15871711730955, 0.0, 0.0, 0.0], 'rewardMean': 0.7654224252989289, 'totalEpisodes': 67, 'stepsPerEpisode': 57, 'rewardPerEpisode': 47.95813100567633
'totalSteps': 8960, 'rewardStep': 0.8691696573325489, 'errorList': [], 'lossList': [0.0, -1.4278857910633087, 0.0, 270.10233215332033, 0.0, 0.0, 0.0], 'rewardMean': 0.7802434584465888, 'totalEpisodes': 115, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.6336223082001675
'totalSteps': 10240, 'rewardStep': 0.6958740117777092, 'errorList': [], 'lossList': [0.0, -1.4173114901781083, 0.0, 125.83605281829834, 0.0, 0.0, 0.0], 'rewardMean': 0.7696972776129789, 'totalEpisodes': 159, 'stepsPerEpisode': 54, 'rewardPerEpisode': 42.24314276116384
'totalSteps': 11520, 'rewardStep': 0.6636683012764423, 'errorList': [], 'lossList': [0.0, -1.4036429899930953, 0.0, 60.66367919921875, 0.0, 0.0, 0.0], 'rewardMean': 0.7579162802422527, 'totalEpisodes': 192, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.292191154987757
'totalSteps': 12800, 'rewardStep': 0.8610192298976717, 'errorList': [], 'lossList': [0.0, -1.3883797705173493, 0.0, 40.58617230415344, 0.0, 0.0, 0.0], 'rewardMean': 0.7682265752077945, 'totalEpisodes': 211, 'stepsPerEpisode': 37, 'rewardPerEpisode': 33.067428653666106
'totalSteps': 14080, 'rewardStep': 0.613043380648191, 'errorList': [], 'lossList': [0.0, -1.3747157871723175, 0.0, 23.225544357299803, 0.0, 0.0, 0.0], 'rewardMean': 0.7702131313145978, 'totalEpisodes': 219, 'stepsPerEpisode': 75, 'rewardPerEpisode': 43.94168575291116
'totalSteps': 15360, 'rewardStep': 0.4556531566209652, 'errorList': [], 'lossList': [0.0, -1.3778482550382614, 0.0, 11.978482573032379, 0.0, 0.0, 0.0], 'rewardMean': 0.7307002408649941, 'totalEpisodes': 225, 'stepsPerEpisode': 187, 'rewardPerEpisode': 134.82307765426137
'totalSteps': 16640, 'rewardStep': 0.6792962156828226, 'errorList': [], 'lossList': [0.0, -1.3844432854652404, 0.0, 10.426098147630691, 0.0, 0.0, 0.0], 'rewardMean': 0.7074253361401797, 'totalEpisodes': 231, 'stepsPerEpisode': 303, 'rewardPerEpisode': 240.25810766832802
'totalSteps': 17920, 'rewardStep': 0.7084932068009543, 'errorList': [], 'lossList': [0.0, -1.3800815504789352, 0.0, 6.986232810616493, 0.0, 0.0, 0.0], 'rewardMean': 0.709072083412907, 'totalEpisodes': 235, 'stepsPerEpisode': 170, 'rewardPerEpisode': 136.43627725807485
'totalSteps': 19200, 'rewardStep': 0.8390197762610918, 'errorList': [], 'lossList': [0.0, -1.3719730401039123, 0.0, 11.024272193908692, 0.0, 0.0, 0.0], 'rewardMean': 0.6994681271126852, 'totalEpisodes': 238, 'stepsPerEpisode': 494, 'rewardPerEpisode': 405.124750683393
'totalSteps': 20480, 'rewardStep': 0.8727824756569579, 'errorList': [], 'lossList': [0.0, -1.36852425634861, 0.0, 6.144142199158669, 0.0, 0.0, 0.0], 'rewardMean': 0.7258019411955354, 'totalEpisodes': 240, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.206235019449906
'totalSteps': 21760, 'rewardStep': 0.7608647915717209, 'errorList': [], 'lossList': [0.0, -1.3598441505432128, 0.0, 21.683019453287123, 0.0, 0.0, 0.0], 'rewardMean': 0.7149714546194528, 'totalEpisodes': 242, 'stepsPerEpisode': 90, 'rewardPerEpisode': 77.46423158964619
'totalSteps': 23040, 'rewardStep': 0.8962880475120694, 'errorList': [], 'lossList': [0.0, -1.3320061898231506, 0.0, 2.406875926107168, 0.0, 0.0, 0.0], 'rewardMean': 0.7350128581928886, 'totalEpisodes': 242, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.9911877564425
'totalSteps': 24320, 'rewardStep': 0.7455583935823277, 'errorList': [], 'lossList': [0.0, -1.3003147619962692, 0.0, 1.7532680933922529, 0.0, 0.0, 0.0], 'rewardMean': 0.7432018674234773, 'totalEpisodes': 242, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.9836686000838
'totalSteps': 25600, 'rewardStep': 0.9321016972472584, 'errorList': [0.13859378950724055, 0.11901716197816546, 0.12912835951837195, 0.11012245324266857, 0.1499733127953857, 0.09625487312610134, 0.11409323037198256, 0.10922451432458836, 0.11699105192542936, 0.12367065373226505, 0.22662937750949563, 0.0960346527241615, 0.11996349382399957, 0.21275765448822687, 0.12978811117633549, 0.12182874379333974, 0.12464904012157774, 0.11745016526855333, 0.1142127911242576, 0.09422716259182119, 0.1532635669957051, 0.10909047026945266, 0.12922105576656406, 0.12242256481743603, 0.12881429555008536, 0.15218955654949226, 0.14475253644305472, 0.12957956722809022, 0.14528767552786095, 0.12027602396555127, 0.1466214691807386, 0.17003179007805702, 0.13579605218849977, 0.11818276901573796, 0.13590102731122783, 0.11274083671018872, 0.2266815604242767, 0.12241250383863765, 0.11012099489797839, 0.17243482890554931, 0.2080702429336628, 0.1675266722203442, 0.11252953549929572, 0.13776701657223855, 0.16461697810352438, 0.22860071526283196, 0.11153413112920525, 0.1417392255277768, 0.12386630985135946, 0.10915671594371493], 'lossList': [0.0, -1.2474740374088287, 0.0, 1.4080499664321542, 0.0, 0.0, 0.0], 'rewardMean': 0.7503101141584358, 'totalEpisodes': 242, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1163.5168676958174, 'successfulTests': 45
#maxSuccessfulTests=45, maxSuccessfulTestsAtStep=25600, timeSpent=108.04
