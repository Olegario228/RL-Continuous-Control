#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 47
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8663769990037149, 'errorList': [], 'lossList': [0.0, -1.4424236595630646, 0.0, 30.24666601061821, 0.0, 0.0, 0.0], 'rewardMean': 0.6847389234538633, 'totalEpisodes': 10, 'stepsPerEpisode': 905, 'rewardPerEpisode': 676.1619782270964
'totalSteps': 3840, 'rewardStep': 0.8318612758400938, 'errorList': [], 'lossList': [0.0, -1.4595861428976058, 0.0, 44.8659469127655, 0.0, 0.0, 0.0], 'rewardMean': 0.7337797075826069, 'totalEpisodes': 12, 'stepsPerEpisode': 668, 'rewardPerEpisode': 531.5237637867058
'totalSteps': 5120, 'rewardStep': 0.9420636962973828, 'errorList': [], 'lossList': [0.0, -1.4606796836853027, 0.0, 29.767981889247896, 0.0, 0.0, 0.0], 'rewardMean': 0.7858507047613009, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.4430166496704
'totalSteps': 6400, 'rewardStep': 0.7083415081798111, 'errorList': [], 'lossList': [0.0, -1.4487400782108306, 0.0, 25.63570770084858, 0.0, 0.0, 0.0], 'rewardMean': 0.7703488654450029, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.4453318273113
'totalSteps': 7680, 'rewardStep': 0.6195774361852242, 'errorList': [], 'lossList': [0.0, -1.4325515359640122, 0.0, 543.2889057922363, 0.0, 0.0, 0.0], 'rewardMean': 0.7452202939017064, 'totalEpisodes': 87, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.584758110366326
'totalSteps': 8960, 'rewardStep': 0.7896836792183899, 'errorList': [], 'lossList': [0.0, -1.428339295387268, 0.0, 276.3917179107666, 0.0, 0.0, 0.0], 'rewardMean': 0.7515722060898041, 'totalEpisodes': 141, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.020871384783595
'totalSteps': 10240, 'rewardStep': 0.7078820082124925, 'errorList': [], 'lossList': [0.0, -1.4301776510477067, 0.0, 141.34351440429688, 0.0, 0.0, 0.0], 'rewardMean': 0.7461109313551402, 'totalEpisodes': 183, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.406116118394424
'totalSteps': 11520, 'rewardStep': 0.8065491033398708, 'errorList': [], 'lossList': [0.0, -1.441330188512802, 0.0, 77.15011749267578, 0.0, 0.0, 0.0], 'rewardMean': 0.752826283797888, 'totalEpisodes': 219, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.04971127265094
'totalSteps': 12800, 'rewardStep': 0.676571470797061, 'errorList': [], 'lossList': [0.0, -1.4490425580739974, 0.0, 57.088628664016724, 0.0, 0.0, 0.0], 'rewardMean': 0.7452008024978053, 'totalEpisodes': 239, 'stepsPerEpisode': 101, 'rewardPerEpisode': 81.5684895449111
'totalSteps': 14080, 'rewardStep': 0.6886864922482749, 'errorList': [], 'lossList': [0.0, -1.4434876203536988, 0.0, 55.28983742713928, 0.0, 0.0, 0.0], 'rewardMean': 0.7637593669322316, 'totalEpisodes': 250, 'stepsPerEpisode': 22, 'rewardPerEpisode': 14.571360368074497
'totalSteps': 15360, 'rewardStep': 0.6981118368559391, 'errorList': [], 'lossList': [0.0, -1.432955275774002, 0.0, 54.81334319114685, 0.0, 0.0, 0.0], 'rewardMean': 0.7469328507174541, 'totalEpisodes': 260, 'stepsPerEpisode': 36, 'rewardPerEpisode': 21.813623276343726
'totalSteps': 16640, 'rewardStep': 0.3200970891208018, 'errorList': [], 'lossList': [0.0, -1.4222224307060243, 0.0, 51.23250154495239, 0.0, 0.0, 0.0], 'rewardMean': 0.6957564320455248, 'totalEpisodes': 267, 'stepsPerEpisode': 340, 'rewardPerEpisode': 265.08706932177876
'totalSteps': 17920, 'rewardStep': 0.5432066419585376, 'errorList': [], 'lossList': [0.0, -1.4161850363016129, 0.0, 19.949841767549515, 0.0, 0.0, 0.0], 'rewardMean': 0.6558707266116404, 'totalEpisodes': 272, 'stepsPerEpisode': 74, 'rewardPerEpisode': 54.17887335569063
'totalSteps': 19200, 'rewardStep': 0.9269762104650584, 'errorList': [], 'lossList': [0.0, -1.399562464952469, 0.0, 18.85774505376816, 0.0, 0.0, 0.0], 'rewardMean': 0.677734196840165, 'totalEpisodes': 275, 'stepsPerEpisode': 332, 'rewardPerEpisode': 274.65049183701643
'totalSteps': 20480, 'rewardStep': 0.754491972099193, 'errorList': [], 'lossList': [0.0, -1.3746433871984483, 0.0, 26.988945733308793, 0.0, 0.0, 0.0], 'rewardMean': 0.6912256504315619, 'totalEpisodes': 276, 'stepsPerEpisode': 444, 'rewardPerEpisode': 387.90638806206067
'totalSteps': 21760, 'rewardStep': 0.6615762573118312, 'errorList': [], 'lossList': [0.0, -1.3568823009729385, 0.0, 6.744874957799912, 0.0, 0.0, 0.0], 'rewardMean': 0.6784149082409061, 'totalEpisodes': 277, 'stepsPerEpisode': 892, 'rewardPerEpisode': 684.7189364150983
'totalSteps': 23040, 'rewardStep': 0.6537979123281541, 'errorList': [], 'lossList': [0.0, -1.3458557564020157, 0.0, 2.83745614528656, 0.0, 0.0, 0.0], 'rewardMean': 0.6730064986524723, 'totalEpisodes': 277, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 953.163140757298
'totalSteps': 24320, 'rewardStep': 0.9957943239588117, 'errorList': [0.36417674470603506, 0.4257645052812303, 0.36743725362225454, 0.3633723406392177, 0.3672809815150436, 0.36473674195438893, 0.3629803018417701, 0.3659241708793039, 0.36169950010876245, 0.3617468151429244, 0.35962882538658947, 0.36365874448978774, 0.3652982794899497, 0.36292667818340124, 0.36061396944316854, 0.36410236394439743, 0.36072615417856646, 0.3632355930692046, 0.3642243211382734, 0.37411371262656096, 0.36133952151759574, 0.3650603581280697, 0.3646716241538857, 0.3621819809793028, 0.36210019888086364, 0.3817325825371953, 0.36693238748079343, 0.37373199631944803, 0.3786915589376139, 0.3598061481826657, 0.3621078501469644, 0.359737477821768, 0.3628010916774335, 0.3592694994955489, 0.3631769721270886, 0.36446800500487603, 0.3604186697858728, 0.36334253708068626, 0.36521714721841536, 0.35991861716828666, 0.3775618869805361, 0.36350052482005124, 0.36377317759011907, 0.36356814066500165, 0.36072390192536746, 0.36075216170356306, 0.3771445722196935, 0.3622971360994293, 0.3645146663243115, 0.36232598049667053], 'lossList': [0.0, -1.3128241896629333, 0.0, 1.8288407710194587, 0.0, 0.0, 0.0], 'rewardMean': 0.6919310207143664, 'totalEpisodes': 277, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 925.5822645833712, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.768727402018591, 'errorList': [], 'lossList': [0.0, -1.2890096336603165, 0.0, 2.7857536709308626, 0.0, 0.0, 0.0], 'rewardMean': 0.7011466138365193, 'totalEpisodes': 277, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.541063041685
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=87.01
