#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 48
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.574368653695752, 'errorList': [], 'lossList': [0.0, -1.4227782970666885, 0.0, 32.65724693536758, 0.0, 0.0, 0.0], 'rewardMean': 0.7366495347612462, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.26894055682111
'totalSteps': 3840, 'rewardStep': 0.9642710036560319, 'errorList': [], 'lossList': [0.0, -1.422820115685463, 0.0, 36.074976375103, 0.0, 0.0, 0.0], 'rewardMean': 0.8125233577261747, 'totalEpisodes': 18, 'stepsPerEpisode': 486, 'rewardPerEpisode': 404.51896440838476
'totalSteps': 5120, 'rewardStep': 0.8515721396544952, 'errorList': [], 'lossList': [0.0, -1.4184762996435165, 0.0, 33.07030442774296, 0.0, 0.0, 0.0], 'rewardMean': 0.8222855532082548, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.9682976252727
'totalSteps': 6400, 'rewardStep': 0.8336663349294923, 'errorList': [], 'lossList': [0.0, -1.402276433110237, 0.0, 25.176099026203154, 0.0, 0.0, 0.0], 'rewardMean': 0.8245617095525024, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.9857121463865
'totalSteps': 7680, 'rewardStep': 0.7306044604811059, 'errorList': [], 'lossList': [0.0, -1.3923004227876663, 0.0, 498.8804493713379, 0.0, 0.0, 0.0], 'rewardMean': 0.808902168040603, 'totalEpisodes': 78, 'stepsPerEpisode': 42, 'rewardPerEpisode': 36.85400340969773
'totalSteps': 8960, 'rewardStep': 0.622207383702977, 'errorList': [], 'lossList': [0.0, -1.3871728432178498, 0.0, 265.16076889038084, 0.0, 0.0, 0.0], 'rewardMean': 0.7822314845637993, 'totalEpisodes': 123, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2882570711871588
'totalSteps': 10240, 'rewardStep': 0.4648221685884699, 'errorList': [], 'lossList': [0.0, -1.3826987701654434, 0.0, 121.37489521026612, 0.0, 0.0, 0.0], 'rewardMean': 0.7425553200668831, 'totalEpisodes': 163, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.35271089199436
'totalSteps': 11520, 'rewardStep': 0.6173907239640403, 'errorList': [], 'lossList': [0.0, -1.3965385514497757, 0.0, 69.34848247528076, 0.0, 0.0, 0.0], 'rewardMean': 0.7286481427221227, 'totalEpisodes': 188, 'stepsPerEpisode': 81, 'rewardPerEpisode': 67.1604036278758
'totalSteps': 12800, 'rewardStep': 0.4292986567643987, 'errorList': [], 'lossList': [0.0, -1.393902217745781, 0.0, 58.46440354347229, 0.0, 0.0, 0.0], 'rewardMean': 0.6987131941263504, 'totalEpisodes': 204, 'stepsPerEpisode': 8, 'rewardPerEpisode': 3.657522958267365
'totalSteps': 14080, 'rewardStep': 0.6011386367130185, 'errorList': [], 'lossList': [0.0, -1.3910173982381822, 0.0, 56.23429665565491, 0.0, 0.0, 0.0], 'rewardMean': 0.6689340162149783, 'totalEpisodes': 215, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.439511634933526
'totalSteps': 15360, 'rewardStep': 0.7304941305267462, 'errorList': [], 'lossList': [0.0, -1.3896714395284653, 0.0, 19.77792382478714, 0.0, 0.0, 0.0], 'rewardMean': 0.6845465638980776, 'totalEpisodes': 224, 'stepsPerEpisode': 147, 'rewardPerEpisode': 123.57723894014097
'totalSteps': 16640, 'rewardStep': 0.8056325009390368, 'errorList': [], 'lossList': [0.0, -1.3840404146909713, 0.0, 11.615772882699966, 0.0, 0.0, 0.0], 'rewardMean': 0.6686827136263781, 'totalEpisodes': 228, 'stepsPerEpisode': 145, 'rewardPerEpisode': 117.67634762909887
'totalSteps': 17920, 'rewardStep': 0.7555887985053369, 'errorList': [], 'lossList': [0.0, -1.3766125255823136, 0.0, 8.739512379765511, 0.0, 0.0, 0.0], 'rewardMean': 0.6590843795114624, 'totalEpisodes': 231, 'stepsPerEpisode': 168, 'rewardPerEpisode': 143.9411612294128
'totalSteps': 19200, 'rewardStep': 0.7695163370137206, 'errorList': [], 'lossList': [0.0, -1.3716232180595398, 0.0, 7.723464915156365, 0.0, 0.0, 0.0], 'rewardMean': 0.652669379719885, 'totalEpisodes': 234, 'stepsPerEpisode': 284, 'rewardPerEpisode': 226.0999137518778
'totalSteps': 20480, 'rewardStep': 0.9000890491700432, 'errorList': [], 'lossList': [0.0, -1.3820420867204666, 0.0, 10.660246770381928, 0.0, 0.0, 0.0], 'rewardMean': 0.6696178385887788, 'totalEpisodes': 236, 'stepsPerEpisode': 87, 'rewardPerEpisode': 76.81256274122782
'totalSteps': 21760, 'rewardStep': 0.896559593967234, 'errorList': [], 'lossList': [0.0, -1.357623865008354, 0.0, 6.572009718418121, 0.0, 0.0, 0.0], 'rewardMean': 0.6970530596152045, 'totalEpisodes': 237, 'stepsPerEpisode': 709, 'rewardPerEpisode': 604.2605820351398
'totalSteps': 23040, 'rewardStep': 0.748063804815335, 'errorList': [], 'lossList': [0.0, -1.316062524318695, 0.0, 3.090331608057022, 0.0, 0.0, 0.0], 'rewardMean': 0.725377223237891, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.1131355329217
'totalSteps': 24320, 'rewardStep': 0.6614100800633694, 'errorList': [], 'lossList': [0.0, -1.270402284860611, 0.0, 2.134205974191427, 0.0, 0.0, 0.0], 'rewardMean': 0.7297791588478239, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1086.947474009993
'totalSteps': 25600, 'rewardStep': 0.9672795293207898, 'errorList': [0.06195767453932597, 0.043896326353949244, 0.04255842358506407, 0.05056621748399745, 0.05699670455837003, 0.04273332097330981, 0.04671985852971425, 0.042546880786605214, 0.05497906139704807, 0.049639226816907106, 0.058999134646416866, 0.04199471590694222, 0.04511297509681571, 0.06575590224408029, 0.04415980511655309, 0.04905895059620842, 0.05896732849145895, 0.0560257431386319, 0.043202991838451284, 0.059382733914448406, 0.04923917485784187, 0.059293741357545206, 0.042681682356323226, 0.043431992500005456, 0.046883215369464985, 0.042591824490784344, 0.04261933839628927, 0.04372083012593795, 0.04434055908664144, 0.043820801444922, 0.04384289303315923, 0.04203328900363414, 0.057641740822300655, 0.05572581089569623, 0.058400917994593605, 0.04507973889295005, 0.05609810521789255, 0.0548839560974148, 0.04260006105646912, 0.0440385486168589, 0.05047578093364005, 0.05014668653160387, 0.052285488323759094, 0.05888629671438487, 0.05024355980452996, 0.051825233123103755, 0.04347921637243549, 0.06540367931648229, 0.0657657008286877, 0.05501821394736704], 'lossList': [0.0, -1.2323258537054063, 0.0, 1.9891402603685855, 0.0, 0.0, 0.0], 'rewardMean': 0.7835772461034629, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.8800104991433, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=87.2
