#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 49
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8201616791084557, 'errorList': [], 'lossList': [0.0, -1.4210871738195419, 0.0, 28.444636491537093, 0.0, 0.0, 0.0], 'rewardMean': 0.7847262717038683, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9817592529649
'totalSteps': 3840, 'rewardStep': 0.7653923850432686, 'errorList': [], 'lossList': [0.0, -1.434804788827896, 0.0, 35.22184423208237, 0.0, 0.0, 0.0], 'rewardMean': 0.7782816428170017, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.43908212466643
'totalSteps': 5120, 'rewardStep': 0.7040208628153881, 'errorList': [], 'lossList': [0.0, -1.4309330904483795, 0.0, 26.119540514349936, 0.0, 0.0, 0.0], 'rewardMean': 0.7597164478165983, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.8211163323339
'totalSteps': 6400, 'rewardStep': 0.9060037613770545, 'errorList': [], 'lossList': [0.0, -1.4235157144069672, 0.0, 19.444068129062654, 0.0, 0.0, 0.0], 'rewardMean': 0.7889739105286895, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.771177195542
'totalSteps': 7680, 'rewardStep': 0.8975944181780421, 'errorList': [], 'lossList': [0.0, -1.4323261260986329, 0.0, 498.42056991577147, 0.0, 0.0, 0.0], 'rewardMean': 0.8070773284702483, 'totalEpisodes': 84, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.5195002793068606
'totalSteps': 8960, 'rewardStep': 0.8794349171558211, 'errorList': [], 'lossList': [0.0, -1.4316732358932496, 0.0, 390.6318585205078, 0.0, 0.0, 0.0], 'rewardMean': 0.8174141268539016, 'totalEpisodes': 173, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.555245034261824
'totalSteps': 10240, 'rewardStep': 0.91228654599034, 'errorList': [], 'lossList': [0.0, -1.429274224638939, 0.0, 134.32342273712158, 0.0, 0.0, 0.0], 'rewardMean': 0.8292731792459563, 'totalEpisodes': 234, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.7924991167576723
'totalSteps': 11520, 'rewardStep': 0.48609914505794505, 'errorList': [], 'lossList': [0.0, -1.4200340116024017, 0.0, 57.05657224655151, 0.0, 0.0, 0.0], 'rewardMean': 0.7911427310028439, 'totalEpisodes': 295, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.182953126934041
'totalSteps': 12800, 'rewardStep': 0.7734117054579285, 'errorList': [], 'lossList': [0.0, -1.4054111683368682, 0.0, 45.18589075088501, 0.0, 0.0, 0.0], 'rewardMean': 0.7893696284483525, 'totalEpisodes': 322, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.8744273975371915
'totalSteps': 14080, 'rewardStep': 0.9022820180531552, 'errorList': [], 'lossList': [0.0, -1.3804451251029968, 0.0, 40.55412763595581, 0.0, 0.0, 0.0], 'rewardMean': 0.8046687438237399, 'totalEpisodes': 335, 'stepsPerEpisode': 63, 'rewardPerEpisode': 44.446819705052874
'totalSteps': 15360, 'rewardStep': 0.8750883179064045, 'errorList': [], 'lossList': [0.0, -1.3539149218797684, 0.0, 41.456841745376586, 0.0, 0.0, 0.0], 'rewardMean': 0.8101614077035348, 'totalEpisodes': 347, 'stepsPerEpisode': 166, 'rewardPerEpisode': 148.20424033436885
'totalSteps': 16640, 'rewardStep': 0.28597020040784715, 'errorList': [], 'lossList': [0.0, -1.336388440132141, 0.0, 49.99490626811981, 0.0, 0.0, 0.0], 'rewardMean': 0.7622191892399927, 'totalEpisodes': 355, 'stepsPerEpisode': 219, 'rewardPerEpisode': 156.3165393614958
'totalSteps': 17920, 'rewardStep': 0.6389995471871222, 'errorList': [], 'lossList': [0.0, -1.3269579684734345, 0.0, 12.874277877807618, 0.0, 0.0, 0.0], 'rewardMean': 0.7557170576771661, 'totalEpisodes': 361, 'stepsPerEpisode': 78, 'rewardPerEpisode': 62.44309420455469
'totalSteps': 19200, 'rewardStep': 0.7830490346381374, 'errorList': [], 'lossList': [0.0, -1.3300521475076676, 0.0, 8.36270653963089, 0.0, 0.0, 0.0], 'rewardMean': 0.7434215850032743, 'totalEpisodes': 366, 'stepsPerEpisode': 180, 'rewardPerEpisode': 157.75312684556724
'totalSteps': 20480, 'rewardStep': 0.9772418835399295, 'errorList': [0.37533069780132555, 0.11844185688796523, 1.5794079868457243, 0.41299043766234916, 0.5843816929374488, 0.9115594920481978, 0.5855530575468104, 0.14371892113171833, 0.7356713918768668, 1.2454533887667716, 0.3364689971046391, 0.1301747740113823, 0.3132197285006685, 0.9877561343668295, 0.3133353783193046, 0.746790807937897, 0.9847970581395727, 0.1336541622218639, 0.3247086026075921, 1.1263913160374694, 2.408630959065208, 0.47313342595601265, 0.6073838065294263, 0.12385014476297047, 0.9937630877043777, 0.2228669604805413, 0.7704191581897423, 1.007390281336004, 1.1721603007870065, 2.1800375629625375, 0.2598482477882017, 0.6946702913922801, 0.1479746258926741, 0.3180263798673795, 0.6008266097914068, 0.15943501567839738, 1.0919382524203478, 1.3090738122757175, 1.131163482482251, 0.6545669204622132, 0.3386341056836518, 0.573955844874688, 0.15686569642634685, 2.383758450530885, 0.7852099080146375, 0.6487510949536806, 0.25116590036294834, 0.5905890484764982, 1.9881161807631278, 0.8147596043829791], 'lossList': [0.0, -1.3299755305051804, 0.0, 4.823963502645492, 0.0, 0.0, 0.0], 'rewardMean': 0.7513863315394631, 'totalEpisodes': 370, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.246129526896798, 'successfulTests': 8
'totalSteps': 21760, 'rewardStep': 0.7322996732910321, 'errorList': [], 'lossList': [0.0, -1.314925188422203, 0.0, 6.265679261088371, 0.0, 0.0, 0.0], 'rewardMean': 0.7366728071529842, 'totalEpisodes': 372, 'stepsPerEpisode': 291, 'rewardPerEpisode': 243.32393019128813
'totalSteps': 23040, 'rewardStep': 0.7509138905241852, 'errorList': [], 'lossList': [0.0, -1.3001246279478074, 0.0, 3.310281564295292, 0.0, 0.0, 0.0], 'rewardMean': 0.7205355416063688, 'totalEpisodes': 372, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 884.12365014566
'totalSteps': 24320, 'rewardStep': 0.7233198787459366, 'errorList': [], 'lossList': [0.0, -1.2822404706478119, 0.0, 4.137278172969818, 0.0, 0.0, 0.0], 'rewardMean': 0.7442576149751678, 'totalEpisodes': 374, 'stepsPerEpisode': 297, 'rewardPerEpisode': 263.6573929508373
'totalSteps': 25600, 'rewardStep': 0.8853588016210063, 'errorList': [], 'lossList': [0.0, -1.270571061372757, 0.0, 2.7247685307264327, 0.0, 0.0, 0.0], 'rewardMean': 0.7554523245914756, 'totalEpisodes': 376, 'stepsPerEpisode': 127, 'rewardPerEpisode': 110.73830296953072
#maxSuccessfulTests=8, maxSuccessfulTestsAtStep=20480, timeSpent=88.06
