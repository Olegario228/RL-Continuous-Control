#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 5
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8755310625235291, 'errorList': [], 'lossList': [0.0, -1.3985852706432342, 0.0, 28.8172762799263, 0.0, 0.0, 0.0], 'rewardMean': 0.7431005807326898, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.951881461253773
'totalSteps': 3840, 'rewardStep': 0.7240434810803753, 'errorList': [], 'lossList': [0.0, -1.3883588933944702, 0.0, 30.6139417219162, 0.0, 0.0, 0.0], 'rewardMean': 0.7367482141819183, 'totalEpisodes': 26, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.41354584075523
'totalSteps': 5120, 'rewardStep': 0.6585082841710177, 'errorList': [], 'lossList': [0.0, -1.399420103430748, 0.0, 16.227856035232545, 0.0, 0.0, 0.0], 'rewardMean': 0.7171882316791931, 'totalEpisodes': 27, 'stepsPerEpisode': 123, 'rewardPerEpisode': 101.09556843198314
'totalSteps': 6400, 'rewardStep': 0.9265327431249448, 'errorList': [], 'lossList': [0.0, -1.406183002591133, 0.0, 199.49881332397462, 0.0, 0.0, 0.0], 'rewardMean': 0.7590571339683434, 'totalEpisodes': 89, 'stepsPerEpisode': 41, 'rewardPerEpisode': 33.83335062915699
'totalSteps': 7680, 'rewardStep': 0.47318508342443444, 'errorList': [], 'lossList': [0.0, -1.395552631020546, 0.0, 93.31550630569458, 0.0, 0.0, 0.0], 'rewardMean': 0.7114117922110252, 'totalEpisodes': 143, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.364334639001875
'totalSteps': 8960, 'rewardStep': 0.529883982934524, 'errorList': [], 'lossList': [0.0, -1.38558616399765, 0.0, 53.89349660873413, 0.0, 0.0, 0.0], 'rewardMean': 0.6854792480286679, 'totalEpisodes': 182, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.529883982934524
'totalSteps': 10240, 'rewardStep': 0.14196885745993076, 'errorList': [], 'lossList': [0.0, -1.3570125448703765, 0.0, 42.44894529342651, 0.0, 0.0, 0.0], 'rewardMean': 0.6175404492075758, 'totalEpisodes': 199, 'stepsPerEpisode': 109, 'rewardPerEpisode': 64.46020426165084
'totalSteps': 11520, 'rewardStep': 0.8307987874910301, 'errorList': [], 'lossList': [0.0, -1.3128708571195602, 0.0, 45.557977809906006, 0.0, 0.0, 0.0], 'rewardMean': 0.6412358201279597, 'totalEpisodes': 210, 'stepsPerEpisode': 51, 'rewardPerEpisode': 43.06411008112006
'totalSteps': 12800, 'rewardStep': 0.7895236818410238, 'errorList': [], 'lossList': [0.0, -1.2889226531982423, 0.0, 26.128683483600618, 0.0, 0.0, 0.0], 'rewardMean': 0.656064606299266, 'totalEpisodes': 217, 'stepsPerEpisode': 162, 'rewardPerEpisode': 120.11324923340143
'totalSteps': 14080, 'rewardStep': 0.43904493636681907, 'errorList': [], 'lossList': [0.0, -1.287296919822693, 0.0, 11.80669181227684, 0.0, 0.0, 0.0], 'rewardMean': 0.6389020900417628, 'totalEpisodes': 221, 'stepsPerEpisode': 311, 'rewardPerEpisode': 219.64007992689548
'totalSteps': 15360, 'rewardStep': 0.7841948380964925, 'errorList': [], 'lossList': [0.0, -1.2929491531848907, 0.0, 7.153992681503296, 0.0, 0.0, 0.0], 'rewardMean': 0.6297684675990591, 'totalEpisodes': 228, 'stepsPerEpisode': 69, 'rewardPerEpisode': 60.78247573789176
'totalSteps': 16640, 'rewardStep': 0.41929758043944837, 'errorList': [], 'lossList': [0.0, -1.299714440703392, 0.0, 5.891760976314544, 0.0, 0.0, 0.0], 'rewardMean': 0.5992938775349665, 'totalEpisodes': 232, 'stepsPerEpisode': 239, 'rewardPerEpisode': 177.78942686181676
'totalSteps': 17920, 'rewardStep': 0.7882667943089111, 'errorList': [], 'lossList': [0.0, -1.309718097448349, 0.0, 6.504940102100372, 0.0, 0.0, 0.0], 'rewardMean': 0.6122697285487559, 'totalEpisodes': 235, 'stepsPerEpisode': 145, 'rewardPerEpisode': 117.41108042476625
'totalSteps': 19200, 'rewardStep': 0.8489875297570472, 'errorList': [], 'lossList': [0.0, -1.3273552560806274, 0.0, 5.316051588654518, 0.0, 0.0, 0.0], 'rewardMean': 0.6045152072119662, 'totalEpisodes': 238, 'stepsPerEpisode': 153, 'rewardPerEpisode': 136.0270308637964
'totalSteps': 20480, 'rewardStep': 0.929973346590481, 'errorList': [], 'lossList': [0.0, -1.3414514315128327, 0.0, 4.461204208135605, 0.0, 0.0, 0.0], 'rewardMean': 0.6501940335285709, 'totalEpisodes': 240, 'stepsPerEpisode': 43, 'rewardPerEpisode': 37.990398501046315
'totalSteps': 21760, 'rewardStep': 0.8342229887196732, 'errorList': [], 'lossList': [0.0, -1.3302209454774856, 0.0, 1.5387333984673024, 0.0, 0.0, 0.0], 'rewardMean': 0.6806279341070857, 'totalEpisodes': 240, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.536297711162
'totalSteps': 23040, 'rewardStep': 0.8067510024198881, 'errorList': [], 'lossList': [0.0, -1.2856789177656174, 0.0, 1.5974533111602067, 0.0, 0.0, 0.0], 'rewardMean': 0.7471061486030814, 'totalEpisodes': 240, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.927897246701
'totalSteps': 24320, 'rewardStep': 0.8595552615718899, 'errorList': [], 'lossList': [0.0, -1.268242474794388, 0.0, 3.053907683491707, 0.0, 0.0, 0.0], 'rewardMean': 0.7499817960111674, 'totalEpisodes': 243, 'stepsPerEpisode': 39, 'rewardPerEpisode': 34.01288937538976
'totalSteps': 25600, 'rewardStep': 0.93606444299085, 'errorList': [0.2570689060811107, 0.3172475234283759, 0.2525917780418748, 0.19344807452661505, 0.10922961544713547, 0.13225969765841794, 0.31900764047816155, 0.3141524387557007, 0.3046085269134903, 0.3763905742332111, 0.22580302549756887, 0.10730989636061963, 0.3379766190086116, 0.22670503743820125, 0.16366194730139771, 0.3970690268243286, 0.3247657814535055, 0.25743372640109274, 0.4178794568273041, 0.19867078317764764, 0.34810071222475353, 0.35217246205168246, 0.3168649808285569, 0.3214603223141777, 0.3312887567405589, 0.233770208071931, 0.23985061784298234, 0.4233090078790756, 0.2932418609392646, 0.36990303283345977, 0.45066973541542044, 0.35260885645452006, 0.2693810862207695, 0.2815248327270257, 0.16434098011219206, 0.38437658389416907, 0.2584599092746772, 0.37739380930398697, 0.23132213011378158, 0.13780421894700629, 0.2752488108997078, 0.308514377676959, 0.16160753027661787, 0.20020068420470274, 0.3120393464268412, 0.14762883826623172, 0.177337309562364, 0.4346413757684703, 0.2097301840601597, 0.15087712102609604], 'lossList': [0.0, -1.2659275501966476, 0.0, 2.445502930879593, 0.0, 0.0, 0.0], 'rewardMean': 0.76463587212615, 'totalEpisodes': 244, 'stepsPerEpisode': 526, 'rewardPerEpisode': 481.7000504353016, 'successfulTests': 12
#maxSuccessfulTests=12, maxSuccessfulTestsAtStep=25600, timeSpent=75.88
