#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 52
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092971427115871, 'errorList': [], 'lossList': [0.0, -1.4231055521965026, 0.0, 34.86121163368225, 0.0, 0.0, 0.0], 'rewardMean': 0.8624672422476483, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.76661789737042
'totalSteps': 3840, 'rewardStep': 0.5562513755896646, 'errorList': [], 'lossList': [0.0, -1.4137311124801635, 0.0, 33.320800943374635, 0.0, 0.0, 0.0], 'rewardMean': 0.7603952866949871, 'totalEpisodes': 82, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.96326734396544
'totalSteps': 5120, 'rewardStep': 0.41657226176954365, 'errorList': [], 'lossList': [0.0, -1.393478239774704, 0.0, 22.643683416843416, 0.0, 0.0, 0.0], 'rewardMean': 0.6744395304636263, 'totalEpisodes': 91, 'stepsPerEpisode': 209, 'rewardPerEpisode': 111.1692412224747
'totalSteps': 6400, 'rewardStep': 0.5781114103780555, 'errorList': [], 'lossList': [0.0, -1.3759211069345474, 0.0, 55.08653503417969, 0.0, 0.0, 0.0], 'rewardMean': 0.6551739064465121, 'totalEpisodes': 100, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5781114103780555
'totalSteps': 7680, 'rewardStep': 0.6657041553964848, 'errorList': [], 'lossList': [0.0, -1.3629135596752167, 0.0, 43.17267139911652, 0.0, 0.0, 0.0], 'rewardMean': 0.6569289479381742, 'totalEpisodes': 106, 'stepsPerEpisode': 111, 'rewardPerEpisode': 85.581363560485
'totalSteps': 8960, 'rewardStep': 0.952378410323984, 'errorList': [198.71567807594238, 204.5359853294893, 74.59204671878008, 112.9152163924851, 141.6882420239525, 289.8479246529466, 118.39736493073501, 213.27229938321443, 212.45535143400295, 29.96213999278043, 66.94290274672295, 187.5633801023371, 118.33431492662409, 182.88407974153586, 82.99519001744049, 211.91495797794906, 117.310362384528, 211.93664117133955, 71.67569322368831, 192.63673452997068, 93.75217121617625, 88.76388229287551, 257.0859630229287, 186.90630904396363, 218.26103350200933, 85.12629070457174, 146.53625994679228, 26.08738060652943, 179.36107530107583, 147.94681623310368, 119.96095581596279, 153.63392471644576, 160.91648198553818, 195.52773951138164, 206.28070954408997, 213.00733172292044, 94.11418429312215, 210.09355830285278, 26.079865408151825, 245.57487368998892, 105.02627954970912, 204.41253012146046, 182.54019367652745, 125.24969965466643, 200.9695940070178, 68.47269782518426, 114.80304357070646, 143.80719182490614, 188.9312293891536, 175.95881297395343], 'lossList': [0.0, -1.3503436952829362, 0.0, 179.57621475219727, 0.0, 0.0, 0.0], 'rewardMean': 0.69913601399329, 'totalEpisodes': 146, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.27469187996366, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.5356980299509339, 'errorList': [], 'lossList': [0.0, -1.3480856263637542, 0.0, 81.29794017791748, 0.0, 0.0, 0.0], 'rewardMean': 0.6787062659879953, 'totalEpisodes': 165, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.071143792871656
'totalSteps': 11520, 'rewardStep': 0.7076031840287054, 'errorList': [], 'lossList': [0.0, -1.3651912933588028, 0.0, 42.691280145645145, 0.0, 0.0, 0.0], 'rewardMean': 0.6819170346591853, 'totalEpisodes': 176, 'stepsPerEpisode': 214, 'rewardPerEpisode': 174.86463119155553
'totalSteps': 12800, 'rewardStep': 0.929973569715299, 'errorList': [], 'lossList': [0.0, -1.3667788910865784, 0.0, 33.60692160367966, 0.0, 0.0, 0.0], 'rewardMean': 0.7067226881647967, 'totalEpisodes': 183, 'stepsPerEpisode': 153, 'rewardPerEpisode': 127.99438034511628
'totalSteps': 14080, 'rewardStep': 0.9116095869437494, 'errorList': [], 'lossList': [0.0, -1.3383412140607833, 0.0, 11.342993432581425, 0.0, 0.0, 0.0], 'rewardMean': 0.7163199126808008, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 985.2337850428486
'totalSteps': 15360, 'rewardStep': 0.7234261676195375, 'errorList': [], 'lossList': [0.0, -1.2914914113283158, 0.0, 8.73976784646511, 0.0, 0.0, 0.0], 'rewardMean': 0.6977328151715957, 'totalEpisodes': 184, 'stepsPerEpisode': 254, 'rewardPerEpisode': 194.82921061219454
'totalSteps': 16640, 'rewardStep': 0.6338017649625255, 'errorList': [], 'lossList': [0.0, -1.288042590022087, 0.0, 6.152485232353211, 0.0, 0.0, 0.0], 'rewardMean': 0.7054878541088818, 'totalEpisodes': 185, 'stepsPerEpisode': 1154, 'rewardPerEpisode': 900.4190821894083
'totalSteps': 17920, 'rewardStep': 0.7796701770990596, 'errorList': [], 'lossList': [0.0, -1.2769510513544082, 0.0, 3.441347956955433, 0.0, 0.0, 0.0], 'rewardMean': 0.7417976456418334, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 971.3602240130795
'totalSteps': 19200, 'rewardStep': 0.9182367341576586, 'errorList': [], 'lossList': [0.0, -1.2453266710042954, 0.0, 3.5806763430684807, 0.0, 0.0, 0.0], 'rewardMean': 0.7758101780197937, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.3992832419772
'totalSteps': 20480, 'rewardStep': 0.9504837796509219, 'errorList': [0.13572913082805121, 0.1578618495461561, 0.16454052099761202, 0.17120714542902046, 0.15340321747150004, 0.16048618070871362, 0.13524707606653352, 0.14314072596505714, 0.12087351015709853, 0.1592887360473307, 0.15277760682951796, 0.1423145905033243, 0.16517103108387346, 0.15223463786470198, 0.1682562713734988, 0.17751473754135738, 0.14355321747832223, 0.17687899833452386, 0.14653433610153763, 0.15447947198163497, 0.1510621240503305, 0.15808399464869025, 0.15083546839174303, 0.15900423855725873, 0.15141880463311422, 0.18048218938881286, 0.1758762679768495, 0.15845105531312367, 0.16098605046872633, 0.14132608106149686, 0.13551061233250192, 0.17494615077548997, 0.17826369571877324, 0.14264100598385313, 0.14812879699118486, 0.15666721468749112, 0.13302457564624443, 0.14714187728075453, 0.134263288085564, 0.1920829746129058, 0.16407465511935707, 0.14681522803303787, 0.16659669998630278, 0.16484358003805322, 0.15249826314216963, 0.1576666934817782, 0.13551341452566354, 0.13669716699563098, 0.1641459912046866, 0.16486459831080463], 'lossList': [0.0, -1.211902067065239, 0.0, 3.235701944902539, 0.0, 0.0, 0.0], 'rewardMean': 0.8042881404452376, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.1115311885476, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=93.61
