#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 53
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.43590750673834866, 'errorList': [], 'lossList': [0.0, -1.4341206073760986, 0.0, 31.667020444869994, 0.0, 0.0, 0.0], 'rewardMean': 0.47042336673936674, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.31342545895599
'totalSteps': 3840, 'rewardStep': 0.9553872686153767, 'errorList': [], 'lossList': [0.0, -1.4267620873451232, 0.0, 33.83611287593842, 0.0, 0.0, 0.0], 'rewardMean': 0.6320780006980368, 'totalEpisodes': 72, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.22112575509128
'totalSteps': 5120, 'rewardStep': 0.8388254462094117, 'errorList': [], 'lossList': [0.0, -1.422535389661789, 0.0, 42.977623014450074, 0.0, 0.0, 0.0], 'rewardMean': 0.6837648620758805, 'totalEpisodes': 78, 'stepsPerEpisode': 72, 'rewardPerEpisode': 64.54850374529123
'totalSteps': 6400, 'rewardStep': 0.48326593053442374, 'errorList': [], 'lossList': [0.0, -1.4289336198568343, 0.0, 39.22549348831177, 0.0, 0.0, 0.0], 'rewardMean': 0.6436650757675891, 'totalEpisodes': 83, 'stepsPerEpisode': 143, 'rewardPerEpisode': 101.4031990359977
'totalSteps': 7680, 'rewardStep': 0.9520836709226697, 'errorList': [315.07576126279594, 192.70027317909154, 304.9470255217477, 340.7311895943934, 14.310087076162581, 314.6358548778492, 303.0213730490113, 336.9099234505343, 219.7370028468186, 216.63854378894928, 348.65068971080325, 324.4175686862397, 344.6087849112544, 208.52548560729272, 319.55924923243236, 278.97569524482924, 257.1570738172288, 312.1355933881547, 324.90417209743873, 272.21555340720784, 326.3551013316413, 276.53820148872944, 312.16983074075847, 339.72188114799235, 176.7550267483131, 252.6377264905833, 82.98784472479063, 208.72017329782676, 259.9295021177991, 239.38421461592822, 347.8644879849562, 328.0198028995891, 326.50763057271774, 283.48354124977186, 303.7693580931363, 247.99128283811967, 332.12969579934156, 223.57217759921068, 147.6511515912711, 339.1410038705224, 288.4618883938629, 177.79023430220852, 226.751448896802, 333.97736910919105, 349.32750983522266, 199.00579459206125, 322.104736919183, 192.9959386930347, 253.2254523944551, 161.26584044797949], 'lossList': [0.0, -1.4247060024738312, 0.0, 45.84367401599884, 0.0, 0.0, 0.0], 'rewardMean': 0.6950681749601025, 'totalEpisodes': 88, 'stepsPerEpisode': 53, 'rewardPerEpisode': 48.64993932157514, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.9201682052389843, 'errorList': [], 'lossList': [0.0, -1.4124553614854813, 0.0, 140.4948527908325, 0.0, 0.0, 0.0], 'rewardMean': 0.7272253221428, 'totalEpisodes': 110, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.623084989170046
'totalSteps': 10240, 'rewardStep': 0.6915977697217509, 'errorList': [], 'lossList': [0.0, -1.4114007151126862, 0.0, 76.14482370376587, 0.0, 0.0, 0.0], 'rewardMean': 0.7227718780901689, 'totalEpisodes': 125, 'stepsPerEpisode': 165, 'rewardPerEpisode': 143.64214622522456
'totalSteps': 11520, 'rewardStep': 0.46689454568995015, 'errorList': [], 'lossList': [0.0, -1.410766007900238, 0.0, 22.528918228149415, 0.0, 0.0, 0.0], 'rewardMean': 0.6943410633790335, 'totalEpisodes': 134, 'stepsPerEpisode': 80, 'rewardPerEpisode': 56.32288356622657
'totalSteps': 12800, 'rewardStep': 0.7537760812663002, 'errorList': [], 'lossList': [0.0, -1.415167219042778, 0.0, 19.607411830425264, 0.0, 0.0, 0.0], 'rewardMean': 0.7002845651677602, 'totalEpisodes': 143, 'stepsPerEpisode': 73, 'rewardPerEpisode': 61.34332985153329
'totalSteps': 14080, 'rewardStep': 0.4993549663196454, 'errorList': [], 'lossList': [0.0, -1.4280432105064391, 0.0, 9.970736567974091, 0.0, 0.0, 0.0], 'rewardMean': 0.6997261391256862, 'totalEpisodes': 149, 'stepsPerEpisode': 109, 'rewardPerEpisode': 74.07422429303614
'totalSteps': 15360, 'rewardStep': 0.7297640396603773, 'errorList': [], 'lossList': [0.0, -1.4396833902597428, 0.0, 8.676077506542207, 0.0, 0.0, 0.0], 'rewardMean': 0.729111792417889, 'totalEpisodes': 153, 'stepsPerEpisode': 140, 'rewardPerEpisode': 117.45282817939763
'totalSteps': 16640, 'rewardStep': 0.7012819640722854, 'errorList': [], 'lossList': [0.0, -1.4608826434612274, 0.0, 6.467054982185363, 0.0, 0.0, 0.0], 'rewardMean': 0.70370126196358, 'totalEpisodes': 155, 'stepsPerEpisode': 166, 'rewardPerEpisode': 129.0249828068333
'totalSteps': 17920, 'rewardStep': 0.7250002138477719, 'errorList': [], 'lossList': [0.0, -1.4532136029005052, 0.0, 4.562321459054947, 0.0, 0.0, 0.0], 'rewardMean': 0.6923187387274159, 'totalEpisodes': 158, 'stepsPerEpisode': 121, 'rewardPerEpisode': 95.56306054313936
'totalSteps': 19200, 'rewardStep': 0.7418987105519702, 'errorList': [], 'lossList': [0.0, -1.4297378957271576, 0.0, 4.670729538202286, 0.0, 0.0, 0.0], 'rewardMean': 0.7181820167291705, 'totalEpisodes': 161, 'stepsPerEpisode': 164, 'rewardPerEpisode': 132.4329703916266
'totalSteps': 20480, 'rewardStep': 0.8407221492256862, 'errorList': [], 'lossList': [0.0, -1.4082841581106187, 0.0, 4.225582172274589, 0.0, 0.0, 0.0], 'rewardMean': 0.7070458645594722, 'totalEpisodes': 163, 'stepsPerEpisode': 114, 'rewardPerEpisode': 93.41863030165881
'totalSteps': 21760, 'rewardStep': 0.5626657314639838, 'errorList': [], 'lossList': [0.0, -1.3879605048894883, 0.0, 2.82924996137619, 0.0, 0.0, 0.0], 'rewardMean': 0.6712956171819722, 'totalEpisodes': 164, 'stepsPerEpisode': 903, 'rewardPerEpisode': 679.9966222108422
'totalSteps': 23040, 'rewardStep': 0.7883138771959178, 'errorList': [], 'lossList': [0.0, -1.378281012773514, 0.0, 3.8119882708787918, 0.0, 0.0, 0.0], 'rewardMean': 0.6809672279293888, 'totalEpisodes': 164, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 853.6613374483665
'totalSteps': 24320, 'rewardStep': 0.3106025062081409, 'errorList': [], 'lossList': [0.0, -1.3550559318065643, 0.0, 3.917292793393135, 0.0, 0.0, 0.0], 'rewardMean': 0.6653380239812079, 'totalEpisodes': 164, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 885.8024542467035
'totalSteps': 25600, 'rewardStep': 0.9537805213792114, 'errorList': [0.15110474111771816, 0.13606625034985975, 0.13624381370260735, 0.14105358029245282, 0.1476516639801714, 0.13404807559930584, 0.18524402245281854, 0.1350005976308043, 0.14532664075918614, 0.13928438944648464, 0.14794188871428726, 0.13498081131647724, 0.13851404395998054, 0.15010679380428313, 0.1771199068764234, 0.14080229858525464, 0.14952667474880685, 0.14499032175682872, 0.15304790638028923, 0.14926916801511458, 0.1399952766390778, 0.15012837517713767, 0.13654543898054614, 0.13758600709791627, 0.1369019290603082, 0.13296037702871685, 0.13235153691015863, 0.1669449090291252, 0.1362735717569244, 0.18022235695363675, 0.1693246115394216, 0.13501654445075853, 0.14763072126864898, 0.14517082736586004, 0.14781501712492015, 0.13700405705121008, 0.14696738585671346, 0.14460085771718129, 0.15995595863329837, 0.1719937825713042, 0.1426500736704083, 0.14189410428023985, 0.14359468506545797, 0.15910578166129088, 0.14088002864571786, 0.14202017095573657, 0.17183136155087483, 0.15267299549920876, 0.15410692291649958, 0.17254817193665498], 'lossList': [0.0, -1.3206713443994522, 0.0, 2.3609278652071954, 0.0, 0.0, 0.0], 'rewardMean': 0.685338467992499, 'totalEpisodes': 164, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1001.7419395652528, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=108.65
