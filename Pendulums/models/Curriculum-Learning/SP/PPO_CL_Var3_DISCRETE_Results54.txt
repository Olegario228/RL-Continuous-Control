#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 54
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764237844444184, 'errorList': [], 'lossList': [0.0, -1.4117648071050644, 0.0, 34.56727179527283, 0.0, 0.0, 0.0], 'rewardMean': 0.8487569592979127, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.540084423708798
'totalSteps': 3840, 'rewardStep': 0.8301048750001292, 'errorList': [], 'lossList': [0.0, -1.4150108939409256, 0.0, 39.124620037078856, 0.0, 0.0, 0.0], 'rewardMean': 0.8425395978653182, 'totalEpisodes': 84, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.84030782755015
'totalSteps': 5120, 'rewardStep': 0.6848389580966191, 'errorList': [], 'lossList': [0.0, -1.4063812613487243, 0.0, 28.86642453432083, 0.0, 0.0, 0.0], 'rewardMean': 0.8031144379231434, 'totalEpisodes': 90, 'stepsPerEpisode': 117, 'rewardPerEpisode': 92.77188059566005
'totalSteps': 6400, 'rewardStep': 0.522264938468509, 'errorList': [], 'lossList': [0.0, -1.3970475417375565, 0.0, 23.584555982351304, 0.0, 0.0, 0.0], 'rewardMean': 0.7469445380322165, 'totalEpisodes': 94, 'stepsPerEpisode': 456, 'rewardPerEpisode': 368.87075139144315
'totalSteps': 7680, 'rewardStep': 0.4707934727118634, 'errorList': [], 'lossList': [0.0, -1.3926397609710692, 0.0, 25.67977596640587, 0.0, 0.0, 0.0], 'rewardMean': 0.7009193604788243, 'totalEpisodes': 96, 'stepsPerEpisode': 428, 'rewardPerEpisode': 314.24768187725965
'totalSteps': 8960, 'rewardStep': 0.9379994314055243, 'errorList': [329.23918186212615, 297.6506184388038, 297.33949511002504, 290.66014637410206, 260.34165517450253, 290.2626915285075, 334.7940891285639, 345.2414194226823, 285.39279171009554, 302.91590783294134, 119.63331497446615, 246.3613573425987, 312.75479744229136, 273.1726922278983, 346.7894898719995, 253.85690638949725, 353.7797190199528, 331.9604266906118, 225.63062019851805, 300.6872237025783, 252.42650802091444, 291.08485740275285, 340.55726173416485, 258.777677439484, 276.1658921818825, 272.3501159609798, 208.8572975825839, 274.3037357207691, 291.25233563394477, 247.95434127533596, 298.44600827894635, 330.7273588319239, 291.6501908122698, 316.90592469264874, 321.4495308053135, 286.23161869692285, 348.02848160943455, 300.9317999054573, 265.51197688066406, 248.13987305433113, 247.0501021302304, 309.8616602858053, 239.01904575647933, 325.5319268677953, 219.2207994194528, 199.5178050387752, 309.0744323732615, 279.84163028036625, 274.11199120299256, 290.9486718138909], 'lossList': [0.0, -1.3777083724737167, 0.0, 221.80822265625, 0.0, 0.0, 0.0], 'rewardMean': 0.7347879420397814, 'totalEpisodes': 126, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.816542130719963, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.46239763059319394, 'errorList': [], 'lossList': [0.0, -1.379438049197197, 0.0, 106.5252950668335, 0.0, 0.0, 0.0], 'rewardMean': 0.7007391531089581, 'totalEpisodes': 155, 'stepsPerEpisode': 27, 'rewardPerEpisode': 16.350535481464668
'totalSteps': 11520, 'rewardStep': 0.49022845789698066, 'errorList': [], 'lossList': [0.0, -1.3700575840473175, 0.0, 42.347592458724975, 0.0, 0.0, 0.0], 'rewardMean': 0.6773490758631828, 'totalEpisodes': 169, 'stepsPerEpisode': 119, 'rewardPerEpisode': 82.00166487820815
'totalSteps': 12800, 'rewardStep': 0.5503906839419153, 'errorList': [], 'lossList': [0.0, -1.347953450679779, 0.0, 33.01086234569549, 0.0, 0.0, 0.0], 'rewardMean': 0.6646532366710561, 'totalEpisodes': 180, 'stepsPerEpisode': 61, 'rewardPerEpisode': 41.29798821222309
'totalSteps': 14080, 'rewardStep': 0.7277205385049887, 'errorList': [], 'lossList': [0.0, -1.331549100279808, 0.0, 11.197379505634308, 0.0, 0.0, 0.0], 'rewardMean': 0.6453162771064143, 'totalEpisodes': 185, 'stepsPerEpisode': 236, 'rewardPerEpisode': 180.52854059491753
'totalSteps': 15360, 'rewardStep': 0.5335882850367448, 'errorList': [], 'lossList': [0.0, -1.321962673664093, 0.0, 30.85694373846054, 0.0, 0.0, 0.0], 'rewardMean': 0.621032727165647, 'totalEpisodes': 192, 'stepsPerEpisode': 218, 'rewardPerEpisode': 165.1689761340194
'totalSteps': 16640, 'rewardStep': 0.5474731334429379, 'errorList': [], 'lossList': [0.0, -1.3018017649650573, 0.0, 14.612351812124253, 0.0, 0.0, 0.0], 'rewardMean': 0.5927695530099277, 'totalEpisodes': 196, 'stepsPerEpisode': 142, 'rewardPerEpisode': 116.11997698344351
'totalSteps': 17920, 'rewardStep': 0.7889833135625234, 'errorList': [], 'lossList': [0.0, -1.2742546105384827, 0.0, 7.137176535129547, 0.0, 0.0, 0.0], 'rewardMean': 0.6031839885565182, 'totalEpisodes': 198, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.870250850121395
'totalSteps': 19200, 'rewardStep': 0.853721574371726, 'errorList': [], 'lossList': [0.0, -1.2755277508497238, 0.0, 6.472068804502487, 0.0, 0.0, 0.0], 'rewardMean': 0.6363296521468398, 'totalEpisodes': 200, 'stepsPerEpisode': 57, 'rewardPerEpisode': 49.25428255905394
'totalSteps': 20480, 'rewardStep': 0.8848890617924396, 'errorList': [], 'lossList': [0.0, -1.298246030807495, 0.0, 4.144128402471543, 0.0, 0.0, 0.0], 'rewardMean': 0.6777392110548975, 'totalEpisodes': 201, 'stepsPerEpisode': 65, 'rewardPerEpisode': 50.1181224819201
'totalSteps': 21760, 'rewardStep': 0.6998280470381499, 'errorList': [], 'lossList': [0.0, -1.3363271379470825, 0.0, 2.903501584529877, 0.0, 0.0, 0.0], 'rewardMean': 0.65392207261816, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1038.8551593196592
'totalSteps': 23040, 'rewardStep': 0.8830988431095955, 'errorList': [], 'lossList': [0.0, -1.3600706022977829, 0.0, 2.1122529934346677, 0.0, 0.0, 0.0], 'rewardMean': 0.6959921938698002, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.721179229679
'totalSteps': 24320, 'rewardStep': 0.8906066450985141, 'errorList': [], 'lossList': [0.0, -1.354516389966011, 0.0, 1.8665569174662233, 0.0, 0.0, 0.0], 'rewardMean': 0.7360300125899536, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.2738092668383
'totalSteps': 25600, 'rewardStep': 0.9888621792776918, 'errorList': [0.02589789543034191, 0.030734320091328118, 0.02410824634762033, 0.023625159010560202, 0.02343974622703813, 0.028149380668189616, 0.06949954196858187, 0.03051782258525652, 0.05830827820533818, 0.05341187324788966, 0.04834947010994328, 0.04647497970736384, 0.057396724208619715, 0.029746224870330094, 0.06842312322481157, 0.05143414946125532, 0.028340275368913356, 0.0757358184596715, 0.02117513593951125, 0.027519978878022256, 0.043050407499542045, 0.025612540190367707, 0.03710336851853535, 0.056226894881775985, 0.02705871233484827, 0.025243352328868953, 0.028021316852721995, 0.022304486311872054, 0.024081854298630277, 0.029717654235200477, 0.06715916647814389, 0.02770151720056146, 0.02307201243821252, 0.02432414295709607, 0.025842613402255778, 0.029543909350774847, 0.07088190276875979, 0.027490921444701832, 0.033602478487154246, 0.02481402338921297, 0.02749501303337335, 0.0242901237173094, 0.030592440637371834, 0.03395408471198688, 0.03113980123260038, 0.06925951622466589, 0.03509320184569703, 0.053105069184703126, 0.06392877285469326, 0.035699378987181986], 'lossList': [0.0, -1.3330629062652588, 0.0, 1.4029037189297378, 0.0, 0.0, 0.0], 'rewardMean': 0.7798771621235312, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.685764400678, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.11
