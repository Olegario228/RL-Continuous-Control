#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 55
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8755310625235291, 'errorList': [], 'lossList': [0.0, -1.3985852706432342, 0.0, 28.8172762799263, 0.0, 0.0, 0.0], 'rewardMean': 0.7431005807326898, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.951881461253773
'totalSteps': 3840, 'rewardStep': 0.7240434810803753, 'errorList': [], 'lossList': [0.0, -1.3883588933944702, 0.0, 30.6139417219162, 0.0, 0.0, 0.0], 'rewardMean': 0.7367482141819183, 'totalEpisodes': 26, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.41354584075523
'totalSteps': 5120, 'rewardStep': 0.6585082841710177, 'errorList': [], 'lossList': [0.0, -1.399420103430748, 0.0, 16.227856035232545, 0.0, 0.0, 0.0], 'rewardMean': 0.7171882316791931, 'totalEpisodes': 27, 'stepsPerEpisode': 123, 'rewardPerEpisode': 101.09556843198314
'totalSteps': 6400, 'rewardStep': 0.9488612710731995, 'errorList': [], 'lossList': [0.0, -1.410182209610939, 0.0, 16.873179924488067, 0.0, 0.0, 0.0], 'rewardMean': 0.7635228395579944, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 958.9682218704281
'totalSteps': 7680, 'rewardStep': 0.7028640015072771, 'errorList': [], 'lossList': [0.0, -1.3963335663080216, 0.0, 13.15499938905239, 0.0, 0.0, 0.0], 'rewardMean': 0.7534130332162081, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.9997383320181
'totalSteps': 8960, 'rewardStep': 0.8637818510561226, 'errorList': [], 'lossList': [0.0, -1.362910560965538, 0.0, 377.04111824035647, 0.0, 0.0, 0.0], 'rewardMean': 0.7691800071933388, 'totalEpisodes': 69, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.77678613470608
'totalSteps': 10240, 'rewardStep': 0.8486399395810921, 'errorList': [], 'lossList': [0.0, -1.360268263220787, 0.0, 226.34093551635743, 0.0, 0.0, 0.0], 'rewardMean': 0.779112498741808, 'totalEpisodes': 107, 'stepsPerEpisode': 72, 'rewardPerEpisode': 58.54713817822131
'totalSteps': 11520, 'rewardStep': 0.616811451960604, 'errorList': [], 'lossList': [0.0, -1.3572285819053649, 0.0, 113.4588901901245, 0.0, 0.0, 0.0], 'rewardMean': 0.761079049099452, 'totalEpisodes': 142, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.47652674165084
'totalSteps': 12800, 'rewardStep': 0.673352017047755, 'errorList': [], 'lossList': [0.0, -1.3569785368442535, 0.0, 82.91098327636719, 0.0, 0.0, 0.0], 'rewardMean': 0.7523063458942822, 'totalEpisodes': 165, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.001383401042958
'totalSteps': 14080, 'rewardStep': 0.6155169916242264, 'errorList': [], 'lossList': [0.0, -1.350834704041481, 0.0, 44.56647591590881, 0.0, 0.0, 0.0], 'rewardMean': 0.75279103516252, 'totalEpisodes': 178, 'stepsPerEpisode': 63, 'rewardPerEpisode': 48.16246136112261
'totalSteps': 15360, 'rewardStep': 0.635204799826601, 'errorList': [], 'lossList': [0.0, -1.339253737926483, 0.0, 47.17101927757263, 0.0, 0.0, 0.0], 'rewardMean': 0.7287584088928271, 'totalEpisodes': 188, 'stepsPerEpisode': 68, 'rewardPerEpisode': 54.14351517352553
'totalSteps': 16640, 'rewardStep': 0.4021033661962172, 'errorList': [], 'lossList': [0.0, -1.3307187861204148, 0.0, 14.212650756835938, 0.0, 0.0, 0.0], 'rewardMean': 0.6965643974044113, 'totalEpisodes': 194, 'stepsPerEpisode': 169, 'rewardPerEpisode': 115.0285227181207
'totalSteps': 17920, 'rewardStep': 0.6705618255798266, 'errorList': [], 'lossList': [0.0, -1.3273562759160995, 0.0, 20.28378783941269, 0.0, 0.0, 0.0], 'rewardMean': 0.6977697515452921, 'totalEpisodes': 202, 'stepsPerEpisode': 54, 'rewardPerEpisode': 35.26819960715689
'totalSteps': 19200, 'rewardStep': 0.7910267317673907, 'errorList': [], 'lossList': [0.0, -1.3341119462251663, 0.0, 13.278877729177475, 0.0, 0.0, 0.0], 'rewardMean': 0.6819862976147114, 'totalEpisodes': 207, 'stepsPerEpisode': 156, 'rewardPerEpisode': 130.99361076753527
'totalSteps': 20480, 'rewardStep': 0.7872866580656788, 'errorList': [], 'lossList': [0.0, -1.329821475148201, 0.0, 6.471016854047775, 0.0, 0.0, 0.0], 'rewardMean': 0.6904285632705515, 'totalEpisodes': 211, 'stepsPerEpisode': 97, 'rewardPerEpisode': 75.79370479432629
'totalSteps': 21760, 'rewardStep': 0.9510769227858578, 'errorList': [3.2446032527758963, 1.1210829984102741, 4.643181863710795, 0.8471373759394987, 0.5850929029491414, 4.35562421477963, 1.039054492029761, 1.7763163325709475, 1.647247621488154, 1.7913270768500849, 4.063605989872465, 0.35284448484132236, 4.443957734934995, 5.994418700696704, 2.810673729223827, 1.0773534057307614, 2.0290650441886315, 2.532177770644996, 3.0367902060530816, 0.939056993040736, 0.2351038767994039, 4.865634929840905, 2.1589284289844834, 1.0071664719670332, 3.565367301033888, 1.1886502026324557, 2.4714383484998197, 0.729233336252395, 1.8724293659932219, 4.236416763725215, 1.5080927659965528, 5.547032371918956, 0.16644755181784118, 0.7293070479047556, 6.521456721155571, 7.247774386204423, 0.3131631875636845, 2.3958768731106534, 6.6225298754177615, 3.228494329844291, 0.2779172524860805, 1.029362887626558, 0.76765492186019, 5.955746205658966, 2.081114041090862, 6.29063653319874, 6.377594761360628, 0.33800242175753026, 4.835378853676722, 2.814536722271534], 'lossList': [0.0, -1.3103771257400512, 0.0, 3.8875344514846804, 0.0, 0.0, 0.0], 'rewardMean': 0.699158070443525, 'totalEpisodes': 215, 'stepsPerEpisode': 380, 'rewardPerEpisode': 307.37049530203745, 'successfulTests': 1
'totalSteps': 23040, 'rewardStep': 0.5694578863147483, 'errorList': [], 'lossList': [0.0, -1.3019571489095687, 0.0, 3.587763814330101, 0.0, 0.0, 0.0], 'rewardMean': 0.6712398651168906, 'totalEpisodes': 218, 'stepsPerEpisode': 95, 'rewardPerEpisode': 74.08112727343283
'totalSteps': 24320, 'rewardStep': 0.959599659598345, 'errorList': [0.26133548986101085, 0.0553073398641139, 0.010743514057666836, 0.07626186701453115, 0.0710954888794591, 0.12412000863048603, 0.6492821808896538, 0.4921383368061076, 0.3631489850988237, 0.06126558755515825, 0.43040149595130167, 0.15892611222633785, 0.3266792280999328, 0.0470555563554905, 0.6000181143156799, 0.13356909014622423, 0.5482651444757821, 0.37880409186481656, 0.12859970383936467, 0.24405607572401225, 0.32867278965707125, 0.18489854138913922, 0.18209837007229115, 0.027266731632992695, 0.10526198064925116, 0.7504339878515325, 0.08852578293997307, 0.0815889800994828, 0.13709706900259533, 0.20438839932305472, 0.5420311452433118, 0.010041440518529392, 0.4647939407668587, 0.19275052801311363, 0.39731850147612896, 0.07706037753606973, 0.8703098827434887, 0.6076266770329783, 0.7467543551567327, 0.017017743166435204, 0.17715670688381466, 0.46271312886445315, 0.05920271105184322, 0.10778843015899482, 1.0690066676965415, 0.2610151373586254, 0.21708248707568933, 0.481717002185037, 0.3987912222259422, 0.6920802969173222], 'lossList': [0.0, -1.2783525735139847, 0.0, 3.1946492338180543, 0.0, 0.0, 0.0], 'rewardMean': 0.7055186858806646, 'totalEpisodes': 223, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.6933894942866066, 'successfulTests': 24
'totalSteps': 25600, 'rewardStep': 0.840330503434524, 'errorList': [], 'lossList': [0.0, -1.2671989214420318, 0.0, 3.0278406739234924, 0.0, 0.0, 0.0], 'rewardMean': 0.7222165345193415, 'totalEpisodes': 224, 'stepsPerEpisode': 228, 'rewardPerEpisode': 205.142155323171
#maxSuccessfulTests=24, maxSuccessfulTestsAtStep=24320, timeSpent=106.39
