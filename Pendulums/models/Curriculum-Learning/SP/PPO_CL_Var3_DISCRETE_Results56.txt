#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 56
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6324348664108064, 'errorList': [], 'lossList': [0.0, -1.4222406631708144, 0.0, 26.879743020534516, 0.0, 0.0, 0.0], 'rewardMean': 0.7978622091607581, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.81111973182212
'totalSteps': 3840, 'rewardStep': 0.8403693505209163, 'errorList': [], 'lossList': [0.0, -1.4242901980876923, 0.0, 22.455256407260894, 0.0, 0.0, 0.0], 'rewardMean': 0.8120312562808109, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 255.20613735561352
'totalSteps': 5120, 'rewardStep': 0.7010127482662514, 'errorList': [], 'lossList': [0.0, -1.4249479389190673, 0.0, 16.55255002617836, 0.0, 0.0, 0.0], 'rewardMean': 0.784276629277171, 'totalEpisodes': 23, 'stepsPerEpisode': 72, 'rewardPerEpisode': 50.69641083423738
'totalSteps': 6400, 'rewardStep': 0.7043212459844532, 'errorList': [], 'lossList': [0.0, -1.415982192158699, 0.0, 20.11273698925972, 0.0, 0.0, 0.0], 'rewardMean': 0.7682855526186275, 'totalEpisodes': 24, 'stepsPerEpisode': 824, 'rewardPerEpisode': 585.8677893159934
'totalSteps': 7680, 'rewardStep': 0.6444584355766487, 'errorList': [], 'lossList': [0.0, -1.391428725719452, 0.0, 12.84353662520647, 0.0, 0.0, 0.0], 'rewardMean': 0.7476476997782977, 'totalEpisodes': 24, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1012.6775125649676
'totalSteps': 8960, 'rewardStep': 0.7757456908943039, 'errorList': [], 'lossList': [0.0, -1.3649542492628097, 0.0, 378.3643044281006, 0.0, 0.0, 0.0], 'rewardMean': 0.7516616985091558, 'totalEpisodes': 67, 'stepsPerEpisode': 68, 'rewardPerEpisode': 61.73604805535566
'totalSteps': 10240, 'rewardStep': 0.7684555992014654, 'errorList': [], 'lossList': [0.0, -1.3608348447084426, 0.0, 114.86931121826171, 0.0, 0.0, 0.0], 'rewardMean': 0.7537609360956944, 'totalEpisodes': 96, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.163263691617402
'totalSteps': 11520, 'rewardStep': 0.8884887467050542, 'errorList': [], 'lossList': [0.0, -1.3590142554044724, 0.0, 67.5040567779541, 0.0, 0.0, 0.0], 'rewardMean': 0.7687306928300677, 'totalEpisodes': 128, 'stepsPerEpisode': 50, 'rewardPerEpisode': 42.27684749592248
'totalSteps': 12800, 'rewardStep': 0.6601081884872825, 'errorList': [], 'lossList': [0.0, -1.3575899636745452, 0.0, 44.44662020683288, 0.0, 0.0, 0.0], 'rewardMean': 0.7578684423957892, 'totalEpisodes': 144, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.228880320467336
'totalSteps': 14080, 'rewardStep': 0.9543404620980803, 'errorList': [5.810186598545421, 4.720580868931593, 7.385583409272352, 8.163683211976332, 2.6459358874098418, 8.241029476057045, 4.724054119337945, 2.242102176388137, 7.630222484685289, 4.272162375546368, 1.1954225917768448, 7.466990846110897, 5.379657398126215, 7.00611778781986, 5.675748753426904, 3.049118770423837, 7.823866840081158, 8.360403268141354, 7.122361684377756, 6.582480597877682, 8.167857244592575, 0.30051613116483816, 0.8264683080567715, 8.034325110462467, 10.163709178868855, 5.447891813682759, 0.46257735573983616, 5.138200240257215, 0.9895340316695743, 7.569339420549221, 5.060846357459512, 7.943787278521077, 0.7279486767288805, 0.3522413101327672, 6.79384893563739, 7.058464221647944, 7.0185561123989855, 9.32530658663314, 5.3786645136428595, 2.2065564290953694, 4.640884871826093, 3.4066385807709545, 2.982538854744771, 0.35002838512122253, 4.424017799069102, 5.7935910229600704, 6.367251330165453, 8.687316317893808, 8.284826440131337, 0.9597437533804705], 'lossList': [0.0, -1.3450557589530945, 0.0, 43.233086705207825, 0.0, 0.0, 0.0], 'rewardMean': 0.7569735334145262, 'totalEpisodes': 154, 'stepsPerEpisode': 89, 'rewardPerEpisode': 70.3492743671681, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7809405576858692, 'errorList': [], 'lossList': [0.0, -1.3327105093002318, 0.0, 14.83588518857956, 0.0, 0.0, 0.0], 'rewardMean': 0.7718241025420325, 'totalEpisodes': 158, 'stepsPerEpisode': 40, 'rewardPerEpisode': 28.845705036488447
'totalSteps': 16640, 'rewardStep': 0.8847318149264307, 'errorList': [], 'lossList': [0.0, -1.3217297631502152, 0.0, 33.01927073955536, 0.0, 0.0, 0.0], 'rewardMean': 0.776260348982584, 'totalEpisodes': 160, 'stepsPerEpisode': 70, 'rewardPerEpisode': 61.54432476593216
'totalSteps': 17920, 'rewardStep': 0.8129227769970133, 'errorList': [], 'lossList': [0.0, -1.2858472448587417, 0.0, 20.77675077855587, 0.0, 0.0, 0.0], 'rewardMean': 0.7874513518556601, 'totalEpisodes': 161, 'stepsPerEpisode': 926, 'rewardPerEpisode': 735.3488730341306
'totalSteps': 19200, 'rewardStep': 0.943215571354372, 'errorList': [0.27431554174698136, 0.309619468289878, 0.30043060522725085, 0.3338184149367003, 0.32419402240175293, 0.2859093492822515, 0.3073400829982291, 0.3120320940441897, 0.3081195920076915, 0.30572565626603676, 0.31839539083649937, 0.2727835002851719, 0.3161126670864442, 0.2931580628185034, 0.28208031431161784, 0.2981960929613633, 0.26874776865531486, 0.3064812088325964, 0.2768115919140238, 0.3019055585404927, 0.2899355355777787, 0.3198798818675613, 0.29515244263640894, 0.27175302666933593, 0.29912465151646833, 0.3277942720365281, 0.27414892770043553, 0.27385517053352915, 0.26504423934357363, 0.28423738208950183, 0.30544122014658825, 0.2957219760440762, 0.294891325011629, 0.2923757819951495, 0.2792994774171991, 0.2570217268129308, 0.3049448169305281, 0.2741827533510843, 0.2653774640946744, 0.30761004299178685, 0.24954207428852715, 0.28918155770431797, 0.3223115047245454, 0.31000378727199035, 0.2990452196032349, 0.3335721996112272, 0.2816523467735003, 0.2767630721057204, 0.3081465846990148, 0.2729638334487447], 'lossList': [0.0, -1.2549247497320175, 0.0, 7.233913098573685, 0.0, 0.0, 0.0], 'rewardMean': 0.811340784392652, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1109.1285982315405, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8204524039156299, 'errorList': [], 'lossList': [0.0, -1.2087116146087646, 0.0, 4.1069014506787065, 0.0, 0.0, 0.0], 'rewardMean': 0.8289401812265501, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.127321788124
'totalSteps': 21760, 'rewardStep': 0.7782132301096493, 'errorList': [], 'lossList': [0.0, -1.155836963057518, 0.0, 4.062514533624053, 0.0, 0.0, 0.0], 'rewardMean': 0.8291869351480846, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.7683517352327
'totalSteps': 23040, 'rewardStep': 0.7556181799400241, 'errorList': [], 'lossList': [0.0, -1.1194678139686585, 0.0, 0.9256054634600878, 0.0, 0.0, 0.0], 'rewardMean': 0.8279031932219405, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 984.7664665360439
'totalSteps': 24320, 'rewardStep': 0.6937331462851031, 'errorList': [], 'lossList': [0.0, -1.1103432756662368, 0.0, 1.0461455315351487, 0.0, 0.0, 0.0], 'rewardMean': 0.8084276331799455, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.5294769241355
'totalSteps': 25600, 'rewardStep': 0.6951341652631, 'errorList': [], 'lossList': [0.0, -1.07054977953434, 0.0, 0.46452344745397567, 0.0, 0.0, 0.0], 'rewardMean': 0.8119302308575274, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 843.5549551129625
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=104.5
