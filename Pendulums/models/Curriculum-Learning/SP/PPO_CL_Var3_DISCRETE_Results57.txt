#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 57
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.6024665948879426, 'errorList': [], 'lossList': [0.0, -1.4165354841947555, 0.0, 23.93550584554672, 0.0, 0.0, 0.0], 'rewardMean': 0.6981182980340469, 'totalEpisodes': 20, 'stepsPerEpisode': 134, 'rewardPerEpisode': 81.29176411951762
'totalSteps': 3840, 'rewardStep': 0.7476596497586331, 'errorList': [], 'lossList': [0.0, -1.4073269003629685, 0.0, 25.12256549358368, 0.0, 0.0, 0.0], 'rewardMean': 0.7146320819422423, 'totalEpisodes': 23, 'stepsPerEpisode': 491, 'rewardPerEpisode': 360.6189836227023
'totalSteps': 5120, 'rewardStep': 0.8392984180292674, 'errorList': [], 'lossList': [0.0, -1.3948573023080826, 0.0, 19.90495609998703, 0.0, 0.0, 0.0], 'rewardMean': 0.7457986659639986, 'totalEpisodes': 27, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.942805791393013
'totalSteps': 6400, 'rewardStep': 0.16544997743437073, 'errorList': [], 'lossList': [0.0, -1.396834184527397, 0.0, 32.123555212020875, 0.0, 0.0, 0.0], 'rewardMean': 0.6297289282580729, 'totalEpisodes': 29, 'stepsPerEpisode': 558, 'rewardPerEpisode': 389.36792736240625
'totalSteps': 7680, 'rewardStep': 0.6943710165787682, 'errorList': [], 'lossList': [0.0, -1.393035066127777, 0.0, 15.24554249703884, 0.0, 0.0, 0.0], 'rewardMean': 0.6405026096448555, 'totalEpisodes': 29, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.6548360624729
'totalSteps': 8960, 'rewardStep': 0.572294713878841, 'errorList': [], 'lossList': [0.0, -1.4097928011417389, 0.0, 386.03541015625, 0.0, 0.0, 0.0], 'rewardMean': 0.6307586245354248, 'totalEpisodes': 81, 'stepsPerEpisode': 54, 'rewardPerEpisode': 33.254984605103054
'totalSteps': 10240, 'rewardStep': 0.606213064717475, 'errorList': [], 'lossList': [0.0, -1.411079265475273, 0.0, 124.85497581481934, 0.0, 0.0, 0.0], 'rewardMean': 0.6276904295581811, 'totalEpisodes': 115, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.606213064717475
'totalSteps': 11520, 'rewardStep': 0.598512923664897, 'errorList': [], 'lossList': [0.0, -1.412795620560646, 0.0, 55.79372400283813, 0.0, 0.0, 0.0], 'rewardMean': 0.6244484844589273, 'totalEpisodes': 139, 'stepsPerEpisode': 70, 'rewardPerEpisode': 52.28851833867198
'totalSteps': 12800, 'rewardStep': 0.8277533252768781, 'errorList': [], 'lossList': [0.0, -1.40243181347847, 0.0, 43.68521876335144, 0.0, 0.0, 0.0], 'rewardMean': 0.6447789685407224, 'totalEpisodes': 156, 'stepsPerEpisode': 76, 'rewardPerEpisode': 68.90537301811999
'totalSteps': 14080, 'rewardStep': 0.7920556432844024, 'errorList': [], 'lossList': [0.0, -1.3830324333906174, 0.0, 20.272516565322874, 0.0, 0.0, 0.0], 'rewardMean': 0.6446075327511475, 'totalEpisodes': 161, 'stepsPerEpisode': 39, 'rewardPerEpisode': 28.254876022487323
'totalSteps': 15360, 'rewardStep': 0.6922544667344175, 'errorList': [], 'lossList': [0.0, -1.365599557161331, 0.0, 25.03974211215973, 0.0, 0.0, 0.0], 'rewardMean': 0.653586319935795, 'totalEpisodes': 167, 'stepsPerEpisode': 83, 'rewardPerEpisode': 55.39886523331775
'totalSteps': 16640, 'rewardStep': 0.9347892143506278, 'errorList': [0.09221245885771323, 0.2695394808199025, 0.10393013056591921, 0.12026173698556857, 0.16308945846933365, 0.14847064114340394, 0.12560650179288246, 0.09609174442885687, 0.11946804092053186, 0.17965206801773018, 0.13549275046374137, 0.0991422806795749, 0.11149983322335888, 0.38666050797016316, 0.14476342533566391, 0.20573308258033599, 0.21508222105932187, 0.17631875293473245, 0.11342444870940097, 0.16998771512088176, 0.1042028770896588, 0.10976678217583907, 0.09392697084272754, 0.23903506536530642, 0.14265645758011128, 0.13920550479065757, 0.3239320263795381, 0.19212668722102, 0.12368961256603679, 0.10033611983642941, 0.21976774577621974, 0.2762019031541663, 0.11119234837613713, 0.15972656736643, 0.1273263199213714, 0.10758879621615888, 0.13143778566035902, 0.20985299584705147, 0.19983277137257763, 0.10289086535971874, 0.10435497867475162, 0.2954440252381714, 0.18324390199122878, 0.11570251739174148, 0.1938623049644815, 0.10144035473601415, 0.16569653828263167, 0.14715872751396752, 0.18402144300276785, 0.14068895712873003], 'lossList': [0.0, -1.367764056324959, 0.0, 10.749889904260636, 0.0, 0.0, 0.0], 'rewardMean': 0.6722992763949944, 'totalEpisodes': 171, 'stepsPerEpisode': 154, 'rewardPerEpisode': 135.5007299795527, 'successfulTests': 40
'totalSteps': 17920, 'rewardStep': 0.8247659477788883, 'errorList': [], 'lossList': [0.0, -1.3532794803380965, 0.0, 6.875776343345642, 0.0, 0.0, 0.0], 'rewardMean': 0.6708460293699565, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.1855125062755
'totalSteps': 19200, 'rewardStep': 0.9365451695353153, 'errorList': [0.04985976444291268, 0.04981961581334992, 0.04989691861391872, 0.11386344718740467, 0.06545061096762288, 0.04991738708426276, 0.0931855954424572, 0.04983288982332955, 0.050030695342998296, 0.05006334458004692, 0.05625271918800363, 0.04975228464533852, 0.08085744202001983, 0.06920502804790274, 0.0499399132517483, 0.04984265861626361, 0.04987161270901361, 0.049853980979297864, 0.09917893183808409, 0.04989885153646565, 0.05006335059394006, 0.04977654520193741, 0.08352231069345073, 0.049855265253824596, 0.04993254203693107, 0.05058316363868845, 0.050255977232325305, 0.0713420418707465, 0.0847285277449807, 0.0500824174837533, 0.10303835994844998, 0.04972238613105135, 0.05802193134711464, 0.04982606034303804, 0.07070424783074317, 0.049892542344917606, 0.08833795826194973, 0.05191622995524474, 0.07651228878867246, 0.055675339725600195, 0.08186469307601121, 0.04979488378651144, 0.050008521895646564, 0.049791110106304284, 0.08372749823663589, 0.06696852961649798, 0.058681105033054896, 0.06962113866481336, 0.04974104826281319, 0.04971834462570411], 'lossList': [0.0, -1.3109973287582397, 0.0, 12.51481049209833, 0.0, 0.0, 0.0], 'rewardMean': 0.7479555485800511, 'totalEpisodes': 172, 'stepsPerEpisode': 758, 'rewardPerEpisode': 657.6418927050719, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=91.28
