#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 58
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.716747579218783, 'errorList': [], 'lossList': [0.0, -1.4116221374273301, 0.0, 24.99706236243248, 0.0, 0.0, 0.0], 'rewardMean': 0.5305513853821189, 'totalEpisodes': 14, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.019792431251155
'totalSteps': 3840, 'rewardStep': 0.9149805715430457, 'errorList': [], 'lossList': [0.0, -1.4000626653432846, 0.0, 28.992608392238616, 0.0, 0.0, 0.0], 'rewardMean': 0.6586944474357611, 'totalEpisodes': 19, 'stepsPerEpisode': 299, 'rewardPerEpisode': 219.51185256085597
'totalSteps': 5120, 'rewardStep': 0.7414091158191021, 'errorList': [], 'lossList': [0.0, -1.4075797414779663, 0.0, 30.47893680214882, 0.0, 0.0, 0.0], 'rewardMean': 0.6793731145315964, 'totalEpisodes': 20, 'stepsPerEpisode': 180, 'rewardPerEpisode': 150.03326592077696
'totalSteps': 6400, 'rewardStep': 0.5105548125615363, 'errorList': [], 'lossList': [0.0, -1.3972861695289611, 0.0, 14.491003359556197, 0.0, 0.0, 0.0], 'rewardMean': 0.6456094541375844, 'totalEpisodes': 20, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.8942408313345
'totalSteps': 7680, 'rewardStep': 0.7271510477238368, 'errorList': [], 'lossList': [0.0, -1.378848547935486, 0.0, 12.3759394043684, 0.0, 0.0, 0.0], 'rewardMean': 0.6591997197352931, 'totalEpisodes': 20, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.6219235406976
'totalSteps': 8960, 'rewardStep': 0.5486378605439889, 'errorList': [], 'lossList': [0.0, -1.382620649933815, 0.0, 451.0227843475342, 0.0, 0.0, 0.0], 'rewardMean': 0.6434051684222497, 'totalEpisodes': 69, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.415057807656112
'totalSteps': 10240, 'rewardStep': 0.8716373435086455, 'errorList': [], 'lossList': [0.0, -1.382418799996376, 0.0, 152.95389503479004, 0.0, 0.0, 0.0], 'rewardMean': 0.6719341903080491, 'totalEpisodes': 99, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.920913691222193
'totalSteps': 11520, 'rewardStep': 0.8317960324195292, 'errorList': [], 'lossList': [0.0, -1.3829886549711228, 0.0, 105.44754014968872, 0.0, 0.0, 0.0], 'rewardMean': 0.6896966172093246, 'totalEpisodes': 126, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.353034344311075
'totalSteps': 12800, 'rewardStep': 0.9064994156481185, 'errorList': [], 'lossList': [0.0, -1.3797759419679643, 0.0, 51.62227066993714, 0.0, 0.0, 0.0], 'rewardMean': 0.7113768970532041, 'totalEpisodes': 137, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.57478361059536
'totalSteps': 14080, 'rewardStep': 0.8799766376584481, 'errorList': [], 'lossList': [0.0, -1.378825491666794, 0.0, 25.13690544128418, 0.0, 0.0, 0.0], 'rewardMean': 0.7649390416645034, 'totalEpisodes': 141, 'stepsPerEpisode': 75, 'rewardPerEpisode': 59.27041838728246
'totalSteps': 15360, 'rewardStep': 0.5121718819799485, 'errorList': [], 'lossList': [0.0, -1.3741484367847443, 0.0, 42.507863779067996, 0.0, 0.0, 0.0], 'rewardMean': 0.74448147194062, 'totalEpisodes': 147, 'stepsPerEpisode': 180, 'rewardPerEpisode': 142.18831807330878
'totalSteps': 16640, 'rewardStep': 0.6037429298917949, 'errorList': [], 'lossList': [0.0, -1.3745541948080062, 0.0, 12.317189549207688, 0.0, 0.0, 0.0], 'rewardMean': 0.7133577077754949, 'totalEpisodes': 151, 'stepsPerEpisode': 150, 'rewardPerEpisode': 80.71240841321091
'totalSteps': 17920, 'rewardStep': 0.4885770199040065, 'errorList': [], 'lossList': [0.0, -1.3758783400058747, 0.0, 6.7167737287282945, 0.0, 0.0, 0.0], 'rewardMean': 0.6880744981839854, 'totalEpisodes': 154, 'stepsPerEpisode': 445, 'rewardPerEpisode': 289.65909156142556
'totalSteps': 19200, 'rewardStep': 0.5141165357040318, 'errorList': [], 'lossList': [0.0, -1.361237931251526, 0.0, 8.740906577706337, 0.0, 0.0, 0.0], 'rewardMean': 0.6884306704982348, 'totalEpisodes': 156, 'stepsPerEpisode': 251, 'rewardPerEpisode': 154.70580325443902
'totalSteps': 20480, 'rewardStep': 0.7396761879096534, 'errorList': [], 'lossList': [0.0, -1.3562307542562484, 0.0, 18.674543137550355, 0.0, 0.0, 0.0], 'rewardMean': 0.6896831845168165, 'totalEpisodes': 158, 'stepsPerEpisode': 949, 'rewardPerEpisode': 745.1948555361339
'totalSteps': 21760, 'rewardStep': 0.9245433271516964, 'errorList': [], 'lossList': [0.0, -1.342262778878212, 0.0, 2.432621745467186, 0.0, 0.0, 0.0], 'rewardMean': 0.7272737311775873, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1006.6308038372421
'totalSteps': 23040, 'rewardStep': 0.748118841834569, 'errorList': [], 'lossList': [0.0, -1.3079596501588822, 0.0, 2.3869269847124817, 0.0, 0.0, 0.0], 'rewardMean': 0.7149218810101796, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.8307985691692
'totalSteps': 24320, 'rewardStep': 0.8622346436357804, 'errorList': [], 'lossList': [0.0, -1.2530473512411118, 0.0, 1.9605427429825069, 0.0, 0.0, 0.0], 'rewardMean': 0.7179657421318046, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.2373775508909
'totalSteps': 25600, 'rewardStep': 0.9519621575298751, 'errorList': [0.01830734800488791, 0.031596113679288024, 0.00985696279200613, 0.010334946932012524, 0.020439805567776748, 0.04523020269280314, 0.032775765354899025, 0.012887475367593739, 0.012184082395752393, 0.012423087324409119, 0.02069925630176152, 0.02209819739711567, 0.040317990831684164, 0.06000235999664287, 0.02249250194396041, 0.01580558593184664, 0.00976542298200785, 0.029038286190293135, 0.03649998607955534, 0.046773601777189575, 0.05503021076126769, 0.03254749965487838, 0.027604920394001198, 0.008744942015387927, 0.04791472472082324, 0.024984214638056248, 0.033029130565071695, 0.04485029105585631, 0.013142312138222562, 0.023684100908230758, 0.02462220431705774, 0.050034142943599376, 0.029191058361972187, 0.01864857737189068, 0.010530706154209531, 0.016265257722557904, 0.042672945764122304, 0.010493174040370125, 0.03789406057821503, 0.016103511193469103, 0.02174578501437575, 0.016813778519039056, 0.029749191187963604, 0.05174431187970953, 0.0423140043423968, 0.0292073587406026, 0.01656704925719817, 0.00916999814975374, 0.01996588584160297, 0.02919328567890578], 'lossList': [0.0, -1.2237457036972046, 0.0, 1.5898385914601385, 0.0, 0.0, 0.0], 'rewardMean': 0.7225120163199804, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1190.1621146980283, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=88.16
