#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 59
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7930420769534675, 'errorList': [], 'lossList': [0.0, -1.4114374756813048, 0.0, 23.55243757247925, 0.0, 0.0, 0.0], 'rewardMean': 0.578556091364528, 'totalEpisodes': 20, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.512210118521697
'totalSteps': 3840, 'rewardStep': 0.46685752540477443, 'errorList': [], 'lossList': [0.0, -1.422312022447586, 0.0, 22.715906591415404, 0.0, 0.0, 0.0], 'rewardMean': 0.5413232360446102, 'totalEpisodes': 27, 'stepsPerEpisode': 113, 'rewardPerEpisode': 72.55741603431655
'totalSteps': 5120, 'rewardStep': 0.623695989080709, 'errorList': [], 'lossList': [0.0, -1.432346613407135, 0.0, 19.172786490917204, 0.0, 0.0, 0.0], 'rewardMean': 0.5619164243036349, 'totalEpisodes': 31, 'stepsPerEpisode': 208, 'rewardPerEpisode': 168.83040591018786
'totalSteps': 6400, 'rewardStep': 0.6727221060452948, 'errorList': [], 'lossList': [0.0, -1.4396896713972092, 0.0, 29.235621185302733, 0.0, 0.0, 0.0], 'rewardMean': 0.5840775606519669, 'totalEpisodes': 33, 'stepsPerEpisode': 866, 'rewardPerEpisode': 658.5288995931718
'totalSteps': 7680, 'rewardStep': 0.7977640525822484, 'errorList': [], 'lossList': [0.0, -1.4289227557182311, 0.0, 12.979836930632592, 0.0, 0.0, 0.0], 'rewardMean': 0.6196919759736804, 'totalEpisodes': 33, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 990.3523733334271
'totalSteps': 8960, 'rewardStep': 0.8834727647071202, 'errorList': [], 'lossList': [0.0, -1.4095641154050826, 0.0, 334.74824409484864, 0.0, 0.0, 0.0], 'rewardMean': 0.6573749457927434, 'totalEpisodes': 73, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.629558399595403
'totalSteps': 10240, 'rewardStep': 0.9423470235650333, 'errorList': [234.68740296134513, 247.2303257307902, 234.25133429079213, 223.90178169272477, 217.35924878397577, 227.02978761575199, 268.5149554548369, 232.55851904671115, 212.876757089009, 204.82406209054187, 213.49656643831662, 235.3507923174806, 95.16571514816685, 256.90451166616856, 183.1544624304438, 248.26821487172558, 245.1618208280989, 220.29661824143892, 207.3484702869444, 210.6210119736587, 233.27710861520873, 230.20696058893117, 229.65267563854962, 247.22344372296033, 250.64594515278588, 255.69139195413373, 255.66159666197257, 72.44272995911248, 236.58312244697555, 219.40545028953863, 164.55954708200107, 219.46481823179906, 253.63457356659447, 236.77942085869694, 199.81917401863313, 241.9923887205771, 258.0057344268292, 260.28135548042644, 233.97047395436718, 190.28511997875427, 257.1254059561307, 247.41589839270662, 259.7538584870409, 242.25770145401268, 241.29434251900088, 242.19114139670265, 259.5100598872902, 219.21851644095344, 243.88278513387144, 258.91440767483414], 'lossList': [0.0, -1.4090404224395752, 0.0, 91.10565242767333, 0.0, 0.0, 0.0], 'rewardMean': 0.6929964555142796, 'totalEpisodes': 105, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.76949023474845, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6470872256396295, 'errorList': [], 'lossList': [0.0, -1.4104531252384185, 0.0, 40.74405623435974, 0.0, 0.0, 0.0], 'rewardMean': 0.6878954299726519, 'totalEpisodes': 129, 'stepsPerEpisode': 24, 'rewardPerEpisode': 15.745656175181876
'totalSteps': 12800, 'rewardStep': 0.7427182284147613, 'errorList': [], 'lossList': [0.0, -1.4005324584245682, 0.0, 23.291096267700194, 0.0, 0.0, 0.0], 'rewardMean': 0.6933777098168628, 'totalEpisodes': 139, 'stepsPerEpisode': 57, 'rewardPerEpisode': 37.011208934899365
'totalSteps': 14080, 'rewardStep': 0.6459990519999885, 'errorList': [], 'lossList': [0.0, -1.3831623870134353, 0.0, 21.6387562084198, 0.0, 0.0, 0.0], 'rewardMean': 0.7215706044393027, 'totalEpisodes': 147, 'stepsPerEpisode': 46, 'rewardPerEpisode': 29.041744164305936
'totalSteps': 15360, 'rewardStep': 0.5081042779372353, 'errorList': [], 'lossList': [0.0, -1.3658237165212632, 0.0, 40.71057165145874, 0.0, 0.0, 0.0], 'rewardMean': 0.6930768245376795, 'totalEpisodes': 157, 'stepsPerEpisode': 132, 'rewardPerEpisode': 96.70508102846996
'totalSteps': 16640, 'rewardStep': 0.5233486295216999, 'errorList': [], 'lossList': [0.0, -1.3542754709720612, 0.0, 22.94053092479706, 0.0, 0.0, 0.0], 'rewardMean': 0.698725934949372, 'totalEpisodes': 163, 'stepsPerEpisode': 119, 'rewardPerEpisode': 102.10053365785255
'totalSteps': 17920, 'rewardStep': 0.58450592391371, 'errorList': [], 'lossList': [0.0, -1.3550811809301377, 0.0, 11.046291401386261, 0.0, 0.0, 0.0], 'rewardMean': 0.6948069284326721, 'totalEpisodes': 168, 'stepsPerEpisode': 132, 'rewardPerEpisode': 97.97544256473341
'totalSteps': 19200, 'rewardStep': 0.8873019506910155, 'errorList': [], 'lossList': [0.0, -1.3484596019983293, 0.0, 10.362404934167863, 0.0, 0.0, 0.0], 'rewardMean': 0.7162649128972443, 'totalEpisodes': 170, 'stepsPerEpisode': 228, 'rewardPerEpisode': 186.49269952431717
'totalSteps': 20480, 'rewardStep': 0.8461078584594747, 'errorList': [], 'lossList': [0.0, -1.3126376551389694, 0.0, 4.515291262865066, 0.0, 0.0, 0.0], 'rewardMean': 0.7210992934849669, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1056.6466000511437
'totalSteps': 21760, 'rewardStep': 0.7800923221800137, 'errorList': [], 'lossList': [0.0, -1.3012450248003007, 0.0, 2.76603333234787, 0.0, 0.0, 0.0], 'rewardMean': 0.7107612492322563, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.8152824203441
'totalSteps': 23040, 'rewardStep': 0.8859688157974811, 'errorList': [], 'lossList': [0.0, -1.304144686460495, 0.0, 2.138772510141134, 0.0, 0.0, 0.0], 'rewardMean': 0.705123428455501, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.5463230940263
'totalSteps': 24320, 'rewardStep': 0.9197043177097308, 'errorList': [], 'lossList': [0.0, -1.289486305117607, 0.0, 2.3514889163151382, 0.0, 0.0, 0.0], 'rewardMean': 0.7323851376625111, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1177.741738057836
'totalSteps': 25600, 'rewardStep': 0.9633301641786057, 'errorList': [0.053355808683094565, 0.01661732055937972, 0.02899792351314223, 0.04204030664801679, 0.019324956734195656, 0.014864914570824964, 0.01448524787469309, 0.011369895136061283, 0.017304301740514196, 0.022554138930249743, 0.019803106534566142, 0.0189413488735272, 0.06897430586208787, 0.05384604538307194, 0.015631583110568732, 0.0950906725023862, 0.016206258008613018, 0.08945118157505484, 0.07281634666494805, 0.08215762076267089, 0.11335945348627632, 0.07686400599912016, 0.08294698276573405, 0.017664947803723775, 0.07858105549268966, 0.017737876103183975, 0.0960213242882679, 0.09470342492505288, 0.050225648020949115, 0.04520515232659216, 0.05814274083810865, 0.0708762256720781, 0.13588469000414508, 0.15119106332508517, 0.1446484550434038, 0.08237916031495039, 0.05926638978884406, 0.11645001471240642, 0.023235976180416517, 0.06271720563934649, 0.09520350834482758, 0.07327919308635407, 0.07703669574267002, 0.041195295367514345, 0.09006360810377086, 0.18237079086902452, 0.03932449038339486, 0.05984052110443417, 0.1703303542644614, 0.1606317318731637], 'lossList': [0.0, -1.2695738792419433, 0.0, 1.5333700155839325, 0.0, 0.0, 0.0], 'rewardMean': 0.7544463312388955, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.1876814770278, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=108.82
