#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 6
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6324348664108064, 'errorList': [], 'lossList': [0.0, -1.4222406631708144, 0.0, 26.879743020534516, 0.0, 0.0, 0.0], 'rewardMean': 0.7978622091607581, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.81111973182212
'totalSteps': 3840, 'rewardStep': 0.8403693505209163, 'errorList': [], 'lossList': [0.0, -1.4242901980876923, 0.0, 22.455256407260894, 0.0, 0.0, 0.0], 'rewardMean': 0.8120312562808109, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 255.20613735561352
'totalSteps': 5120, 'rewardStep': 0.7010127482662514, 'errorList': [], 'lossList': [0.0, -1.4249479389190673, 0.0, 16.55255002617836, 0.0, 0.0, 0.0], 'rewardMean': 0.784276629277171, 'totalEpisodes': 23, 'stepsPerEpisode': 72, 'rewardPerEpisode': 50.69641083423738
'totalSteps': 6400, 'rewardStep': 0.46489717615718873, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6778168115705103, 'totalEpisodes': 64, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.279763913555346
'totalSteps': 7680, 'rewardStep': 0.7737110409815623, 'errorList': [], 'lossList': [0.0, -1.4247717159986495, 0.0, 173.78295417785645, 0.0, 0.0, 0.0], 'rewardMean': 0.6915159872006607, 'totalEpisodes': 111, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.463124773908785
'totalSteps': 8960, 'rewardStep': 0.8390961028455323, 'errorList': [], 'lossList': [0.0, -1.4255240398645401, 0.0, 67.80473804473877, 0.0, 0.0, 0.0], 'rewardMean': 0.7099635016562695, 'totalEpisodes': 148, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.939697377122263
'totalSteps': 10240, 'rewardStep': 0.7946223500947205, 'errorList': [], 'lossList': [0.0, -1.4128600186109543, 0.0, 53.400863876342775, 0.0, 0.0, 0.0], 'rewardMean': 0.719370040371653, 'totalEpisodes': 172, 'stepsPerEpisode': 41, 'rewardPerEpisode': 33.268077933098255
'totalSteps': 11520, 'rewardStep': 0.5531323522268083, 'errorList': [], 'lossList': [0.0, -1.4045943593978882, 0.0, 40.17159320831299, 0.0, 0.0, 0.0], 'rewardMean': 0.7027462715571685, 'totalEpisodes': 181, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.195392652061571
'totalSteps': 12800, 'rewardStep': 0.8394719662470606, 'errorList': [], 'lossList': [0.0, -1.3781217396259309, 0.0, 30.915269157886506, 0.0, 0.0, 0.0], 'rewardMean': 0.6903645129908036, 'totalEpisodes': 190, 'stepsPerEpisode': 86, 'rewardPerEpisode': 71.4510348661975
'totalSteps': 14080, 'rewardStep': 0.7622020234281544, 'errorList': [], 'lossList': [0.0, -1.3596145659685135, 0.0, 38.20506703853607, 0.0, 0.0, 0.0], 'rewardMean': 0.7033412286925383, 'totalEpisodes': 195, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.78550050655892
'totalSteps': 15360, 'rewardStep': 0.6639329069545308, 'errorList': [], 'lossList': [0.0, -1.355987257361412, 0.0, 10.278212468624115, 0.0, 0.0, 0.0], 'rewardMean': 0.6856975843358998, 'totalEpisodes': 199, 'stepsPerEpisode': 178, 'rewardPerEpisode': 135.44751567971701
'totalSteps': 16640, 'rewardStep': 0.7989152472982166, 'errorList': [], 'lossList': [0.0, -1.3676427710056305, 0.0, 7.588063504695892, 0.0, 0.0, 0.0], 'rewardMean': 0.6954878342390963, 'totalEpisodes': 203, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.703168111504404
'totalSteps': 17920, 'rewardStep': 0.85574453524593, 'errorList': [], 'lossList': [0.0, -1.3938407492637634, 0.0, 4.995114579796791, 0.0, 0.0, 0.0], 'rewardMean': 0.7345725701479704, 'totalEpisodes': 207, 'stepsPerEpisode': 14, 'rewardPerEpisode': 13.042482636033448
'totalSteps': 19200, 'rewardStep': 0.9064391713752454, 'errorList': [], 'lossList': [0.0, -1.408498877286911, 0.0, 4.932584124207497, 0.0, 0.0, 0.0], 'rewardMean': 0.778726769669776, 'totalEpisodes': 209, 'stepsPerEpisode': 201, 'rewardPerEpisode': 173.01256021055704
'totalSteps': 20480, 'rewardStep': 0.7074866324287531, 'errorList': [], 'lossList': [0.0, -1.401472377181053, 0.0, 3.4277872058749197, 0.0, 0.0, 0.0], 'rewardMean': 0.7721043288144952, 'totalEpisodes': 211, 'stepsPerEpisode': 80, 'rewardPerEpisode': 64.4084239817615
'totalSteps': 21760, 'rewardStep': 0.8028021213526937, 'errorList': [], 'lossList': [0.0, -1.3869206345081329, 0.0, 3.3759579116106035, 0.0, 0.0, 0.0], 'rewardMean': 0.7684749306652113, 'totalEpisodes': 214, 'stepsPerEpisode': 123, 'rewardPerEpisode': 108.4622184002352
'totalSteps': 23040, 'rewardStep': 0.5147258897004973, 'errorList': [], 'lossList': [0.0, -1.373662075996399, 0.0, 2.252094002366066, 0.0, 0.0, 0.0], 'rewardMean': 0.740485284625789, 'totalEpisodes': 215, 'stepsPerEpisode': 719, 'rewardPerEpisode': 540.0443246320398
'totalSteps': 24320, 'rewardStep': 0.8515105375795937, 'errorList': [], 'lossList': [0.0, -1.3600501430034637, 0.0, 2.04205054551363, 0.0, 0.0, 0.0], 'rewardMean': 0.7703231031610676, 'totalEpisodes': 218, 'stepsPerEpisode': 215, 'rewardPerEpisode': 196.98414443443488
'totalSteps': 25600, 'rewardStep': 0.9007975740514544, 'errorList': [], 'lossList': [0.0, -1.3115204167366028, 0.0, 0.8015389627218247, 0.0, 0.0, 0.0], 'rewardMean': 0.7764556639415068, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.074651214291
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=54.26
