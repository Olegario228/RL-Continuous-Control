#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 60
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9248751944253679, 'errorList': [], 'lossList': [0.0, -1.407356454730034, 0.0, 27.204375113248826, 0.0, 0.0, 0.0], 'rewardMean': 0.8623370774621655, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.1036728716099
'totalSteps': 3840, 'rewardStep': 0.6395554860146846, 'errorList': [], 'lossList': [0.0, -1.3999825465679168, 0.0, 33.922742812633516, 0.0, 0.0, 0.0], 'rewardMean': 0.7880765469796719, 'totalEpisodes': 11, 'stepsPerEpisode': 263, 'rewardPerEpisode': 201.59114429570403
'totalSteps': 5120, 'rewardStep': 0.7105120977953714, 'errorList': [], 'lossList': [0.0, -1.4016036814451218, 0.0, 25.431722968816757, 0.0, 0.0, 0.0], 'rewardMean': 0.7686854346835967, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.9747604482272
'totalSteps': 6400, 'rewardStep': 0.8612668484829329, 'errorList': [], 'lossList': [0.0, -1.3974467796087264, 0.0, 20.9412685623765, 0.0, 0.0, 0.0], 'rewardMean': 0.787201717443464, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.486297484792
'totalSteps': 7680, 'rewardStep': 0.8317803183002082, 'errorList': [], 'lossList': [0.0, -1.3909426349401475, 0.0, 16.817427765727043, 0.0, 0.0, 0.0], 'rewardMean': 0.7946314842529213, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.9878090634184
'totalSteps': 8960, 'rewardStep': 0.8117060227895272, 'errorList': [], 'lossList': [0.0, -1.3727052634954453, 0.0, 500.8177142333984, 0.0, 0.0, 0.0], 'rewardMean': 0.7970707040438649, 'totalEpisodes': 57, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.9801630444482474
'totalSteps': 10240, 'rewardStep': 0.8621234449732474, 'errorList': [], 'lossList': [0.0, -1.3717724519968033, 0.0, 324.57077545166015, 0.0, 0.0, 0.0], 'rewardMean': 0.8052022966600378, 'totalEpisodes': 97, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.070690231016361
'totalSteps': 11520, 'rewardStep': 0.8793943764087121, 'errorList': [], 'lossList': [0.0, -1.3655451011657715, 0.0, 146.09391315460206, 0.0, 0.0, 0.0], 'rewardMean': 0.8134458610765571, 'totalEpisodes': 133, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7862387645321969
'totalSteps': 12800, 'rewardStep': 0.8768418432178114, 'errorList': [], 'lossList': [0.0, -1.3580590963363648, 0.0, 96.24073972702027, 0.0, 0.0, 0.0], 'rewardMean': 0.8197854592906826, 'totalEpisodes': 166, 'stepsPerEpisode': 51, 'rewardPerEpisode': 44.13164414457321
'totalSteps': 14080, 'rewardStep': 0.7131985905579509, 'errorList': [], 'lossList': [0.0, -1.3497910541296005, 0.0, 61.68084371566773, 0.0, 0.0, 0.0], 'rewardMean': 0.8111254222965814, 'totalEpisodes': 191, 'stepsPerEpisode': 57, 'rewardPerEpisode': 34.535636744005046
'totalSteps': 15360, 'rewardStep': 0.8518023026059174, 'errorList': [], 'lossList': [0.0, -1.334704258441925, 0.0, 55.00624811172485, 0.0, 0.0, 0.0], 'rewardMean': 0.8038181331146366, 'totalEpisodes': 209, 'stepsPerEpisode': 71, 'rewardPerEpisode': 63.06397337770642
'totalSteps': 16640, 'rewardStep': 0.8640397184984301, 'errorList': [], 'lossList': [0.0, -1.3330250197649003, 0.0, 28.21474051475525, 0.0, 0.0, 0.0], 'rewardMean': 0.8262665563630108, 'totalEpisodes': 215, 'stepsPerEpisode': 97, 'rewardPerEpisode': 80.55741060156804
'totalSteps': 17920, 'rewardStep': 0.9018448464810491, 'errorList': [], 'lossList': [0.0, -1.3338161051273345, 0.0, 49.0640472984314, 0.0, 0.0, 0.0], 'rewardMean': 0.8453998312315788, 'totalEpisodes': 224, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.108410732278585
'totalSteps': 19200, 'rewardStep': 0.7093447127551179, 'errorList': [], 'lossList': [0.0, -1.3217600643634797, 0.0, 36.210621671676634, 0.0, 0.0, 0.0], 'rewardMean': 0.8302076176587972, 'totalEpisodes': 231, 'stepsPerEpisode': 54, 'rewardPerEpisode': 42.05780034063001
'totalSteps': 20480, 'rewardStep': 0.8058507037913909, 'errorList': [], 'lossList': [0.0, -1.3114143127202988, 0.0, 18.927775502204895, 0.0, 0.0, 0.0], 'rewardMean': 0.8276146562079155, 'totalEpisodes': 236, 'stepsPerEpisode': 60, 'rewardPerEpisode': 49.392920093757255
'totalSteps': 21760, 'rewardStep': 0.6688398110738418, 'errorList': [], 'lossList': [0.0, -1.3140335500240325, 0.0, 15.310383009910584, 0.0, 0.0, 0.0], 'rewardMean': 0.8133280350363469, 'totalEpisodes': 237, 'stepsPerEpisode': 534, 'rewardPerEpisode': 412.60646112002973
'totalSteps': 23040, 'rewardStep': 0.6788769166413826, 'errorList': [], 'lossList': [0.0, -1.3008229261636735, 0.0, 9.179973564743996, 0.0, 0.0, 0.0], 'rewardMean': 0.7950033822031605, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 996.9662563898372
'totalSteps': 24320, 'rewardStep': 0.7706357777317161, 'errorList': [], 'lossList': [0.0, -1.2665349018573762, 0.0, 5.498638198077678, 0.0, 0.0, 0.0], 'rewardMean': 0.7841275223354609, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.5294092436725
'totalSteps': 25600, 'rewardStep': 0.8213479352223519, 'errorList': [], 'lossList': [0.0, -1.2366108053922653, 0.0, 3.8875966845452785, 0.0, 0.0, 0.0], 'rewardMean': 0.7785781315359148, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.065170533498
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=66.14
