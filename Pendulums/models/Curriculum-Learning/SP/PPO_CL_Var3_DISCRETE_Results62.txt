#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 62
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.817604565351236, 'errorList': [], 'lossList': [0.0, -1.4439547497034073, 0.0, 25.714645471572876, 0.0, 0.0, 0.0], 'rewardMean': 0.627817578778555, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.8221866151944
'totalSteps': 3840, 'rewardStep': 0.8819465627971335, 'errorList': [], 'lossList': [0.0, -1.452749683856964, 0.0, 40.4849648976326, 0.0, 0.0, 0.0], 'rewardMean': 0.7125272401180812, 'totalEpisodes': 14, 'stepsPerEpisode': 487, 'rewardPerEpisode': 393.1458591944858
'totalSteps': 5120, 'rewardStep': 0.6909629918169633, 'errorList': [], 'lossList': [0.0, -1.4402446401119233, 0.0, 19.477284516096116, 0.0, 0.0, 0.0], 'rewardMean': 0.7071361780428018, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 915.6590066339056
'totalSteps': 6400, 'rewardStep': 0.5316313862161144, 'errorList': [], 'lossList': [0.0, -1.424210580587387, 0.0, 30.624091376066207, 0.0, 0.0, 0.0], 'rewardMean': 0.6720352196774643, 'totalEpisodes': 15, 'stepsPerEpisode': 560, 'rewardPerEpisode': 442.0557397243816
'totalSteps': 7680, 'rewardStep': 0.7409094539208414, 'errorList': [], 'lossList': [0.0, -1.414549109339714, 0.0, 15.928519613742829, 0.0, 0.0, 0.0], 'rewardMean': 0.6835142587180272, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.5592715977277
'totalSteps': 8960, 'rewardStep': 0.8730656678027249, 'errorList': [], 'lossList': [0.0, -1.4178160399198532, 0.0, 481.966563873291, 0.0, 0.0, 0.0], 'rewardMean': 0.7105930314444125, 'totalEpisodes': 66, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.059157255519799
'totalSteps': 10240, 'rewardStep': 0.9314493122598759, 'errorList': [117.28566004890386, 100.09644260507494, 116.22426767748682, 110.66934510574542, 120.86059144789286, 119.5881255528222, 120.23625981727382, 114.08220858678176, 117.2878322908246, 110.54856509432447, 86.31798905511926, 110.30936604352728, 112.36386752415208, 109.09145714854073, 110.33355255712121, 117.44838122907136, 120.06750041946144, 115.27334861021868, 120.83708380231064, 113.41471246598142, 98.31937537673844, 114.17028324431097, 114.65556588550912, 117.07335451939973, 105.67462186371009, 115.59278928653411, 110.69727401139752, 107.59115723202756, 120.74255080525728, 119.60177787212983, 111.48174187637369, 116.28266972470786, 117.3552125921267, 104.74218737671126, 120.92514877554231, 109.29011343293244, 96.89558765900823, 120.67110278896368, 119.48096423351254, 114.35744360199166, 120.086416764512, 119.88263284013468, 113.77170784294434, 114.39619029233921, 118.60493041553188, 121.72449694778682, 117.80970391157962, 122.27878731648251, 110.52309058429047, 112.98550812831529], 'lossList': [0.0, -1.416402034163475, 0.0, 209.43138717651368, 0.0, 0.0, 0.0], 'rewardMean': 0.7382000665463455, 'totalEpisodes': 108, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.48304068478839, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9225951236828647, 'errorList': [], 'lossList': [0.0, -1.414659184217453, 0.0, 98.1233309173584, 0.0, 0.0, 0.0], 'rewardMean': 0.7586884062281809, 'totalEpisodes': 142, 'stepsPerEpisode': 43, 'rewardPerEpisode': 37.18667546567335
'totalSteps': 12800, 'rewardStep': 0.4883777537085048, 'errorList': [], 'lossList': [0.0, -1.4104673212766647, 0.0, 90.77582939147949, 0.0, 0.0, 0.0], 'rewardMean': 0.7316573409762134, 'totalEpisodes': 176, 'stepsPerEpisode': 9, 'rewardPerEpisode': 4.526560094175791
'totalSteps': 14080, 'rewardStep': 0.45714200929227494, 'errorList': [], 'lossList': [0.0, -1.4029410725831986, 0.0, 50.87085753440857, 0.0, 0.0, 0.0], 'rewardMean': 0.7335684826848533, 'totalEpisodes': 200, 'stepsPerEpisode': 15, 'rewardPerEpisode': 9.447931871768677
'totalSteps': 15360, 'rewardStep': 0.4511135554758178, 'errorList': [], 'lossList': [0.0, -1.3937001401185989, 0.0, 29.990434532165526, 0.0, 0.0, 0.0], 'rewardMean': 0.6969193816973116, 'totalEpisodes': 218, 'stepsPerEpisode': 81, 'rewardPerEpisode': 55.61958453478707
'totalSteps': 16640, 'rewardStep': 0.5424700139370779, 'errorList': [], 'lossList': [0.0, -1.3872272217273711, 0.0, 25.421985578536987, 0.0, 0.0, 0.0], 'rewardMean': 0.662971726811306, 'totalEpisodes': 229, 'stepsPerEpisode': 80, 'rewardPerEpisode': 61.48507727100193
'totalSteps': 17920, 'rewardStep': 0.7993579610945118, 'errorList': [], 'lossList': [0.0, -1.384382849931717, 0.0, 25.967921199798585, 0.0, 0.0, 0.0], 'rewardMean': 0.673811223739061, 'totalEpisodes': 239, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.275483288202516
'totalSteps': 19200, 'rewardStep': 0.9118899683932887, 'errorList': [], 'lossList': [0.0, -1.3774502277374268, 0.0, 20.23630873680115, 0.0, 0.0, 0.0], 'rewardMean': 0.7118370819567783, 'totalEpisodes': 250, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.706054018737944
'totalSteps': 20480, 'rewardStep': 0.9550682282153797, 'errorList': [4.473204087346644, 85.61361273173783, 67.00117821412915, 65.89927884343524, 112.26416683745622, 6.399935140192953, 11.565277856871411, 78.76013396249354, 87.44430049100822, 37.69348729225626, 62.24162361730016, 89.90184980970432, 56.58531786396906, 0.10068814445012705, 31.930286704654424, 69.18145213948875, 14.818573634177543, 60.67891108040124, 12.292073070152696, 6.834996689742216, 33.132902166085195, 89.53054144920033, 58.21850410225055, 22.17218022267625, 51.0258497409654, 74.30281976950809, 10.053555160540306, 54.34649457259626, 110.09201299024258, 81.92587622703155, 58.16706929393378, 120.61789956080497, 82.67132312816844, 17.65582761866259, 39.91006127315582, 86.88580809594856, 39.9683516518644, 30.50808731428459, 79.08981931853387, 107.67679163332609, 46.805155296309394, 84.51244528498165, 62.55295865679015, 84.88842903495421, 17.927049370976523, 46.857498957316075, 24.054560580261146, 51.868632976783125, 69.95364912484925, 27.179782312353673], 'lossList': [0.0, -1.3815129834413529, 0.0, 12.671136255264281, 0.0, 0.0, 0.0], 'rewardMean': 0.7332529593862321, 'totalEpisodes': 256, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.571416382810401, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.7030315694440392, 'errorList': [], 'lossList': [0.0, -1.3730035722255707, 0.0, 13.681514432430268, 0.0, 0.0, 0.0], 'rewardMean': 0.7162495495503636, 'totalEpisodes': 261, 'stepsPerEpisode': 239, 'rewardPerEpisode': 191.94862139314594
'totalSteps': 23040, 'rewardStep': 0.8692645557900139, 'errorList': [], 'lossList': [0.0, -1.3481443375349045, 0.0, 9.378166708946228, 0.0, 0.0, 0.0], 'rewardMean': 0.7100310739033775, 'totalEpisodes': 267, 'stepsPerEpisode': 74, 'rewardPerEpisode': 63.13533829566862
'totalSteps': 24320, 'rewardStep': 0.8059115411273806, 'errorList': [], 'lossList': [0.0, -1.3400333374738693, 0.0, 5.401152131557464, 0.0, 0.0, 0.0], 'rewardMean': 0.6983627156478289, 'totalEpisodes': 272, 'stepsPerEpisode': 160, 'rewardPerEpisode': 139.46311434945272
'totalSteps': 25600, 'rewardStep': 0.8635949575448956, 'errorList': [], 'lossList': [0.0, -1.3317939406633377, 0.0, 5.243312587738037, 0.0, 0.0, 0.0], 'rewardMean': 0.735884436031468, 'totalEpisodes': 276, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.486295289566023
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=20480, timeSpent=107.59
