#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 63
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.657187940170172, 'errorList': [], 'lossList': [0.0, -1.4219418781995774, 0.0, 27.9683056306839, 0.0, 0.0, 0.0], 'rewardMean': 0.711256886558614, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.155583018468704
'totalSteps': 3840, 'rewardStep': 0.9612180606389581, 'errorList': [], 'lossList': [0.0, -1.4156839829683303, 0.0, 27.140998220443727, 0.0, 0.0, 0.0], 'rewardMean': 0.7945772779187287, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 374.2310408960704
'totalSteps': 5120, 'rewardStep': 0.8097549536927952, 'errorList': [], 'lossList': [0.0, -1.4097404754161835, 0.0, 27.785908808112143, 0.0, 0.0, 0.0], 'rewardMean': 0.7983716968622453, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.4483637329215
'totalSteps': 6400, 'rewardStep': 0.7022961852205006, 'errorList': [], 'lossList': [0.0, -1.3900028657913208, 0.0, 20.386204572319983, 0.0, 0.0, 0.0], 'rewardMean': 0.7791565945338964, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.0193214434457
'totalSteps': 7680, 'rewardStep': 0.9918523167229493, 'errorList': [80.33318700010369, 78.111868237376, 82.5223957351747, 79.82024050768833, 81.91406459167156, 76.38852461714468, 79.6681950911861, 80.61279229515338, 82.17810919938833, 80.1153045503885, 75.11849200217686, 73.81469852073486, 78.72288893797935, 81.21883841164545, 80.99556387253243, 81.40064210904166, 78.34349056732911, 79.31824355300934, 80.15673320828897, 79.3468586624121, 76.92218322035238, 79.92802514591486, 77.27579158213882, 78.78671442060575, 81.05096792929078, 81.074325765895, 80.38282009184493, 81.00234930521535, 79.16596980519242, 80.58019852165283, 74.75936104484248, 71.06838454655859, 80.90308508953188, 79.24390080117504, 75.03120984271007, 78.66882032381724, 80.441263254471, 80.42795942358211, 81.28217232514534, 79.70539709984159, 81.68247998982915, 80.48801129872643, 82.15069835509588, 80.61242720064335, 75.96475168863047, 80.64881110641839, 75.79911286141582, 76.82549268509433, 78.98531801726068, 78.1958269938211], 'lossList': [0.0, -1.3700878071784972, 0.0, 18.063391667604446, 0.0, 0.0, 0.0], 'rewardMean': 0.8146058815654053, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1120.5015039084467, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.6740864073718177, 'errorList': [], 'lossList': [0.0, -1.3645391643047333, 0.0, 535.9532525634766, 0.0, 0.0, 0.0], 'rewardMean': 0.7945316709663214, 'totalEpisodes': 60, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6740864073718177
'totalSteps': 10240, 'rewardStep': 0.7280629395379358, 'errorList': [], 'lossList': [0.0, -1.36196941614151, 0.0, 377.8528255462646, 0.0, 0.0, 0.0], 'rewardMean': 0.7862230795377732, 'totalEpisodes': 114, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.744551521644013
'totalSteps': 11520, 'rewardStep': 0.5513384125280983, 'errorList': [], 'lossList': [0.0, -1.3578673088550568, 0.0, 133.63816844940186, 0.0, 0.0, 0.0], 'rewardMean': 0.7601247832033649, 'totalEpisodes': 154, 'stepsPerEpisode': 15, 'rewardPerEpisode': 8.43880761088468
'totalSteps': 12800, 'rewardStep': 0.8485375023190448, 'errorList': [], 'lossList': [0.0, -1.3598908978700637, 0.0, 74.22045112609864, 0.0, 0.0, 0.0], 'rewardMean': 0.7689660551149328, 'totalEpisodes': 179, 'stepsPerEpisode': 80, 'rewardPerEpisode': 69.37276052053441
'totalSteps': 14080, 'rewardStep': 0.6744579170411509, 'errorList': [], 'lossList': [0.0, -1.3651937872171402, 0.0, 45.60005499839783, 0.0, 0.0, 0.0], 'rewardMean': 0.7598792635243423, 'totalEpisodes': 194, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.61393188989819
'totalSteps': 15360, 'rewardStep': 0.4806295279607225, 'errorList': [], 'lossList': [0.0, -1.3578411662578582, 0.0, 36.995968794822694, 0.0, 0.0, 0.0], 'rewardMean': 0.7422234223033973, 'totalEpisodes': 205, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.4662320830600475
'totalSteps': 16640, 'rewardStep': 0.4360066621652831, 'errorList': [], 'lossList': [0.0, -1.3506474995613098, 0.0, 14.98555423974991, 0.0, 0.0, 0.0], 'rewardMean': 0.6897022824560299, 'totalEpisodes': 212, 'stepsPerEpisode': 90, 'rewardPerEpisode': 47.83616898040757
'totalSteps': 17920, 'rewardStep': 0.8457152155342182, 'errorList': [], 'lossList': [0.0, -1.364962500333786, 0.0, 10.454145617485047, 0.0, 0.0, 0.0], 'rewardMean': 0.6932983086401722, 'totalEpisodes': 217, 'stepsPerEpisode': 165, 'rewardPerEpisode': 144.26798829759016
'totalSteps': 19200, 'rewardStep': 0.8448125231549343, 'errorList': [], 'lossList': [0.0, -1.3683097118139267, 0.0, 55.63422636508942, 0.0, 0.0, 0.0], 'rewardMean': 0.7075499424336156, 'totalEpisodes': 221, 'stepsPerEpisode': 85, 'rewardPerEpisode': 70.63073964040314
'totalSteps': 20480, 'rewardStep': 0.8589335684633677, 'errorList': [], 'lossList': [0.0, -1.3394525724649429, 0.0, 6.224358142614364, 0.0, 0.0, 0.0], 'rewardMean': 0.6942580676076573, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.5796356250091
'totalSteps': 21760, 'rewardStep': 0.8914656969479141, 'errorList': [], 'lossList': [0.0, -1.3028363591432572, 0.0, 5.281941128075123, 0.0, 0.0, 0.0], 'rewardMean': 0.7159959965652669, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.3910905354444
'totalSteps': 23040, 'rewardStep': 0.7667320539801287, 'errorList': [], 'lossList': [0.0, -1.256721727848053, 0.0, 4.358884691447019, 0.0, 0.0, 0.0], 'rewardMean': 0.7198629080094863, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.3673496614686
'totalSteps': 24320, 'rewardStep': 0.7537224324156336, 'errorList': [], 'lossList': [0.0, -1.2118000334501267, 0.0, 2.818724564015865, 0.0, 0.0, 0.0], 'rewardMean': 0.7401013099982399, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.2760841435888
'totalSteps': 25600, 'rewardStep': 0.9670796706861045, 'errorList': [0.0131652871982352, 0.006828954738208734, 0.01064229069076977, 0.017237560898028595, 0.01794613528765291, 0.025061161927713863, 0.021877239875632216, 0.012251061523615383, 0.006150981122733153, 0.03121201574582374, 0.020719301729018612, 0.04674464743158132, 0.011790764412944715, 0.026067639053492197, 0.010852290411883927, 0.012621565507074004, 0.020799748692375983, 0.023092599131050575, 0.0259569287762627, 0.03801474139592226, 0.015551549698670366, 0.013083655797132398, 0.011962563335221953, 0.039527130762024375, 0.004242011223530011, 0.022541539766681346, 0.014461519299227357, 0.021898885828697208, 0.02095132361479719, 0.004960740259016738, 0.01629647922564468, 0.014000985267931473, 0.004683227355699215, 0.023207028139304437, 0.021891988887104606, 0.03593025082325978, 0.003834115366116043, 0.027953871458963053, 0.006394629636791621, 0.024091620712480128, 0.025387582673273214, 0.009994421700819132, 0.021704933189275327, 0.040259770583812825, 0.04973888511520185, 0.035582926365601945, 0.014260701674193393, 0.018487084766113, 0.024001812293453977, 0.005079168566037062], 'lossList': [0.0, -1.1892950218915939, 0.0, 2.6161896742135284, 0.0, 0.0, 0.0], 'rewardMean': 0.7519555268349457, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.393902785573, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=106.19
