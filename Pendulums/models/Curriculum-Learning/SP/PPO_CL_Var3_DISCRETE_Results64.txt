#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 64
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8691065996348363, 'errorList': [], 'lossList': [0.0, -1.4240562045574188, 0.0, 30.64595845937729, 0.0, 0.0, 0.0], 'rewardMean': 0.7705110028890898, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 252.45415072774313
'totalSteps': 3840, 'rewardStep': 0.7734360832441656, 'errorList': [], 'lossList': [0.0, -1.4417775994539261, 0.0, 30.45178715825081, 0.0, 0.0, 0.0], 'rewardMean': 0.7714860296741151, 'totalEpisodes': 15, 'stepsPerEpisode': 168, 'rewardPerEpisode': 126.71211759057093
'totalSteps': 5120, 'rewardStep': 0.6133946466983342, 'errorList': [], 'lossList': [0.0, -1.4452192854881287, 0.0, 23.043071581721307, 0.0, 0.0, 0.0], 'rewardMean': 0.7319631839301699, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 957.1308145624366
'totalSteps': 6400, 'rewardStep': 0.8500092483935022, 'errorList': [], 'lossList': [0.0, -1.4314566057920457, 0.0, 19.500214830636978, 0.0, 0.0, 0.0], 'rewardMean': 0.7555723968228364, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.3355867835556
'totalSteps': 7680, 'rewardStep': 0.9087090377667698, 'errorList': [], 'lossList': [0.0, -1.431219025850296, 0.0, 12.907236521840096, 0.0, 0.0, 0.0], 'rewardMean': 0.781095170313492, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.1900096331794
'totalSteps': 8960, 'rewardStep': 0.8906080773259869, 'errorList': [], 'lossList': [0.0, -1.4155626261234284, 0.0, 548.0558575439453, 0.0, 0.0, 0.0], 'rewardMean': 0.796739871315277, 'totalEpisodes': 70, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.526675669966842
'totalSteps': 10240, 'rewardStep': 0.7341637641437727, 'errorList': [], 'lossList': [0.0, -1.413008398413658, 0.0, 348.23450927734376, 0.0, 0.0, 0.0], 'rewardMean': 0.788917857918839, 'totalEpisodes': 126, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.874484578270285
'totalSteps': 11520, 'rewardStep': 0.7934952720011065, 'errorList': [], 'lossList': [0.0, -1.4060821437835693, 0.0, 141.0515163421631, 0.0, 0.0, 0.0], 'rewardMean': 0.7894264594835353, 'totalEpisodes': 181, 'stepsPerEpisode': 68, 'rewardPerEpisode': 56.49111596261236
'totalSteps': 12800, 'rewardStep': 0.9398340933608568, 'errorList': [151.34453522894896, 155.15571880646692, 131.13231969224594, 145.02992384091542, 123.63014430884964, 156.9820309497552, 147.36834537316267, 146.84417964713353, 156.63623369709308, 141.57110347832756, 149.3989640052636, 161.27918800351964, 156.51195365502429, 156.98090005161052, 157.28276821989826, 160.862100051169, 128.61868514713632, 156.59203759341062, 148.29821135013748, 154.74475613865545, 160.13631131323638, 156.49294557632567, 133.4094846004744, 153.81523914297443, 157.36449605329832, 153.18520526372583, 165.5566542513122, 139.55760091970086, 126.67814993026356, 144.38116724208717, 143.74398648554103, 151.29774871519763, 139.39803049706552, 150.96279329888063, 137.1379114352258, 151.61144784943488, 152.31822345457016, 147.22584877116645, 154.07682968342257, 150.27634629315887, 150.29432081988804, 164.61706012119825, 150.00534217399016, 160.1557168144418, 162.1570827810008, 149.04407206548802, 161.62117527568205, 132.7410332754392, 153.3398692633199, 143.05189965327338], 'lossList': [0.0, -1.3959228849411012, 0.0, 69.94190488815308, 0.0, 0.0, 0.0], 'rewardMean': 0.8044672228712674, 'totalEpisodes': 225, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.929074621127262, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.789569322399583, 'errorList': [], 'lossList': [0.0, -1.3911060243844986, 0.0, 57.06554581642151, 0.0, 0.0, 0.0], 'rewardMean': 0.8162326144968916, 'totalEpisodes': 248, 'stepsPerEpisode': 64, 'rewardPerEpisode': 47.041129293655835
'totalSteps': 15360, 'rewardStep': 0.9469811036539898, 'errorList': [193.7294829679138, 184.14815778602767, 178.7493808092082, 158.38829883320778, 18.193952282548185, 163.90380244277364, 195.87023743628654, 91.02064261208153, 147.81528916939175, 176.07526732193247, 89.04644790933817, 165.7694084852163, 201.21525025485585, 192.44116010377047, 188.37619646880785, 175.93678642044375, 34.70595950135709, 62.37263032200983, 135.6889835065478, 145.83155772607378, 172.39480274312294, 181.49994574041884, 173.69988475660182, 170.80456224375843, 60.05414833110618, 145.2216141324909, 177.67551345635889, 181.14348736959082, 134.5584313932711, 190.24291278750303, 168.90305836559182, 121.42534399172835, 150.15974016911989, 105.88732649312843, 123.15197024645676, 87.12136675844853, 168.03449349065397, 160.89597126886224, 189.35253535956244, 183.6188570920034, 141.2062943109886, 128.71436165179838, 133.7966462308632, 128.48578968508858, 164.45012797845428, 167.1108014745099, 193.3316505025035, 166.66103620786083, 83.25439989869066, 187.66027143688862], 'lossList': [0.0, -1.3816241186857223, 0.0, 38.064136276245115, 0.0, 0.0, 0.0], 'rewardMean': 0.8240200648988066, 'totalEpisodes': 260, 'stepsPerEpisode': 42, 'rewardPerEpisode': 40.105233171332095, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7106735931099998, 'errorList': [], 'lossList': [0.0, -1.3656060206890106, 0.0, 42.45687985420227, 0.0, 0.0, 0.0], 'rewardMean': 0.8177438158853901, 'totalEpisodes': 273, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7106735931099998
'totalSteps': 17920, 'rewardStep': 0.8443173154895173, 'errorList': [], 'lossList': [0.0, -1.358952579498291, 0.0, 27.952903976440428, 0.0, 0.0, 0.0], 'rewardMean': 0.8408360827645085, 'totalEpisodes': 282, 'stepsPerEpisode': 98, 'rewardPerEpisode': 84.43123238694803
'totalSteps': 19200, 'rewardStep': 0.26366275423460833, 'errorList': [], 'lossList': [0.0, -1.3498333489894867, 0.0, 16.455915808677673, 0.0, 0.0, 0.0], 'rewardMean': 0.7822014333486191, 'totalEpisodes': 287, 'stepsPerEpisode': 322, 'rewardPerEpisode': 243.66827022562043
'totalSteps': 20480, 'rewardStep': 0.1785638455356099, 'errorList': [], 'lossList': [0.0, -1.3491489326953887, 0.0, 14.135499279499054, 0.0, 0.0, 0.0], 'rewardMean': 0.709186914125503, 'totalEpisodes': 292, 'stepsPerEpisode': 283, 'rewardPerEpisode': 213.5454788558952
'totalSteps': 21760, 'rewardStep': 0.8211268684795198, 'errorList': [], 'lossList': [0.0, -1.3570172649621963, 0.0, 12.977675079107284, 0.0, 0.0, 0.0], 'rewardMean': 0.7022387932408564, 'totalEpisodes': 298, 'stepsPerEpisode': 108, 'rewardPerEpisode': 93.08322338064012
'totalSteps': 23040, 'rewardStep': 0.848499110118013, 'errorList': [], 'lossList': [0.0, -1.3547846972942352, 0.0, 8.037549374103547, 0.0, 0.0, 0.0], 'rewardMean': 0.7136723278382804, 'totalEpisodes': 302, 'stepsPerEpisode': 81, 'rewardPerEpisode': 70.30838841472391
'totalSteps': 24320, 'rewardStep': 0.7425088461452913, 'errorList': [], 'lossList': [0.0, -1.3440498232841491, 0.0, 7.084551443457603, 0.0, 0.0, 0.0], 'rewardMean': 0.7085736852526989, 'totalEpisodes': 304, 'stepsPerEpisode': 124, 'rewardPerEpisode': 106.62174097432872
'totalSteps': 25600, 'rewardStep': 0.9020267388636257, 'errorList': [], 'lossList': [0.0, -1.3338854289054871, 0.0, 5.335291690826416, 0.0, 0.0, 0.0], 'rewardMean': 0.7047929498029758, 'totalEpisodes': 307, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.772327289370468
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=106.51
