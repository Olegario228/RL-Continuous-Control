#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 65
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9360084744127758, 'errorList': [], 'lossList': [0.0, -1.4352493596076965, 0.0, 30.507024736404418, 0.0, 0.0, 0.0], 'rewardMean': 0.9157950328104714, 'totalEpisodes': 8, 'stepsPerEpisode': 529, 'rewardPerEpisode': 377.10779752439424
'totalSteps': 3840, 'rewardStep': 0.7557452886750211, 'errorList': [], 'lossList': [0.0, -1.4207161974906921, 0.0, 32.01956871271133, 0.0, 0.0, 0.0], 'rewardMean': 0.8624451180986545, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 203.96302985448395
'totalSteps': 5120, 'rewardStep': 0.7637633211487607, 'errorList': [], 'lossList': [0.0, -1.4165323793888092, 0.0, 27.78206905066967, 0.0, 0.0, 0.0], 'rewardMean': 0.8377746688611811, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.0989296707711
'totalSteps': 6400, 'rewardStep': 0.9641597693966607, 'errorList': [], 'lossList': [0.0, -1.4044303733110428, 0.0, 23.92006703555584, 0.0, 0.0, 0.0], 'rewardMean': 0.863051688968277, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.5310283831257
'totalSteps': 7680, 'rewardStep': 0.7471233495034122, 'errorList': [], 'lossList': [0.0, -1.3953341794013978, 0.0, 17.48394833922386, 0.0, 0.0, 0.0], 'rewardMean': 0.8437302990574662, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.6654996091224
'totalSteps': 8960, 'rewardStep': 0.7920290781930566, 'errorList': [], 'lossList': [0.0, -1.3641984176635742, 0.0, 541.0478718566894, 0.0, 0.0, 0.0], 'rewardMean': 0.8363444103625506, 'totalEpisodes': 60, 'stepsPerEpisode': 72, 'rewardPerEpisode': 60.2879812550518
'totalSteps': 10240, 'rewardStep': 0.6375492039506879, 'errorList': [], 'lossList': [0.0, -1.3634972041845321, 0.0, 404.08664108276366, 0.0, 0.0, 0.0], 'rewardMean': 0.8114950095610678, 'totalEpisodes': 110, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.813442802426414
'totalSteps': 11520, 'rewardStep': 0.6641903785365184, 'errorList': [], 'lossList': [0.0, -1.3637367004156113, 0.0, 220.81422782897948, 0.0, 0.0, 0.0], 'rewardMean': 0.7951278283361178, 'totalEpisodes': 154, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.220563710381363
'totalSteps': 12800, 'rewardStep': 0.9464365304068924, 'errorList': [148.4636813771994, 172.16016434948452, 156.20837045383357, 155.00183459742829, 172.71028884820345, 168.34264259435488, 166.08881999626823, 138.17556467758683, 169.37903435057333, 159.80103574406942, 133.86226750572598, 171.6226415808901, 151.12300438807753, 157.98576613389187, 156.11291146101541, 152.71942057327715, 161.74320594366944, 161.63984743419098, 163.12627766009095, 151.86314390504228, 168.29093965973092, 157.64575100955742, 155.71240828230322, 168.37607340046316, 120.761939595866, 148.61379988925958, 157.72970194786896, 165.63970671490753, 112.08118419816162, 169.1523459895795, 172.26917675599876, 146.7614711981088, 172.49976258073966, 172.72849247242777, 165.7047974432917, 158.14295127243673, 165.08696406454365, 172.4730272463823, 166.42814711649297, 168.1124857643184, 168.6673607469128, 171.4846119821071, 168.56322653065166, 159.62103491707367, 166.37557423916044, 164.0946046473781, 167.76666143635615, 163.2932255713417, 151.4998920481492, 166.85231417195712], 'lossList': [0.0, -1.3659561741352082, 0.0, 113.69734336853027, 0.0, 0.0, 0.0], 'rewardMean': 0.8102586985431953, 'totalEpisodes': 195, 'stepsPerEpisode': 26, 'rewardPerEpisode': 24.062597112158233, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.49279177402393515, 'errorList': [], 'lossList': [0.0, -1.3660463547706605, 0.0, 62.51247846603393, 0.0, 0.0, 0.0], 'rewardMean': 0.7699797168247721, 'totalEpisodes': 212, 'stepsPerEpisode': 24, 'rewardPerEpisode': 15.939391036446098
'totalSteps': 15360, 'rewardStep': 0.8518305972483132, 'errorList': [], 'lossList': [0.0, -1.3679365795850753, 0.0, 68.46830867767333, 0.0, 0.0, 0.0], 'rewardMean': 0.7615619291083259, 'totalEpisodes': 237, 'stepsPerEpisode': 37, 'rewardPerEpisode': 35.04424827872298
'totalSteps': 16640, 'rewardStep': 0.6826745740276562, 'errorList': [], 'lossList': [0.0, -1.3725572049617767, 0.0, 26.351560726165772, 0.0, 0.0, 0.0], 'rewardMean': 0.7542548576435893, 'totalEpisodes': 250, 'stepsPerEpisode': 104, 'rewardPerEpisode': 85.31048469081503
'totalSteps': 17920, 'rewardStep': 0.8396082677385434, 'errorList': [], 'lossList': [0.0, -1.3741678005456925, 0.0, 42.302781867980954, 0.0, 0.0, 0.0], 'rewardMean': 0.7618393523025676, 'totalEpisodes': 266, 'stepsPerEpisode': 36, 'rewardPerEpisode': 32.159478928539244
'totalSteps': 19200, 'rewardStep': 0.8808489215387107, 'errorList': [], 'lossList': [0.0, -1.3772161096334457, 0.0, 20.330713083744048, 0.0, 0.0, 0.0], 'rewardMean': 0.7535082675167726, 'totalEpisodes': 273, 'stepsPerEpisode': 33, 'rewardPerEpisode': 25.865727141715908
'totalSteps': 20480, 'rewardStep': 0.9610714625482425, 'errorList': [117.51515876452203, 92.55697654360038, 13.187670177541149, 155.59194891168428, 103.79100973087048, 207.07954618842678, 179.96247836119556, 162.51248950651132, 52.46831069503762, 124.3050236174318, 29.046324511640897, 193.65768336866418, 1.7746195554308777, 186.30677968866502, 58.290773025966644, 148.5650467035272, 209.22086865909614, 74.71897937355524, 138.96346620950243, 134.05624320363384, 163.83837193995106, 107.29793841664402, 114.75825844920512, 160.58428448021976, 83.67747645702838, 98.76319451790741, 114.12275337206566, 117.36789290756641, 119.14858427750947, 186.100873185719, 9.935046743029417, 166.36754426219846, 181.38394430758632, 163.00474410128672, 164.96359720649676, 72.74542042714886, 152.56671748971152, 117.75249389213663, 136.6143781675168, 142.02782824184007, 135.16100550082928, 157.61829769146667, 83.52160944234622, 0.6430447969199335, 161.1652581626008, 91.37926780695499, 182.51597389613949, 187.3425346181185, 132.0515197878908, 187.6301091891992], 'lossList': [0.0, -1.3756620854139328, 0.0, 24.56016042470932, 0.0, 0.0, 0.0], 'rewardMean': 0.7749030788212556, 'totalEpisodes': 280, 'stepsPerEpisode': 25, 'rewardPerEpisode': 22.31969135116328, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.5496739708673022, 'errorList': [], 'lossList': [0.0, -1.3752380698919295, 0.0, 24.35258515357971, 0.0, 0.0, 0.0], 'rewardMean': 0.7506675680886803, 'totalEpisodes': 285, 'stepsPerEpisode': 266, 'rewardPerEpisode': 201.82690338274907
'totalSteps': 23040, 'rewardStep': 0.8796385190726507, 'errorList': [], 'lossList': [0.0, -1.378000938296318, 0.0, 23.21315339565277, 0.0, 0.0, 0.0], 'rewardMean': 0.7748764996008765, 'totalEpisodes': 292, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.992401676264759
'totalSteps': 24320, 'rewardStep': 0.8732359332682115, 'errorList': [], 'lossList': [0.0, -1.3808483582735063, 0.0, 9.846187851428986, 0.0, 0.0, 0.0], 'rewardMean': 0.7957810550740458, 'totalEpisodes': 296, 'stepsPerEpisode': 38, 'rewardPerEpisode': 32.69686072316773
'totalSteps': 25600, 'rewardStep': 0.6254277949937525, 'errorList': [], 'lossList': [0.0, -1.3701048171520234, 0.0, 12.45175553202629, 0.0, 0.0, 0.0], 'rewardMean': 0.7636801815327318, 'totalEpisodes': 299, 'stepsPerEpisode': 301, 'rewardPerEpisode': 254.50869897306927
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=109.49
