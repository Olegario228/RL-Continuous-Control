#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 66
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8584804343243699, 'errorList': [], 'lossList': [0.0, -1.444619840979576, 0.0, 36.9149765253067, 0.0, 0.0, 0.0], 'rewardMean': 0.7085640764003625, 'totalEpisodes': 12, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.028341765022766
'totalSteps': 3840, 'rewardStep': 0.8945312046550165, 'errorList': [], 'lossList': [0.0, -1.4570085686445235, 0.0, 30.45998518407345, 0.0, 0.0, 0.0], 'rewardMean': 0.7705531191519137, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.9854716647518
'totalSteps': 5120, 'rewardStep': 0.6426887006587513, 'errorList': [], 'lossList': [0.0, -1.4410074639320374, 0.0, 29.687185053825377, 0.0, 0.0, 0.0], 'rewardMean': 0.7385870145286231, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.2280427395226
'totalSteps': 6400, 'rewardStep': 0.9462534970079105, 'errorList': [], 'lossList': [0.0, -1.4400340634584428, 0.0, 23.752558589577674, 0.0, 0.0, 0.0], 'rewardMean': 0.7801203110244805, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.7706869240055
'totalSteps': 7680, 'rewardStep': 0.8494213103252838, 'errorList': [], 'lossList': [0.0, -1.421427276134491, 0.0, 19.919883989840745, 0.0, 0.0, 0.0], 'rewardMean': 0.7916704775746144, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.0201867950414
'totalSteps': 8960, 'rewardStep': 0.6368994944033963, 'errorList': [], 'lossList': [0.0, -1.4116316485404967, 0.0, 623.5940921020508, 0.0, 0.0, 0.0], 'rewardMean': 0.7695603371215832, 'totalEpisodes': 61, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.652799296614619
'totalSteps': 10240, 'rewardStep': 0.9533026318101209, 'errorList': [104.20861782201034, 111.9605178327057, 108.11469597097292, 110.39598460181263, 113.49385673767438, 112.10763894893843, 107.7561358811897, 108.51335670420093, 112.59970470132296, 110.20946324512191, 94.90584489928078, 103.95518210254492, 112.30620549074936, 115.08817086735671, 101.05832103446244, 77.96894948418903, 112.73218815924896, 109.3576819233511, 100.10197918613625, 113.18478758117476, 114.63778023104265, 107.08050862061842, 112.97976323331157, 110.46423613583826, 103.58962900627822, 115.36473058665952, 109.12662424637111, 114.10799837642296, 112.45431883468963, 111.44953657200915, 109.51180650775392, 114.22547182621777, 106.82330402777497, 109.43814166037258, 112.76400528978077, 108.42250031305872, 105.7759187362135, 113.8638238031467, 110.64231818717508, 106.55568058514369, 114.6449596223837, 115.94147381058166, 89.97293172044257, 106.42656570245643, 112.74697666732766, 109.01122141086962, 95.76093999792666, 111.4073074108146, 114.619368162241, 114.36647275474166], 'lossList': [0.0, -1.4095374405384065, 0.0, 417.9762801361084, 0.0, 0.0, 0.0], 'rewardMean': 0.7925281239576505, 'totalEpisodes': 106, 'stepsPerEpisode': 40, 'rewardPerEpisode': 36.40011988136713, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5893163949456122, 'errorList': [], 'lossList': [0.0, -1.4082589572668076, 0.0, 184.9815761947632, 0.0, 0.0, 0.0], 'rewardMean': 0.769949042956313, 'totalEpisodes': 138, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.6741016779754085
'totalSteps': 12800, 'rewardStep': 0.9470403900134096, 'errorList': [211.65555658948944, 204.73069946607538, 174.9155344421818, 218.17934581754406, 198.48191603538214, 219.3763880072232, 227.92686556311463, 177.65973294984198, 204.34091209950287, 180.49340010195078, 192.40869457565276, 218.52645121328817, 202.023626216474, 178.978785148055, 215.7382440878903, 209.48335921374206, 220.98606042758072, 206.2019584916637, 222.7956421782725, 215.01436996821937, 205.35437560261462, 215.35195742802813, 224.57273629383639, 212.24897282222048, 214.91539969111216, 217.82810677397495, 224.6911830760817, 217.20617381436756, 219.41200874865422, 172.49961516106572, 213.0031697797402, 188.31157815268674, 202.40093777303966, 228.15078605590105, 114.65314142754562, 217.13456195712305, 217.7612900477606, 217.18539606334883, 137.04943131051138, 221.82834276514274, 221.2670720720622, 216.91946351700193, 202.83750306766316, 152.99283314577644, 222.42306201755713, 212.2892352623569, 217.58836885444776, 224.73300836734225, 119.52808414102378, 222.57282847285356], 'lossList': [0.0, -1.4130792433023454, 0.0, 100.75124797821044, 0.0, 0.0, 0.0], 'rewardMean': 0.7876581776620226, 'totalEpisodes': 169, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.907808493247167, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.9533613565847833, 'errorList': [193.69272676607375, 179.8080494995, 242.71021548658868, 218.7272642460855, 257.7930492202745, 118.27428923732853, 218.20714290005623, 143.5451681993077, 216.07745402315663, 238.7792531965689, 179.18100351306094, 210.50503415633364, 147.43833575534896, 128.71827956311992, 258.8932505817635, 164.2594111409132, 165.71744882760666, 178.16983640300907, 194.7965464814376, 225.96228331567602, 184.7663053976158, 214.34592854732017, 95.28331520879894, 112.03058304193597, 203.16754049108647, 145.12833143682053, 244.34253107558305, 171.27925475481916, 231.01993199420505, 219.22824369333296, 194.6544477050526, 239.00712830502152, 238.48878729962613, 170.5652249302358, 222.74057324344784, 211.640916974547, 227.63989437213715, 87.2075686472481, 147.59519608546046, 194.04529041116612, 216.88701488733827, 226.79822930767932, 187.9996042424026, 184.20486007277458, 121.4168707598632, 210.52175611900375, 226.63266917233545, 197.77374397800162, 167.80868750351914, 203.96214896022775], 'lossList': [0.0, -1.4272222870588303, 0.0, 63.232585201263426, 0.0, 0.0, 0.0], 'rewardMean': 0.8271295414728655, 'totalEpisodes': 188, 'stepsPerEpisode': 72, 'rewardPerEpisode': 61.361587488955664, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.8996595734642915, 'errorList': [], 'lossList': [0.0, -1.447481126189232, 0.0, 29.451948986053466, 0.0, 0.0, 0.0], 'rewardMean': 0.8312474553868576, 'totalEpisodes': 198, 'stepsPerEpisode': 41, 'rewardPerEpisode': 37.07899160478517
'totalSteps': 16640, 'rewardStep': 0.9641353630159725, 'errorList': [151.7536779230312, 99.39795180548889, 2.614723824457095, 39.208598155387456, 181.8455978035584, 82.53142057300292, 173.35869998733355, 166.8084325059273, 94.44553403521923, 123.96941488182075, 46.66331841471876, 206.15669742211898, 196.26579057263856, 158.04921259684278, 8.954642748966611, 127.09693576738184, 209.07129790228262, 193.05143629627779, 49.08578976888323, 149.75416316746654, 152.11517600538045, 198.29139990896203, 218.7374436886114, 151.34671540123855, 23.17779422941266, 184.09074329365552, 206.487747157016, 178.8697385955583, 190.74338448144448, 159.18624658599092, 205.0232834510771, 189.1354097876327, 164.0468309574498, 9.2199014280932, 147.37294844496165, 193.30394895456766, 131.68861323109527, 89.11579601811513, 208.73722610810628, 187.82037791891773, 146.3933501343914, 205.7009214729534, 53.836107916322604, 215.79614913629902, 157.4045379000471, 206.00586443154248, 197.62891143848253, 125.01060416807546, 202.33646312889312, 146.67731244653044], 'lossList': [0.0, -1.4594273746013642, 0.0, 20.200640568733217, 0.0, 0.0, 0.0], 'rewardMean': 0.8382078712229533, 'totalEpisodes': 208, 'stepsPerEpisode': 73, 'rewardPerEpisode': 66.74759547977922, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.5327012489034348, 'errorList': [], 'lossList': [0.0, -1.4657177567481994, 0.0, 28.804572732448577, 0.0, 0.0, 0.0], 'rewardMean': 0.8272091260474216, 'totalEpisodes': 216, 'stepsPerEpisode': 115, 'rewardPerEpisode': 91.54621877993044
'totalSteps': 19200, 'rewardStep': 0.7401913758257828, 'errorList': [], 'lossList': [0.0, -1.4727146357297898, 0.0, 21.317738745212555, 0.0, 0.0, 0.0], 'rewardMean': 0.8066029139292088, 'totalEpisodes': 224, 'stepsPerEpisode': 41, 'rewardPerEpisode': 29.61010933657199
'totalSteps': 20480, 'rewardStep': 0.6976536234117425, 'errorList': [], 'lossList': [0.0, -1.4530161744356156, 0.0, 12.808753530979157, 0.0, 0.0, 0.0], 'rewardMean': 0.7914261452378545, 'totalEpisodes': 230, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.950405522324804
'totalSteps': 21760, 'rewardStep': 0.7448743312128764, 'errorList': [], 'lossList': [0.0, -1.4389066785573958, 0.0, 17.252720925807953, 0.0, 0.0, 0.0], 'rewardMean': 0.8022236289188026, 'totalEpisodes': 234, 'stepsPerEpisode': 75, 'rewardPerEpisode': 64.40080628564878
'totalSteps': 23040, 'rewardStep': 0.6330469270206824, 'errorList': [], 'lossList': [0.0, -1.4283773189783096, 0.0, 5.6109150773286816, 0.0, 0.0, 0.0], 'rewardMean': 0.7701980584398588, 'totalEpisodes': 235, 'stepsPerEpisode': 925, 'rewardPerEpisode': 737.8894416940325
'totalSteps': 24320, 'rewardStep': 0.5419319433638866, 'errorList': [], 'lossList': [0.0, -1.3925493896007537, 0.0, 4.940034417510033, 0.0, 0.0, 0.0], 'rewardMean': 0.7654596132816863, 'totalEpisodes': 236, 'stepsPerEpisode': 448, 'rewardPerEpisode': 365.94201163233896
'totalSteps': 25600, 'rewardStep': 0.6650892051023494, 'errorList': [], 'lossList': [0.0, -1.34767142534256, 0.0, 4.061187698841095, 0.0, 0.0, 0.0], 'rewardMean': 0.7372644947905801, 'totalEpisodes': 239, 'stepsPerEpisode': 297, 'rewardPerEpisode': 227.7372182525161
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=148.1
