#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 68
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5846711921719314, 'errorList': [], 'lossList': [0.0, -1.421724066734314, 0.0, 30.98340278506279, 0.0, 0.0, 0.0], 'rewardMean': 0.7214416503637355, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.439177292370605
'totalSteps': 3840, 'rewardStep': 0.9713347298877761, 'errorList': [], 'lossList': [0.0, -1.421346955895424, 0.0, 32.09056904911995, 0.0, 0.0, 0.0], 'rewardMean': 0.8047393435384157, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 391.86324735082087
'totalSteps': 5120, 'rewardStep': 0.836866084518641, 'errorList': [], 'lossList': [0.0, -1.4178673738241196, 0.0, 31.220293309092522, 0.0, 0.0, 0.0], 'rewardMean': 0.8127710287834721, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.6268705646223
'totalSteps': 6400, 'rewardStep': 0.7916054620296396, 'errorList': [], 'lossList': [0.0, -1.399567020535469, 0.0, 23.390938937067986, 0.0, 0.0, 0.0], 'rewardMean': 0.8085379154327056, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.9615870559815
'totalSteps': 7680, 'rewardStep': 0.9791091457614167, 'errorList': [79.42245289668922, 77.28684997482868, 81.57043372108102, 80.92703132628277, 83.27421834444223, 75.75091643728211, 80.8606102270988, 79.69073319776011, 81.25882471938606, 78.91777058687042, 74.31905481584721, 72.86495261906487, 79.84727577437057, 80.24226093933409, 80.0987927583319, 82.63507841985064, 77.5584207103272, 80.506791731906, 81.27974683548726, 80.50080758260624, 75.71020406778742, 81.18820585309872, 78.58625053434022, 80.00725324211578, 80.18452612927919, 80.1226441745005, 81.51896662112377, 79.9502057288556, 80.3230698775551, 81.89473622586931, 74.01403153432068, 70.73413360215119, 79.9853120479662, 78.12424086996764, 74.21042028740744, 80.05601947898876, 81.41223559853559, 81.56475707983293, 80.26861138272587, 78.54198159734881, 80.71001412704078, 79.3754070605646, 83.49255133931217, 79.66281319889181, 76.61362159834873, 81.75164664940947, 76.78762211493421, 75.85601178792358, 80.02104415003069, 77.0239273429402], 'lossList': [0.0, -1.383092558979988, 0.0, 18.600734558850526, 0.0, 0.0, 0.0], 'rewardMean': 0.836966453820824, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.921359567801, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.7908348869633395, 'errorList': [], 'lossList': [0.0, -1.3757373654842378, 0.0, 454.1223712158203, 0.0, 0.0, 0.0], 'rewardMean': 0.8303762299840406, 'totalEpisodes': 51, 'stepsPerEpisode': 41, 'rewardPerEpisode': 34.668783919709796
'totalSteps': 10240, 'rewardStep': 0.834508672791501, 'errorList': [], 'lossList': [0.0, -1.374276144504547, 0.0, 298.5806130981445, 0.0, 0.0, 0.0], 'rewardMean': 0.8308927853349731, 'totalEpisodes': 83, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.18260766221769
'totalSteps': 11520, 'rewardStep': 0.60500700643323, 'errorList': [], 'lossList': [0.0, -1.3770167684555055, 0.0, 166.87064937591552, 0.0, 0.0, 0.0], 'rewardMean': 0.8057943654570017, 'totalEpisodes': 119, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.053725868618747
'totalSteps': 12800, 'rewardStep': 0.6970089453494576, 'errorList': [], 'lossList': [0.0, -1.3838418573141098, 0.0, 72.30145959854126, 0.0, 0.0, 0.0], 'rewardMean': 0.7949158234462473, 'totalEpisodes': 137, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.833920466042332
'totalSteps': 14080, 'rewardStep': 0.7699859099923481, 'errorList': [], 'lossList': [0.0, -1.3842878621816634, 0.0, 50.20705171585083, 0.0, 0.0, 0.0], 'rewardMean': 0.7860932035899282, 'totalEpisodes': 156, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.89977282537451
'totalSteps': 15360, 'rewardStep': 0.47859165268271425, 'errorList': [], 'lossList': [0.0, -1.3630522626638413, 0.0, 19.76280255794525, 0.0, 0.0, 0.0], 'rewardMean': 0.7754852496410064, 'totalEpisodes': 163, 'stepsPerEpisode': 145, 'rewardPerEpisode': 98.75172778570484
'totalSteps': 16640, 'rewardStep': 0.7503160762098825, 'errorList': [], 'lossList': [0.0, -1.3365007156133653, 0.0, 10.453926315307617, 0.0, 0.0, 0.0], 'rewardMean': 0.7533833842732169, 'totalEpisodes': 168, 'stepsPerEpisode': 145, 'rewardPerEpisode': 120.36499980384082
'totalSteps': 17920, 'rewardStep': 0.6524617170240549, 'errorList': [], 'lossList': [0.0, -1.32650426030159, 0.0, 9.329986724853516, 0.0, 0.0, 0.0], 'rewardMean': 0.7349429475237585, 'totalEpisodes': 171, 'stepsPerEpisode': 184, 'rewardPerEpisode': 149.37170426116464
'totalSteps': 19200, 'rewardStep': 0.7727325993613047, 'errorList': [], 'lossList': [0.0, -1.326460418701172, 0.0, 8.961640421152115, 0.0, 0.0, 0.0], 'rewardMean': 0.733055661256925, 'totalEpisodes': 173, 'stepsPerEpisode': 702, 'rewardPerEpisode': 555.5407681024742
'totalSteps': 20480, 'rewardStep': 0.8617215480117365, 'errorList': [], 'lossList': [0.0, -1.3125007611513138, 0.0, 5.709566902220249, 0.0, 0.0, 0.0], 'rewardMean': 0.7213169014819569, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1079.1067923759986
'totalSteps': 21760, 'rewardStep': 0.9048470564646472, 'errorList': [], 'lossList': [0.0, -1.2816444557905198, 0.0, 4.749318395406008, 0.0, 0.0, 0.0], 'rewardMean': 0.7327181184320877, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.58188521695
'totalSteps': 23040, 'rewardStep': 0.824775638866194, 'errorList': [], 'lossList': [0.0, -1.2508791387081146, 0.0, 4.0689876430481675, 0.0, 0.0, 0.0], 'rewardMean': 0.731744815039557, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.9753505809015
'totalSteps': 24320, 'rewardStep': 0.8118548500327071, 'errorList': [], 'lossList': [0.0, -1.200292820930481, 0.0, 2.2430029106885194, 0.0, 0.0, 0.0], 'rewardMean': 0.7524295993995046, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.2214284271583
'totalSteps': 25600, 'rewardStep': 0.9532105036157494, 'errorList': [0.14567227988026224, 0.11516391689515157, 0.10468175214294344, 0.12698902953627306, 0.13835715168410886, 0.11166134828149592, 0.16259768065716995, 0.11248505553036156, 0.1342017075668452, 0.12724469777706462, 0.14325915198483127, 0.11307735209107975, 0.11699179681478257, 0.15192555829616677, 0.13766016847496584, 0.1244357006986589, 0.14074692835988717, 0.13554590753100076, 0.13539614990943352, 0.143365717505206, 0.12489737946138407, 0.1448920698679378, 0.11461181205682376, 0.12354783088640327, 0.11998903188971644, 0.10934275135860425, 0.1164469810379491, 0.14011294135023106, 0.1140679277596582, 0.14645475720219647, 0.1427445172587648, 0.11259463263041876, 0.13879901662996624, 0.13529579073969636, 0.13941323406565753, 0.11565388081605131, 0.13781042014751382, 0.13364770740979903, 0.12016548385557985, 0.1264263513312459, 0.12624118847704846, 0.12592517193452105, 0.13051318135217116, 0.1488778268923664, 0.12662077317036508, 0.1289061353745884, 0.14071172511571695, 0.15093203034623714, 0.15291547996130525, 0.1516000260007125], 'lossList': [0.0, -1.1619542413949966, 0.0, 1.6016412582062185, 0.0, 0.0, 0.0], 'rewardMean': 0.7780497552261338, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.4796147144086, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.27
