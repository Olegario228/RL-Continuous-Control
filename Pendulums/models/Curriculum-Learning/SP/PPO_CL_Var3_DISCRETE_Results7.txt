#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 7
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.6024665948879426, 'errorList': [], 'lossList': [0.0, -1.4165354841947555, 0.0, 23.93550584554672, 0.0, 0.0, 0.0], 'rewardMean': 0.6981182980340469, 'totalEpisodes': 20, 'stepsPerEpisode': 134, 'rewardPerEpisode': 81.29176411951762
'totalSteps': 3840, 'rewardStep': 0.7476596497586331, 'errorList': [], 'lossList': [0.0, -1.4073269003629685, 0.0, 25.12256549358368, 0.0, 0.0, 0.0], 'rewardMean': 0.7146320819422423, 'totalEpisodes': 23, 'stepsPerEpisode': 491, 'rewardPerEpisode': 360.6189836227023
'totalSteps': 5120, 'rewardStep': 0.8392984180292674, 'errorList': [], 'lossList': [0.0, -1.3948573023080826, 0.0, 19.90495609998703, 0.0, 0.0, 0.0], 'rewardMean': 0.7457986659639986, 'totalEpisodes': 27, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.942805791393013
'totalSteps': 6400, 'rewardStep': 0.46701544871828976, 'errorList': [], 'lossList': [0.0, -1.401670258641243, 0.0, 196.32336505889893, 0.0, 0.0, 0.0], 'rewardMean': 0.6900420225148568, 'totalEpisodes': 83, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.39080807830725
'totalSteps': 7680, 'rewardStep': 0.4720429588272457, 'errorList': [], 'lossList': [0.0, -1.3928267341852187, 0.0, 88.06350309371948, 0.0, 0.0, 0.0], 'rewardMean': 0.6537088452335883, 'totalEpisodes': 130, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.464832599819315
'totalSteps': 8960, 'rewardStep': 0.7807600578670777, 'errorList': [], 'lossList': [0.0, -1.371089848279953, 0.0, 70.0023131942749, 0.0, 0.0, 0.0], 'rewardMean': 0.6718590184669438, 'totalEpisodes': 165, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7807600578670777
'totalSteps': 10240, 'rewardStep': 0.23901463352737579, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5756713773692621, 'totalEpisodes': 179, 'stepsPerEpisode': 94, 'rewardPerEpisode': 69.8330148159489
'totalSteps': 11520, 'rewardStep': 0.8629894997886186, 'errorList': [], 'lossList': [0.0, -1.3609696513414382, 0.0, 42.51923481941223, 0.0, 0.0, 0.0], 'rewardMean': 0.6044031896111977, 'totalEpisodes': 198, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.288631767173905
'totalSteps': 12800, 'rewardStep': 0.818404621112072, 'errorList': [], 'lossList': [0.0, -1.3573572850227356, 0.0, 30.623892464637755, 0.0, 0.0, 0.0], 'rewardMean': 0.6068666516043899, 'totalEpisodes': 207, 'stepsPerEpisode': 151, 'rewardPerEpisode': 116.60073453051609
'totalSteps': 14080, 'rewardStep': 0.36463478327776383, 'errorList': [], 'lossList': [0.0, -1.3380263805389405, 0.0, 9.303929626941681, 0.0, 0.0, 0.0], 'rewardMean': 0.5830834704433719, 'totalEpisodes': 215, 'stepsPerEpisode': 153, 'rewardPerEpisode': 106.54073088100053
'totalSteps': 15360, 'rewardStep': 0.8490991607063332, 'errorList': [], 'lossList': [0.0, -1.327148575782776, 0.0, 12.41676605939865, 0.0, 0.0, 0.0], 'rewardMean': 0.5932274215381421, 'totalEpisodes': 222, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.20843153851527
'totalSteps': 16640, 'rewardStep': 0.8625288910072519, 'errorList': [], 'lossList': [0.0, -1.3344215536117554, 0.0, 6.764086793065071, 0.0, 0.0, 0.0], 'rewardMean': 0.5955504688359404, 'totalEpisodes': 226, 'stepsPerEpisode': 195, 'rewardPerEpisode': 161.04440900766326
'totalSteps': 17920, 'rewardStep': 0.6454810414708118, 'errorList': [], 'lossList': [0.0, -1.3407203257083893, 0.0, 4.048546917438507, 0.0, 0.0, 0.0], 'rewardMean': 0.6133970281111927, 'totalEpisodes': 231, 'stepsPerEpisode': 71, 'rewardPerEpisode': 53.11170870385688
'totalSteps': 19200, 'rewardStep': 0.9265568422570322, 'errorList': [], 'lossList': [0.0, -1.357661954164505, 0.0, 4.849703961610794, 0.0, 0.0, 0.0], 'rewardMean': 0.6588484164541712, 'totalEpisodes': 235, 'stepsPerEpisode': 139, 'rewardPerEpisode': 124.85114693543885
'totalSteps': 20480, 'rewardStep': 0.7933131333528444, 'errorList': [], 'lossList': [0.0, -1.3531877666711807, 0.0, 2.6941898158192634, 0.0, 0.0, 0.0], 'rewardMean': 0.660103724002748, 'totalEpisodes': 238, 'stepsPerEpisode': 134, 'rewardPerEpisode': 110.79766769361864
'totalSteps': 21760, 'rewardStep': 0.8138744335689094, 'errorList': [], 'lossList': [0.0, -1.343608209490776, 0.0, 3.542659584283829, 0.0, 0.0, 0.0], 'rewardMean': 0.7175897040069014, 'totalEpisodes': 240, 'stepsPerEpisode': 299, 'rewardPerEpisode': 266.31279478013573
'totalSteps': 23040, 'rewardStep': 0.7987660875381248, 'errorList': [], 'lossList': [0.0, -1.3394558781385422, 0.0, 2.080283362865448, 0.0, 0.0, 0.0], 'rewardMean': 0.7735648494079761, 'totalEpisodes': 241, 'stepsPerEpisode': 790, 'rewardPerEpisode': 663.621110950691
'totalSteps': 24320, 'rewardStep': 0.9368885520449648, 'errorList': [0.08557347960467963, 0.05319512400675548, 0.06245704221442382, 0.06464299462687588, 0.12455646614328873, 0.14301805680945406, 0.05334629364222802, 0.05287830843197133, 0.05314227048083437, 0.053209075611217876, 0.06296079431598015, 0.0673795661899999, 0.053050042855345154, 0.05297082505352382, 0.053246125698364836, 0.0820146781384611, 0.14107696273000808, 0.05815426126529039, 0.053065384989743905, 0.05320142890767672, 0.10488875853441904, 0.053106157115655445, 0.05325551126759913, 0.0604761653930198, 0.05311670117499264, 0.052796636090095286, 0.05342271656239506, 0.0529497658792654, 0.05288842133692171, 0.08768140183373974, 0.10356195429474496, 0.11731755881889881, 0.09481912984593427, 0.06845911117299405, 0.08348962923862388, 0.05320236126762754, 0.05491353674799071, 0.07322456016894467, 0.053317093401838045, 0.05280907065807684, 0.09422083080652716, 0.1096946296493049, 0.05343166225986992, 0.05345691477210473, 0.05296616412261274, 0.052607418849508064, 0.11641516787709086, 0.07814715251691741, 0.05330765735679419, 0.05308666040266156], 'lossList': [0.0, -1.3223748594522475, 0.0, 2.013720688521862, 0.0, 0.0, 0.0], 'rewardMean': 0.7809547546336109, 'totalEpisodes': 241, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.0317592920276, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=70.1
