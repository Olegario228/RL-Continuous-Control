#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 70
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9120293869764686, 'errorList': [], 'lossList': [0.0, -1.4368718886375427, 0.0, 31.965963249802588, 0.0, 0.0, 0.0], 'rewardMean': 0.9134250087786484, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 383.4933527492489
'totalSteps': 3840, 'rewardStep': 0.7201087920380944, 'errorList': [], 'lossList': [0.0, -1.4210810518264771, 0.0, 31.269216170310973, 0.0, 0.0, 0.0], 'rewardMean': 0.8489862698651304, 'totalEpisodes': 12, 'stepsPerEpisode': 268, 'rewardPerEpisode': 211.44214883633225
'totalSteps': 5120, 'rewardStep': 0.7380898310945112, 'errorList': [], 'lossList': [0.0, -1.4190899974107742, 0.0, 27.047922303676604, 0.0, 0.0, 0.0], 'rewardMean': 0.8212621601724757, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.3847194564404
'totalSteps': 6400, 'rewardStep': 0.9046049391236792, 'errorList': [], 'lossList': [0.0, -1.4086470800638198, 0.0, 25.48635731458664, 0.0, 0.0, 0.0], 'rewardMean': 0.8379307159627164, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.3090301323646
'totalSteps': 7680, 'rewardStep': 0.8050494271816422, 'errorList': [], 'lossList': [0.0, -1.3986014062166214, 0.0, 18.3441914100945, 0.0, 0.0, 0.0], 'rewardMean': 0.8324505011658707, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.642023787852
'totalSteps': 8960, 'rewardStep': 0.7458233258080502, 'errorList': [], 'lossList': [0.0, -1.3637795567512512, 0.0, 687.1642655944825, 0.0, 0.0, 0.0], 'rewardMean': 0.8200751904004678, 'totalEpisodes': 74, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4984623069731873
'totalSteps': 10240, 'rewardStep': 0.5266656873001688, 'errorList': [], 'lossList': [0.0, -1.3620895367860795, 0.0, 462.52815490722656, 0.0, 0.0, 0.0], 'rewardMean': 0.7833990025129304, 'totalEpisodes': 133, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.68899903823614
'totalSteps': 11520, 'rewardStep': 0.4641852584769138, 'errorList': [], 'lossList': [0.0, -1.3626741367578505, 0.0, 245.33242385864258, 0.0, 0.0, 0.0], 'rewardMean': 0.7479308087311508, 'totalEpisodes': 188, 'stepsPerEpisode': 69, 'rewardPerEpisode': 58.118184020404136
'totalSteps': 12800, 'rewardStep': 0.8693706494283843, 'errorList': [], 'lossList': [0.0, -1.36663223028183, 0.0, 94.88592430114745, 0.0, 0.0, 0.0], 'rewardMean': 0.7600747928008741, 'totalEpisodes': 235, 'stepsPerEpisode': 64, 'rewardPerEpisode': 53.8009401383532
'totalSteps': 14080, 'rewardStep': 0.9093956133344279, 'errorList': [], 'lossList': [0.0, -1.3657035505771637, 0.0, 52.94496562957764, 0.0, 0.0, 0.0], 'rewardMean': 0.7595322910762341, 'totalEpisodes': 268, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.804909609281192
'totalSteps': 15360, 'rewardStep': 0.7768830586096773, 'errorList': [], 'lossList': [0.0, -1.366274998188019, 0.0, 35.42783467292786, 0.0, 0.0, 0.0], 'rewardMean': 0.7460176582395549, 'totalEpisodes': 285, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.5053875773362755
'totalSteps': 16640, 'rewardStep': 0.7166521045127048, 'errorList': [], 'lossList': [0.0, -1.3505567210912703, 0.0, 18.745961503982542, 0.0, 0.0, 0.0], 'rewardMean': 0.7456719894870161, 'totalEpisodes': 294, 'stepsPerEpisode': 47, 'rewardPerEpisode': 29.538523498994962
'totalSteps': 17920, 'rewardStep': 0.16862452568602965, 'errorList': [], 'lossList': [0.0, -1.3287610626220703, 0.0, 18.96460004091263, 0.0, 0.0, 0.0], 'rewardMean': 0.6887254589461678, 'totalEpisodes': 300, 'stepsPerEpisode': 191, 'rewardPerEpisode': 122.59055175005821
'totalSteps': 19200, 'rewardStep': 0.7591561197856109, 'errorList': [], 'lossList': [0.0, -1.3168134427070617, 0.0, 37.04737511634826, 0.0, 0.0, 0.0], 'rewardMean': 0.674180577012361, 'totalEpisodes': 308, 'stepsPerEpisode': 137, 'rewardPerEpisode': 115.79490747194198
'totalSteps': 20480, 'rewardStep': 0.9323002937421341, 'errorList': [0.41127485277282066, 0.8410670534994397, 11.457833034318224, 3.099122373146048, 0.4811631846776145, 1.4690727217306656, 1.5680927824299429, 0.3909554033953752, 0.48558371639912423, 1.2750173610184385, 0.9308770792765548, 0.8663083859656058, 0.792108195726598, 0.7720915054591008, 0.7724607105529543, 0.501638266106202, 0.8372731535678221, 0.27249029972426386, 0.27771725858677027, 1.6246155750211422, 1.0789863199529097, 1.5146571536323792, 1.0577850244498446, 0.4111970329770627, 0.505144260525566, 0.5010909322419291, 0.44651781018849346, 1.4529979807463194, 1.7577502798661953, 0.48554714919269076, 0.6632480651056957, 0.29160476006722547, 0.437065770234962, 0.6496478850450192, 1.1355654791639487, 1.11070591729149, 0.791757320673292, 0.21350076242486818, 0.8540170879602232, 6.298800866122063, 0.5699256758374429, 1.342624336384542, 1.2112118414076654, 4.5013611048807665, 0.34498837253374764, 0.9994564265838757, 0.37944175030745225, 0.8396125207087293, 0.8872335471250186, 0.7898210095475611], 'lossList': [0.0, -1.3229808932542801, 0.0, 8.120384752750397, 0.0, 0.0, 0.0], 'rewardMean': 0.6869056636684102, 'totalEpisodes': 313, 'stepsPerEpisode': 28, 'rewardPerEpisode': 23.06789928991874, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.9567254505115261, 'errorList': [0.21739014313022537, 0.22613988201548205, 0.2231007360733476, 0.2206052991723947, 0.22972107854234575, 0.2381123004232516, 0.2154413359501021, 0.21789495986864107, 0.2291499202748221, 0.24133411186760134, 0.22753358427936396, 0.22578896776670543, 0.2505101337811803, 0.2114562921784008, 0.23460660363019534, 0.2324036654108502, 0.24405048818766215, 0.2233104949346951, 0.23110730044310535, 0.2182101369854768, 0.20684066746481145, 0.2142178395070772, 0.233837775962777, 0.21775087743569513, 0.2221292675290218, 0.26269712725955197, 0.21170158134537082, 0.21727187763072373, 0.21821867993988578, 0.2335234281184734, 0.197572673728678, 0.23672645989503857, 0.21992463494360842, 0.20588288587453954, 0.22111180238523315, 0.2396668706414274, 0.2008633459423279, 0.23094241762324283, 0.23346381111192002, 0.2351142112935302, 0.22687785868479815, 0.22344395123059932, 0.242521436697605, 0.23906190634056354, 0.23656063045661824, 0.21788305978171163, 0.2210575444239202, 0.2317208037609092, 0.24445929812247322, 0.2356276334139545], 'lossList': [0.0, -1.3215114480257035, 0.0, 23.39524827718735, 0.0, 0.0, 0.0], 'rewardMean': 0.7079958761387578, 'totalEpisodes': 316, 'stepsPerEpisode': 61, 'rewardPerEpisode': 50.978061850536754, 'successfulTests': 1
'totalSteps': 23040, 'rewardStep': 0.5847007744407327, 'errorList': [], 'lossList': [0.0, -1.2926250290870667, 0.0, 4.999675452411175, 0.0, 0.0, 0.0], 'rewardMean': 0.7137993848528141, 'totalEpisodes': 316, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1045.8631988913455
'totalSteps': 24320, 'rewardStep': 0.6779098259458534, 'errorList': [], 'lossList': [0.0, -1.251057004928589, 0.0, 3.7113083417713644, 0.0, 0.0, 0.0], 'rewardMean': 0.7351718415997082, 'totalEpisodes': 316, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.1039951419418
'totalSteps': 25600, 'rewardStep': 0.9169425381673921, 'errorList': [], 'lossList': [0.0, -1.214707419872284, 0.0, 4.416102211996913, 0.0, 0.0, 0.0], 'rewardMean': 0.7399290304736089, 'totalEpisodes': 316, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.7440225997766
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=21760, timeSpent=102.05
