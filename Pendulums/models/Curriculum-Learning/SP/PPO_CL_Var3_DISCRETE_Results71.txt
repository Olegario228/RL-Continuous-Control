#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 71
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8507820611170008, 'errorList': [], 'lossList': [0.0, -1.4478689223527907, 0.0, 39.36603865623474, 0.0, 0.0, 0.0], 'rewardMean': 0.72197994034858, 'totalEpisodes': 12, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.246092380443315
'totalSteps': 3840, 'rewardStep': 0.912045262930967, 'errorList': [], 'lossList': [0.0, -1.4647571444511414, 0.0, 34.22333289682865, 0.0, 0.0, 0.0], 'rewardMean': 0.7853350478760422, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.6693443357922
'totalSteps': 5120, 'rewardStep': 0.6920257340736807, 'errorList': [], 'lossList': [0.0, -1.4508162778615952, 0.0, 29.470210913419724, 0.0, 0.0, 0.0], 'rewardMean': 0.7620077194254519, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.1034262720764
'totalSteps': 6400, 'rewardStep': 0.9350593392633095, 'errorList': [], 'lossList': [0.0, -1.4478138899803161, 0.0, 24.043236311674118, 0.0, 0.0, 0.0], 'rewardMean': 0.7966180433930233, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.0425432279892
'totalSteps': 7680, 'rewardStep': 0.8365402088078523, 'errorList': [], 'lossList': [0.0, -1.4295602226257325, 0.0, 20.90125725865364, 0.0, 0.0, 0.0], 'rewardMean': 0.8032717376288282, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.5239888302845
'totalSteps': 8960, 'rewardStep': 0.5120466213691703, 'errorList': [], 'lossList': [0.0, -1.4195945531129837, 0.0, 549.3584272766113, 0.0, 0.0, 0.0], 'rewardMean': 0.7616681495917341, 'totalEpisodes': 54, 'stepsPerEpisode': 41, 'rewardPerEpisode': 34.07422044762506
'totalSteps': 10240, 'rewardStep': 0.8843051460365879, 'errorList': [], 'lossList': [0.0, -1.419496106505394, 0.0, 416.1326036834717, 0.0, 0.0, 0.0], 'rewardMean': 0.7769977741473411, 'totalEpisodes': 97, 'stepsPerEpisode': 66, 'rewardPerEpisode': 56.56515427526115
'totalSteps': 11520, 'rewardStep': 0.7256461790373228, 'errorList': [], 'lossList': [0.0, -1.4195385593175889, 0.0, 231.6821511077881, 0.0, 0.0, 0.0], 'rewardMean': 0.7712920413573391, 'totalEpisodes': 150, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.751185428354548
'totalSteps': 12800, 'rewardStep': 0.5282026140998979, 'errorList': [], 'lossList': [0.0, -1.427038694024086, 0.0, 61.320212841033936, 0.0, 0.0, 0.0], 'rewardMean': 0.746983098631595, 'totalEpisodes': 183, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.0561383541396916
'totalSteps': 14080, 'rewardStep': 0.9906415345770604, 'errorList': [188.0416956043951, 153.326326510464, 202.55532161859338, 194.07593977603582, 221.51075971605925, 139.96975963752573, 182.71279904539682, 110.84668590146903, 31.896691414906844, 91.38370236852779, 187.5830026268066, 179.68182985477586, 211.69188635671244, 195.67518733530696, 215.516552858969, 194.35201473112724, 212.97964235123973, 173.6640233437784, 198.74845983185827, 154.1772173507224, 187.72513684341192, 211.97086065083835, 175.2277957710088, 194.1045990600022, 181.58306787756965, 121.12687164616888, 175.15575008001912, 122.7844820383235, 166.7583763633332, 191.77387380688728, 99.64974214287562, 198.95256592654903, 206.58874186548704, 163.4231812869942, 80.34003984229172, 208.26694578127288, 187.62830912469246, 167.88326149890958, 198.21990078667613, 156.39733071021814, 147.1208061114866, 200.0483137109029, 203.94085436048076, 196.96237856546625, 150.13570996994443, 219.2286433560774, 145.5690380689574, 206.45136796551265, 146.32075867666214, 92.22726067980592], 'lossList': [0.0, -1.4452555161714553, 0.0, 32.26278380393982, 0.0, 0.0, 0.0], 'rewardMean': 0.7867294701312849, 'totalEpisodes': 203, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.774138479092619, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.9372463928450071, 'errorList': [201.13741238307148, 130.53250477163093, 134.56475045021162, 197.86067563681047, 185.0193151977522, 139.60317506769186, 211.09068371611585, 101.37711505280889, 122.01639012445652, 152.45490721010316, 93.71471462146768, 138.88051520425367, 117.51565038854405, 65.22995570707396, 178.3308706175451, 158.16897937317384, 64.82872934552906, 173.7643748852275, 80.57025488634652, 173.77817649140607, 149.85304074011634, 160.3309904816611, 184.96892305751075, 132.85039165591422, 192.62966862233924, 189.71977034604285, 184.77688372238129, 196.58851043703694, 184.82016821988302, 191.75093265475525, 197.13704699603477, 106.08291231014393, 85.96832809032641, 167.18583011087927, 178.70955718345178, 174.69035241229045, 76.88308148294027, 158.2667799281474, 210.02985257794074, 156.50956340740183, 137.34830139807136, 192.0163160923106, 167.2671903164976, 218.04069998106078, 31.111283664985063, 195.10322464185472, 171.8100628773879, 114.74967878827314, 155.72443649223678, 170.87564079316198], 'lossList': [0.0, -1.4657706665992736, 0.0, 14.871847486495971, 0.0, 0.0, 0.0], 'rewardMean': 0.7953759033040856, 'totalEpisodes': 213, 'stepsPerEpisode': 52, 'rewardPerEpisode': 46.42833529527866, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.8262643708363856, 'errorList': [], 'lossList': [0.0, -1.4752406364679336, 0.0, 18.884872210025787, 0.0, 0.0, 0.0], 'rewardMean': 0.7867978140946275, 'totalEpisodes': 223, 'stepsPerEpisode': 69, 'rewardPerEpisode': 58.2992627601562
'totalSteps': 17920, 'rewardStep': 0.8632846676662346, 'errorList': [], 'lossList': [0.0, -1.4680092811584473, 0.0, 9.776216044425965, 0.0, 0.0, 0.0], 'rewardMean': 0.8039237074538829, 'totalEpisodes': 230, 'stepsPerEpisode': 62, 'rewardPerEpisode': 49.389032814213046
'totalSteps': 19200, 'rewardStep': 0.6806689564930093, 'errorList': [], 'lossList': [0.0, -1.4592957884073257, 0.0, 7.60633984208107, 0.0, 0.0, 0.0], 'rewardMean': 0.7784846691768529, 'totalEpisodes': 235, 'stepsPerEpisode': 279, 'rewardPerEpisode': 234.4370119739828
'totalSteps': 20480, 'rewardStep': 0.3906817570459414, 'errorList': [], 'lossList': [0.0, -1.4523236513137818, 0.0, 7.698879327774048, 0.0, 0.0, 0.0], 'rewardMean': 0.7338988240006616, 'totalEpisodes': 239, 'stepsPerEpisode': 329, 'rewardPerEpisode': 278.6740780582019
'totalSteps': 21760, 'rewardStep': 0.6574283715653539, 'errorList': [], 'lossList': [0.0, -1.4219541442394257, 0.0, 6.558040167093277, 0.0, 0.0, 0.0], 'rewardMean': 0.74843699902028, 'totalEpisodes': 241, 'stepsPerEpisode': 575, 'rewardPerEpisode': 499.5423964000934
'totalSteps': 23040, 'rewardStep': 0.33912447087087577, 'errorList': [], 'lossList': [0.0, -1.4085168892145157, 0.0, 6.404501438736916, 0.0, 0.0, 0.0], 'rewardMean': 0.6939189315037088, 'totalEpisodes': 242, 'stepsPerEpisode': 1120, 'rewardPerEpisode': 918.2201909477384
'totalSteps': 24320, 'rewardStep': 0.8943650485458822, 'errorList': [], 'lossList': [0.0, -1.4001680368185043, 0.0, 3.4462400871515273, 0.0, 0.0, 0.0], 'rewardMean': 0.7107908184545649, 'totalEpisodes': 244, 'stepsPerEpisode': 262, 'rewardPerEpisode': 220.808417131304
'totalSteps': 25600, 'rewardStep': 0.9142317960379146, 'errorList': [], 'lossList': [0.0, -1.338304122686386, 0.0, 1.6018656945228578, 0.0, 0.0, 0.0], 'rewardMean': 0.7493937366483665, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.363904501315
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=103.54
