#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 73
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.574368653695752, 'errorList': [], 'lossList': [0.0, -1.4227782970666885, 0.0, 32.65724693536758, 0.0, 0.0, 0.0], 'rewardMean': 0.7366495347612462, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.26894055682111
'totalSteps': 3840, 'rewardStep': 0.9642710036560319, 'errorList': [], 'lossList': [0.0, -1.422820115685463, 0.0, 36.074976375103, 0.0, 0.0, 0.0], 'rewardMean': 0.8125233577261747, 'totalEpisodes': 18, 'stepsPerEpisode': 486, 'rewardPerEpisode': 404.51896440838476
'totalSteps': 5120, 'rewardStep': 0.8515721396544952, 'errorList': [], 'lossList': [0.0, -1.4184762996435165, 0.0, 33.07030442774296, 0.0, 0.0, 0.0], 'rewardMean': 0.8222855532082548, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.9682976252727
'totalSteps': 6400, 'rewardStep': 0.8336663349294923, 'errorList': [], 'lossList': [0.0, -1.402276433110237, 0.0, 25.176099026203154, 0.0, 0.0, 0.0], 'rewardMean': 0.8245617095525024, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.9857121463865
'totalSteps': 7680, 'rewardStep': 0.9699351155057788, 'errorList': [83.98627105500164, 81.9631663847226, 86.74329279291466, 80.2359487792623, 82.33199389307468, 80.23356742703663, 80.15706689811635, 84.3105724768411, 86.3660247732068, 83.44487868813168, 78.10875170670649, 76.42212578292762, 79.13296849625985, 84.99717445849424, 84.85368376139563, 81.79130579327771, 82.23501085807473, 79.80037467342503, 80.53801769826117, 79.83680884861002, 80.11171850604296, 80.50711909468272, 78.0155053051877, 79.30506599430618, 84.92995566317873, 84.85938601189874, 80.77346012596733, 84.57613630637108, 79.62224834987092, 81.15890359069353, 77.73561906038924, 71.68784812356392, 84.65005635987296, 82.72533829678513, 77.94627175182188, 79.37498706214012, 80.64644462635185, 80.82966059387681, 85.00860423928636, 83.09150655771187, 85.59158342460887, 83.93700704247503, 82.50213655977387, 84.25848717637282, 76.28986671569548, 80.98841523193528, 76.48696061284569, 80.32593127154453, 79.28992928644674, 81.62992135728376], 'lossList': [0.0, -1.392553585767746, 0.0, 19.627057040184738, 0.0, 0.0, 0.0], 'rewardMean': 0.848790610544715, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.0557908709573, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.8844561366285744, 'errorList': [], 'lossList': [0.0, -1.3830500411987305, 0.0, 615.5417796325684, 0.0, 0.0, 0.0], 'rewardMean': 0.8538856856995521, 'totalEpisodes': 65, 'stepsPerEpisode': 41, 'rewardPerEpisode': 36.956218685398554
'totalSteps': 10240, 'rewardStep': 0.6452756124794645, 'errorList': [], 'lossList': [0.0, -1.3812171643972397, 0.0, 401.0874533843994, 0.0, 0.0, 0.0], 'rewardMean': 0.8278094265470413, 'totalEpisodes': 108, 'stepsPerEpisode': 66, 'rewardPerEpisode': 57.02834536830084
'totalSteps': 11520, 'rewardStep': 0.4943185161177505, 'errorList': [], 'lossList': [0.0, -1.3803476673364639, 0.0, 189.07965019226074, 0.0, 0.0, 0.0], 'rewardMean': 0.7907548809437867, 'totalEpisodes': 143, 'stepsPerEpisode': 14, 'rewardPerEpisode': 7.992727033315591
'totalSteps': 12800, 'rewardStep': 0.7670106250588387, 'errorList': [], 'lossList': [0.0, -1.3897789937257767, 0.0, 134.93394374847412, 0.0, 0.0, 0.0], 'rewardMean': 0.7883804553552919, 'totalEpisodes': 176, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.658132832458577
'totalSteps': 14080, 'rewardStep': 0.8938716380079992, 'errorList': [], 'lossList': [0.0, -1.4055669325590134, 0.0, 58.09844415664673, 0.0, 0.0, 0.0], 'rewardMean': 0.7878745775734177, 'totalEpisodes': 194, 'stepsPerEpisode': 34, 'rewardPerEpisode': 24.312261913046463
'totalSteps': 15360, 'rewardStep': 0.5943383895083931, 'errorList': [], 'lossList': [0.0, -1.3917215269804002, 0.0, 30.761076340675356, 0.0, 0.0, 0.0], 'rewardMean': 0.7898715511546819, 'totalEpisodes': 204, 'stepsPerEpisode': 21, 'rewardPerEpisode': 12.228345402722045
'totalSteps': 16640, 'rewardStep': 0.9252326815912503, 'errorList': [], 'lossList': [0.0, -1.3745032787322997, 0.0, 17.800481684207917, 0.0, 0.0, 0.0], 'rewardMean': 0.7859677189482037, 'totalEpisodes': 213, 'stepsPerEpisode': 74, 'rewardPerEpisode': 58.73201440273482
'totalSteps': 17920, 'rewardStep': 0.8258898923526834, 'errorList': [], 'lossList': [0.0, -1.3744047033786773, 0.0, 9.253451915979385, 0.0, 0.0, 0.0], 'rewardMean': 0.7833994942180225, 'totalEpisodes': 219, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.450347297742
'totalSteps': 19200, 'rewardStep': 0.7457235057333966, 'errorList': [], 'lossList': [0.0, -1.3619496935606004, 0.0, 10.529207079410552, 0.0, 0.0, 0.0], 'rewardMean': 0.774605211298413, 'totalEpisodes': 223, 'stepsPerEpisode': 198, 'rewardPerEpisode': 150.08064562701344
'totalSteps': 20480, 'rewardStep': 0.8410996385581354, 'errorList': [], 'lossList': [0.0, -1.3561497443914414, 0.0, 5.383684986829758, 0.0, 0.0, 0.0], 'rewardMean': 0.7617216636036486, 'totalEpisodes': 226, 'stepsPerEpisode': 127, 'rewardPerEpisode': 97.71041306962135
'totalSteps': 21760, 'rewardStep': 0.6046474829429478, 'errorList': [], 'lossList': [0.0, -1.338002819418907, 0.0, 4.674993092417717, 0.0, 0.0, 0.0], 'rewardMean': 0.7337407982350859, 'totalEpisodes': 228, 'stepsPerEpisode': 703, 'rewardPerEpisode': 516.6901943933195
'totalSteps': 23040, 'rewardStep': 0.7002836241633406, 'errorList': [], 'lossList': [0.0, -1.3022347688674927, 0.0, 6.112126562595368, 0.0, 0.0, 0.0], 'rewardMean': 0.7392415994034736, 'totalEpisodes': 229, 'stepsPerEpisode': 1183, 'rewardPerEpisode': 857.5100342216572
'totalSteps': 24320, 'rewardStep': 0.90350604030775, 'errorList': [], 'lossList': [0.0, -1.2972183007001876, 0.0, 8.871067790985107, 0.0, 0.0, 0.0], 'rewardMean': 0.7801603518224736, 'totalEpisodes': 233, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.736389527444902
'totalSteps': 25600, 'rewardStep': 0.4500371209130166, 'errorList': [], 'lossList': [0.0, -1.2893300354480743, 0.0, 5.686988633871079, 0.0, 0.0, 0.0], 'rewardMean': 0.7484630014078913, 'totalEpisodes': 233, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 780.3191945400421
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.06
