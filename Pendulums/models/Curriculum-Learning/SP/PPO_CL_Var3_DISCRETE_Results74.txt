#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 74
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8201616791084557, 'errorList': [], 'lossList': [0.0, -1.4210871738195419, 0.0, 28.444636491537093, 0.0, 0.0, 0.0], 'rewardMean': 0.7847262717038683, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9817592529649
'totalSteps': 3840, 'rewardStep': 0.7653923850432686, 'errorList': [], 'lossList': [0.0, -1.434804788827896, 0.0, 35.22184423208237, 0.0, 0.0, 0.0], 'rewardMean': 0.7782816428170017, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.43908212466643
'totalSteps': 5120, 'rewardStep': 0.7040208628153881, 'errorList': [], 'lossList': [0.0, -1.4309330904483795, 0.0, 26.119540514349936, 0.0, 0.0, 0.0], 'rewardMean': 0.7597164478165983, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.8211163323339
'totalSteps': 6400, 'rewardStep': 0.9060037613770545, 'errorList': [], 'lossList': [0.0, -1.4235157144069672, 0.0, 19.444068129062654, 0.0, 0.0, 0.0], 'rewardMean': 0.7889739105286895, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.771177195542
'totalSteps': 7680, 'rewardStep': 0.8555522224957413, 'errorList': [], 'lossList': [0.0, -1.431319814324379, 0.0, 14.310615525841714, 0.0, 0.0, 0.0], 'rewardMean': 0.8000702958565314, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.3263114753922
'totalSteps': 8960, 'rewardStep': 0.6797425021635117, 'errorList': [], 'lossList': [0.0, -1.4264376783370971, 0.0, 651.6079504394531, 0.0, 0.0, 0.0], 'rewardMean': 0.7828806110432429, 'totalEpisodes': 77, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.942079651439899
'totalSteps': 10240, 'rewardStep': 0.5469376184502138, 'errorList': [], 'lossList': [0.0, -1.4245343208312988, 0.0, 385.59631843566893, 0.0, 0.0, 0.0], 'rewardMean': 0.7533877369691143, 'totalEpisodes': 134, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.526054598718703
'totalSteps': 11520, 'rewardStep': 0.6940511831139432, 'errorList': [], 'lossList': [0.0, -1.419867588877678, 0.0, 135.6417965698242, 0.0, 0.0, 0.0], 'rewardMean': 0.746794786540762, 'totalEpisodes': 195, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.235220794907797
'totalSteps': 12800, 'rewardStep': 0.7861046432612713, 'errorList': [], 'lossList': [0.0, -1.4119426995515822, 0.0, 77.13938163757324, 0.0, 0.0, 0.0], 'rewardMean': 0.7507257722128129, 'totalEpisodes': 238, 'stepsPerEpisode': 58, 'rewardPerEpisode': 51.484155143651016
'totalSteps': 14080, 'rewardStep': 0.725886594873521, 'errorList': [], 'lossList': [0.0, -1.4086215102672577, 0.0, 50.39610752105713, 0.0, 0.0, 0.0], 'rewardMean': 0.7483853452702369, 'totalEpisodes': 267, 'stepsPerEpisode': 106, 'rewardPerEpisode': 87.30212050643385
'totalSteps': 15360, 'rewardStep': 0.8831283978004061, 'errorList': [], 'lossList': [0.0, -1.4016877239942551, 0.0, 33.23084865093231, 0.0, 0.0, 0.0], 'rewardMean': 0.754682017139432, 'totalEpisodes': 282, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.93698505046971
'totalSteps': 16640, 'rewardStep': 0.4027855878752746, 'errorList': [], 'lossList': [0.0, -1.3825382822752, 0.0, 36.04007258415222, 0.0, 0.0, 0.0], 'rewardMean': 0.7184213374226326, 'totalEpisodes': 296, 'stepsPerEpisode': 158, 'rewardPerEpisode': 116.97361105267694
'totalSteps': 17920, 'rewardStep': 0.8690458835272453, 'errorList': [], 'lossList': [0.0, -1.3605977857112885, 0.0, 14.96633995294571, 0.0, 0.0, 0.0], 'rewardMean': 0.7349238394938182, 'totalEpisodes': 305, 'stepsPerEpisode': 84, 'rewardPerEpisode': 74.63049741832647
'totalSteps': 19200, 'rewardStep': 0.8366979023082901, 'errorList': [], 'lossList': [0.0, -1.3448439508676528, 0.0, 14.140368721485139, 0.0, 0.0, 0.0], 'rewardMean': 0.7279932535869419, 'totalEpisodes': 312, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.956329889995583
'totalSteps': 20480, 'rewardStep': 0.7945680930995753, 'errorList': [], 'lossList': [0.0, -1.3287322843074798, 0.0, 10.7608769094944, 0.0, 0.0, 0.0], 'rewardMean': 0.7218948406473252, 'totalEpisodes': 315, 'stepsPerEpisode': 221, 'rewardPerEpisode': 186.50769616987841
'totalSteps': 21760, 'rewardStep': 0.7191321874274212, 'errorList': [], 'lossList': [0.0, -1.3050975012779236, 0.0, 10.258680926561356, 0.0, 0.0, 0.0], 'rewardMean': 0.7258338091737161, 'totalEpisodes': 317, 'stepsPerEpisode': 807, 'rewardPerEpisode': 699.1345824813452
'totalSteps': 23040, 'rewardStep': 0.4930710507989706, 'errorList': [], 'lossList': [0.0, -1.27452418923378, 0.0, 6.610934371948242, 0.0, 0.0, 0.0], 'rewardMean': 0.7204471524085919, 'totalEpisodes': 318, 'stepsPerEpisode': 637, 'rewardPerEpisode': 459.8243584502048
'totalSteps': 24320, 'rewardStep': 0.6859134173491901, 'errorList': [], 'lossList': [0.0, -1.2427559173107148, 0.0, 6.516856890320778, 0.0, 0.0, 0.0], 'rewardMean': 0.7196333758321166, 'totalEpisodes': 320, 'stepsPerEpisode': 251, 'rewardPerEpisode': 198.4493977839453
'totalSteps': 25600, 'rewardStep': 0.9171680044416695, 'errorList': [], 'lossList': [0.0, -1.201641017794609, 0.0, 2.9902554021775725, 0.0, 0.0, 0.0], 'rewardMean': 0.7327397119501564, 'totalEpisodes': 320, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1116.8136033194808
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=61.41
