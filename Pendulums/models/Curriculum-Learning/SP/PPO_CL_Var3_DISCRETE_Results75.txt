#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 75
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7615115961618314, 'errorList': [], 'lossList': [0.0, -1.416316899061203, 0.0, 34.04166961669922, 0.0, 0.0, 0.0], 'rewardMean': 0.6028671685686746, 'totalEpisodes': 54, 'stepsPerEpisode': 22, 'rewardPerEpisode': 14.045930817942493
'totalSteps': 3840, 'rewardStep': 0.24376022335712494, 'errorList': [], 'lossList': [0.0, -1.4158412671089173, 0.0, 39.97222840309143, 0.0, 0.0, 0.0], 'rewardMean': 0.4831648534981581, 'totalEpisodes': 69, 'stepsPerEpisode': 95, 'rewardPerEpisode': 70.66786970327684
'totalSteps': 5120, 'rewardStep': 0.6881630550608158, 'errorList': [], 'lossList': [0.0, -1.4050467377901077, 0.0, 30.977895011901854, 0.0, 0.0, 0.0], 'rewardMean': 0.5344144038888226, 'totalEpisodes': 78, 'stepsPerEpisode': 193, 'rewardPerEpisode': 146.5557322671595
'totalSteps': 6400, 'rewardStep': 0.5538731503602887, 'errorList': [], 'lossList': [0.0, -1.4017364168167115, 0.0, 16.829771225452422, 0.0, 0.0, 0.0], 'rewardMean': 0.5383061531831158, 'totalEpisodes': 85, 'stepsPerEpisode': 160, 'rewardPerEpisode': 120.06529699410683
'totalSteps': 7680, 'rewardStep': 0.856045953496996, 'errorList': [], 'lossList': [0.0, -1.3981187486648559, 0.0, 25.058083976507188, 0.0, 0.0, 0.0], 'rewardMean': 0.5912627865687625, 'totalEpisodes': 89, 'stepsPerEpisode': 418, 'rewardPerEpisode': 339.72563791799064
'totalSteps': 8960, 'rewardStep': 0.54459494524558, 'errorList': [], 'lossList': [0.0, -1.3821819698810578, 0.0, 11.590758373737335, 0.0, 0.0, 0.0], 'rewardMean': 0.5845959520940222, 'totalEpisodes': 94, 'stepsPerEpisode': 123, 'rewardPerEpisode': 86.98111651911053
'totalSteps': 10240, 'rewardStep': 0.5760395984982656, 'errorList': [], 'lossList': [0.0, -1.3892254626750946, 0.0, 295.7713512420654, 0.0, 0.0, 0.0], 'rewardMean': 0.5835264078945526, 'totalEpisodes': 134, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.139811155506035
'totalSteps': 11520, 'rewardStep': 0.6299804160999408, 'errorList': [], 'lossList': [0.0, -1.383367794752121, 0.0, 85.07111936569214, 0.0, 0.0, 0.0], 'rewardMean': 0.5886879643618179, 'totalEpisodes': 168, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.084916484408537
'totalSteps': 12800, 'rewardStep': 0.49450040145289526, 'errorList': [], 'lossList': [0.0, -1.374663861989975, 0.0, 51.7454797744751, 0.0, 0.0, 0.0], 'rewardMean': 0.5792692080709256, 'totalEpisodes': 195, 'stepsPerEpisode': 44, 'rewardPerEpisode': 26.083929784349156
'totalSteps': 14080, 'rewardStep': 0.9736357163283716, 'errorList': [3.796937808333923, 3.7613187485031103, 3.748438989650965, 3.751202928802343, 3.7752856824697525, 3.7824634731995963, 3.750429567814958, 3.765325190196822, 3.748346810630124, 3.7774689148642167, 3.7675720119459393, 3.798557912945091, 3.7683851861340947, 3.755250835421657, 3.8062341611548054, 3.7681612997234715, 3.756968087306082, 3.7753806357844737, 3.756350242708066, 3.76089159553341, 3.758896442623783, 3.7658744355084224, 3.754333987249259, 3.751702223836382, 3.7593356648669647, 3.770131168680605, 3.5802684648551346, 3.7789128610142364, 1.152908582392578, 3.768028315338926, 3.767288602463909, 3.7693616093880395, 2.8164274753451775, 3.7700426665328175, 3.797586667090371, 3.78469763707573, 3.7664675648617774, 3.7612166266135607, 3.7382546276785615, 3.7677524606376265, 3.7571537276480904, 3.781718130204514, 3.7603609330180707, 3.7452054254471236, 3.75192461181539, 3.8041948416061184, 3.752118721577298, 3.7508489784038552, 3.7438905567635294, 3.7585339246531277], 'lossList': [0.0, -1.3752362060546874, 0.0, 11.995858590602875, 0.0, 0.0, 0.0], 'rewardMean': 0.6322105056062111, 'totalEpisodes': 208, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.775470186695651, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.8535592030528549, 'errorList': [], 'lossList': [0.0, -1.3677037143707276, 0.0, 27.09651087284088, 0.0, 0.0, 0.0], 'rewardMean': 0.6414152662953134, 'totalEpisodes': 222, 'stepsPerEpisode': 68, 'rewardPerEpisode': 62.609905211144664
'totalSteps': 16640, 'rewardStep': 0.7400724515144543, 'errorList': [], 'lossList': [0.0, -1.3562805062532426, 0.0, 13.068761959075928, 0.0, 0.0, 0.0], 'rewardMean': 0.6910464891110463, 'totalEpisodes': 223, 'stepsPerEpisode': 290, 'rewardPerEpisode': 251.42043140013237
'totalSteps': 17920, 'rewardStep': 0.6139701606112808, 'errorList': [], 'lossList': [0.0, -1.3496051532030107, 0.0, 8.590642926692963, 0.0, 0.0, 0.0], 'rewardMean': 0.6836271996660928, 'totalEpisodes': 228, 'stepsPerEpisode': 296, 'rewardPerEpisode': 187.6505950599977
'totalSteps': 19200, 'rewardStep': 0.8819033655351776, 'errorList': [], 'lossList': [0.0, -1.3445074754953383, 0.0, 17.54166715860367, 0.0, 0.0, 0.0], 'rewardMean': 0.7164302211835817, 'totalEpisodes': 231, 'stepsPerEpisode': 202, 'rewardPerEpisode': 165.5805066194429
'totalSteps': 20480, 'rewardStep': 0.8617121268745669, 'errorList': [], 'lossList': [0.0, -1.340550816655159, 0.0, 6.017829580307007, 0.0, 0.0, 0.0], 'rewardMean': 0.7169968385213388, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1023.674150666254
'totalSteps': 21760, 'rewardStep': 0.8637661339536014, 'errorList': [], 'lossList': [0.0, -1.3344734978675843, 0.0, 3.526659778803587, 0.0, 0.0, 0.0], 'rewardMean': 0.7489139573921408, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.2025895228721
'totalSteps': 23040, 'rewardStep': 0.8211864641363131, 'errorList': [], 'lossList': [0.0, -1.318808965086937, 0.0, 3.6638860285282133, 0.0, 0.0, 0.0], 'rewardMean': 0.7734286439559457, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.6587396786042
'totalSteps': 24320, 'rewardStep': 0.7959402064218936, 'errorList': [], 'lossList': [0.0, -1.279161800146103, 0.0, 2.7112345867976546, 0.0, 0.0, 0.0], 'rewardMean': 0.7900246229881409, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.077506979209
'totalSteps': 25600, 'rewardStep': 0.9381720495040216, 'errorList': [0.03156587654967887, 0.02094148014295558, 0.04302833264092827, 0.059990702389255086, 0.02112527004445318, 0.02904114686590701, 0.054043709160794, 0.06729934044841702, 0.04856427165552331, 0.03311951040586227, 0.029111442533692192, 0.0298564164592582, 0.060431144497042726, 0.043757452396211316, 0.034592586431025515, 0.05654416865941159, 0.03538228921950249, 0.014992954097972536, 0.03581315757323333, 0.034559569821571344, 0.011843695548788316, 0.04082439615874938, 0.07046657639770171, 0.022827528584776262, 0.02751737673875282, 0.010491453016485335, 0.054597393202629026, 0.03515894558256729, 0.019836644560851165, 0.06938372061781617, 0.09406006894339486, 0.013071125496142701, 0.032916436046666844, 0.06915438764968371, 0.02268348025049797, 0.03194464267301012, 0.05383781827108463, 0.01677726452107641, 0.05773043537741596, 0.04714877551976619, 0.05441107364071829, 0.016954138793812766, 0.07689695889236407, 0.05043340657912703, 0.0020010495501278872, 0.021434752229641654, 0.06180954754214268, 0.013418166111740492, 0.025752690596509142, 0.023875360251486327], 'lossList': [0.0, -1.237121948003769, 0.0, 2.155328361578286, 0.0, 0.0, 0.0], 'rewardMean': 0.8343917877932535, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.06221671769, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=103.21
