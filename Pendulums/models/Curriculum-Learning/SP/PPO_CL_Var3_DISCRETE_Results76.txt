#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 76
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980785764922136, 'errorList': [], 'lossList': [0.0, -1.417504243850708, 0.0, 30.41210502624512, 0.0, 0.0, 0.0], 'rewardMean': 0.7065643546668106, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.34356450654182
'totalSteps': 3840, 'rewardStep': 0.7337342130467079, 'errorList': [], 'lossList': [0.0, -1.4060174733400346, 0.0, 37.440305285453796, 0.0, 0.0, 0.0], 'rewardMean': 0.7156209741267764, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.247863075167686
'totalSteps': 5120, 'rewardStep': 0.8983266968431368, 'errorList': [], 'lossList': [0.0, -1.4023935985565186, 0.0, 28.85670949935913, 0.0, 0.0, 0.0], 'rewardMean': 0.7612974048058665, 'totalEpisodes': 76, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.645272888643724
'totalSteps': 6400, 'rewardStep': 0.5405434309987291, 'errorList': [], 'lossList': [0.0, -1.3908120518922806, 0.0, 28.452368409633635, 0.0, 0.0, 0.0], 'rewardMean': 0.717146610044439, 'totalEpisodes': 79, 'stepsPerEpisode': 83, 'rewardPerEpisode': 68.01456295733756
'totalSteps': 7680, 'rewardStep': 0.9298070272068737, 'errorList': [], 'lossList': [0.0, -1.3761466151475907, 0.0, 11.433969601392747, 0.0, 0.0, 0.0], 'rewardMean': 0.7525900129048448, 'totalEpisodes': 82, 'stepsPerEpisode': 54, 'rewardPerEpisode': 46.009717960296676
'totalSteps': 8960, 'rewardStep': 0.7478267082714463, 'errorList': [], 'lossList': [0.0, -1.3627811884880066, 0.0, 18.326602531671526, 0.0, 0.0, 0.0], 'rewardMean': 0.7519095408143593, 'totalEpisodes': 83, 'stepsPerEpisode': 444, 'rewardPerEpisode': 365.94615226628525
'totalSteps': 10240, 'rewardStep': 0.8955457541899999, 'errorList': [], 'lossList': [0.0, -1.3532727402448654, 0.0, 193.9335556793213, 0.0, 0.0, 0.0], 'rewardMean': 0.7698640674863144, 'totalEpisodes': 107, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.718473057199274
'totalSteps': 11520, 'rewardStep': 0.39829987034743414, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6955512280585383, 'totalEpisodes': 122, 'stepsPerEpisode': 6, 'rewardPerEpisode': 2.7537519558758317
'totalSteps': 12800, 'rewardStep': 0.8883324725848638, 'errorList': [], 'lossList': [0.0, -1.342373406291008, 0.0, 94.15288896560669, 0.0, 0.0, 0.0], 'rewardMean': 0.702879462032884, 'totalEpisodes': 143, 'stepsPerEpisode': 56, 'rewardPerEpisode': 49.55203015510141
'totalSteps': 14080, 'rewardStep': 0.33328052511798156, 'errorList': [], 'lossList': [0.0, -1.326112385392189, 0.0, 23.174070246219635, 0.0, 0.0, 0.0], 'rewardMean': 0.6763996568954607, 'totalEpisodes': 150, 'stepsPerEpisode': 231, 'rewardPerEpisode': 180.04732876740846
'totalSteps': 15360, 'rewardStep': 0.6994585016578114, 'errorList': [], 'lossList': [0.0, -1.3153153854608535, 0.0, 8.950754671096801, 0.0, 0.0, 0.0], 'rewardMean': 0.6729720857565711, 'totalEpisodes': 156, 'stepsPerEpisode': 85, 'rewardPerEpisode': 68.50236793088989
'totalSteps': 16640, 'rewardStep': 0.578185408944931, 'errorList': [], 'lossList': [0.0, -1.3060775589942932, 0.0, 6.371222645044327, 0.0, 0.0, 0.0], 'rewardMean': 0.6409579569667505, 'totalEpisodes': 161, 'stepsPerEpisode': 171, 'rewardPerEpisode': 132.19323931793065
'totalSteps': 17920, 'rewardStep': 0.37870838214606845, 'errorList': [], 'lossList': [0.0, -1.319465423822403, 0.0, 5.425313429832459, 0.0, 0.0, 0.0], 'rewardMean': 0.6247744520814844, 'totalEpisodes': 165, 'stepsPerEpisode': 153, 'rewardPerEpisode': 105.09402998771664
'totalSteps': 19200, 'rewardStep': 0.7195010363988784, 'errorList': [], 'lossList': [0.0, -1.3303735893964768, 0.0, 5.416302278637886, 0.0, 0.0, 0.0], 'rewardMean': 0.6037438530006849, 'totalEpisodes': 169, 'stepsPerEpisode': 436, 'rewardPerEpisode': 292.9854957805809
'totalSteps': 20480, 'rewardStep': 0.7724387869075953, 'errorList': [], 'lossList': [0.0, -1.316293870806694, 0.0, 7.767469412684441, 0.0, 0.0, 0.0], 'rewardMean': 0.6062050608642998, 'totalEpisodes': 170, 'stepsPerEpisode': 1179, 'rewardPerEpisode': 904.3873294222228
'totalSteps': 21760, 'rewardStep': 0.800643977615162, 'errorList': [], 'lossList': [0.0, -1.2906654131412507, 0.0, 3.61202994287014, 0.0, 0.0, 0.0], 'rewardMean': 0.596714883206816, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1030.5982895171198
'totalSteps': 23040, 'rewardStep': 0.8647205120510422, 'errorList': [], 'lossList': [0.0, -1.2684628742933273, 0.0, 2.659462543874979, 0.0, 0.0, 0.0], 'rewardMean': 0.6433569473771769, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.9019040353762
'totalSteps': 24320, 'rewardStep': 0.805509693502328, 'errorList': [], 'lossList': [0.0, -1.2281008523702621, 0.0, 1.9334887424856424, 0.0, 0.0, 0.0], 'rewardMean': 0.6840779296926662, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.39533757312
'totalSteps': 25600, 'rewardStep': 0.9595542843577876, 'errorList': [0.059084991385097305, 0.07647112500952043, 0.09050550513797688, 0.05990284961237635, 0.05562510102227296, 0.05910788557634522, 0.10233307508129605, 0.07036611563997716, 0.08194699651103818, 0.0932292420297235, 0.059443229836219996, 0.05806568518980199, 0.05890258900106363, 0.06672699836877542, 0.06959141420458277, 0.08164221353410957, 0.10864519734435037, 0.07172626948622875, 0.10007632191863967, 0.07253330971900271, 0.0581191344434518, 0.07565298339089055, 0.059374073859775126, 0.09261602798641859, 0.06995608655020323, 0.08920841682902574, 0.07438183263052255, 0.05777458372941559, 0.06824484451793673, 0.06737892406054731, 0.08005886120689881, 0.09410245672121766, 0.06593272817459996, 0.05759863710886558, 0.05743374608518339, 0.0778930559259691, 0.09821225424153268, 0.0786005730106414, 0.07078141816871383, 0.08277019689828251, 0.09180463032149944, 0.05923827909946329, 0.07224502313524554, 0.079466089039254, 0.05901266171403006, 0.0939984963892943, 0.060694229592822606, 0.08539262874876115, 0.07284406018726124, 0.07491542019531064], 'lossList': [0.0, -1.1759632158279418, 0.0, 1.4198145578056574, 0.0, 0.0, 0.0], 'rewardMean': 0.6912001108699586, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.968182687896, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.8
