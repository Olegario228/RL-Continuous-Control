#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 78
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.43590750673834866, 'errorList': [], 'lossList': [0.0, -1.4341206073760986, 0.0, 31.667020444869994, 0.0, 0.0, 0.0], 'rewardMean': 0.47042336673936674, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.31342545895599
'totalSteps': 3840, 'rewardStep': 0.9553872686153767, 'errorList': [], 'lossList': [0.0, -1.4267620873451232, 0.0, 33.83611287593842, 0.0, 0.0, 0.0], 'rewardMean': 0.6320780006980368, 'totalEpisodes': 72, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.22112575509128
'totalSteps': 5120, 'rewardStep': 0.8388254462094117, 'errorList': [], 'lossList': [0.0, -1.422535389661789, 0.0, 42.977623014450074, 0.0, 0.0, 0.0], 'rewardMean': 0.6837648620758805, 'totalEpisodes': 78, 'stepsPerEpisode': 72, 'rewardPerEpisode': 64.54850374529123
'totalSteps': 6400, 'rewardStep': 0.48326593053442374, 'errorList': [], 'lossList': [0.0, -1.4289336198568343, 0.0, 39.22549348831177, 0.0, 0.0, 0.0], 'rewardMean': 0.6436650757675891, 'totalEpisodes': 83, 'stepsPerEpisode': 143, 'rewardPerEpisode': 101.4031990359977
'totalSteps': 7680, 'rewardStep': 0.9520836709226697, 'errorList': [], 'lossList': [0.0, -1.4247060024738312, 0.0, 45.84367401599884, 0.0, 0.0, 0.0], 'rewardMean': 0.6950681749601025, 'totalEpisodes': 88, 'stepsPerEpisode': 53, 'rewardPerEpisode': 48.64993932157514
'totalSteps': 8960, 'rewardStep': 0.8106282727462086, 'errorList': [], 'lossList': [0.0, -1.4190297430753709, 0.0, 37.19685288906098, 0.0, 0.0, 0.0], 'rewardMean': 0.7115767603581177, 'totalEpisodes': 90, 'stepsPerEpisode': 372, 'rewardPerEpisode': 252.54289202473865
'totalSteps': 10240, 'rewardStep': 0.890324022986828, 'errorList': [], 'lossList': [0.0, -1.4277822375297546, 0.0, 211.28768947601318, 0.0, 0.0, 0.0], 'rewardMean': 0.7339201681867065, 'totalEpisodes': 120, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.733868890218389
'totalSteps': 11520, 'rewardStep': 0.4104153738344009, 'errorList': [], 'lossList': [0.0, -1.4287089222669602, 0.0, 61.83420916557312, 0.0, 0.0, 0.0], 'rewardMean': 0.6979751910364503, 'totalEpisodes': 136, 'stepsPerEpisode': 115, 'rewardPerEpisode': 74.87347803017687
'totalSteps': 12800, 'rewardStep': 0.6304286646310628, 'errorList': [], 'lossList': [0.0, -1.436217115521431, 0.0, 21.019198331832886, 0.0, 0.0, 0.0], 'rewardMean': 0.6912205383959116, 'totalEpisodes': 146, 'stepsPerEpisode': 54, 'rewardPerEpisode': 45.03944614412304
'totalSteps': 14080, 'rewardStep': 0.2066680337575162, 'errorList': [], 'lossList': [0.0, -1.4494774997234345, 0.0, 20.23066885471344, 0.0, 0.0, 0.0], 'rewardMean': 0.6613934190976247, 'totalEpisodes': 153, 'stepsPerEpisode': 228, 'rewardPerEpisode': 159.31686420925539
'totalSteps': 15360, 'rewardStep': 0.6512884728112616, 'errorList': [], 'lossList': [0.0, -1.4575297778844833, 0.0, 11.739162101745606, 0.0, 0.0, 0.0], 'rewardMean': 0.6829315157049161, 'totalEpisodes': 158, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.294705997053831
'totalSteps': 16640, 'rewardStep': 0.6445648979712222, 'errorList': [], 'lossList': [0.0, -1.4644758492708205, 0.0, 6.861691780090332, 0.0, 0.0, 0.0], 'rewardMean': 0.6518492786405006, 'totalEpisodes': 162, 'stepsPerEpisode': 167, 'rewardPerEpisode': 120.50177410388186
'totalSteps': 17920, 'rewardStep': 0.8627135561521128, 'errorList': [], 'lossList': [0.0, -1.469328562617302, 0.0, 5.254373055696488, 0.0, 0.0, 0.0], 'rewardMean': 0.6542380896347708, 'totalEpisodes': 168, 'stepsPerEpisode': 60, 'rewardPerEpisode': 53.304412887934106
'totalSteps': 19200, 'rewardStep': 0.7572911024334387, 'errorList': [], 'lossList': [0.0, -1.4608087080717087, 0.0, 5.714323936700821, 0.0, 0.0, 0.0], 'rewardMean': 0.6816406068246722, 'totalEpisodes': 171, 'stepsPerEpisode': 191, 'rewardPerEpisode': 139.03362243252624
'totalSteps': 20480, 'rewardStep': 0.8996219401495582, 'errorList': [], 'lossList': [0.0, -1.4594636034965516, 0.0, 3.9521102595329283, 0.0, 0.0, 0.0], 'rewardMean': 0.6763944337473611, 'totalEpisodes': 174, 'stepsPerEpisode': 179, 'rewardPerEpisode': 154.8052976529647
'totalSteps': 21760, 'rewardStep': 0.8446418486949422, 'errorList': [], 'lossList': [0.0, -1.4497599279880524, 0.0, 3.2752495354413984, 0.0, 0.0, 0.0], 'rewardMean': 0.6797957913422343, 'totalEpisodes': 175, 'stepsPerEpisode': 118, 'rewardPerEpisode': 102.82917788996748
'totalSteps': 23040, 'rewardStep': 0.8573342587631357, 'errorList': [], 'lossList': [0.0, -1.416485733985901, 0.0, 3.6657193154096603, 0.0, 0.0, 0.0], 'rewardMean': 0.6764968149198651, 'totalEpisodes': 176, 'stepsPerEpisode': 165, 'rewardPerEpisode': 148.35212914238542
'totalSteps': 24320, 'rewardStep': 0.6029614702536465, 'errorList': [], 'lossList': [0.0, -1.3793638360500335, 0.0, 1.352291412949562, 0.0, 0.0, 0.0], 'rewardMean': 0.6957514245617897, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1079.6383328237994
'totalSteps': 25600, 'rewardStep': 0.9863094767741587, 'errorList': [0.08149030943641153, 0.08679657000474983, 0.07387601415138124, 0.0862274711563878, 0.08065540375429833, 0.07223458409033019, 0.06951822615388563, 0.08169252844475676, 0.0748939964645759, 0.09037265229219123, 0.07341734487589327, 0.08131759184978082, 0.07839760467547967, 0.08248710036601489, 0.08310530058062102, 0.06949083151525864, 0.08466251485574823, 0.07652012036930467, 0.08690889258386497, 0.07336924870814303, 0.08187274659953285, 0.07548090818043891, 0.07278094085287239, 0.07869173832384835, 0.08151646112897808, 0.0837404314746443, 0.07076791224573933, 0.07372517923762943, 0.0858813125780868, 0.07271840002855443, 0.06979746968042501, 0.0836922223663849, 0.08040738771825261, 0.06943826693214791, 0.08179385731103497, 0.07090573874055224, 0.08395665615315123, 0.08097526924639314, 0.07078617755193083, 0.07339887824895674, 0.08783255488997758, 0.06974242930237508, 0.08382538172558923, 0.06965094726492148, 0.08670716722356157, 0.07281427544135448, 0.07050600038106426, 0.07012410681942723, 0.07140256153225347, 0.07153234767708244], 'lossList': [0.0, -1.360046209692955, 0.0, 0.9950482629984617, 0.0, 0.0, 0.0], 'rewardMean': 0.7313395057760992, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.6254507614424, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.55
