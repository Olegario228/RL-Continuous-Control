#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 79
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764237844444184, 'errorList': [], 'lossList': [0.0, -1.4117648071050644, 0.0, 34.56727179527283, 0.0, 0.0, 0.0], 'rewardMean': 0.8487569592979127, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.540084423708798
'totalSteps': 3840, 'rewardStep': 0.8301048750001292, 'errorList': [], 'lossList': [0.0, -1.4150108939409256, 0.0, 39.124620037078856, 0.0, 0.0, 0.0], 'rewardMean': 0.8425395978653182, 'totalEpisodes': 84, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.84030782755015
'totalSteps': 5120, 'rewardStep': 0.6848389580966191, 'errorList': [], 'lossList': [0.0, -1.4063812613487243, 0.0, 28.86642453432083, 0.0, 0.0, 0.0], 'rewardMean': 0.8031144379231434, 'totalEpisodes': 90, 'stepsPerEpisode': 117, 'rewardPerEpisode': 92.77188059566005
'totalSteps': 6400, 'rewardStep': 0.522264938468509, 'errorList': [], 'lossList': [0.0, -1.3970475417375565, 0.0, 23.584555982351304, 0.0, 0.0, 0.0], 'rewardMean': 0.7469445380322165, 'totalEpisodes': 94, 'stepsPerEpisode': 456, 'rewardPerEpisode': 368.87075139144315
'totalSteps': 7680, 'rewardStep': 0.4707934727118634, 'errorList': [], 'lossList': [0.0, -1.3926397609710692, 0.0, 25.67977596640587, 0.0, 0.0, 0.0], 'rewardMean': 0.7009193604788243, 'totalEpisodes': 96, 'stepsPerEpisode': 428, 'rewardPerEpisode': 314.24768187725965
'totalSteps': 8960, 'rewardStep': 0.9312512349808809, 'errorList': [192.83996069153068, 189.29616348999215, 223.60402298195038, 203.569766236336, 211.40368412839587, 220.13069592875755, 206.32655472854205, 204.5104984083509, 200.19801078143607, 197.55905032163326, 213.18369522964684, 213.02670912954946, 216.62642519981395, 196.72406312326436, 221.03387949879, 185.9731642967888, 201.86981822836051, 216.3621350019524, 192.0136204407075, 193.69432830675308, 219.12196351996536, 215.0828825201461, 207.87913768464367, 212.7983085056542, 168.82067848333205, 217.30496249559255, 174.12579789663218, 215.05692026252865, 220.45460715711587, 207.30459304300845, 207.5076013719049, 217.47254533921392, 218.6089901384876, 173.03953011999303, 187.8096179541997, 204.34780709392308, 140.28486885889848, 205.49546080744634, 176.47725044393349, 196.97865784452193, 199.0393525507858, 176.01855565705344, 205.3393527254708, 178.28760276211918, 174.09821570308705, 207.73509859907918, 216.93832660076706, 188.75289985120529, 206.86390378726406, 186.89714660314087], 'lossList': [0.0, -1.3727877336740493, 0.0, 21.57163408994675, 0.0, 0.0, 0.0], 'rewardMean': 0.7338239139791181, 'totalEpisodes': 97, 'stepsPerEpisode': 1141, 'rewardPerEpisode': 871.4339552471587, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.807020171079835, 'errorList': [], 'lossList': [0.0, -1.3701494580507279, 0.0, 296.9559836578369, 0.0, 0.0, 0.0], 'rewardMean': 0.7429734461167077, 'totalEpisodes': 132, 'stepsPerEpisode': 20, 'rewardPerEpisode': 17.15638174547041
'totalSteps': 11520, 'rewardStep': 0.912229207909896, 'errorList': [], 'lossList': [0.0, -1.3726081919670106, 0.0, 122.71984203338623, 0.0, 0.0, 0.0], 'rewardMean': 0.7617796418715064, 'totalEpisodes': 164, 'stepsPerEpisode': 77, 'rewardPerEpisode': 69.23565459198578
'totalSteps': 12800, 'rewardStep': 0.36971845316516244, 'errorList': [], 'lossList': [0.0, -1.3708718150854111, 0.0, 33.032821164131164, 0.0, 0.0, 0.0], 'rewardMean': 0.722573523000872, 'totalEpisodes': 179, 'stepsPerEpisode': 93, 'rewardPerEpisode': 60.90462123080704
'totalSteps': 14080, 'rewardStep': 0.6348479396960128, 'errorList': [], 'lossList': [0.0, -1.3559108966588973, 0.0, 14.670124657154084, 0.0, 0.0, 0.0], 'rewardMean': 0.6939493035553326, 'totalEpisodes': 190, 'stepsPerEpisode': 166, 'rewardPerEpisode': 139.56628230359274
'totalSteps': 15360, 'rewardStep': 0.8131230042715908, 'errorList': [], 'lossList': [0.0, -1.34903934776783, 0.0, 14.149898216724395, 0.0, 0.0, 0.0], 'rewardMean': 0.6976192255380499, 'totalEpisodes': 198, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.135394129381687
'totalSteps': 16640, 'rewardStep': 0.5941010679173035, 'errorList': [], 'lossList': [0.0, -1.3435671800374984, 0.0, 20.90846073627472, 0.0, 0.0, 0.0], 'rewardMean': 0.6740188448297673, 'totalEpisodes': 205, 'stepsPerEpisode': 29, 'rewardPerEpisode': 20.860343805260378
'totalSteps': 17920, 'rewardStep': 0.8322382107396241, 'errorList': [], 'lossList': [0.0, -1.3287557530403138, 0.0, 6.59769727230072, 0.0, 0.0, 0.0], 'rewardMean': 0.6887587700940678, 'totalEpisodes': 210, 'stepsPerEpisode': 56, 'rewardPerEpisode': 50.464668588956165
'totalSteps': 19200, 'rewardStep': 0.6548129199859304, 'errorList': [], 'lossList': [0.0, -1.3262752109766007, 0.0, 5.820586342811584, 0.0, 0.0, 0.0], 'rewardMean': 0.70201356824581, 'totalEpisodes': 214, 'stepsPerEpisode': 120, 'rewardPerEpisode': 101.60508272574941
'totalSteps': 20480, 'rewardStep': 0.4881856194877998, 'errorList': [], 'lossList': [0.0, -1.3145742124319078, 0.0, 5.490267567634582, 0.0, 0.0, 0.0], 'rewardMean': 0.7037527829234037, 'totalEpisodes': 218, 'stepsPerEpisode': 219, 'rewardPerEpisode': 160.67306978412515
'totalSteps': 21760, 'rewardStep': 0.2801532237439789, 'errorList': [], 'lossList': [0.0, -1.3041775983572006, 0.0, 3.7402646064758303, 0.0, 0.0, 0.0], 'rewardMean': 0.6386429817997133, 'totalEpisodes': 221, 'stepsPerEpisode': 423, 'rewardPerEpisode': 340.94263293441463
'totalSteps': 23040, 'rewardStep': 0.8973598912856493, 'errorList': [], 'lossList': [0.0, -1.3203647714853286, 0.0, 4.531645721197128, 0.0, 0.0, 0.0], 'rewardMean': 0.6476769538202949, 'totalEpisodes': 223, 'stepsPerEpisode': 171, 'rewardPerEpisode': 149.13535264964673
'totalSteps': 24320, 'rewardStep': 0.822672609334307, 'errorList': [], 'lossList': [0.0, -1.30748131275177, 0.0, 1.7734813670814038, 0.0, 0.0, 0.0], 'rewardMean': 0.638721293962736, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.171223600584
'totalSteps': 25600, 'rewardStep': 0.9269257493068132, 'errorList': [], 'lossList': [0.0, -1.2843888694047927, 0.0, 1.2514155501127242, 0.0, 0.0, 0.0], 'rewardMean': 0.694442023576901, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.619752209431
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.84
