#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 8
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.716747579218783, 'errorList': [], 'lossList': [0.0, -1.4116221374273301, 0.0, 24.99706236243248, 0.0, 0.0, 0.0], 'rewardMean': 0.5305513853821189, 'totalEpisodes': 14, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.019792431251155
'totalSteps': 3840, 'rewardStep': 0.9149805715430457, 'errorList': [], 'lossList': [0.0, -1.4000626653432846, 0.0, 28.992608392238616, 0.0, 0.0, 0.0], 'rewardMean': 0.6586944474357611, 'totalEpisodes': 19, 'stepsPerEpisode': 299, 'rewardPerEpisode': 219.51185256085597
'totalSteps': 5120, 'rewardStep': 0.7414091158191021, 'errorList': [], 'lossList': [0.0, -1.4075797414779663, 0.0, 30.47893680214882, 0.0, 0.0, 0.0], 'rewardMean': 0.6793731145315964, 'totalEpisodes': 20, 'stepsPerEpisode': 180, 'rewardPerEpisode': 150.03326592077696
'totalSteps': 6400, 'rewardStep': 0.6776694699609809, 'errorList': [], 'lossList': [0.0, -1.4038728946447372, 0.0, 219.913413772583, 0.0, 0.0, 0.0], 'rewardMean': 0.6790323856174734, 'totalEpisodes': 73, 'stepsPerEpisode': 66, 'rewardPerEpisode': 52.8819845775624
'totalSteps': 7680, 'rewardStep': 0.588820344496633, 'errorList': [], 'lossList': [0.0, -1.3935179722309112, 0.0, 109.79466583251953, 0.0, 0.0, 0.0], 'rewardMean': 0.6639970454306666, 'totalEpisodes': 129, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.588820344496633
'totalSteps': 8960, 'rewardStep': 0.882125560271469, 'errorList': [], 'lossList': [0.0, -1.3834736359119415, 0.0, 57.90972528457642, 0.0, 0.0, 0.0], 'rewardMean': 0.6951582618364957, 'totalEpisodes': 158, 'stepsPerEpisode': 105, 'rewardPerEpisode': 75.27061194069327
'totalSteps': 10240, 'rewardStep': 0.859824815938484, 'errorList': [], 'lossList': [0.0, -1.3740599328279495, 0.0, 50.39459958076477, 0.0, 0.0, 0.0], 'rewardMean': 0.715741581099244, 'totalEpisodes': 173, 'stepsPerEpisode': 174, 'rewardPerEpisode': 143.0266945401797
'totalSteps': 11520, 'rewardStep': 0.6782181405268733, 'errorList': [], 'lossList': [0.0, -1.3700059354305267, 0.0, 17.55961806535721, 0.0, 0.0, 0.0], 'rewardMean': 0.7115723099245361, 'totalEpisodes': 181, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.56891820281884
'totalSteps': 12800, 'rewardStep': 0.8109966095188615, 'errorList': [], 'lossList': [0.0, -1.3666701251268387, 0.0, 32.449727158546445, 0.0, 0.0, 0.0], 'rewardMean': 0.7215147398839686, 'totalEpisodes': 188, 'stepsPerEpisode': 185, 'rewardPerEpisode': 143.8509500634333
'totalSteps': 14080, 'rewardStep': 0.6330405792096759, 'errorList': [], 'lossList': [0.0, -1.3549249643087387, 0.0, 11.119467270374297, 0.0, 0.0, 0.0], 'rewardMean': 0.7503832786503909, 'totalEpisodes': 190, 'stepsPerEpisode': 849, 'rewardPerEpisode': 602.0937135465684
'totalSteps': 15360, 'rewardStep': 0.6755451687131794, 'errorList': [], 'lossList': [0.0, -1.3305417358875276, 0.0, 12.015040348768235, 0.0, 0.0, 0.0], 'rewardMean': 0.7462630375998305, 'totalEpisodes': 192, 'stepsPerEpisode': 900, 'rewardPerEpisode': 596.1073604645529
'totalSteps': 16640, 'rewardStep': 0.5766656063402936, 'errorList': [], 'lossList': [0.0, -1.3172514891624452, 0.0, 3.9542286437749863, 0.0, 0.0, 0.0], 'rewardMean': 0.7124315410795552, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 791.582418874678
'totalSteps': 17920, 'rewardStep': 0.9091464937805073, 'errorList': [], 'lossList': [0.0, -1.3164674180746079, 0.0, 6.251617307066917, 0.0, 0.0, 0.0], 'rewardMean': 0.7292052788756958, 'totalEpisodes': 193, 'stepsPerEpisode': 952, 'rewardPerEpisode': 710.5297379384267
'totalSteps': 19200, 'rewardStep': 0.8573377954335725, 'errorList': [], 'lossList': [0.0, -1.2975772100687026, 0.0, 3.161240345984697, 0.0, 0.0, 0.0], 'rewardMean': 0.747172111422955, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.6289783772179
'totalSteps': 20480, 'rewardStep': 0.8832887464914722, 'errorList': [], 'lossList': [0.0, -1.260984423160553, 0.0, 2.692241071239114, 0.0, 0.0, 0.0], 'rewardMean': 0.7766189516224389, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.8242597783728
'totalSteps': 21760, 'rewardStep': 0.8722708030619161, 'errorList': [], 'lossList': [0.0, -1.2254866486787797, 0.0, 1.722438638471067, 0.0, 0.0, 0.0], 'rewardMean': 0.7756334759014837, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.7932619975218
'totalSteps': 23040, 'rewardStep': 0.8317811700883674, 'errorList': [], 'lossList': [0.0, -1.1978831106424332, 0.0, 1.9002784653007985, 0.0, 0.0, 0.0], 'rewardMean': 0.772829111316472, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.1650917395054
'totalSteps': 24320, 'rewardStep': 0.9171400457012442, 'errorList': [], 'lossList': [0.0, -1.1589848840236663, 0.0, 1.4918161296844483, 0.0, 0.0, 0.0], 'rewardMean': 0.796721301833909, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.9223982012772
'totalSteps': 25600, 'rewardStep': 0.9315328306079576, 'errorList': [0.06058310774754454, 0.05409991090228105, 0.0633238810944068, 0.0494718946079764, 0.0916917964271539, 0.06052458663579314, 0.08911861525084049, 0.053129321701956535, 0.06460118324784724, 0.047963274226140086, 0.11986024126445882, 0.05290569966320393, 0.04901178699569307, 0.043016281247998674, 0.09389485115479651, 0.03452108950199684, 0.06282563069674427, 0.03480100096834539, 0.07767009257245854, 0.11061228393961244, 0.05670603977888404, 0.14073997151957784, 0.04947020493855153, 0.05911682634797696, 0.0970556442165371, 0.06463973613008775, 0.06500151213645187, 0.08734821314982019, 0.06385342831366665, 0.08819675610522226, 0.0902186766242645, 0.05404527934795295, 0.0706622183741821, 0.04188409333958781, 0.04866013719487735, 0.06960354696976048, 0.11421763836361891, 0.03924440102103789, 0.05978951219869897, 0.08996490579370485, 0.045276222142189036, 0.05930459607779508, 0.102689585130151, 0.04877103927591182, 0.05006621071686727, 0.03743035436207876, 0.08098000459105983, 0.05048554793791951, 0.08523478601944402, 0.05794477235972102], 'lossList': [0.0, -1.123261232972145, 0.0, 1.0451356054190546, 0.0, 0.0, 0.0], 'rewardMean': 0.8087749239428186, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.8589002494282, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=78.36
