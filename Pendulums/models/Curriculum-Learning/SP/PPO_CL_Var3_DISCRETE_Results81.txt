#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 81
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6324348664108064, 'errorList': [], 'lossList': [0.0, -1.4222406631708144, 0.0, 26.879743020534516, 0.0, 0.0, 0.0], 'rewardMean': 0.7978622091607581, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.81111973182212
'totalSteps': 3840, 'rewardStep': 0.8403693505209163, 'errorList': [], 'lossList': [0.0, -1.4242901980876923, 0.0, 22.455256407260894, 0.0, 0.0, 0.0], 'rewardMean': 0.8120312562808109, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 255.20613735561352
'totalSteps': 5120, 'rewardStep': 0.7010127482662514, 'errorList': [], 'lossList': [0.0, -1.4249479389190673, 0.0, 16.55255002617836, 0.0, 0.0, 0.0], 'rewardMean': 0.784276629277171, 'totalEpisodes': 23, 'stepsPerEpisode': 72, 'rewardPerEpisode': 50.69641083423738
'totalSteps': 6400, 'rewardStep': 0.7043212459844532, 'errorList': [], 'lossList': [0.0, -1.415982192158699, 0.0, 20.11273698925972, 0.0, 0.0, 0.0], 'rewardMean': 0.7682855526186275, 'totalEpisodes': 24, 'stepsPerEpisode': 824, 'rewardPerEpisode': 585.8677893159934
'totalSteps': 7680, 'rewardStep': 0.6444584355766487, 'errorList': [], 'lossList': [0.0, -1.391428725719452, 0.0, 12.84353662520647, 0.0, 0.0, 0.0], 'rewardMean': 0.7476476997782977, 'totalEpisodes': 24, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1012.6775125649676
'totalSteps': 8960, 'rewardStep': 0.9541971110332693, 'errorList': [188.97662249428367, 154.2680523087532, 186.7393465608088, 182.39183894446813, 195.23596131316444, 169.03666689469077, 179.86032132465792, 189.41482534475435, 177.68020889661275, 193.54311019335782, 176.9264992704989, 168.6143853285538, 167.10280631390893, 180.22993831187398, 182.08522006416257, 190.80618777452722, 189.59542487017632, 169.30103942542115, 199.556166445294, 134.18590163713674, 186.6514131520198, 176.4470812759385, 161.59181314722986, 179.8769230704322, 193.54121839517123, 150.5657309805648, 192.91292263657024, 169.00979069996612, 189.02829973589442, 139.57011461877656, 196.24452743081054, 197.36205139496798, 190.84030893625598, 196.39916939974765, 191.263526738598, 194.53338258212509, 192.06584814813755, 190.78733112066334, 197.62057706904758, 165.17815542920997, 165.71757441774045, 198.21449286100847, 181.29132296066635, 198.78345223578359, 174.44263873784922, 187.08528702794877, 191.9698754967635, 140.5423563530151, 190.69254650223326, 184.1167698088331], 'lossList': [0.0, -1.3609222477674485, 0.0, 10.278241114616394, 0.0, 0.0, 0.0], 'rewardMean': 0.777154758529008, 'totalEpisodes': 24, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.9017435778014, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.8328841134538016, 'errorList': [], 'lossList': [0.0, -1.3473672968149186, 0.0, 390.49501525878907, 0.0, 0.0, 0.0], 'rewardMean': 0.7841209278946071, 'totalEpisodes': 58, 'stepsPerEpisode': 86, 'rewardPerEpisode': 72.50585338112862
'totalSteps': 11520, 'rewardStep': 0.9189140520645875, 'errorList': [], 'lossList': [0.0, -1.3464015066623687, 0.0, 106.74830753326415, 0.0, 0.0, 0.0], 'rewardMean': 0.7990979416912716, 'totalEpisodes': 88, 'stepsPerEpisode': 41, 'rewardPerEpisode': 33.812637091340704
'totalSteps': 12800, 'rewardStep': 0.9591377346536902, 'errorList': [249.71709856953498, 253.71869417166442, 258.8805576568792, 255.3049201288233, 251.656923993743, 271.7378885196809, 269.3056087458145, 266.4525677003747, 213.81162701484493, 265.36930840421235, 272.00814427506384, 211.9432875984553, 292.27107144081873, 211.404826623213, 229.11038575419778, 241.5590649480893, 216.5945627837233, 244.9390971621258, 87.95901602116027, 261.9493103755624, 178.97442945809473, 240.84817631964074, 286.1325384194651, 244.70870644412673, 253.23768619949294, 238.7083944206502, 263.5888311538309, 283.3387547070656, 195.4050089294312, 255.23900146081203, 258.4970914663381, 268.51021300900794, 247.26836274580864, 277.0730698647254, 293.0149970083196, 270.1045595577959, 278.17286626444604, 167.13194552904068, 276.59109588984467, 272.315006051188, 185.64935344705924, 282.7325233673126, 138.96981256338077, 233.471467787776, 262.6444254497081, 241.6127430366938, 268.24889888112364, 134.02864141568563, 296.2078327030624, 279.57263719523274], 'lossList': [0.0, -1.3417501878738403, 0.0, 51.13206512451172, 0.0, 0.0, 0.0], 'rewardMean': 0.8151019209875134, 'totalEpisodes': 120, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.728872250007932, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.5463682236856433, 'errorList': [], 'lossList': [0.0, -1.3233176958560944, 0.0, 36.116007528305055, 0.0, 0.0, 0.0], 'rewardMean': 0.7734097881650068, 'totalEpisodes': 139, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.06241713136635
'totalSteps': 15360, 'rewardStep': 0.29083866145709425, 'errorList': [], 'lossList': [0.0, -1.305441370010376, 0.0, 16.916885747909546, 0.0, 0.0, 0.0], 'rewardMean': 0.7392501676696356, 'totalEpisodes': 148, 'stepsPerEpisode': 123, 'rewardPerEpisode': 86.97214790989253
'totalSteps': 16640, 'rewardStep': 0.7999701277039403, 'errorList': [], 'lossList': [0.0, -1.2834722220897674, 0.0, 19.35897166490555, 0.0, 0.0, 0.0], 'rewardMean': 0.735210245387938, 'totalEpisodes': 156, 'stepsPerEpisode': 73, 'rewardPerEpisode': 65.21335013541757
'totalSteps': 17920, 'rewardStep': 0.8897633375468735, 'errorList': [], 'lossList': [0.0, -1.2636569398641586, 0.0, 8.57099969804287, 0.0, 0.0, 0.0], 'rewardMean': 0.7540853043160001, 'totalEpisodes': 160, 'stepsPerEpisode': 101, 'rewardPerEpisode': 87.4482078723284
'totalSteps': 19200, 'rewardStep': 0.8256855303781395, 'errorList': [], 'lossList': [0.0, -1.2435200440883636, 0.0, 7.64227811574936, 0.0, 0.0, 0.0], 'rewardMean': 0.7662217327553689, 'totalEpisodes': 166, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.59297762025435
'totalSteps': 20480, 'rewardStep': 0.3942755548493794, 'errorList': [], 'lossList': [0.0, -1.2380718165636062, 0.0, 5.687689525485038, 0.0, 0.0, 0.0], 'rewardMean': 0.7412034446826419, 'totalEpisodes': 169, 'stepsPerEpisode': 220, 'rewardPerEpisode': 178.009056957211
'totalSteps': 21760, 'rewardStep': 0.578458220485842, 'errorList': [], 'lossList': [0.0, -1.2275748813152314, 0.0, 4.2871054792404175, 0.0, 0.0, 0.0], 'rewardMean': 0.7036295556278992, 'totalEpisodes': 171, 'stepsPerEpisode': 515, 'rewardPerEpisode': 431.37677303318117
'totalSteps': 23040, 'rewardStep': 0.6786484124313272, 'errorList': [], 'lossList': [0.0, -1.2078864872455597, 0.0, 4.500287300944328, 0.0, 0.0, 0.0], 'rewardMean': 0.6882059855256518, 'totalEpisodes': 172, 'stepsPerEpisode': 458, 'rewardPerEpisode': 349.10189283394925
'totalSteps': 24320, 'rewardStep': 0.8446361295424493, 'errorList': [], 'lossList': [0.0, -1.1728458660840988, 0.0, 2.2803519481420516, 0.0, 0.0, 0.0], 'rewardMean': 0.6807781932734379, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1038.6514429655645
'totalSteps': 25600, 'rewardStep': 0.849763019378104, 'errorList': [], 'lossList': [0.0, -1.1077819186449052, 0.0, 1.7908544105291366, 0.0, 0.0, 0.0], 'rewardMean': 0.6698407217458792, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.7824642469352
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=101.79
