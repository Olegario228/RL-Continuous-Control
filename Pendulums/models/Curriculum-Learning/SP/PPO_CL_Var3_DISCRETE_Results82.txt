#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 82
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.6024665948879426, 'errorList': [], 'lossList': [0.0, -1.4165354841947555, 0.0, 23.93550584554672, 0.0, 0.0, 0.0], 'rewardMean': 0.6981182980340469, 'totalEpisodes': 20, 'stepsPerEpisode': 134, 'rewardPerEpisode': 81.29176411951762
'totalSteps': 3840, 'rewardStep': 0.7476596497586331, 'errorList': [], 'lossList': [0.0, -1.4073269003629685, 0.0, 25.12256549358368, 0.0, 0.0, 0.0], 'rewardMean': 0.7146320819422423, 'totalEpisodes': 23, 'stepsPerEpisode': 491, 'rewardPerEpisode': 360.6189836227023
'totalSteps': 5120, 'rewardStep': 0.8392984180292674, 'errorList': [], 'lossList': [0.0, -1.3948573023080826, 0.0, 19.90495609998703, 0.0, 0.0, 0.0], 'rewardMean': 0.7457986659639986, 'totalEpisodes': 27, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.942805791393013
'totalSteps': 6400, 'rewardStep': 0.16544997743437073, 'errorList': [], 'lossList': [0.0, -1.396834184527397, 0.0, 32.123555212020875, 0.0, 0.0, 0.0], 'rewardMean': 0.6297289282580729, 'totalEpisodes': 29, 'stepsPerEpisode': 558, 'rewardPerEpisode': 389.36792736240625
'totalSteps': 7680, 'rewardStep': 0.6943710165787682, 'errorList': [], 'lossList': [0.0, -1.393035066127777, 0.0, 15.24554249703884, 0.0, 0.0, 0.0], 'rewardMean': 0.6405026096448555, 'totalEpisodes': 29, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.6548360624729
'totalSteps': 8960, 'rewardStep': 0.8917068343024768, 'errorList': [], 'lossList': [0.0, -1.4008522897958755, 0.0, 10.7542341786623, 0.0, 0.0, 0.0], 'rewardMean': 0.6763889274530871, 'totalEpisodes': 29, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.4722960039269
'totalSteps': 10240, 'rewardStep': 0.566170465167009, 'errorList': [], 'lossList': [0.0, -1.3818743979930879, 0.0, 384.77247581481936, 0.0, 0.0, 0.0], 'rewardMean': 0.6626116196673274, 'totalEpisodes': 63, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.115260827841217
'totalSteps': 11520, 'rewardStep': 0.9366804107926101, 'errorList': [245.82108641466024, 228.34647018150804, 251.01096212380023, 216.41114875033108, 200.6624160477097, 220.49452456380766, 220.9113401338203, 212.78991037040814, 243.96628241887922, 211.6220733883952, 224.70694438815917, 231.4658229809599, 240.8542214450196, 215.19625331239686, 235.1009167969504, 201.32336146888196, 231.44964560612385, 240.2425293422339, 236.03822388382707, 195.14827809746095, 230.2821715719057, 175.1315616539064, 229.78010424747353, 242.26139072354823, 172.26460895355115, 244.61486319843272, 242.63719300193463, 129.8483853074781, 246.5669509426178, 248.92651242351664, 233.76363070917773, 204.70531961294674, 245.29761836537645, 248.9616455258769, 203.97943519224884, 243.72133430735227, 228.42421441246188, 248.860234340987, 227.0351029301033, 243.77083059035732, 221.65895493691283, 252.49248261565796, 223.09002664599942, 239.90855076229101, 213.32364576474714, 176.96300921412592, 222.2979345049055, 233.24581360300124, 235.10149318786952, 217.93959797504252], 'lossList': [0.0, -1.3802785181999206, 0.0, 111.38054092407226, 0.0, 0.0, 0.0], 'rewardMean': 0.6930637075701367, 'totalEpisodes': 98, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.91440707034893, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.5450289780164925, 'errorList': [], 'lossList': [0.0, -1.3806518179178238, 0.0, 43.555427656173705, 0.0, 0.0, 0.0], 'rewardMean': 0.6782602346147723, 'totalEpisodes': 125, 'stepsPerEpisode': 57, 'rewardPerEpisode': 41.96129720239523
'totalSteps': 14080, 'rewardStep': 0.4774123525091916, 'errorList': [], 'lossList': [0.0, -1.379362341761589, 0.0, 21.614596362113954, 0.0, 0.0, 0.0], 'rewardMean': 0.6466244697476763, 'totalEpisodes': 147, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.04188567348485
'totalSteps': 15360, 'rewardStep': 0.8565745202984443, 'errorList': [], 'lossList': [0.0, -1.3771342170238494, 0.0, 26.89435996055603, 0.0, 0.0, 0.0], 'rewardMean': 0.6720352622887263, 'totalEpisodes': 160, 'stepsPerEpisode': 36, 'rewardPerEpisode': 29.41546080551705
'totalSteps': 16640, 'rewardStep': 0.6198299231974873, 'errorList': [], 'lossList': [0.0, -1.3719900667667388, 0.0, 9.949324251413346, 0.0, 0.0, 0.0], 'rewardMean': 0.6592522896326118, 'totalEpisodes': 165, 'stepsPerEpisode': 121, 'rewardPerEpisode': 93.5624665604384
'totalSteps': 17920, 'rewardStep': 0.7677021868593699, 'errorList': [], 'lossList': [0.0, -1.3673268908262253, 0.0, 17.008917028903962, 0.0, 0.0, 0.0], 'rewardMean': 0.652092666515622, 'totalEpisodes': 170, 'stepsPerEpisode': 225, 'rewardPerEpisode': 195.1693656755058
'totalSteps': 19200, 'rewardStep': 0.8690221815727504, 'errorList': [], 'lossList': [0.0, -1.3609794038534164, 0.0, 7.425915640592575, 0.0, 0.0, 0.0], 'rewardMean': 0.72244988692946, 'totalEpisodes': 173, 'stepsPerEpisode': 105, 'rewardPerEpisode': 89.7060426588243
'totalSteps': 20480, 'rewardStep': 0.7477315493286947, 'errorList': [], 'lossList': [0.0, -1.345008749961853, 0.0, 17.158265899419785, 0.0, 0.0, 0.0], 'rewardMean': 0.7277859402044526, 'totalEpisodes': 175, 'stepsPerEpisode': 442, 'rewardPerEpisode': 373.8874822637513
'totalSteps': 21760, 'rewardStep': 0.7102033896926172, 'errorList': [], 'lossList': [0.0, -1.321605595946312, 0.0, 8.613203135728837, 0.0, 0.0, 0.0], 'rewardMean': 0.7096355957434666, 'totalEpisodes': 178, 'stepsPerEpisode': 282, 'rewardPerEpisode': 223.58917309562318
'totalSteps': 23040, 'rewardStep': 0.9329338613375108, 'errorList': [17.840316547441212, 3.0439935308317665, 9.985727616096552, 7.284327093234767, 2.1050634222521185, 44.54632041969592, 2.191581481253023, 33.00317633464692, 54.83640180780234, 2.893285072410219, 52.08434721497347, 8.741568628898502, 25.431518377007855, 26.86316164899711, 4.804887270154559, 9.404224322818227, 8.860989572535164, 13.89737743064216, 6.543603471948163, 21.494358627996284, 72.02831483289695, 44.77102635196174, 15.489002252336904, 33.52716032521109, 23.47468186827545, 17.928156308152015, 16.03791473748278, 28.682121627074864, 3.497791277300971, 11.553792417160622, 15.533590142801438, 34.66146956009969, 15.668232395073174, 12.52491975315596, 71.01034354747806, 26.043650010314277, 71.95950208063043, 14.00546690547581, 5.529591178447751, 42.97495126799984, 3.0930167090447944, 2.8290969567005178, 45.42094866698669, 1.116658960757215, 2.153177921631813, 8.712191766405027, 14.962064271620374, 10.636929233420792, 36.604841007989506, 49.790828850325084], 'lossList': [0.0, -1.308993811607361, 0.0, 2.535555808246136, 0.0, 0.0, 0.0], 'rewardMean': 0.746311935360517, 'totalEpisodes': 181, 'stepsPerEpisode': 531, 'rewardPerEpisode': 457.5766876029691, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.8952846239185311, 'errorList': [], 'lossList': [0.0, -1.304947099685669, 0.0, 2.016984919309616, 0.0, 0.0, 0.0], 'rewardMean': 0.742172356673109, 'totalEpisodes': 186, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.5689227213213446
'totalSteps': 25600, 'rewardStep': 0.7950988175083888, 'errorList': [], 'lossList': [0.0, -1.2966210621595382, 0.0, 2.6501276382803916, 0.0, 0.0, 0.0], 'rewardMean': 0.7671793406222986, 'totalEpisodes': 187, 'stepsPerEpisode': 336, 'rewardPerEpisode': 281.42304458440054
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=104.58
