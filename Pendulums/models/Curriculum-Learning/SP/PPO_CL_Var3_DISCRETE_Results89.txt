#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 89
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8691065996348363, 'errorList': [], 'lossList': [0.0, -1.4240562045574188, 0.0, 30.64595845937729, 0.0, 0.0, 0.0], 'rewardMean': 0.7705110028890898, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 252.45415072774313
'totalSteps': 3840, 'rewardStep': 0.7734360832441656, 'errorList': [], 'lossList': [0.0, -1.4417775994539261, 0.0, 30.45178715825081, 0.0, 0.0, 0.0], 'rewardMean': 0.7714860296741151, 'totalEpisodes': 15, 'stepsPerEpisode': 168, 'rewardPerEpisode': 126.71211759057093
'totalSteps': 5120, 'rewardStep': 0.6133946466983342, 'errorList': [], 'lossList': [0.0, -1.4452192854881287, 0.0, 23.043071581721307, 0.0, 0.0, 0.0], 'rewardMean': 0.7319631839301699, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 957.1308145624366
'totalSteps': 6400, 'rewardStep': 0.8500092483935022, 'errorList': [], 'lossList': [0.0, -1.4314566057920457, 0.0, 19.500214830636978, 0.0, 0.0, 0.0], 'rewardMean': 0.7555723968228364, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.3355867835556
'totalSteps': 7680, 'rewardStep': 0.9087090377667698, 'errorList': [], 'lossList': [0.0, -1.431219025850296, 0.0, 12.907236521840096, 0.0, 0.0, 0.0], 'rewardMean': 0.781095170313492, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.1900096331794
'totalSteps': 8960, 'rewardStep': 0.6558325911991193, 'errorList': [], 'lossList': [0.0, -1.4121973752975463, 0.0, 23.715955864191056, 0.0, 0.0, 0.0], 'rewardMean': 0.7632005161542958, 'totalEpisodes': 16, 'stepsPerEpisode': 1124, 'rewardPerEpisode': 881.5230142420917
'totalSteps': 10240, 'rewardStep': 0.5865563151054185, 'errorList': [], 'lossList': [0.0, -1.4044135463237764, 0.0, 571.9232832336426, 0.0, 0.0, 0.0], 'rewardMean': 0.7411199910231862, 'totalEpisodes': 62, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.149037131099952
'totalSteps': 11520, 'rewardStep': 0.6849663183266558, 'errorList': [], 'lossList': [0.0, -1.4033023911714553, 0.0, 232.8020890045166, 0.0, 0.0, 0.0], 'rewardMean': 0.734880694056905, 'totalEpisodes': 108, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.01224565678231
'totalSteps': 12800, 'rewardStep': 0.5138479707283768, 'errorList': [], 'lossList': [0.0, -1.4006761342287064, 0.0, 114.66998184204101, 0.0, 0.0, 0.0], 'rewardMean': 0.7127774217240522, 'totalEpisodes': 144, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.484420790590953
'totalSteps': 14080, 'rewardStep': 0.9266544628459074, 'errorList': [], 'lossList': [0.0, -1.392979184985161, 0.0, 71.37888233184815, 0.0, 0.0, 0.0], 'rewardMean': 0.7382513273943087, 'totalEpisodes': 177, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.141098911323096
'totalSteps': 15360, 'rewardStep': 0.9663298923424609, 'errorList': [143.3877059014549, 187.5813019995416, 198.08008959916236, 185.4522613703323, 151.5165029605343, 171.93324350461592, 182.57987494295878, 184.4119973238114, 197.90065225065754, 124.86455186459526, 133.1062986444864, 114.84380339432946, 154.27032579106051, 111.48049878161736, 168.70677114092152, 198.33343242417033, 172.8257163274211, 166.87455343951612, 180.7209656684581, 184.38121714357032, 189.96151411582173, 80.56400921319941, 88.78519155478014, 159.28506673276945, 199.27572190620452, 182.53752712121914, 156.59470167648408, 149.83745341213526, 199.08971935694672, 191.3742814835816, 197.96951728100058, 174.09771437281407, 195.41470602635957, 94.22193709999091, 186.9796240977198, 122.60473134541877, 167.78615394530456, 194.6364037592095, 196.44794960538033, 188.0089344321562, 137.85512210219824, 189.05841916017405, 200.25622710766234, 193.60941163895626, 56.38261060054577, 161.73778046472708, 196.66878796080718, 52.54890706316704, 148.9339287579458, 181.05015116793672], 'lossList': [0.0, -1.3867649567127227, 0.0, 51.61748527526856, 0.0, 0.0, 0.0], 'rewardMean': 0.747973656665071, 'totalEpisodes': 196, 'stepsPerEpisode': 42, 'rewardPerEpisode': 36.657179460861066, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5966022678465067, 'errorList': [], 'lossList': [0.0, -1.3805468505620957, 0.0, 43.3800028514862, 0.0, 0.0, 0.0], 'rewardMean': 0.7302902751253052, 'totalEpisodes': 210, 'stepsPerEpisode': 80, 'rewardPerEpisode': 56.31203845873094
'totalSteps': 17920, 'rewardStep': 0.6627670542955564, 'errorList': [], 'lossList': [0.0, -1.3631932950019836, 0.0, 18.247275099754333, 0.0, 0.0, 0.0], 'rewardMean': 0.7352275158850273, 'totalEpisodes': 220, 'stepsPerEpisode': 81, 'rewardPerEpisode': 67.63203743334573
'totalSteps': 19200, 'rewardStep': 0.9284990523978804, 'errorList': [], 'lossList': [0.0, -1.3364925521612168, 0.0, 29.819320163726807, 0.0, 0.0, 0.0], 'rewardMean': 0.7430764962854652, 'totalEpisodes': 229, 'stepsPerEpisode': 79, 'rewardPerEpisode': 68.51760228635379
'totalSteps': 20480, 'rewardStep': 0.5987190196371848, 'errorList': [], 'lossList': [0.0, -1.3240868139266968, 0.0, 9.223510906696319, 0.0, 0.0, 0.0], 'rewardMean': 0.7120774944725067, 'totalEpisodes': 234, 'stepsPerEpisode': 113, 'rewardPerEpisode': 74.52919000080517
'totalSteps': 21760, 'rewardStep': 0.7498843777057882, 'errorList': [], 'lossList': [0.0, -1.323528497815132, 0.0, 21.129300870895385, 0.0, 0.0, 0.0], 'rewardMean': 0.7214826731231735, 'totalEpisodes': 239, 'stepsPerEpisode': 73, 'rewardPerEpisode': 62.4839178314086
'totalSteps': 23040, 'rewardStep': 0.8060597881702359, 'errorList': [], 'lossList': [0.0, -1.3179720884561539, 0.0, 25.844571603536608, 0.0, 0.0, 0.0], 'rewardMean': 0.7434330204296553, 'totalEpisodes': 242, 'stepsPerEpisode': 310, 'rewardPerEpisode': 258.3840330855528
'totalSteps': 24320, 'rewardStep': 0.48366845425633465, 'errorList': [], 'lossList': [0.0, -1.311387522816658, 0.0, 3.6672004705667494, 0.0, 0.0, 0.0], 'rewardMean': 0.7233032340226232, 'totalEpisodes': 245, 'stepsPerEpisode': 301, 'rewardPerEpisode': 235.64694608820676
'totalSteps': 25600, 'rewardStep': 0.9809961482966576, 'errorList': [0.20173687656560355, 0.17951317150340812, 0.16920306965491855, 0.1900197068694025, 0.1945957513084804, 0.18166826390424046, 0.16060556384813385, 0.20951975185967692, 0.1824946037461529, 0.16498790960914664, 0.17495997391374968, 0.2090988726903303, 0.20656716474255155, 0.16523277918157403, 0.15568479668327828, 0.16225394809761975, 0.19129659868618287, 0.21476878886409462, 0.19067863097768972, 0.17388582196631097, 0.16634753137544628, 0.1811878782646762, 0.1683762329434262, 0.18431772545839273, 0.15976071093278724, 0.19891474367255627, 0.2069664247628886, 0.16717592294895062, 0.1775847734682902, 0.19961635043882964, 0.16518917108142658, 0.1959215307290887, 0.17868630895391208, 0.20231911767990285, 0.19311941565427376, 0.17341392926333818, 0.19722152433988208, 0.2113427399171191, 0.19925901715198155, 0.18754149726293762, 0.18800722674416093, 0.239216606136567, 0.1719753750843314, 0.1934133185443821, 0.1805233304678508, 0.1609441515363166, 0.19927851511844893, 0.19631498169217923, 0.18816654069279312, 0.15564793955603357], 'lossList': [0.0, -1.3016084760427475, 0.0, 3.4708300110697747, 0.0, 0.0, 0.0], 'rewardMean': 0.7700180517794514, 'totalEpisodes': 247, 'stepsPerEpisode': 536, 'rewardPerEpisode': 471.22607632866954, 'successfulTests': 41
#maxSuccessfulTests=41, maxSuccessfulTestsAtStep=25600, timeSpent=102.76
