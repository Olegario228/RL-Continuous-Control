#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 9
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7930420769534675, 'errorList': [], 'lossList': [0.0, -1.4114374756813048, 0.0, 23.55243757247925, 0.0, 0.0, 0.0], 'rewardMean': 0.578556091364528, 'totalEpisodes': 20, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.512210118521697
'totalSteps': 3840, 'rewardStep': 0.46685752540477443, 'errorList': [], 'lossList': [0.0, -1.422312022447586, 0.0, 22.715906591415404, 0.0, 0.0, 0.0], 'rewardMean': 0.5413232360446102, 'totalEpisodes': 27, 'stepsPerEpisode': 113, 'rewardPerEpisode': 72.55741603431655
'totalSteps': 5120, 'rewardStep': 0.623695989080709, 'errorList': [], 'lossList': [0.0, -1.432346613407135, 0.0, 19.172786490917204, 0.0, 0.0, 0.0], 'rewardMean': 0.5619164243036349, 'totalEpisodes': 31, 'stepsPerEpisode': 208, 'rewardPerEpisode': 168.83040591018786
'totalSteps': 6400, 'rewardStep': 0.8573584168174677, 'errorList': [], 'lossList': [0.0, -1.4381928557157517, 0.0, 188.23742462158202, 0.0, 0.0, 0.0], 'rewardMean': 0.6210048228064015, 'totalEpisodes': 89, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.63246720928328
'totalSteps': 7680, 'rewardStep': 0.8151807410201156, 'errorList': [], 'lossList': [0.0, -1.4226584261655808, 0.0, 93.11475000381469, 0.0, 0.0, 0.0], 'rewardMean': 0.6533674758420205, 'totalEpisodes': 137, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.6715778918401423
'totalSteps': 8960, 'rewardStep': 0.5270715093474126, 'errorList': [], 'lossList': [0.0, -1.4100326544046402, 0.0, 66.04725631713868, 0.0, 0.0, 0.0], 'rewardMean': 0.6353251949142195, 'totalEpisodes': 171, 'stepsPerEpisode': 25, 'rewardPerEpisode': 17.901869128632317
'totalSteps': 10240, 'rewardStep': 0.9465449567960255, 'errorList': [99.19826358743944, 80.18118934284696, 119.5299019579907, 124.26627513457866, 4.553924914950494, 84.1960669877068, 93.3712745087445, 141.878772802123, 104.51636104647187, 65.62769083588411, 40.69300017164052, 100.91686723701946, 13.075282233765781, 4.142766878807033, 73.61387252409493, 7.672322723819272, 49.73440268881663, 63.847336354495866, 88.66810314297506, 92.69061038146033, 112.78848753617316, 130.25859238387307, 62.31653118486052, 116.77856970539297, 46.87753037290575, 92.28771115313754, 23.54721912146514, 0.4928401431880639, 53.54939221351894, 97.31798192332518, 135.57716310629237, 7.568945247004847, 64.9891809731951, 137.41501672236942, 135.15364483164035, 86.36512440473092, 54.29119610637481, 99.42197301217989, 78.79097039181333, 115.70588352813503, 76.30335906199647, 117.19600920577857, 117.68875969180998, 123.293771022253, 92.76189192028633, 85.586818237661, 76.60056759048145, 117.89423146213254, 7.76516198293721, 106.17775791732912], 'lossList': [0.0, -1.4034371888637542, 0.0, 35.09156199455261, 0.0, 0.0, 0.0], 'rewardMean': 0.6742276651494452, 'totalEpisodes': 185, 'stepsPerEpisode': 101, 'rewardPerEpisode': 80.8408682056318, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.43382340111186873, 'errorList': [], 'lossList': [0.0, -1.4025552350282668, 0.0, 17.607216705083847, 0.0, 0.0, 0.0], 'rewardMean': 0.6475160802563811, 'totalEpisodes': 190, 'stepsPerEpisode': 258, 'rewardPerEpisode': 204.83602362003617
'totalSteps': 12800, 'rewardStep': 0.8888919818625862, 'errorList': [], 'lossList': [0.0, -1.400911250114441, 0.0, 18.261218633651733, 0.0, 0.0, 0.0], 'rewardMean': 0.6716536704170016, 'totalEpisodes': 199, 'stepsPerEpisode': 65, 'rewardPerEpisode': 58.86233010406565
'totalSteps': 14080, 'rewardStep': 0.8877951031712051, 'errorList': [], 'lossList': [0.0, -1.399539657831192, 0.0, 8.538560560941697, 0.0, 0.0, 0.0], 'rewardMean': 0.7240261701565632, 'totalEpisodes': 204, 'stepsPerEpisode': 71, 'rewardPerEpisode': 63.05498720516532
'totalSteps': 15360, 'rewardStep': 0.8890561303292156, 'errorList': [], 'lossList': [0.0, -1.4072203880548477, 0.0, 19.207184178829195, 0.0, 0.0, 0.0], 'rewardMean': 0.7336275754941382, 'totalEpisodes': 210, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.290872123419083
'totalSteps': 16640, 'rewardStep': 0.6427703604231871, 'errorList': [], 'lossList': [0.0, -1.3925312620401382, 0.0, 17.98599637031555, 0.0, 0.0, 0.0], 'rewardMean': 0.7512188589959793, 'totalEpisodes': 216, 'stepsPerEpisode': 165, 'rewardPerEpisode': 144.49307569952856
'totalSteps': 17920, 'rewardStep': 0.2855246538902578, 'errorList': [], 'lossList': [0.0, -1.3737158197164536, 0.0, 4.36183987557888, 0.0, 0.0, 0.0], 'rewardMean': 0.7174017254769343, 'totalEpisodes': 220, 'stepsPerEpisode': 153, 'rewardPerEpisode': 105.98125525295572
'totalSteps': 19200, 'rewardStep': 0.780488810897694, 'errorList': [], 'lossList': [0.0, -1.3740440499782562, 0.0, 4.422117419242859, 0.0, 0.0, 0.0], 'rewardMean': 0.7097147648849569, 'totalEpisodes': 225, 'stepsPerEpisode': 86, 'rewardPerEpisode': 72.46003492128476
'totalSteps': 20480, 'rewardStep': 0.9478218703156416, 'errorList': [0.7166233806822591, 0.6948851651431732, 3.062212531196676, 4.338964026397163, 0.8434854952093144, 3.64410551415523, 1.984966685369223, 6.740193778449589, 1.7853164291098407, 2.7195179400126737, 5.662769044390066, 0.543961896441328, 1.0056068149999315, 6.016637668590693, 1.9697952958716194, 3.260166158971666, 0.4397380593434709, 2.715884817928718, 0.5656123892481413, 1.4602710817000568, 1.6677537435047738, 0.7037347176039837, 3.388256749811959, 7.42367165924115, 1.9699765594602558, 4.661519411348911, 1.6255034861181108, 2.7257966578066064, 7.8888878292597076, 2.297435237890146, 4.312850909563199, 1.2282004452221482, 2.42137076418021, 0.48245474774118846, 1.9559824738306815, 0.4456664688530981, 1.783012395666226, 5.710907842013597, 5.207602523672031, 4.962055910580348, 3.99267108003373, 1.873122079930625, 4.156512054961422, 0.015352973517846145, 5.009931511938971, 1.4863531685988085, 1.3656708201977537, 4.067122593077009, 0.6335548624987938, 0.9513916228334052], 'lossList': [0.0, -1.3842088025808335, 0.0, 3.1910721278190612, 0.0, 0.0, 0.0], 'rewardMean': 0.7229788778145094, 'totalEpisodes': 228, 'stepsPerEpisode': 88, 'rewardPerEpisode': 75.01679288495234, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.8212272284164189, 'errorList': [], 'lossList': [0.0, -1.3879420202970505, 0.0, 3.3764094626903534, 0.0, 0.0, 0.0], 'rewardMean': 0.7523944497214101, 'totalEpisodes': 230, 'stepsPerEpisode': 71, 'rewardPerEpisode': 64.76329665932123
'totalSteps': 23040, 'rewardStep': 0.8924528068850645, 'errorList': [], 'lossList': [0.0, -1.3837869065999984, 0.0, 2.7158729457855224, 0.0, 0.0, 0.0], 'rewardMean': 0.7469852347303139, 'totalEpisodes': 233, 'stepsPerEpisode': 170, 'rewardPerEpisode': 147.91755293726914
'totalSteps': 24320, 'rewardStep': 0.7027391029241015, 'errorList': [], 'lossList': [0.0, -1.36632938683033, 0.0, 2.03208193808794, 0.0, 0.0, 0.0], 'rewardMean': 0.7738768049115372, 'totalEpisodes': 236, 'stepsPerEpisode': 313, 'rewardPerEpisode': 280.0597255365388
'totalSteps': 25600, 'rewardStep': 0.8526857940500898, 'errorList': [], 'lossList': [0.0, -1.3351760917901994, 0.0, 1.3717775420844556, 0.0, 0.0, 0.0], 'rewardMean': 0.7702561861302876, 'totalEpisodes': 240, 'stepsPerEpisode': 168, 'rewardPerEpisode': 146.39074411275053
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=20480, timeSpent=96.75
