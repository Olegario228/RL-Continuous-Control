#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 90
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9360084744127758, 'errorList': [], 'lossList': [0.0, -1.4352493596076965, 0.0, 30.507024736404418, 0.0, 0.0, 0.0], 'rewardMean': 0.9157950328104714, 'totalEpisodes': 8, 'stepsPerEpisode': 529, 'rewardPerEpisode': 377.10779752439424
'totalSteps': 3840, 'rewardStep': 0.7557452886750211, 'errorList': [], 'lossList': [0.0, -1.4207161974906921, 0.0, 32.01956871271133, 0.0, 0.0, 0.0], 'rewardMean': 0.8624451180986545, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 203.96302985448395
'totalSteps': 5120, 'rewardStep': 0.7637633211487607, 'errorList': [], 'lossList': [0.0, -1.4165323793888092, 0.0, 27.78206905066967, 0.0, 0.0, 0.0], 'rewardMean': 0.8377746688611811, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.0989296707711
'totalSteps': 6400, 'rewardStep': 0.9641597693966607, 'errorList': [], 'lossList': [0.0, -1.4044303733110428, 0.0, 23.92006703555584, 0.0, 0.0, 0.0], 'rewardMean': 0.863051688968277, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.5310283831257
'totalSteps': 7680, 'rewardStep': 0.7471233495034122, 'errorList': [], 'lossList': [0.0, -1.3953341794013978, 0.0, 17.48394833922386, 0.0, 0.0, 0.0], 'rewardMean': 0.8437302990574662, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.6654996091224
'totalSteps': 8960, 'rewardStep': 0.9130543384222685, 'errorList': [], 'lossList': [0.0, -1.3636931151151657, 0.0, 10.636468620598317, 0.0, 0.0, 0.0], 'rewardMean': 0.853633733252438, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.709398165792
'totalSteps': 10240, 'rewardStep': 0.4346083584534846, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.760516983297115, 'totalEpisodes': 69, 'stepsPerEpisode': 13, 'rewardPerEpisode': 7.636534606059395
'totalSteps': 11520, 'rewardStep': 0.8495073957426605, 'errorList': [], 'lossList': [0.0, -1.3653582692146302, 0.0, 764.3593650817871, 0.0, 0.0, 0.0], 'rewardMean': 0.7694160245416696, 'totalEpisodes': 125, 'stepsPerEpisode': 32, 'rewardPerEpisode': 29.762276851252796
'totalSteps': 12800, 'rewardStep': 0.7077215480851139, 'errorList': [], 'lossList': [0.0, -1.3644831889867783, 0.0, 604.8379257202148, 0.0, 0.0, 0.0], 'rewardMean': 0.7506300202293643, 'totalEpisodes': 186, 'stepsPerEpisode': 66, 'rewardPerEpisode': 55.77502245874894
'totalSteps': 14080, 'rewardStep': 0.9185052662475133, 'errorList': [], 'lossList': [0.0, -1.3645039677619935, 0.0, 229.46878227233887, 0.0, 0.0, 0.0], 'rewardMean': 0.748879699412838, 'totalEpisodes': 249, 'stepsPerEpisode': 23, 'rewardPerEpisode': 18.859487039870082
'totalSteps': 15360, 'rewardStep': 0.5377802864562803, 'errorList': [], 'lossList': [0.0, -1.3642060631513595, 0.0, 84.67266653060913, 0.0, 0.0, 0.0], 'rewardMean': 0.727083199190964, 'totalEpisodes': 291, 'stepsPerEpisode': 42, 'rewardPerEpisode': 26.104341217630097
'totalSteps': 16640, 'rewardStep': 0.5663359331084229, 'errorList': [], 'lossList': [0.0, -1.3622220611572267, 0.0, 49.854210977554324, 0.0, 0.0, 0.0], 'rewardMean': 0.7073404603869302, 'totalEpisodes': 323, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.440949693176375
'totalSteps': 17920, 'rewardStep': 0.4854789538315919, 'errorList': [], 'lossList': [0.0, -1.3618376743793488, 0.0, 29.088488054275512, 0.0, 0.0, 0.0], 'rewardMean': 0.6594723788304233, 'totalEpisodes': 342, 'stepsPerEpisode': 77, 'rewardPerEpisode': 52.10940059558387
'totalSteps': 19200, 'rewardStep': 0.6178060989777701, 'errorList': [], 'lossList': [0.0, -1.3574262487888336, 0.0, 33.03880157470703, 0.0, 0.0, 0.0], 'rewardMean': 0.6465406537778591, 'totalEpisodes': 355, 'stepsPerEpisode': 168, 'rewardPerEpisode': 125.52911988624632
'totalSteps': 20480, 'rewardStep': 0.8747146060395873, 'errorList': [], 'lossList': [0.0, -1.3465051770210266, 0.0, 27.373459072113036, 0.0, 0.0, 0.0], 'rewardMean': 0.6427066805395909, 'totalEpisodes': 361, 'stepsPerEpisode': 48, 'rewardPerEpisode': 34.999153094546145
'totalSteps': 21760, 'rewardStep': 0.9484762698531639, 'errorList': [0.18297791398231134, 0.24409108171714006, 0.1286679008353144, 0.2674364757136316, 0.15163292697383582, 0.47948326125623975, 0.3604030813401545, 0.283904544873763, 0.10444782635247477, 0.22557184907596997, 0.08964458897746858, 0.4219820377026975, 0.10620307378636462, 0.3665247979557046, 0.08187235417746899, 0.25040586383977337, 0.4943085119124743, 0.11743589277729208, 0.236217089275499, 0.23725117794909797, 0.3080983696675994, 0.192955948070106, 0.1950461304302914, 0.29268238717165357, 0.1370283261124121, 0.14329745638863284, 0.18089594550701, 0.19500971677284137, 0.18208627624002313, 0.3728070937498829, 0.15774149358316353, 0.29690299481982246, 0.3434082276339005, 0.29277102500395413, 0.3159723414962022, 0.1268180082412386, 0.27191668864078694, 0.20592479812297892, 0.24318492423944021, 0.25436482224060586, 0.23865813749573256, 0.2820163873251682, 0.11772800904427959, 0.12304736854630721, 0.3137005914794163, 0.14754598994138945, 0.3538786501991276, 0.3785092476891818, 0.22232911719671833, 0.3897598642040786], 'lossList': [0.0, -1.3324826228618623, 0.0, 29.37455228090286, 0.0, 0.0, 0.0], 'rewardMean': 0.6940934716795589, 'totalEpisodes': 364, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.97979435516212, 'successfulTests': 20
'totalSteps': 23040, 'rewardStep': 0.35039898228776334, 'errorList': [], 'lossList': [0.0, -1.323784090280533, 0.0, 8.856821936368942, 0.0, 0.0, 0.0], 'rewardMean': 0.6856725340629868, 'totalEpisodes': 364, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 926.4277109611169
'totalSteps': 24320, 'rewardStep': 0.7218586154023688, 'errorList': [], 'lossList': [0.0, -1.3070569217205048, 0.0, 6.033558964431286, 0.0, 0.0, 0.0], 'rewardMean': 0.6729076560289575, 'totalEpisodes': 364, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.8073417308115
'totalSteps': 25600, 'rewardStep': 0.8621785153109401, 'errorList': [], 'lossList': [0.0, -1.2708627927303313, 0.0, 5.373627978116274, 0.0, 0.0, 0.0], 'rewardMean': 0.6883533527515402, 'totalEpisodes': 364, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.1308904897146
#maxSuccessfulTests=20, maxSuccessfulTestsAtStep=21760, timeSpent=80.77
