#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 91
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8584804343243699, 'errorList': [], 'lossList': [0.0, -1.444619840979576, 0.0, 36.9149765253067, 0.0, 0.0, 0.0], 'rewardMean': 0.7085640764003625, 'totalEpisodes': 12, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.028341765022766
'totalSteps': 3840, 'rewardStep': 0.8945312046550165, 'errorList': [], 'lossList': [0.0, -1.4570085686445235, 0.0, 30.45998518407345, 0.0, 0.0, 0.0], 'rewardMean': 0.7705531191519137, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.9854716647518
'totalSteps': 5120, 'rewardStep': 0.6426887006587513, 'errorList': [], 'lossList': [0.0, -1.4410074639320374, 0.0, 29.687185053825377, 0.0, 0.0, 0.0], 'rewardMean': 0.7385870145286231, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.2280427395226
'totalSteps': 6400, 'rewardStep': 0.9462534970079105, 'errorList': [], 'lossList': [0.0, -1.4400340634584428, 0.0, 23.752558589577674, 0.0, 0.0, 0.0], 'rewardMean': 0.7801203110244805, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.7706869240055
'totalSteps': 7680, 'rewardStep': 0.8494213103252838, 'errorList': [], 'lossList': [0.0, -1.421427276134491, 0.0, 19.919883989840745, 0.0, 0.0, 0.0], 'rewardMean': 0.7916704775746144, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.0201867950414
'totalSteps': 8960, 'rewardStep': 0.9708906659178227, 'errorList': [97.8226937080075, 89.19726190359269, 98.13534469850481, 94.21342718862346, 100.39204312660007, 80.36546964428015, 93.08976043277849, 98.8979865429725, 95.73247883032191, 99.63826190786482, 90.39730048454922, 78.87809713318345, 94.27645134952708, 93.65291686928221, 94.32546727206727, 99.27230940930095, 98.02594032006996, 94.41559500924929, 101.58337362295562, 92.2566741734365, 96.64466139532347, 95.70222044661243, 85.89786585750741, 92.87332828955516, 99.90428676739229, 91.99405707122122, 99.73281434166692, 84.16174997629703, 97.60810296365403, 91.25697321774496, 100.71783116059726, 100.89298190144338, 98.46784648544593, 100.68171758760039, 98.50634022014002, 100.17199290088155, 98.99975155737012, 98.49739399700833, 101.29302626941586, 83.50387839040518, 84.40219377281295, 101.21469162616106, 96.56805087005434, 101.7059079251144, 88.66753268115947, 96.83418019356631, 98.95791157125821, 90.35863208448343, 98.9952228946478, 97.28055552600931], 'lossList': [0.0, -1.4051207172870637, 0.0, 11.891999492123723, 0.0, 0.0, 0.0], 'rewardMean': 0.8172733616236442, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.5452090213143, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.9472859703770288, 'errorList': [86.15564998641284, 101.63533438193451, 94.52840555411152, 97.78840587947359, 101.03075746489549, 99.27417143774444, 99.58837478058099, 99.7921992524381, 102.11398888860307, 96.73360601579189, 95.05288487615944, 74.5659550674977, 99.53432474320584, 102.05532151800922, 97.25729733863649, 94.0266031157948, 102.14676621527492, 100.45597883914577, 97.37408791031012, 102.48176332987377, 103.49411391437702, 94.06821394391183, 100.26647269842111, 100.81705938609349, 96.92606158866394, 102.42300669198362, 100.08841620077519, 101.53019306514604, 101.90732873917979, 101.50230049801137, 96.60042350493546, 103.03301353046017, 99.03920501800789, 100.27332723439369, 99.91584711602911, 94.31995096563567, 88.81102414994179, 102.9221064309762, 100.83635015584804, 91.40865252710061, 103.33033918362162, 103.25957682187594, 93.86810768990276, 98.79490785151114, 99.97643237326409, 95.67674660790031, 95.75821169282005, 98.24174278364912, 101.7089198386918, 103.09604820617753], 'lossList': [0.0, -1.3807983940839768, 0.0, 708.3855033874512, 0.0, 0.0, 0.0], 'rewardMean': 0.8335249377178173, 'totalEpisodes': 53, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.74287366644606, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.7452487684587246, 'errorList': [], 'lossList': [0.0, -1.3801833081245423, 0.0, 519.1076100158691, 0.0, 0.0, 0.0], 'rewardMean': 0.823716474466807, 'totalEpisodes': 94, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4566207771863369
'totalSteps': 12800, 'rewardStep': 0.6743465622001754, 'errorList': [], 'lossList': [0.0, -1.3770149064064026, 0.0, 318.9780577087402, 0.0, 0.0, 0.0], 'rewardMean': 0.8087794832401439, 'totalEpisodes': 137, 'stepsPerEpisode': 46, 'rewardPerEpisode': 37.20086989977852
'totalSteps': 14080, 'rewardStep': 0.7446943664028409, 'errorList': [], 'lossList': [0.0, -1.3707859909534454, 0.0, 107.93124139785766, 0.0, 0.0, 0.0], 'rewardMean': 0.8273841480327924, 'totalEpisodes': 180, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.36902148333406
'totalSteps': 15360, 'rewardStep': 0.6525843472873037, 'errorList': [], 'lossList': [0.0, -1.3710041642189026, 0.0, 32.59388828277588, 0.0, 0.0, 0.0], 'rewardMean': 0.806794539329086, 'totalEpisodes': 207, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.817147179091071
'totalSteps': 16640, 'rewardStep': 0.9635351242251157, 'errorList': [260.4897377506986, 224.71099892256578, 243.57837921737053, 228.27205437165622, 261.8267447974368, 243.9716886836901, 247.4748819155949, 241.8659908834334, 258.75886293941574, 219.56110893947658, 228.82231746486917, 215.19361580524685, 261.78213940577456, 226.67092754379001, 216.8685427864161, 249.63967637249397, 247.17516045651473, 251.62855659263337, 242.27769787499074, 238.16104477535254, 203.57256892568643, 263.09279470212124, 257.5329394413099, 235.88491621984448, 201.23441361321215, 252.28458694891762, 236.66012667438528, 255.72848312767593, 251.6482365990528, 244.75996941740883, 248.363930473678, 248.25694429224814, 218.34395728548768, 226.67421305443662, 251.75071004431098, 249.4373982372194, 243.15508566361797, 259.3062410105316, 248.05759498154293, 259.14737619276036, 245.30303830802032, 234.93746073203252, 204.42575691795747, 254.20585334000188, 235.6470299263627, 234.77279188528902, 224.3145988097338, 247.12556258361448, 191.53761246033568, 245.83062910156863], 'lossList': [0.0, -1.371130545735359, 0.0, 19.214159507751464, 0.0, 0.0, 0.0], 'rewardMean': 0.8136949312860956, 'totalEpisodes': 233, 'stepsPerEpisode': 38, 'rewardPerEpisode': 34.49068142714777, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.580824906824824, 'errorList': [], 'lossList': [0.0, -1.3640645110607148, 0.0, 17.630882081985472, 0.0, 0.0, 0.0], 'rewardMean': 0.8075085519027031, 'totalEpisodes': 260, 'stepsPerEpisode': 58, 'rewardPerEpisode': 41.82843524410991
'totalSteps': 19200, 'rewardStep': 0.8607187101902932, 'errorList': [], 'lossList': [0.0, -1.3548107886314391, 0.0, 19.123095555305483, 0.0, 0.0, 0.0], 'rewardMean': 0.7989550732209413, 'totalEpisodes': 276, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.78397085971369
'totalSteps': 20480, 'rewardStep': 0.697874494802311, 'errorList': [], 'lossList': [0.0, -1.3428838461637498, 0.0, 17.065978813171387, 0.0, 0.0, 0.0], 'rewardMean': 0.783800391668644, 'totalEpisodes': 287, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.73406020770409
'totalSteps': 21760, 'rewardStep': 0.9371962910148188, 'errorList': [232.36444779350313, 190.40873181657662, 38.91255194348687, 160.41518321752477, 147.17307205066302, 181.90920934061504, 140.8098498452124, 173.72830717409724, 239.01730275239697, 212.11672637905116, 192.87032566384576, 200.04868309787517, 156.99445521952865, 165.74315035485984, 29.626970439478914, 188.8499730551287, 169.49377809497548, 212.84644565638496, 109.01969910608963, 221.6904913325166, 182.04216037099582, 208.31919315095567, 157.33842553297882, 166.00111548461118, 241.2288544287645, 126.54867301763238, 190.72363174592232, 221.10736962113137, 75.7973518517407, 128.38326689120868, 148.60253039949194, 215.4487378245949, 200.98838435095013, 158.25436694000902, 184.02651385324683, 177.66915295744838, 162.83879748302198, 43.54650744547265, 224.67079689257434, 165.68007516265178, 232.6877834420601, 159.72235546814824, 212.29126037137377, 225.05248823827262, 180.5740146447447, 217.50150417841738, 206.09925907091977, 207.4547999286271, 133.43277050398294, 211.38945122853576], 'lossList': [0.0, -1.333811411857605, 0.0, 12.52551985502243, 0.0, 0.0, 0.0], 'rewardMean': 0.7804309541783436, 'totalEpisodes': 297, 'stepsPerEpisode': 29, 'rewardPerEpisode': 26.63173528953336, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.8647703069874372, 'errorList': [], 'lossList': [0.0, -1.3274172216653823, 0.0, 10.90773598909378, 0.0, 0.0, 0.0], 'rewardMean': 0.7721793878393844, 'totalEpisodes': 305, 'stepsPerEpisode': 85, 'rewardPerEpisode': 76.64051359703339
'totalSteps': 24320, 'rewardStep': 0.767907039252339, 'errorList': [], 'lossList': [0.0, -1.3302974098920821, 0.0, 7.465681200027466, 0.0, 0.0, 0.0], 'rewardMean': 0.7744452149187459, 'totalEpisodes': 313, 'stepsPerEpisode': 89, 'rewardPerEpisode': 78.77012595423494
'totalSteps': 25600, 'rewardStep': 0.42183454417500793, 'errorList': [], 'lossList': [0.0, -1.3217223078012466, 0.0, 5.589783329367638, 0.0, 0.0, 0.0], 'rewardMean': 0.7491940131162291, 'totalEpisodes': 319, 'stepsPerEpisode': 117, 'rewardPerEpisode': 83.25351917770682
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=144.84
