#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 92
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.855185987524522, 'errorList': [], 'lossList': [0.0, -1.4427587568759919, 0.0, 27.66762019395828, 0.0, 0.0, 0.0], 'rewardMean': 0.66648588137366, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 654.8354046413383
'totalSteps': 3840, 'rewardStep': 0.84452893941359, 'errorList': [], 'lossList': [0.0, -1.4598970460891723, 0.0, 43.213913540840146, 0.0, 0.0, 0.0], 'rewardMean': 0.72583356738697, 'totalEpisodes': 12, 'stepsPerEpisode': 665, 'rewardPerEpisode': 518.9578920646809
'totalSteps': 5120, 'rewardStep': 0.934200902645594, 'errorList': [], 'lossList': [0.0, -1.4645401257276536, 0.0, 28.880606985092165, 0.0, 0.0, 0.0], 'rewardMean': 0.777925401201626, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.8862821958191
'totalSteps': 6400, 'rewardStep': 0.6884515116920435, 'errorList': [], 'lossList': [0.0, -1.4489372456073761, 0.0, 24.948446321487427, 0.0, 0.0, 0.0], 'rewardMean': 0.7600306232997095, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.2426607322516
'totalSteps': 7680, 'rewardStep': 0.8099342163712256, 'errorList': [], 'lossList': [0.0, -1.431065845489502, 0.0, 16.503789306879042, 0.0, 0.0, 0.0], 'rewardMean': 0.768347888811629, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.387686141491
'totalSteps': 8960, 'rewardStep': 0.8824313539780038, 'errorList': [], 'lossList': [0.0, -1.4277180850505828, 0.0, 16.270056975781916, 0.0, 0.0, 0.0], 'rewardMean': 0.7846455266925396, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.9963482209068
'totalSteps': 10240, 'rewardStep': 0.8090779823591099, 'errorList': [], 'lossList': [0.0, -1.4165372091531754, 0.0, 617.9475820922852, 0.0, 0.0, 0.0], 'rewardMean': 0.7876995836508609, 'totalEpisodes': 54, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.6937439488056985
'totalSteps': 11520, 'rewardStep': 0.741307953065649, 'errorList': [], 'lossList': [0.0, -1.4152777552604676, 0.0, 597.5060978698731, 0.0, 0.0, 0.0], 'rewardMean': 0.7825449580302818, 'totalEpisodes': 108, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.534573609746907
'totalSteps': 12800, 'rewardStep': 0.8745200038465522, 'errorList': [], 'lossList': [0.0, -1.4138607954978943, 0.0, 412.6807160949707, 0.0, 0.0, 0.0], 'rewardMean': 0.7917424626119087, 'totalEpisodes': 160, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.436016056465892
'totalSteps': 14080, 'rewardStep': 0.9339000454367958, 'errorList': [121.3339519267479, 126.53724797982791, 104.08576536084712, 121.2835840435799, 111.04776037394154, 122.95078328777181, 120.65570442427439, 125.43172909677352, 123.10387278173029, 127.56970671961913, 119.78225258994688, 122.04233320396484, 126.69543711024681, 116.97564091307707, 128.11419220011612, 121.42995814526846, 127.46058885312009, 117.41171498215643, 122.69350364125611, 108.56461919820998, 124.74117386611013, 118.99747100957507, 127.60588632764858, 120.15633245004166, 118.49547160429587, 121.29341357893708, 102.52934194397287, 123.30217532910994, 117.4122240824889, 117.03493102607469, 118.0974967010452, 126.62829161443656, 121.40673730200295, 123.48508984682384, 125.40245792106813, 127.92281470119137, 120.73003866604564, 122.78483974578616, 116.03398279699037, 127.28795831122348, 127.93402236254391, 119.16104448906708, 117.64451307102031, 118.9003258663209, 123.3816301207006, 122.70286746514704, 121.29648611053996, 103.46096059653041, 101.67651945690912, 122.56868388998193], 'lossList': [0.0, -1.4140361881256103, 0.0, 177.11352519989015, 0.0, 0.0, 0.0], 'rewardMean': 0.8373538896333086, 'totalEpisodes': 203, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.710494783006352, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.8267210980583506, 'errorList': [], 'lossList': [0.0, -1.423329426050186, 0.0, 98.3424081993103, 0.0, 0.0, 0.0], 'rewardMean': 0.8345074006866915, 'totalEpisodes': 238, 'stepsPerEpisode': 46, 'rewardPerEpisode': 39.40327288031776
'totalSteps': 16640, 'rewardStep': 0.6408242153158769, 'errorList': [], 'lossList': [0.0, -1.4296273308992387, 0.0, 39.94877743721008, 0.0, 0.0, 0.0], 'rewardMean': 0.8141369282769201, 'totalEpisodes': 276, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.011342168013363
'totalSteps': 17920, 'rewardStep': 0.8226541412692424, 'errorList': [], 'lossList': [0.0, -1.422784532904625, 0.0, 24.342310886383057, 0.0, 0.0, 0.0], 'rewardMean': 0.8029822521392849, 'totalEpisodes': 309, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.219338302536563
'totalSteps': 19200, 'rewardStep': 0.90869667355342, 'errorList': [], 'lossList': [0.0, -1.4224857074022292, 0.0, 13.426206755638123, 0.0, 0.0, 0.0], 'rewardMean': 0.8250067683254226, 'totalEpisodes': 325, 'stepsPerEpisode': 64, 'rewardPerEpisode': 55.79053146701849
'totalSteps': 20480, 'rewardStep': 0.8998479038826556, 'errorList': [], 'lossList': [0.0, -1.4289133578538895, 0.0, 15.857492926120758, 0.0, 0.0, 0.0], 'rewardMean': 0.8339981370765657, 'totalEpisodes': 338, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.384288104233473
'totalSteps': 21760, 'rewardStep': 0.7681503934109358, 'errorList': [], 'lossList': [0.0, -1.4372033506631852, 0.0, 10.972346556186675, 0.0, 0.0, 0.0], 'rewardMean': 0.8225700410198588, 'totalEpisodes': 346, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.524139573704275
'totalSteps': 23040, 'rewardStep': 0.561292382991575, 'errorList': [], 'lossList': [0.0, -1.4420118451118469, 0.0, 8.385869594812393, 0.0, 0.0, 0.0], 'rewardMean': 0.7977914810831053, 'totalEpisodes': 352, 'stepsPerEpisode': 198, 'rewardPerEpisode': 157.9717731118381
'totalSteps': 24320, 'rewardStep': 0.9428456628456915, 'errorList': [154.44262225731381, 74.92312008020218, 138.4784745196753, 135.25307975455905, 25.595484189990216, 123.34519439417508, 23.947503358419844, 97.01238443630473, 106.32487078967795, 25.462276520794276, 128.5021386583972, 119.57780552067858, 1.9916316388719202, 31.995550633518075, 83.15650067507084, 93.76562651162364, 40.32762611979019, 70.11287460755777, 89.08025318749195, 1.4257515266272074, 140.73002678331076, 26.44339676057871, 7.618057094453848, 28.728256856047132, 124.24598510559338, 39.827613727127556, 89.3049006492818, 128.37944569856737, 85.96049225646301, 74.7088143230726, 138.5405436451354, 1.348241412851129, 38.05575026995377, 40.944744264697526, 47.18643689246611, 153.51478442250328, 106.98709688923458, 134.86751782807062, 68.35615339917268, 0.957887264218706, 117.622856111781, 60.35360480948724, 76.68301604447075, 5.755612995802522, 43.031794175019606, 16.2761015446137, 108.59816391095455, 113.37815072146354, 59.986020397577185, 0.41288925657438935], 'lossList': [0.0, -1.4405120956897735, 0.0, 7.490569695234298, 0.0, 0.0, 0.0], 'rewardMean': 0.8179452520611097, 'totalEpisodes': 357, 'stepsPerEpisode': 205, 'rewardPerEpisode': 184.57282952764936, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.9601630531476992, 'errorList': [0.04033666064811689, 0.10539310556286662, 0.028761371580110436, 1.8636315173462505, 0.27977731696272434, 0.35939862372020925, 0.19202119762257858, 2.1294101132056285, 0.09287443921844464, 0.2069952292011228, 0.0883505271949414, 0.2739279015721285, 0.05867021034084746, 0.06797158115068712, 0.03331735856604003, 0.3827486783389106, 0.2269718593447484, 0.29740668204860693, 0.20631097963548325, 0.2390303463310015, 0.636285399350566, 0.024383767694068484, 0.3578534168533278, 0.04007071107856044, 0.028413376678790986, 0.16638588964888135, 0.028422910744330614, 0.22544982847191836, 0.15956983216344553, 0.737148183804942, 0.1317275371916388, 0.3184535006392753, 0.17440953601802375, 0.033160317455583496, 0.22807660146665512, 0.560711372195025, 0.07674138970351924, 5.014333143791288, 0.21893984756483018, 0.149164597883524, 0.05154519030099606, 0.23450064046383837, 0.0514813939808263, 0.03578479090031948, 0.08722877247851861, 0.10845391738667645, 19.14157078733643, 0.02597137255132753, 0.3462771238506718, 0.22647779320113484], 'lossList': [0.0, -1.4384843963384628, 0.0, 7.766031173467636, 0.0, 0.0, 0.0], 'rewardMean': 0.8265095569912244, 'totalEpisodes': 362, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.51492706276066, 'successfulTests': 26
#maxSuccessfulTests=26, maxSuccessfulTestsAtStep=25600, timeSpent=127.37
