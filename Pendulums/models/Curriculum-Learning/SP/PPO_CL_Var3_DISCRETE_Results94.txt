#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 94
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8807831400329279, 'errorList': [], 'lossList': [0.0, -1.4239101493358612, 0.0, 34.1101538413763, 0.0, 0.0, 0.0], 'rewardMean': 0.8020670319561144, 'totalEpisodes': 13, 'stepsPerEpisode': 311, 'rewardPerEpisode': 259.5193165874563
'totalSteps': 3840, 'rewardStep': 0.7149764825536262, 'errorList': [], 'lossList': [0.0, -1.4262116760015489, 0.0, 33.58108193159104, 0.0, 0.0, 0.0], 'rewardMean': 0.7730368488219517, 'totalEpisodes': 16, 'stepsPerEpisode': 174, 'rewardPerEpisode': 131.53359940844769
'totalSteps': 5120, 'rewardStep': 0.7105539823194872, 'errorList': [], 'lossList': [0.0, -1.4302703541517259, 0.0, 20.59622856259346, 0.0, 0.0, 0.0], 'rewardMean': 0.7574161321963355, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 944.9368777886872
'totalSteps': 6400, 'rewardStep': 0.8902491246567459, 'errorList': [], 'lossList': [0.0, -1.4038825798034669, 0.0, 18.17871629357338, 0.0, 0.0, 0.0], 'rewardMean': 0.7839827306884176, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 984.9605268040733
'totalSteps': 7680, 'rewardStep': 0.8549897689917594, 'errorList': [], 'lossList': [0.0, -1.4109313309192657, 0.0, 14.03353094816208, 0.0, 0.0, 0.0], 'rewardMean': 0.795817237072308, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.589886997253
'totalSteps': 8960, 'rewardStep': 0.6357057049120484, 'errorList': [], 'lossList': [0.0, -1.4085927349328995, 0.0, 8.294088984727859, 0.0, 0.0, 0.0], 'rewardMean': 0.7729441610494138, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.5563965650211
'totalSteps': 10240, 'rewardStep': 0.7659259485780794, 'errorList': [], 'lossList': [0.0, -1.415999779701233, 0.0, 698.731244354248, 0.0, 0.0, 0.0], 'rewardMean': 0.7720668844904969, 'totalEpisodes': 73, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.169637592242828
'totalSteps': 11520, 'rewardStep': 0.7049250403227828, 'errorList': [], 'lossList': [0.0, -1.415565204024315, 0.0, 664.3336720275879, 0.0, 0.0, 0.0], 'rewardMean': 0.7646066795829731, 'totalEpisodes': 152, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.380490933040624
'totalSteps': 12800, 'rewardStep': 0.857454109681971, 'errorList': [], 'lossList': [0.0, -1.4147992181777953, 0.0, 437.5844207763672, 0.0, 0.0, 0.0], 'rewardMean': 0.7738914225928728, 'totalEpisodes': 229, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.857454109681971
'totalSteps': 14080, 'rewardStep': 0.7935011807167626, 'errorList': [], 'lossList': [0.0, -1.413010526895523, 0.0, 133.24502866744996, 0.0, 0.0, 0.0], 'rewardMean': 0.780906448276619, 'totalEpisodes': 301, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.293587267492703
'totalSteps': 15360, 'rewardStep': 0.9533515007576816, 'errorList': [99.82145019454578, 102.44827684511073, 98.07790396630617, 94.81646251572512, 100.5323101234618, 104.35770903330766, 99.05432473693133, 104.34776130641474, 104.8895543140227, 101.99389064272499, 108.14891950588694, 98.7038478152088, 105.21181501238527, 92.11842601488101, 108.06504618236393, 108.24080303395826, 108.99580125381534, 88.94924389042554, 101.28597490922638, 107.59072308684787, 104.78704001428423, 108.25560942806219, 95.09475392441402, 107.4293293590334, 105.93375127176553, 95.02999236945985, 106.26194963454012, 106.67961623764882, 98.94812363017353, 96.19771261874527, 107.4862762150814, 105.10568316885524, 102.12050149146205, 105.28242023661787, 95.14139324412723, 101.96618405156251, 110.58769466343027, 98.48398150204811, 107.95664826584022, 96.52875998993805, 94.25374055195589, 102.32496894653511, 103.13056273344763, 104.02352169764364, 97.42813098162718, 102.47052280250193, 106.55019079816178, 105.56121267243111, 97.70492266589264, 99.15652039896845], 'lossList': [0.0, -1.4067230874300003, 0.0, 64.68972694396973, 0.0, 0.0, 0.0], 'rewardMean': 0.7881632843490944, 'totalEpisodes': 351, 'stepsPerEpisode': 50, 'rewardPerEpisode': 45.31250096608671, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6524210551233757, 'errorList': [], 'lossList': [0.0, -1.3966828095912933, 0.0, 48.58864629745484, 0.0, 0.0, 0.0], 'rewardMean': 0.7819077416060695, 'totalEpisodes': 393, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.0173580766798964
'totalSteps': 17920, 'rewardStep': 0.959949350943029, 'errorList': [162.25339431978253, 95.92437474753866, 193.8342184030522, 128.79277740758067, 109.72602875824194, 190.56059411128146, 179.592421338867, 195.10070455469634, 108.2435044193127, 169.78796829926736, 202.60229661065873, 164.64162746393333, 95.63817724500981, 144.22207022340532, 85.0759458682947, 125.45741594601022, 133.42415990467495, 165.61133097306762, 106.66782249461188, 37.1347246452054, 170.10744517488033, 192.64818110143503, 177.52993562748415, 180.31744145823657, 190.28718006358758, 178.66400370979136, 150.18092283805885, 115.68243936664553, 185.08791038572633, 187.2051457591567, 37.710734209992985, 122.8139627622261, 120.04266065123608, 193.01825088719102, 110.98733457828791, 143.88701099806357, 194.02699866642408, 128.3003965021092, 154.23323396637545, 193.9503762681833, 154.2679319474398, 137.5486426224358, 191.3892511424834, 197.830079407903, 90.59993809226249, 195.89068688608864, 160.4631621298103, 144.36139692050665, 122.16639425014908, 70.08722484624464], 'lossList': [0.0, -1.3871149218082428, 0.0, 44.61823001861572, 0.0, 0.0, 0.0], 'rewardMean': 0.8068472784684235, 'totalEpisodes': 417, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.077709393335534, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.8650302496722037, 'errorList': [], 'lossList': [0.0, -1.3734735989570617, 0.0, 45.49170240402222, 0.0, 0.0, 0.0], 'rewardMean': 0.8043253909699694, 'totalEpisodes': 432, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.50575528593954
'totalSteps': 20480, 'rewardStep': 0.8633909477190967, 'errorList': [], 'lossList': [0.0, -1.3508258044719696, 0.0, 39.719215540885926, 0.0, 0.0, 0.0], 'rewardMean': 0.8051655088427031, 'totalEpisodes': 444, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.770851292838667
'totalSteps': 21760, 'rewardStep': 0.6570466978613809, 'errorList': [], 'lossList': [0.0, -1.3361517417430877, 0.0, 18.74436409711838, 0.0, 0.0, 0.0], 'rewardMean': 0.8072996081376363, 'totalEpisodes': 449, 'stepsPerEpisode': 79, 'rewardPerEpisode': 57.54883119390252
'totalSteps': 23040, 'rewardStep': 0.3734553263985614, 'errorList': [], 'lossList': [0.0, -1.3345163691043853, 0.0, 26.755656814575197, 0.0, 0.0, 0.0], 'rewardMean': 0.7680525459196845, 'totalEpisodes': 457, 'stepsPerEpisode': 152, 'rewardPerEpisode': 111.84703402450533
'totalSteps': 24320, 'rewardStep': 0.7097760114409308, 'errorList': [], 'lossList': [0.0, -1.3355758744478226, 0.0, 12.886942319869995, 0.0, 0.0, 0.0], 'rewardMean': 0.7685376430314993, 'totalEpisodes': 464, 'stepsPerEpisode': 84, 'rewardPerEpisode': 73.21073070571808
'totalSteps': 25600, 'rewardStep': 0.8682086723350478, 'errorList': [], 'lossList': [0.0, -1.3250901865959168, 0.0, 10.657602301836015, 0.0, 0.0, 0.0], 'rewardMean': 0.7696130992968071, 'totalEpisodes': 469, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.545119425728848
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=104.26
