#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 95
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9120293869764686, 'errorList': [], 'lossList': [0.0, -1.4368718886375427, 0.0, 31.965963249802588, 0.0, 0.0, 0.0], 'rewardMean': 0.9134250087786484, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 383.4933527492489
'totalSteps': 3840, 'rewardStep': 0.7201087920380944, 'errorList': [], 'lossList': [0.0, -1.4210810518264771, 0.0, 31.269216170310973, 0.0, 0.0, 0.0], 'rewardMean': 0.8489862698651304, 'totalEpisodes': 12, 'stepsPerEpisode': 268, 'rewardPerEpisode': 211.44214883633225
'totalSteps': 5120, 'rewardStep': 0.7380898310945112, 'errorList': [], 'lossList': [0.0, -1.4190899974107742, 0.0, 27.047922303676604, 0.0, 0.0, 0.0], 'rewardMean': 0.8212621601724757, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.3847194564404
'totalSteps': 6400, 'rewardStep': 0.9046049391236792, 'errorList': [], 'lossList': [0.0, -1.4086470800638198, 0.0, 25.48635731458664, 0.0, 0.0, 0.0], 'rewardMean': 0.8379307159627164, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.3090301323646
'totalSteps': 7680, 'rewardStep': 0.8050494271816422, 'errorList': [], 'lossList': [0.0, -1.3986014062166214, 0.0, 18.3441914100945, 0.0, 0.0, 0.0], 'rewardMean': 0.8324505011658707, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.642023787852
'totalSteps': 8960, 'rewardStep': 0.9433250184097923, 'errorList': [69.19070160410497, 69.5336773528815, 67.0303502892076, 71.37974907573248, 70.20933269442968, 67.9633950778915, 67.20338109743288, 64.65470233951088, 69.56213826224241, 71.35954493987461, 66.79444712304893, 68.98135932031168, 70.43104963521648, 70.63468950251458, 69.37040833578537, 65.13915510345748, 72.22225734382805, 71.00180358411384, 69.01696322492852, 69.09225908291285, 69.1357063012661, 68.04468991526099, 70.54109422372652, 69.12948356646257, 69.7640451977402, 62.743775447748085, 68.76135649508227, 69.30631508446434, 66.33611150700226, 68.72083543819919, 69.01995886471671, 68.57460746459819, 68.30588228928139, 69.58177158090108, 70.69767219084284, 68.4803611479987, 69.90699062206122, 68.79254563061184, 62.63024494310368, 70.45395124918038, 70.29953156601746, 44.89127431698095, 64.6965799523384, 66.56210098055125, 68.07287979204173, 70.93229693801815, 69.00130031718102, 69.65360570336708, 68.13624869914318, 70.43708179012532], 'lossList': [0.0, -1.3636749202013017, 0.0, 10.737519578933716, 0.0, 0.0, 0.0], 'rewardMean': 0.8482897179150024, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.4406392275732, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.781625487606461, 'errorList': [], 'lossList': [0.0, -1.3696785712242125, 0.0, 862.3068673706055, 0.0, 0.0, 0.0], 'rewardMean': 0.8399566891264347, 'totalEpisodes': 73, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.800818593289055
'totalSteps': 11520, 'rewardStep': 0.9229022231433648, 'errorList': [], 'lossList': [0.0, -1.368834552168846, 0.0, 537.0928324890136, 0.0, 0.0, 0.0], 'rewardMean': 0.8491728595727603, 'totalEpisodes': 137, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.315434374127297
'totalSteps': 12800, 'rewardStep': 0.8274739259350756, 'errorList': [], 'lossList': [0.0, -1.3653738987445831, 0.0, 105.49916366577149, 0.0, 0.0, 0.0], 'rewardMean': 0.8470029662089917, 'totalEpisodes': 206, 'stepsPerEpisode': 40, 'rewardPerEpisode': 34.77111137300699
'totalSteps': 14080, 'rewardStep': 0.5585430443853081, 'errorList': [], 'lossList': [0.0, -1.3586925381422044, 0.0, 62.024353408813475, 0.0, 0.0, 0.0], 'rewardMean': 0.8113752075894398, 'totalEpisodes': 243, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5585430443853081
'totalSteps': 15360, 'rewardStep': 0.9324664554590435, 'errorList': [162.53735109003864, 156.40674101199343, 139.3883651054935, 149.86176721620072, 168.8691853726166, 152.31427716769295, 156.3014455728732, 145.3824512617195, 145.640555430074, 159.7303223354126, 168.74500914299668, 143.19591415062186, 144.65600975477594, 140.20478622379852, 164.37410873049043, 118.46848867171866, 147.4789149220012, 132.9081938176046, 119.34387213423135, 167.7459783992694, 147.51801226819398, 155.55897301300521, 171.43997454584047, 172.39876088155182, 114.34188864729879, 154.11058912779856, 100.46314106698962, 144.56540524303568, 167.2070438425418, 156.7604499630018, 158.0005585333375, 156.5261099627303, 138.47946263790303, 169.86906025953706, 172.52565205787062, 163.66535946391951, 143.75819269812845, 129.3102166299646, 163.79435910688554, 87.32678637115175, 171.05386335108193, 155.9286045345022, 154.5171960406965, 169.9460635601074, 118.42394252221727, 161.2101374973444, 153.93202366925277, 156.247582367175, 158.6493808058623, 158.28501310135582], 'lossList': [0.0, -1.3517769193649292, 0.0, 41.87675937652588, 0.0, 0.0, 0.0], 'rewardMean': 0.8134189144376972, 'totalEpisodes': 282, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.369420352899176, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5883745047653278, 'errorList': [], 'lossList': [0.0, -1.347465009689331, 0.0, 30.39909882068634, 0.0, 0.0, 0.0], 'rewardMean': 0.8002454857104206, 'totalEpisodes': 306, 'stepsPerEpisode': 37, 'rewardPerEpisode': 25.920620701005152
'totalSteps': 17920, 'rewardStep': 0.8844784630216379, 'errorList': [], 'lossList': [0.0, -1.344251794219017, 0.0, 26.967800064086916, 0.0, 0.0, 0.0], 'rewardMean': 0.8148843489031334, 'totalEpisodes': 319, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.19958775890838
'totalSteps': 19200, 'rewardStep': 0.5483317447185017, 'errorList': [], 'lossList': [0.0, -1.3368179827928544, 0.0, 22.8457311630249, 0.0, 0.0, 0.0], 'rewardMean': 0.7792570294626154, 'totalEpisodes': 327, 'stepsPerEpisode': 188, 'rewardPerEpisode': 154.92461057436506
'totalSteps': 20480, 'rewardStep': 0.8695400341680224, 'errorList': [], 'lossList': [0.0, -1.329101739525795, 0.0, 9.526484105587006, 0.0, 0.0, 0.0], 'rewardMean': 0.7857060901612535, 'totalEpisodes': 333, 'stepsPerEpisode': 62, 'rewardPerEpisode': 51.84752046864102
'totalSteps': 21760, 'rewardStep': 0.8705638387543643, 'errorList': [], 'lossList': [0.0, -1.326516717672348, 0.0, 10.64976097226143, 0.0, 0.0, 0.0], 'rewardMean': 0.7784299721957106, 'totalEpisodes': 337, 'stepsPerEpisode': 62, 'rewardPerEpisode': 51.26807048709303
'totalSteps': 23040, 'rewardStep': 0.31549677047903807, 'errorList': [], 'lossList': [0.0, -1.3237699496746063, 0.0, 5.825729677677154, 0.0, 0.0, 0.0], 'rewardMean': 0.7318171004829684, 'totalEpisodes': 340, 'stepsPerEpisode': 176, 'rewardPerEpisode': 126.75196218713666
'totalSteps': 24320, 'rewardStep': 0.7978465123966453, 'errorList': [], 'lossList': [0.0, -1.3164336520433426, 0.0, 4.6122043496370315, 0.0, 0.0, 0.0], 'rewardMean': 0.7193115294082965, 'totalEpisodes': 343, 'stepsPerEpisode': 181, 'rewardPerEpisode': 146.6780806891773
'totalSteps': 25600, 'rewardStep': 0.8799671804103433, 'errorList': [], 'lossList': [0.0, -1.3064039033651351, 0.0, 3.5917323769629004, 0.0, 0.0, 0.0], 'rewardMean': 0.7245608548558231, 'totalEpisodes': 344, 'stepsPerEpisode': 1120, 'rewardPerEpisode': 996.6277423956984
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=102.99
