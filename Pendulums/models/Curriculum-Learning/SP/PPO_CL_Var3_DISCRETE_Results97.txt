#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 97
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8663769990037149, 'errorList': [], 'lossList': [0.0, -1.4424236595630646, 0.0, 30.24666601061821, 0.0, 0.0, 0.0], 'rewardMean': 0.6847389234538633, 'totalEpisodes': 10, 'stepsPerEpisode': 905, 'rewardPerEpisode': 676.1619782270964
'totalSteps': 3840, 'rewardStep': 0.8318612758400938, 'errorList': [], 'lossList': [0.0, -1.4595861428976058, 0.0, 44.8659469127655, 0.0, 0.0, 0.0], 'rewardMean': 0.7337797075826069, 'totalEpisodes': 12, 'stepsPerEpisode': 668, 'rewardPerEpisode': 531.5237637867058
'totalSteps': 5120, 'rewardStep': 0.9420636962973828, 'errorList': [], 'lossList': [0.0, -1.4606796836853027, 0.0, 29.767981889247896, 0.0, 0.0, 0.0], 'rewardMean': 0.7858507047613009, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.4430166496704
'totalSteps': 6400, 'rewardStep': 0.7083415081798111, 'errorList': [], 'lossList': [0.0, -1.4487400782108306, 0.0, 25.63570770084858, 0.0, 0.0, 0.0], 'rewardMean': 0.7703488654450029, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.4453318273113
'totalSteps': 7680, 'rewardStep': 0.8333019530015451, 'errorList': [], 'lossList': [0.0, -1.4341128611564635, 0.0, 16.80647209405899, 0.0, 0.0, 0.0], 'rewardMean': 0.7808410467044267, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.8633334670064
'totalSteps': 8960, 'rewardStep': 0.8839684413984422, 'errorList': [], 'lossList': [0.0, -1.4346774917840959, 0.0, 16.156789700463413, 0.0, 0.0, 0.0], 'rewardMean': 0.7955735316607147, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.637516643312
'totalSteps': 10240, 'rewardStep': 0.8648428122671088, 'errorList': [], 'lossList': [0.0, -1.4211949783563613, 0.0, 827.4690509033203, 0.0, 0.0, 0.0], 'rewardMean': 0.8042321917365138, 'totalEpisodes': 66, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7334273290693152
'totalSteps': 11520, 'rewardStep': 0.5827199698743287, 'errorList': [], 'lossList': [0.0, -1.4204223251342774, 0.0, 510.80880950927735, 0.0, 0.0, 0.0], 'rewardMean': 0.7796197226407154, 'totalEpisodes': 110, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.082766149515344
'totalSteps': 12800, 'rewardStep': 0.6125163078159277, 'errorList': [], 'lossList': [0.0, -1.4198636609315871, 0.0, 409.824538269043, 0.0, 0.0, 0.0], 'rewardMean': 0.7629093811582367, 'totalEpisodes': 167, 'stepsPerEpisode': 39, 'rewardPerEpisode': 31.971352591537407
'totalSteps': 14080, 'rewardStep': 0.7962382941665151, 'errorList': [], 'lossList': [0.0, -1.4140682810544967, 0.0, 123.14360771179199, 0.0, 0.0, 0.0], 'rewardMean': 0.792223125784487, 'totalEpisodes': 213, 'stepsPerEpisode': 41, 'rewardPerEpisode': 35.78682360719727
'totalSteps': 15360, 'rewardStep': 0.6923915141142473, 'errorList': [], 'lossList': [0.0, -1.4052948755025865, 0.0, 59.12456304550171, 0.0, 0.0, 0.0], 'rewardMean': 0.7748245772955402, 'totalEpisodes': 252, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.6591683048555
'totalSteps': 16640, 'rewardStep': 0.7750978983399773, 'errorList': [], 'lossList': [0.0, -1.4135788267850875, 0.0, 35.72290451049805, 0.0, 0.0, 0.0], 'rewardMean': 0.7691482395455285, 'totalEpisodes': 279, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.770056288260708
'totalSteps': 17920, 'rewardStep': 0.9311482354044327, 'errorList': [234.89883533071696, 236.31344414518333, 246.82808249661616, 269.7916514375383, 184.16931261472934, 248.92250335340378, 237.7174265588393, 198.11320763079308, 214.70313788649082, 145.09628819164686, 245.3779020591598, 240.6575216629819, 245.81388204866917, 218.9863544474713, 175.1643461344351, 207.21269672206824, 251.69865881013385, 187.49345123826663, 250.61134657691926, 267.3074908383778, 202.27739792240612, 171.2590197034928, 265.6832532591185, 256.8386956717028, 256.30912263674884, 188.76987502662274, 237.6377090291284, 252.10016860981557, 202.7380526419788, 228.89927388750016, 253.34107901643438, 260.53851793202745, 212.73511070481322, 265.85603016519525, 237.9711846788259, 214.28538254790223, 260.2473291826589, 188.07850141270407, 228.09241703386863, 242.68378884572388, 215.918601187823, 182.82921672472582, 230.88244460093273, 201.04014258276908, 159.84513435542726, 220.41877594949418, 253.47904506715233, 260.8661971711523, 209.71949979592702, 269.6756437310799], 'lossList': [0.0, -1.4097685348987579, 0.0, 23.588705196380616, 0.0, 0.0, 0.0], 'rewardMean': 0.7680566934562336, 'totalEpisodes': 297, 'stepsPerEpisode': 65, 'rewardPerEpisode': 56.88183198172058, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.7530375492198484, 'errorList': [], 'lossList': [0.0, -1.3881674933433532, 0.0, 24.966928853988648, 0.0, 0.0, 0.0], 'rewardMean': 0.7725262975602374, 'totalEpisodes': 315, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.877101930126784
'totalSteps': 20480, 'rewardStep': 0.8643851549777729, 'errorList': [], 'lossList': [0.0, -1.3722967433929443, 0.0, 14.548052823543548, 0.0, 0.0, 0.0], 'rewardMean': 0.7756346177578601, 'totalEpisodes': 327, 'stepsPerEpisode': 48, 'rewardPerEpisode': 39.5535068761034
'totalSteps': 21760, 'rewardStep': 0.732580853994735, 'errorList': [], 'lossList': [0.0, -1.356646981239319, 0.0, 12.733680799007416, 0.0, 0.0, 0.0], 'rewardMean': 0.7604958590174894, 'totalEpisodes': 337, 'stepsPerEpisode': 136, 'rewardPerEpisode': 113.60036771342395
'totalSteps': 23040, 'rewardStep': 0.6827341630690467, 'errorList': [], 'lossList': [0.0, -1.354298621416092, 0.0, 7.717675967216492, 0.0, 0.0, 0.0], 'rewardMean': 0.7422849940976832, 'totalEpisodes': 346, 'stepsPerEpisode': 39, 'rewardPerEpisode': 28.545946661643253
'totalSteps': 24320, 'rewardStep': 0.6378456830547766, 'errorList': [], 'lossList': [0.0, -1.370356130003929, 0.0, 6.886793338060379, 0.0, 0.0, 0.0], 'rewardMean': 0.747797565415728, 'totalEpisodes': 352, 'stepsPerEpisode': 162, 'rewardPerEpisode': 130.65110633867104
'totalSteps': 25600, 'rewardStep': 0.7191102719048532, 'errorList': [], 'lossList': [0.0, -1.3581447905302049, 0.0, 9.561986439228058, 0.0, 0.0, 0.0], 'rewardMean': 0.7584569618246205, 'totalEpisodes': 357, 'stepsPerEpisode': 565, 'rewardPerEpisode': 456.88943325982694
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=85.3
