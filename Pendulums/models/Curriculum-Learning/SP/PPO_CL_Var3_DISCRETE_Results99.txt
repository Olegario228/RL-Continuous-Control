#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 99
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_DISCRETE_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'discrete', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8201616791084557, 'errorList': [], 'lossList': [0.0, -1.4210871738195419, 0.0, 28.444636491537093, 0.0, 0.0, 0.0], 'rewardMean': 0.7847262717038683, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9817592529649
'totalSteps': 3840, 'rewardStep': 0.7653923850432686, 'errorList': [], 'lossList': [0.0, -1.434804788827896, 0.0, 35.22184423208237, 0.0, 0.0, 0.0], 'rewardMean': 0.7782816428170017, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.43908212466643
'totalSteps': 5120, 'rewardStep': 0.7040208628153881, 'errorList': [], 'lossList': [0.0, -1.4309330904483795, 0.0, 26.119540514349936, 0.0, 0.0, 0.0], 'rewardMean': 0.7597164478165983, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.8211163323339
'totalSteps': 6400, 'rewardStep': 0.9060037613770545, 'errorList': [], 'lossList': [0.0, -1.4235157144069672, 0.0, 19.444068129062654, 0.0, 0.0, 0.0], 'rewardMean': 0.7889739105286895, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.771177195542
'totalSteps': 7680, 'rewardStep': 0.8555522224957413, 'errorList': [], 'lossList': [0.0, -1.431319814324379, 0.0, 14.310615525841714, 0.0, 0.0, 0.0], 'rewardMean': 0.8000702958565314, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.3263114753922
'totalSteps': 8960, 'rewardStep': 0.6686171623150308, 'errorList': [], 'lossList': [0.0, -1.427231957912445, 0.0, 9.493658114671707, 0.0, 0.0, 0.0], 'rewardMean': 0.7812912767791742, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.1251569766994
'totalSteps': 10240, 'rewardStep': 0.7630279530561482, 'errorList': [], 'lossList': [0.0, -1.4358164966106415, 0.0, 834.2521411132813, 0.0, 0.0, 0.0], 'rewardMean': 0.7790083613137959, 'totalEpisodes': 77, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.121298608577657
'totalSteps': 11520, 'rewardStep': 0.6702947408324799, 'errorList': [], 'lossList': [0.0, -1.4354302620887756, 0.0, 710.1445510864257, 0.0, 0.0, 0.0], 'rewardMean': 0.7669290701492053, 'totalEpisodes': 156, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.091197642720385
'totalSteps': 12800, 'rewardStep': 0.6076857624018165, 'errorList': [], 'lossList': [0.0, -1.4349067556858062, 0.0, 324.4463027572632, 0.0, 0.0, 0.0], 'rewardMean': 0.7510047393744663, 'totalEpisodes': 216, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.932673421360089
'totalSteps': 14080, 'rewardStep': 0.7832865717165509, 'errorList': [], 'lossList': [0.0, -1.4341888689994813, 0.0, 102.79321620941163, 0.0, 0.0, 0.0], 'rewardMean': 0.7544043101161935, 'totalEpisodes': 273, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.406422588124155
'totalSteps': 15360, 'rewardStep': 0.808204176410208, 'errorList': [], 'lossList': [0.0, -1.4316030025482178, 0.0, 56.40706464767456, 0.0, 0.0, 0.0], 'rewardMean': 0.7532085598463686, 'totalEpisodes': 324, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.303413891180737
'totalSteps': 16640, 'rewardStep': 0.8044408274247162, 'errorList': [], 'lossList': [0.0, -1.432998884320259, 0.0, 53.8253461265564, 0.0, 0.0, 0.0], 'rewardMean': 0.7571134040845134, 'totalEpisodes': 359, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.133257116627469
'totalSteps': 17920, 'rewardStep': 0.6841547328124626, 'errorList': [], 'lossList': [0.0, -1.432872651219368, 0.0, 27.56943257331848, 0.0, 0.0, 0.0], 'rewardMean': 0.7551267910842209, 'totalEpisodes': 376, 'stepsPerEpisode': 54, 'rewardPerEpisode': 38.840685877315096
'totalSteps': 19200, 'rewardStep': 0.6978707524025143, 'errorList': [], 'lossList': [0.0, -1.4293944138288497, 0.0, 21.718451447486878, 0.0, 0.0, 0.0], 'rewardMean': 0.7343134901867668, 'totalEpisodes': 390, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.222881021581491
'totalSteps': 20480, 'rewardStep': 0.989658320757236, 'errorList': [90.09100924928055, 19.53097017076871, 99.85915985812343, 121.6050452858091, 36.7177989784424, 161.5821567740934, 154.67771616813025, 125.75611583737741, 91.9786749077351, 94.09210143358094, 107.13478449889082, 49.69737895225019, 22.34678484955262, 141.86583036429985, 138.51272419490286, 79.71110847628597, 44.210140472175624, 142.9791885586624, 139.27590959234732, 95.46888376378227, 82.524991307713, 136.11724477045004, 95.72592657605963, 43.391664936105954, 76.62262962498316, 152.07014501005844, 137.38020875091158, 77.7201641641526, 117.50395273470244, 142.85348329728768, 132.31662190462578, 65.5774258489105, 123.10075366722766, 176.48930133451503, 53.450127288127135, 47.12936911744664, 48.692104087039986, 183.869048624071, 106.31572446388789, 101.67949838296828, 138.25889815488193, 128.0034359752934, 134.02454685944016, 149.37464805334739, 145.26296394009063, 130.47783949139506, 145.26405915387392, 87.88049479304492, 155.55676585891976, 49.551400013336426], 'lossList': [0.0, -1.4125267523527145, 0.0, 17.114523890018464, 0.0, 0.0, 0.0], 'rewardMean': 0.7477241000129164, 'totalEpisodes': 398, 'stepsPerEpisode': 59, 'rewardPerEpisode': 46.4076209831235, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.7418534184197147, 'errorList': [], 'lossList': [0.0, -1.3951830768585205, 0.0, 18.087202579975127, 0.0, 0.0, 0.0], 'rewardMean': 0.7550477256233847, 'totalEpisodes': 404, 'stepsPerEpisode': 92, 'rewardPerEpisode': 73.65402546414286
'totalSteps': 23040, 'rewardStep': 0.8269460881162407, 'errorList': [], 'lossList': [0.0, -1.3930146259069442, 0.0, 17.37228705406189, 0.0, 0.0, 0.0], 'rewardMean': 0.761439539129394, 'totalEpisodes': 409, 'stepsPerEpisode': 89, 'rewardPerEpisode': 74.83681757290908
'totalSteps': 24320, 'rewardStep': 0.7244827990533728, 'errorList': [], 'lossList': [0.0, -1.4026537877321243, 0.0, 7.324364117383957, 0.0, 0.0, 0.0], 'rewardMean': 0.7668583449514832, 'totalEpisodes': 413, 'stepsPerEpisode': 53, 'rewardPerEpisode': 45.14642853266841
'totalSteps': 25600, 'rewardStep': 0.9695674528321699, 'errorList': [19.438088410707415, 0.7341608061718311, 0.5499869716745958, 0.6441293456110571, 18.091982264525193, 0.549481696333569, 2.233163838895939, 0.5303485009022548, 0.9766907322169093, 0.17736761888401684, 0.2680521340268893, 0.42716794489364923, 0.9338965332491055, 0.3863996252612915, 0.5866487280591965, 0.8518593032445162, 0.46384915370241575, 0.4645028527291256, 1.5290687950784716, 0.297812414700293, 0.2600585265193416, 1.9082470084954104, 0.38154614553543276, 9.863818064220826, 3.5549490491503986, 0.5520230422331508, 18.18169416614376, 1.0575792008089644, 0.18278162245211133, 0.23734044948281874, 0.5402028666776226, 0.4206779309884355, 1.2621183623056345, 4.268291350405209, 32.665988522629924, 0.5083916258740092, 0.44907503187308734, 0.3057857471198917, 0.4194862767160386, 1.9370488331010833, 0.7486920637584855, 0.9790457575989344, 0.849265911661796, 0.6549689553181225, 2.378433389999885, 0.38080292211639266, 1.4049723683074364, 0.4987683463277787, 0.16234328413235047, 53.6541433640957], 'lossList': [0.0, -1.3920997351408004, 0.0, 7.240648335814476, 0.0, 0.0, 0.0], 'rewardMean': 0.8030465139945185, 'totalEpisodes': 414, 'stepsPerEpisode': 500, 'rewardPerEpisode': 458.6998854291932, 'successfulTests': 3
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=25600, timeSpent=105.1
