#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 10
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.6480180639002443, 'errorList': [], 'lossList': [0.0, -1.3987533229589462, 0.0, 32.972056274414065, 0.0, 0.0, 0.0], 'rewardMean': 0.7239085121996036, 'totalEpisodes': 49, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.229747181086509
'totalSteps': 3840, 'rewardStep': 0.6417518340400558, 'errorList': [], 'lossList': [0.0, -1.3892326456308366, 0.0, 43.326155529022216, 0.0, 0.0, 0.0], 'rewardMean': 0.6965229528130877, 'totalEpisodes': 113, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2426009476433926
'totalSteps': 5120, 'rewardStep': 0.7238518817871344, 'errorList': [], 'lossList': [0.0, -1.3871837723255158, 0.0, 43.66061285972595, 0.0, 0.0, 0.0], 'rewardMean': 0.7033551850565993, 'totalEpisodes': 143, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.243249513742896
'totalSteps': 6400, 'rewardStep': 0.9349673822272182, 'errorList': [26.192783675017523, 20.017301141570076, 18.58115163319384, 15.24658167433296, 18.046674893845502, 18.427059043173415, 17.431776821807826, 12.43695352264657, 17.54668022846758, 18.046732928500507, 18.040126488373676, 12.630624679189554, 7.40089529431821, 18.212056120970452, 15.546259233668183, 17.254720853828548, 18.04667824138223, 23.447780586422088, 17.12690448953481, 17.202068016645217, 5.963485869614488, 18.384308358987017, 18.082189703852023, 23.099507878144227, 6.276146175697212, 18.046763232200995, 12.250275021648974, 18.046638201651035, 17.6064789906089, 18.04674510032594, 17.672019277250143, 18.420764424286993, 18.04675881496869, 22.26350834107097, 17.151595168807603, 18.046752987845043, 15.036716852443098, 19.59224238806672, 18.046758509553857, 3.3928008180135754, 13.317871912767393, 18.04661570440406, 17.21012752572219, 20.585059118786063, 8.644913288672361, 21.752536689218392, 19.185047878635206, 17.934635352323596, 18.00997822202702, 15.253116710045772], 'lossList': [0.0, -1.386212437748909, 0.0, 29.465966744422914, 0.0, 0.0, 0.0], 'rewardMean': 0.7496776244907231, 'totalEpisodes': 150, 'stepsPerEpisode': 276, 'rewardPerEpisode': 199.95784615293672, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.7456697424181884, 'errorList': [], 'lossList': [0.0, -1.3854464203119279, 0.0, 31.856226867437364, 0.0, 0.0, 0.0], 'rewardMean': 0.7490096441453007, 'totalEpisodes': 155, 'stepsPerEpisode': 200, 'rewardPerEpisode': 146.72475147145434
'totalSteps': 8960, 'rewardStep': 0.8593333002032022, 'errorList': [], 'lossList': [0.0, -1.3771059042215348, 0.0, 51.48884698867798, 0.0, 0.0, 0.0], 'rewardMean': 0.7647701664392866, 'totalEpisodes': 163, 'stepsPerEpisode': 270, 'rewardPerEpisode': 231.37998835840799
'totalSteps': 10240, 'rewardStep': 0.6179416649588569, 'errorList': [], 'lossList': [0.0, -1.3815639412403107, 0.0, 40.45943693161011, 0.0, 0.0, 0.0], 'rewardMean': 0.7464166037542329, 'totalEpisodes': 170, 'stepsPerEpisode': 110, 'rewardPerEpisode': 87.80463609374301
'totalSteps': 11520, 'rewardStep': 0.6667785412028775, 'errorList': [], 'lossList': [0.0, -1.3770994710922242, 0.0, 53.89646642208099, 0.0, 0.0, 0.0], 'rewardMean': 0.7375679301374157, 'totalEpisodes': 175, 'stepsPerEpisode': 68, 'rewardPerEpisode': 58.701743787896625
'totalSteps': 12800, 'rewardStep': 0.9040307533596407, 'errorList': [], 'lossList': [0.0, -1.3576336461305618, 0.0, 19.996131732463837, 0.0, 0.0, 0.0], 'rewardMean': 0.7542142124596383, 'totalEpisodes': 178, 'stepsPerEpisode': 70, 'rewardPerEpisode': 58.777013797753746
'totalSteps': 14080, 'rewardStep': 0.5841236761701923, 'errorList': [], 'lossList': [0.0, -1.3311493718624114, 0.0, 8.076540741324425, 0.0, 0.0, 0.0], 'rewardMean': 0.7326466840267611, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 968.692367950705
'totalSteps': 15360, 'rewardStep': 0.8933819072004777, 'errorList': [], 'lossList': [0.0, -1.3065170127153396, 0.0, 43.1112469291687, 0.0, 0.0, 0.0], 'rewardMean': 0.7571830683567844, 'totalEpisodes': 182, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.33916801017009
'totalSteps': 16640, 'rewardStep': 0.8940696405720086, 'errorList': [], 'lossList': [0.0, -1.318415874838829, 0.0, 12.503807411193847, 0.0, 0.0, 0.0], 'rewardMean': 0.7824148490099797, 'totalEpisodes': 185, 'stepsPerEpisode': 50, 'rewardPerEpisode': 45.094575948172896
'totalSteps': 17920, 'rewardStep': 0.6624794708836743, 'errorList': [], 'lossList': [0.0, -1.3363685697317123, 0.0, 1.342991309762001, 0.0, 0.0, 0.0], 'rewardMean': 0.7762776079196337, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 839.3340341062672
'totalSteps': 19200, 'rewardStep': 0.7460884756473145, 'errorList': [], 'lossList': [0.0, -1.3190295428037644, 0.0, 0.8114621728658676, 0.0, 0.0, 0.0], 'rewardMean': 0.7573897172616434, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 931.4126505409931
'totalSteps': 20480, 'rewardStep': 0.8951951412354018, 'errorList': [], 'lossList': [0.0, -1.3097291326522826, 0.0, 1.9828014282882214, 0.0, 0.0, 0.0], 'rewardMean': 0.7723422571433647, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.1423144069333
'totalSteps': 21760, 'rewardStep': 0.9477951490099575, 'errorList': [0.18438895698801272, 0.13428722300208135, 0.1262639501280028, 0.3948552178845162, 0.3059691550758334, 0.1282757338279159, 0.21318790624250072, 0.1389354356727909, 0.2240597553538132, 0.37336616546270257, 0.14953770517613593, 0.17182044753533218, 0.1437674731408143, 0.1295580415666442, 0.13184036798753962, 0.1493141301791314, 0.1497358755199286, 0.14142678013289983, 0.14417238660173456, 0.19052320907678746, 0.2417861762911773, 0.13908079106475968, 0.1766092879967153, 0.16306506277297259, 0.12860828611541752, 0.22513489985437413, 0.14943582258604826, 0.2641763360864845, 0.12520308781925618, 0.12185886884209773, 0.13573655844041588, 0.13010360860670786, 0.22735575089384516, 0.14419828527744685, 0.17420975704977704, 0.23347263688481193, 0.2871607051767126, 0.14760882579621334, 0.12946420603553804, 0.1414270071812152, 0.250181720116855, 0.14265932806501055, 0.17357560135691671, 0.14694450155736388, 0.20666339264689967, 0.13394641746432184, 0.14226824832561477, 0.2583951760571855, 0.131102937927487, 0.14013497406349937], 'lossList': [0.0, -1.2893592834472656, 0.0, 1.4458058550953865, 0.0, 0.0, 0.0], 'rewardMean': 0.7811884420240401, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.9593059335632, 'successfulTests': 36
'totalSteps': 23040, 'rewardStep': 0.8141811567753535, 'errorList': [], 'lossList': [0.0, -1.262436915040016, 0.0, 0.8025988148525357, 0.0, 0.0, 0.0], 'rewardMean': 0.8008123912056899, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.4414632947428
'totalSteps': 24320, 'rewardStep': 0.7781214748844608, 'errorList': [], 'lossList': [0.0, -1.2315946328639984, 0.0, 0.7360291331261396, 0.0, 0.0, 0.0], 'rewardMean': 0.8119466845738481, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.9323694172817
'totalSteps': 25600, 'rewardStep': 0.9316075812419821, 'errorList': [0.05027097750442548, 0.019358174135135114, 0.13355258860978242, 0.10051901709949396, 0.02120448604186592, 0.05021579756356865, 0.09476412985767059, 0.07283400195313237, 0.04342863397954594, 0.05848139384142048, 0.09340354519198205, 0.03613752758120334, 0.04039855831342309, 0.06650611179978218, 0.013129248038967867, 0.05184350718398448, 0.10018503595616932, 0.031578937774519575, 0.03280801121518756, 0.08826097908787461, 0.050195137234537425, 0.03200336979898227, 0.07478933003305303, 0.06365695750005616, 0.08127259973524548, 0.19145512231002978, 0.05981677842137876, 0.08350925092636377, 0.1354069784042182, 0.02546249723511033, 0.02594885088987597, 0.0972104593807537, 0.02963371116044297, 0.018176995087274172, 0.06278587282969611, 0.04622783280354402, 0.03283405810830759, 0.16257696817107606, 0.057687497220450426, 0.04722039426465477, 0.029233822247251684, 0.023736269647654223, 0.04322015519651372, 0.027720780463368743, 0.045741959223443276, 0.07280734824572636, 0.021240803749704558, 0.020559335636822888, 0.1095067666378884, 0.08549008214558322], 'lossList': [0.0, -1.189223964214325, 0.0, 0.428840618878603, 0.0, 0.0, 0.0], 'rewardMean': 0.8147043673620823, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.5130524439571, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=109.71
