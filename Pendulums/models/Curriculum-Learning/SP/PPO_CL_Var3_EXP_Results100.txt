#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 100
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7752250360672042, 'errorList': [], 'lossList': [0.0, -1.4135721319913863, 0.0, 30.824314413070677, 0.0, 0.0, 0.0], 'rewardMean': 0.609723888521361, 'totalEpisodes': 78, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.4725757789706995
'totalSteps': 3840, 'rewardStep': 0.5844795130538001, 'errorList': [], 'lossList': [0.0, -1.4122273325920105, 0.0, 39.389905300140384, 0.0, 0.0, 0.0], 'rewardMean': 0.6013090966988407, 'totalEpisodes': 133, 'stepsPerEpisode': 25, 'rewardPerEpisode': 17.783224392699946
'totalSteps': 5120, 'rewardStep': 0.5133840834436962, 'errorList': [], 'lossList': [0.0, -1.406567839384079, 0.0, 42.67814044952392, 0.0, 0.0, 0.0], 'rewardMean': 0.5793278433850546, 'totalEpisodes': 171, 'stepsPerEpisode': 103, 'rewardPerEpisode': 72.17400370121517
'totalSteps': 6400, 'rewardStep': 0.8250262899937072, 'errorList': [], 'lossList': [0.0, -1.402166895866394, 0.0, 42.326168632507326, 0.0, 0.0, 0.0], 'rewardMean': 0.6284675327067851, 'totalEpisodes': 189, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.930034075756947
'totalSteps': 7680, 'rewardStep': 0.853308948232062, 'errorList': [], 'lossList': [0.0, -1.3906927198171615, 0.0, 25.80738745212555, 0.0, 0.0, 0.0], 'rewardMean': 0.6659411019609979, 'totalEpisodes': 197, 'stepsPerEpisode': 70, 'rewardPerEpisode': 55.80414258567613
'totalSteps': 8960, 'rewardStep': 0.7975165106850536, 'errorList': [], 'lossList': [0.0, -1.3614508759975434, 0.0, 21.158979853391646, 0.0, 0.0, 0.0], 'rewardMean': 0.6847375889215773, 'totalEpisodes': 203, 'stepsPerEpisode': 165, 'rewardPerEpisode': 129.86221393549172
'totalSteps': 10240, 'rewardStep': 0.6785145186380258, 'errorList': [], 'lossList': [0.0, -1.3510792112350465, 0.0, 32.11334034919739, 0.0, 0.0, 0.0], 'rewardMean': 0.6839597051361334, 'totalEpisodes': 211, 'stepsPerEpisode': 110, 'rewardPerEpisode': 92.98680568944629
'totalSteps': 11520, 'rewardStep': 0.6156196046838321, 'errorList': [], 'lossList': [0.0, -1.3241722291707994, 0.0, 45.268379950523375, 0.0, 0.0, 0.0], 'rewardMean': 0.6763663606414333, 'totalEpisodes': 218, 'stepsPerEpisode': 66, 'rewardPerEpisode': 54.0711368882028
'totalSteps': 12800, 'rewardStep': 0.9082493905272917, 'errorList': [], 'lossList': [0.0, -1.309194256067276, 0.0, 8.24843868970871, 0.0, 0.0, 0.0], 'rewardMean': 0.6995546636300192, 'totalEpisodes': 224, 'stepsPerEpisode': 121, 'rewardPerEpisode': 109.84820325377439
'totalSteps': 14080, 'rewardStep': 0.708829081126829, 'errorList': [], 'lossList': [0.0, -1.3028294920921326, 0.0, 5.861975779533386, 0.0, 0.0, 0.0], 'rewardMean': 0.7260152976451503, 'totalEpisodes': 229, 'stepsPerEpisode': 46, 'rewardPerEpisode': 38.26829583401913
'totalSteps': 15360, 'rewardStep': 0.3469887111387002, 'errorList': [], 'lossList': [0.0, -1.2953112226724626, 0.0, 6.6980595672130585, 0.0, 0.0, 0.0], 'rewardMean': 0.6831916651522998, 'totalEpisodes': 231, 'stepsPerEpisode': 326, 'rewardPerEpisode': 215.90597532531868
'totalSteps': 16640, 'rewardStep': 0.9241626353308905, 'errorList': [], 'lossList': [0.0, -1.2831148129701615, 0.0, 4.1393409132957455, 0.0, 0.0, 0.0], 'rewardMean': 0.7171599773800088, 'totalEpisodes': 233, 'stepsPerEpisode': 601, 'rewardPerEpisode': 527.9582445593661
'totalSteps': 17920, 'rewardStep': 0.8057607456539897, 'errorList': [], 'lossList': [0.0, -1.266609837412834, 0.0, 2.0007595720887186, 0.0, 0.0, 0.0], 'rewardMean': 0.7463976436010382, 'totalEpisodes': 233, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.119133020293
'totalSteps': 19200, 'rewardStep': 0.9014968584897154, 'errorList': [], 'lossList': [0.0, -1.2420616507530213, 0.0, 2.096219789683819, 0.0, 0.0, 0.0], 'rewardMean': 0.754044700450639, 'totalEpisodes': 233, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.02227368567
'totalSteps': 20480, 'rewardStep': 0.9396320876124633, 'errorList': [0.023662690615190045, 0.029523547979241022, 0.055106193404201184, 0.060543534007845005, 0.06405540215856219, 0.040306660640724634, 0.06604431041193655, 0.05476780056713325, 0.07188653826537346, 0.020953918626211047, 0.05265357097264383, 0.1261831103909002, 0.028182226811274372, 0.09443169592556863, 0.04452018049902648, 0.02142157150824133, 0.04503387173658899, 0.07490999811376695, 0.08771371945028658, 0.029846684506875363, 0.019126761668092268, 0.032841958568242464, 0.05507893179858979, 0.0733409308038351, 0.06936976989353943, 0.07193519354333762, 0.06415717679819043, 0.06008968947772702, 0.046956702438535794, 0.10692029814317591, 0.04020944335134992, 0.01830179815607, 0.08246409125890251, 0.04544033789720241, 0.09218447967242072, 0.09117255757374472, 0.03170351497817026, 0.05735895789455636, 0.058761688979817484, 0.031626396939210016, 0.043851007926922946, 0.05173065803361191, 0.03388718575266058, 0.07854259327377935, 0.042009391146018545, 0.08083227833316357, 0.018582769841367794, 0.06771912487834153, 0.019632065996560727, 0.026554421176328544], 'lossList': [0.0, -1.2153950971364975, 0.0, 1.5602138608694076, 0.0, 0.0, 0.0], 'rewardMean': 0.7626770143886792, 'totalEpisodes': 233, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.931877717401, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=70.37
