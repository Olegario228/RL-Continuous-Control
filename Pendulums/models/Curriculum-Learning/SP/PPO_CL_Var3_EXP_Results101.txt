#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 101
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.8263904229442195, 'errorList': [], 'lossList': [0.0, -1.423803671002388, 0.0, 30.839019861221313, 0.0, 0.0, 0.0], 'rewardMean': 0.8207202778928134, 'totalEpisodes': 80, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.14872378408234
'totalSteps': 3840, 'rewardStep': 0.7724450261714081, 'errorList': [], 'lossList': [0.0, -1.4134076070785522, 0.0, 44.10836166381836, 0.0, 0.0, 0.0], 'rewardMean': 0.8046285273190117, 'totalEpisodes': 114, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.294885574982964
'totalSteps': 5120, 'rewardStep': 0.9553546429696972, 'errorList': [], 'lossList': [0.0, -1.3995651870965957, 0.0, 54.26612897872925, 0.0, 0.0, 0.0], 'rewardMean': 0.8423100562316831, 'totalEpisodes': 149, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.724324526936847
'totalSteps': 6400, 'rewardStep': 0.775801457255524, 'errorList': [], 'lossList': [0.0, -1.4007840263843536, 0.0, 54.56540893554688, 0.0, 0.0, 0.0], 'rewardMean': 0.8290083364364513, 'totalEpisodes': 162, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.964969209752408
'totalSteps': 7680, 'rewardStep': 0.8097056093905344, 'errorList': [], 'lossList': [0.0, -1.3981623148918152, 0.0, 28.103211373090744, 0.0, 0.0, 0.0], 'rewardMean': 0.8257912152621318, 'totalEpisodes': 166, 'stepsPerEpisode': 37, 'rewardPerEpisode': 32.03294172625363
'totalSteps': 8960, 'rewardStep': 0.8148609508513998, 'errorList': [], 'lossList': [0.0, -1.3781604284048081, 0.0, 48.203308029174806, 0.0, 0.0, 0.0], 'rewardMean': 0.8242297489177416, 'totalEpisodes': 176, 'stepsPerEpisode': 115, 'rewardPerEpisode': 98.60609380946187
'totalSteps': 10240, 'rewardStep': 0.7297607988058261, 'errorList': [], 'lossList': [0.0, -1.3496508646011351, 0.0, 16.052125769853593, 0.0, 0.0, 0.0], 'rewardMean': 0.8124211301537521, 'totalEpisodes': 184, 'stepsPerEpisode': 57, 'rewardPerEpisode': 48.10044621075216
'totalSteps': 11520, 'rewardStep': 0.7561046890068485, 'errorList': [], 'lossList': [0.0, -1.322327001094818, 0.0, 15.579910737276077, 0.0, 0.0, 0.0], 'rewardMean': 0.8061637478040962, 'totalEpisodes': 189, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.183100619668604
'totalSteps': 12800, 'rewardStep': 0.8472347681738887, 'errorList': [], 'lossList': [0.0, -1.3128519022464753, 0.0, 27.21261981725693, 0.0, 0.0, 0.0], 'rewardMean': 0.8102708498410754, 'totalEpisodes': 195, 'stepsPerEpisode': 175, 'rewardPerEpisode': 151.4217845224599
'totalSteps': 14080, 'rewardStep': 0.8101174730146365, 'errorList': [], 'lossList': [0.0, -1.3120740681886673, 0.0, 7.05372882604599, 0.0, 0.0, 0.0], 'rewardMean': 0.8097775838583983, 'totalEpisodes': 200, 'stepsPerEpisode': 95, 'rewardPerEpisode': 81.39501552703612
'totalSteps': 15360, 'rewardStep': 0.8338563322185656, 'errorList': [], 'lossList': [0.0, -1.3154479855298995, 0.0, 5.362261139154434, 0.0, 0.0, 0.0], 'rewardMean': 0.8105241747858329, 'totalEpisodes': 204, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.024545628934014
'totalSteps': 16640, 'rewardStep': 0.8165845602270666, 'errorList': [], 'lossList': [0.0, -1.295947772860527, 0.0, 3.661264568567276, 0.0, 0.0, 0.0], 'rewardMean': 0.8149381281913988, 'totalEpisodes': 208, 'stepsPerEpisode': 181, 'rewardPerEpisode': 155.58697345763505
'totalSteps': 17920, 'rewardStep': 0.8783931161512583, 'errorList': [], 'lossList': [0.0, -1.2683325815200805, 0.0, 2.8547194612026217, 0.0, 0.0, 0.0], 'rewardMean': 0.8072419755095547, 'totalEpisodes': 209, 'stepsPerEpisode': 50, 'rewardPerEpisode': 44.321471555296974
'totalSteps': 19200, 'rewardStep': 0.8830375800167898, 'errorList': [], 'lossList': [0.0, -1.2245667421817779, 0.0, 3.9189026987552644, 0.0, 0.0, 0.0], 'rewardMean': 0.8179655877856813, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1045.4267677187584
'totalSteps': 20480, 'rewardStep': 0.5211154790452955, 'errorList': [], 'lossList': [0.0, -1.206419290304184, 0.0, 3.8732085645198824, 0.0, 0.0, 0.0], 'rewardMean': 0.7891065747511576, 'totalEpisodes': 212, 'stepsPerEpisode': 160, 'rewardPerEpisode': 124.06716238734653
'totalSteps': 21760, 'rewardStep': 0.7296398513456548, 'errorList': [], 'lossList': [0.0, -1.2003613078594209, 0.0, 3.936130896806717, 0.0, 0.0, 0.0], 'rewardMean': 0.7805844648005831, 'totalEpisodes': 215, 'stepsPerEpisode': 119, 'rewardPerEpisode': 98.1351437529988
'totalSteps': 23040, 'rewardStep': 0.8165843944848397, 'errorList': [], 'lossList': [0.0, -1.1943639361858367, 0.0, 1.5130260591208935, 0.0, 0.0, 0.0], 'rewardMean': 0.7892668243684845, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.5579035522433
'totalSteps': 24320, 'rewardStep': 0.7981309006829811, 'errorList': [], 'lossList': [0.0, -1.1644188690185546, 0.0, 0.7338278271257878, 0.0, 0.0, 0.0], 'rewardMean': 0.7934694455360976, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.2582911326413
'totalSteps': 25600, 'rewardStep': 0.9672117466592215, 'errorList': [0.04425783372991727, 0.04744240916335239, 0.05364852874581402, 0.04399152096748622, 0.047697643066373085, 0.06803366612777158, 0.05834671431067872, 0.05672458742423418, 0.06907791726037806, 0.052870587256465096, 0.04716461610206613, 0.05896263603818206, 0.060416712606231, 0.051030521478473485, 0.04953866276938895, 0.05877769450524547, 0.05626280763267225, 0.04892671935974859, 0.06261637860482627, 0.05823639059258985, 0.058385780823461166, 0.05024934387941768, 0.051460167688813264, 0.06910749349431981, 0.05049982639163052, 0.053733095649504715, 0.06861302176799336, 0.059752530979137464, 0.05847041713332332, 0.0492571172924225, 0.06133910778187047, 0.044766144547588795, 0.045802431634319955, 0.06635981363534862, 0.059194204997124765, 0.05203118129110951, 0.08093702561323927, 0.05146876402869639, 0.05600190699834681, 0.07062134854439849, 0.050899464112107856, 0.049887474252253594, 0.05091748725064726, 0.05987766454845787, 0.05404935118722029, 0.05755832310105817, 0.05025819648270189, 0.055206770033447095, 0.05952143663952683, 0.04721784824415911], 'lossList': [0.0, -1.0942685067653657, 0.0, 0.5442698394507169, 0.0, 0.0, 0.0], 'rewardMean': 0.8054671433846311, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.036853390476, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.74
