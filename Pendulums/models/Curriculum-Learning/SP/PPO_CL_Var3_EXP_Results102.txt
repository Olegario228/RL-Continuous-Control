#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 102
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.8955364219375933, 'errorList': [], 'lossList': [0.0, -1.4260788875818253, 0.0, 32.82947927474976, 0.0, 0.0, 0.0], 'rewardMean': 0.8555868818606513, 'totalEpisodes': 89, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.82511232911037
'totalSteps': 3840, 'rewardStep': 0.8317341247036292, 'errorList': [], 'lossList': [0.0, -1.3927227646112441, 0.0, 36.93219144821167, 0.0, 0.0, 0.0], 'rewardMean': 0.8476359628083107, 'totalEpisodes': 148, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.412928250075135
'totalSteps': 5120, 'rewardStep': 0.5800655314896483, 'errorList': [], 'lossList': [0.0, -1.361802392601967, 0.0, 43.00903951644897, 0.0, 0.0, 0.0], 'rewardMean': 0.7807433549786451, 'totalEpisodes': 187, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.098468251436223
'totalSteps': 6400, 'rewardStep': 0.8116557201396728, 'errorList': [], 'lossList': [0.0, -1.353259773850441, 0.0, 50.24712847709656, 0.0, 0.0, 0.0], 'rewardMean': 0.7869258280108506, 'totalEpisodes': 206, 'stepsPerEpisode': 34, 'rewardPerEpisode': 24.452832725136638
'totalSteps': 7680, 'rewardStep': 0.8571204744403198, 'errorList': [], 'lossList': [0.0, -1.3403774869441987, 0.0, 48.06156788825989, 0.0, 0.0, 0.0], 'rewardMean': 0.7986249357490954, 'totalEpisodes': 220, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.536560662303735
'totalSteps': 8960, 'rewardStep': 0.7141512576010789, 'errorList': [], 'lossList': [0.0, -1.3307455092668534, 0.0, 31.477647998332976, 0.0, 0.0, 0.0], 'rewardMean': 0.7865572674422359, 'totalEpisodes': 229, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.954368284947215
'totalSteps': 10240, 'rewardStep': 0.6069802485676213, 'errorList': [], 'lossList': [0.0, -1.3290277206897736, 0.0, 11.329975827932358, 0.0, 0.0, 0.0], 'rewardMean': 0.7641101400829091, 'totalEpisodes': 235, 'stepsPerEpisode': 35, 'rewardPerEpisode': 26.663968952569935
'totalSteps': 11520, 'rewardStep': 0.8486841678353638, 'errorList': [], 'lossList': [0.0, -1.333621653318405, 0.0, 12.711815093755723, 0.0, 0.0, 0.0], 'rewardMean': 0.7735072542776263, 'totalEpisodes': 237, 'stepsPerEpisode': 511, 'rewardPerEpisode': 429.62502807836665
'totalSteps': 12800, 'rewardStep': 0.9255806724355339, 'errorList': [], 'lossList': [0.0, -1.3335007220506667, 0.0, 21.32173404455185, 0.0, 0.0, 0.0], 'rewardMean': 0.7887145960934171, 'totalEpisodes': 240, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.17746256722938
'totalSteps': 14080, 'rewardStep': 0.8943996718710163, 'errorList': [], 'lossList': [0.0, -1.3288105428218842, 0.0, 8.129301652312279, 0.0, 0.0, 0.0], 'rewardMean': 0.7965908291021477, 'totalEpisodes': 241, 'stepsPerEpisode': 399, 'rewardPerEpisode': 353.8131116235383
'totalSteps': 15360, 'rewardStep': 0.9361299425702735, 'errorList': [0.14803558712198614, 0.1512754523834359, 0.15052647064804284, 0.1464379134406844, 0.15313690020289866, 0.15202902840122004, 0.15345076732624602, 0.15409602576360407, 0.15232395756712877, 0.2658749182025116, 0.15006586150735532, 0.1941541104428382, 0.44660305863885913, 0.15401012918289347, 0.24907184204722782, 0.2356216529853212, 0.30381824545419267, 0.2893739342863735, 0.1489291519184809, 0.39406946827126105, 0.2960128375952196, 0.14882965413273394, 0.14652293996415555, 0.14950365983965713, 0.1583455141311208, 0.144909948557267, 0.2516448273320531, 0.14985018059860505, 0.15160953038336025, 0.14324700295995896, 0.14468269438527584, 0.2632171806363378, 0.29336549379873766, 0.14944019822119728, 0.1400854128356967, 0.1470929378233165, 0.14181800083472487, 0.1466297279913282, 0.1817354739895291, 0.13942377640239, 0.14908257674955, 0.15135454984374613, 0.18433525462126588, 0.1770873553437902, 0.14877567621774324, 0.28385542637098543, 0.14882892054240937, 0.1495068753787673, 0.1645204938195884, 0.18709993187959637], 'lossList': [0.0, -1.3313580161333085, 0.0, 4.498941769897938, 0.0, 0.0, 0.0], 'rewardMean': 0.8006501811654158, 'totalEpisodes': 243, 'stepsPerEpisode': 154, 'rewardPerEpisode': 132.51785630742668, 'successfulTests': 38
'totalSteps': 16640, 'rewardStep': 0.8953495855417384, 'errorList': [], 'lossList': [0.0, -1.322568098306656, 0.0, 4.12591556429863, 0.0, 0.0, 0.0], 'rewardMean': 0.8070117272492267, 'totalEpisodes': 243, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 805.3312860257155
'totalSteps': 17920, 'rewardStep': 0.7222156650351558, 'errorList': [], 'lossList': [0.0, -1.2987168663740158, 0.0, 3.8243528097867965, 0.0, 0.0, 0.0], 'rewardMean': 0.8212267406037773, 'totalEpisodes': 243, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 913.805405614187
'totalSteps': 19200, 'rewardStep': 0.9552657244274079, 'errorList': [0.06587322105816208, 0.039450689906536184, 0.03488726716140844, 0.05635321650028985, 0.04099583802813208, 0.03649771899918424, 0.04273304804505804, 0.03725903233711315, 0.0755575729103292, 0.06006115929640688, 0.06498529866471008, 0.041801171684274344, 0.04972924655647422, 0.05664415804005118, 0.03166277955889248, 0.0383021893805282, 0.05994630668738046, 0.024766297589525792, 0.030800769599494175, 0.03488318905623432, 0.10245248431204791, 0.07267363420574191, 0.08087039231276764, 0.03677054481924444, 0.04957913915645391, 0.05136225556476256, 0.03429599379989242, 0.033244173008862135, 0.044422572776643104, 0.08283930532605385, 0.07960847690168207, 0.0666999539792246, 0.051280939520145695, 0.02657658044150488, 0.040758560064456786, 0.02586131482087724, 0.04880421984983393, 0.0704263168046511, 0.0630580430069715, 0.04570330658722537, 0.05906186401745049, 0.04604320042938823, 0.04174476235207516, 0.03066999624863154, 0.05776222057490696, 0.0530630349873519, 0.04813151131893824, 0.031014441121473266, 0.05699478892793204, 0.06687400670015912], 'lossList': [0.0, -1.2827142584323883, 0.0, 2.9154966044425965, 0.0, 0.0, 0.0], 'rewardMean': 0.835587741032551, 'totalEpisodes': 243, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.6199545060481, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=88.74
