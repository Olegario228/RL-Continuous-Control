#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 103
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.7784348380676673, 'errorList': [], 'lossList': [0.0, -1.4211796534061432, 0.0, 28.8062703704834, 0.0, 0.0, 0.0], 'rewardMean': 0.641687032404026, 'totalEpisodes': 85, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.585366224579618
'totalSteps': 3840, 'rewardStep': 0.9629373010164729, 'errorList': [], 'lossList': [0.0, -1.402101041674614, 0.0, 41.368432712554934, 0.0, 0.0, 0.0], 'rewardMean': 0.7487704552748417, 'totalEpisodes': 131, 'stepsPerEpisode': 15, 'rewardPerEpisode': 14.085110926116787
'totalSteps': 5120, 'rewardStep': 0.6229447830133539, 'errorList': [], 'lossList': [0.0, -1.392119728922844, 0.0, 45.278998355865475, 0.0, 0.0, 0.0], 'rewardMean': 0.7173140372094697, 'totalEpisodes': 157, 'stepsPerEpisode': 76, 'rewardPerEpisode': 56.52830466573156
'totalSteps': 6400, 'rewardStep': 0.8047020974448154, 'errorList': [], 'lossList': [0.0, -1.3734780311584474, 0.0, 54.856206035614015, 0.0, 0.0, 0.0], 'rewardMean': 0.734791649256539, 'totalEpisodes': 170, 'stepsPerEpisode': 26, 'rewardPerEpisode': 16.417496566244434
'totalSteps': 7680, 'rewardStep': 0.9283212267795958, 'errorList': [], 'lossList': [0.0, -1.3544445276260375, 0.0, 46.77269173622131, 0.0, 0.0, 0.0], 'rewardMean': 0.767046578843715, 'totalEpisodes': 180, 'stepsPerEpisode': 97, 'rewardPerEpisode': 74.96022066014913
'totalSteps': 8960, 'rewardStep': 0.9216113869103434, 'errorList': [], 'lossList': [0.0, -1.3449594247341157, 0.0, 39.56238922595978, 0.0, 0.0, 0.0], 'rewardMean': 0.7891272657103762, 'totalEpisodes': 186, 'stepsPerEpisode': 122, 'rewardPerEpisode': 98.41565058697556
'totalSteps': 10240, 'rewardStep': 0.9490769632554171, 'errorList': [60.809221087231954, 48.729839089278556, 99.87913849387637, 35.21047781771687, 49.93283816377139, 93.30154814780774, 51.81098983692519, 99.27017471102715, 15.502667814330643, 118.60409724389501, 15.496388295384413, 16.64476580578245, 70.55111152006607, 12.408011682806462, 47.66977385689278, 17.062015555832303, 41.587412574701844, 68.11333383293118, 98.7426775235208, 3.5051537272852418, 105.55897676213411, 18.706012941806257, 107.20284922517043, 111.23056402465564, 34.65385109897962, 34.78752625653374, 63.420310370441385, 67.4253241554323, 128.14723820748435, 81.08493762841842, 103.54708263827241, 159.70216120144454, 60.65676911879085, 42.90371910254576, 65.99356539395534, 98.79775321590789, 57.77463630726693, 103.30426840982477, 50.438874016896094, 48.67607932298998, 102.77079429954541, 6.6126180163843875, 48.73445846213592, 150.25905513106204, 79.93022743346134, 38.48542566382936, 60.52591668540807, 130.42127822120804, 38.13716761223556, 65.18640382629606], 'lossList': [0.0, -1.3269154846668243, 0.0, 31.67875569343567, 0.0, 0.0, 0.0], 'rewardMean': 0.8091209779035063, 'totalEpisodes': 193, 'stepsPerEpisode': 161, 'rewardPerEpisode': 145.42682307770497, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5065258059893102, 'errorList': [], 'lossList': [0.0, -1.3077509748935698, 0.0, 19.124490913152695, 0.0, 0.0, 0.0], 'rewardMean': 0.7754992921352623, 'totalEpisodes': 198, 'stepsPerEpisode': 65, 'rewardPerEpisode': 44.94358508283882
'totalSteps': 12800, 'rewardStep': 0.6254466240312009, 'errorList': [], 'lossList': [0.0, -1.3004988598823548, 0.0, 11.784255948066711, 0.0, 0.0, 0.0], 'rewardMean': 0.7604940253248562, 'totalEpisodes': 204, 'stepsPerEpisode': 172, 'rewardPerEpisode': 146.23113996596715
'totalSteps': 14080, 'rewardStep': 0.9644730240452044, 'errorList': [10.94241427634952, 2.8586496069680924, 1.2455366500889267, 10.591856373152748, 3.0640100342608596, 27.225716609325243, 31.29901247328882, 18.425211058066665, 29.60884231692797, 3.7203703484713855, 1.935625472049799, 2.5180116416550833, 9.948787789846687, 18.322093538980116, 5.623125736954574, 37.896238315366475, 47.78958095536592, 24.076543680028585, 5.849315352781834, 26.111060016669487, 29.91476618154957, 1.5622318059689877, 26.735016743159083, 23.716882792284004, 15.457599748807874, 41.839170950812694, 7.497228962580815, 22.499921193846035, 20.601553688034166, 12.37899045556928, 7.6421926238023135, 10.739356733269107, 6.426756892024652, 17.913118886111324, 28.887249025135954, 6.515794075541229, 1.9942908415559577, 14.754390394587494, 2.7863428613076735, 13.432422138128306, 24.24069225942927, 7.056886301938572, 10.065672883628997, 12.778812234834595, 24.467508738497283, 5.445011849328938, 22.448954972606373, 2.129793306778402, 14.035067258812017, 25.406183103143565], 'lossList': [0.0, -1.2824668037891387, 0.0, 9.134920339584351, 0.0, 0.0, 0.0], 'rewardMean': 0.8064474050553381, 'totalEpisodes': 210, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.6470704121818, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7438543063642215, 'errorList': [], 'lossList': [0.0, -1.2776847827434539, 0.0, 6.732933201193809, 0.0, 0.0, 0.0], 'rewardMean': 0.8029893518849937, 'totalEpisodes': 214, 'stepsPerEpisode': 191, 'rewardPerEpisode': 168.2459338782468
'totalSteps': 16640, 'rewardStep': 0.5865687336651684, 'errorList': [], 'lossList': [0.0, -1.2932566338777542, 0.0, 4.122384214997291, 0.0, 0.0, 0.0], 'rewardMean': 0.7653524951498631, 'totalEpisodes': 218, 'stepsPerEpisode': 176, 'rewardPerEpisode': 131.47962240222535
'totalSteps': 17920, 'rewardStep': 0.894193510558368, 'errorList': [], 'lossList': [0.0, -1.3091731697320939, 0.0, 3.8475453463196754, 0.0, 0.0, 0.0], 'rewardMean': 0.7924773679043644, 'totalEpisodes': 220, 'stepsPerEpisode': 256, 'rewardPerEpisode': 231.75764626008802
'totalSteps': 19200, 'rewardStep': 0.6763536440025981, 'errorList': [], 'lossList': [0.0, -1.3190976226329802, 0.0, 3.1048565661907195, 0.0, 0.0, 0.0], 'rewardMean': 0.7796425225601429, 'totalEpisodes': 221, 'stepsPerEpisode': 699, 'rewardPerEpisode': 581.7911754392317
'totalSteps': 20480, 'rewardStep': 0.9036152272344349, 'errorList': [], 'lossList': [0.0, -1.2752812272310257, 0.0, 1.7556099744141103, 0.0, 0.0, 0.0], 'rewardMean': 0.7771719226056267, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.083948208571
'totalSteps': 21760, 'rewardStep': 0.8181474606924943, 'errorList': [], 'lossList': [0.0, -1.2223656040430069, 0.0, 1.5671553102135658, 0.0, 0.0, 0.0], 'rewardMean': 0.7668255299838418, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 961.82177946468
'totalSteps': 23040, 'rewardStep': 0.8118934203589963, 'errorList': [], 'lossList': [0.0, -1.1759160763025285, 0.0, 0.6881213445588946, 0.0, 0.0, 0.0], 'rewardMean': 0.7531071756941997, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.4386064109397
'totalSteps': 24320, 'rewardStep': 0.8688670719761417, 'errorList': [], 'lossList': [0.0, -1.1233754128217697, 0.0, 0.6163259116746486, 0.0, 0.0, 0.0], 'rewardMean': 0.7893413022928828, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1160.8705672385452
'totalSteps': 25600, 'rewardStep': 0.9596453789133198, 'errorList': [0.06834461862791752, 0.05067328027534914, 0.08884037365038322, 0.05772456866375163, 0.06973204454085237, 0.09192673176647881, 0.05871934031674121, 0.0849567843178447, 0.05232415236617711, 0.057889574543368474, 0.10820217407794934, 0.07756722266401572, 0.10362482309377975, 0.10993692323828429, 0.08771473898228743, 0.0873424173889378, 0.0742346600038299, 0.05245552348825013, 0.04946209854089575, 0.075988835313932, 0.05912043162938492, 0.050410500276381505, 0.05287230215008535, 0.06075195969004988, 0.05284098022056216, 0.04879726693771429, 0.07534260517044528, 0.07098993600411942, 0.11682248855695493, 0.04695546507143364, 0.05694784852366197, 0.10034684682460775, 0.05320722669816498, 0.09908584956134196, 0.04557573366173969, 0.04976793266225052, 0.05898542970742633, 0.062390575736259876, 0.08561757826223626, 0.07800517804483834, 0.10170952813331963, 0.05634473996890011, 0.05119861065139017, 0.09629260674117683, 0.05887314398074949, 0.04775605740711002, 0.07362675744576301, 0.10567105257315908, 0.07734612398703751, 0.0585328349148213], 'lossList': [0.0, -1.099763746857643, 0.0, 0.5526116824150086, 0.0, 0.0, 0.0], 'rewardMean': 0.8227611777810946, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1195.1922273876894, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=122.85
