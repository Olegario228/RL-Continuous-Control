#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 106
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6772268021562856, 'errorList': [], 'lossList': [0.0, -1.4070948404073715, 0.0, 36.10465436935425, 0.0, 0.0, 0.0], 'rewardMean': 0.8202581770334977, 'totalEpisodes': 42, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.96558278575244
'totalSteps': 3840, 'rewardStep': 0.5606127030350134, 'errorList': [], 'lossList': [0.0, -1.406554439663887, 0.0, 52.63444013595581, 0.0, 0.0, 0.0], 'rewardMean': 0.7337096857006696, 'totalEpisodes': 87, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.738343891486345
'totalSteps': 5120, 'rewardStep': 0.9207522422473976, 'errorList': [], 'lossList': [0.0, -1.4042699217796326, 0.0, 50.73676714897156, 0.0, 0.0, 0.0], 'rewardMean': 0.7804703248373516, 'totalEpisodes': 121, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.875342114449555
'totalSteps': 6400, 'rewardStep': 0.5485356725420967, 'errorList': [], 'lossList': [0.0, -1.3906857371330261, 0.0, 52.970195846557615, 0.0, 0.0, 0.0], 'rewardMean': 0.7340833943783005, 'totalEpisodes': 137, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.462503594784167
'totalSteps': 7680, 'rewardStep': 0.4005093344519296, 'errorList': [], 'lossList': [0.0, -1.371153935790062, 0.0, 30.21074898004532, 0.0, 0.0, 0.0], 'rewardMean': 0.6784877177239054, 'totalEpisodes': 140, 'stepsPerEpisode': 270, 'rewardPerEpisode': 183.1374458318298
'totalSteps': 8960, 'rewardStep': 0.7752363125277729, 'errorList': [], 'lossList': [0.0, -1.3472555440664291, 0.0, 44.34621795654297, 0.0, 0.0, 0.0], 'rewardMean': 0.6923089455530294, 'totalEpisodes': 149, 'stepsPerEpisode': 109, 'rewardPerEpisode': 80.61312865108705
'totalSteps': 10240, 'rewardStep': 0.762355038035091, 'errorList': [], 'lossList': [0.0, -1.3391197228431702, 0.0, 23.76733989238739, 0.0, 0.0, 0.0], 'rewardMean': 0.7010647071132872, 'totalEpisodes': 154, 'stepsPerEpisode': 54, 'rewardPerEpisode': 44.32437940174496
'totalSteps': 11520, 'rewardStep': 0.4522934349730341, 'errorList': [], 'lossList': [0.0, -1.34224867105484, 0.0, 18.614797642230986, 0.0, 0.0, 0.0], 'rewardMean': 0.6734234546532591, 'totalEpisodes': 157, 'stepsPerEpisode': 128, 'rewardPerEpisode': 96.75716188878569
'totalSteps': 12800, 'rewardStep': 0.7740784544134137, 'errorList': [], 'lossList': [0.0, -1.330937486886978, 0.0, 13.971479510068894, 0.0, 0.0, 0.0], 'rewardMean': 0.6834889546292746, 'totalEpisodes': 162, 'stepsPerEpisode': 96, 'rewardPerEpisode': 75.67646917380112
'totalSteps': 14080, 'rewardStep': 0.6082783922007098, 'errorList': [], 'lossList': [0.0, -1.3006520062685012, 0.0, 10.167656444311142, 0.0, 0.0, 0.0], 'rewardMean': 0.6479878386582745, 'totalEpisodes': 163, 'stepsPerEpisode': 890, 'rewardPerEpisode': 675.8310982804281
'totalSteps': 15360, 'rewardStep': 0.676268513571419, 'errorList': [], 'lossList': [0.0, -1.2781562328338623, 0.0, 6.636306091547012, 0.0, 0.0, 0.0], 'rewardMean': 0.6478920097997877, 'totalEpisodes': 164, 'stepsPerEpisode': 637, 'rewardPerEpisode': 414.83091775292417
'totalSteps': 16640, 'rewardStep': 0.8318648882576986, 'errorList': [], 'lossList': [0.0, -1.2736449003219605, 0.0, 13.390018569231033, 0.0, 0.0, 0.0], 'rewardMean': 0.6750172283220564, 'totalEpisodes': 167, 'stepsPerEpisode': 146, 'rewardPerEpisode': 129.5347542186932
'totalSteps': 17920, 'rewardStep': 0.7899707294941791, 'errorList': [], 'lossList': [0.0, -1.251631770133972, 0.0, 2.585302580296993, 0.0, 0.0, 0.0], 'rewardMean': 0.6619390770467345, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.9913617771359
'totalSteps': 19200, 'rewardStep': 0.7685837249808072, 'errorList': [], 'lossList': [0.0, -1.2089000254869462, 0.0, 1.5156760317087175, 0.0, 0.0, 0.0], 'rewardMean': 0.6839438822906054, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 992.2114967991007
'totalSteps': 20480, 'rewardStep': 0.7221743388852231, 'errorList': [], 'lossList': [0.0, -1.1734838324785233, 0.0, 1.7844066532701255, 0.0, 0.0, 0.0], 'rewardMean': 0.7161103827339348, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.5964912963998
'totalSteps': 21760, 'rewardStep': 0.8337720408760627, 'errorList': [], 'lossList': [0.0, -1.125310583114624, 0.0, 1.1561346405372024, 0.0, 0.0, 0.0], 'rewardMean': 0.7219639555687639, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.7916436288629
'totalSteps': 23040, 'rewardStep': 0.9747976580978174, 'errorList': [0.0843865538775159, 0.09012503463294673, 0.06887034411861358, 0.07496984811301992, 0.10605973601203429, 0.07307018051873694, 0.08523632386902066, 0.0867268679746848, 0.0547947355387754, 0.08760011391017351, 0.06404179478519825, 0.05803486360803425, 0.07355779297436321, 0.07932065401208602, 0.10490469512298606, 0.07161010714107696, 0.08803992994816584, 0.06037314212047691, 0.07023298552020782, 0.07157657433727925, 0.0946133922665709, 0.0860966288030014, 0.07610970768278556, 0.09171561799064429, 0.07692269978994459, 0.05369919679926008, 0.08904250555276126, 0.07572685916783413, 0.05923285988226653, 0.06763013424151888, 0.06338174691974849, 0.06986513239220975, 0.09667258167748856, 0.08785766618867434, 0.06395468043843834, 0.05903416212196037, 0.08834891678326301, 0.06662899120794251, 0.08402082099015877, 0.07824128529628693, 0.07023407721935199, 0.09126630702270742, 0.08667683846784065, 0.12050869054552416, 0.0896103292024819, 0.07651512303614903, 0.07674097067308469, 0.07443106318046598, 0.07426070430041354, 0.05723109574859172], 'lossList': [0.0, -1.0957595193386078, 0.0, 0.7849491160362959, 0.0, 0.0, 0.0], 'rewardMean': 0.7432082175750365, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.281259823939, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=81.02
