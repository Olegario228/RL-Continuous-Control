#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 107
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.7784139349714735, 'errorList': [], 'lossList': [0.0, -1.4180474573373794, 0.0, 35.213380641937256, 0.0, 0.0, 0.0], 'rewardMean': 0.7860919680758123, 'totalEpisodes': 50, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.68136144810074
'totalSteps': 3840, 'rewardStep': 0.6047545189361716, 'errorList': [], 'lossList': [0.0, -1.3947923493385315, 0.0, 53.65487155914307, 0.0, 0.0, 0.0], 'rewardMean': 0.725646151695932, 'totalEpisodes': 97, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.141031863724569
'totalSteps': 5120, 'rewardStep': 0.8064365665435607, 'errorList': [], 'lossList': [0.0, -1.3618830442428589, 0.0, 56.42385276794434, 0.0, 0.0, 0.0], 'rewardMean': 0.7458437554078392, 'totalEpisodes': 137, 'stepsPerEpisode': 47, 'rewardPerEpisode': 39.06842671636104
'totalSteps': 6400, 'rewardStep': 0.5990748460857941, 'errorList': [], 'lossList': [0.0, -1.3464303743839263, 0.0, 43.34385370254517, 0.0, 0.0, 0.0], 'rewardMean': 0.7164899735434302, 'totalEpisodes': 161, 'stepsPerEpisode': 26, 'rewardPerEpisode': 19.348228574822947
'totalSteps': 7680, 'rewardStep': 0.8523511872309527, 'errorList': [], 'lossList': [0.0, -1.3316827005147933, 0.0, 43.84984721183777, 0.0, 0.0, 0.0], 'rewardMean': 0.7391335091580173, 'totalEpisodes': 174, 'stepsPerEpisode': 56, 'rewardPerEpisode': 38.89506573206796
'totalSteps': 8960, 'rewardStep': 0.458277264856497, 'errorList': [], 'lossList': [0.0, -1.3245277082920075, 0.0, 29.787047204971312, 0.0, 0.0, 0.0], 'rewardMean': 0.6990111885435145, 'totalEpisodes': 182, 'stepsPerEpisode': 135, 'rewardPerEpisode': 102.06923940330392
'totalSteps': 10240, 'rewardStep': 0.6306718959912802, 'errorList': [], 'lossList': [0.0, -1.3304500967264175, 0.0, 17.142424132823944, 0.0, 0.0, 0.0], 'rewardMean': 0.690468776974485, 'totalEpisodes': 187, 'stepsPerEpisode': 300, 'rewardPerEpisode': 223.74355938124523
'totalSteps': 11520, 'rewardStep': 0.5592668881305336, 'errorList': [], 'lossList': [0.0, -1.3243913066387176, 0.0, 9.020271739959718, 0.0, 0.0, 0.0], 'rewardMean': 0.6758907893251571, 'totalEpisodes': 192, 'stepsPerEpisode': 275, 'rewardPerEpisode': 212.05492596845664
'totalSteps': 12800, 'rewardStep': 0.7587447886383214, 'errorList': [], 'lossList': [0.0, -1.3030250370502472, 0.0, 21.8783212351799, 0.0, 0.0, 0.0], 'rewardMean': 0.6841761892564735, 'totalEpisodes': 198, 'stepsPerEpisode': 219, 'rewardPerEpisode': 177.8271025790217
'totalSteps': 14080, 'rewardStep': 0.8085223054500524, 'errorList': [], 'lossList': [0.0, -1.321437821984291, 0.0, 6.988150399923325, 0.0, 0.0, 0.0], 'rewardMean': 0.6856514196834638, 'totalEpisodes': 203, 'stepsPerEpisode': 131, 'rewardPerEpisode': 104.79126739383439
'totalSteps': 15360, 'rewardStep': 0.632182602628527, 'errorList': [], 'lossList': [0.0, -1.3275215250253678, 0.0, 5.780129460096359, 0.0, 0.0, 0.0], 'rewardMean': 0.6710282864491691, 'totalEpisodes': 205, 'stepsPerEpisode': 419, 'rewardPerEpisode': 295.94253130008656
'totalSteps': 16640, 'rewardStep': 0.5550905567533089, 'errorList': [], 'lossList': [0.0, -1.3053136032819748, 0.0, 4.362677819728852, 0.0, 0.0, 0.0], 'rewardMean': 0.6660618902308828, 'totalEpisodes': 209, 'stepsPerEpisode': 165, 'rewardPerEpisode': 119.22739708964086
'totalSteps': 17920, 'rewardStep': 0.6671840650961718, 'errorList': [], 'lossList': [0.0, -1.2884921783208847, 0.0, 3.739575915336609, 0.0, 0.0, 0.0], 'rewardMean': 0.6521366400861439, 'totalEpisodes': 211, 'stepsPerEpisode': 651, 'rewardPerEpisode': 521.1190778585847
'totalSteps': 19200, 'rewardStep': 0.7932150526503966, 'errorList': [], 'lossList': [0.0, -1.2639969295263291, 0.0, 5.86257283449173, 0.0, 0.0, 0.0], 'rewardMean': 0.6715506607426042, 'totalEpisodes': 214, 'stepsPerEpisode': 176, 'rewardPerEpisode': 150.91043170356673
'totalSteps': 20480, 'rewardStep': 0.7951039555219523, 'errorList': [], 'lossList': [0.0, -1.2448310846090316, 0.0, 4.855183758735657, 0.0, 0.0, 0.0], 'rewardMean': 0.6658259375717042, 'totalEpisodes': 216, 'stepsPerEpisode': 208, 'rewardPerEpisode': 185.70532230178645
'totalSteps': 21760, 'rewardStep': 0.8094056311708291, 'errorList': [], 'lossList': [0.0, -1.2031694394350052, 0.0, 1.0609008891135454, 0.0, 0.0, 0.0], 'rewardMean': 0.7009387742031373, 'totalEpisodes': 216, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.6384024938227
'totalSteps': 23040, 'rewardStep': 0.94408102790202, 'errorList': [0.1220787526202734, 0.04450009871582133, 0.022017215823029047, 0.10878251252331717, 0.08103199031693764, 0.10332774877939173, 0.04891513233446979, 0.09530942787007167, 0.0644136261046117, 0.03566416991793427, 0.01513610292575711, 0.01333472361855928, 0.050635614843046975, 0.06631244597559408, 0.03879434464564848, 0.04433874664793379, 0.09247186364330874, 0.012947622128184968, 0.07541169086261407, 0.028683352190728602, 0.05694614537529001, 0.08050111850324805, 0.037910480443347314, 0.10568273637371907, 0.03945795406542925, 0.07617476987991903, 0.03346914419378904, 0.04501923524944499, 0.05807909699429324, 0.04698282840198566, 0.0720509920016996, 0.0679946138052043, 0.05285871084366014, 0.017112863256618465, 0.03943369488181024, 0.0578506492406334, 0.10191889788360114, 0.03303614133712166, 0.037717386976025655, 0.09375096414515474, 0.04949040904502482, 0.05712318974363694, 0.059410853894548296, 0.012159393794111415, 0.053613681700708564, 0.09051609133006185, 0.07039203999275466, 0.025911061612458115, 0.02677950323602772, 0.060570426478862224], 'lossList': [0.0, -1.164637914299965, 0.0, 1.0283928760141134, 0.0, 0.0, 0.0], 'rewardMean': 0.7322796873942112, 'totalEpisodes': 216, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.5573213861621, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=79.92
