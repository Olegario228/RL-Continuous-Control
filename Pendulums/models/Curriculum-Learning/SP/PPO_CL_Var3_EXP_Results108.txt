#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 108
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.4067904111636161, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.3859786712908957, 'totalEpisodes': 48, 'stepsPerEpisode': 36, 'rewardPerEpisode': 26.49933221940125
'totalSteps': 3840, 'rewardStep': 0.43654094398769927, 'errorList': [], 'lossList': [0.0, -1.4253749245405196, 0.0, 29.7880223941803, 0.0, 0.0, 0.0], 'rewardMean': 0.3986192394650966, 'totalEpisodes': 107, 'stepsPerEpisode': 16, 'rewardPerEpisode': 9.55019187714568
'totalSteps': 5120, 'rewardStep': 0.5994034086381493, 'errorList': [], 'lossList': [0.0, -1.4153119605779647, 0.0, 45.817262201309205, 0.0, 0.0, 0.0], 'rewardMean': 0.43877607329970714, 'totalEpisodes': 148, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.370861839069024
'totalSteps': 6400, 'rewardStep': 0.6658663791013912, 'errorList': [], 'lossList': [0.0, -1.394518204331398, 0.0, 43.7636215209961, 0.0, 0.0, 0.0], 'rewardMean': 0.47662445759998784, 'totalEpisodes': 174, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.281017912917385
'totalSteps': 7680, 'rewardStep': 0.8922783770914107, 'errorList': [], 'lossList': [0.0, -1.3729868471622466, 0.0, 43.831564579010006, 0.0, 0.0, 0.0], 'rewardMean': 0.5360035889559054, 'totalEpisodes': 190, 'stepsPerEpisode': 52, 'rewardPerEpisode': 44.824464153543815
'totalSteps': 8960, 'rewardStep': 0.9666176983943267, 'errorList': [], 'lossList': [0.0, -1.3628698891401292, 0.0, 55.231888256073, 0.0, 0.0, 0.0], 'rewardMean': 0.589830352635708, 'totalEpisodes': 198, 'stepsPerEpisode': 106, 'rewardPerEpisode': 85.55224270412128
'totalSteps': 10240, 'rewardStep': 0.9334840925235923, 'errorList': [38.138687693966304, 3.7592842546235374, 0.30554362778035793, 11.485183274190879, 2.039913139007741, 2.615589706401172, 24.736057323913087, 39.11798798112937, 52.98574721386692, 9.364603767700695, 0.8663361206133856, 1.2857867819698117, 3.0469328166414864, 22.507861017294825, 25.726682239088834, 0.2311978382343362, 7.788237329728117, 5.52110313501545, 16.63434575874169, 0.18389546894643774, 37.57787645603355, 39.64380881319604, 23.061088850673745, 8.739894699750408, 20.870932228340138, 37.16351131481271, 2.3656306413616424, 6.925924190173377, 7.439624157079204, 23.74513669751191, 3.5760373245034796, 27.053190237313657, 1.0366763505012493, 10.944794408339742, 4.077302519648345, 4.907834022786479, 2.5632896396350318, 24.380854021206037, 31.18509609042626, 0.45246658185288435, 31.371572513867022, 22.568991845620186, 12.217317202217911, 4.594936022827181, 3.8367273514929243, 28.2192034334414, 8.098995025179374, 20.151900480705336, 13.616085235293856, 8.08114311443042], 'lossList': [0.0, -1.3585615986585617, 0.0, 39.84483775615692, 0.0, 0.0, 0.0], 'rewardMean': 0.6280141015121397, 'totalEpisodes': 205, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.071827260269842, 'successfulTests': 1
'totalSteps': 11520, 'rewardStep': 0.4255743429586986, 'errorList': [], 'lossList': [0.0, -1.3634974926710128, 0.0, 11.50945443570614, 0.0, 0.0, 0.0], 'rewardMean': 0.6077701256567956, 'totalEpisodes': 208, 'stepsPerEpisode': 71, 'rewardPerEpisode': 44.49901957009958
'totalSteps': 12800, 'rewardStep': 0.48502465071842044, 'errorList': [], 'lossList': [0.0, -1.3769945800304413, 0.0, 34.64424302577972, 0.0, 0.0, 0.0], 'rewardMean': 0.6218370715740921, 'totalEpisodes': 216, 'stepsPerEpisode': 121, 'rewardPerEpisode': 95.25816368246716
'totalSteps': 14080, 'rewardStep': 0.7391122916755867, 'errorList': [], 'lossList': [0.0, -1.3807735139131545, 0.0, 27.28597936630249, 0.0, 0.0, 0.0], 'rewardMean': 0.6550692596252892, 'totalEpisodes': 221, 'stepsPerEpisode': 91, 'rewardPerEpisode': 62.980201343769885
'totalSteps': 15360, 'rewardStep': 0.6056248738177336, 'errorList': [], 'lossList': [0.0, -1.3817925089597702, 0.0, 11.738804841041565, 0.0, 0.0, 0.0], 'rewardMean': 0.6749527058907009, 'totalEpisodes': 223, 'stepsPerEpisode': 514, 'rewardPerEpisode': 305.8502865959403
'totalSteps': 16640, 'rewardStep': 0.9385293689075522, 'errorList': [2.6185675776081996, 2.053450536841329, 1.529422351948017, 3.9260636167680873, 2.5686589198908205, 1.4407385332962903, 3.7496309413086264, 2.2721258953770027, 0.7685236006871382, 1.0075363598643825, 3.894833460211497, 1.747354891283798, 5.354570137953337, 3.2264839104501, 6.552837491746113, 4.622765983174986, 2.341639864622655, 2.0066495055547664, 1.1716055011426105, 2.0997472919362212, 0.3342137410795995, 2.033195729993407, 0.3886636135411746, 0.8803418049214561, 1.8672798607033663, 0.8296506434810429, 11.270197993794673, 2.3103411529862394, 0.08794439307515643, 4.941211730029049, 0.1272246945202511, 0.2077957084009788, 2.4674096997644845, 1.8737320616002757, 0.7254608591256035, 0.1008576559478524, 3.2126915505425124, 1.7991796181719109, 1.4501161861229253, 3.2322526336385264, 0.062273714022271366, 0.959606956531745, 0.12821743112113454, 0.12083745720510039, 0.13198969543683975, 5.11160396573265, 4.696776585881697, 0.49568686339977996, 0.9440033498063904, 2.0037121472732746], 'lossList': [0.0, -1.385014423727989, 0.0, 7.104470457881689, 0.0, 0.0, 0.0], 'rewardMean': 0.7251515483826863, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.2541534157374, 'successfulTests': 7
'totalSteps': 17920, 'rewardStep': 0.9298756648288558, 'errorList': [], 'lossList': [0.0, -1.3857822036743164, 0.0, 7.732891049385071, 0.0, 0.0, 0.0], 'rewardMean': 0.7581987740017568, 'totalEpisodes': 225, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.494601835178077
'totalSteps': 19200, 'rewardStep': 0.6684390218905472, 'errorList': [], 'lossList': [0.0, -1.3828437703847885, 0.0, 10.806862785816193, 0.0, 0.0, 0.0], 'rewardMean': 0.7584560382806724, 'totalEpisodes': 228, 'stepsPerEpisode': 193, 'rewardPerEpisode': 148.36208078094668
'totalSteps': 20480, 'rewardStep': 0.8469251191310629, 'errorList': [], 'lossList': [0.0, -1.3787519723176955, 0.0, 3.320538813173771, 0.0, 0.0, 0.0], 'rewardMean': 0.7539207124846377, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 878.870240413914
'totalSteps': 21760, 'rewardStep': 0.8645376073253983, 'errorList': [], 'lossList': [0.0, -1.3593411761522294, 0.0, 2.3972458222508433, 0.0, 0.0, 0.0], 'rewardMean': 0.7437127033777448, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 996.6909883200681
'totalSteps': 23040, 'rewardStep': 0.7721183415878295, 'errorList': [], 'lossList': [0.0, -1.31640833735466, 0.0, 1.412428026869893, 0.0, 0.0, 0.0], 'rewardMean': 0.7275761282841685, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.2381272494713
'totalSteps': 24320, 'rewardStep': 0.7858634085004644, 'errorList': [], 'lossList': [0.0, -1.2898205882310867, 0.0, 1.0335073117166758, 0.0, 0.0, 0.0], 'rewardMean': 0.7636050348383452, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.1841015848572
'totalSteps': 25600, 'rewardStep': 0.9635306533265388, 'errorList': [0.014558685125454142, 0.023069868828492355, 0.0402245973054726, 0.023964646499368535, 0.01060970304387379, 0.019662345678377862, 0.02169919880684985, 0.018090997218699226, 0.03572611108282918, 0.0644594175756199, 0.024556024477167305, 0.05335009399090044, 0.02807529807025547, 0.02908241244551643, 0.02886634460599369, 0.03277160030202945, 0.0054218364875831745, 0.01772819259834285, 0.05041736514338934, 0.02623026684440138, 0.03479359915309562, 0.03939243219052096, 0.023045491079735712, 0.03191864349252026, 0.026339985624784415, 0.027218005438471096, 0.013030886892894, 0.01650535613694323, 0.02494234764809129, 0.03963261505791383, 0.036244048485395565, 0.03933271609557507, 0.014410337563661387, 0.01601861208113393, 0.03837149793723618, 0.030389632232351697, 0.02794156198291666, 0.0565055289000095, 0.006921005255771506, 0.010974119177356117, 0.008683431712679112, 0.009332756545989166, 0.011045600859390567, 0.047288837534242775, 0.020657327223147854, 0.04170474702010366, 0.01651250512172026, 0.08850840669966085, 0.013128360802028088, 0.012330792501750312], 'lossList': [0.0, -1.2712324845790863, 0.0, 0.8480500835180282, 0.0, 0.0, 0.0], 'rewardMean': 0.8114556350991569, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.6700146190465, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=119.22
