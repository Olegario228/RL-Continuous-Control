#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 109
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7692527934361707, 'errorList': [], 'lossList': [0.0, -1.4050867033004761, 0.0, 36.58416495323181, 0.0, 0.0, 0.0], 'rewardMean': 0.5666614496058797, 'totalEpisodes': 48, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.22435919918687
'totalSteps': 3840, 'rewardStep': 0.9837718802794972, 'errorList': [], 'lossList': [0.0, -1.3943663334846497, 0.0, 48.90020042419434, 0.0, 0.0, 0.0], 'rewardMean': 0.7056982598304189, 'totalEpisodes': 98, 'stepsPerEpisode': 54, 'rewardPerEpisode': 42.07331358474783
'totalSteps': 5120, 'rewardStep': 0.458731568911845, 'errorList': [], 'lossList': [0.0, -1.3818824249505997, 0.0, 60.18108551025391, 0.0, 0.0, 0.0], 'rewardMean': 0.6439565871007754, 'totalEpisodes': 135, 'stepsPerEpisode': 44, 'rewardPerEpisode': 29.110779071716138
'totalSteps': 6400, 'rewardStep': 0.617170975815933, 'errorList': [], 'lossList': [0.0, -1.382360143661499, 0.0, 44.540367183685305, 0.0, 0.0, 0.0], 'rewardMean': 0.6385994648438069, 'totalEpisodes': 148, 'stepsPerEpisode': 33, 'rewardPerEpisode': 25.483988976407254
'totalSteps': 7680, 'rewardStep': 0.7540334283191137, 'errorList': [], 'lossList': [0.0, -1.3944241321086883, 0.0, 52.887868318557736, 0.0, 0.0, 0.0], 'rewardMean': 0.657838458756358, 'totalEpisodes': 159, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.323786996567737
'totalSteps': 8960, 'rewardStep': 0.51061449130435, 'errorList': [], 'lossList': [0.0, -1.3870352107286452, 0.0, 31.636444475650787, 0.0, 0.0, 0.0], 'rewardMean': 0.6368064634060712, 'totalEpisodes': 166, 'stepsPerEpisode': 328, 'rewardPerEpisode': 196.44752683175585
'totalSteps': 10240, 'rewardStep': 0.5201986906737732, 'errorList': [], 'lossList': [0.0, -1.3674983751773835, 0.0, 24.450366771221162, 0.0, 0.0, 0.0], 'rewardMean': 0.622230491814534, 'totalEpisodes': 170, 'stepsPerEpisode': 410, 'rewardPerEpisode': 274.4233131436917
'totalSteps': 11520, 'rewardStep': 0.7432729758566883, 'errorList': [], 'lossList': [0.0, -1.337927428483963, 0.0, 12.97615215241909, 0.0, 0.0, 0.0], 'rewardMean': 0.6356796567081067, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.5218887426678
'totalSteps': 12800, 'rewardStep': 0.8695800269033862, 'errorList': [], 'lossList': [0.0, -1.2869172310829162, 0.0, 7.514670187234879, 0.0, 0.0, 0.0], 'rewardMean': 0.6590696937276347, 'totalEpisodes': 171, 'stepsPerEpisode': 113, 'rewardPerEpisode': 92.5311952632065
'totalSteps': 14080, 'rewardStep': 0.7049628963342998, 'errorList': [], 'lossList': [0.0, -1.26098490357399, 0.0, 2.876191575229168, 0.0, 0.0, 0.0], 'rewardMean': 0.6931589727835057, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 840.1055007995891
'totalSteps': 15360, 'rewardStep': 0.5494317922489245, 'errorList': [], 'lossList': [0.0, -1.2417387974262237, 0.0, 16.060017775297165, 0.0, 0.0, 0.0], 'rewardMean': 0.6711768726647811, 'totalEpisodes': 172, 'stepsPerEpisode': 841, 'rewardPerEpisode': 581.4496606450606
'totalSteps': 16640, 'rewardStep': 0.6303250653651071, 'errorList': [], 'lossList': [0.0, -1.2399830734729766, 0.0, 19.67692842721939, 0.0, 0.0, 0.0], 'rewardMean': 0.635832191173342, 'totalEpisodes': 174, 'stepsPerEpisode': 185, 'rewardPerEpisode': 153.26798058611217
'totalSteps': 17920, 'rewardStep': 0.5251597077731345, 'errorList': [], 'lossList': [0.0, -1.2410977804660797, 0.0, 14.979253537654877, 0.0, 0.0, 0.0], 'rewardMean': 0.6424750050594711, 'totalEpisodes': 176, 'stepsPerEpisode': 556, 'rewardPerEpisode': 442.10737018141634
'totalSteps': 19200, 'rewardStep': 0.9196445903166495, 'errorList': [], 'lossList': [0.0, -1.213778166770935, 0.0, 1.5980240407586097, 0.0, 0.0, 0.0], 'rewardMean': 0.6727223665095428, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 967.2466766184391
'totalSteps': 20480, 'rewardStep': 0.8668681722271094, 'errorList': [], 'lossList': [0.0, -1.1692860019207, 0.0, 1.1540343636274337, 0.0, 0.0, 0.0], 'rewardMean': 0.6840058409003422, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.0609900287404
'totalSteps': 21760, 'rewardStep': 0.8910398265120021, 'errorList': [], 'lossList': [0.0, -1.1451434415578843, 0.0, 1.2000804879516362, 0.0, 0.0, 0.0], 'rewardMean': 0.7220483744211074, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.0206919988982
'totalSteps': 23040, 'rewardStep': 0.8518903681507325, 'errorList': [], 'lossList': [0.0, -1.1347126615047456, 0.0, 0.8944912949204444, 0.0, 0.0, 0.0], 'rewardMean': 0.7552175421688034, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.7912859041885
'totalSteps': 24320, 'rewardStep': 0.9079494046995513, 'errorList': [], 'lossList': [0.0, -1.1253442358970642, 0.0, 0.7135420059971511, 0.0, 0.0, 0.0], 'rewardMean': 0.7716851850530897, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.8913323996237
'totalSteps': 25600, 'rewardStep': 0.9377407751969205, 'errorList': [0.03444024782610841, 0.23702334601641123, 0.037642489625193835, 0.16099142646346348, 0.13148392460961342, 0.04443278086543626, 0.24945598704360514, 0.0698716316053885, 0.20288026090546873, 0.18852151512921672, 0.14984560813647846, 0.14270759316943452, 0.18874217218633627, 0.05034018636646225, 0.24710037901512047, 0.15970884091902046, 0.08019301264705392, 0.29566494408659283, 0.013537034230754476, 0.04845010990637063, 0.1378285977827732, 0.11284707376329182, 0.10157311732223205, 0.18809107229349156, 0.08870055868472833, 0.095413111633026, 0.048081277857392996, 0.08127780970280912, 0.023859176441840768, 0.04690830237430504, 0.2358709285422724, 0.17816490252238426, 0.08058231473834203, 0.08361179947053887, 0.16623069050988962, 0.05655929722590772, 0.25932779294483665, 0.21970720291882181, 0.06031093739588003, 0.10693885875128728, 0.14685546145645487, 0.02621367017630997, 0.07228073278052721, 0.24842974604465481, 0.05441293383609863, 0.2744039596208729, 0.08640438510067135, 0.16673472282426297, 0.2528155125556468, 0.09629771922957063], 'lossList': [0.0, -1.099404827952385, 0.0, 0.4155536293797195, 0.0, 0.0, 0.0], 'rewardMean': 0.7785012598824432, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.4617204416686, 'successfulTests': 39
#maxSuccessfulTests=39, maxSuccessfulTestsAtStep=25600, timeSpent=82.82
