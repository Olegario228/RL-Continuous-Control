#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 11
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7680278710726858, 'errorList': [], 'lossList': [0.0, -1.4071088445186615, 0.0, 29.773504819869995, 0.0, 0.0, 0.0], 'rewardMean': 0.6424108863860694, 'totalEpisodes': 70, 'stepsPerEpisode': 39, 'rewardPerEpisode': 23.076308750203623
'totalSteps': 3840, 'rewardStep': 0.6690209270983083, 'errorList': [], 'lossList': [0.0, -1.3871505212783815, 0.0, 40.17488122940063, 0.0, 0.0, 0.0], 'rewardMean': 0.6512808999568157, 'totalEpisodes': 114, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.941681243761791
'totalSteps': 5120, 'rewardStep': 0.7464154871026916, 'errorList': [], 'lossList': [0.0, -1.3746116226911544, 0.0, 47.08840934753418, 0.0, 0.0, 0.0], 'rewardMean': 0.6750645467432846, 'totalEpisodes': 147, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.672830645957818
'totalSteps': 6400, 'rewardStep': 0.49088881831926984, 'errorList': [], 'lossList': [0.0, -1.3607694935798644, 0.0, 51.19440554618836, 0.0, 0.0, 0.0], 'rewardMean': 0.6382294010584817, 'totalEpisodes': 165, 'stepsPerEpisode': 77, 'rewardPerEpisode': 61.696316032022395
'totalSteps': 7680, 'rewardStep': 0.6972501623023066, 'errorList': [], 'lossList': [0.0, -1.34787015914917, 0.0, 38.33894871234894, 0.0, 0.0, 0.0], 'rewardMean': 0.6480661945991192, 'totalEpisodes': 174, 'stepsPerEpisode': 55, 'rewardPerEpisode': 42.82300592022822
'totalSteps': 8960, 'rewardStep': 0.5456031266277963, 'errorList': [], 'lossList': [0.0, -1.3455186372995376, 0.0, 40.623988704681395, 0.0, 0.0, 0.0], 'rewardMean': 0.6334286134603587, 'totalEpisodes': 182, 'stepsPerEpisode': 116, 'rewardPerEpisode': 80.57047035292253
'totalSteps': 10240, 'rewardStep': 0.731880367073524, 'errorList': [], 'lossList': [0.0, -1.3464481556415557, 0.0, 22.22470336675644, 0.0, 0.0, 0.0], 'rewardMean': 0.6457350826620045, 'totalEpisodes': 188, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.730959867596766
'totalSteps': 11520, 'rewardStep': 0.7600579927942224, 'errorList': [], 'lossList': [0.0, -1.3620135456323623, 0.0, 17.668297290802002, 0.0, 0.0, 0.0], 'rewardMean': 0.6584376282322509, 'totalEpisodes': 194, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.85271416051748
'totalSteps': 12800, 'rewardStep': 0.8219978749621802, 'errorList': [], 'lossList': [0.0, -1.3702624762058258, 0.0, 13.33476243853569, 0.0, 0.0, 0.0], 'rewardMean': 0.674793652905244, 'totalEpisodes': 199, 'stepsPerEpisode': 91, 'rewardPerEpisode': 78.36766398247207
'totalSteps': 14080, 'rewardStep': 0.6402651973450233, 'errorList': [], 'lossList': [0.0, -1.362496373653412, 0.0, 10.772781419157981, 0.0, 0.0, 0.0], 'rewardMean': 0.6871407824698009, 'totalEpisodes': 201, 'stepsPerEpisode': 561, 'rewardPerEpisode': 473.0192320298214
'totalSteps': 15360, 'rewardStep': 0.9710126455701097, 'errorList': [5.531916069126314, 3.7784915621306703, 6.164953237513614, 3.256731282462539, 2.4233629316867344, 2.132380170132462, 2.7061126495390155, 0.4023382414578551, 0.9439389369561153, 0.4724510946133071, 4.0073607010030505, 4.595596071855108, 1.1710886549194373, 2.037998367988876, 1.4669014019340652, 0.5085089619525002, 2.917580826981648, 1.519294133714822, 0.13795650540574445, 0.6017076318927124, 0.5811765297187028, 2.9757495004959766, 2.230759641402431, 0.1992343393384443, 0.9420134846755542, 0.5107169342338379, 1.2108458989410293, 0.7370890397679207, 3.640390780011992, 0.9752538430354222, 3.1135927976612376, 1.0916550271253898, 0.1220896895299343, 1.2732161908360664, 4.618216475379389, 5.10428872506258, 0.5233268651009825, 0.25099435588611124, 0.5103725648121914, 1.9811339343108625, 3.489386466993188, 3.1244830597166366, 2.2878521501349764, 6.832091084284466, 0.9589443734898188, 0.5784804499480489, 0.5152198917394782, 0.6169863153743259, 0.25711013622641016, 0.20530444380383964], 'lossList': [0.0, -1.3465887552499771, 0.0, 10.56887925505638, 0.0, 0.0, 0.0], 'rewardMean': 0.7074392599195433, 'totalEpisodes': 205, 'stepsPerEpisode': 39, 'rewardPerEpisode': 35.5740818424878, 'successfulTests': 3
'totalSteps': 16640, 'rewardStep': 0.5629460232879959, 'errorList': [], 'lossList': [0.0, -1.3228256857395173, 0.0, 4.196245206594467, 0.0, 0.0, 0.0], 'rewardMean': 0.6968317695385119, 'totalEpisodes': 206, 'stepsPerEpisode': 655, 'rewardPerEpisode': 508.44212202575443
'totalSteps': 17920, 'rewardStep': 0.8198720567063303, 'errorList': [], 'lossList': [0.0, -1.3087190055847169, 0.0, 5.38294064104557, 0.0, 0.0, 0.0], 'rewardMean': 0.7041774264988759, 'totalEpisodes': 207, 'stepsPerEpisode': 1149, 'rewardPerEpisode': 888.5739263707352
'totalSteps': 19200, 'rewardStep': 0.9363993039222914, 'errorList': [0.03647271833279384, 0.029764810022484364, 0.03672315786930467, 0.06104449193095334, 0.057197707332824914, 0.027633151727518217, 0.03402315217092555, 0.009968252277918208, 0.026675681600151168, 0.020320010486117406, 0.04543379996285613, 0.014704822643200497, 0.034425999091703775, 0.02839478089308623, 0.05604974793996903, 0.05639895226424348, 0.08169414185444072, 0.03034188775704509, 0.06897351583066269, 0.05139959909816262, 0.06873544273170006, 0.011535066658130403, 0.08505622824387483, 0.032232826324237886, 0.08179402017846352, 0.05752057031730761, 0.023280157969609798, 0.024965696109236785, 0.0411216589789394, 0.016811784827912575, 0.04908880547972425, 0.03570981006799612, 0.05430668539671604, 0.013938953654932817, 0.035542042576268855, 0.09529214269511893, 0.03856727807224474, 0.020951776342602972, 0.06059759547501571, 0.0419172591588322, 0.044082998285286584, 0.04771090942318335, 0.036793259902102395, 0.01825789594232365, 0.06634009410308445, 0.044339702637340994, 0.07147806372355286, 0.014175206756231545, 0.056825897486247515, 0.024748211812828144], 'lossList': [0.0, -1.268019863963127, 0.0, 1.397043330669403, 0.0, 0.0, 0.0], 'rewardMean': 0.7487284750591781, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.2978399630574, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=75.92
