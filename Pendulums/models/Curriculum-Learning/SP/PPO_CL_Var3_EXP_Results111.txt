#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 111
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7294604676991456, 'errorList': [], 'lossList': [0.0, -1.4140148055553436, 0.0, 31.072615613937376, 0.0, 0.0, 0.0], 'rewardMean': 0.6231271846992992, 'totalEpisodes': 30, 'stepsPerEpisode': 64, 'rewardPerEpisode': 47.50112752960768
'totalSteps': 3840, 'rewardStep': 0.4642888366029302, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5437080106511147, 'totalEpisodes': 83, 'stepsPerEpisode': 21, 'rewardPerEpisode': 13.027818806883351
'totalSteps': 5120, 'rewardStep': 0.6470817545759697, 'errorList': [], 'lossList': [0.0, -1.4085454332828522, 0.0, 48.40828619003296, 0.0, 0.0, 0.0], 'rewardMean': 0.5643827594360857, 'totalEpisodes': 147, 'stepsPerEpisode': 10, 'rewardPerEpisode': 5.26393728611795
'totalSteps': 6400, 'rewardStep': 0.7187761512932249, 'errorList': [], 'lossList': [0.0, -1.386280294060707, 0.0, 43.81742687225342, 0.0, 0.0, 0.0], 'rewardMean': 0.5901149914122756, 'totalEpisodes': 195, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.786755309815013
'totalSteps': 7680, 'rewardStep': 0.5936170358024198, 'errorList': [], 'lossList': [0.0, -1.3606916892528533, 0.0, 43.183305311203, 0.0, 0.0, 0.0], 'rewardMean': 0.5906152834680105, 'totalEpisodes': 213, 'stepsPerEpisode': 172, 'rewardPerEpisode': 127.17118456910944
'totalSteps': 8960, 'rewardStep': 0.7989998185572521, 'errorList': [], 'lossList': [0.0, -1.3427691787481308, 0.0, 33.27416412353516, 0.0, 0.0, 0.0], 'rewardMean': 0.6166633503541656, 'totalEpisodes': 227, 'stepsPerEpisode': 116, 'rewardPerEpisode': 97.48357239619054
'totalSteps': 10240, 'rewardStep': 0.7650868081757746, 'errorList': [], 'lossList': [0.0, -1.3295400470495224, 0.0, 15.783970843553544, 0.0, 0.0, 0.0], 'rewardMean': 0.6331548456676778, 'totalEpisodes': 234, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.642214182411394
'totalSteps': 11520, 'rewardStep': 0.7125821327139144, 'errorList': [], 'lossList': [0.0, -1.322012147307396, 0.0, 21.536565549373627, 0.0, 0.0, 0.0], 'rewardMean': 0.6410975743723014, 'totalEpisodes': 241, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.096926602568495
'totalSteps': 12800, 'rewardStep': 0.7905282771630284, 'errorList': [], 'lossList': [0.0, -1.307172229886055, 0.0, 30.333468465805055, 0.0, 0.0, 0.0], 'rewardMean': 0.668471011918659, 'totalEpisodes': 247, 'stepsPerEpisode': 66, 'rewardPerEpisode': 52.33024158249489
'totalSteps': 14080, 'rewardStep': 0.8070819797231773, 'errorList': [], 'lossList': [0.0, -1.3009175342321395, 0.0, 6.887218334674835, 0.0, 0.0, 0.0], 'rewardMean': 0.6762331631210621, 'totalEpisodes': 250, 'stepsPerEpisode': 54, 'rewardPerEpisode': 49.183396807939
'totalSteps': 15360, 'rewardStep': 0.7106106116893369, 'errorList': [], 'lossList': [0.0, -1.3132053536176682, 0.0, 5.255847286581993, 0.0, 0.0, 0.0], 'rewardMean': 0.7008653406297028, 'totalEpisodes': 251, 'stepsPerEpisode': 659, 'rewardPerEpisode': 514.5192748893147
'totalSteps': 16640, 'rewardStep': 0.7978519910015158, 'errorList': [], 'lossList': [0.0, -1.2965000301599503, 0.0, 4.473235361278057, 0.0, 0.0, 0.0], 'rewardMean': 0.7342216560695614, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 972.3672241348547
'totalSteps': 17920, 'rewardStep': 0.8151240086627544, 'errorList': [], 'lossList': [0.0, -1.2422287786006927, 0.0, 3.460855084806681, 0.0, 0.0, 0.0], 'rewardMean': 0.7510258814782398, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.5378222608356
'totalSteps': 19200, 'rewardStep': 0.9095804240348078, 'errorList': [], 'lossList': [0.0, -1.1939724791049957, 0.0, 2.785498985722661, 0.0, 0.0, 0.0], 'rewardMean': 0.770106308752398, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.1643215973131
'totalSteps': 20480, 'rewardStep': 0.8117906163784395, 'errorList': [], 'lossList': [0.0, -1.1582520151138305, 0.0, 1.9781228677928449, 0.0, 0.0, 0.0], 'rewardMean': 0.79192366681, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.2111778540786
'totalSteps': 21760, 'rewardStep': 0.8447061599993992, 'errorList': [], 'lossList': [0.0, -1.0967980825901031, 0.0, 0.9845307391136885, 0.0, 0.0, 0.0], 'rewardMean': 0.7964943009542147, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.574897608358
'totalSteps': 23040, 'rewardStep': 0.9542590233773128, 'errorList': [0.15039687091822765, 0.160448755681303, 0.11996285714211992, 0.1238786838270237, 0.13559600910594488, 0.1467861948170462, 0.14260477201288993, 0.1409082241157311, 0.13032204508314676, 0.11360687593800207, 0.14007607283902038, 0.13171282240164553, 0.11614425230812349, 0.13893727046322044, 0.1092449625229162, 0.11851736918983818, 0.16743796389359092, 0.1401330967515375, 0.11760522890963261, 0.1381173276073538, 0.19075783843856556, 0.13251258842005978, 0.12562631534291904, 0.14019005706214208, 0.14941059136825494, 0.11448268322831118, 0.15179705860699885, 0.12850325490142228, 0.14604284637310058, 0.12667883615471973, 0.1503532897132503, 0.14935817737942134, 0.1994571840421242, 0.18221105537750093, 0.2114535377131992, 0.16429967817211297, 0.14640034494425974, 0.13152815790780403, 0.12261374730037335, 0.14239019692549532, 0.16366428441186945, 0.1534096438927974, 0.19951324754911093, 0.14081706283662546, 0.13771047914379228, 0.12611550594658502, 0.13113154992701215, 0.17492384104332168, 0.1137877461727692, 0.12396728401161036], 'lossList': [0.0, -1.0624099338054658, 0.0, 1.5478415552712976, 0.0, 0.0, 0.0], 'rewardMean': 0.8154115224743685, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1210.6155325702848, 'successfulTests': 49
'totalSteps': 24320, 'rewardStep': 0.9168872066016124, 'errorList': [], 'lossList': [0.0, -1.0337145483493806, 0.0, 0.45818832678720356, 0.0, 0.0, 0.0], 'rewardMean': 0.8358420298631385, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.2531607078008
'totalSteps': 25600, 'rewardStep': 0.9672804585760589, 'errorList': [0.2181990157593347, 0.14282815842670352, 0.05496911610372912, 0.10281696536928349, 0.2311613041801498, 0.10619007410793344, 0.15128131706116005, 0.06522716177137605, 0.22531155978526685, 0.4333833017698453, 0.0359979042585264, 0.1378119436253768, 0.19717560900589226, 0.1328964925045025, 0.04490627258119776, 0.17491728922713531, 0.05417871461744258, 0.08340017584197508, 0.03837902370615651, 0.15003083518644414, 0.1820713600137553, 0.15351613654677543, 0.17136719455133367, 0.14920049258083892, 0.11624144683078107, 0.09340771977843083, 0.06532837058384733, 0.08942433787712828, 0.1671515793845163, 0.23672500671243843, 0.14149560024085214, 0.04294681625183427, 0.2346290978234303, 0.03472952658856865, 0.12845079625601885, 0.0533663827210474, 0.26847231925897824, 0.3874694827333992, 0.3744932333615157, 0.11799045610153308, 0.048422219124008244, 0.11859533990617177, 0.05862713151524111, 0.04452250111349388, 0.2081864571639687, 0.03740046223147154, 0.20616858436762478, 0.14198105899861602, 0.3439262887473883, 0.03650065266163308], 'lossList': [0.0, -0.9899178090691566, 0.0, 0.47117297125048935, 0.0, 0.0, 0.0], 'rewardMean': 0.8535172480044414, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1200.623839248997, 'successfulTests': 38
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=23040, timeSpent=104.95
