#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 113
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.8062140794139064, 'errorList': [], 'lossList': [0.0, -1.4283247154951095, 0.0, 35.18889049530029, 0.0, 0.0, 0.0], 'rewardMean': 0.7857699561804812, 'totalEpisodes': 43, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.2371129071117926
'totalSteps': 3840, 'rewardStep': 0.9760260110735195, 'errorList': [], 'lossList': [0.0, -1.4240287792682649, 0.0, 43.18431798934937, 0.0, 0.0, 0.0], 'rewardMean': 0.8491886411448273, 'totalEpisodes': 102, 'stepsPerEpisode': 15, 'rewardPerEpisode': 14.56694124239918
'totalSteps': 5120, 'rewardStep': 0.500684468291735, 'errorList': [], 'lossList': [0.0, -1.4042576706409455, 0.0, 49.35540061950684, 0.0, 0.0, 0.0], 'rewardMean': 0.7620625979315542, 'totalEpisodes': 148, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.222654588381938
'totalSteps': 6400, 'rewardStep': 0.8412361127622061, 'errorList': [], 'lossList': [0.0, -1.3804463189840317, 0.0, 54.62133790016174, 0.0, 0.0, 0.0], 'rewardMean': 0.7778973008976846, 'totalEpisodes': 181, 'stepsPerEpisode': 89, 'rewardPerEpisode': 68.18004290983474
'totalSteps': 7680, 'rewardStep': 0.8976425450859538, 'errorList': [], 'lossList': [0.0, -1.3658732295036315, 0.0, 49.47450954437256, 0.0, 0.0, 0.0], 'rewardMean': 0.7978548415957295, 'totalEpisodes': 197, 'stepsPerEpisode': 49, 'rewardPerEpisode': 37.7878652910128
'totalSteps': 8960, 'rewardStep': 0.9034212137424336, 'errorList': [], 'lossList': [0.0, -1.3559032779932023, 0.0, 27.85074622154236, 0.0, 0.0, 0.0], 'rewardMean': 0.8129357519024014, 'totalEpisodes': 206, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.359106505117733
'totalSteps': 10240, 'rewardStep': 0.6990100979466862, 'errorList': [], 'lossList': [0.0, -1.3597305887937545, 0.0, 13.49678705573082, 0.0, 0.0, 0.0], 'rewardMean': 0.7986950451579371, 'totalEpisodes': 212, 'stepsPerEpisode': 139, 'rewardPerEpisode': 104.91191315081828
'totalSteps': 11520, 'rewardStep': 0.7195757728768818, 'errorList': [], 'lossList': [0.0, -1.3654749548435212, 0.0, 11.503096685409545, 0.0, 0.0, 0.0], 'rewardMean': 0.7899040149044865, 'totalEpisodes': 217, 'stepsPerEpisode': 145, 'rewardPerEpisode': 128.9177215881521
'totalSteps': 12800, 'rewardStep': 0.820936214234447, 'errorList': [], 'lossList': [0.0, -1.3719926393032074, 0.0, 7.717378932833672, 0.0, 0.0, 0.0], 'rewardMean': 0.7930072348374825, 'totalEpisodes': 221, 'stepsPerEpisode': 138, 'rewardPerEpisode': 118.21819454940601
'totalSteps': 14080, 'rewardStep': 0.8655156926883047, 'errorList': [], 'lossList': [0.0, -1.3615704083442688, 0.0, 13.636048815846443, 0.0, 0.0, 0.0], 'rewardMean': 0.8030262208116075, 'totalEpisodes': 224, 'stepsPerEpisode': 200, 'rewardPerEpisode': 165.47358699469555
'totalSteps': 15360, 'rewardStep': 0.7667444770280967, 'errorList': [], 'lossList': [0.0, -1.352172340154648, 0.0, 5.683881097435951, 0.0, 0.0, 0.0], 'rewardMean': 0.7990792605730264, 'totalEpisodes': 226, 'stepsPerEpisode': 525, 'rewardPerEpisode': 415.40231331433705
'totalSteps': 16640, 'rewardStep': 0.6561314507542813, 'errorList': [], 'lossList': [0.0, -1.3392302584648133, 0.0, 5.4152487587928775, 0.0, 0.0, 0.0], 'rewardMean': 0.7670898045411026, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.9686596315405
'totalSteps': 17920, 'rewardStep': 0.5560385716401991, 'errorList': [], 'lossList': [0.0, -1.3316055309772492, 0.0, 4.419141007661819, 0.0, 0.0, 0.0], 'rewardMean': 0.7726252148759489, 'totalEpisodes': 228, 'stepsPerEpisode': 363, 'rewardPerEpisode': 293.52483530746116
'totalSteps': 19200, 'rewardStep': 0.8170286321551498, 'errorList': [], 'lossList': [0.0, -1.3307078766822815, 0.0, 3.3527163270115854, 0.0, 0.0, 0.0], 'rewardMean': 0.7702044668152433, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 892.3479607105407
'totalSteps': 20480, 'rewardStep': 0.9782392288775004, 'errorList': [0.06934309941879598, 0.05680328609825137, 0.06953996226438798, 0.04589929622376805, 0.08688450377008611, 0.09632619390350956, 0.09916437417431954, 0.08453048591144402, 0.0985883068518455, 0.05780425168985339, 0.06541636516807336, 0.08574555366169595, 0.07294149002360928, 0.06711805022614374, 0.08260084386016595, 0.1072396067172765, 0.12521777930011568, 0.04749195054036241, 0.07548169515122316, 0.08328768764711292, 0.06868955539598501, 0.12271060907863574, 0.13048145333518002, 0.10073826205305472, 0.11221951831978834, 0.10126285038787225, 0.0748010334049381, 0.0758741148976419, 0.06168100694577278, 0.046530121508283756, 0.12469717487442059, 0.03946165879654581, 0.06412286233997815, 0.053537929209689084, 0.07649572615790515, 0.08345471787322303, 0.13223534320384128, 0.12506177780005293, 0.05718931865595711, 0.09258376096881614, 0.06983385404528826, 0.036908033991146186, 0.0399157549863463, 0.0710153068005437, 0.11847085988333939, 0.1284714018522568, 0.11668154932541323, 0.07519701277793231, 0.13854561757803685, 0.11659095225390997], 'lossList': [0.0, -1.314727526307106, 0.0, 1.5601182347536087, 0.0, 0.0, 0.0], 'rewardMean': 0.7782641351943981, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.203993268859, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=68.16
