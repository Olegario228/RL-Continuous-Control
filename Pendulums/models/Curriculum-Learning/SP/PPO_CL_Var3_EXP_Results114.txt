#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 114
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8763143105898412, 'errorList': [], 'lossList': [0.0, -1.42992061316967, 0.0, 33.61059730529785, 0.0, 0.0, 0.0], 'rewardMean': 0.7741148583665922, 'totalEpisodes': 41, 'stepsPerEpisode': 21, 'rewardPerEpisode': 19.18736014087398
'totalSteps': 3840, 'rewardStep': 0.6339568401652714, 'errorList': [], 'lossList': [0.0, -1.4310854464769363, 0.0, 45.18196018218994, 0.0, 0.0, 0.0], 'rewardMean': 0.727395518966152, 'totalEpisodes': 93, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.07033311319558
'totalSteps': 5120, 'rewardStep': 0.6701708678502786, 'errorList': [], 'lossList': [0.0, -1.4223227906227112, 0.0, 62.336453189849856, 0.0, 0.0, 0.0], 'rewardMean': 0.7130893561871836, 'totalEpisodes': 139, 'stepsPerEpisode': 38, 'rewardPerEpisode': 29.102413473955128
'totalSteps': 6400, 'rewardStep': 0.4049872295297998, 'errorList': [], 'lossList': [0.0, -1.4176889568567277, 0.0, 52.84720676422119, 0.0, 0.0, 0.0], 'rewardMean': 0.6514689308557069, 'totalEpisodes': 164, 'stepsPerEpisode': 104, 'rewardPerEpisode': 54.127013276589445
'totalSteps': 7680, 'rewardStep': 0.6993117908149478, 'errorList': [], 'lossList': [0.0, -1.4124843102693558, 0.0, 44.003319368362426, 0.0, 0.0, 0.0], 'rewardMean': 0.6594427408489136, 'totalEpisodes': 178, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.29571955704607
'totalSteps': 8960, 'rewardStep': 0.7012940349433302, 'errorList': [], 'lossList': [0.0, -1.4004614168405534, 0.0, 25.23399554014206, 0.0, 0.0, 0.0], 'rewardMean': 0.6654214971481159, 'totalEpisodes': 185, 'stepsPerEpisode': 113, 'rewardPerEpisode': 90.92966897405366
'totalSteps': 10240, 'rewardStep': 0.47717152588613915, 'errorList': [], 'lossList': [0.0, -1.3882692885398864, 0.0, 24.394108867645265, 0.0, 0.0, 0.0], 'rewardMean': 0.641890250740369, 'totalEpisodes': 188, 'stepsPerEpisode': 410, 'rewardPerEpisode': 309.127116934979
'totalSteps': 11520, 'rewardStep': 0.866969553693841, 'errorList': [], 'lossList': [0.0, -1.3708722770214081, 0.0, 7.439585402011871, 0.0, 0.0, 0.0], 'rewardMean': 0.6668990621796437, 'totalEpisodes': 190, 'stepsPerEpisode': 459, 'rewardPerEpisode': 261.9753482427368
'totalSteps': 12800, 'rewardStep': 0.48914004426809915, 'errorList': [], 'lossList': [0.0, -1.3545813900232315, 0.0, 25.633671479225157, 0.0, 0.0, 0.0], 'rewardMean': 0.6491231603884893, 'totalEpisodes': 195, 'stepsPerEpisode': 230, 'rewardPerEpisode': 140.6041257570406
'totalSteps': 14080, 'rewardStep': 0.5898767545930881, 'errorList': [], 'lossList': [0.0, -1.3325689709186554, 0.0, 9.952767851352691, 0.0, 0.0, 0.0], 'rewardMean': 0.6409192952334636, 'totalEpisodes': 199, 'stepsPerEpisode': 178, 'rewardPerEpisode': 135.3673252129241
'totalSteps': 15360, 'rewardStep': 0.8689606455894792, 'errorList': [], 'lossList': [0.0, -1.3299135494232177, 0.0, 7.672229158878326, 0.0, 0.0, 0.0], 'rewardMean': 0.6401839287334274, 'totalEpisodes': 206, 'stepsPerEpisode': 28, 'rewardPerEpisode': 26.05532612600113
'totalSteps': 16640, 'rewardStep': 0.24854626802016733, 'errorList': [], 'lossList': [0.0, -1.3311001044511794, 0.0, 5.640476434826851, 0.0, 0.0, 0.0], 'rewardMean': 0.601642871518917, 'totalEpisodes': 210, 'stepsPerEpisode': 234, 'rewardPerEpisode': 147.2395880695901
'totalSteps': 17920, 'rewardStep': 0.7542294269043347, 'errorList': [], 'lossList': [0.0, -1.3016299110651017, 0.0, 3.621684260070324, 0.0, 0.0, 0.0], 'rewardMean': 0.6100487274243227, 'totalEpisodes': 211, 'stepsPerEpisode': 1273, 'rewardPerEpisode': 917.374278102409
'totalSteps': 19200, 'rewardStep': 0.8329057010579701, 'errorList': [], 'lossList': [0.0, -1.275694090127945, 0.0, 4.645936516821385, 0.0, 0.0, 0.0], 'rewardMean': 0.6528405745771396, 'totalEpisodes': 212, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.14921479098441
'totalSteps': 20480, 'rewardStep': 0.7938785217684987, 'errorList': [], 'lossList': [0.0, -1.2531569296121596, 0.0, 1.5019385540485382, 0.0, 0.0, 0.0], 'rewardMean': 0.6622972476724949, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1011.9401446847174
'totalSteps': 21760, 'rewardStep': 0.8091143249895816, 'errorList': [], 'lossList': [0.0, -1.246165037751198, 0.0, 6.8448795056343075, 0.0, 0.0, 0.0], 'rewardMean': 0.6730792766771199, 'totalEpisodes': 213, 'stepsPerEpisode': 313, 'rewardPerEpisode': 272.179877094792
'totalSteps': 23040, 'rewardStep': 0.7661500889395955, 'errorList': [], 'lossList': [0.0, -1.2374939006567, 0.0, 0.9333355501294136, 0.0, 0.0, 0.0], 'rewardMean': 0.7019771329824656, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1030.645082244198
'totalSteps': 24320, 'rewardStep': 0.9110470983083077, 'errorList': [], 'lossList': [0.0, -1.2194033080339433, 0.0, 0.7895342015475034, 0.0, 0.0, 0.0], 'rewardMean': 0.7063848874439123, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.3774082087543
'totalSteps': 25600, 'rewardStep': 0.9803418487282142, 'errorList': [0.04241393136189247, 0.0508393191467847, 0.030131987296253167, 0.03887933647372431, 0.036747658743251865, 0.03231445782126674, 0.08788833967991365, 0.03881326791574399, 0.08332140570408228, 0.08224287465988113, 0.0614795727016762, 0.05648374829895115, 0.07102003796534642, 0.05090606061489019, 0.09492649753122684, 0.061708721923331177, 0.05689192387872702, 0.10542900932364285, 0.028488782693768484, 0.044534705508682854, 0.06243864223073886, 0.03935943640802231, 0.04874934652966647, 0.06726296801433844, 0.04299095107677093, 0.03899016548288721, 0.02990496648161408, 0.03361001300068131, 0.035710617226873395, 0.02748198430186586, 0.08826220017916475, 0.0443589398297339, 0.03698245774748718, 0.03618123394300685, 0.03985641446015065, 0.0520562316615784, 0.09639418588028084, 0.047503828438249, 0.027521811616114243, 0.038378682737657696, 0.046769815994563126, 0.03737299604181434, 0.0569744346509316, 0.05383955412422297, 0.03011057439432812, 0.10700323941357226, 0.047192096715777794, 0.06246921268213922, 0.10516729031144312, 0.04578777342043537], 'lossList': [0.0, -1.2023589688539504, 0.0, 0.7708717588707805, 0.0, 0.0, 0.0], 'rewardMean': 0.7555050678899237, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.789088979496, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.57
