#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 115
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.7282885701645386, 'errorList': [], 'lossList': [0.0, -1.4376336175203324, 0.0, 27.77312003135681, 0.0, 0.0, 0.0], 'rewardMean': 0.8119350806863528, 'totalEpisodes': 21, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.194829692709483
'totalSteps': 3840, 'rewardStep': 0.6743977633821441, 'errorList': [], 'lossList': [0.0, -1.434283567070961, 0.0, 54.32231489181518, 0.0, 0.0, 0.0], 'rewardMean': 0.7660893082516166, 'totalEpisodes': 67, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.488089495340809
'totalSteps': 5120, 'rewardStep': 0.7297253329310178, 'errorList': [], 'lossList': [0.0, -1.4261263197660445, 0.0, 59.57053918838501, 0.0, 0.0, 0.0], 'rewardMean': 0.7569983144214669, 'totalEpisodes': 112, 'stepsPerEpisode': 47, 'rewardPerEpisode': 40.8975525242206
'totalSteps': 6400, 'rewardStep': 0.8810647312270091, 'errorList': [], 'lossList': [0.0, -1.4110947054624559, 0.0, 47.89569750785827, 0.0, 0.0, 0.0], 'rewardMean': 0.7818115977825754, 'totalEpisodes': 131, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.14580799523153
'totalSteps': 7680, 'rewardStep': 0.5451487784069139, 'errorList': [], 'lossList': [0.0, -1.3835340684652329, 0.0, 44.28717282295227, 0.0, 0.0, 0.0], 'rewardMean': 0.7423677945532985, 'totalEpisodes': 143, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.260742620379162
'totalSteps': 8960, 'rewardStep': 0.8588161257088889, 'errorList': [], 'lossList': [0.0, -1.3653959393501283, 0.0, 24.989638369083405, 0.0, 0.0, 0.0], 'rewardMean': 0.7590032704326685, 'totalEpisodes': 149, 'stepsPerEpisode': 96, 'rewardPerEpisode': 83.2152534326597
'totalSteps': 10240, 'rewardStep': 0.30463373566272245, 'errorList': [], 'lossList': [0.0, -1.3718726909160615, 0.0, 26.67623876094818, 0.0, 0.0, 0.0], 'rewardMean': 0.7022070785864253, 'totalEpisodes': 155, 'stepsPerEpisode': 165, 'rewardPerEpisode': 115.58844715447361
'totalSteps': 11520, 'rewardStep': 0.7734626399541248, 'errorList': [], 'lossList': [0.0, -1.3782952326536178, 0.0, 47.50522669315338, 0.0, 0.0, 0.0], 'rewardMean': 0.7101243631828363, 'totalEpisodes': 164, 'stepsPerEpisode': 67, 'rewardPerEpisode': 60.476441637611124
'totalSteps': 12800, 'rewardStep': 0.8055904645268526, 'errorList': [], 'lossList': [0.0, -1.3602169448137282, 0.0, 10.890237838029861, 0.0, 0.0, 0.0], 'rewardMean': 0.719670973317238, 'totalEpisodes': 167, 'stepsPerEpisode': 177, 'rewardPerEpisode': 147.98925266311403
'totalSteps': 14080, 'rewardStep': 0.681086653111676, 'errorList': [], 'lossList': [0.0, -1.3424157297611237, 0.0, 5.522747144699097, 0.0, 0.0, 0.0], 'rewardMean': 0.698221479507589, 'totalEpisodes': 171, 'stepsPerEpisode': 18, 'rewardPerEpisode': 11.583988366698028
'totalSteps': 15360, 'rewardStep': 0.519583011442506, 'errorList': [], 'lossList': [0.0, -1.3356516915559768, 0.0, 6.232188024520874, 0.0, 0.0, 0.0], 'rewardMean': 0.6773509236353856, 'totalEpisodes': 173, 'stepsPerEpisode': 195, 'rewardPerEpisode': 149.60240791434185
'totalSteps': 16640, 'rewardStep': 0.47834585730541557, 'errorList': [], 'lossList': [0.0, -1.3228318196535112, 0.0, 3.3770259001851084, 0.0, 0.0, 0.0], 'rewardMean': 0.6577457330277128, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 905.4154186466031
'totalSteps': 17920, 'rewardStep': 0.8733855464913202, 'errorList': [], 'lossList': [0.0, -1.305258074402809, 0.0, 5.539804890155792, 0.0, 0.0, 0.0], 'rewardMean': 0.6721117543837429, 'totalEpisodes': 175, 'stepsPerEpisode': 182, 'rewardPerEpisode': 145.33064044646846
'totalSteps': 19200, 'rewardStep': 0.822688106259763, 'errorList': [], 'lossList': [0.0, -1.307867259979248, 0.0, 3.8049854639172556, 0.0, 0.0, 0.0], 'rewardMean': 0.6662740918870184, 'totalEpisodes': 176, 'stepsPerEpisode': 566, 'rewardPerEpisode': 492.2159176963533
'totalSteps': 20480, 'rewardStep': 0.9435242485112647, 'errorList': [0.037187930466344346, 0.047635712712414784, 0.054917635856958476, 0.05870636426633621, 0.05539086323298744, 0.03914721046590985, 0.06723185440963705, 0.05251504733759141, 0.04051961394631683, 0.06415397605924593, 0.0633657928444987, 0.04790886618494394, 0.06453595344499742, 0.092983314254276, 0.04382801457557835, 0.05054931560591321, 0.050363443684828854, 0.03815063194651585, 0.057078779985164634, 0.07686927306978814, 0.10183051995632296, 0.07437381447555075, 0.07568096124027562, 0.08686008602194734, 0.05023948582176492, 0.061695015288771055, 0.050603209274452124, 0.04667259582814954, 0.03704713560595603, 0.037530251811491666, 0.09179744700873238, 0.061256732329608286, 0.06811878276576329, 0.030505421028959527, 0.07929041388753687, 0.03180688862665846, 0.058374035411596796, 0.0861713588298588, 0.048437853473943755, 0.06990563799794056, 0.04905280298605664, 0.0602197073313257, 0.07512821086116712, 0.07112462943789097, 0.07666257885998502, 0.04184405348109665, 0.062127048260268575, 0.06590089324975258, 0.032683403834368734, 0.05931871763770694], 'lossList': [0.0, -1.3307201820611954, 0.0, 1.7268587630987167, 0.0, 0.0, 0.0], 'rewardMean': 0.7061116388974534, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1062.191344544129, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=68.07
