#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 116
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.6934691854276881, 'errorList': [], 'lossList': [0.0, -1.4086626648902894, 0.0, 32.4583882188797, 0.0, 0.0, 0.0], 'rewardMean': 0.6260584519520216, 'totalEpisodes': 24, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.66216584051522
'totalSteps': 3840, 'rewardStep': 0.5641669393164902, 'errorList': [], 'lossList': [0.0, -1.3869718438386918, 0.0, 59.83480987548828, 0.0, 0.0, 0.0], 'rewardMean': 0.6054279477401777, 'totalEpisodes': 71, 'stepsPerEpisode': 28, 'rewardPerEpisode': 18.307582886148683
'totalSteps': 5120, 'rewardStep': 0.43530306227277826, 'errorList': [], 'lossList': [0.0, -1.372282686829567, 0.0, 62.17578212738037, 0.0, 0.0, 0.0], 'rewardMean': 0.5628967263733279, 'totalEpisodes': 118, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.0563401624347826
'totalSteps': 6400, 'rewardStep': 0.8239303694846754, 'errorList': [], 'lossList': [0.0, -1.355972997546196, 0.0, 52.57083218574524, 0.0, 0.0, 0.0], 'rewardMean': 0.6151034549955974, 'totalEpisodes': 144, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.38373265253315
'totalSteps': 7680, 'rewardStep': 0.5056469298457512, 'errorList': [], 'lossList': [0.0, -1.3461370211839676, 0.0, 29.14926669597626, 0.0, 0.0, 0.0], 'rewardMean': 0.5968607008039563, 'totalEpisodes': 154, 'stepsPerEpisode': 139, 'rewardPerEpisode': 103.90058358969641
'totalSteps': 8960, 'rewardStep': 0.5805883112566727, 'errorList': [], 'lossList': [0.0, -1.3471683943271637, 0.0, 31.12913398742676, 0.0, 0.0, 0.0], 'rewardMean': 0.5945360737257729, 'totalEpisodes': 165, 'stepsPerEpisode': 115, 'rewardPerEpisode': 86.85200632957267
'totalSteps': 10240, 'rewardStep': 0.7740889720995292, 'errorList': [], 'lossList': [0.0, -1.3439298582077026, 0.0, 18.790975527763365, 0.0, 0.0, 0.0], 'rewardMean': 0.6169801860224925, 'totalEpisodes': 174, 'stepsPerEpisode': 28, 'rewardPerEpisode': 24.704440672877013
'totalSteps': 11520, 'rewardStep': 0.6543591789342167, 'errorList': [], 'lossList': [0.0, -1.3452129292488098, 0.0, 16.789055315256117, 0.0, 0.0, 0.0], 'rewardMean': 0.6211334074571285, 'totalEpisodes': 179, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.962897487903321
'totalSteps': 12800, 'rewardStep': 0.6887961579629055, 'errorList': [], 'lossList': [0.0, -1.3324014115333558, 0.0, 9.292942920923233, 0.0, 0.0, 0.0], 'rewardMean': 0.6278996825077063, 'totalEpisodes': 185, 'stepsPerEpisode': 121, 'rewardPerEpisode': 87.83185762668059
'totalSteps': 14080, 'rewardStep': 0.8033841864962846, 'errorList': [], 'lossList': [0.0, -1.3158373105525971, 0.0, 6.257044959068298, 0.0, 0.0, 0.0], 'rewardMean': 0.6523733293096992, 'totalEpisodes': 191, 'stepsPerEpisode': 81, 'rewardPerEpisode': 69.78338157524597
'totalSteps': 15360, 'rewardStep': 0.6667340398350141, 'errorList': [], 'lossList': [0.0, -1.3196747833490372, 0.0, 4.768569969534874, 0.0, 0.0, 0.0], 'rewardMean': 0.6496998147504318, 'totalEpisodes': 195, 'stepsPerEpisode': 260, 'rewardPerEpisode': 213.4091656079065
'totalSteps': 16640, 'rewardStep': 0.28099684439132633, 'errorList': [], 'lossList': [0.0, -1.3243499952554703, 0.0, 3.5737818270921706, 0.0, 0.0, 0.0], 'rewardMean': 0.6213828052579154, 'totalEpisodes': 199, 'stepsPerEpisode': 460, 'rewardPerEpisode': 366.6455754700418
'totalSteps': 17920, 'rewardStep': 0.8001560278999336, 'errorList': [], 'lossList': [0.0, -1.2991261488199235, 0.0, 3.4295192432403563, 0.0, 0.0, 0.0], 'rewardMean': 0.6578681018206309, 'totalEpisodes': 203, 'stepsPerEpisode': 64, 'rewardPerEpisode': 55.34263540162301
'totalSteps': 19200, 'rewardStep': 0.7901511738316603, 'errorList': [], 'lossList': [0.0, -1.2680322659015655, 0.0, 3.0582563984394073, 0.0, 0.0, 0.0], 'rewardMean': 0.6544901822553293, 'totalEpisodes': 205, 'stepsPerEpisode': 445, 'rewardPerEpisode': 391.726387780737
'totalSteps': 20480, 'rewardStep': 0.8076875652729846, 'errorList': [], 'lossList': [0.0, -1.2280814588069915, 0.0, 2.029987531453371, 0.0, 0.0, 0.0], 'rewardMean': 0.6846942457980527, 'totalEpisodes': 206, 'stepsPerEpisode': 1103, 'rewardPerEpisode': 1000.2089922862478
'totalSteps': 21760, 'rewardStep': 0.7008370846750093, 'errorList': [], 'lossList': [0.0, -1.1751104837656021, 0.0, 1.3770975324511527, 0.0, 0.0, 0.0], 'rewardMean': 0.6967191231398864, 'totalEpisodes': 206, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.1851989476675
'totalSteps': 23040, 'rewardStep': 0.96254509143196, 'errorList': [0.08398309910669724, 0.18305545679824023, 0.07758004155483277, 0.08438326727109309, 0.08524616618814441, 0.07659395411496055, 0.07509117000685406, 0.07700286320030918, 0.07323066597468075, 0.07423950578254948, 0.08157723320912319, 0.28661698757629084, 0.08186848142569818, 0.1005155403654478, 0.1407797592062884, 0.08026771812170387, 0.13073410235714716, 0.07653975925415886, 0.09037940509037415, 0.08580104536157077, 0.07568687467261967, 0.07668999531821578, 0.13282207060797022, 0.1321262469518796, 0.08564897584302585, 0.08168836887531647, 0.15425456838169524, 0.07791184595570635, 0.10626582644648531, 0.07503271932330911, 0.08388194044786342, 0.07746837960015426, 0.07864791525144545, 0.18442334168307456, 0.08221763044836611, 0.07964462595574556, 0.08134545478620911, 0.13532143698588886, 0.07064079812868916, 0.07443462807751647, 0.07163317304893156, 0.0834859363661196, 0.27302259939223744, 0.07959953341629204, 0.07841113191890837, 0.07557315847177745, 0.09402975914543724, 0.09995869094893478, 0.07178839669074774, 0.20267200982236386], 'lossList': [0.0, -1.1543675941228866, 0.0, 1.7929763755202293, 0.0, 0.0, 0.0], 'rewardMean': 0.7155647350731295, 'totalEpisodes': 208, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.19053874621194, 'successfulTests': 47
'totalSteps': 24320, 'rewardStep': 0.8247998731520612, 'errorList': [], 'lossList': [0.0, -1.1461200100183486, 0.0, 1.609688554108143, 0.0, 0.0, 0.0], 'rewardMean': 0.732608804494914, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.8727937232306
'totalSteps': 25600, 'rewardStep': 0.926906871261934, 'errorList': [], 'lossList': [0.0, -1.0948923277854918, 0.0, 0.4109915828704834, 0.0, 0.0, 0.0], 'rewardMean': 0.756419875824817, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1140.0104437136379
#maxSuccessfulTests=47, maxSuccessfulTestsAtStep=23040, timeSpent=81.0
