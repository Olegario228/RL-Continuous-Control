#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 118
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.39858076734838493, 'errorList': [], 'lossList': [0.0, -1.4252103996276855, 0.0, 32.10736130714417, 0.0, 0.0, 0.0], 'rewardMean': 0.6283964379519623, 'totalEpisodes': 29, 'stepsPerEpisode': 44, 'rewardPerEpisode': 29.08500427908875
'totalSteps': 3840, 'rewardStep': 0.809756064482728, 'errorList': [], 'lossList': [0.0, -1.4302433294057846, 0.0, 56.02416669845581, 0.0, 0.0, 0.0], 'rewardMean': 0.6888496467955508, 'totalEpisodes': 80, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.674035834031107
'totalSteps': 5120, 'rewardStep': 0.8388866699523801, 'errorList': [], 'lossList': [0.0, -1.4205391836166381, 0.0, 54.83377752304077, 0.0, 0.0, 0.0], 'rewardMean': 0.7263589025847582, 'totalEpisodes': 124, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.05722289183241
'totalSteps': 6400, 'rewardStep': 0.5715400757439878, 'errorList': [], 'lossList': [0.0, -1.4010215854644776, 0.0, 56.535037937164304, 0.0, 0.0, 0.0], 'rewardMean': 0.6953951372166041, 'totalEpisodes': 160, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.566152903171776
'totalSteps': 7680, 'rewardStep': 0.8402420559325746, 'errorList': [], 'lossList': [0.0, -1.3888466995954514, 0.0, 41.77968861579895, 0.0, 0.0, 0.0], 'rewardMean': 0.7195362903359325, 'totalEpisodes': 180, 'stepsPerEpisode': 84, 'rewardPerEpisode': 59.908855002865735
'totalSteps': 8960, 'rewardStep': 0.41305694386173697, 'errorList': [], 'lossList': [0.0, -1.381300807595253, 0.0, 23.53524266719818, 0.0, 0.0, 0.0], 'rewardMean': 0.6757535265539046, 'totalEpisodes': 188, 'stepsPerEpisode': 140, 'rewardPerEpisode': 95.98456727671055
'totalSteps': 10240, 'rewardStep': 0.721104904795434, 'errorList': [], 'lossList': [0.0, -1.381948670744896, 0.0, 17.81409990310669, 0.0, 0.0, 0.0], 'rewardMean': 0.6814224488340958, 'totalEpisodes': 197, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.203578462274716
'totalSteps': 11520, 'rewardStep': 0.7241731917095445, 'errorList': [], 'lossList': [0.0, -1.3907226902246475, 0.0, 18.74356962442398, 0.0, 0.0, 0.0], 'rewardMean': 0.6861725313758122, 'totalEpisodes': 202, 'stepsPerEpisode': 190, 'rewardPerEpisode': 169.20181027929468
'totalSteps': 12800, 'rewardStep': 0.07591299336534874, 'errorList': [], 'lossList': [0.0, -1.4018201702833175, 0.0, 8.192892536520958, 0.0, 0.0, 0.0], 'rewardMean': 0.625146577574766, 'totalEpisodes': 207, 'stepsPerEpisode': 213, 'rewardPerEpisode': 144.77597413072468
'totalSteps': 14080, 'rewardStep': 0.8602625444471718, 'errorList': [], 'lossList': [0.0, -1.4263546752929688, 0.0, 7.665077886581421, 0.0, 0.0, 0.0], 'rewardMean': 0.6253516211639292, 'totalEpisodes': 213, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.474800926561354
'totalSteps': 15360, 'rewardStep': 0.740167803222348, 'errorList': [], 'lossList': [0.0, -1.4347866147756576, 0.0, 25.027259628772736, 0.0, 0.0, 0.0], 'rewardMean': 0.6595103247513254, 'totalEpisodes': 216, 'stepsPerEpisode': 351, 'rewardPerEpisode': 290.4645067620376
'totalSteps': 16640, 'rewardStep': 0.5922458889035658, 'errorList': [], 'lossList': [0.0, -1.4025376105308534, 0.0, 6.038644223213196, 0.0, 0.0, 0.0], 'rewardMean': 0.6377593071934092, 'totalEpisodes': 219, 'stepsPerEpisode': 168, 'rewardPerEpisode': 126.77121743696951
'totalSteps': 17920, 'rewardStep': 0.8256421438425252, 'errorList': [], 'lossList': [0.0, -1.3941434025764465, 0.0, 4.794664803743363, 0.0, 0.0, 0.0], 'rewardMean': 0.6364348545824237, 'totalEpisodes': 223, 'stepsPerEpisode': 154, 'rewardPerEpisode': 138.47101708264591
'totalSteps': 19200, 'rewardStep': 0.7670751589751096, 'errorList': [], 'lossList': [0.0, -1.4051039958000182, 0.0, 4.4630616402626035, 0.0, 0.0, 0.0], 'rewardMean': 0.6559883629055359, 'totalEpisodes': 225, 'stepsPerEpisode': 215, 'rewardPerEpisode': 171.31632980989303
'totalSteps': 20480, 'rewardStep': 0.7481948880481682, 'errorList': [], 'lossList': [0.0, -1.3730430620908738, 0.0, 3.615233547091484, 0.0, 0.0, 0.0], 'rewardMean': 0.6467836461170953, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.7212986598803
'totalSteps': 21760, 'rewardStep': 0.7369740776100357, 'errorList': [], 'lossList': [0.0, -1.3225263488292693, 0.0, 2.8091761749982833, 0.0, 0.0, 0.0], 'rewardMean': 0.6791753594919252, 'totalEpisodes': 226, 'stepsPerEpisode': 718, 'rewardPerEpisode': 557.3614923912563
'totalSteps': 23040, 'rewardStep': 0.7293206506873477, 'errorList': [], 'lossList': [0.0, -1.2946103650331497, 0.0, 1.5544803646206855, 0.0, 0.0, 0.0], 'rewardMean': 0.6799969340811165, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.2199931464907
'totalSteps': 24320, 'rewardStep': 0.5821372956172733, 'errorList': [], 'lossList': [0.0, -1.274870542883873, 0.0, 1.8590119323134422, 0.0, 0.0, 0.0], 'rewardMean': 0.6657933444718894, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.8448616371122
'totalSteps': 25600, 'rewardStep': 0.9687712804643842, 'errorList': [0.032831999429293796, 0.03708910090055673, 0.03966115656986165, 0.03321846518063351, 0.038149291101264894, 0.0379150387619996, 0.04360177113614361, 0.0373508880327916, 0.03499871335604441, 0.03352845515335798, 0.03901769415977364, 0.037221686600141676, 0.038061223373120935, 0.032098202090892904, 0.046444677104141546, 0.034412507592106456, 0.03496232189238262, 0.03180945777206368, 0.041651158248473694, 0.03905500417919662, 0.034353054009022575, 0.04421679367992764, 0.03699655558668708, 0.03794065548320206, 0.041695682691771936, 0.03868539728166145, 0.03956744221053257, 0.04415433781179825, 0.03981612547420099, 0.044772085706880606, 0.04466772065792149, 0.03751722808203825, 0.035045333221793144, 0.03178725626615194, 0.03243158134853206, 0.04051116015950492, 0.04141882959094897, 0.03210143005152804, 0.039810535703921034, 0.04615850640644625, 0.035902487288279404, 0.0367895515673189, 0.041594875247729236, 0.03450236390066611, 0.03269506620114131, 0.0321866769697859, 0.04322834437617717, 0.032196207321119066, 0.03397363072799713, 0.03744115912808375], 'lossList': [0.0, -1.2420775347948074, 0.0, 1.0072358153015375, 0.0, 0.0, 0.0], 'rewardMean': 0.755079173181793, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1120.8217838621292, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=78.81
