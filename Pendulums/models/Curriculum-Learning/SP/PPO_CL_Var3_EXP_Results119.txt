#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 119
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.7894057407782967, 'errorList': [], 'lossList': [0.0, -1.422274987101555, 0.0, 32.04121138095856, 0.0, 0.0, 0.0], 'rewardMean': 0.7563783323287988, 'totalEpisodes': 30, 'stepsPerEpisode': 38, 'rewardPerEpisode': 28.74478704069596
'totalSteps': 3840, 'rewardStep': 0.956379148018917, 'errorList': [], 'lossList': [0.0, -1.4246005058288573, 0.0, 47.434293785095214, 0.0, 0.0, 0.0], 'rewardMean': 0.8230452708921715, 'totalEpisodes': 84, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.716321665456394
'totalSteps': 5120, 'rewardStep': 0.6489296169350675, 'errorList': [], 'lossList': [0.0, -1.4088102614879607, 0.0, 49.79912094116211, 0.0, 0.0, 0.0], 'rewardMean': 0.7795163574028955, 'totalEpisodes': 130, 'stepsPerEpisode': 43, 'rewardPerEpisode': 32.4390892117738
'totalSteps': 6400, 'rewardStep': 0.4950776998799784, 'errorList': [], 'lossList': [0.0, -1.3819339227676393, 0.0, 48.58733169555664, 0.0, 0.0, 0.0], 'rewardMean': 0.7226286258983121, 'totalEpisodes': 158, 'stepsPerEpisode': 19, 'rewardPerEpisode': 11.674244846546765
'totalSteps': 7680, 'rewardStep': 0.1024886624918912, 'errorList': [], 'lossList': [0.0, -1.3571475160121917, 0.0, 47.626719121932986, 0.0, 0.0, 0.0], 'rewardMean': 0.6192719653305753, 'totalEpisodes': 175, 'stepsPerEpisode': 130, 'rewardPerEpisode': 78.5421640316204
'totalSteps': 8960, 'rewardStep': 0.455243210641494, 'errorList': [], 'lossList': [0.0, -1.3389360213279724, 0.0, 40.63003079414368, 0.0, 0.0, 0.0], 'rewardMean': 0.595839286089278, 'totalEpisodes': 190, 'stepsPerEpisode': 97, 'rewardPerEpisode': 69.19534835866429
'totalSteps': 10240, 'rewardStep': 0.6059691591543015, 'errorList': [], 'lossList': [0.0, -1.3316909205913543, 0.0, 27.421075644493104, 0.0, 0.0, 0.0], 'rewardMean': 0.5971055202224059, 'totalEpisodes': 199, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.324967384485439
'totalSteps': 11520, 'rewardStep': 0.2963698331674754, 'errorList': [], 'lossList': [0.0, -1.325361840724945, 0.0, 10.724870913028717, 0.0, 0.0, 0.0], 'rewardMean': 0.5636904438829693, 'totalEpisodes': 203, 'stepsPerEpisode': 188, 'rewardPerEpisode': 141.677215224671
'totalSteps': 12800, 'rewardStep': 0.8002348274777715, 'errorList': [], 'lossList': [0.0, -1.2944012212753295, 0.0, 14.11943403840065, 0.0, 0.0, 0.0], 'rewardMean': 0.5873448822424494, 'totalEpisodes': 210, 'stepsPerEpisode': 64, 'rewardPerEpisode': 54.94170070549459
'totalSteps': 14080, 'rewardStep': 0.7357744353093136, 'errorList': [], 'lossList': [0.0, -1.282780322432518, 0.0, 4.968813478946686, 0.0, 0.0, 0.0], 'rewardMean': 0.5885872333854506, 'totalEpisodes': 217, 'stepsPerEpisode': 132, 'rewardPerEpisode': 104.8366541752695
'totalSteps': 15360, 'rewardStep': 0.9337324078377646, 'errorList': [22.695863605024854, 20.854427274072584, 13.156552648714916, 3.6127520773905126, 12.16694209576444, 7.580467872644015, 31.848627945713446, 3.987351261926461, 1.3457173635184243, 31.36892497164772, 1.9907607077391176, 49.38126067386745, 4.927194002720352, 8.918528440921799, 0.3505898022691878, 2.178700565233097, 17.941602833935633, 4.9841579848430575, 26.54203474796411, 51.94674935113365, 2.46164356604617, 3.262080771207052, 28.996013321021344, 15.727832320319866, 7.897591455562283, 51.119629250805566, 24.992737904123953, 35.437787371429145, 40.8064502044288, 5.911180546859299, 28.900402982612967, 16.648793442505745, 2.785211849541998, 14.653154246415196, 11.394684852147137, 43.6697880398213, 36.665913795112075, 13.648853081323256, 2.8251938524476157, 7.339336372098991, 5.61674131506057, 0.21673300947372146, 5.501545549579605, 19.39107348566883, 14.74215639428978, 34.53974397061107, 7.670904911817777, 32.6507943126059, 0.4598605335383463, 7.831713662158686], 'lossList': [0.0, -1.3007483768463135, 0.0, 5.158600617647171, 0.0, 0.0, 0.0], 'rewardMean': 0.6030199000913974, 'totalEpisodes': 222, 'stepsPerEpisode': 202, 'rewardPerEpisode': 181.5288348948793, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5982781038504915, 'errorList': [], 'lossList': [0.0, -1.2820294618606567, 0.0, 4.337416857481003, 0.0, 0.0, 0.0], 'rewardMean': 0.567209795674555, 'totalEpisodes': 226, 'stepsPerEpisode': 288, 'rewardPerEpisode': 240.12406452431418
'totalSteps': 17920, 'rewardStep': 0.5643948973351032, 'errorList': [], 'lossList': [0.0, -1.256862494945526, 0.0, 3.628606063723564, 0.0, 0.0, 0.0], 'rewardMean': 0.5587563237145584, 'totalEpisodes': 228, 'stepsPerEpisode': 498, 'rewardPerEpisode': 396.6051429178136
'totalSteps': 19200, 'rewardStep': 0.8199510211473386, 'errorList': [], 'lossList': [0.0, -1.2390401202440262, 0.0, 2.6468540081381797, 0.0, 0.0, 0.0], 'rewardMean': 0.5912436558412945, 'totalEpisodes': 232, 'stepsPerEpisode': 173, 'rewardPerEpisode': 151.28248503707215
'totalSteps': 20480, 'rewardStep': 0.9292994893905121, 'errorList': [], 'lossList': [0.0, -1.2226324331760408, 0.0, 2.3623349025845526, 0.0, 0.0, 0.0], 'rewardMean': 0.6739247385311565, 'totalEpisodes': 233, 'stepsPerEpisode': 351, 'rewardPerEpisode': 304.5324778255022
'totalSteps': 21760, 'rewardStep': 0.8921763573196796, 'errorList': [], 'lossList': [0.0, -1.2092102295160294, 0.0, 1.6128082740306855, 0.0, 0.0, 0.0], 'rewardMean': 0.7176180531989751, 'totalEpisodes': 234, 'stepsPerEpisode': 379, 'rewardPerEpisode': 336.70459043616506
'totalSteps': 23040, 'rewardStep': 0.9207371337042267, 'errorList': [], 'lossList': [0.0, -1.183204980492592, 0.0, 0.895476876348257, 0.0, 0.0, 0.0], 'rewardMean': 0.7490948506539675, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.9369878645027
'totalSteps': 24320, 'rewardStep': 0.8832300831009097, 'errorList': [], 'lossList': [0.0, -1.1643287616968154, 0.0, 0.8747800903767348, 0.0, 0.0, 0.0], 'rewardMean': 0.8077808756473113, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1194.0794106173191
'totalSteps': 25600, 'rewardStep': 0.9872794504756403, 'errorList': [0.007461351367719285, 0.01934086005337107, 0.006991304137459487, 0.01438720749835151, 0.01330131042492581, 0.014202148461614408, 0.023014650802025048, 0.015403661892888993, 0.014943721141781961, 0.006829164532658698, 0.017684371114846324, 0.0036667889018901895, 0.01767961003643398, 0.018416589233851588, 0.009185059768657751, 0.007294158157655794, 0.01135739232507428, 0.011769844111383211, 0.0279938949587947, 0.031247685796179172, 0.03083896346744902, 0.01405460019313847, 0.012051657528979817, 0.02144429596213003, 0.006766066787777849, 0.013780857717069103, 0.018734573911473307, 0.011397367156860825, 0.013909456139930275, 0.006910427517246103, 0.017228599872870436, 0.04086784545063712, 0.005476380520569158, 0.009414408783631681, 0.036323319432665556, 0.0337293878056075, 0.0018318389677384201, 0.0015744482496709675, 0.0037483445273562414, 0.01245529473081525, 0.02119157242512207, 0.00756019363085026, 0.024182338784617785, 0.021402335868998257, 0.004299731496814977, 0.010071187501075675, 0.0051695799009598845, 0.004796973719437502, 0.004270737479551191, 0.004628402654646909], 'lossList': [0.0, -1.1399958854913712, 0.0, 0.7104522909596562, 0.0, 0.0, 0.0], 'rewardMean': 0.826485337947098, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1210.5793027398504, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=100.94
