#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 12
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.6240481832698448, 'errorList': [], 'lossList': [0.0, -1.4201746755838394, 0.0, 30.37262445449829, 0.0, 0.0, 0.0], 'rewardMean': 0.5310393877378594, 'totalEpisodes': 67, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.4680865631312185
'totalSteps': 3840, 'rewardStep': 0.4134549859386055, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4722471868382325, 'totalEpisodes': 140, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.260536855025983
'totalSteps': 5120, 'rewardStep': 0.8936983270993611, 'errorList': [], 'lossList': [0.0, -1.4005865889787674, 0.0, 35.54251029968262, 0.0, 0.0, 0.0], 'rewardMean': 0.5565374148904583, 'totalEpisodes': 209, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8936983270993611
'totalSteps': 6400, 'rewardStep': 0.84871507271262, 'errorList': [], 'lossList': [0.0, -1.3838075804710388, 0.0, 37.91499056816101, 0.0, 0.0, 0.0], 'rewardMean': 0.6052336911941518, 'totalEpisodes': 257, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.461216611929943
'totalSteps': 7680, 'rewardStep': 0.8620835844250698, 'errorList': [], 'lossList': [0.0, -1.3680445390939713, 0.0, 42.82648965835571, 0.0, 0.0, 0.0], 'rewardMean': 0.641926533084283, 'totalEpisodes': 279, 'stepsPerEpisode': 56, 'rewardPerEpisode': 40.042204420059285
'totalSteps': 8960, 'rewardStep': 0.4822041840287251, 'errorList': [], 'lossList': [0.0, -1.3539167404174806, 0.0, 42.80111979484558, 0.0, 0.0, 0.0], 'rewardMean': 0.6219612394523383, 'totalEpisodes': 291, 'stepsPerEpisode': 136, 'rewardPerEpisode': 107.75010361861384
'totalSteps': 10240, 'rewardStep': 0.7021235297725154, 'errorList': [], 'lossList': [0.0, -1.3544991087913514, 0.0, 21.79191450357437, 0.0, 0.0, 0.0], 'rewardMean': 0.6308681605990246, 'totalEpisodes': 296, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.028910944771653
'totalSteps': 11520, 'rewardStep': 0.6377611614680627, 'errorList': [], 'lossList': [0.0, -1.3525911051034927, 0.0, 19.18799149274826, 0.0, 0.0, 0.0], 'rewardMean': 0.6315574606859284, 'totalEpisodes': 299, 'stepsPerEpisode': 159, 'rewardPerEpisode': 113.14801200382195
'totalSteps': 12800, 'rewardStep': 0.8870377075720912, 'errorList': [], 'lossList': [0.0, -1.348005132675171, 0.0, 27.699600200653077, 0.0, 0.0, 0.0], 'rewardMean': 0.6764581722225502, 'totalEpisodes': 303, 'stepsPerEpisode': 141, 'rewardPerEpisode': 115.42153929738842
'totalSteps': 14080, 'rewardStep': 0.7186427882291362, 'errorList': [], 'lossList': [0.0, -1.347835557460785, 0.0, 7.298001893758774, 0.0, 0.0, 0.0], 'rewardMean': 0.6859176327184793, 'totalEpisodes': 303, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 949.7005907803663
'totalSteps': 15360, 'rewardStep': 0.8064408624140906, 'errorList': [], 'lossList': [0.0, -1.3589297115802765, 0.0, 17.758908601999284, 0.0, 0.0, 0.0], 'rewardMean': 0.7252162203660277, 'totalEpisodes': 308, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.94505789288157
'totalSteps': 16640, 'rewardStep': 0.9521022150313793, 'errorList': [0.3625215902818572, 0.3364684467592294, 0.38244861704517596, 0.414920331224216, 0.3545753997851146, 0.33614868808386444, 0.4389244438682269, 0.3445621481465396, 0.3435688788782773, 0.36803910719175975, 0.2962708125827364, 0.36736972721531835, 0.31743850382189076, 0.33404388881985986, 0.3530390431773884, 0.353992999750648, 0.3495838217778663, 0.32269147116807406, 0.30966423947976707, 0.33472261392444214, 0.30405718594756487, 0.3538579343184082, 0.33508236405012964, 0.3496300040881559, 0.39645619110459135, 0.34181433063136457, 0.3555530152448118, 0.3144269776016769, 0.36188818802751427, 0.36616373928933565, 0.2993919129714924, 0.3465859932871242, 0.3959327885141487, 0.3517713959234058, 0.28150386923702353, 0.3874064728792936, 0.33877596648062047, 0.3163387759720135, 0.29406556246859206, 0.31074971854095673, 0.3735690586814973, 0.38203585664784645, 0.3320717523840843, 0.28273573842654043, 0.38968255172236055, 0.3459859083739071, 0.3614512998930161, 0.38419052439075707, 0.38035942135191936, 0.3163514453580705], 'lossList': [0.0, -1.353195650577545, 0.0, 6.239175686240197, 0.0, 0.0, 0.0], 'rewardMean': 0.7790809432753052, 'totalEpisodes': 309, 'stepsPerEpisode': 1118, 'rewardPerEpisode': 776.4396190265244, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.8175870743639394, 'errorList': [], 'lossList': [0.0, -1.3357279509305955, 0.0, 3.550468334257603, 0.0, 0.0, 0.0], 'rewardMean': 0.7714698180017631, 'totalEpisodes': 309, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.8159086699507
'totalSteps': 19200, 'rewardStep': 0.953065174993055, 'errorList': [0.10312995524399544, 0.10193746704226245, 0.07828704363499397, 0.07480248157359061, 0.09579751650885461, 0.06127734690615179, 0.10399339840526513, 0.09229587116560578, 0.11168818262743875, 0.0814046541590764, 0.10995183330218321, 0.11952733771189404, 0.06812482378222166, 0.10506293285183718, 0.10851668104631396, 0.0920629204395982, 0.11944211584336997, 0.09780783748259754, 0.11092076981924782, 0.14265701273904596, 0.06857305844344205, 0.08277299668589737, 0.06808601106203255, 0.10037044449033004, 0.09246034471654399, 0.07714676677662853, 0.07771532216213459, 0.07240218183045623, 0.09610073635001075, 0.07518797656915907, 0.12795011122041297, 0.1292812837626585, 0.1057367799907863, 0.07531474232380583, 0.09428887079963519, 0.10041769644766206, 0.09666413639734626, 0.06813589197285168, 0.05935371427803112, 0.11451152259794721, 0.089879337031698, 0.10329875072517497, 0.09388312757926603, 0.08869309727588112, 0.08416279526440454, 0.06391925882986299, 0.12357302054994232, 0.13562707325756335, 0.0766443155524463, 0.07559681775353322], 'lossList': [0.0, -1.3275271731615066, 0.0, 2.761797155290842, 0.0, 0.0, 0.0], 'rewardMean': 0.7819048282298066, 'totalEpisodes': 309, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.7414307708964, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=75.64
