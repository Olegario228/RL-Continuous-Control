#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 120
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9020843173099665, 'errorList': [], 'lossList': [0.0, -1.4377434355020524, 0.0, 23.86785472393036, 0.0, 0.0, 0.0], 'rewardMean': 0.9084524739453973, 'totalEpisodes': 16, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.256310846301545
'totalSteps': 3840, 'rewardStep': 0.47439551431221444, 'errorList': [], 'lossList': [0.0, -1.4286126792430878, 0.0, 55.84252317428589, 0.0, 0.0, 0.0], 'rewardMean': 0.7637668207343363, 'totalEpisodes': 61, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.432124693753577
'totalSteps': 5120, 'rewardStep': 0.7326833765058427, 'errorList': [], 'lossList': [0.0, -1.4106247919797896, 0.0, 55.07215721130371, 0.0, 0.0, 0.0], 'rewardMean': 0.7559959596772129, 'totalEpisodes': 99, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.386406084032764
'totalSteps': 6400, 'rewardStep': 0.5826350192748888, 'errorList': [], 'lossList': [0.0, -1.397672865986824, 0.0, 59.957993049621585, 0.0, 0.0, 0.0], 'rewardMean': 0.721323771596748, 'totalEpisodes': 139, 'stepsPerEpisode': 81, 'rewardPerEpisode': 63.90992068356005
'totalSteps': 7680, 'rewardStep': 0.7845246836736413, 'errorList': [], 'lossList': [0.0, -1.396131129860878, 0.0, 50.293463697433474, 0.0, 0.0, 0.0], 'rewardMean': 0.7318572569428969, 'totalEpisodes': 157, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.53992769247203
'totalSteps': 8960, 'rewardStep': 0.5514897018228027, 'errorList': [], 'lossList': [0.0, -1.3867317056655883, 0.0, 36.48383406639099, 0.0, 0.0, 0.0], 'rewardMean': 0.706090463354312, 'totalEpisodes': 166, 'stepsPerEpisode': 98, 'rewardPerEpisode': 70.40896932125429
'totalSteps': 10240, 'rewardStep': 0.6521617243904504, 'errorList': [], 'lossList': [0.0, -1.3613825798034669, 0.0, 45.32285012722016, 0.0, 0.0, 0.0], 'rewardMean': 0.6993493709838294, 'totalEpisodes': 174, 'stepsPerEpisode': 109, 'rewardPerEpisode': 91.013523115258
'totalSteps': 11520, 'rewardStep': 0.7155091998476355, 'errorList': [], 'lossList': [0.0, -1.3283074116706848, 0.0, 43.17994222640991, 0.0, 0.0, 0.0], 'rewardMean': 0.7011449075242523, 'totalEpisodes': 179, 'stepsPerEpisode': 67, 'rewardPerEpisode': 59.027633432522414
'totalSteps': 12800, 'rewardStep': 0.7766499160015047, 'errorList': [], 'lossList': [0.0, -1.3113920629024505, 0.0, 23.666414308547974, 0.0, 0.0, 0.0], 'rewardMean': 0.7086954083719775, 'totalEpisodes': 183, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.34242958636919
'totalSteps': 14080, 'rewardStep': 0.7678219469479235, 'errorList': [], 'lossList': [0.0, -1.3058278822898866, 0.0, 9.488053883612157, 0.0, 0.0, 0.0], 'rewardMean': 0.693995540008687, 'totalEpisodes': 184, 'stepsPerEpisode': 218, 'rewardPerEpisode': 185.38435231947204
'totalSteps': 15360, 'rewardStep': 0.7536428643072903, 'errorList': [], 'lossList': [0.0, -1.281596320271492, 0.0, 7.620186297893524, 0.0, 0.0, 0.0], 'rewardMean': 0.6791513947084195, 'totalEpisodes': 185, 'stepsPerEpisode': 292, 'rewardPerEpisode': 233.14423109296624
'totalSteps': 16640, 'rewardStep': 0.9723383256331025, 'errorList': [0.03361013891687554, 0.06758981140229395, 0.06571036787195605, 0.06892816790335521, 0.04324719932061099, 0.06377035651948688, 0.02717111398125057, 0.025394530889477105, 0.01917596769715234, 0.02429978771547609, 0.02783779046424899, 0.0833726357469219, 0.04392809217303897, 0.05404188498734872, 0.03725244459560933, 0.05221318527926783, 0.01647298540884427, 0.04451136170572254, 0.04977810843225762, 0.030234339313567903, 0.03393861212860203, 0.05061994843898365, 0.03846480801530778, 0.02230600063449274, 0.03800834856930855, 0.033181576275507184, 0.016707506941565448, 0.05376143001991507, 0.026725080842332178, 0.015894533479678324, 0.033467157609816583, 0.04213474984186066, 0.038259087813130796, 0.018947185658915303, 0.01418203414728457, 0.05690321941560139, 0.04086647040575626, 0.03003868629369485, 0.0165529957399562, 0.060532311825014555, 0.051864129560437684, 0.03889666946880298, 0.018185247024409656, 0.03021420611655092, 0.04172395133078218, 0.07301745666971995, 0.0521962626439403, 0.014998827843902612, 0.07147456453930681, 0.024027115468644382], 'lossList': [0.0, -1.2674364233016968, 0.0, 5.583218743056059, 0.0, 0.0, 0.0], 'rewardMean': 0.7289456758405082, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.870055940505, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=16640, timeSpent=56.83
