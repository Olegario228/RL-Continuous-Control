#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 121
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.6109756019292173, 'errorList': [], 'lossList': [0.0, -1.4408213287591933, 0.0, 28.79032474517822, 0.0, 0.0, 0.0], 'rewardMean': 0.6020767107546883, 'totalEpisodes': 16, 'stepsPerEpisode': 109, 'rewardPerEpisode': 83.13487580245231
'totalSteps': 3840, 'rewardStep': 0.9404989068230648, 'errorList': [], 'lossList': [0.0, -1.4565872418880463, 0.0, 67.65142345428467, 0.0, 0.0, 0.0], 'rewardMean': 0.7148841094441472, 'totalEpisodes': 57, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.940896150602653
'totalSteps': 5120, 'rewardStep': 0.9491536726436852, 'errorList': [], 'lossList': [0.0, -1.455234742164612, 0.0, 67.96225856781005, 0.0, 0.0, 0.0], 'rewardMean': 0.7734515002440316, 'totalEpisodes': 110, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.927024181826058
'totalSteps': 6400, 'rewardStep': 0.7557152929879566, 'errorList': [], 'lossList': [0.0, -1.4377393412590027, 0.0, 58.56275707244873, 0.0, 0.0, 0.0], 'rewardMean': 0.7699042587928167, 'totalEpisodes': 145, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.782948575646548
'totalSteps': 7680, 'rewardStep': 0.6852386689808166, 'errorList': [], 'lossList': [0.0, -1.405649906396866, 0.0, 32.55327064275742, 0.0, 0.0, 0.0], 'rewardMean': 0.7557933271574834, 'totalEpisodes': 153, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.440578117954775
'totalSteps': 8960, 'rewardStep': 0.6479662308198072, 'errorList': [], 'lossList': [0.0, -1.3726402145624161, 0.0, 47.43629120349884, 0.0, 0.0, 0.0], 'rewardMean': 0.7403894562521011, 'totalEpisodes': 164, 'stepsPerEpisode': 118, 'rewardPerEpisode': 93.92396690330665
'totalSteps': 10240, 'rewardStep': 0.786417107450047, 'errorList': [], 'lossList': [0.0, -1.3536601626873017, 0.0, 33.28883914470673, 0.0, 0.0, 0.0], 'rewardMean': 0.7461429126518443, 'totalEpisodes': 172, 'stepsPerEpisode': 54, 'rewardPerEpisode': 48.683345468495325
'totalSteps': 11520, 'rewardStep': 0.5311282092444712, 'errorList': [], 'lossList': [0.0, -1.347526027560234, 0.0, 10.348095870018005, 0.0, 0.0, 0.0], 'rewardMean': 0.722252390051025, 'totalEpisodes': 176, 'stepsPerEpisode': 167, 'rewardPerEpisode': 139.13830058324493
'totalSteps': 12800, 'rewardStep': 0.7926991237674706, 'errorList': [], 'lossList': [0.0, -1.3372794324159623, 0.0, 16.39801277399063, 0.0, 0.0, 0.0], 'rewardMean': 0.7292970634226696, 'totalEpisodes': 181, 'stepsPerEpisode': 140, 'rewardPerEpisode': 115.35835398340488
'totalSteps': 14080, 'rewardStep': 0.6412302635206796, 'errorList': [], 'lossList': [0.0, -1.3186852860450744, 0.0, 6.554327210187912, 0.0, 0.0, 0.0], 'rewardMean': 0.7341023078167216, 'totalEpisodes': 184, 'stepsPerEpisode': 230, 'rewardPerEpisode': 184.67648272806463
'totalSteps': 15360, 'rewardStep': 0.9775380799441021, 'errorList': [5.052924567424246, 4.242061795016956, 5.365204803731368, 3.975672272265725, 3.7702069111914187, 3.508013913819807, 3.8579373831353743, 1.217997623879135, 2.642838266121258, 2.102965414802072, 4.4510737171653, 4.586485407636616, 2.7651752665599334, 3.300055065809452, 3.1601680240771333, 1.0291694299905396, 3.9751576131688395, 3.1177484855928306, 1.8259357600881403, 1.0176023911796792, 2.2239236735316945, 3.9571342459550802, 3.4535220343755126, 1.8420267754159658, 2.647568236252128, 0.9061595615436417, 2.804849971672762, 0.6421466657558363, 4.248875964716644, 2.4779290550426185, 4.035788172113106, 2.836481137092072, 1.859106774392026, 2.954643823027798, 4.637874219452912, 4.878206078753178, 2.3573429156046957, 1.913164553608365, 2.2008120007430225, 3.4310882153606395, 4.292063996982355, 4.078862747466301, 3.544030891346796, 5.553934123155407, 0.5450484280781341, 2.1978409152899703, 0.9147646897897909, 0.8655675639181447, 1.811308981418023, 1.705135788125204], 'lossList': [0.0, -1.3051441198587417, 0.0, 5.47521065056324, 0.0, 0.0, 0.0], 'rewardMean': 0.7707585556182102, 'totalEpisodes': 188, 'stepsPerEpisode': 93, 'rewardPerEpisode': 84.22708229387437, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7902355810053617, 'errorList': [], 'lossList': [0.0, -1.29181017100811, 0.0, 4.2540858164429665, 0.0, 0.0, 0.0], 'rewardMean': 0.7557322230364398, 'totalEpisodes': 190, 'stepsPerEpisode': 475, 'rewardPerEpisode': 403.2441328280507
'totalSteps': 17920, 'rewardStep': 0.4688877295112069, 'errorList': [], 'lossList': [0.0, -1.2404259473085404, 0.0, 2.166494257375598, 0.0, 0.0, 0.0], 'rewardMean': 0.707705628723192, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1042.2790121440016
'totalSteps': 19200, 'rewardStep': 0.818369137851922, 'errorList': [], 'lossList': [0.0, -1.2025518220663072, 0.0, 2.9853654339909554, 0.0, 0.0, 0.0], 'rewardMean': 0.7139710132095886, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 895.9273216943074
'totalSteps': 20480, 'rewardStep': 0.5433057091990572, 'errorList': [], 'lossList': [0.0, -1.1909133887290955, 0.0, 2.119158717393875, 0.0, 0.0, 0.0], 'rewardMean': 0.6997777172314127, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 955.0177395739956
'totalSteps': 21760, 'rewardStep': 0.6834590750533553, 'errorList': [], 'lossList': [0.0, -1.173353150486946, 0.0, 2.0627415159344675, 0.0, 0.0, 0.0], 'rewardMean': 0.7033270016547675, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 909.9299983296776
'totalSteps': 23040, 'rewardStep': 0.9038892739151217, 'errorList': [], 'lossList': [0.0, -1.161731321811676, 0.0, 1.2192543654888868, 0.0, 0.0, 0.0], 'rewardMean': 0.7150742183012749, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.1911678997112
'totalSteps': 24320, 'rewardStep': 0.8901901352643536, 'errorList': [], 'lossList': [0.0, -1.1209121030569076, 0.0, 0.8790374366194009, 0.0, 0.0, 0.0], 'rewardMean': 0.7509804109032632, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.785102454816
'totalSteps': 25600, 'rewardStep': 0.9854427423322889, 'errorList': [0.0795555467394603, 0.05632850672023143, 0.07475431500408616, 0.058335515962440174, 0.08432451968101044, 0.0502404912047169, 0.06162803095076737, 0.057180506021070006, 0.06445075624902037, 0.07056232630345087, 0.10718777312797159, 0.04817834467696934, 0.06639803103565192, 0.11717040211123024, 0.0749046876502464, 0.06957397006518236, 0.07088838260013232, 0.06492751972802693, 0.061556514618166205, 0.047993962877716186, 0.06830665618695651, 0.05535826852707237, 0.07490472333129423, 0.06958810131082567, 0.0771533328600518, 0.09282671875643964, 0.08501401240615722, 0.06068471537483152, 0.08625090879574593, 0.0672875709344478, 0.07123416917200975, 0.08783348049425826, 0.07922817107143937, 0.06561046049033449, 0.0789342120644596, 0.06091770594587801, 0.10483945104403168, 0.06903009167895863, 0.058385669787590876, 0.07924204917577264, 0.09774735886558929, 0.09875392198529305, 0.06036762789974605, 0.0796895899381649, 0.10280667993889511, 0.10613814784935309, 0.06063495223790967, 0.06951247950818229, 0.07090107930255797, 0.057431993898374174], 'lossList': [0.0, -1.0637108010053635, 0.0, 0.9171325371600687, 0.0, 0.0, 0.0], 'rewardMean': 0.7702547727597449, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1211.952556896325, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=101.1
