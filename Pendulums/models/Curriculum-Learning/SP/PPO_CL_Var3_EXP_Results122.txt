#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 122
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.5670285281090718, 'errorList': [], 'lossList': [0.0, -1.4380269134044648, 0.0, 29.406476917266847, 0.0, 0.0, 0.0], 'rewardMean': 0.5350646880065417, 'totalEpisodes': 23, 'stepsPerEpisode': 135, 'rewardPerEpisode': 80.58201482824987
'totalSteps': 3840, 'rewardStep': 0.49193097925424367, 'errorList': [], 'lossList': [0.0, -1.413522094488144, 0.0, 54.13732500076294, 0.0, 0.0, 0.0], 'rewardMean': 0.520686785089109, 'totalEpisodes': 63, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.616485432254093
'totalSteps': 5120, 'rewardStep': 0.6960175982784229, 'errorList': [], 'lossList': [0.0, -1.3757556939125062, 0.0, 55.48594930648804, 0.0, 0.0, 0.0], 'rewardMean': 0.5645194883864375, 'totalEpisodes': 109, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.301490121828778
'totalSteps': 6400, 'rewardStep': 0.7091845576597392, 'errorList': [], 'lossList': [0.0, -1.3606823563575745, 0.0, 56.46564586639404, 0.0, 0.0, 0.0], 'rewardMean': 0.5934525022410979, 'totalEpisodes': 143, 'stepsPerEpisode': 46, 'rewardPerEpisode': 32.9601572999243
'totalSteps': 7680, 'rewardStep': 0.8086179241029325, 'errorList': [], 'lossList': [0.0, -1.3451300597190856, 0.0, 56.07420875549317, 0.0, 0.0, 0.0], 'rewardMean': 0.629313405884737, 'totalEpisodes': 174, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.4823159635635
'totalSteps': 8960, 'rewardStep': 0.5957432954631094, 'errorList': [], 'lossList': [0.0, -1.333426359295845, 0.0, 33.75129783153534, 0.0, 0.0, 0.0], 'rewardMean': 0.6245176758245046, 'totalEpisodes': 186, 'stepsPerEpisode': 132, 'rewardPerEpisode': 97.20533943659015
'totalSteps': 10240, 'rewardStep': 0.7632565596917501, 'errorList': [], 'lossList': [0.0, -1.3372715157270432, 0.0, 15.849188854694367, 0.0, 0.0, 0.0], 'rewardMean': 0.6418600363079101, 'totalEpisodes': 198, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.42642787565463
'totalSteps': 11520, 'rewardStep': 0.9648177534073831, 'errorList': [32.48693837774687, 8.016118709434869, 43.83226311642295, 24.832815085458602, 11.078109339686032, 68.77451337252178, 8.472698454266858, 41.31088816390407, 36.13674788933856, 66.24873640227827, 27.103526495267698, 51.98674557207021, 89.71878200013879, 16.704871793551504, 70.03228615506119, 18.70881571484312, 16.72235833803029, 36.41428053129016, 79.92624312831653, 78.64148850384139, 75.25472244966994, 3.5200881728512803, 0.6147012676535802, 39.85535947790636, 96.602594197507, 54.38633314901808, 51.8932444544434, 20.74070003047497, 61.28022580096221, 100.72780432795132, 5.940295452913069, 71.49184693396542, 1.3574718435650175, 2.607208641740603, 39.2159291142535, 21.095117574650356, 36.546696436291725, 75.28166172791653, 31.21374497579498, 65.39728951091853, 21.036843221539215, 82.28839912184792, 48.05006637317097, 26.26957606351982, 41.06841222201954, 32.906686578016405, 88.9845332409722, 85.86474127806673, 26.800238665450802, 57.500849100129486], 'lossList': [0.0, -1.3461823165416718, 0.0, 16.750820714235306, 0.0, 0.0, 0.0], 'rewardMean': 0.6777442270967404, 'totalEpisodes': 206, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.5207026959908, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.578291770217773, 'errorList': [], 'lossList': [0.0, -1.3390591871738433, 0.0, 22.726890242099763, 0.0, 0.0, 0.0], 'rewardMean': 0.6677989814088436, 'totalEpisodes': 214, 'stepsPerEpisode': 141, 'rewardPerEpisode': 102.35958633779705
'totalSteps': 14080, 'rewardStep': 0.8576260956119354, 'errorList': [], 'lossList': [0.0, -1.331104969382286, 0.0, 6.965822295546531, 0.0, 0.0, 0.0], 'rewardMean': 0.703251506179636, 'totalEpisodes': 220, 'stepsPerEpisode': 28, 'rewardPerEpisode': 25.181390572152186
'totalSteps': 15360, 'rewardStep': 0.829026019849866, 'errorList': [], 'lossList': [0.0, -1.3176035213470458, 0.0, 15.755328488349914, 0.0, 0.0, 0.0], 'rewardMean': 0.7294512553537155, 'totalEpisodes': 226, 'stepsPerEpisode': 53, 'rewardPerEpisode': 41.23747405493607
'totalSteps': 16640, 'rewardStep': 0.3716954599054742, 'errorList': [], 'lossList': [0.0, -1.3155332839488982, 0.0, 4.335845377445221, 0.0, 0.0, 0.0], 'rewardMean': 0.7174277034188385, 'totalEpisodes': 230, 'stepsPerEpisode': 252, 'rewardPerEpisode': 186.93997120449419
'totalSteps': 17920, 'rewardStep': 0.7889953112433065, 'errorList': [], 'lossList': [0.0, -1.311641082763672, 0.0, 3.826192983984947, 0.0, 0.0, 0.0], 'rewardMean': 0.726725474715327, 'totalEpisodes': 234, 'stepsPerEpisode': 129, 'rewardPerEpisode': 112.15225908577668
'totalSteps': 19200, 'rewardStep': 0.9160397160840767, 'errorList': [], 'lossList': [0.0, -1.2837686812877656, 0.0, 19.487790566682815, 0.0, 0.0, 0.0], 'rewardMean': 0.7474109905577607, 'totalEpisodes': 237, 'stepsPerEpisode': 129, 'rewardPerEpisode': 116.2391369722868
'totalSteps': 20480, 'rewardStep': 0.9812419026160368, 'errorList': [1.305542911516036, 0.5767305982065571, 1.1894237261501612, 1.6915589257287327, 0.8056772587242901, 2.0144335537552784, 0.5493411024896347, 1.8541020476778725, 0.8173522065530725, 0.9777055828677068, 2.1832035824198353, 0.7792407726378979, 2.269425263094466, 0.9941064449896198, 0.9122646532736999, 1.8988164666404803, 1.1605158921170093, 1.02919002498132, 1.248284322678228, 1.1813476142545831, 1.3140977894323909, 1.5861487642545649, 0.3678508168680684, 0.645778366550949, 1.0700837280461812, 2.6283751021342368, 1.7338297431545142, 1.5156635374915943, 0.7777396767610469, 2.541096336472238, 1.4157591180907407, 1.3726035869071707, 0.4023128993454991, 1.9355390761922089, 1.1182185653175094, 2.1332833072879116, 1.434602602998647, 1.1561704849735437, 1.148454389259278, 0.7424649444756376, 1.2245616705437008, 0.23172094752923217, 1.8874760350521613, 0.1921762771471478, 1.357116845170834, 0.8914305568581594, 1.7490793092438992, 1.3089986765882757, 1.667426587207956, 1.9421492670930083], 'lossList': [0.0, -1.2546951687335968, 0.0, 3.8011673426628114, 0.0, 0.0, 0.0], 'rewardMean': 0.7646733884090712, 'totalEpisodes': 239, 'stepsPerEpisode': 219, 'rewardPerEpisode': 201.81664607915522, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.8539784879917415, 'errorList': [], 'lossList': [0.0, -1.2282499957084656, 0.0, 2.1775639513134957, 0.0, 0.0, 0.0], 'rewardMean': 0.7904969076619344, 'totalEpisodes': 241, 'stepsPerEpisode': 127, 'rewardPerEpisode': 110.98734399373048
'totalSteps': 23040, 'rewardStep': 0.8716109937456963, 'errorList': [], 'lossList': [0.0, -1.197557378411293, 0.0, 2.2362150901556017, 0.0, 0.0, 0.0], 'rewardMean': 0.801332351067329, 'totalEpisodes': 242, 'stepsPerEpisode': 315, 'rewardPerEpisode': 263.58432284579794
'totalSteps': 24320, 'rewardStep': 0.8938670912717677, 'errorList': [], 'lossList': [0.0, -1.1745238077640534, 0.0, 1.0659524757415055, 0.0, 0.0, 0.0], 'rewardMean': 0.7942372848537673, 'totalEpisodes': 242, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1136.4683130759613
'totalSteps': 25600, 'rewardStep': 0.8980794393189309, 'errorList': [], 'lossList': [0.0, -1.1534047746658325, 0.0, 0.8441317519173026, 0.0, 0.0, 0.0], 'rewardMean': 0.8262160517638831, 'totalEpisodes': 242, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.5071754509502
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=20480, timeSpent=100.23
