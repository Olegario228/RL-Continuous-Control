#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 123
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5672077435617066, 'errorList': [], 'lossList': [0.0, -1.4165933126211165, 0.0, 28.864094061851503, 0.0, 0.0, 0.0], 'rewardMean': 0.7330690796942234, 'totalEpisodes': 21, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.66225349406091
'totalSteps': 3840, 'rewardStep': 0.9590675018691036, 'errorList': [], 'lossList': [0.0, -1.3994342881441115, 0.0, 63.671048412322996, 0.0, 0.0, 0.0], 'rewardMean': 0.8084018870858501, 'totalEpisodes': 73, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.484515087759116
'totalSteps': 5120, 'rewardStep': 0.9627052971373801, 'errorList': [], 'lossList': [0.0, -1.3827045840024947, 0.0, 53.67585968017578, 0.0, 0.0, 0.0], 'rewardMean': 0.8469777395987327, 'totalEpisodes': 120, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.65242150006608
'totalSteps': 6400, 'rewardStep': 0.3749799299643579, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6896451363872744, 'totalEpisodes': 169, 'stepsPerEpisode': 20, 'rewardPerEpisode': 11.993042785957796
'totalSteps': 7680, 'rewardStep': 0.8758841405683339, 'errorList': [], 'lossList': [0.0, -1.3697018498182296, 0.0, 54.02623296737671, 0.0, 0.0, 0.0], 'rewardMean': 0.7162507084131401, 'totalEpisodes': 215, 'stepsPerEpisode': 44, 'rewardPerEpisode': 38.01429494391005
'totalSteps': 8960, 'rewardStep': 0.6718028601275785, 'errorList': [], 'lossList': [0.0, -1.365341141819954, 0.0, 52.92993955612182, 0.0, 0.0, 0.0], 'rewardMean': 0.7106947273774449, 'totalEpisodes': 248, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.78846996843777
'totalSteps': 10240, 'rewardStep': 0.8730493665638739, 'errorList': [], 'lossList': [0.0, -1.3579419082403184, 0.0, 46.3205303144455, 0.0, 0.0, 0.0], 'rewardMean': 0.7287341317314926, 'totalEpisodes': 262, 'stepsPerEpisode': 138, 'rewardPerEpisode': 116.01932228239802
'totalSteps': 11520, 'rewardStep': 0.8701666065643006, 'errorList': [], 'lossList': [0.0, -1.3588371753692627, 0.0, 24.78883840441704, 0.0, 0.0, 0.0], 'rewardMean': 0.7428773792147734, 'totalEpisodes': 269, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.766553601177655
'totalSteps': 12800, 'rewardStep': 0.7415738147648681, 'errorList': [], 'lossList': [0.0, -1.3653237593173981, 0.0, 29.743664543628693, 0.0, 0.0, 0.0], 'rewardMean': 0.7271417191085859, 'totalEpisodes': 274, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.946703194974752
'totalSteps': 14080, 'rewardStep': 0.8524413392540718, 'errorList': [], 'lossList': [0.0, -1.3601319253444673, 0.0, 10.49406759262085, 0.0, 0.0, 0.0], 'rewardMean': 0.7556650786778226, 'totalEpisodes': 279, 'stepsPerEpisode': 72, 'rewardPerEpisode': 60.71944446619943
'totalSteps': 15360, 'rewardStep': 0.7740623852101266, 'errorList': [], 'lossList': [0.0, -1.3651742070913315, 0.0, 6.37383955001831, 0.0, 0.0, 0.0], 'rewardMean': 0.7371645670119249, 'totalEpisodes': 284, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.112843874133354
'totalSteps': 16640, 'rewardStep': 0.5863865043622687, 'errorList': [], 'lossList': [0.0, -1.361432484984398, 0.0, 7.6306693136692045, 0.0, 0.0, 0.0], 'rewardMean': 0.6995326877344138, 'totalEpisodes': 285, 'stepsPerEpisode': 803, 'rewardPerEpisode': 666.5257662137132
'totalSteps': 17920, 'rewardStep': 0.8638663600316046, 'errorList': [], 'lossList': [0.0, -1.3530848717689514, 0.0, 5.583648000359535, 0.0, 0.0, 0.0], 'rewardMean': 0.7484213307411384, 'totalEpisodes': 287, 'stepsPerEpisode': 408, 'rewardPerEpisode': 359.92260239380124
'totalSteps': 19200, 'rewardStep': 0.8773330508810551, 'errorList': [], 'lossList': [0.0, -1.3362796568870545, 0.0, 4.809411850571633, 0.0, 0.0, 0.0], 'rewardMean': 0.7986566428328081, 'totalEpisodes': 288, 'stepsPerEpisode': 109, 'rewardPerEpisode': 94.78490325143893
'totalSteps': 20480, 'rewardStep': 0.9406927433211957, 'errorList': [0.267948993054473, 0.35779077777204277, 0.3240553584340906, 0.26572666018008495, 0.3110598259975732, 0.3480229809634622, 0.2630283449459152, 0.27438279677380484, 0.2879981688216016, 0.2708929885236732, 0.30482924121228155, 0.2635974882087338, 0.27119764533077734, 0.25550234374423847, 0.33270566726291506, 0.32294773590807685, 0.3205425337148996, 0.30526626797233536, 0.29767580638239616, 0.2831093898898029, 0.31495614757553275, 0.3189307698388855, 0.3258752941401549, 0.2573655479466977, 0.3042305500645917, 0.330296487672116, 0.3364781634420537, 0.2660845350529317, 0.294517921260091, 0.31215723845274407, 0.35210196429781937, 0.24721496797486872, 0.27622759129489016, 0.29977081056316024, 0.2887186460040589, 0.2700391271674677, 0.28470530976218517, 0.31835897502957383, 0.2590470987657546, 0.285731144735481, 0.2718778655552886, 0.31045520716141667, 0.31701023047434684, 0.25989126770998433, 0.26617449280673294, 0.2988923717405147, 0.3339190625025326, 0.2965478242745878, 0.30329879306377844, 0.2918646618655753], 'lossList': [0.0, -1.3216274172067641, 0.0, 2.2039440792798994, 0.0, 0.0, 0.0], 'rewardMean': 0.8051375031080944, 'totalEpisodes': 289, 'stepsPerEpisode': 159, 'rewardPerEpisode': 142.82993901053055, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.7605461568017812, 'errorList': [], 'lossList': [0.0, -1.2942585295438767, 0.0, 2.7977834606170653, 0.0, 0.0, 0.0], 'rewardMean': 0.8140118327755147, 'totalEpisodes': 289, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 881.3373731709592
'totalSteps': 23040, 'rewardStep': 0.8434470605234516, 'errorList': [], 'lossList': [0.0, -1.2470834416151046, 0.0, 1.497211413383484, 0.0, 0.0, 0.0], 'rewardMean': 0.8110516021714723, 'totalEpisodes': 289, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.5104769234968
'totalSteps': 24320, 'rewardStep': 0.8306468294733703, 'errorList': [], 'lossList': [0.0, -1.1878147006034852, 0.0, 1.2650315790995956, 0.0, 0.0, 0.0], 'rewardMean': 0.8070996244623794, 'totalEpisodes': 289, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1163.3361637096632
'totalSteps': 25600, 'rewardStep': 0.9742785171618231, 'errorList': [0.060513015665815004, 0.04357702955940568, 0.028980663403696833, 0.055398590696895796, 0.04842627546274034, 0.03933353641878889, 0.01583755606115233, 0.052844859062482576, 0.030154440520666383, 0.03951564637023596, 0.04699345746682734, 0.036953725847385856, 0.02900454130894394, 0.03742302214319616, 0.06492276594575264, 0.03814973715422402, 0.03507650330679595, 0.04604455981451867, 0.05629132821940942, 0.02178687993207157, 0.009954277163075061, 0.023079224444679965, 0.04831588109250745, 0.0341005588575518, 0.027468223797809686, 0.010421716658018068, 0.05570080878142862, 0.05722410171764482, 0.049953229416446705, 0.03373967839563459, 0.04879423399572518, 0.03910864182462176, 0.01863232121864179, 0.029153449343074445, 0.03601944599759784, 0.05537326019114921, 0.030009185218631287, 0.01663842358051938, 0.011552461106069075, 0.05727211301563589, 0.03219407416014334, 0.04467539288645861, 0.050597478214587904, 0.030460369606557517, 0.02812503279314951, 0.034939406571552215, 0.013924087258354393, 0.03247392939773991, 0.05510218619862978, 0.02131110693052963], 'lossList': [0.0, -1.1586395525932311, 0.0, 0.8695175676979124, 0.0, 0.0, 0.0], 'rewardMean': 0.8303700947020749, 'totalEpisodes': 289, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.0533995977478, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=97.89
