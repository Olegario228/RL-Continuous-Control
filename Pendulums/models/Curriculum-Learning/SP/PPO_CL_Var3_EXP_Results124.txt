#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 124
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8335747627765183, 'errorList': [], 'lossList': [0.0, -1.4195928430557252, 0.0, 29.67628634929657, 0.0, 0.0, 0.0], 'rewardMean': 0.7914328135378996, 'totalEpisodes': 20, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.162404112119976
'totalSteps': 3840, 'rewardStep': 0.8571154936230175, 'errorList': [], 'lossList': [0.0, -1.414490858912468, 0.0, 61.97550220489502, 0.0, 0.0, 0.0], 'rewardMean': 0.8133270402329389, 'totalEpisodes': 65, 'stepsPerEpisode': 55, 'rewardPerEpisode': 39.64476409708963
'totalSteps': 5120, 'rewardStep': 0.48872876996115544, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6834877321242254, 'totalEpisodes': 114, 'stepsPerEpisode': 20, 'rewardPerEpisode': 12.59649987899958
'totalSteps': 6400, 'rewardStep': 0.8802372917216534, 'errorList': [], 'lossList': [0.0, -1.4059055626392365, 0.0, 60.53200729370117, 0.0, 0.0, 0.0], 'rewardMean': 0.7162793253904635, 'totalEpisodes': 150, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.427565989098323
'totalSteps': 7680, 'rewardStep': 0.5722251903653415, 'errorList': [], 'lossList': [0.0, -1.3850230091810227, 0.0, 70.5114524269104, 0.0, 0.0, 0.0], 'rewardMean': 0.6957001632440175, 'totalEpisodes': 187, 'stepsPerEpisode': 38, 'rewardPerEpisode': 29.987972372088162
'totalSteps': 8960, 'rewardStep': 0.8206481074754737, 'errorList': [], 'lossList': [0.0, -1.3669637113809585, 0.0, 56.669573593139646, 0.0, 0.0, 0.0], 'rewardMean': 0.7113186562729495, 'totalEpisodes': 210, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.880627112143959
'totalSteps': 10240, 'rewardStep': 0.8642067907810154, 'errorList': [], 'lossList': [0.0, -1.3381947553157807, 0.0, 29.560792207717896, 0.0, 0.0, 0.0], 'rewardMean': 0.7283062267738457, 'totalEpisodes': 220, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.964948182713902
'totalSteps': 11520, 'rewardStep': 0.9278685934207582, 'errorList': [], 'lossList': [0.0, -1.309297485947609, 0.0, 36.41351793289184, 0.0, 0.0, 0.0], 'rewardMean': 0.748262463438537, 'totalEpisodes': 230, 'stepsPerEpisode': 24, 'rewardPerEpisode': 21.28154661605792
'totalSteps': 12800, 'rewardStep': 0.8049282621704065, 'errorList': [], 'lossList': [0.0, -1.2956090414524077, 0.0, 35.85710553646088, 0.0, 0.0, 0.0], 'rewardMean': 0.7538262032256495, 'totalEpisodes': 241, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.6042998478603
'totalSteps': 14080, 'rewardStep': 0.9215118106499848, 'errorList': [], 'lossList': [0.0, -1.2911341625452042, 0.0, 8.26178121805191, 0.0, 0.0, 0.0], 'rewardMean': 0.7626199080129963, 'totalEpisodes': 248, 'stepsPerEpisode': 64, 'rewardPerEpisode': 56.946594562704774
'totalSteps': 15360, 'rewardStep': 0.7605093312331356, 'errorList': [], 'lossList': [0.0, -1.2860557097196579, 0.0, 36.08889471054077, 0.0, 0.0, 0.0], 'rewardMean': 0.752959291774008, 'totalEpisodes': 257, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.5451672072341738
'totalSteps': 16640, 'rewardStep': 0.6319024917339954, 'errorList': [], 'lossList': [0.0, -1.2798812246322633, 0.0, 16.027209913730623, 0.0, 0.0, 0.0], 'rewardMean': 0.767276663951292, 'totalEpisodes': 264, 'stepsPerEpisode': 133, 'rewardPerEpisode': 115.60140111192615
'totalSteps': 17920, 'rewardStep': 0.8196446122773734, 'errorList': [], 'lossList': [0.0, -1.2474311810731888, 0.0, 5.2202452230453495, 0.0, 0.0, 0.0], 'rewardMean': 0.8003682481829137, 'totalEpisodes': 269, 'stepsPerEpisode': 53, 'rewardPerEpisode': 49.49264319789117
'totalSteps': 19200, 'rewardStep': 0.589666547946528, 'errorList': [], 'lossList': [0.0, -1.203858072757721, 0.0, 3.3110769337415693, 0.0, 0.0, 0.0], 'rewardMean': 0.7713111738054013, 'totalEpisodes': 273, 'stepsPerEpisode': 231, 'rewardPerEpisode': 181.548062779399
'totalSteps': 20480, 'rewardStep': 0.8118234920478369, 'errorList': [], 'lossList': [0.0, -1.2039948618412017, 0.0, 4.104973544478416, 0.0, 0.0, 0.0], 'rewardMean': 0.7952710039736508, 'totalEpisodes': 276, 'stepsPerEpisode': 174, 'rewardPerEpisode': 147.56517738924606
'totalSteps': 21760, 'rewardStep': 0.7092467467488077, 'errorList': [], 'lossList': [0.0, -1.2006150275468825, 0.0, 4.328179830312729, 0.0, 0.0, 0.0], 'rewardMean': 0.7841308679009842, 'totalEpisodes': 278, 'stepsPerEpisode': 540, 'rewardPerEpisode': 448.97899846194883
'totalSteps': 23040, 'rewardStep': 0.8700242465882637, 'errorList': [], 'lossList': [0.0, -1.1574582171440124, 0.0, 2.7561298653483393, 0.0, 0.0, 0.0], 'rewardMean': 0.784712613481709, 'totalEpisodes': 278, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.6367741401086
'totalSteps': 24320, 'rewardStep': 0.83021534649698, 'errorList': [], 'lossList': [0.0, -1.1287147319316864, 0.0, 1.5474580283463002, 0.0, 0.0, 0.0], 'rewardMean': 0.7749472887893313, 'totalEpisodes': 278, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.8862349169035
'totalSteps': 25600, 'rewardStep': 0.9593753011896018, 'errorList': [0.04885717008909602, 0.05011788878187567, 0.05334094541585231, 0.052329767583267024, 0.052025776977895705, 0.050192964971702524, 0.053122823078768054, 0.04507186803683989, 0.05356035718727849, 0.04659079452633035, 0.05299343329724341, 0.05297382052747845, 0.05361538132271226, 0.053445341102437705, 0.051840072979082656, 0.05174465106747245, 0.048228296790617686, 0.049416356302265804, 0.04739187103285913, 0.0528920649986619, 0.04697078909731918, 0.05120209776604001, 0.04768871671768975, 0.05121319277882039, 0.05210072994960889, 0.05002495228539807, 0.05328281884840636, 0.05272995530992253, 0.056965055732135, 0.04805597220564187, 0.046167824000642586, 0.0497725205663247, 0.05619656198112421, 0.04752954456856487, 0.045226382894322896, 0.04515810406147973, 0.04635936609292137, 0.051885216167198586, 0.04890213228996307, 0.05030064353449173, 0.045134004820630624, 0.04995249245896468, 0.05242308486176691, 0.05241148235116655, 0.051937927361708484, 0.04882218342951179, 0.053363660167951174, 0.04993828844892269, 0.04888635357866521, 0.04739325068320692], 'lossList': [0.0, -1.0913738924264909, 0.0, 1.5690249387174844, 0.0, 0.0, 0.0], 'rewardMean': 0.7903919926912508, 'totalEpisodes': 278, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1191.8702530438095, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=78.55
