#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 125
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.8254904346552838, 'errorList': [], 'lossList': [0.0, -1.4131022453308106, 0.0, 33.53930220603943, 0.0, 0.0, 0.0], 'rewardMean': 0.6348565878154008, 'totalEpisodes': 77, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.375370707206394
'totalSteps': 3840, 'rewardStep': 0.686192791294489, 'errorList': [], 'lossList': [0.0, -1.4053840231895447, 0.0, 38.37874718666077, 0.0, 0.0, 0.0], 'rewardMean': 0.6519686556417635, 'totalEpisodes': 131, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.5678903909443314
'totalSteps': 5120, 'rewardStep': 0.6914784500897183, 'errorList': [], 'lossList': [0.0, -1.3896526223421097, 0.0, 46.0183101272583, 0.0, 0.0, 0.0], 'rewardMean': 0.6618461042537522, 'totalEpisodes': 165, 'stepsPerEpisode': 57, 'rewardPerEpisode': 43.29947657106684
'totalSteps': 6400, 'rewardStep': 0.3989786352983312, 'errorList': [], 'lossList': [0.0, -1.3826303637027741, 0.0, 43.95277976036072, 0.0, 0.0, 0.0], 'rewardMean': 0.609272610462668, 'totalEpisodes': 181, 'stepsPerEpisode': 82, 'rewardPerEpisode': 55.761351441035494
'totalSteps': 7680, 'rewardStep': 0.7586403293952347, 'errorList': [], 'lossList': [0.0, -1.3794762659072877, 0.0, 34.90591012477875, 0.0, 0.0, 0.0], 'rewardMean': 0.6341672302847625, 'totalEpisodes': 190, 'stepsPerEpisode': 138, 'rewardPerEpisode': 105.39176312341041
'totalSteps': 8960, 'rewardStep': 0.6824414432532608, 'errorList': [], 'lossList': [0.0, -1.3788510459661483, 0.0, 18.868883543014526, 0.0, 0.0, 0.0], 'rewardMean': 0.6410635464231194, 'totalEpisodes': 195, 'stepsPerEpisode': 271, 'rewardPerEpisode': 222.03704181268284
'totalSteps': 10240, 'rewardStep': 0.5397640229941908, 'errorList': [], 'lossList': [0.0, -1.3987489408254623, 0.0, 31.977124242782594, 0.0, 0.0, 0.0], 'rewardMean': 0.6284011059945033, 'totalEpisodes': 202, 'stepsPerEpisode': 109, 'rewardPerEpisode': 82.907261239216
'totalSteps': 11520, 'rewardStep': 0.5822873953188961, 'errorList': [], 'lossList': [0.0, -1.384586465358734, 0.0, 13.247370567321777, 0.0, 0.0, 0.0], 'rewardMean': 0.6232773603638803, 'totalEpisodes': 204, 'stepsPerEpisode': 334, 'rewardPerEpisode': 257.20609675250125
'totalSteps': 12800, 'rewardStep': 0.6333143155728826, 'errorList': [], 'lossList': [0.0, -1.3614796847105026, 0.0, 9.285119907259942, 0.0, 0.0, 0.0], 'rewardMean': 0.6242810558847804, 'totalEpisodes': 205, 'stepsPerEpisode': 467, 'rewardPerEpisode': 379.0908149500719
'totalSteps': 14080, 'rewardStep': 0.5498769986067076, 'errorList': [], 'lossList': [0.0, -1.3318107813596725, 0.0, 28.475010404586794, 0.0, 0.0, 0.0], 'rewardMean': 0.6348464816478996, 'totalEpisodes': 209, 'stepsPerEpisode': 213, 'rewardPerEpisode': 153.13374859828545
'totalSteps': 15360, 'rewardStep': 0.8553334267751056, 'errorList': [], 'lossList': [0.0, -1.3322569739818573, 0.0, 5.020158351063729, 0.0, 0.0, 0.0], 'rewardMean': 0.6378307808598818, 'totalEpisodes': 213, 'stepsPerEpisode': 50, 'rewardPerEpisode': 45.6537007052063
'totalSteps': 16640, 'rewardStep': 0.7634918604239245, 'errorList': [], 'lossList': [0.0, -1.3288932949304582, 0.0, 3.0077210323512555, 0.0, 0.0, 0.0], 'rewardMean': 0.6455606877728253, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.6201013269464
'totalSteps': 17920, 'rewardStep': 0.7801957258582102, 'errorList': [], 'lossList': [0.0, -1.309964965581894, 0.0, 1.904757409542799, 0.0, 0.0, 0.0], 'rewardMean': 0.6544324153496743, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.6435588153695
'totalSteps': 19200, 'rewardStep': 0.9322456276021408, 'errorList': [0.08559894734986488, 0.05069782036954059, 0.1286182585115997, 0.08463793489633883, 0.15179844589949684, 0.10009813310112027, 0.13793640455436235, 0.08170458543055335, 0.14719191911776264, 0.15076331241816457, 0.1605824066461035, 0.059617277977603536, 0.14507168144161656, 0.07121027667759962, 0.08337995959033578, 0.08011502449947185, 0.08619721131914837, 0.08542528492768156, 0.22883726297684834, 0.0825197502099456, 0.09427713480026914, 0.12455933203466756, 0.057842471820403925, 0.09542877708273029, 0.1360078854442267, 0.10231518547071793, 0.10524206892750304, 0.09395426798687834, 0.08201461807071218, 0.13587075554089023, 0.07926236258456924, 0.15693137894564285, 0.1327706246517482, 0.051572951033305156, 0.05777573126651538, 0.18006632645402407, 0.13503274403874685, 0.21086690975737651, 0.1231904363270355, 0.1297968347094853, 0.09220798677032785, 0.10648928618029169, 0.07513649069124957, 0.08967719910473816, 0.0764554670187818, 0.08462632754019088, 0.08500541615426047, 0.10042184373368937, 0.04672166072525546, 0.07755834879350883], 'lossList': [0.0, -1.2704124349355697, 0.0, 2.065045443102717, 0.0, 0.0, 0.0], 'rewardMean': 0.7077591145800554, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.2040187139103, 'successfulTests': 48
'totalSteps': 20480, 'rewardStep': 0.9494575068919626, 'errorList': [0.10977203542033391, 0.09494532405862732, 0.14090211138361264, 0.15321178892568338, 0.09691271683846742, 0.10142091275600763, 0.04931877957992926, 0.10225332406946638, 0.09069424000866083, 0.07314960940633747, 0.08400523370798733, 0.07723706177164223, 0.11321893013244466, 0.06345097243956863, 0.12295289256188643, 0.08869749964905799, 0.08925702564095164, 0.13915326546890852, 0.18631498437989263, 0.0664025764438655, 0.07275324028346812, 0.08101869415518784, 0.076944759296185, 0.13273039565227054, 0.13578741687858556, 0.16535723422596985, 0.1085514705489543, 0.07918981310895223, 0.09015089936748556, 0.13042164699254305, 0.08996867694390796, 0.1267727939718314, 0.10753018326098059, 0.07689081132656198, 0.12069736588093162, 0.10199961882282818, 0.08129167978830021, 0.1784727640783105, 0.09023103965866462, 0.21726605314000236, 0.10337272475347734, 0.16126716432348778, 0.0704326870894819, 0.09219971640738016, 0.12798222310274762, 0.04580656964399457, 0.13190548591832332, 0.09592631488083395, 0.08864960394228308, 0.07796477762080432], 'lossList': [0.0, -1.2521942710876466, 0.0, 1.6070198463276029, 0.0, 0.0, 0.0], 'rewardMean': 0.7268408323297282, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.9262840829765, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.8930907105100552, 'errorList': [], 'lossList': [0.0, -1.226718514561653, 0.0, 1.0932542815431951, 0.0, 0.0, 0.0], 'rewardMean': 0.7479057590554076, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1140.6149621511329
'totalSteps': 23040, 'rewardStep': 0.8824425492850573, 'errorList': [], 'lossList': [0.0, -1.2146356427669525, 0.0, 0.9420850067213178, 0.0, 0.0, 0.0], 'rewardMean': 0.7821736116844942, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.1930582567165
'totalSteps': 24320, 'rewardStep': 0.7975931235909937, 'errorList': [], 'lossList': [0.0, -1.1864035946130753, 0.0, 0.7509340511448681, 0.0, 0.0, 0.0], 'rewardMean': 0.8037041845117041, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1196.940048250406
'totalSteps': 25600, 'rewardStep': 0.9702621914917595, 'errorList': [0.09516679339434057, 0.11471019033195491, 0.125600869952776, 0.05255834453679852, 0.09370968620748996, 0.05892866576364973, 0.028115813077251545, 0.06151505550076542, 0.06863840348354679, 0.05582154559734924, 0.049851672382081276, 0.08282542734392044, 0.06202479010456127, 0.09733267190428488, 0.05983421624382494, 0.030988076034694632, 0.07238658515114789, 0.048629041501903704, 0.0370007392920191, 0.10088399055917621, 0.06584176778805886, 0.05808812734451888, 0.04441593472243839, 0.06089634216472385, 0.13758816636104557, 0.04415960031625245, 0.1293971078418313, 0.034436955441817574, 0.05023830096971522, 0.059901326901757854, 0.05952841880629047, 0.06377400461948933, 0.038379889017530026, 0.04134904621431217, 0.050588705996207024, 0.06654399614906505, 0.040232331624565645, 0.05889828090770629, 0.09356397726661615, 0.049384698339784705, 0.046603562175941846, 0.14668284856890576, 0.07598452139361823, 0.080164034541015, 0.03764167023832621, 0.1322760929471163, 0.045691045512098565, 0.04854134981933005, 0.04444442858171394, 0.038524795909878784], 'lossList': [0.0, -1.1431729716062546, 0.0, 0.5127999761328101, 0.0, 0.0, 0.0], 'rewardMean': 0.8373989721035917, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1208.5436079731164, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=119.82
