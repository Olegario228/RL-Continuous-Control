#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 126
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.8991678567792021, 'errorList': [], 'lossList': [0.0, -1.4135684698820115, 0.0, 32.5803185749054, 0.0, 0.0, 0.0], 'rewardMean': 0.8571089948103048, 'totalEpisodes': 78, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.331691532610762
'totalSteps': 3840, 'rewardStep': 0.5619029719320052, 'errorList': [], 'lossList': [0.0, -1.4027688682079316, 0.0, 43.23437029838562, 0.0, 0.0, 0.0], 'rewardMean': 0.758706987184205, 'totalEpisodes': 123, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.390797897767468
'totalSteps': 5120, 'rewardStep': 0.6328692359517767, 'errorList': [], 'lossList': [0.0, -1.389101305603981, 0.0, 49.64425905227661, 0.0, 0.0, 0.0], 'rewardMean': 0.7272475493760979, 'totalEpisodes': 153, 'stepsPerEpisode': 28, 'rewardPerEpisode': 20.232497909181593
'totalSteps': 6400, 'rewardStep': 0.25450531301734375, 'errorList': [], 'lossList': [0.0, -1.371378099322319, 0.0, 49.54037244796753, 0.0, 0.0, 0.0], 'rewardMean': 0.6326991021043471, 'totalEpisodes': 171, 'stepsPerEpisode': 83, 'rewardPerEpisode': 51.22710233021793
'totalSteps': 7680, 'rewardStep': 0.5380145650688004, 'errorList': [], 'lossList': [0.0, -1.358339604139328, 0.0, 38.42700856685639, 0.0, 0.0, 0.0], 'rewardMean': 0.6169183459317559, 'totalEpisodes': 176, 'stepsPerEpisode': 368, 'rewardPerEpisode': 263.6964199692706
'totalSteps': 8960, 'rewardStep': 0.7466588855787868, 'errorList': [], 'lossList': [0.0, -1.3426006472110747, 0.0, 50.41854845046997, 0.0, 0.0, 0.0], 'rewardMean': 0.6354527087384747, 'totalEpisodes': 185, 'stepsPerEpisode': 109, 'rewardPerEpisode': 81.3058915042774
'totalSteps': 10240, 'rewardStep': 0.746658616731231, 'errorList': [], 'lossList': [0.0, -1.3332482087612152, 0.0, 21.992530246973036, 0.0, 0.0, 0.0], 'rewardMean': 0.6493534472375693, 'totalEpisodes': 190, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.58526481539806
'totalSteps': 11520, 'rewardStep': 0.3131034180459051, 'errorList': [], 'lossList': [0.0, -1.3214762383699417, 0.0, 23.511477487087248, 0.0, 0.0, 0.0], 'rewardMean': 0.61199233288294, 'totalEpisodes': 194, 'stepsPerEpisode': 187, 'rewardPerEpisode': 142.04416860874542
'totalSteps': 12800, 'rewardStep': 0.7540733182122545, 'errorList': [], 'lossList': [0.0, -1.310223172903061, 0.0, 30.552035200595856, 0.0, 0.0, 0.0], 'rewardMean': 0.6262004314158715, 'totalEpisodes': 199, 'stepsPerEpisode': 510, 'rewardPerEpisode': 425.38149589519844
'totalSteps': 14080, 'rewardStep': 0.5007947649866564, 'errorList': [], 'lossList': [0.0, -1.2960734522342683, 0.0, 7.5663216066360475, 0.0, 0.0, 0.0], 'rewardMean': 0.5947748946303962, 'totalEpisodes': 204, 'stepsPerEpisode': 264, 'rewardPerEpisode': 203.82215695070977
'totalSteps': 15360, 'rewardStep': 0.4211956373385061, 'errorList': [], 'lossList': [0.0, -1.2811665219068527, 0.0, 5.594541249871254, 0.0, 0.0, 0.0], 'rewardMean': 0.5469776726863266, 'totalEpisodes': 206, 'stepsPerEpisode': 359, 'rewardPerEpisode': 273.4925435506174
'totalSteps': 16640, 'rewardStep': 0.676386385792233, 'errorList': [], 'lossList': [0.0, -1.2546649503707885, 0.0, 31.05201933860779, 0.0, 0.0, 0.0], 'rewardMean': 0.5584260140723494, 'totalEpisodes': 213, 'stepsPerEpisode': 114, 'rewardPerEpisode': 88.31589553605353
'totalSteps': 17920, 'rewardStep': 0.6977773573770419, 'errorList': [], 'lossList': [0.0, -1.2330969989299774, 0.0, 25.540933904647826, 0.0, 0.0, 0.0], 'rewardMean': 0.5649168262148759, 'totalEpisodes': 214, 'stepsPerEpisode': 654, 'rewardPerEpisode': 495.7545120453494
'totalSteps': 19200, 'rewardStep': 0.8197294534626118, 'errorList': [], 'lossList': [0.0, -1.2050796937942505, 0.0, 2.683650068938732, 0.0, 0.0, 0.0], 'rewardMean': 0.6214392402594028, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.5814155325631
'totalSteps': 20480, 'rewardStep': 0.7468264749903677, 'errorList': [], 'lossList': [0.0, -1.1692113852500916, 0.0, 1.5846162880957126, 0.0, 0.0, 0.0], 'rewardMean': 0.6423204312515594, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1038.1163212519455
'totalSteps': 21760, 'rewardStep': 0.6392091828306129, 'errorList': [], 'lossList': [0.0, -1.122448850274086, 0.0, 1.3802625603973866, 0.0, 0.0, 0.0], 'rewardMean': 0.631575460976742, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1018.0801958627792
'totalSteps': 23040, 'rewardStep': 0.8929058515566217, 'errorList': [], 'lossList': [0.0, -1.0877016830444335, 0.0, 0.8198085337877273, 0.0, 0.0, 0.0], 'rewardMean': 0.6462001844592812, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.760230813982
'totalSteps': 24320, 'rewardStep': 0.8214324865674256, 'errorList': [], 'lossList': [0.0, -1.0378141105175018, 0.0, 0.6249494591727853, 0.0, 0.0, 0.0], 'rewardMean': 0.6970330913114331, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.2914549247698
'totalSteps': 25600, 'rewardStep': 0.9928604227898759, 'errorList': [0.08198698209666132, 0.08808304665529237, 0.09789348650918799, 0.0807865387384497, 0.08731462490394487, 0.11473571577079278, 0.09586220922212332, 0.10247793089753097, 0.11357509479366364, 0.09699389276877111, 0.08747071696600772, 0.09983835377910402, 0.09720379429670903, 0.09395988081484821, 0.09059631624235837, 0.09420962798742998, 0.09680670909662953, 0.08882836397434093, 0.11223949430665336, 0.09662545977718458, 0.10486069057433411, 0.09136969991872979, 0.09415803463316195, 0.12092038527776632, 0.09180086607005919, 0.09869562530400035, 0.12310917725083352, 0.10712843735531263, 0.09458209667853106, 0.0911854097112692, 0.1073074178219433, 0.08027440842969034, 0.08422557243559291, 0.10762769063739563, 0.10617088565551937, 0.09516942898788183, 0.13581282628287752, 0.08350475876606551, 0.10116220278075339, 0.12225164810305758, 0.09350066390674656, 0.09267880890119962, 0.09266487786277457, 0.1073753209568718, 0.0987306315419466, 0.09795986556564067, 0.0916693249763975, 0.09954158575707536, 0.1065598285377859, 0.0886685500243574], 'lossList': [0.0, -0.9779246041178703, 0.0, 0.5905527152121067, 0.0, 0.0, 0.0], 'rewardMean': 0.7209118017691953, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.7419115282648, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.17
