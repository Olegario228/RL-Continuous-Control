#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 127
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.7350694247899606, 'errorList': [], 'lossList': [0.0, -1.4267381995916366, 0.0, 35.4862629032135, 0.0, 0.0, 0.0], 'rewardMean': 0.7753533832868351, 'totalEpisodes': 89, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.8604569401956565
'totalSteps': 3840, 'rewardStep': 0.5824100607866234, 'errorList': [], 'lossList': [0.0, -1.4106644314527512, 0.0, 45.97278860092163, 0.0, 0.0, 0.0], 'rewardMean': 0.7110389424534311, 'totalEpisodes': 130, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.538954154215
'totalSteps': 5120, 'rewardStep': 0.6335114825045962, 'errorList': [], 'lossList': [0.0, -1.3970266383886338, 0.0, 57.05509065628052, 0.0, 0.0, 0.0], 'rewardMean': 0.6916570774662224, 'totalEpisodes': 171, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2856156808146881
'totalSteps': 6400, 'rewardStep': 0.7201180603202199, 'errorList': [], 'lossList': [0.0, -1.3918633407354355, 0.0, 49.51317702293396, 0.0, 0.0, 0.0], 'rewardMean': 0.6973492740370218, 'totalEpisodes': 200, 'stepsPerEpisode': 24, 'rewardPerEpisode': 16.718308477815658
'totalSteps': 7680, 'rewardStep': 0.8992222969489619, 'errorList': [], 'lossList': [0.0, -1.386817716360092, 0.0, 49.273257913589475, 0.0, 0.0, 0.0], 'rewardMean': 0.7309947778556786, 'totalEpisodes': 218, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.47467376693417
'totalSteps': 8960, 'rewardStep': 0.8190217201477078, 'errorList': [], 'lossList': [0.0, -1.389016929268837, 0.0, 28.92700261116028, 0.0, 0.0, 0.0], 'rewardMean': 0.7435700553259685, 'totalEpisodes': 227, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.267524986131967
'totalSteps': 10240, 'rewardStep': 0.11229468649913016, 'errorList': [], 'lossList': [0.0, -1.3885800343751908, 0.0, 10.409837582111358, 0.0, 0.0, 0.0], 'rewardMean': 0.6646606342226138, 'totalEpisodes': 232, 'stepsPerEpisode': 181, 'rewardPerEpisode': 130.0128738668637
'totalSteps': 11520, 'rewardStep': 0.907514963391203, 'errorList': [], 'lossList': [0.0, -1.3699145954847336, 0.0, 16.742353757619856, 0.0, 0.0, 0.0], 'rewardMean': 0.6916444485746793, 'totalEpisodes': 239, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.14896108083668
'totalSteps': 12800, 'rewardStep': 0.3832089283151302, 'errorList': [], 'lossList': [0.0, -1.3592988604307175, 0.0, 33.00927386522293, 0.0, 0.0, 0.0], 'rewardMean': 0.6608008965487244, 'totalEpisodes': 243, 'stepsPerEpisode': 158, 'rewardPerEpisode': 95.27928991179202
'totalSteps': 14080, 'rewardStep': 0.6759447275107913, 'errorList': [], 'lossList': [0.0, -1.3467786121368408, 0.0, 8.531332826316357, 0.0, 0.0, 0.0], 'rewardMean': 0.6468316351214325, 'totalEpisodes': 245, 'stepsPerEpisode': 667, 'rewardPerEpisode': 557.5217828789467
'totalSteps': 15360, 'rewardStep': 0.8537152296007043, 'errorList': [], 'lossList': [0.0, -1.320912858247757, 0.0, 4.477511638402939, 0.0, 0.0, 0.0], 'rewardMean': 0.6586962156025068, 'totalEpisodes': 247, 'stepsPerEpisode': 569, 'rewardPerEpisode': 368.41076815195225
'totalSteps': 16640, 'rewardStep': 0.734087948933889, 'errorList': [], 'lossList': [0.0, -1.3195784163475037, 0.0, 3.667997639775276, 0.0, 0.0, 0.0], 'rewardMean': 0.6738640044172334, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.2009680795402
'totalSteps': 17920, 'rewardStep': 0.8714438000661074, 'errorList': [], 'lossList': [0.0, -1.287053507566452, 0.0, 2.8048014640808105, 0.0, 0.0, 0.0], 'rewardMean': 0.6976572361733845, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1087.4516705383992
'totalSteps': 19200, 'rewardStep': 0.905938191374181, 'errorList': [], 'lossList': [0.0, -1.2330453062057496, 0.0, 2.673611097931862, 0.0, 0.0, 0.0], 'rewardMean': 0.7162392492787807, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.2758194219452
'totalSteps': 20480, 'rewardStep': 0.9355655384307442, 'errorList': [0.09576292683496326, 0.13601341142136228, 0.1403974207047577, 0.16474694150063093, 0.12621956977342508, 0.1349217592821478, 0.10651288937496664, 0.10916229211153425, 0.09004460025552845, 0.12702539327882958, 0.12398982004097474, 0.10324600317367758, 0.13783066298346366, 0.12439353179345343, 0.15676607336972057, 0.19796682044766234, 0.1146159832726742, 0.189523087313943, 0.11633538414413086, 0.15174333258983166, 0.11917307742968285, 0.13244056584441508, 0.14035140767437235, 0.13589750441641124, 0.13306209476239272, 0.15943481677486662, 0.16608581940928246, 0.13301420654329302, 0.13325158794612932, 0.10934250686586848, 0.10335518940397259, 0.16442242604403745, 0.18690204467775604, 0.10832135067187139, 0.11231993608278013, 0.12830531746837362, 0.10907447337952006, 0.11661954036282013, 0.12485097422696308, 0.20763691542037843, 0.1399541500846367, 0.11585343069197675, 0.17647184487922143, 0.13789972559357538, 0.12205720076035541, 0.12843629371507742, 0.09840092347582156, 0.11294407280194774, 0.13884689472641312, 0.1508303569284099], 'lossList': [0.0, -1.1908173173666001, 0.0, 2.251977138891816, 0.0, 0.0, 0.0], 'rewardMean': 0.7198735734269588, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.72409420468, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.9774975948493784, 'errorList': [0.15087416553462876, 0.14915401249539997, 0.166206996296737, 0.13373030495076468, 0.16033382792467188, 0.16202745648618372, 0.15375160786716405, 0.15889583974973795, 0.14348729305371355, 0.16239764109315413, 0.14903184943739012, 0.1299407414347094, 0.18638173611593542, 0.13905744049244104, 0.18158088505852285, 0.144025214888316, 0.1349556318481457, 0.16853469913969601, 0.15489866888333284, 0.1693096567803242, 0.18886686071217115, 0.17383753442886218, 0.14633643282456965, 0.15855820286848846, 0.18355497682731492, 0.14595650279950317, 0.12707553909723932, 0.13196753395371286, 0.19071588873404324, 0.12860332067647068, 0.1476479721569302, 0.17735064313383403, 0.1651208655912066, 0.15378508624182516, 0.16439925990770682, 0.16440045686748067, 0.14174411693033903, 0.14614289165152158, 0.14937372022674086, 0.1961081947150622, 0.13468685886067952, 0.18585277259987992, 0.16115725591580327, 0.17635287447281134, 0.16091413837424867, 0.17404468280578292, 0.1498053963586575, 0.14156393784399104, 0.21360346570338334, 0.14643426268950854], 'lossList': [0.0, -1.1477201920747757, 0.0, 1.353748199623078, 0.0, 0.0, 0.0], 'rewardMean': 0.7357211608971259, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.6109364242693, 'successfulTests': 49
'totalSteps': 23040, 'rewardStep': 0.9560696255631351, 'errorList': [0.03259171177959333, 0.04081224797516117, 0.04155085697231657, 0.0745059557074072, 0.07111796185041451, 0.044020302435476964, 0.0866710054959, 0.13502103697260406, 0.1155457211329408, 0.06999723889968422, 0.04183722446975539, 0.12730860382013162, 0.034105397041723495, 0.060419295268502676, 0.10337491647794038, 0.06031559483910401, 0.044311841921215074, 0.08319539388625222, 0.03158648351228407, 0.22011152906715792, 0.08485875297243577, 0.17662262828332187, 0.07291489622696694, 0.04218849562018118, 0.07595114267886524, 0.057867615685698585, 0.0471838078646097, 0.14200741814225934, 0.0611511937753545, 0.05511857108652985, 0.04046359369950745, 0.11413677129302693, 0.10155744069614263, 0.14601347951860197, 0.04052906736492881, 0.10392691748416469, 0.06367975287581888, 0.053313894482493636, 0.1267911903126873, 0.16085028460793402, 0.06442766383271321, 0.04799870931695676, 0.036638683833653006, 0.04324716511585188, 0.08555765561601299, 0.11019870464712632, 0.07388762724082952, 0.17899882485964416, 0.08428631327686012, 0.05355622981905682], 'lossList': [0.0, -1.116243940591812, 0.0, 0.8942352592200041, 0.0, 0.0, 0.0], 'rewardMean': 0.8200986548035264, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1196.4104094775435, 'successfulTests': 49
'totalSteps': 24320, 'rewardStep': 0.9355682762191095, 'errorList': [0.03180364291715552, 0.043557878827076374, 0.0693726794260358, 0.02899642464202417, 0.02452430815347946, 0.031067098644032592, 0.08997631669046678, 0.011578012576811658, 0.04053489076015219, 0.03348117203537028, 0.07258351521527896, 0.054878481597828446, 0.05161359861530692, 0.028526636094337823, 0.011845282404512042, 0.04971177170036872, 0.03752688485499614, 0.09180662506295306, 0.010502468905355156, 0.04698139741635795, 0.02369674278114755, 0.02934558295975938, 0.03314987479548393, 0.06128870739111307, 0.08142437054578053, 0.0419281626451952, 0.051753949294371826, 0.011412084567845891, 0.013684942614760533, 0.04072900321892926, 0.026779854783761152, 0.03588021940169755, 0.06234564546319294, 0.033987913619011795, 0.05766222334822092, 0.028838287648526403, 0.04776739236504395, 0.06301484503621849, 0.031184116749338183, 0.03012222170728658, 0.04224166391903567, 0.09212741750821442, 0.025966569357021723, 0.05608308335982184, 0.060102054829020944, 0.026865217945920453, 0.02245881150655565, 0.07038976332995237, 0.04658755579906486, 0.014694035536502321], 'lossList': [0.0, -1.0843939644098282, 0.0, 0.7356267899274826, 0.0, 0.0, 0.0], 'rewardMean': 0.8229039860863171, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1214.2413077755964, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=136.58
