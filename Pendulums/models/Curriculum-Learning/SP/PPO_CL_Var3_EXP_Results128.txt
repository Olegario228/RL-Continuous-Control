#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 128
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.8017934872621256, 'errorList': [], 'lossList': [0.0, -1.4332990115880966, 0.0, 31.391083822250366, 0.0, 0.0, 0.0], 'rewardMean': 0.6533663570012552, 'totalEpisodes': 81, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.6325335162668906
'totalSteps': 3840, 'rewardStep': 0.6823854954053024, 'errorList': [], 'lossList': [0.0, -1.4270323592424392, 0.0, 44.99822108268738, 0.0, 0.0, 0.0], 'rewardMean': 0.6630394031359376, 'totalEpisodes': 130, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.7638201675342176
'totalSteps': 5120, 'rewardStep': 0.6227652779549024, 'errorList': [], 'lossList': [0.0, -1.421262159347534, 0.0, 49.98425161361694, 0.0, 0.0, 0.0], 'rewardMean': 0.6529708718406788, 'totalEpisodes': 162, 'stepsPerEpisode': 39, 'rewardPerEpisode': 29.777917250153262
'totalSteps': 6400, 'rewardStep': 0.756420911824394, 'errorList': [], 'lossList': [0.0, -1.410651186108589, 0.0, 56.277389678955075, 0.0, 0.0, 0.0], 'rewardMean': 0.6736608798374218, 'totalEpisodes': 179, 'stepsPerEpisode': 169, 'rewardPerEpisode': 123.83397842058307
'totalSteps': 7680, 'rewardStep': 0.8458791181747123, 'errorList': [], 'lossList': [0.0, -1.3981805473566056, 0.0, 57.383213663101195, 0.0, 0.0, 0.0], 'rewardMean': 0.7023639195603035, 'totalEpisodes': 193, 'stepsPerEpisode': 84, 'rewardPerEpisode': 61.80835659842205
'totalSteps': 8960, 'rewardStep': 0.6174160907083213, 'errorList': [], 'lossList': [0.0, -1.3935193634033203, 0.0, 35.389411811828616, 0.0, 0.0, 0.0], 'rewardMean': 0.6902285154385918, 'totalEpisodes': 201, 'stepsPerEpisode': 108, 'rewardPerEpisode': 75.16709448172551
'totalSteps': 10240, 'rewardStep': 0.7710399408343691, 'errorList': [], 'lossList': [0.0, -1.392659073472023, 0.0, 13.696060831546783, 0.0, 0.0, 0.0], 'rewardMean': 0.700329943613064, 'totalEpisodes': 205, 'stepsPerEpisode': 215, 'rewardPerEpisode': 181.08992846727418
'totalSteps': 11520, 'rewardStep': 0.6706993627718902, 'errorList': [], 'lossList': [0.0, -1.3825914287567138, 0.0, 9.45967140674591, 0.0, 0.0, 0.0], 'rewardMean': 0.6970376568529336, 'totalEpisodes': 207, 'stepsPerEpisode': 571, 'rewardPerEpisode': 435.51870368579955
'totalSteps': 12800, 'rewardStep': 0.7195490279300847, 'errorList': [], 'lossList': [0.0, -1.367078697681427, 0.0, 16.305963198542596, 0.0, 0.0, 0.0], 'rewardMean': 0.6992887939606487, 'totalEpisodes': 208, 'stepsPerEpisode': 582, 'rewardPerEpisode': 424.9824294608202
'totalSteps': 14080, 'rewardStep': 0.918965439666002, 'errorList': [], 'lossList': [0.0, -1.381041494011879, 0.0, 3.6317418697476387, 0.0, 0.0, 0.0], 'rewardMean': 0.7406914152532105, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 888.6569653616338
'totalSteps': 15360, 'rewardStep': 0.7977613379269965, 'errorList': [], 'lossList': [0.0, -1.379688732624054, 0.0, 2.9311960890889166, 0.0, 0.0, 0.0], 'rewardMean': 0.7402882003196976, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 981.4950584002909
'totalSteps': 16640, 'rewardStep': 0.8750104210460041, 'errorList': [], 'lossList': [0.0, -1.3582647836208344, 0.0, 2.392075769007206, 0.0, 0.0, 0.0], 'rewardMean': 0.7595506928837676, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.359463926422
'totalSteps': 17920, 'rewardStep': 0.869738601046381, 'errorList': [], 'lossList': [0.0, -1.3500290036201477, 0.0, 1.5123424420133234, 0.0, 0.0, 0.0], 'rewardMean': 0.7842480251929155, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.4562460278398
'totalSteps': 19200, 'rewardStep': 0.816697789028352, 'errorList': [], 'lossList': [0.0, -1.3372213250398637, 0.0, 1.7141179313883186, 0.0, 0.0, 0.0], 'rewardMean': 0.7902757129133112, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.6032651992577
'totalSteps': 20480, 'rewardStep': 0.9460328492740988, 'errorList': [0.29666149088337573, 0.23029938942829722, 0.32614276078465865, 0.3155080742443925, 0.3110308108471016, 0.2710473061515983, 0.34386330669040516, 0.2836857452124074, 0.2918955711061182, 0.3279005181580615, 0.28449701027130564, 0.2762603216313837, 0.275650217353813, 0.30241587234432543, 0.2802263003201704, 0.23037519367212922, 0.3106362030107754, 0.35143683885607796, 0.3154687795378664, 0.35773819480856905, 0.2528710538348907, 0.2817355606528665, 0.4541069701711238, 0.31041963240816156, 0.26472931496373053, 0.26113801704679274, 0.3168357091608081, 0.3378313582789785, 0.333178887715602, 0.33890712841498544, 0.25275744240439774, 0.3387029226805369, 0.3060396604393743, 0.30557418771871087, 0.392599470485284, 0.314185323038438, 0.3390897380992034, 0.3349668052303443, 0.2204202778948894, 0.234409301409654, 0.27578234836693216, 0.3816169912328031, 0.29332762566871773, 0.31471662532901246, 0.33006873597232583, 0.34484106876257425, 0.24472606206030104, 0.278793478356089, 0.32846451509022945, 0.31830992047088497], 'lossList': [0.0, -1.296314418911934, 0.0, 1.3315183195844293, 0.0, 0.0, 0.0], 'rewardMean': 0.80029108602325, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.1062504516983, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8876009712643391, 'errorList': [], 'lossList': [0.0, -1.2700160866975785, 0.0, 0.4775005871243775, 0.0, 0.0, 0.0], 'rewardMean': 0.8273095740788519, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1086.538215311184
'totalSteps': 23040, 'rewardStep': 0.8243722878792391, 'errorList': [], 'lossList': [0.0, -1.2693776124715805, 0.0, 0.337091463226825, 0.0, 0.0, 0.0], 'rewardMean': 0.8326428087833386, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.2487211230184
'totalSteps': 24320, 'rewardStep': 0.9091955968168663, 'errorList': [], 'lossList': [0.0, -1.2497964972257614, 0.0, 0.4122832268290222, 0.0, 0.0, 0.0], 'rewardMean': 0.8564924321878363, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.5516766116907
'totalSteps': 25600, 'rewardStep': 0.9130030726102502, 'errorList': [], 'lossList': [0.0, -1.2202063989639282, 0.0, 0.3533826202806085, 0.0, 0.0, 0.0], 'rewardMean': 0.875837836655853, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1193.4800872145206
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.49
