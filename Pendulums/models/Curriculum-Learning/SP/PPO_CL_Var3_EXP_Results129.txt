#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 129
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.9622123937776887, 'errorList': [], 'lossList': [0.0, -1.4116418886184692, 0.0, 29.112571678161622, 0.0, 0.0, 0.0], 'rewardMean': 0.941651263964548, 'totalEpisodes': 89, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.8934981314201396
'totalSteps': 3840, 'rewardStep': 0.7972121905632695, 'errorList': [], 'lossList': [0.0, -1.4024979895353318, 0.0, 40.89885219573975, 0.0, 0.0, 0.0], 'rewardMean': 0.8935049061641219, 'totalEpisodes': 134, 'stepsPerEpisode': 38, 'rewardPerEpisode': 27.04066498971547
'totalSteps': 5120, 'rewardStep': 0.8557826745420628, 'errorList': [], 'lossList': [0.0, -1.3955324441194534, 0.0, 47.2206391620636, 0.0, 0.0, 0.0], 'rewardMean': 0.8840743482586071, 'totalEpisodes': 165, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.317904846047332
'totalSteps': 6400, 'rewardStep': 0.9052257414090755, 'errorList': [], 'lossList': [0.0, -1.396437560915947, 0.0, 37.079625387191776, 0.0, 0.0, 0.0], 'rewardMean': 0.8883046268887007, 'totalEpisodes': 182, 'stepsPerEpisode': 34, 'rewardPerEpisode': 27.155284383429134
'totalSteps': 7680, 'rewardStep': 0.6422956135369543, 'errorList': [], 'lossList': [0.0, -1.3788729310035706, 0.0, 46.7726763343811, 0.0, 0.0, 0.0], 'rewardMean': 0.8473031246634096, 'totalEpisodes': 192, 'stepsPerEpisode': 40, 'rewardPerEpisode': 29.083256863785987
'totalSteps': 8960, 'rewardStep': 0.7876171713459572, 'errorList': [], 'lossList': [0.0, -1.3616818547248841, 0.0, 40.70163872241974, 0.0, 0.0, 0.0], 'rewardMean': 0.8387765599037735, 'totalEpisodes': 200, 'stepsPerEpisode': 100, 'rewardPerEpisode': 75.930813864027
'totalSteps': 10240, 'rewardStep': 0.6482178018313143, 'errorList': [], 'lossList': [0.0, -1.3574266672134399, 0.0, 20.387087713479996, 0.0, 0.0, 0.0], 'rewardMean': 0.8149567151447162, 'totalEpisodes': 204, 'stepsPerEpisode': 141, 'rewardPerEpisode': 99.6608270923601
'totalSteps': 11520, 'rewardStep': 0.5367450794609991, 'errorList': [], 'lossList': [0.0, -1.3462576472759247, 0.0, 24.416958045363426, 0.0, 0.0, 0.0], 'rewardMean': 0.7840443111798588, 'totalEpisodes': 207, 'stepsPerEpisode': 210, 'rewardPerEpisode': 149.09775792364314
'totalSteps': 12800, 'rewardStep': 0.8904886761986646, 'errorList': [], 'lossList': [0.0, -1.335581021308899, 0.0, 17.152851121425627, 0.0, 0.0, 0.0], 'rewardMean': 0.7946887476817394, 'totalEpisodes': 212, 'stepsPerEpisode': 63, 'rewardPerEpisode': 58.4629751793139
'totalSteps': 14080, 'rewardStep': 0.9658198201987609, 'errorList': [2.1449461778504073, 2.9306662440642057, 2.2230950496580126, 2.1815599663251803, 2.6792658476384603, 2.908863475608451, 2.218883260302532, 2.660205028218097, 2.4975434125050184, 2.8251719602134604, 2.627784619060573, 3.065041392897463, 2.9463711947190436, 2.9936221059007915, 2.055690261981096, 2.284615825213341, 1.9056614430143395, 2.27184060188393, 2.435641145432546, 2.9549105187037057, 2.128770253983205, 3.0822044320993682, 2.6078223590622107, 2.513247311183899, 2.642939881924879, 2.263920015609289, 2.6405372557458104, 2.4442180227310963, 2.3836722119405307, 2.158718164617985, 2.2724840499298655, 3.0692677213741146, 3.0729984707521965, 2.1946708542852362, 2.097035535687311, 2.6116638533088437, 2.003514392035656, 2.506888385135728, 2.497618935177844, 2.0120737305944743, 2.432981915075869, 2.6435187622196636, 2.3011219132981124, 1.9317523682883002, 2.4152911247365583, 2.8623918977348937, 2.990508201416111, 2.2481707540862668, 2.8232076034729783, 2.628785529343081], 'lossList': [0.0, -1.3155713039636612, 0.0, 7.352553769946098, 0.0, 0.0, 0.0], 'rewardMean': 0.7991617162864747, 'totalEpisodes': 213, 'stepsPerEpisode': 100, 'rewardPerEpisode': 87.95913969807162, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.8248943985274289, 'errorList': [], 'lossList': [0.0, -1.2848648476600646, 0.0, 29.546968854665757, 0.0, 0.0, 0.0], 'rewardMean': 0.7854299167614487, 'totalEpisodes': 217, 'stepsPerEpisode': 160, 'rewardPerEpisode': 142.43425371478867
'totalSteps': 16640, 'rewardStep': 0.6353754920640713, 'errorList': [], 'lossList': [0.0, -1.2641131156682968, 0.0, 7.276117771267891, 0.0, 0.0, 0.0], 'rewardMean': 0.7692462469115288, 'totalEpisodes': 221, 'stepsPerEpisode': 142, 'rewardPerEpisode': 123.74628329110128
'totalSteps': 17920, 'rewardStep': 0.4106415144694329, 'errorList': [], 'lossList': [0.0, -1.237785730957985, 0.0, 2.7834802293777465, 0.0, 0.0, 0.0], 'rewardMean': 0.7247321309042659, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.894260387618
'totalSteps': 19200, 'rewardStep': 0.7661731168040901, 'errorList': [], 'lossList': [0.0, -1.2188626712560653, 0.0, 1.733754659295082, 0.0, 0.0, 0.0], 'rewardMean': 0.7108268684437674, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 957.5670626892031
'totalSteps': 20480, 'rewardStep': 0.9098526696227176, 'errorList': [], 'lossList': [0.0, -1.1834456419944763, 0.0, 1.529835380911827, 0.0, 0.0, 0.0], 'rewardMean': 0.7375825740523437, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.1834846938316
'totalSteps': 21760, 'rewardStep': 0.9366487126691209, 'errorList': [0.025860807051337507, 0.047146694442034696, 0.02975240100017594, 0.03796870465437075, 0.01954381023108422, 0.08535882286495144, 0.03493742190707521, 0.029705508898302948, 0.025617114409279367, 0.010348309846999963, 0.03073140621643781, 0.08156639465401033, 0.02150150311904811, 0.03423185773548993, 0.0330962171596, 0.04386164929245208, 0.035217054478380375, 0.03192892071843431, 0.068725302042788, 0.03707337541193315, 0.06227517856174298, 0.03374835370346291, 0.030329377818977887, 0.023365919577586505, 0.03100636392694363, 0.04749543897604175, 0.029857164466738882, 0.011463212179200693, 0.018432023626360098, 0.01362819467925701, 0.030057701973416843, 0.026564200072468232, 0.07043362285885564, 0.053487747183879074, 0.05794907362237228, 0.05105460907768604, 0.0452905788590789, 0.02929955457090655, 0.017831832280866173, 0.012009548727378442, 0.03506209440905022, 0.03493805275252041, 0.05457458023064028, 0.05741936445105769, 0.05743133506942591, 0.032629061747799976, 0.05726527258259748, 0.05427063379860873, 0.05476319179214431, 0.021629172330985935], 'lossList': [0.0, -1.161739968061447, 0.0, 1.2009738966077566, 0.0, 0.0, 0.0], 'rewardMean': 0.7524857281846601, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.0481517912096, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=91.78
