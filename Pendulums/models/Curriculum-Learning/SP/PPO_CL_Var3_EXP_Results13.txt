#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 13
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.5773515039125415, 'errorList': [], 'lossList': [0.0, -1.4335807341337203, 0.0, 26.853659915924073, 0.0, 0.0, 0.0], 'rewardMean': 0.6713386684297988, 'totalEpisodes': 75, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.543977395715153
'totalSteps': 3840, 'rewardStep': 0.6516830030017632, 'errorList': [], 'lossList': [0.0, -1.4226842588186264, 0.0, 37.955718727111815, 0.0, 0.0, 0.0], 'rewardMean': 0.6647867799537869, 'totalEpisodes': 136, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.934755214013693
'totalSteps': 5120, 'rewardStep': 0.9134143246269335, 'errorList': [], 'lossList': [0.0, -1.4050934088230134, 0.0, 44.50597091674805, 0.0, 0.0, 0.0], 'rewardMean': 0.7269436661220736, 'totalEpisodes': 178, 'stepsPerEpisode': 72, 'rewardPerEpisode': 61.872164371803564
'totalSteps': 6400, 'rewardStep': 0.6969500095901168, 'errorList': [], 'lossList': [0.0, -1.395349144935608, 0.0, 46.46571165084839, 0.0, 0.0, 0.0], 'rewardMean': 0.7209449348156822, 'totalEpisodes': 204, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6969500095901168
'totalSteps': 7680, 'rewardStep': 0.9467959965436404, 'errorList': [10.251734444407745, 117.29939357878926, 79.4735411317062, 133.02721887966027, 5.910212378865805, 64.81628025423788, 47.868300679588316, 73.13048871963922, 83.80318374848598, 36.0856372057189, 23.251857381128588, 4.308130700870376, 101.71702647587576, 68.5980429700444, 82.66142118135988, 132.7193276547463, 36.453213177206756, 101.31806105109928, 67.6694590850483, 54.6005785375022, 63.1478473170715, 21.672065824047465, 85.07200487624307, 62.587016862641576, 75.50249744697122, 68.26972470526158, 19.81054578730901, 59.910005938970556, 73.60491027873084, 52.84715510708951, 54.32473057751327, 137.19440483596455, 88.96784381793519, 64.80165188037326, 96.60040039108254, 50.90105028225429, 103.14071205930979, 64.18825791878758, 2.6931590979042883, 142.10955337495326, 147.52981487286516, 43.32877319957875, 42.730288137939944, 96.89509791321476, 98.52537520606175, 85.59892455026753, 95.59215449969616, 59.052561253174574, 109.0940762320017, 22.07331208102012], 'lossList': [0.0, -1.3802646094560622, 0.0, 43.71940667152405, 0.0, 0.0, 0.0], 'rewardMean': 0.7585867784370087, 'totalEpisodes': 217, 'stepsPerEpisode': 52, 'rewardPerEpisode': 42.80491444463992, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.6963819811002692, 'errorList': [], 'lossList': [0.0, -1.3657809972763062, 0.0, 17.841091945171357, 0.0, 0.0, 0.0], 'rewardMean': 0.7497003788174744, 'totalEpisodes': 226, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6963819811002692
'totalSteps': 10240, 'rewardStep': 0.8902500634867972, 'errorList': [], 'lossList': [0.0, -1.3635411548614502, 0.0, 10.596284555196762, 0.0, 0.0, 0.0], 'rewardMean': 0.7672690894011398, 'totalEpisodes': 232, 'stepsPerEpisode': 55, 'rewardPerEpisode': 49.84083095063395
'totalSteps': 11520, 'rewardStep': 0.8356490237318036, 'errorList': [], 'lossList': [0.0, -1.3686554247140885, 0.0, 9.745268037319184, 0.0, 0.0, 0.0], 'rewardMean': 0.7748668598823246, 'totalEpisodes': 235, 'stepsPerEpisode': 280, 'rewardPerEpisode': 242.91334317052142
'totalSteps': 12800, 'rewardStep': 0.7310437171092443, 'errorList': [], 'lossList': [0.0, -1.3856972503662108, 0.0, 6.8621593374013905, 0.0, 0.0, 0.0], 'rewardMean': 0.7704845456050167, 'totalEpisodes': 238, 'stepsPerEpisode': 243, 'rewardPerEpisode': 200.14958433298216
'totalSteps': 14080, 'rewardStep': 0.7976125700348663, 'errorList': [], 'lossList': [0.0, -1.4049750101566314, 0.0, 6.638904452323914, 0.0, 0.0, 0.0], 'rewardMean': 0.7737132193137976, 'totalEpisodes': 241, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7976125700348663
'totalSteps': 15360, 'rewardStep': 0.7429751923273042, 'errorList': [], 'lossList': [0.0, -1.4205298620462417, 0.0, 4.881008415818214, 0.0, 0.0, 0.0], 'rewardMean': 0.7902755881552739, 'totalEpisodes': 243, 'stepsPerEpisode': 254, 'rewardPerEpisode': 202.3887713186011
'totalSteps': 16640, 'rewardStep': 0.7044888023659135, 'errorList': [], 'lossList': [0.0, -1.4217252337932587, 0.0, 4.185469015836715, 0.0, 0.0, 0.0], 'rewardMean': 0.7955561680916888, 'totalEpisodes': 244, 'stepsPerEpisode': 805, 'rewardPerEpisode': 683.2307863785469
'totalSteps': 17920, 'rewardStep': 0.8812534215446723, 'errorList': [], 'lossList': [0.0, -1.395415214896202, 0.0, 1.9546631835401058, 0.0, 0.0, 0.0], 'rewardMean': 0.7923400777834628, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.8944373726663
'totalSteps': 19200, 'rewardStep': 0.7057405288871804, 'errorList': [], 'lossList': [0.0, -1.3648950958251953, 0.0, 1.550668125897646, 0.0, 0.0, 0.0], 'rewardMean': 0.7932191297131692, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.3811089473672
'totalSteps': 20480, 'rewardStep': 0.9125201263115916, 'errorList': [], 'lossList': [0.0, -1.3305999678373337, 0.0, 1.3464842715114356, 0.0, 0.0, 0.0], 'rewardMean': 0.7897915426899644, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.3703104221986
'totalSteps': 21760, 'rewardStep': 0.8680274987798431, 'errorList': [], 'lossList': [0.0, -1.2907696795463561, 0.0, 0.652222019508481, 0.0, 0.0, 0.0], 'rewardMean': 0.8069560944579217, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.5244878499868
'totalSteps': 23040, 'rewardStep': 0.7802322074343264, 'errorList': [], 'lossList': [0.0, -1.2650526076555253, 0.0, 0.28524526495486496, 0.0, 0.0, 0.0], 'rewardMean': 0.7959543088526745, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.0915207845626
'totalSteps': 24320, 'rewardStep': 0.8989347117016979, 'errorList': [], 'lossList': [0.0, -1.2168536829948424, 0.0, 0.48023610159754754, 0.0, 0.0, 0.0], 'rewardMean': 0.802282877649664, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1160.501425719019
'totalSteps': 25600, 'rewardStep': 0.9405120299265102, 'errorList': [0.04252934490371106, 0.03691575413835945, 0.042538470653836945, 0.034745750178954996, 0.06160327420643941, 0.04084562172291807, 0.05562441550079276, 0.036198859673468274, 0.04432852480278894, 0.03200147760503565, 0.07916323014073574, 0.035987963841157704, 0.03363815399420615, 0.02807678914080989, 0.06050647855409877, 0.0246755227418141, 0.04399745602832625, 0.02570805433526838, 0.04994956966799177, 0.0736412500326869, 0.039212730515667545, 0.09287620742609304, 0.03388791783232647, 0.03927762606466527, 0.06416435957368945, 0.04336563655521763, 0.04318967786126085, 0.05584906530950058, 0.04298401517387165, 0.05645960048133822, 0.057485357231546456, 0.03665983689148986, 0.04847024499784311, 0.030335886936642464, 0.03465742761473859, 0.046897315062876185, 0.07560302784084867, 0.028440304407823698, 0.04029853203252757, 0.05865806050785483, 0.0320939756343579, 0.04077767890031405, 0.06809175924683346, 0.03141872173474774, 0.035229048906271765, 0.027229250552192462, 0.05211025377921116, 0.0363312701272002, 0.05867296036316227, 0.037146954109946596], 'lossList': [0.0, -1.1964895284175874, 0.0, 0.46917766229249536, 0.0, 0.0, 0.0], 'rewardMean': 0.8232297089313907, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1203.794981899801, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=89.52
