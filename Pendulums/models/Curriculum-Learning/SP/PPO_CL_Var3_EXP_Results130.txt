#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 130
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.6053780042023712, 'errorList': [], 'lossList': [0.0, -1.4071094459295272, 0.0, 35.36591017723084, 0.0, 0.0, 0.0], 'rewardMean': 0.6080240515721109, 'totalEpisodes': 38, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.601780058953546
'totalSteps': 3840, 'rewardStep': 0.8954980249287887, 'errorList': [], 'lossList': [0.0, -1.3967867171764374, 0.0, 52.78574014663696, 0.0, 0.0, 0.0], 'rewardMean': 0.7038487093576702, 'totalEpisodes': 83, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8954980249287887
'totalSteps': 5120, 'rewardStep': 0.8379470449419706, 'errorList': [], 'lossList': [0.0, -1.3835762256383897, 0.0, 61.23181594848633, 0.0, 0.0, 0.0], 'rewardMean': 0.7373732932537453, 'totalEpisodes': 126, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.6715831937837
'totalSteps': 6400, 'rewardStep': 0.9258272259233192, 'errorList': [], 'lossList': [0.0, -1.3809971684217452, 0.0, 52.04831331253052, 0.0, 0.0, 0.0], 'rewardMean': 0.7750640797876601, 'totalEpisodes': 149, 'stepsPerEpisode': 32, 'rewardPerEpisode': 28.122869251769508
'totalSteps': 7680, 'rewardStep': 0.9533427120808452, 'errorList': [], 'lossList': [0.0, -1.366351163983345, 0.0, 35.61766253948212, 0.0, 0.0, 0.0], 'rewardMean': 0.8047771851698576, 'totalEpisodes': 158, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.084498909833009
'totalSteps': 8960, 'rewardStep': 0.42870586698811763, 'errorList': [], 'lossList': [0.0, -1.3505556416511535, 0.0, 18.91530590415001, 0.0, 0.0, 0.0], 'rewardMean': 0.7510527111438947, 'totalEpisodes': 162, 'stepsPerEpisode': 257, 'rewardPerEpisode': 193.02554829850607
'totalSteps': 10240, 'rewardStep': 0.5607727872034267, 'errorList': [], 'lossList': [0.0, -1.3542010182142257, 0.0, 27.45183570384979, 0.0, 0.0, 0.0], 'rewardMean': 0.7272677206513363, 'totalEpisodes': 165, 'stepsPerEpisode': 110, 'rewardPerEpisode': 82.50801643423226
'totalSteps': 11520, 'rewardStep': 0.7690668209576668, 'errorList': [], 'lossList': [0.0, -1.3382290345430374, 0.0, 38.661561489105225, 0.0, 0.0, 0.0], 'rewardMean': 0.7319120651298174, 'totalEpisodes': 170, 'stepsPerEpisode': 66, 'rewardPerEpisode': 60.86165406769006
'totalSteps': 12800, 'rewardStep': 0.6148203969934092, 'errorList': [], 'lossList': [0.0, -1.3387576526403426, 0.0, 9.576469539403915, 0.0, 0.0, 0.0], 'rewardMean': 0.7202028983161766, 'totalEpisodes': 172, 'stepsPerEpisode': 290, 'rewardPerEpisode': 223.1674883601702
'totalSteps': 14080, 'rewardStep': 0.5674442678258247, 'errorList': [], 'lossList': [0.0, -1.3483510965108871, 0.0, 8.630192098617554, 0.0, 0.0, 0.0], 'rewardMean': 0.715880315204574, 'totalEpisodes': 173, 'stepsPerEpisode': 1192, 'rewardPerEpisode': 878.7951703974799
'totalSteps': 15360, 'rewardStep': 0.5707250934828184, 'errorList': [], 'lossList': [0.0, -1.3422341018915176, 0.0, 13.241921334266662, 0.0, 0.0, 0.0], 'rewardMean': 0.7124150241326187, 'totalEpisodes': 175, 'stepsPerEpisode': 733, 'rewardPerEpisode': 487.38112313613277
'totalSteps': 16640, 'rewardStep': 0.8960711059339367, 'errorList': [], 'lossList': [0.0, -1.340235297679901, 0.0, 3.034710795134306, 0.0, 0.0, 0.0], 'rewardMean': 0.7124723322331334, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.3349408799021
'totalSteps': 17920, 'rewardStep': 0.8249342114614868, 'errorList': [], 'lossList': [0.0, -1.3337453752756119, 0.0, 1.5096427973359823, 0.0, 0.0, 0.0], 'rewardMean': 0.7111710488850852, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.4536514748547
'totalSteps': 19200, 'rewardStep': 0.8582800115774789, 'errorList': [], 'lossList': [0.0, -1.3017936146259308, 0.0, 1.3096854965388776, 0.0, 0.0, 0.0], 'rewardMean': 0.7044163274505012, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.2835551229286
'totalSteps': 20480, 'rewardStep': 0.8916750414836901, 'errorList': [], 'lossList': [0.0, -1.2811953133344651, 0.0, 1.7582180878147482, 0.0, 0.0, 0.0], 'rewardMean': 0.6982495603907857, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.5775806193326
'totalSteps': 21760, 'rewardStep': 0.9827418284524742, 'errorList': [0.13834408805014714, 0.12454463128059087, 0.1337295309888883, 0.14920791078355816, 0.11680578232178464, 0.1284482610625272, 0.10843657966651138, 0.14130540287528903, 0.15273728817292576, 0.11983381708075984, 0.11859123824763743, 0.1568182104509747, 0.21618005976846563, 0.13135654351954865, 0.1451039094704125, 0.10914448676559012, 0.15588709158700612, 0.33167006084266265, 0.12469902780042144, 0.11574977065213661, 0.2903380400540713, 0.136469329558789, 0.1040459195388745, 0.13983552144810624, 0.111567576724291, 0.13362814802656173, 0.12622315902617814, 0.13077588469448181, 0.12320283268827721, 0.13618587231613222, 0.1302670983294015, 0.15797550989545542, 0.11806674733376184, 0.22962520467582173, 0.1262029628432765, 0.11701379512223148, 0.12505584838968692, 0.1259337139061125, 0.12566037929470095, 0.10597021933229173, 0.19606358064623985, 0.13774058983903376, 0.12380176953158481, 0.15085685786935915, 0.11858287863246422, 0.17222482370790035, 0.11749400185874556, 0.20803384733307267, 0.10061346935248226, 0.11815757619429763], 'lossList': [0.0, -1.2741349226236343, 0.0, 1.1400609654933214, 0.0, 0.0, 0.0], 'rewardMean': 0.7536531565372213, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.4128699026553, 'successfulTests': 45
'totalSteps': 23040, 'rewardStep': 0.8655073489706392, 'errorList': [], 'lossList': [0.0, -1.2590961503982543, 0.0, 0.6977363930642605, 0.0, 0.0, 0.0], 'rewardMean': 0.7841266127139426, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.255722707879
'totalSteps': 24320, 'rewardStep': 0.6679015522937592, 'errorList': [], 'lossList': [0.0, -1.2300249874591827, 0.0, 0.31699982456862924, 0.0, 0.0, 0.0], 'rewardMean': 0.7740100858475516, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1071.6752715794737
'totalSteps': 25600, 'rewardStep': 0.9585634976546192, 'errorList': [0.10996674762560404, 0.06557078881773368, 0.19076666511806722, 0.1651538348722882, 0.06251952677527252, 0.07433718154770275, 0.14826159390819793, 0.09123953793149274, 0.07214214342665004, 0.07648550805020482, 0.06971368378428806, 0.06877991212676952, 0.07413099089413397, 0.06918682933322548, 0.06461370948717424, 0.07231727880075887, 0.17117790885072004, 0.07031932818302153, 0.11114048814202489, 0.1313689835328515, 0.07336684029447581, 0.0743069399178086, 0.09302522676944279, 0.10357858365139759, 0.08118299513448193, 0.3053412639555976, 0.13961012682281104, 0.11825719257868854, 0.23356261179716473, 0.062118812064888826, 0.06690080535007184, 0.1305307547288378, 0.06923561157959916, 0.06570566188852756, 0.15403421649587878, 0.07856934452165783, 0.06833921767475319, 0.2649051927456055, 0.06160378459995724, 0.07212051851770734, 0.06296882274469304, 0.07559078545467661, 0.0710420399176901, 0.06666585799334569, 0.10915143303499254, 0.0720866180919357, 0.0670778890394246, 0.0672368346254042, 0.2287002684608982, 0.06215961330628679], 'lossList': [0.0, -1.2026048570871353, 0.0, 0.5891509292647242, 0.0, 0.0, 0.0], 'rewardMean': 0.8083843959136727, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.5830470341548, 'successfulTests': 46
#maxSuccessfulTests=46, maxSuccessfulTestsAtStep=25600, timeSpent=105.04
