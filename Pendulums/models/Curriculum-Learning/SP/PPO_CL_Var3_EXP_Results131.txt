#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 131
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.5957962074719239, 'errorList': [], 'lossList': [0.0, -1.41088340818882, 0.0, 35.92577078819275, 0.0, 0.0, 0.0], 'rewardMean': 0.7795428796913169, 'totalEpisodes': 44, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.801023179529054
'totalSteps': 3840, 'rewardStep': 0.5550255027651612, 'errorList': [], 'lossList': [0.0, -1.4047466826438904, 0.0, 55.144170398712156, 0.0, 0.0, 0.0], 'rewardMean': 0.704703754049265, 'totalEpisodes': 84, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.92300293477607
'totalSteps': 5120, 'rewardStep': 0.8148246323770859, 'errorList': [], 'lossList': [0.0, -1.398084233403206, 0.0, 54.496984729766844, 0.0, 0.0, 0.0], 'rewardMean': 0.7322339736312202, 'totalEpisodes': 125, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.314574029312066
'totalSteps': 6400, 'rewardStep': 0.5158536660486466, 'errorList': [], 'lossList': [0.0, -1.3871171009540557, 0.0, 38.76238383769989, 0.0, 0.0, 0.0], 'rewardMean': 0.6889579121147055, 'totalEpisodes': 138, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.73576018995782
'totalSteps': 7680, 'rewardStep': 0.5345565837743882, 'errorList': [], 'lossList': [0.0, -1.3725588011741638, 0.0, 30.255824513435364, 0.0, 0.0, 0.0], 'rewardMean': 0.6632243573913192, 'totalEpisodes': 143, 'stepsPerEpisode': 292, 'rewardPerEpisode': 235.1525066284752
'totalSteps': 8960, 'rewardStep': 0.8674129775144589, 'errorList': [], 'lossList': [0.0, -1.356473355293274, 0.0, 32.44300081729889, 0.0, 0.0, 0.0], 'rewardMean': 0.6923941602660534, 'totalEpisodes': 153, 'stepsPerEpisode': 112, 'rewardPerEpisode': 96.81358638677638
'totalSteps': 10240, 'rewardStep': 0.7772009266160222, 'errorList': [], 'lossList': [0.0, -1.3448300391435624, 0.0, 17.25882614850998, 0.0, 0.0, 0.0], 'rewardMean': 0.7029950060597996, 'totalEpisodes': 160, 'stepsPerEpisode': 51, 'rewardPerEpisode': 43.9636999644578
'totalSteps': 11520, 'rewardStep': 0.6208762999212476, 'errorList': [], 'lossList': [0.0, -1.3453371465206145, 0.0, 21.820500524044036, 0.0, 0.0, 0.0], 'rewardMean': 0.6938707053777383, 'totalEpisodes': 166, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.978213800362775
'totalSteps': 12800, 'rewardStep': 0.7772957697003994, 'errorList': [], 'lossList': [0.0, -1.3489367032051087, 0.0, 11.339488430023193, 0.0, 0.0, 0.0], 'rewardMean': 0.7022132118100044, 'totalEpisodes': 170, 'stepsPerEpisode': 59, 'rewardPerEpisode': 52.32338065515755
'totalSteps': 14080, 'rewardStep': 0.7839714176541066, 'errorList': [], 'lossList': [0.0, -1.3503296375274658, 0.0, 5.5455168157815935, 0.0, 0.0, 0.0], 'rewardMean': 0.684281398384344, 'totalEpisodes': 172, 'stepsPerEpisode': 330, 'rewardPerEpisode': 273.934263503449
'totalSteps': 15360, 'rewardStep': 0.6310507972961392, 'errorList': [], 'lossList': [0.0, -1.359915491938591, 0.0, 7.199149399995804, 0.0, 0.0, 0.0], 'rewardMean': 0.6878068573667655, 'totalEpisodes': 173, 'stepsPerEpisode': 664, 'rewardPerEpisode': 518.3990529411575
'totalSteps': 16640, 'rewardStep': 0.716399864816416, 'errorList': [], 'lossList': [0.0, -1.3565465211868286, 0.0, 1.629905539751053, 0.0, 0.0, 0.0], 'rewardMean': 0.703944293571891, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 764.3268158248887
'totalSteps': 17920, 'rewardStep': 0.8368195655292827, 'errorList': [], 'lossList': [0.0, -1.3324225753545762, 0.0, 3.1524404233694074, 0.0, 0.0, 0.0], 'rewardMean': 0.7061437868871107, 'totalEpisodes': 176, 'stepsPerEpisode': 103, 'rewardPerEpisode': 85.84303688058093
'totalSteps': 19200, 'rewardStep': 0.9467273656710926, 'errorList': [0.2495556557480484, 0.1406983474331963, 0.13726658254750215, 0.06037820348315988, 0.14493956859562068, 0.09381195888566303, 0.1172859790661242, 0.06351742444686924, 0.17786837003821596, 0.08102568111507118, 0.285512944277485, 0.11119678854448509, 0.19719589979436483, 0.1602504443210427, 0.11731126611654254, 0.15303338989049048, 0.11479526489425602, 0.10353547682750985, 0.12096036251098916, 0.07568373888370568, 0.08738209979268106, 0.146672809824826, 0.09712024308638037, 0.121706875196571, 0.17671367249099137, 0.2681176504438768, 0.15826574275374017, 0.21554263712843386, 0.10777506735088424, 0.19219543688952054, 0.08775376801776427, 0.09428249181173087, 0.13713411795107402, 0.17616990487733933, 0.1406318831636929, 0.15540516215296252, 0.17582482450753664, 0.09289821840707828, 0.06842930931409398, 0.08418459394732532, 0.16683101776890236, 0.3029384534207352, 0.22245340635683497, 0.06434405795428041, 0.12813974998021227, 0.07005435101675256, 0.19556554857443567, 0.1888770474971775, 0.1995812403975801, 0.1586012593156293], 'lossList': [0.0, -1.3113944399356843, 0.0, 4.287103916406632, 0.0, 0.0, 0.0], 'rewardMean': 0.7492311568493554, 'totalEpisodes': 178, 'stepsPerEpisode': 67, 'rewardPerEpisode': 61.85379912392919, 'successfulTests': 44
'totalSteps': 20480, 'rewardStep': 0.5817471853537252, 'errorList': [], 'lossList': [0.0, -1.276743700504303, 0.0, 0.9743716656416654, 0.0, 0.0, 0.0], 'rewardMean': 0.753950217007289, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.8579203365123
'totalSteps': 21760, 'rewardStep': 0.7802517374691845, 'errorList': [], 'lossList': [0.0, -1.2226075875759124, 0.0, 0.6293528538942337, 0.0, 0.0, 0.0], 'rewardMean': 0.7452340930027616, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.0235929304313
'totalSteps': 23040, 'rewardStep': 0.9014428696995214, 'errorList': [], 'lossList': [0.0, -1.1991997236013412, 0.0, 1.3819344067201018, 0.0, 0.0, 0.0], 'rewardMean': 0.7576582873111115, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.5041627288686
'totalSteps': 24320, 'rewardStep': 0.8626396061886586, 'errorList': [], 'lossList': [0.0, -1.1611711382865906, 0.0, 0.6252660761401058, 0.0, 0.0, 0.0], 'rewardMean': 0.7818346179378527, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.5358430317976
'totalSteps': 25600, 'rewardStep': 0.9762134445498347, 'errorList': [0.3235566793678574, 0.33389865779602557, 0.3495020775271199, 0.3211712527277417, 0.3318351731420524, 0.36432666355193993, 0.3432271132245561, 0.3586078232054838, 0.36525468243428827, 0.3494725226209575, 0.33234343207492395, 0.3467589534207785, 0.34674047993220386, 0.34380918861821314, 0.33519674941201255, 0.3418673982782017, 0.32489170559365504, 0.33423325407887405, 0.3719184603129233, 0.31916081602925994, 0.3630865047473726, 0.3396693646198381, 0.3414817993036071, 0.38251628584949277, 0.3401520722893446, 0.3513978194272138, 0.4004062304382647, 0.36631313506258073, 0.3363942120304498, 0.33924822423197276, 0.35459723671908777, 0.3205748807732395, 0.3263370866799694, 0.3463116320930409, 0.36266967457129967, 0.3459070078240792, 0.43210318888316696, 0.3227352160128866, 0.35630547706122323, 0.38699486944874023, 0.34302028349326036, 0.3412451997451021, 0.33937336923705397, 0.3657284638620294, 0.35226378601845243, 0.3246429295767543, 0.3395054026602492, 0.354044005730989, 0.3643408629385465, 0.3344210036977673], 'lossList': [0.0, -1.1117107266187667, 0.0, 0.6024119930900633, 0.0, 0.0, 0.0], 'rewardMean': 0.8017263854227963, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1193.6026905759759, 'successfulTests': 0
#maxSuccessfulTests=44, maxSuccessfulTestsAtStep=19200, timeSpent=102.09
