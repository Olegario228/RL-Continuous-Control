#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 133
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.480020790657007, 'errorList': [], 'lossList': [0.0, -1.4268661695718765, 0.0, 31.88902400970459, 0.0, 0.0, 0.0], 'rewardMean': 0.41218799110123094, 'totalEpisodes': 35, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.48477855115046
'totalSteps': 3840, 'rewardStep': 0.9894447788811334, 'errorList': [], 'lossList': [0.0, -1.4225081861019135, 0.0, 50.74243295669556, 0.0, 0.0, 0.0], 'rewardMean': 0.6046069203611985, 'totalEpisodes': 73, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.568039647253288
'totalSteps': 5120, 'rewardStep': 0.9001554390763783, 'errorList': [], 'lossList': [0.0, -1.4047587603330611, 0.0, 59.20325799942017, 0.0, 0.0, 0.0], 'rewardMean': 0.6784940500399934, 'totalEpisodes': 114, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.716684878125598
'totalSteps': 6400, 'rewardStep': 0.9353976915686203, 'errorList': [], 'lossList': [0.0, -1.3977211946249009, 0.0, 59.550030879974365, 0.0, 0.0, 0.0], 'rewardMean': 0.7298747783457188, 'totalEpisodes': 137, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.609395812892
'totalSteps': 7680, 'rewardStep': 0.7499041374121939, 'errorList': [], 'lossList': [0.0, -1.3993962270021438, 0.0, 37.56132042884827, 0.0, 0.0, 0.0], 'rewardMean': 0.7332130048567979, 'totalEpisodes': 150, 'stepsPerEpisode': 85, 'rewardPerEpisode': 62.14225040408241
'totalSteps': 8960, 'rewardStep': 0.829974538275585, 'errorList': [], 'lossList': [0.0, -1.3892910367250442, 0.0, 39.03486454963684, 0.0, 0.0, 0.0], 'rewardMean': 0.7470360810594817, 'totalEpisodes': 157, 'stepsPerEpisode': 201, 'rewardPerEpisode': 142.254857872691
'totalSteps': 10240, 'rewardStep': 0.8386989048167974, 'errorList': [], 'lossList': [0.0, -1.3832376194000244, 0.0, 16.244130618572235, 0.0, 0.0, 0.0], 'rewardMean': 0.7584939340291463, 'totalEpisodes': 163, 'stepsPerEpisode': 92, 'rewardPerEpisode': 82.01744567454668
'totalSteps': 11520, 'rewardStep': 0.5034455290791815, 'errorList': [], 'lossList': [0.0, -1.3904722666740417, 0.0, 11.207675137519836, 0.0, 0.0, 0.0], 'rewardMean': 0.7301552223680391, 'totalEpisodes': 165, 'stepsPerEpisode': 396, 'rewardPerEpisode': 299.6722129302832
'totalSteps': 12800, 'rewardStep': 0.6732870133137834, 'errorList': [], 'lossList': [0.0, -1.3734319669008255, 0.0, 7.93043367266655, 0.0, 0.0, 0.0], 'rewardMean': 0.7244684014626135, 'totalEpisodes': 167, 'stepsPerEpisode': 241, 'rewardPerEpisode': 149.48088344014187
'totalSteps': 14080, 'rewardStep': 0.7739625354524187, 'errorList': [], 'lossList': [0.0, -1.35931218624115, 0.0, 5.281117432117462, 0.0, 0.0, 0.0], 'rewardMean': 0.7674291358533099, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 847.3811532215954
'totalSteps': 15360, 'rewardStep': 0.8123109410715783, 'errorList': [], 'lossList': [0.0, -1.3451799553632737, 0.0, 4.653740829974413, 0.0, 0.0, 0.0], 'rewardMean': 0.800658150894767, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.4471161067044
'totalSteps': 16640, 'rewardStep': 0.8506645325062608, 'errorList': [], 'lossList': [0.0, -1.3385107761621475, 0.0, 6.16492313914001, 0.0, 0.0, 0.0], 'rewardMean': 0.7867801262572797, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.260794339741
'totalSteps': 17920, 'rewardStep': 0.9000950236870837, 'errorList': [], 'lossList': [0.0, -1.3174491840600968, 0.0, 4.977645270824432, 0.0, 0.0, 0.0], 'rewardMean': 0.7867740847183503, 'totalEpisodes': 169, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.494778857784524
'totalSteps': 19200, 'rewardStep': 0.8272910458485889, 'errorList': [], 'lossList': [0.0, -1.3157064884901046, 0.0, 4.408136760592461, 0.0, 0.0, 0.0], 'rewardMean': 0.7759634201463472, 'totalEpisodes': 171, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.2737969867391
'totalSteps': 20480, 'rewardStep': 0.9381988843558715, 'errorList': [0.028211502940328263, 0.19903270224760178, 0.060989595776094474, 0.12407556695527543, 0.055365704498756926, 0.041450040927686706, 0.1341938762304449, 0.12134086304770254, 0.03648399704421214, 0.0402218539521404, 0.0724579697254592, 0.03877332914824704, 0.26055251508210586, 0.17780202318906316, 0.08879172800478687, 0.1642548514196965, 0.03810848304438955, 0.21962198079165204, 0.041227667173784795, 0.07233314343640276, 0.03770632625360934, 0.04086054278851952, 0.08576754620282, 0.12177974392495215, 0.039593959720079334, 0.12838955048924167, 0.15700425522709976, 0.14137540944523216, 0.13950983997182223, 0.11092199705992094, 0.15412557259833523, 0.2345864530486137, 0.090769007433279, 0.2164081051824193, 0.09737385044783733, 0.22468972221696454, 0.12405784388464303, 0.04416765622977469, 0.08867136915850664, 0.0941151128887176, 0.04563972899958083, 0.11548471519760774, 0.3121592396263623, 0.1650767349877697, 0.05009101224186161, 0.0475243628727182, 0.1705754646839566, 0.1400145432165954, 0.061127364212743916, 0.16670899860173166], 'lossList': [0.0, -1.3182886230945587, 0.0, 2.183014885187149, 0.0, 0.0, 0.0], 'rewardMean': 0.7947928948407149, 'totalEpisodes': 173, 'stepsPerEpisode': 138, 'rewardPerEpisode': 124.04771287600748, 'successfulTests': 44
'totalSteps': 21760, 'rewardStep': 0.9059659282268302, 'errorList': [], 'lossList': [0.0, -1.3097655075788497, 0.0, 1.1730903606116772, 0.0, 0.0, 0.0], 'rewardMean': 0.8023920338358396, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.5903221153792
'totalSteps': 23040, 'rewardStep': 0.8331298946702804, 'errorList': [], 'lossList': [0.0, -1.260844476222992, 0.0, 0.94964500233531, 0.0, 0.0, 0.0], 'rewardMean': 0.8018351328211878, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.9058738328406
'totalSteps': 24320, 'rewardStep': 0.8737815837219371, 'errorList': [], 'lossList': [0.0, -1.1904087913036348, 0.0, 0.6423871543817222, 0.0, 0.0, 0.0], 'rewardMean': 0.8388687382854633, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.9142811885308
'totalSteps': 25600, 'rewardStep': 0.9733619719558065, 'errorList': [0.08833524043752862, 0.04590215495120255, 0.04355953221088557, 0.06115097521976802, 0.0813516825883552, 0.0456165426893764, 0.09127201576437541, 0.04158680118987705, 0.07255580248158294, 0.07064732794436637, 0.09271506585455028, 0.045212743189892726, 0.04508210189240222, 0.10226852037535733, 0.05437898949751279, 0.055300878002297, 0.08086142402957278, 0.07255513296610484, 0.06453347086445799, 0.0906565089991338, 0.05946191420673286, 0.09568662429166487, 0.045668802336014606, 0.05671159902645817, 0.062185269125738143, 0.046030727626543144, 0.04686219978983496, 0.06424732307203619, 0.0481027301867732, 0.0685422828088274, 0.06597659807033456, 0.045824278587851555, 0.07946398154073617, 0.07315920247665753, 0.07867388645602154, 0.05100153222438099, 0.0838117455302138, 0.06970579805601942, 0.04479965182081668, 0.05227790378149251, 0.058299279249034545, 0.05990285516841089, 0.07314066703879299, 0.09229397327448191, 0.061033165598968146, 0.06250922162873536, 0.06592366669051253, 0.09622908881266551, 0.10081138383077021, 0.08926110229993155], 'lossList': [0.0, -1.1526908135414125, 0.0, 0.5009364574216306, 0.0, 0.0, 0.0], 'rewardMean': 0.8688762341496657, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.0915452627469, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=99.94
