#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 134
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7865034924431473, 'errorList': [], 'lossList': [0.0, -1.4032457768917084, 0.0, 36.420101385116574, 0.0, 0.0, 0.0], 'rewardMean': 0.575286799109368, 'totalEpisodes': 43, 'stepsPerEpisode': 20, 'rewardPerEpisode': 17.87203721705425
'totalSteps': 3840, 'rewardStep': 0.8280461604583129, 'errorList': [], 'lossList': [0.0, -1.3901279222965242, 0.0, 54.350350322723386, 0.0, 0.0, 0.0], 'rewardMean': 0.6595399195590163, 'totalEpisodes': 89, 'stepsPerEpisode': 57, 'rewardPerEpisode': 42.35372088490256
'totalSteps': 5120, 'rewardStep': 0.8224542751438351, 'errorList': [], 'lossList': [0.0, -1.370708128809929, 0.0, 60.73433246612549, 0.0, 0.0, 0.0], 'rewardMean': 0.7002685084552209, 'totalEpisodes': 132, 'stepsPerEpisode': 40, 'rewardPerEpisode': 33.191312158411534
'totalSteps': 6400, 'rewardStep': 0.3892867316792219, 'errorList': [], 'lossList': [0.0, -1.3594203525781632, 0.0, 59.81108236312866, 0.0, 0.0, 0.0], 'rewardMean': 0.6380721531000211, 'totalEpisodes': 155, 'stepsPerEpisode': 262, 'rewardPerEpisode': 189.6079068119306
'totalSteps': 7680, 'rewardStep': 0.6579173072540997, 'errorList': [], 'lossList': [0.0, -1.350097004175186, 0.0, 38.138211469650265, 0.0, 0.0, 0.0], 'rewardMean': 0.6413796787923676, 'totalEpisodes': 167, 'stepsPerEpisode': 37, 'rewardPerEpisode': 25.261663852567544
'totalSteps': 8960, 'rewardStep': 0.6169676357874927, 'errorList': [], 'lossList': [0.0, -1.3322403848171234, 0.0, 37.490193276405336, 0.0, 0.0, 0.0], 'rewardMean': 0.6378922440773854, 'totalEpisodes': 177, 'stepsPerEpisode': 219, 'rewardPerEpisode': 161.11236247694063
'totalSteps': 10240, 'rewardStep': 0.8010320391124094, 'errorList': [], 'lossList': [0.0, -1.3155118006467819, 0.0, 15.375118323564529, 0.0, 0.0, 0.0], 'rewardMean': 0.6582847184567635, 'totalEpisodes': 182, 'stepsPerEpisode': 73, 'rewardPerEpisode': 57.55265888162318
'totalSteps': 11520, 'rewardStep': 0.657580317208641, 'errorList': [], 'lossList': [0.0, -1.2937593340873719, 0.0, 9.846259793639183, 0.0, 0.0, 0.0], 'rewardMean': 0.6582064516514166, 'totalEpisodes': 187, 'stepsPerEpisode': 89, 'rewardPerEpisode': 68.7684288946721
'totalSteps': 12800, 'rewardStep': 0.8843199612911327, 'errorList': [], 'lossList': [0.0, -1.2774280256032944, 0.0, 11.915301775932312, 0.0, 0.0, 0.0], 'rewardMean': 0.6808178026153882, 'totalEpisodes': 191, 'stepsPerEpisode': 64, 'rewardPerEpisode': 56.82824048634926
'totalSteps': 14080, 'rewardStep': 0.6539035168140126, 'errorList': [], 'lossList': [0.0, -1.2664393723011016, 0.0, 5.61512439250946, 0.0, 0.0, 0.0], 'rewardMean': 0.7098011437192305, 'totalEpisodes': 196, 'stepsPerEpisode': 177, 'rewardPerEpisode': 142.90316113704685
'totalSteps': 15360, 'rewardStep': 0.6962929433034963, 'errorList': [], 'lossList': [0.0, -1.258623286485672, 0.0, 26.040597726106643, 0.0, 0.0, 0.0], 'rewardMean': 0.7007800888052654, 'totalEpisodes': 202, 'stepsPerEpisode': 92, 'rewardPerEpisode': 74.64071588919185
'totalSteps': 16640, 'rewardStep': 0.38770373606760045, 'errorList': [], 'lossList': [0.0, -1.2353785729408264, 0.0, 5.301364637613297, 0.0, 0.0, 0.0], 'rewardMean': 0.6567458463661942, 'totalEpisodes': 206, 'stepsPerEpisode': 210, 'rewardPerEpisode': 158.418447159915
'totalSteps': 17920, 'rewardStep': 0.8283970764679747, 'errorList': [], 'lossList': [0.0, -1.1977036225795745, 0.0, 9.604856724739074, 0.0, 0.0, 0.0], 'rewardMean': 0.6573401264986082, 'totalEpisodes': 210, 'stepsPerEpisode': 52, 'rewardPerEpisode': 48.31930299336225
'totalSteps': 19200, 'rewardStep': 0.5338281136555758, 'errorList': [], 'lossList': [0.0, -1.1722233921289444, 0.0, 2.7293105816841123, 0.0, 0.0, 0.0], 'rewardMean': 0.6717942646962436, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 996.7452315833142
'totalSteps': 20480, 'rewardStep': 0.941818793982736, 'errorList': [0.10701655965781565, 0.09324910999506753, 0.07674148994263069, 0.09493978208949777, 0.10145895116856044, 0.1041909808603109, 0.07946206014414103, 0.10533196999064028, 0.09874041900624189, 0.11830668350054009, 0.04929690673455297, 0.058932611359215854, 0.08262974083268514, 0.06522515594266518, 0.10851574940308842, 0.10807336190973074, 0.0796978760540142, 0.07363330628272124, 0.07307160059753759, 0.06747105923929074, 0.11229462596925167, 0.09007923014293369, 0.0829606080859551, 0.057992166531231196, 0.10092639167417113, 0.07778386367539522, 0.09103329931160019, 0.07968991219676172, 0.08019115344936026, 0.08013666003538528, 0.05373836305165858, 0.05080343725324868, 0.09856281288652198, 0.10190949658250749, 0.05165460469384767, 0.07909581751097379, 0.07714091535825023, 0.08366072602863048, 0.09596012946341544, 0.06094607111880027, 0.07135510496703938, 0.112198024226783, 0.08910917740358315, 0.08264777997623458, 0.11519234507089408, 0.09802597638732649, 0.06748184361161808, 0.0918343949192745, 0.07430242735147823, 0.10377254962859515], 'lossList': [0.0, -1.1361508840322494, 0.0, 2.656880384683609, 0.0, 0.0, 0.0], 'rewardMean': 0.7001844133691072, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.4645008901375, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=69.8
