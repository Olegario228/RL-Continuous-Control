#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 135
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.6390758545517637, 'errorList': [], 'lossList': [0.0, -1.4247488582134247, 0.0, 26.021890888214113, 0.0, 0.0, 0.0], 'rewardMean': 0.7194374075253633, 'totalEpisodes': 20, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.934258478679821
'totalSteps': 3840, 'rewardStep': 0.44279394958049206, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5811156785529277, 'totalEpisodes': 69, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.2521174599828155
'totalSteps': 5120, 'rewardStep': 0.8842771134481905, 'errorList': [], 'lossList': [0.0, -1.4347767108678817, 0.0, 54.31555107116699, 0.0, 0.0, 0.0], 'rewardMean': 0.6417479655319802, 'totalEpisodes': 122, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.095452429633966
'totalSteps': 6400, 'rewardStep': 0.647500198845836, 'errorList': [], 'lossList': [0.0, -1.4110821944475174, 0.0, 48.6342006111145, 0.0, 0.0, 0.0], 'rewardMean': 0.6427066710842895, 'totalEpisodes': 166, 'stepsPerEpisode': 112, 'rewardPerEpisode': 78.21975145645267
'totalSteps': 7680, 'rewardStep': 0.5021897451278341, 'errorList': [], 'lossList': [0.0, -1.3803864938020707, 0.0, 46.67192868232727, 0.0, 0.0, 0.0], 'rewardMean': 0.6226328245190815, 'totalEpisodes': 188, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.582059351999996
'totalSteps': 8960, 'rewardStep': 0.7969717416844405, 'errorList': [], 'lossList': [0.0, -1.3526269501447679, 0.0, 36.71941232204437, 0.0, 0.0, 0.0], 'rewardMean': 0.6444251891647514, 'totalEpisodes': 200, 'stepsPerEpisode': 113, 'rewardPerEpisode': 94.62779357398624
'totalSteps': 10240, 'rewardStep': 0.5159547729827926, 'errorList': [], 'lossList': [0.0, -1.3426924860477447, 0.0, 43.87304659843445, 0.0, 0.0, 0.0], 'rewardMean': 0.6301506984778671, 'totalEpisodes': 209, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.104968832889362
'totalSteps': 11520, 'rewardStep': 0.6722453695104644, 'errorList': [], 'lossList': [0.0, -1.3213888615369798, 0.0, 28.30851729154587, 0.0, 0.0, 0.0], 'rewardMean': 0.6343601655811268, 'totalEpisodes': 215, 'stepsPerEpisode': 67, 'rewardPerEpisode': 55.917588942264786
'totalSteps': 12800, 'rewardStep': 0.8517676685392644, 'errorList': [], 'lossList': [0.0, -1.3023118287324906, 0.0, 12.06522392988205, 0.0, 0.0, 0.0], 'rewardMean': 0.639557036385157, 'totalEpisodes': 219, 'stepsPerEpisode': 72, 'rewardPerEpisode': 65.03131756550407
'totalSteps': 14080, 'rewardStep': 0.7100679409086949, 'errorList': [], 'lossList': [0.0, -1.296184756755829, 0.0, 8.222074292302132, 0.0, 0.0, 0.0], 'rewardMean': 0.6466562450208502, 'totalEpisodes': 222, 'stepsPerEpisode': 431, 'rewardPerEpisode': 358.0094235649103
'totalSteps': 15360, 'rewardStep': 0.836021458718772, 'errorList': [], 'lossList': [0.0, -1.2904133933782578, 0.0, 10.702996962070465, 0.0, 0.0, 0.0], 'rewardMean': 0.6859789959346781, 'totalEpisodes': 228, 'stepsPerEpisode': 69, 'rewardPerEpisode': 63.45319605509786
'totalSteps': 16640, 'rewardStep': 0.38352603508121164, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6299770966480522, 'totalEpisodes': 233, 'stepsPerEpisode': 281, 'rewardPerEpisode': 215.39492661834603
'totalSteps': 17920, 'rewardStep': 0.969005360802431, 'errorList': [11.440750688551464, 2.079578851759842, 6.416782894603352, 2.1930862341246455, 13.192304850702747, 7.3043092033568175, 7.74420778834401, 4.5853317180261355, 4.420848667042286, 0.2066639628748749, 1.3686951634377549, 1.355373323681105, 1.2344081934290247, 6.101396537814657, 1.2905156495691812, 4.914870023722613, 5.266358176520853, 3.914249474769874, 2.0730391913836783, 2.614705391522664, 3.0869080107447746, 10.20363570497626, 7.464875327301652, 8.250693919766716, 2.2227589717469414, 12.005880013724004, 6.550274801040726, 5.035689501784937, 5.597037287585268, 1.1455568208091986, 2.938861588755355, 3.7805086164462978, 3.518861528458569, 10.737913237954777, 7.1693825564264735, 4.168987703224332, 2.979197623887551, 5.015485002795214, 6.06117998482668, 1.4377334240390627, 7.821024556573717, 5.896428098750006, 8.628073600940919, 6.710446756274969, 4.890682460453923, 2.040227859138847, 0.4285992444821097, 3.0335781545943536, 6.017948613418076, 4.363487452283983], 'lossList': [0.0, -1.2930494856834411, 0.0, 7.237805202007293, 0.0, 0.0, 0.0], 'rewardMean': 0.6621276128437116, 'totalEpisodes': 238, 'stepsPerEpisode': 59, 'rewardPerEpisode': 45.08156298493029, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.7828636285987106, 'errorList': [], 'lossList': [0.0, -1.295293328166008, 0.0, 23.35887062549591, 0.0, 0.0, 0.0], 'rewardMean': 0.6901950011907994, 'totalEpisodes': 242, 'stepsPerEpisode': 152, 'rewardPerEpisode': 127.45351202521762
'totalSteps': 20480, 'rewardStep': 0.6516273334303948, 'errorList': [], 'lossList': [0.0, -1.2814763522148132, 0.0, 3.159525048136711, 0.0, 0.0, 0.0], 'rewardMean': 0.6756605603653949, 'totalEpisodes': 245, 'stepsPerEpisode': 382, 'rewardPerEpisode': 293.1004876529712
'totalSteps': 21760, 'rewardStep': 0.9384670132206986, 'errorList': [0.46951613340960907, 0.21298244449239737, 0.3346799712783141, 0.4528046913934898, 0.4764844254609527, 0.3285361500556875, 0.23542964422046816, 0.38548201103264795, 0.11934420986714946, 0.6831435674294797, 0.34502990939828315, 0.32350538297095344, 0.3134944090986406, 0.6371978007201734, 0.7084504418282773, 0.11320717806059884, 0.3516843124383641, 0.40226504589800144, 0.7213684910245557, 0.5688378707196705, 0.5554556787478905, 0.6364969686049312, 0.269044060271415, 0.8707806634502646, 0.2824152451038967, 0.667274299640107, 0.1902800011441542, 0.5486655198862394, 0.6069516715482812, 0.35097000878942936, 0.39642415449475843, 0.09219177523479241, 0.15754951389002259, 0.12718195229772208, 0.43563623920057676, 0.277369790075723, 0.42341217212261206, 0.3459131440541893, 0.6125835106933454, 0.4909005389250526, 0.24120500996294555, 0.3839616343411767, 0.6290470788563, 0.23508880260807685, 0.7467479048136572, 0.4614197357414914, 0.1430382445837333, 0.09041044539713863, 0.3860445634958291, 0.18839312163403413], 'lossList': [0.0, -1.2567658525705339, 0.0, 3.172625693380833, 0.0, 0.0, 0.0], 'rewardMean': 0.7179117843891853, 'totalEpisodes': 247, 'stepsPerEpisode': 628, 'rewardPerEpisode': 535.2341646435267, 'successfulTests': 9
'totalSteps': 23040, 'rewardStep': 0.6191109144347307, 'errorList': [], 'lossList': [0.0, -1.2224862682819366, 0.0, 2.8798910447955133, 0.0, 0.0, 0.0], 'rewardMean': 0.712598338881612, 'totalEpisodes': 248, 'stepsPerEpisode': 457, 'rewardPerEpisode': 384.7413802524509
'totalSteps': 24320, 'rewardStep': 0.4553452211449992, 'errorList': [], 'lossList': [0.0, -1.1908943462371826, 0.0, 2.9384222620725633, 0.0, 0.0, 0.0], 'rewardMean': 0.6729560941421855, 'totalEpisodes': 249, 'stepsPerEpisode': 477, 'rewardPerEpisode': 374.82147872810896
'totalSteps': 25600, 'rewardStep': 0.8762651773097381, 'errorList': [], 'lossList': [0.0, -1.171366485953331, 0.0, 2.1668249890208244, 0.0, 0.0, 0.0], 'rewardMean': 0.6895758177822898, 'totalEpisodes': 250, 'stepsPerEpisode': 1258, 'rewardPerEpisode': 1089.307568328814
#maxSuccessfulTests=9, maxSuccessfulTestsAtStep=21760, timeSpent=95.08
