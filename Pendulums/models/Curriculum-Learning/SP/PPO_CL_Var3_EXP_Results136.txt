#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 136
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7423829058379503, 'errorList': [], 'lossList': [0.0, -1.4152381241321563, 0.0, 29.46245907306671, 0.0, 0.0, 0.0], 'rewardMean': 0.6295884037687016, 'totalEpisodes': 26, 'stepsPerEpisode': 63, 'rewardPerEpisode': 48.09235234555983
'totalSteps': 3840, 'rewardStep': 0.59498882957723, 'errorList': [], 'lossList': [0.0, -1.3884571343660355, 0.0, 64.4046821975708, 0.0, 0.0, 0.0], 'rewardMean': 0.6180552123715444, 'totalEpisodes': 68, 'stepsPerEpisode': 27, 'rewardPerEpisode': 18.314992907934943
'totalSteps': 5120, 'rewardStep': 0.6637127018080513, 'errorList': [], 'lossList': [0.0, -1.3767238253355025, 0.0, 59.95198558807373, 0.0, 0.0, 0.0], 'rewardMean': 0.629469584730671, 'totalEpisodes': 104, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.142294720017647
'totalSteps': 6400, 'rewardStep': 0.889856454504519, 'errorList': [], 'lossList': [0.0, -1.358483887910843, 0.0, 60.05105049133301, 0.0, 0.0, 0.0], 'rewardMean': 0.6815469586854406, 'totalEpisodes': 132, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.6310854951561256
'totalSteps': 7680, 'rewardStep': 0.4944946380177411, 'errorList': [], 'lossList': [0.0, -1.3478258895874022, 0.0, 48.48301132678986, 0.0, 0.0, 0.0], 'rewardMean': 0.6503715719074906, 'totalEpisodes': 140, 'stepsPerEpisode': 150, 'rewardPerEpisode': 119.52860560963872
'totalSteps': 8960, 'rewardStep': 0.6803352087482774, 'errorList': [], 'lossList': [0.0, -1.3445324498414992, 0.0, 62.55474062919617, 0.0, 0.0, 0.0], 'rewardMean': 0.6546520914561745, 'totalEpisodes': 154, 'stepsPerEpisode': 108, 'rewardPerEpisode': 83.91754880964817
'totalSteps': 10240, 'rewardStep': 0.8568406087388636, 'errorList': [], 'lossList': [0.0, -1.3402642089128494, 0.0, 32.4005259180069, 0.0, 0.0, 0.0], 'rewardMean': 0.6799256561165108, 'totalEpisodes': 162, 'stepsPerEpisode': 55, 'rewardPerEpisode': 48.43221539621853
'totalSteps': 11520, 'rewardStep': 0.6774872932976679, 'errorList': [], 'lossList': [0.0, -1.3608395975828171, 0.0, 21.819840660095213, 0.0, 0.0, 0.0], 'rewardMean': 0.6796547269144172, 'totalEpisodes': 167, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.722702844090627
'totalSteps': 12800, 'rewardStep': 0.6847823338947188, 'errorList': [], 'lossList': [0.0, -1.3780065089464189, 0.0, 13.72954288959503, 0.0, 0.0, 0.0], 'rewardMean': 0.6801674876124473, 'totalEpisodes': 171, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.897064540108251
'totalSteps': 14080, 'rewardStep': 0.7296571560956013, 'errorList': [], 'lossList': [0.0, -1.3882516062259673, 0.0, 7.886908956170082, 0.0, 0.0, 0.0], 'rewardMean': 0.7014538130520622, 'totalEpisodes': 174, 'stepsPerEpisode': 342, 'rewardPerEpisode': 281.91592552763115
'totalSteps': 15360, 'rewardStep': 0.8855293375111013, 'errorList': [], 'lossList': [0.0, -1.4127373391389846, 0.0, 10.102077224850655, 0.0, 0.0, 0.0], 'rewardMean': 0.7157684562193772, 'totalEpisodes': 178, 'stepsPerEpisode': 135, 'rewardPerEpisode': 110.53784483737674
'totalSteps': 16640, 'rewardStep': 0.7954547465435626, 'errorList': [], 'lossList': [0.0, -1.398075904250145, 0.0, 62.112018113136294, 0.0, 0.0, 0.0], 'rewardMean': 0.7358150479160104, 'totalEpisodes': 183, 'stepsPerEpisode': 70, 'rewardPerEpisode': 57.34048825735144
'totalSteps': 17920, 'rewardStep': 0.8391312788784178, 'errorList': [], 'lossList': [0.0, -1.3886499470472335, 0.0, 39.77967299461365, 0.0, 0.0, 0.0], 'rewardMean': 0.7533569056230471, 'totalEpisodes': 185, 'stepsPerEpisode': 654, 'rewardPerEpisode': 555.1879413581653
'totalSteps': 19200, 'rewardStep': 0.8496762008040823, 'errorList': [], 'lossList': [0.0, -1.365971213579178, 0.0, 4.243188332021236, 0.0, 0.0, 0.0], 'rewardMean': 0.7493388802530034, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.3664316633087
'totalSteps': 20480, 'rewardStep': 0.40530541359448413, 'errorList': [], 'lossList': [0.0, -1.338607338666916, 0.0, 4.615019615888595, 0.0, 0.0, 0.0], 'rewardMean': 0.7404199578106778, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 801.2657785298493
'totalSteps': 21760, 'rewardStep': 0.6186728009376833, 'errorList': [], 'lossList': [0.0, -1.3136682480573654, 0.0, 1.9079196628928186, 0.0, 0.0, 0.0], 'rewardMean': 0.7342537170296183, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 952.1107674581475
'totalSteps': 23040, 'rewardStep': 0.8192446820106983, 'errorList': [], 'lossList': [0.0, -1.290806159377098, 0.0, 1.2176483768224715, 0.0, 0.0, 0.0], 'rewardMean': 0.7304941243568017, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 950.0302670548973
'totalSteps': 24320, 'rewardStep': 0.8282375907401169, 'errorList': [], 'lossList': [0.0, -1.244321464896202, 0.0, 1.1773927364125847, 0.0, 0.0, 0.0], 'rewardMean': 0.7455691541010466, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.1347557834433
'totalSteps': 25600, 'rewardStep': 0.9547081315169021, 'errorList': [0.09699439150395792, 0.10706304659345682, 0.1296707965476889, 0.09537426342255531, 0.1100536722224464, 0.20272618349041685, 0.13386140070659294, 0.1355607056125893, 0.19356530081664824, 0.12444454085969625, 0.1082156505064077, 0.14028020979138858, 0.13969121070285564, 0.11693619634238281, 0.11896026639985649, 0.13283201820532406, 0.14480999051701177, 0.11074072124022898, 0.19158137922380591, 0.1559249820024428, 0.13797103054225787, 0.11676884099428755, 0.12516894471769585, 0.21296162489092146, 0.11804485504149169, 0.12431604275355745, 0.18815307827759375, 0.14551327882247975, 0.13431517013533237, 0.11352661037926362, 0.15878758795105022, 0.09379490251723398, 0.10355737205970827, 0.1905733267423533, 0.148575043551806, 0.12202453621244695, 0.26519418943429984, 0.10707921981699182, 0.13394236434869386, 0.22227449983784464, 0.11651480937432875, 0.11755646595166151, 0.12256497848100607, 0.14856343736863156, 0.1260551881337921, 0.1529519389362539, 0.11804526674860762, 0.12988109455177033, 0.14778164373143995, 0.10873813575157171], 'lossList': [0.0, -1.1939269465208053, 0.0, 0.6538009699620306, 0.0, 0.0, 0.0], 'rewardMean': 0.7725617338632651, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.6647501476027, 'successfulTests': 46
#maxSuccessfulTests=46, maxSuccessfulTestsAtStep=25600, timeSpent=80.86
