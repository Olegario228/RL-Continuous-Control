#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 138
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.47301388264036875, 'errorList': [], 'lossList': [0.0, -1.425860382914543, 0.0, 34.70365594863892, 0.0, 0.0, 0.0], 'rewardMean': 0.6191698577937124, 'totalEpisodes': 39, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.117790104443106
'totalSteps': 3840, 'rewardStep': 0.7304773982739363, 'errorList': [], 'lossList': [0.0, -1.4260449802875519, 0.0, 47.07345432281494, 0.0, 0.0, 0.0], 'rewardMean': 0.6562723712871203, 'totalEpisodes': 75, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.29110518323081
'totalSteps': 5120, 'rewardStep': 0.9549574273221799, 'errorList': [], 'lossList': [0.0, -1.4093948137760162, 0.0, 63.104745330810545, 0.0, 0.0, 0.0], 'rewardMean': 0.7309436352958852, 'totalEpisodes': 113, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.024987382643245
'totalSteps': 6400, 'rewardStep': 0.7476317144125808, 'errorList': [], 'lossList': [0.0, -1.3873725235462189, 0.0, 61.69979330062866, 0.0, 0.0, 0.0], 'rewardMean': 0.7342812511192244, 'totalEpisodes': 143, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.61472731611781
'totalSteps': 7680, 'rewardStep': 0.89363090287068, 'errorList': [], 'lossList': [0.0, -1.3668392944335936, 0.0, 57.749783506393435, 0.0, 0.0, 0.0], 'rewardMean': 0.7608395264111336, 'totalEpisodes': 162, 'stepsPerEpisode': 85, 'rewardPerEpisode': 65.41044508239418
'totalSteps': 8960, 'rewardStep': 0.8614942850223287, 'errorList': [], 'lossList': [0.0, -1.3511696523427963, 0.0, 40.1325804233551, 0.0, 0.0, 0.0], 'rewardMean': 0.7752187776413043, 'totalEpisodes': 176, 'stepsPerEpisode': 72, 'rewardPerEpisode': 58.62819142594287
'totalSteps': 10240, 'rewardStep': 0.8680064171568491, 'errorList': [], 'lossList': [0.0, -1.350793282389641, 0.0, 40.439119095802305, 0.0, 0.0, 0.0], 'rewardMean': 0.7868172325807474, 'totalEpisodes': 185, 'stepsPerEpisode': 138, 'rewardPerEpisode': 122.13274179643282
'totalSteps': 11520, 'rewardStep': 0.7608103459219562, 'errorList': [], 'lossList': [0.0, -1.3486146461963653, 0.0, 9.213721996545791, 0.0, 0.0, 0.0], 'rewardMean': 0.7839275785075484, 'totalEpisodes': 192, 'stepsPerEpisode': 44, 'rewardPerEpisode': 35.75515712081816
'totalSteps': 12800, 'rewardStep': 0.6960523299373821, 'errorList': [], 'lossList': [0.0, -1.3406276100873946, 0.0, 28.69807337760925, 0.0, 0.0, 0.0], 'rewardMean': 0.7751400536505317, 'totalEpisodes': 197, 'stepsPerEpisode': 184, 'rewardPerEpisode': 160.39671086725258
'totalSteps': 14080, 'rewardStep': 0.5569748166492671, 'errorList': [], 'lossList': [0.0, -1.3504216432571412, 0.0, 7.530773336291313, 0.0, 0.0, 0.0], 'rewardMean': 0.754304952020753, 'totalEpisodes': 202, 'stepsPerEpisode': 246, 'rewardPerEpisode': 187.43922155340178
'totalSteps': 15360, 'rewardStep': 0.21682407640266987, 'errorList': [], 'lossList': [0.0, -1.3705719751119614, 0.0, 7.163342899680138, 0.0, 0.0, 0.0], 'rewardMean': 0.728685971396983, 'totalEpisodes': 206, 'stepsPerEpisode': 205, 'rewardPerEpisode': 131.86429701666458
'totalSteps': 16640, 'rewardStep': 0.8536366944644644, 'errorList': [], 'lossList': [0.0, -1.360565985441208, 0.0, 3.8236161851882935, 0.0, 0.0, 0.0], 'rewardMean': 0.7410019010160358, 'totalEpisodes': 210, 'stepsPerEpisode': 111, 'rewardPerEpisode': 94.33801317008997
'totalSteps': 17920, 'rewardStep': 0.7907919623270335, 'errorList': [], 'lossList': [0.0, -1.3253162121772766, 0.0, 5.502430321574211, 0.0, 0.0, 0.0], 'rewardMean': 0.7245853545165212, 'totalEpisodes': 213, 'stepsPerEpisode': 60, 'rewardPerEpisode': 47.960847241643236
'totalSteps': 19200, 'rewardStep': 0.7648734088822307, 'errorList': [], 'lossList': [0.0, -1.2915516376495362, 0.0, 4.3960237324237825, 0.0, 0.0, 0.0], 'rewardMean': 0.7263095239634862, 'totalEpisodes': 215, 'stepsPerEpisode': 88, 'rewardPerEpisode': 69.95288674060387
'totalSteps': 20480, 'rewardStep': 0.8468763716343712, 'errorList': [], 'lossList': [0.0, -1.2492698240280151, 0.0, 2.1408643996715546, 0.0, 0.0, 0.0], 'rewardMean': 0.7216340708398553, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.0571680059222
'totalSteps': 21760, 'rewardStep': 0.9230092983100648, 'errorList': [], 'lossList': [0.0, -1.1931037843227386, 0.0, 1.5210208651423454, 0.0, 0.0, 0.0], 'rewardMean': 0.7277855721686289, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.1492383103953
'totalSteps': 23040, 'rewardStep': 0.783936221538299, 'errorList': [], 'lossList': [0.0, -1.1432941722869874, 0.0, 1.4584866078570484, 0.0, 0.0, 0.0], 'rewardMean': 0.7193785526067741, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1157.1403566782656
'totalSteps': 24320, 'rewardStep': 0.8726860627974308, 'errorList': [], 'lossList': [0.0, -1.0839961194992065, 0.0, 0.9921613228693604, 0.0, 0.0, 0.0], 'rewardMean': 0.7305661242943213, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.2475137959134
'totalSteps': 25600, 'rewardStep': 0.9765320556393429, 'errorList': [0.048630676370944204, 0.08176921329509237, 0.030374131962585038, 0.029370519029523492, 0.036960767649885415, 0.06285121370998395, 0.03835900855982002, 0.028537224251496903, 0.03826698662034446, 0.0362369113603354, 0.05603689791720909, 0.061247468732125575, 0.068947355609274, 0.04047783268102689, 0.0598792696594801, 0.044258093187548316, 0.03112203672869898, 0.030552759057050325, 0.09381503450861767, 0.04183170962604209, 0.046829198709063984, 0.032326445775446175, 0.031181269836093938, 0.02623314586728377, 0.08918892371226443, 0.06737461379906515, 0.06266120537688259, 0.04317800314764715, 0.031778845135856416, 0.045675861720219826, 0.03857579768458079, 0.03993665300017376, 0.03241357757610335, 0.03224037159025104, 0.03160453826197607, 0.045758207309493926, 0.09420747010217485, 0.035613063579952496, 0.0602780181181946, 0.030408152683902522, 0.05838043295601381, 0.04892455690934301, 0.03811275802501889, 0.05704410920775468, 0.08125832131713413, 0.03347464504016195, 0.04808994821600536, 0.028350760685941043, 0.057779488983408206, 0.07852701406830093], 'lossList': [0.0, -1.0504127603769302, 0.0, 0.9140339505858719, 0.0, 0.0, 0.0], 'rewardMean': 0.7586140968645175, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.4906673647672, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.12
