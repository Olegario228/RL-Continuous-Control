#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 139
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.9648007607483678, 'errorList': [], 'lossList': [0.0, -1.4138064181804657, 0.0, 33.124183893203735, 0.0, 0.0, 0.0], 'rewardMean': 0.8183580834458556, 'totalEpisodes': 36, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.136744513318735
'totalSteps': 3840, 'rewardStep': 0.764585723949983, 'errorList': [], 'lossList': [0.0, -1.4061757719516754, 0.0, 48.06755653381347, 0.0, 0.0, 0.0], 'rewardMean': 0.8004339636138981, 'totalEpisodes': 84, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.4273406932633
'totalSteps': 5120, 'rewardStep': 0.8227148377055732, 'errorList': [], 'lossList': [0.0, -1.3979790830612182, 0.0, 62.77297595977783, 0.0, 0.0, 0.0], 'rewardMean': 0.8060041821368169, 'totalEpisodes': 135, 'stepsPerEpisode': 39, 'rewardPerEpisode': 34.651601432969855
'totalSteps': 6400, 'rewardStep': 0.6583881310997397, 'errorList': [], 'lossList': [0.0, -1.3976392406225204, 0.0, 52.78160048484802, 0.0, 0.0, 0.0], 'rewardMean': 0.7764809719294015, 'totalEpisodes': 163, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.632374786684956
'totalSteps': 7680, 'rewardStep': 0.7575284726092784, 'errorList': [], 'lossList': [0.0, -1.3959812754392624, 0.0, 59.51066486358643, 0.0, 0.0, 0.0], 'rewardMean': 0.7733222220427143, 'totalEpisodes': 183, 'stepsPerEpisode': 38, 'rewardPerEpisode': 28.75115588871139
'totalSteps': 8960, 'rewardStep': 0.5232789230045032, 'errorList': [], 'lossList': [0.0, -1.381264238357544, 0.0, 35.27959320068359, 0.0, 0.0, 0.0], 'rewardMean': 0.7376017507515413, 'totalEpisodes': 194, 'stepsPerEpisode': 114, 'rewardPerEpisode': 83.61079795588633
'totalSteps': 10240, 'rewardStep': 0.8241564186635031, 'errorList': [], 'lossList': [0.0, -1.374315663576126, 0.0, 27.232526779174805, 0.0, 0.0, 0.0], 'rewardMean': 0.7484210842405365, 'totalEpisodes': 202, 'stepsPerEpisode': 155, 'rewardPerEpisode': 126.86186592984093
'totalSteps': 11520, 'rewardStep': 0.7859877384816405, 'errorList': [], 'lossList': [0.0, -1.3713045579195022, 0.0, 10.840882511138917, 0.0, 0.0, 0.0], 'rewardMean': 0.7525951569339926, 'totalEpisodes': 209, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.859611381651254
'totalSteps': 12800, 'rewardStep': 0.8094375677425614, 'errorList': [], 'lossList': [0.0, -1.3539034968614578, 0.0, 40.81428121805191, 0.0, 0.0, 0.0], 'rewardMean': 0.7582793980148494, 'totalEpisodes': 217, 'stepsPerEpisode': 64, 'rewardPerEpisode': 56.210005522457244
'totalSteps': 14080, 'rewardStep': 0.5160604764996636, 'errorList': [], 'lossList': [0.0, -1.342514111995697, 0.0, 9.040293657183646, 0.0, 0.0, 0.0], 'rewardMean': 0.7426939050504814, 'totalEpisodes': 220, 'stepsPerEpisode': 521, 'rewardPerEpisode': 425.8564573443916
'totalSteps': 15360, 'rewardStep': 0.9239155951043984, 'errorList': [], 'lossList': [0.0, -1.330705357193947, 0.0, 16.37633020043373, 0.0, 0.0, 0.0], 'rewardMean': 0.7386053884860844, 'totalEpisodes': 224, 'stepsPerEpisode': 135, 'rewardPerEpisode': 119.8244704563998
'totalSteps': 16640, 'rewardStep': 0.39168053398862496, 'errorList': [], 'lossList': [0.0, -1.3162150710821152, 0.0, 5.578539169430733, 0.0, 0.0, 0.0], 'rewardMean': 0.7013148694899487, 'totalEpisodes': 225, 'stepsPerEpisode': 593, 'rewardPerEpisode': 466.4695525935411
'totalSteps': 17920, 'rewardStep': 0.869586533934072, 'errorList': [], 'lossList': [0.0, -1.292630848288536, 0.0, 6.44625338613987, 0.0, 0.0, 0.0], 'rewardMean': 0.7060020391127985, 'totalEpisodes': 226, 'stepsPerEpisode': 128, 'rewardPerEpisode': 117.82845856073187
'totalSteps': 19200, 'rewardStep': 0.740960334607301, 'errorList': [], 'lossList': [0.0, -1.2837914496660232, 0.0, 2.1019110859930517, 0.0, 0.0, 0.0], 'rewardMean': 0.7142592594635546, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.7082416900047
'totalSteps': 20480, 'rewardStep': 0.9388659481844615, 'errorList': [0.1746737060881706, 0.0627021337782178, 0.18975962997974116, 0.0810820550872183, 0.12134266875681457, 0.04941400599505365, 0.14350447450086923, 0.06867342993091516, 0.12302217392341265, 0.07507403340218187, 0.10546115444951144, 0.10673586461377611, 0.05811586264679692, 0.06393542879717477, 0.07440764349719864, 0.0672503075446642, 0.0799664997691495, 0.08375182625335797, 0.05761155137087658, 0.11504002305911505, 0.058276616689301675, 0.07240799859287195, 0.08422665297533258, 0.20289263993368212, 0.045546666293873, 0.0710716548146894, 0.04898475090134238, 0.08011565888947647, 0.06365130586613645, 0.09759468531508002, 0.06727217747975155, 0.06238440816610195, 0.08110230628044199, 0.08175883381931622, 0.09225675832308744, 0.05733881253387772, 0.14253054791692055, 0.10810897014792356, 0.06209594627830193, 0.06433338616350139, 0.0673385931270384, 0.16124010217896884, 0.11580210299821517, 0.07499386771697582, 0.0600458717875447, 0.06635320442002554, 0.06534322132634555, 0.056370139316919005, 0.057365758046921476, 0.09859180011161414], 'lossList': [0.0, -1.2696550768613815, 0.0, 1.7739762882888317, 0.0, 0.0, 0.0], 'rewardMean': 0.7323930070210729, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.6180076422297, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.9484653746370595, 'errorList': [0.07221646901895437, 0.09145684322387435, 0.09268770582751737, 0.11009698784065189, 0.1671786896438958, 0.05037956447138986, 0.08980529884161008, 0.10659332535455049, 0.07332985615621136, 0.10704967819652796, 0.09887790194750568, 0.11025002098557413, 0.10288002841652102, 0.12973039673154715, 0.08721445874474738, 0.06545222242160381, 0.12446178576725866, 0.08576028731775612, 0.07796159838557938, 0.07495035498166615, 0.0839722468699203, 0.09874943937161446, 0.10242230364562672, 0.12483683849990701, 0.10005043020307297, 0.058302723673300236, 0.1326115303146981, 0.11991329706711573, 0.0854982147626537, 0.12200662415014604, 0.1281735116020528, 0.1433248105954962, 0.05579712064713278, 0.06612771490644344, 0.10997022154528827, 0.05767554360805862, 0.0556730496344979, 0.10458138966130819, 0.10123693045671502, 0.1354375971030489, 0.09393719030063145, 0.1285093581562498, 0.10164071445591247, 0.10556406311024073, 0.10572402615953805, 0.08129047229290648, 0.11731134719921825, 0.09880310244600744, 0.13532517476350694, 0.10493850096268482], 'lossList': [0.0, -1.232457219362259, 0.0, 1.2013338121399284, 0.0, 0.0, 0.0], 'rewardMean': 0.7749116521843287, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.6281501798946, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=87.68
