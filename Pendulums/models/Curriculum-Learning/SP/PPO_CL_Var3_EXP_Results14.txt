#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 14
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.7048320486071196, 'errorList': [], 'lossList': [0.0, -1.4186922007799148, 0.0, 31.39039733886719, 0.0, 0.0, 0.0], 'rewardMean': 0.6883737273752315, 'totalEpisodes': 66, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.925254532811525
'totalSteps': 3840, 'rewardStep': 0.8899230680637839, 'errorList': [], 'lossList': [0.0, -1.4125978446006775, 0.0, 33.556723556518556, 0.0, 0.0, 0.0], 'rewardMean': 0.7555568409380823, 'totalEpisodes': 140, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.41584221106416
'totalSteps': 5120, 'rewardStep': 0.6476643594593963, 'errorList': [], 'lossList': [0.0, -1.4038755416870117, 0.0, 36.76657554626465, 0.0, 0.0, 0.0], 'rewardMean': 0.7285837205684108, 'totalEpisodes': 181, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.469624472546256
'totalSteps': 6400, 'rewardStep': 0.8866746005489374, 'errorList': [], 'lossList': [0.0, -1.38845261156559, 0.0, 47.27246534347534, 0.0, 0.0, 0.0], 'rewardMean': 0.7602018965645161, 'totalEpisodes': 198, 'stepsPerEpisode': 36, 'rewardPerEpisode': 28.882988152102907
'totalSteps': 7680, 'rewardStep': 0.7228049805619472, 'errorList': [], 'lossList': [0.0, -1.380390028357506, 0.0, 40.088674397468566, 0.0, 0.0, 0.0], 'rewardMean': 0.7539690772307547, 'totalEpisodes': 209, 'stepsPerEpisode': 29, 'rewardPerEpisode': 21.55373157430371
'totalSteps': 8960, 'rewardStep': 0.8335768006876914, 'errorList': [], 'lossList': [0.0, -1.371744111776352, 0.0, 37.34534646511078, 0.0, 0.0, 0.0], 'rewardMean': 0.7653416091531742, 'totalEpisodes': 222, 'stepsPerEpisode': 110, 'rewardPerEpisode': 83.90898281555714
'totalSteps': 10240, 'rewardStep': 0.7725338316857663, 'errorList': [], 'lossList': [0.0, -1.364240380525589, 0.0, 8.98512333393097, 0.0, 0.0, 0.0], 'rewardMean': 0.7662406369697482, 'totalEpisodes': 229, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.65607321737073
'totalSteps': 11520, 'rewardStep': 0.8749187076196606, 'errorList': [], 'lossList': [0.0, -1.361109174489975, 0.0, 13.853657943606377, 0.0, 0.0, 0.0], 'rewardMean': 0.7783159781530719, 'totalEpisodes': 234, 'stepsPerEpisode': 57, 'rewardPerEpisode': 47.29621734083379
'totalSteps': 12800, 'rewardStep': 0.8530438193791686, 'errorList': [], 'lossList': [0.0, -1.3666132944822311, 0.0, 39.11632663249969, 0.0, 0.0, 0.0], 'rewardMean': 0.7857887622756815, 'totalEpisodes': 237, 'stepsPerEpisode': 64, 'rewardPerEpisode': 56.24942196999081
'totalSteps': 14080, 'rewardStep': 0.9315072783996003, 'errorList': [4.408610254100703, 7.094240032566164, 4.825441167766634, 4.180276281133778, 6.3333624382859846, 6.675508894882874, 4.388859722479926, 6.09936478531924, 5.129852181920579, 5.945795131732055, 6.071640326381566, 7.5002071417825045, 6.538182527224568, 6.776738055375181, 4.106473051010515, 4.708175083901457, 3.4976938281548677, 4.484305781793166, 4.7353790130544855, 6.787722275161618, 4.0595666759847555, 7.447003954037529, 5.483525832304419, 5.509254051271599, 5.763452836563202, 4.832357083576354, 5.471013313793971, 4.900317542961064, 4.772787171436796, 4.683103903477007, 4.868004247902084, 7.32117181755271, 7.007832378952352, 4.563720595462975, 4.39055656534476, 5.678754723777657, 3.9675510542100794, 5.000230826558128, 5.178669349430062, 4.0892965878342515, 5.510398446973361, 5.379757654167488, 5.260986166614029, 3.831804308240917, 5.551888408268398, 6.681834791653357, 7.278986650244936, 4.727211645005789, 6.227465718015699, 5.906092473974997], 'lossList': [0.0, -1.3469291269779204, 0.0, 4.6910884910821915, 0.0, 0.0, 0.0], 'rewardMean': 0.8117479495013071, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 831.7672508093355, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.8836046640084521, 'errorList': [], 'lossList': [0.0, -1.3285638374090194, 0.0, 28.778105976581575, 0.0, 0.0, 0.0], 'rewardMean': 0.8296252110414404, 'totalEpisodes': 242, 'stepsPerEpisode': 110, 'rewardPerEpisode': 96.20592470572302
'totalSteps': 16640, 'rewardStep': 0.6408830745807877, 'errorList': [], 'lossList': [0.0, -1.3032013440132142, 0.0, 7.083007463216782, 0.0, 0.0, 0.0], 'rewardMean': 0.8047212116931408, 'totalEpisodes': 245, 'stepsPerEpisode': 79, 'rewardPerEpisode': 65.18673211990259
'totalSteps': 17920, 'rewardStep': 0.6900923702155951, 'errorList': [], 'lossList': [0.0, -1.264963942170143, 0.0, 2.704040965437889, 0.0, 0.0, 0.0], 'rewardMean': 0.8089640127687607, 'totalEpisodes': 246, 'stepsPerEpisode': 528, 'rewardPerEpisode': 423.0371605942903
'totalSteps': 19200, 'rewardStep': 0.6091728736547328, 'errorList': [], 'lossList': [0.0, -1.2554729235172273, 0.0, 5.830642816722393, 0.0, 0.0, 0.0], 'rewardMean': 0.7812138400793402, 'totalEpisodes': 247, 'stepsPerEpisode': 630, 'rewardPerEpisode': 522.2218093642056
'totalSteps': 20480, 'rewardStep': 0.9206412955019234, 'errorList': [], 'lossList': [0.0, -1.2491838812828064, 0.0, 1.5567325752973558, 0.0, 0.0, 0.0], 'rewardMean': 0.8009974715733378, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1041.5617371963483
'totalSteps': 21760, 'rewardStep': 0.9187925871086571, 'errorList': [], 'lossList': [0.0, -1.2244401252269745, 0.0, 1.3203618331998586, 0.0, 0.0, 0.0], 'rewardMean': 0.8095190502154344, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.3186564748132
'totalSteps': 23040, 'rewardStep': 0.8885792539840678, 'errorList': [], 'lossList': [0.0, -1.202869114279747, 0.0, 1.336786130219698, 0.0, 0.0, 0.0], 'rewardMean': 0.8211235924452644, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.801309034428
'totalSteps': 24320, 'rewardStep': 0.9249861574983252, 'errorList': [], 'lossList': [0.0, -1.1969769543409348, 0.0, 0.582830461151898, 0.0, 0.0, 0.0], 'rewardMean': 0.826130337433131, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.8837179651243
'totalSteps': 25600, 'rewardStep': 0.9785020734751766, 'errorList': [0.03437680052642644, 0.020617664478369914, 0.020577702701958394, 0.03662980512588955, 0.05216539715047492, 0.06762055670683166, 0.02993095989898212, 0.023988743518715175, 0.05140640389198428, 0.07896984571819873, 0.06704292689496481, 0.03433173897974399, 0.05694652644019358, 0.09121965000179419, 0.08153388537277195, 0.04623356759214385, 0.08504068467206398, 0.03989148949166287, 0.059015800141283395, 0.027646884508408916, 0.10249507700668256, 0.03630656185301803, 0.11000440177522136, 0.031939817008619545, 0.030766129641436688, 0.05530949645709403, 0.026080308391894833, 0.03311284767499806, 0.033080113717473277, 0.04233792893583643, 0.03262006354427002, 0.04948902916982638, 0.030490437732572925, 0.06452076192188336, 0.04795289435566242, 0.028566010858835107, 0.02942274826264561, 0.07505928217846927, 0.02349680965360823, 0.06840113730546951, 0.07014068859903617, 0.08210336215343896, 0.06305603427985709, 0.024085563577811844, 0.034602665129072833, 0.08275982857788654, 0.08103744093263025, 0.0219182343065227, 0.0318318543154814, 0.117220718283278], 'lossList': [0.0, -1.179900734424591, 0.0, 0.4749604322388768, 0.0, 0.0, 0.0], 'rewardMean': 0.8386761628427317, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.0356509624887, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=92.15
