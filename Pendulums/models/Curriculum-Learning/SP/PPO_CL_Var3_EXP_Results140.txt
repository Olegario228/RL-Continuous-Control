#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 140
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.7733639601637015, 'errorList': [], 'lossList': [0.0, -1.4349063682556151, 0.0, 23.84099908351898, 0.0, 0.0, 0.0], 'rewardMean': 0.8344727756859343, 'totalEpisodes': 17, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.248087820606834
'totalSteps': 3840, 'rewardStep': 0.5398797174682425, 'errorList': [], 'lossList': [0.0, -1.4238869279623032, 0.0, 51.698333683013914, 0.0, 0.0, 0.0], 'rewardMean': 0.7362750896133704, 'totalEpisodes': 55, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.256627741612596
'totalSteps': 5120, 'rewardStep': 0.7448128393567287, 'errorList': [], 'lossList': [0.0, -1.4078312957286834, 0.0, 58.50302875518799, 0.0, 0.0, 0.0], 'rewardMean': 0.7384095270492099, 'totalEpisodes': 96, 'stepsPerEpisode': 45, 'rewardPerEpisode': 34.7644796634083
'totalSteps': 6400, 'rewardStep': 0.9777027683006805, 'errorList': [], 'lossList': [0.0, -1.388315634727478, 0.0, 59.606431255340574, 0.0, 0.0, 0.0], 'rewardMean': 0.7862681752995041, 'totalEpisodes': 125, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.483747117001858
'totalSteps': 7680, 'rewardStep': 0.9050447836947577, 'errorList': [], 'lossList': [0.0, -1.378208223581314, 0.0, 36.1887697315216, 0.0, 0.0, 0.0], 'rewardMean': 0.806064276698713, 'totalEpisodes': 137, 'stepsPerEpisode': 38, 'rewardPerEpisode': 27.248422687024448
'totalSteps': 8960, 'rewardStep': 0.8404929656553553, 'errorList': [], 'lossList': [0.0, -1.3559202069044114, 0.0, 33.76545360088348, 0.0, 0.0, 0.0], 'rewardMean': 0.8109826608353761, 'totalEpisodes': 146, 'stepsPerEpisode': 25, 'rewardPerEpisode': 22.698197256475677
'totalSteps': 10240, 'rewardStep': 0.7095673148857298, 'errorList': [], 'lossList': [0.0, -1.3396818935871124, 0.0, 23.478365901708603, 0.0, 0.0, 0.0], 'rewardMean': 0.7983057425916704, 'totalEpisodes': 152, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.734969263004052
'totalSteps': 11520, 'rewardStep': 0.7133304383238427, 'errorList': [], 'lossList': [0.0, -1.3233502221107483, 0.0, 33.55868987083435, 0.0, 0.0, 0.0], 'rewardMean': 0.7888640421174674, 'totalEpisodes': 160, 'stepsPerEpisode': 68, 'rewardPerEpisode': 51.261681743660404
'totalSteps': 12800, 'rewardStep': 0.8286116743595157, 'errorList': [], 'lossList': [0.0, -1.3169862282276155, 0.0, 19.61732855796814, 0.0, 0.0, 0.0], 'rewardMean': 0.7928388053416722, 'totalEpisodes': 164, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.901238929749896
'totalSteps': 14080, 'rewardStep': 0.7739551504366199, 'errorList': [], 'lossList': [0.0, -1.3112124156951905, 0.0, 6.018940440118313, 0.0, 0.0, 0.0], 'rewardMean': 0.7806761612645173, 'totalEpisodes': 168, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.596782242555992
'totalSteps': 15360, 'rewardStep': 0.7575094633739753, 'errorList': [], 'lossList': [0.0, -1.3065946859121322, 0.0, 4.092868244647979, 0.0, 0.0, 0.0], 'rewardMean': 0.7790907115855448, 'totalEpisodes': 171, 'stepsPerEpisode': 81, 'rewardPerEpisode': 66.39190262458537
'totalSteps': 16640, 'rewardStep': 0.7594227900318039, 'errorList': [], 'lossList': [0.0, -1.31192465364933, 0.0, 21.871460968255995, 0.0, 0.0, 0.0], 'rewardMean': 0.8010450188419009, 'totalEpisodes': 174, 'stepsPerEpisode': 303, 'rewardPerEpisode': 261.50967459772477
'totalSteps': 17920, 'rewardStep': 0.8350184962957908, 'errorList': [], 'lossList': [0.0, -1.3114386957883835, 0.0, 17.232157961726188, 0.0, 0.0, 0.0], 'rewardMean': 0.8100655845358071, 'totalEpisodes': 178, 'stepsPerEpisode': 245, 'rewardPerEpisode': 186.40877691581153
'totalSteps': 19200, 'rewardStep': 0.5523611346903402, 'errorList': [], 'lossList': [0.0, -1.29461760699749, 0.0, 11.923893123865128, 0.0, 0.0, 0.0], 'rewardMean': 0.767531421174773, 'totalEpisodes': 182, 'stepsPerEpisode': 326, 'rewardPerEpisode': 271.17684081894924
'totalSteps': 20480, 'rewardStep': 0.7895884342985032, 'errorList': [], 'lossList': [0.0, -1.2973474764823913, 0.0, 9.30764944434166, 0.0, 0.0, 0.0], 'rewardMean': 0.7559857862351477, 'totalEpisodes': 186, 'stepsPerEpisode': 173, 'rewardPerEpisode': 148.4368106073947
'totalSteps': 21760, 'rewardStep': 0.7944147379874675, 'errorList': [], 'lossList': [0.0, -1.2895651173591614, 0.0, 6.792479895949364, 0.0, 0.0, 0.0], 'rewardMean': 0.7513779634683588, 'totalEpisodes': 189, 'stepsPerEpisode': 249, 'rewardPerEpisode': 212.09087197540612
'totalSteps': 23040, 'rewardStep': 0.7020067054838236, 'errorList': [], 'lossList': [0.0, -1.2815001899003982, 0.0, 2.698853244930506, 0.0, 0.0, 0.0], 'rewardMean': 0.7506219025281682, 'totalEpisodes': 190, 'stepsPerEpisode': 1053, 'rewardPerEpisode': 914.5034480590084
'totalSteps': 24320, 'rewardStep': 0.8087632804888288, 'errorList': [], 'lossList': [0.0, -1.277795779109001, 0.0, 3.0336211374402047, 0.0, 0.0, 0.0], 'rewardMean': 0.7601651867446668, 'totalEpisodes': 191, 'stepsPerEpisode': 157, 'rewardPerEpisode': 142.34315469923746
'totalSteps': 25600, 'rewardStep': 0.9365064482269055, 'errorList': [0.07239480690687, 0.1317123387682117, 0.06663923125601384, 0.06156433962745866, 0.053573490894309114, 0.05249948017852173, 0.1107600245199004, 0.08856525370365152, 0.0813735941892517, 0.14603094003078207, 0.06319181805715776, 0.053076709410956394, 0.12000983624989985, 0.06566068721850847, 0.05939670117512064, 0.15934507093287306, 0.10875933422699977, 0.1059047318000026, 0.16323317966045997, 0.06093337647011815, 0.12086155737779429, 0.12669967746533187, 0.08854763707505495, 0.11192287564816278, 0.10211256722413857, 0.06505050952268278, 0.06764184984729758, 0.17201767697386677, 0.07516957777055897, 0.14280465007123036, 0.17092761279478214, 0.12009552335145475, 0.0680811121532838, 0.06995802686980396, 0.051492785754684876, 0.15104684518146078, 0.06737619001986232, 0.1296934213530581, 0.06438486943788006, 0.05673098995726886, 0.06853275838063361, 0.12185446177818883, 0.05817737523558348, 0.0621093479685087, 0.1100498502430308, 0.057049030043533254, 0.06251032563713288, 0.17438217453617125, 0.06406847163721277, 0.05723087654227512], 'lossList': [0.0, -1.2598965394496917, 0.0, 1.1644287580251693, 0.0, 0.0, 0.0], 'rewardMean': 0.7709546641314059, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.527677649194, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=76.03
