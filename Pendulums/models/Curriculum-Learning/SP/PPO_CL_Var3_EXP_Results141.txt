#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 141
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.659845281126005, 'errorList': [], 'lossList': [0.0, -1.4197723776102067, 0.0, 31.706321821212768, 0.0, 0.0, 0.0], 'rewardMean': 0.6092464998011801, 'totalEpisodes': 20, 'stepsPerEpisode': 107, 'rewardPerEpisode': 78.98944489994143
'totalSteps': 3840, 'rewardStep': 0.5992131216541268, 'errorList': [], 'lossList': [0.0, -1.389587050676346, 0.0, 65.72055850982666, 0.0, 0.0, 0.0], 'rewardMean': 0.605902040418829, 'totalEpisodes': 61, 'stepsPerEpisode': 20, 'rewardPerEpisode': 11.828149118453448
'totalSteps': 5120, 'rewardStep': 0.6373858427808217, 'errorList': [], 'lossList': [0.0, -1.3673376190662383, 0.0, 65.28290544509888, 0.0, 0.0, 0.0], 'rewardMean': 0.6137729910093271, 'totalEpisodes': 102, 'stepsPerEpisode': 29, 'rewardPerEpisode': 20.59234134615365
'totalSteps': 6400, 'rewardStep': 0.6344372825489926, 'errorList': [], 'lossList': [0.0, -1.3449155646562576, 0.0, 62.76484205245972, 0.0, 0.0, 0.0], 'rewardMean': 0.6179058493172602, 'totalEpisodes': 131, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.297393721858134
'totalSteps': 7680, 'rewardStep': 0.7925341007149557, 'errorList': [], 'lossList': [0.0, -1.3294405245780945, 0.0, 41.79074419975281, 0.0, 0.0, 0.0], 'rewardMean': 0.6470105578835429, 'totalEpisodes': 143, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.97632805029309
'totalSteps': 8960, 'rewardStep': 0.8201226694805684, 'errorList': [], 'lossList': [0.0, -1.3204414910078048, 0.0, 47.2753893661499, 0.0, 0.0, 0.0], 'rewardMean': 0.6717408595402609, 'totalEpisodes': 155, 'stepsPerEpisode': 112, 'rewardPerEpisode': 97.99856994751225
'totalSteps': 10240, 'rewardStep': 0.7564569699450672, 'errorList': [], 'lossList': [0.0, -1.3103271973133088, 0.0, 22.586617677211763, 0.0, 0.0, 0.0], 'rewardMean': 0.6823303733408616, 'totalEpisodes': 161, 'stepsPerEpisode': 53, 'rewardPerEpisode': 41.30963821619474
'totalSteps': 11520, 'rewardStep': 0.6689609739402506, 'errorList': [], 'lossList': [0.0, -1.3265790110826492, 0.0, 13.939007189273834, 0.0, 0.0, 0.0], 'rewardMean': 0.6808448845185714, 'totalEpisodes': 168, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.58287721609262
'totalSteps': 12800, 'rewardStep': 0.7009860793136938, 'errorList': [], 'lossList': [0.0, -1.3371455454826355, 0.0, 30.725401830673217, 0.0, 0.0, 0.0], 'rewardMean': 0.6828590039980836, 'totalEpisodes': 174, 'stepsPerEpisode': 69, 'rewardPerEpisode': 58.92063143232526
'totalSteps': 14080, 'rewardStep': 0.7344092573171732, 'errorList': [], 'lossList': [0.0, -1.3226899152994156, 0.0, 10.668460701704026, 0.0, 0.0, 0.0], 'rewardMean': 0.7004351578821655, 'totalEpisodes': 177, 'stepsPerEpisode': 335, 'rewardPerEpisode': 274.4156277981452
'totalSteps': 15360, 'rewardStep': 0.8649638695443072, 'errorList': [], 'lossList': [0.0, -1.3197528332471848, 0.0, 5.607880353927612, 0.0, 0.0, 0.0], 'rewardMean': 0.7209470167239956, 'totalEpisodes': 179, 'stepsPerEpisode': 397, 'rewardPerEpisode': 335.54105356660506
'totalSteps': 16640, 'rewardStep': 0.576079018753019, 'errorList': [], 'lossList': [0.0, -1.2997992038726807, 0.0, 3.5585904502868653, 0.0, 0.0, 0.0], 'rewardMean': 0.7186336064338849, 'totalEpisodes': 180, 'stepsPerEpisode': 924, 'rewardPerEpisode': 720.9892693768769
'totalSteps': 17920, 'rewardStep': 0.749238476097566, 'errorList': [], 'lossList': [0.0, -1.2809318900108337, 0.0, 3.660671356320381, 0.0, 0.0, 0.0], 'rewardMean': 0.7298188697655592, 'totalEpisodes': 183, 'stepsPerEpisode': 140, 'rewardPerEpisode': 122.66022458383816
'totalSteps': 19200, 'rewardStep': 0.5958524999602646, 'errorList': [], 'lossList': [0.0, -1.2647342854738235, 0.0, 4.333351674079895, 0.0, 0.0, 0.0], 'rewardMean': 0.7259603915066866, 'totalEpisodes': 184, 'stepsPerEpisode': 820, 'rewardPerEpisode': 550.4987806688838
'totalSteps': 20480, 'rewardStep': 0.5845667252324418, 'errorList': [], 'lossList': [0.0, -1.2528088742494583, 0.0, 3.3350477766990663, 0.0, 0.0, 0.0], 'rewardMean': 0.705163653958435, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.5693857276584
'totalSteps': 21760, 'rewardStep': 0.7226511613645257, 'errorList': [], 'lossList': [0.0, -1.2340731078386307, 0.0, 1.9352404969930648, 0.0, 0.0, 0.0], 'rewardMean': 0.6954165031468309, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1071.7325828718779
'totalSteps': 23040, 'rewardStep': 0.8997756366683978, 'errorList': [], 'lossList': [0.0, -1.2060564064979553, 0.0, 1.175035674571991, 0.0, 0.0, 0.0], 'rewardMean': 0.709748369819164, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.3053498403478
'totalSteps': 24320, 'rewardStep': 0.8044974646601892, 'errorList': [], 'lossList': [0.0, -1.168600845336914, 0.0, 0.7608422274515033, 0.0, 0.0, 0.0], 'rewardMean': 0.7233020188911577, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.011363491584
'totalSteps': 25600, 'rewardStep': 0.9841584990943704, 'errorList': [0.06037255426570874, 0.07322060930000346, 0.09441390295433372, 0.05981087315273234, 0.0733033300736289, 0.07449752205166979, 0.08453151960975044, 0.10388783134777288, 0.08558249990159168, 0.09145673661670273, 0.07241097609010845, 0.08653844336191276, 0.08752181484274965, 0.08557898642359481, 0.08071080465516121, 0.08347484487096402, 0.0634228143326382, 0.07472665729179036, 0.12500674321346292, 0.06345265598238466, 0.10897734169149051, 0.08141885353408312, 0.08738859370398153, 0.08177954685343843, 0.08237809316756224, 0.09497044819073573, 0.12227683286046051, 0.11386684665482157, 0.07772288870694014, 0.07973507325336573, 0.08725779534949131, 0.0588740424741724, 0.0669063083329247, 0.06719456743591733, 0.11270374577656397, 0.08831074023270684, 0.09098095165029021, 0.06355220092652529, 0.10132653930784517, 0.07792816350631816, 0.08464731696381118, 0.08292817954813186, 0.08469577855800356, 0.11467943721934945, 0.09550723998218047, 0.06240943860988923, 0.08202313410520942, 0.09809154802220807, 0.1133318928784259, 0.07405097614463282], 'lossList': [0.0, -1.1179505491256714, 0.0, 0.7498536530323326, 0.0, 0.0, 0.0], 'rewardMean': 0.7516192608692254, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1209.7693801471287, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=69.71
