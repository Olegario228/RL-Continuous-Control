#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 143
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.638013742297759, 'errorList': [], 'lossList': [0.0, -1.4097434186935425, 0.0, 30.15844977378845, 0.0, 0.0, 0.0], 'rewardMean': 0.7481129254266493, 'totalEpisodes': 27, 'stepsPerEpisode': 35, 'rewardPerEpisode': 27.37945520405899
'totalSteps': 3840, 'rewardStep': 0.8573306744949621, 'errorList': [], 'lossList': [0.0, -1.393343409895897, 0.0, 59.39823217391968, 0.0, 0.0, 0.0], 'rewardMean': 0.7845188417827536, 'totalEpisodes': 76, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.562888818294034
'totalSteps': 5120, 'rewardStep': 0.5612322765664967, 'errorList': [], 'lossList': [0.0, -1.3908506059646606, 0.0, 53.367808170318604, 0.0, 0.0, 0.0], 'rewardMean': 0.7286972004786894, 'totalEpisodes': 117, 'stepsPerEpisode': 17, 'rewardPerEpisode': 10.374448963298605
'totalSteps': 6400, 'rewardStep': 0.7577345632121258, 'errorList': [], 'lossList': [0.0, -1.3773152810335159, 0.0, 61.162848491668704, 0.0, 0.0, 0.0], 'rewardMean': 0.7345046730253767, 'totalEpisodes': 155, 'stepsPerEpisode': 110, 'rewardPerEpisode': 80.2507235339746
'totalSteps': 7680, 'rewardStep': 0.9019425542360012, 'errorList': [], 'lossList': [0.0, -1.3762489175796508, 0.0, 52.58303087234497, 0.0, 0.0, 0.0], 'rewardMean': 0.7624109865604808, 'totalEpisodes': 184, 'stepsPerEpisode': 57, 'rewardPerEpisode': 48.12700566416803
'totalSteps': 8960, 'rewardStep': 0.8759583393898582, 'errorList': [], 'lossList': [0.0, -1.3730614000558854, 0.0, 35.528451552391054, 0.0, 0.0, 0.0], 'rewardMean': 0.7786320369646775, 'totalEpisodes': 195, 'stepsPerEpisode': 68, 'rewardPerEpisode': 52.590005971478284
'totalSteps': 10240, 'rewardStep': 0.6302460807031021, 'errorList': [], 'lossList': [0.0, -1.3596282225847245, 0.0, 28.600185017585755, 0.0, 0.0, 0.0], 'rewardMean': 0.7600837924319805, 'totalEpisodes': 202, 'stepsPerEpisode': 162, 'rewardPerEpisode': 136.13763631778272
'totalSteps': 11520, 'rewardStep': 0.6000786139977613, 'errorList': [], 'lossList': [0.0, -1.3588026690483093, 0.0, 12.545743834376335, 0.0, 0.0, 0.0], 'rewardMean': 0.7423054392726228, 'totalEpisodes': 207, 'stepsPerEpisode': 343, 'rewardPerEpisode': 281.0996068693546
'totalSteps': 12800, 'rewardStep': 0.6672227026979856, 'errorList': [], 'lossList': [0.0, -1.3654287642240523, 0.0, 12.4372510099411, 0.0, 0.0, 0.0], 'rewardMean': 0.7347971656151591, 'totalEpisodes': 212, 'stepsPerEpisode': 213, 'rewardPerEpisode': 180.68733586766854
'totalSteps': 14080, 'rewardStep': 0.7713180331581764, 'errorList': [], 'lossList': [0.0, -1.3340228706598283, 0.0, 8.239201536178589, 0.0, 0.0, 0.0], 'rewardMean': 0.7261077580754229, 'totalEpisodes': 216, 'stepsPerEpisode': 144, 'rewardPerEpisode': 114.24582092171201
'totalSteps': 15360, 'rewardStep': 0.7511401884276518, 'errorList': [], 'lossList': [0.0, -1.3171236032247544, 0.0, 6.658242985010147, 0.0, 0.0, 0.0], 'rewardMean': 0.7374204026884121, 'totalEpisodes': 221, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.061952890685596
'totalSteps': 16640, 'rewardStep': 0.8625019536380388, 'errorList': [], 'lossList': [0.0, -1.3144581538438798, 0.0, 3.8077821272611616, 0.0, 0.0, 0.0], 'rewardMean': 0.7379375306027197, 'totalEpisodes': 226, 'stepsPerEpisode': 40, 'rewardPerEpisode': 31.969235807717606
'totalSteps': 17920, 'rewardStep': 0.760449888524314, 'errorList': [], 'lossList': [0.0, -1.3066997307538986, 0.0, 3.2467426574230194, 0.0, 0.0, 0.0], 'rewardMean': 0.7578592917985014, 'totalEpisodes': 228, 'stepsPerEpisode': 396, 'rewardPerEpisode': 333.3933406723766
'totalSteps': 19200, 'rewardStep': 0.7540901552072404, 'errorList': [], 'lossList': [0.0, -1.2917306542396545, 0.0, 5.7144199407100675, 0.0, 0.0, 0.0], 'rewardMean': 0.7574948509980131, 'totalEpisodes': 232, 'stepsPerEpisode': 36, 'rewardPerEpisode': 30.317283853517864
'totalSteps': 20480, 'rewardStep': 0.8159011036121053, 'errorList': [], 'lossList': [0.0, -1.2823628133535385, 0.0, 4.56632416844368, 0.0, 0.0, 0.0], 'rewardMean': 0.7488907059356235, 'totalEpisodes': 236, 'stepsPerEpisode': 157, 'rewardPerEpisode': 127.07890764072187
'totalSteps': 21760, 'rewardStep': 0.48059775465903676, 'errorList': [], 'lossList': [0.0, -1.2683553892374038, 0.0, 3.4093952661752702, 0.0, 0.0, 0.0], 'rewardMean': 0.7093546474625413, 'totalEpisodes': 238, 'stepsPerEpisode': 583, 'rewardPerEpisode': 469.7062366727776
'totalSteps': 23040, 'rewardStep': 0.7304936638550554, 'errorList': [], 'lossList': [0.0, -1.2718469375371932, 0.0, 3.088514322042465, 0.0, 0.0, 0.0], 'rewardMean': 0.7193794057777365, 'totalEpisodes': 239, 'stepsPerEpisode': 1259, 'rewardPerEpisode': 983.8989246779513
'totalSteps': 24320, 'rewardStep': 0.6076590332171923, 'errorList': [], 'lossList': [0.0, -1.251933890581131, 0.0, 2.5851329162716867, 0.0, 0.0, 0.0], 'rewardMean': 0.7201374476996797, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.1430849061555
'totalSteps': 25600, 'rewardStep': 0.9847768479788, 'errorList': [0.09366616673781873, 0.06845797895401208, 0.06370224183418631, 0.07745131160391215, 0.08734879064758881, 0.06524693039123042, 0.0710932560166703, 0.06639701223960826, 0.08400878369540425, 0.07414154882121396, 0.08906128594678225, 0.06569437745666985, 0.07142870671908948, 0.09550167814028877, 0.06670494019073718, 0.07606397412516022, 0.09005393826520955, 0.08464566973145309, 0.06494116886243442, 0.09040177370854502, 0.07559669297313255, 0.09100242094518397, 0.06809048717532658, 0.06740226110148288, 0.071713901058448, 0.06360062774561766, 0.06377163847577723, 0.06590577332309211, 0.06906185427073515, 0.06625167462677158, 0.06611113264333561, 0.06547297809373835, 0.08779038206319942, 0.08432106359299836, 0.08848656429061574, 0.07024953542315082, 0.08622775760941512, 0.08342263938090862, 0.06747544949866438, 0.06648138563351555, 0.07860045190232214, 0.0777697221590751, 0.08083452592244676, 0.08793772304925447, 0.07700104037761292, 0.07919762062574988, 0.06565454369013565, 0.09755703001395004, 0.09886974092552786, 0.08338126447102903], 'lossList': [0.0, -1.2155344092845917, 0.0, 0.9512960468232632, 0.0, 0.0, 0.0], 'rewardMean': 0.7518928622277612, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.3622517628814, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=67.63
