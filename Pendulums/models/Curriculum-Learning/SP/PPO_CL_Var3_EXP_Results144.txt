#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 144
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.890475926648466, 'errorList': [], 'lossList': [0.0, -1.4264821779727936, 0.0, 28.339881443977355, 0.0, 0.0, 0.0], 'rewardMean': 0.8069134252638834, 'totalEpisodes': 25, 'stepsPerEpisode': 21, 'rewardPerEpisode': 19.37277295863293
'totalSteps': 3840, 'rewardStep': 0.8027660247455606, 'errorList': [], 'lossList': [0.0, -1.427937029004097, 0.0, 51.81419792175293, 0.0, 0.0, 0.0], 'rewardMean': 0.8055309584244424, 'totalEpisodes': 67, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.423656155047487
'totalSteps': 5120, 'rewardStep': 0.871338972770059, 'errorList': [], 'lossList': [0.0, -1.4127251249551773, 0.0, 54.910761756896974, 0.0, 0.0, 0.0], 'rewardMean': 0.8219829620108465, 'totalEpisodes': 117, 'stepsPerEpisode': 40, 'rewardPerEpisode': 33.403277417360094
'totalSteps': 6400, 'rewardStep': 0.8079655797739954, 'errorList': [], 'lossList': [0.0, -1.3986191105842591, 0.0, 53.84048868179321, 0.0, 0.0, 0.0], 'rewardMean': 0.8191794855634763, 'totalEpisodes': 147, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.41311009512826
'totalSteps': 7680, 'rewardStep': 0.7363905010854189, 'errorList': [], 'lossList': [0.0, -1.3823953503370285, 0.0, 52.741578073501586, 0.0, 0.0, 0.0], 'rewardMean': 0.8053813214838, 'totalEpisodes': 165, 'stepsPerEpisode': 39, 'rewardPerEpisode': 28.964965627803288
'totalSteps': 8960, 'rewardStep': 0.9483517695659778, 'errorList': [], 'lossList': [0.0, -1.364140220284462, 0.0, 27.971021797657013, 0.0, 0.0, 0.0], 'rewardMean': 0.8258056712098254, 'totalEpisodes': 174, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.644843555749069
'totalSteps': 10240, 'rewardStep': 0.8863355025097095, 'errorList': [], 'lossList': [0.0, -1.3491043722629548, 0.0, 17.40606486082077, 0.0, 0.0, 0.0], 'rewardMean': 0.833371900122311, 'totalEpisodes': 181, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.87016308385295
'totalSteps': 11520, 'rewardStep': 0.6208788746052714, 'errorList': [], 'lossList': [0.0, -1.3223971027135848, 0.0, 13.492773098945618, 0.0, 0.0, 0.0], 'rewardMean': 0.809761563953751, 'totalEpisodes': 184, 'stepsPerEpisode': 264, 'rewardPerEpisode': 219.34975462697156
'totalSteps': 12800, 'rewardStep': 0.883925346888373, 'errorList': [], 'lossList': [0.0, -1.3054703825712204, 0.0, 7.35385273039341, 0.0, 0.0, 0.0], 'rewardMean': 0.8171779422472133, 'totalEpisodes': 189, 'stepsPerEpisode': 131, 'rewardPerEpisode': 111.09799964392052
'totalSteps': 14080, 'rewardStep': 0.449135555948625, 'errorList': [], 'lossList': [0.0, -1.287051277756691, 0.0, 11.761025899648667, 0.0, 0.0, 0.0], 'rewardMean': 0.7897564054541457, 'totalEpisodes': 193, 'stepsPerEpisode': 215, 'rewardPerEpisode': 169.45048388076182
'totalSteps': 15360, 'rewardStep': 0.6851371575911889, 'errorList': [], 'lossList': [0.0, -1.2834080153703689, 0.0, 27.690573148727417, 0.0, 0.0, 0.0], 'rewardMean': 0.769222528548418, 'totalEpisodes': 199, 'stepsPerEpisode': 85, 'rewardPerEpisode': 66.53891587090322
'totalSteps': 16640, 'rewardStep': 0.31765243523736997, 'errorList': [], 'lossList': [0.0, -1.2708209127187728, 0.0, 18.986039961576463, 0.0, 0.0, 0.0], 'rewardMean': 0.720711169597599, 'totalEpisodes': 205, 'stepsPerEpisode': 166, 'rewardPerEpisode': 122.67892920561428
'totalSteps': 17920, 'rewardStep': 0.695251056021236, 'errorList': [], 'lossList': [0.0, -1.2566272872686386, 0.0, 6.946384329795837, 0.0, 0.0, 0.0], 'rewardMean': 0.7031023779227166, 'totalEpisodes': 209, 'stepsPerEpisode': 348, 'rewardPerEpisode': 316.16305158734565
'totalSteps': 19200, 'rewardStep': 0.9493655931534387, 'errorList': [2.954494222065158, 0.09815485166715751, 1.3731375928906775, 1.8588552939166476, 2.5681565278214937, 2.219622172110251, 1.0649270059332065, 1.4407262430101906, 1.777983005450056, 4.0671080475348385, 0.7974013320421305, 0.22000431502636086, 0.8416390361807048, 6.0821296099420366, 0.057066338239164155, 0.7255271834738438, 4.895156555132116, 3.0448306108786065, 2.8327980987164327, 0.5414711235629893, 4.207660725241755, 3.9109799746521157, 0.6511848761577106, 3.6939523929081295, 0.04516330603044794, 7.222765364900215, 0.6026211570356566, 7.50225792265195, 0.28614816484383193, 2.9708218384577876, 1.3598961740361926, 2.505852449696025, 0.8101003350455533, 4.200944203905141, 2.4255487128097797, 2.0307825560888055, 3.344498210515961, 2.6313488217104486, 1.0233075986102422, 1.3413867799104697, 2.911505419498543, 2.956360643458388, 0.8630808661566338, 2.466087823797999, 2.511411128239839, 6.77605480626983, 1.8787681667365015, 0.8424034071134329, 2.4887144658780587, 0.5072903075553766], 'lossList': [0.0, -1.263848102092743, 0.0, 4.291472086906433, 0.0, 0.0, 0.0], 'rewardMean': 0.7172423792606609, 'totalEpisodes': 214, 'stepsPerEpisode': 106, 'rewardPerEpisode': 99.07010091782617, 'successfulTests': 3
'totalSteps': 20480, 'rewardStep': 0.46456164170888825, 'errorList': [], 'lossList': [0.0, -1.2787369775772095, 0.0, 1.8927753621339798, 0.0, 0.0, 0.0], 'rewardMean': 0.6900594933230079, 'totalEpisodes': 217, 'stepsPerEpisode': 254, 'rewardPerEpisode': 187.09166062402866
'totalSteps': 21760, 'rewardStep': 0.8940169589128392, 'errorList': [], 'lossList': [0.0, -1.2679059737920761, 0.0, 2.2553492933511734, 0.0, 0.0, 0.0], 'rewardMean': 0.684626012257694, 'totalEpisodes': 220, 'stepsPerEpisode': 226, 'rewardPerEpisode': 200.37260507089826
'totalSteps': 23040, 'rewardStep': 0.951418336271239, 'errorList': [6.661524168071667, 3.3553607537091463, 3.607622407734132, 0.24014475719885806, 3.570507938553207, 1.1230955469992268, 5.773303406484605, 2.0435461349083006, 1.8715029737078697, 4.541536492777189, 6.3500668716584565, 5.48834748725564, 1.7188369295237118, 5.993689793304062, 5.040184481293081, 4.225159521543287, 3.9009180786864874, 0.46481761251694165, 2.128402891646078, 10.47537541120167, 1.4437376456650333, 3.9263917087048084, 4.4826081855268916, 4.460038394574948, 2.0295413773937323, 10.009626761267427, 3.3520725380226377, 5.470851045689659, 1.4260076209898205, 6.87442285800687, 3.9921661207279637, 9.751096042183072, 2.060250445132383, 1.3779074551259178, 3.8344674416747435, 3.1740631224927127, 0.1837476937453311, 3.3127273937178554, 3.543244841337731, 1.300839086579444, 5.617445865822446, 5.101745626535664, 5.1941392565758235, 5.207437834516948, 6.538432776007223, 9.012107402401364, 1.5989897180354584, 4.980604657685607, 3.635040581583896, 1.2096451062702451], 'lossList': [0.0, -1.261443452835083, 0.0, 1.6656424883008003, 0.0, 0.0, 0.0], 'rewardMean': 0.6911342956338469, 'totalEpisodes': 222, 'stepsPerEpisode': 652, 'rewardPerEpisode': 571.0996293041835, 'successfulTests': 1
'totalSteps': 24320, 'rewardStep': 0.7808148976478095, 'errorList': [], 'lossList': [0.0, -1.2601310616731645, 0.0, 1.7584621453285216, 0.0, 0.0, 0.0], 'rewardMean': 0.7071278979381008, 'totalEpisodes': 224, 'stepsPerEpisode': 268, 'rewardPerEpisode': 235.45190305458243
'totalSteps': 25600, 'rewardStep': 0.771501080143759, 'errorList': [], 'lossList': [0.0, -1.252301163673401, 0.0, 1.44625250518322, 0.0, 0.0, 0.0], 'rewardMean': 0.6958854712636394, 'totalEpisodes': 227, 'stepsPerEpisode': 217, 'rewardPerEpisode': 184.246225552506
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=19200, timeSpent=75.61
