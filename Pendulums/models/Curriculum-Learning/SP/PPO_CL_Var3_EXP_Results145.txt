#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 145
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.8833587550700254, 'errorList': [], 'lossList': [0.0, -1.435744668841362, 0.0, 25.822556810379027, 0.0, 0.0, 0.0], 'rewardMean': 0.8990896928254267, 'totalEpisodes': 14, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.469989583671847
'totalSteps': 3840, 'rewardStep': 0.7701848056574346, 'errorList': [], 'lossList': [0.0, -1.4196788042783737, 0.0, 57.237506275177005, 0.0, 0.0, 0.0], 'rewardMean': 0.8561213971027627, 'totalEpisodes': 53, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.260906933398519
'totalSteps': 5120, 'rewardStep': 0.7338587697801058, 'errorList': [], 'lossList': [0.0, -1.405683452486992, 0.0, 59.375363445281984, 0.0, 0.0, 0.0], 'rewardMean': 0.8255557402720985, 'totalEpisodes': 92, 'stepsPerEpisode': 48, 'rewardPerEpisode': 30.238892302005688
'totalSteps': 6400, 'rewardStep': 0.7748604847417853, 'errorList': [], 'lossList': [0.0, -1.3899749416112899, 0.0, 69.04772905349732, 0.0, 0.0, 0.0], 'rewardMean': 0.8154166891660359, 'totalEpisodes': 125, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.455326815790311
'totalSteps': 7680, 'rewardStep': 0.6706907384100091, 'errorList': [], 'lossList': [0.0, -1.3672387850284577, 0.0, 55.02433888435364, 0.0, 0.0, 0.0], 'rewardMean': 0.7912956973733648, 'totalEpisodes': 142, 'stepsPerEpisode': 46, 'rewardPerEpisode': 38.12777178208484
'totalSteps': 8960, 'rewardStep': 0.7596723268461951, 'errorList': [], 'lossList': [0.0, -1.3419196325540543, 0.0, 55.853562097549435, 0.0, 0.0, 0.0], 'rewardMean': 0.7867780730123405, 'totalEpisodes': 156, 'stepsPerEpisode': 78, 'rewardPerEpisode': 60.3465852514932
'totalSteps': 10240, 'rewardStep': 0.7076086216622639, 'errorList': [], 'lossList': [0.0, -1.3278876167535782, 0.0, 36.01906762599945, 0.0, 0.0, 0.0], 'rewardMean': 0.7768818915935809, 'totalEpisodes': 166, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.72728992279786
'totalSteps': 11520, 'rewardStep': 0.7113527822584833, 'errorList': [], 'lossList': [0.0, -1.2969958519935607, 0.0, 31.416948692798613, 0.0, 0.0, 0.0], 'rewardMean': 0.7696008794452367, 'totalEpisodes': 173, 'stepsPerEpisode': 81, 'rewardPerEpisode': 65.99126141045119
'totalSteps': 12800, 'rewardStep': 0.9277202979011, 'errorList': [], 'lossList': [0.0, -1.2800485044717789, 0.0, 16.312221994400023, 0.0, 0.0, 0.0], 'rewardMean': 0.7854128212908231, 'totalEpisodes': 178, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.89773961616255
'totalSteps': 14080, 'rewardStep': 0.5854572000633571, 'errorList': [], 'lossList': [0.0, -1.290963550209999, 0.0, 12.041513450145722, 0.0, 0.0, 0.0], 'rewardMean': 0.7524764782390759, 'totalEpisodes': 180, 'stepsPerEpisode': 57, 'rewardPerEpisode': 41.85061184122962
'totalSteps': 15360, 'rewardStep': 0.6381613336472937, 'errorList': [], 'lossList': [0.0, -1.2809892123937607, 0.0, 9.010916966199876, 0.0, 0.0, 0.0], 'rewardMean': 0.7279567360968028, 'totalEpisodes': 183, 'stepsPerEpisode': 261, 'rewardPerEpisode': 217.8128015392301
'totalSteps': 16640, 'rewardStep': 0.8653958792852597, 'errorList': [], 'lossList': [0.0, -1.2691520959138871, 0.0, 7.439268980622291, 0.0, 0.0, 0.0], 'rewardMean': 0.7374778434595853, 'totalEpisodes': 185, 'stepsPerEpisode': 453, 'rewardPerEpisode': 392.83033917819523
'totalSteps': 17920, 'rewardStep': 0.9418494772818251, 'errorList': [1.9387881349887734, 0.4205977438015325, 1.4449498452384961, 1.257865910984897, 1.47854288777555, 1.284972821050357, 2.371967267231054, 1.5406750747440716, 1.252927339062124, 2.0800417341832804, 1.8319698427226927, 2.4688363282785186, 2.19848896126758, 1.260403541592559, 1.386303531087279, 2.7719245062907207, 1.0574617223623128, 2.080030303352911, 1.3262084442284077, 0.7328800594153986, 1.2907999273575133, 2.0833842063711905, 0.6944458967699401, 1.5034915361132113, 1.715206023280483, 1.89693762980558, 1.7055353594049492, 1.6068300912817006, 2.3138361126325093, 2.3156522287986823, 0.6466407258927114, 0.9924590732475261, 1.056385141766333, 2.0969146944684103, 1.8083301537512304, 1.2712538446373658, 1.2852748106119367, 2.2258662362417985, 0.5571932984218596, 0.9466193808824797, 2.513743810778966, 2.578849656724895, 1.5116004776539167, 2.151431294976741, 2.120002730729319, 2.6430174132445483, 0.8830565948668649, 0.7028655936958319, 1.5812841790156869, 0.8577800381437437], 'lossList': [0.0, -1.2706967014074326, 0.0, 4.749116026163101, 0.0, 0.0, 0.0], 'rewardMean': 0.7582769142097573, 'totalEpisodes': 186, 'stepsPerEpisode': 536, 'rewardPerEpisode': 450.2484686500017, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.9207240002457485, 'errorList': [], 'lossList': [0.0, -1.2736225885152817, 0.0, 3.872799665927887, 0.0, 0.0, 0.0], 'rewardMean': 0.7728632657601535, 'totalEpisodes': 189, 'stepsPerEpisode': 111, 'rewardPerEpisode': 95.2994283607142
'totalSteps': 20480, 'rewardStep': 0.7133379770738444, 'errorList': [], 'lossList': [0.0, -1.2830481594800949, 0.0, 3.2089079028367995, 0.0, 0.0, 0.0], 'rewardMean': 0.7771279896265371, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 880.4563078253306
'totalSteps': 21760, 'rewardStep': 0.6399925547672478, 'errorList': [], 'lossList': [0.0, -1.2660015684366226, 0.0, 2.3274088144302367, 0.0, 0.0, 0.0], 'rewardMean': 0.7651600124186423, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 869.206938746329
'totalSteps': 23040, 'rewardStep': 0.7835554574332596, 'errorList': [], 'lossList': [0.0, -1.223959521651268, 0.0, 0.990278990790248, 0.0, 0.0, 0.0], 'rewardMean': 0.7727546959957419, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.4198833394569
'totalSteps': 24320, 'rewardStep': 0.7110507289455863, 'errorList': [], 'lossList': [0.0, -1.1838645005226136, 0.0, 0.6621313948929309, 0.0, 0.0, 0.0], 'rewardMean': 0.7727244906644521, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.986994586178
'totalSteps': 25600, 'rewardStep': 0.9302761481828735, 'errorList': [0.12424086669487863, 0.07764386055823042, 0.04803517340127723, 0.03451820275277523, 0.04449908316831937, 0.13996958850175947, 0.06471377872737889, 0.10130015483570987, 0.04412681866750954, 0.045095885527769795, 0.05742923567578501, 0.09402970550457174, 0.02212806438822796, 0.052853649309757736, 0.03505168261166832, 0.044922594530995445, 0.09419059374992271, 0.045275264253123386, 0.10182891307273448, 0.018130251745521426, 0.01672637342394868, 0.03990509433841018, 0.018961793311293464, 0.037052254813292065, 0.1267034745255997, 0.06666558558023071, 0.08811564804391665, 0.05106296082061389, 0.07840542412264961, 0.015191522671453895, 0.05232387522586921, 0.016691185733714052, 0.06398147551075303, 0.049160527140393075, 0.04445053922757488, 0.036870721403762324, 0.13037420991460053, 0.052543859881632585, 0.016802793585143153, 0.058562596450870995, 0.05154543140262015, 0.022654401377135133, 0.04860676625220016, 0.039200182340613905, 0.024302387189697704, 0.043139171815043516, 0.04764079473915622, 0.057203626708389854, 0.047985982194290566, 0.07440425148446976], 'lossList': [0.0, -1.1406759375333786, 0.0, 0.6760846246778965, 0.0, 0.0, 0.0], 'rewardMean': 0.7729800756926295, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.0473376504897, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=77.23
