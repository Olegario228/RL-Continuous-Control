#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 147
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.6305998043998102, 'errorList': [], 'lossList': [0.0, -1.4386485439538956, 0.0, 26.507574090957643, 0.0, 0.0, 0.0], 'rewardMean': 0.566850326151911, 'totalEpisodes': 18, 'stepsPerEpisode': 134, 'rewardPerEpisode': 85.82642742344937
'totalSteps': 3840, 'rewardStep': 0.7200385569115609, 'errorList': [], 'lossList': [0.0, -1.4204686671495437, 0.0, 55.10150939941406, 0.0, 0.0, 0.0], 'rewardMean': 0.6179130697384609, 'totalEpisodes': 54, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.623914790850256
'totalSteps': 5120, 'rewardStep': 0.6474607601600055, 'errorList': [], 'lossList': [0.0, -1.3957671147584916, 0.0, 57.848607559204105, 0.0, 0.0, 0.0], 'rewardMean': 0.6252999923438471, 'totalEpisodes': 99, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.840292294056827
'totalSteps': 6400, 'rewardStep': 0.6614797417627117, 'errorList': [], 'lossList': [0.0, -1.3809624475240707, 0.0, 57.213220348358156, 0.0, 0.0, 0.0], 'rewardMean': 0.63253594222762, 'totalEpisodes': 139, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.35421791640382
'totalSteps': 7680, 'rewardStep': 0.9718735260838064, 'errorList': [], 'lossList': [0.0, -1.368862321972847, 0.0, 56.948565254211424, 0.0, 0.0, 0.0], 'rewardMean': 0.6890922062036511, 'totalEpisodes': 167, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.459709764040621
'totalSteps': 8960, 'rewardStep': 0.3861506867104399, 'errorList': [], 'lossList': [0.0, -1.3645675045251846, 0.0, 44.179554777145384, 0.0, 0.0, 0.0], 'rewardMean': 0.6458148462760496, 'totalEpisodes': 179, 'stepsPerEpisode': 134, 'rewardPerEpisode': 99.6684757859555
'totalSteps': 10240, 'rewardStep': 0.6702886073461387, 'errorList': [], 'lossList': [0.0, -1.353320564031601, 0.0, 30.47050044775009, 0.0, 0.0, 0.0], 'rewardMean': 0.6488740664098107, 'totalEpisodes': 189, 'stepsPerEpisode': 109, 'rewardPerEpisode': 90.02104984442212
'totalSteps': 11520, 'rewardStep': 0.8913264868985316, 'errorList': [], 'lossList': [0.0, -1.3497760307788849, 0.0, 30.868147883415222, 0.0, 0.0, 0.0], 'rewardMean': 0.6758132242418908, 'totalEpisodes': 197, 'stepsPerEpisode': 142, 'rewardPerEpisode': 119.76741277622769
'totalSteps': 12800, 'rewardStep': 0.8009955628308324, 'errorList': [], 'lossList': [0.0, -1.3465454488992692, 0.0, 25.926015577316285, 0.0, 0.0, 0.0], 'rewardMean': 0.6883314581007849, 'totalEpisodes': 204, 'stepsPerEpisode': 220, 'rewardPerEpisode': 184.8769109275915
'totalSteps': 14080, 'rewardStep': 0.6448259357736903, 'errorList': [], 'lossList': [0.0, -1.3379549086093903, 0.0, 9.517557221651078, 0.0, 0.0, 0.0], 'rewardMean': 0.7025039668877529, 'totalEpisodes': 210, 'stepsPerEpisode': 88, 'rewardPerEpisode': 69.56707593573914
'totalSteps': 15360, 'rewardStep': 0.9153593276514161, 'errorList': [], 'lossList': [0.0, -1.349772160053253, 0.0, 9.610346958637237, 0.0, 0.0, 0.0], 'rewardMean': 0.7309799192129134, 'totalEpisodes': 216, 'stepsPerEpisode': 142, 'rewardPerEpisode': 117.82349191936947
'totalSteps': 16640, 'rewardStep': 0.8199021206296268, 'errorList': [], 'lossList': [0.0, -1.3683898907899856, 0.0, 6.878358145952225, 0.0, 0.0, 0.0], 'rewardMean': 0.74096627558472, 'totalEpisodes': 221, 'stepsPerEpisode': 422, 'rewardPerEpisode': 346.3127955458131
'totalSteps': 17920, 'rewardStep': 0.8278649289475917, 'errorList': [], 'lossList': [0.0, -1.378152099251747, 0.0, 5.6980228972435, 0.0, 0.0, 0.0], 'rewardMean': 0.7590066924634786, 'totalEpisodes': 224, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.4148071575662624
'totalSteps': 19200, 'rewardStep': 0.7797174459409292, 'errorList': [], 'lossList': [0.0, -1.3647481375932693, 0.0, 24.3338566327095, 0.0, 0.0, 0.0], 'rewardMean': 0.7708304628813003, 'totalEpisodes': 225, 'stepsPerEpisode': 760, 'rewardPerEpisode': 637.7928620617934
'totalSteps': 20480, 'rewardStep': 0.7181732452512467, 'errorList': [], 'lossList': [0.0, -1.3477880787849426, 0.0, 17.197291803359985, 0.0, 0.0, 0.0], 'rewardMean': 0.7454604347980444, 'totalEpisodes': 226, 'stepsPerEpisode': 443, 'rewardPerEpisode': 350.6961262172724
'totalSteps': 21760, 'rewardStep': 0.831532248538334, 'errorList': [], 'lossList': [0.0, -1.3152487295866013, 0.0, 2.0432145808637143, 0.0, 0.0, 0.0], 'rewardMean': 0.7899985909808338, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.7151012666882
'totalSteps': 23040, 'rewardStep': 0.9396691842336227, 'errorList': [0.20385646460580867, 0.19329797001780927, 0.21046975015476643, 0.1949644370151491, 0.18898710244934358, 0.2331970483811135, 0.18816531440817869, 0.23135003712410568, 0.26017968900213606, 0.19135753198095415, 0.30799952670834213, 0.20378314238693268, 0.21231772927144263, 0.2327069281192105, 0.19430531035140675, 0.19908997003001672, 0.20917482755237807, 0.1959802291528637, 0.19116289450682364, 0.21003433125611234, 0.3067668283577872, 0.23362511994670324, 0.2072368606893084, 0.2123081747482603, 0.20810494946848723, 0.21201519597081675, 0.20955345852383725, 0.21189831154335795, 0.186812211964782, 0.19645714889033825, 0.20824855255284813, 0.22121083620411924, 0.20722021012932965, 0.19519899050187095, 0.30098113260737824, 0.21051338090124638, 0.31356947689460246, 0.2054894607209204, 0.2015159771640585, 0.2321274404053921, 0.19798840519749045, 0.1946234185988497, 0.21404560912656354, 0.18060907045835004, 0.18194264547294486, 0.19461831383053554, 0.19998967295543604, 0.195513215099735, 0.2199007618919809, 0.2677574701095628], 'lossList': [0.0, -1.2581795197725296, 0.0, 1.7647611379250885, 0.0, 0.0, 0.0], 'rewardMean': 0.8169366486695822, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.2089571280871, 'successfulTests': 19
'totalSteps': 24320, 'rewardStep': 0.8624585985644394, 'errorList': [], 'lossList': [0.0, -1.2354578280448913, 0.0, 0.9354963849484921, 0.0, 0.0, 0.0], 'rewardMean': 0.814049859836173, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.2263195333442
'totalSteps': 25600, 'rewardStep': 0.8809434043948613, 'errorList': [], 'lossList': [0.0, -1.2223075968027115, 0.0, 0.9195001456141472, 0.0, 0.0, 0.0], 'rewardMean': 0.8220446439925757, 'totalEpisodes': 226, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.5692206304186
#maxSuccessfulTests=19, maxSuccessfulTestsAtStep=23040, timeSpent=60.96
