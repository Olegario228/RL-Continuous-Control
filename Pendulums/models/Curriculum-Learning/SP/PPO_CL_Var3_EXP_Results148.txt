#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 148
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.7880011736062333, 'errorList': [], 'lossList': [0.0, -1.4160270804166795, 0.0, 28.029215881824495, 0.0, 0.0, 0.0], 'rewardMean': 0.8434657947164869, 'totalEpisodes': 20, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.999291988436445
'totalSteps': 3840, 'rewardStep': 0.9418363825077906, 'errorList': [], 'lossList': [0.0, -1.3916313457489013, 0.0, 65.52488119125366, 0.0, 0.0, 0.0], 'rewardMean': 0.8762559906469214, 'totalEpisodes': 57, 'stepsPerEpisode': 17, 'rewardPerEpisode': 16.294609623548492
'totalSteps': 5120, 'rewardStep': 0.656865037728159, 'errorList': [], 'lossList': [0.0, -1.3757003551721574, 0.0, 72.14587129592896, 0.0, 0.0, 0.0], 'rewardMean': 0.8214082524172308, 'totalEpisodes': 97, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.357666057104607
'totalSteps': 6400, 'rewardStep': 0.9747523068402082, 'errorList': [], 'lossList': [0.0, -1.3635711866617202, 0.0, 63.49215866088867, 0.0, 0.0, 0.0], 'rewardMean': 0.8520770633018262, 'totalEpisodes': 129, 'stepsPerEpisode': 40, 'rewardPerEpisode': 32.09793702002027
'totalSteps': 7680, 'rewardStep': 0.8865338928343417, 'errorList': [], 'lossList': [0.0, -1.36082696557045, 0.0, 58.917325286865236, 0.0, 0.0, 0.0], 'rewardMean': 0.8578198682239121, 'totalEpisodes': 156, 'stepsPerEpisode': 41, 'rewardPerEpisode': 31.849983353392282
'totalSteps': 8960, 'rewardStep': 0.8274926563485877, 'errorList': [], 'lossList': [0.0, -1.3432388526201249, 0.0, 38.47662633895874, 0.0, 0.0, 0.0], 'rewardMean': 0.85348740938458, 'totalEpisodes': 171, 'stepsPerEpisode': 24, 'rewardPerEpisode': 18.75077639480839
'totalSteps': 10240, 'rewardStep': 0.5141650469084538, 'errorList': [], 'lossList': [0.0, -1.3301451319456101, 0.0, 35.63046206474304, 0.0, 0.0, 0.0], 'rewardMean': 0.8110721140750643, 'totalEpisodes': 177, 'stepsPerEpisode': 160, 'rewardPerEpisode': 126.4080233448551
'totalSteps': 11520, 'rewardStep': 0.946689562998635, 'errorList': [15.835773498465876, 19.40405208479333, 4.655869883524967, 5.970777816559586, 5.815365010277226, 2.797760471733203, 8.306896324535824, 3.230247712890441, 2.679684857045447, 6.957958078434153, 6.320887941022185, 0.256377492587582, 17.84665608690196, 3.9106560112318167, 4.931472903024796, 0.47128264355247823, 8.192779034583047, 13.510152208046346, 8.073181017940673, 11.33326005154872, 19.479503723114235, 17.152611296737316, 7.281658979728185, 9.955735432499813, 17.282943823441734, 1.1856843923823228, 11.413374124708149, 6.285508578434037, 5.90504071453817, 0.2791150974102357, 1.0020319622070084, 5.962016879360007, 7.88537910741085, 11.682601710841936, 15.690660361214713, 8.766478052048884, 20.78433529505742, 5.881689675936084, 2.690680045859303, 10.581007812657917, 3.549037253113339, 2.7883831664457324, 8.724010207601387, 2.156057819140011, 4.90274097114535, 4.926290356763237, 9.061979507439203, 8.17231009145132, 20.68012977588728, 8.58539077781799], 'lossList': [0.0, -1.3169892454147338, 0.0, 44.78090993404388, 0.0, 0.0, 0.0], 'rewardMean': 0.8261407195110166, 'totalEpisodes': 184, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.709250811259064, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.7514442563333865, 'errorList': [], 'lossList': [0.0, -1.3045274817943573, 0.0, 12.381082619428634, 0.0, 0.0, 0.0], 'rewardMean': 0.8186710731932536, 'totalEpisodes': 189, 'stepsPerEpisode': 41, 'rewardPerEpisode': 35.73349731186197
'totalSteps': 14080, 'rewardStep': 0.7186636858200496, 'errorList': [], 'lossList': [0.0, -1.3007651096582413, 0.0, 13.24640101313591, 0.0, 0.0, 0.0], 'rewardMean': 0.8006444001925846, 'totalEpisodes': 193, 'stepsPerEpisode': 74, 'rewardPerEpisode': 55.47117086874731
'totalSteps': 15360, 'rewardStep': 0.6757915601655411, 'errorList': [], 'lossList': [0.0, -1.3044787973165513, 0.0, 9.297779792547226, 0.0, 0.0, 0.0], 'rewardMean': 0.7894234388485153, 'totalEpisodes': 196, 'stepsPerEpisode': 402, 'rewardPerEpisode': 339.5549480531851
'totalSteps': 16640, 'rewardStep': 0.6615849209281982, 'errorList': [], 'lossList': [0.0, -1.3188401865959167, 0.0, 5.3284093302488325, 0.0, 0.0, 0.0], 'rewardMean': 0.761398292690556, 'totalEpisodes': 199, 'stepsPerEpisode': 218, 'rewardPerEpisode': 177.19669654804875
'totalSteps': 17920, 'rewardStep': 0.6099693929943393, 'errorList': [], 'lossList': [0.0, -1.3362752044200896, 0.0, 3.455221470594406, 0.0, 0.0, 0.0], 'rewardMean': 0.756708728217174, 'totalEpisodes': 202, 'stepsPerEpisode': 196, 'rewardPerEpisode': 144.6262540347207
'totalSteps': 19200, 'rewardStep': 0.9186791848360376, 'errorList': [], 'lossList': [0.0, -1.345392665863037, 0.0, 2.7959328892827036, 0.0, 0.0, 0.0], 'rewardMean': 0.751101416016757, 'totalEpisodes': 203, 'stepsPerEpisode': 1130, 'rewardPerEpisode': 831.330161883522
'totalSteps': 20480, 'rewardStep': 0.8023799681784207, 'errorList': [], 'lossList': [0.0, -1.3213709539175034, 0.0, 2.7061680525541307, 0.0, 0.0, 0.0], 'rewardMean': 0.7426860235511651, 'totalEpisodes': 204, 'stepsPerEpisode': 264, 'rewardPerEpisode': 224.7042064709238
'totalSteps': 21760, 'rewardStep': 0.7939146040401458, 'errorList': [], 'lossList': [0.0, -1.2943351894617081, 0.0, 2.3178915750980376, 0.0, 0.0, 0.0], 'rewardMean': 0.7393282183203207, 'totalEpisodes': 205, 'stepsPerEpisode': 695, 'rewardPerEpisode': 598.0532782920993
'totalSteps': 23040, 'rewardStep': 0.7685156406494559, 'errorList': [], 'lossList': [0.0, -1.2763463366031647, 0.0, 1.1102542988955975, 0.0, 0.0, 0.0], 'rewardMean': 0.764763277694421, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1042.9881549981446
'totalSteps': 24320, 'rewardStep': 0.7975519804024848, 'errorList': [], 'lossList': [0.0, -1.238337777853012, 0.0, 0.7208318225294351, 0.0, 0.0, 0.0], 'rewardMean': 0.7498495194348059, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.1316196814644
'totalSteps': 25600, 'rewardStep': 0.9538453179214913, 'errorList': [0.02437587713081795, 0.03260943733850174, 0.037626177288271376, 0.02586680812119612, 0.032817193724267266, 0.035352246573141816, 0.04486657900072509, 0.032746689199672614, 0.02691156827947247, 0.02796957875550509, 0.0378355270559921, 0.03235478377376756, 0.030948713639327975, 0.025294000555732294, 0.05027966133974035, 0.023998423160021817, 0.02422792473711276, 0.02350604220333922, 0.041804814624923956, 0.035778247759669785, 0.028496246105218614, 0.04552537327060823, 0.030755018680979983, 0.034341981179354, 0.043312416882174104, 0.037215949847986644, 0.037577986253968736, 0.04626813556243461, 0.03670015171119006, 0.046702917181313815, 0.047408198688774564, 0.03280072075957568, 0.026666318280289484, 0.023526786912321625, 0.023854234041870707, 0.03784284383599271, 0.04004102912919603, 0.02335507273324409, 0.03587885786828294, 0.04949077988871166, 0.026292069491275275, 0.0298170160255789, 0.04042740160709809, 0.027393890115813464, 0.025892092046352875, 0.022954542724255265, 0.04359757250119832, 0.024839410759598814, 0.024965302180028556, 0.031645243242663615], 'lossList': [0.0, -1.2043877267837524, 0.0, 0.6695257741212844, 0.0, 0.0, 0.0], 'rewardMean': 0.7700896255936165, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.646994688303, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=57.5
