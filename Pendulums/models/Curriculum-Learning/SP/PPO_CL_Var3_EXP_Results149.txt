#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 149
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8988077481829316, 'errorList': [], 'lossList': [0.0, -1.4252438408136368, 0.0, 29.687528133392334, 0.0, 0.0, 0.0], 'rewardMean': 0.8240493062411063, 'totalEpisodes': 19, 'stepsPerEpisode': 37, 'rewardPerEpisode': 30.42008626278728
'totalSteps': 3840, 'rewardStep': 0.7298825911377629, 'errorList': [], 'lossList': [0.0, -1.4321710324287416, 0.0, 59.26775043487549, 0.0, 0.0, 0.0], 'rewardMean': 0.7926604012066584, 'totalEpisodes': 55, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.723159604571677
'totalSteps': 5120, 'rewardStep': 0.8530910319916695, 'errorList': [], 'lossList': [0.0, -1.414381023645401, 0.0, 73.09476469039917, 0.0, 0.0, 0.0], 'rewardMean': 0.8077680589029111, 'totalEpisodes': 101, 'stepsPerEpisode': 43, 'rewardPerEpisode': 37.796118718995615
'totalSteps': 6400, 'rewardStep': 0.4922759511435546, 'errorList': [], 'lossList': [0.0, -1.3922814017534255, 0.0, 64.94009077072144, 0.0, 0.0, 0.0], 'rewardMean': 0.7446696373510399, 'totalEpisodes': 133, 'stepsPerEpisode': 21, 'rewardPerEpisode': 13.562807423589792
'totalSteps': 7680, 'rewardStep': 0.8089914444363463, 'errorList': [], 'lossList': [0.0, -1.37878593146801, 0.0, 58.83532161712647, 0.0, 0.0, 0.0], 'rewardMean': 0.7553899385319243, 'totalEpisodes': 154, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.92466496751783
'totalSteps': 8960, 'rewardStep': 0.8871233631245088, 'errorList': [], 'lossList': [0.0, -1.3715590637922288, 0.0, 57.30096693038941, 0.0, 0.0, 0.0], 'rewardMean': 0.7742089991880078, 'totalEpisodes': 166, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.496656343121023
'totalSteps': 10240, 'rewardStep': 0.7732295373981228, 'errorList': [], 'lossList': [0.0, -1.3727431493997573, 0.0, 15.8001331615448, 0.0, 0.0, 0.0], 'rewardMean': 0.7740865664642722, 'totalEpisodes': 174, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.70146742245715
'totalSteps': 11520, 'rewardStep': 0.547132070478104, 'errorList': [], 'lossList': [0.0, -1.3787197422981263, 0.0, 15.66872218132019, 0.0, 0.0, 0.0], 'rewardMean': 0.7488694002435868, 'totalEpisodes': 181, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.1084820081639757
'totalSteps': 12800, 'rewardStep': 0.6928363929740251, 'errorList': [], 'lossList': [0.0, -1.3774598342180253, 0.0, 28.731052780151366, 0.0, 0.0, 0.0], 'rewardMean': 0.7432660995166307, 'totalEpisodes': 188, 'stepsPerEpisode': 67, 'rewardPerEpisode': 56.69144825424628
'totalSteps': 14080, 'rewardStep': 0.8373280310789807, 'errorList': [], 'lossList': [0.0, -1.3580815309286118, 0.0, 8.249325561523438, 0.0, 0.0, 0.0], 'rewardMean': 0.7520698161946007, 'totalEpisodes': 193, 'stepsPerEpisode': 145, 'rewardPerEpisode': 125.31310419142312
'totalSteps': 15360, 'rewardStep': 0.8662825789839185, 'errorList': [], 'lossList': [0.0, -1.3399353206157685, 0.0, 11.357970134019851, 0.0, 0.0, 0.0], 'rewardMean': 0.7488172992746993, 'totalEpisodes': 200, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8662825789839185
'totalSteps': 16640, 'rewardStep': 0.42167035127089436, 'errorList': [], 'lossList': [0.0, -1.3248153340816498, 0.0, 6.002052655220032, 0.0, 0.0, 0.0], 'rewardMean': 0.7179960752880123, 'totalEpisodes': 204, 'stepsPerEpisode': 163, 'rewardPerEpisode': 122.4681733089355
'totalSteps': 17920, 'rewardStep': 0.8090288241861721, 'errorList': [], 'lossList': [0.0, -1.3028785681724548, 0.0, 5.169676301479339, 0.0, 0.0, 0.0], 'rewardMean': 0.7135898545074627, 'totalEpisodes': 208, 'stepsPerEpisode': 142, 'rewardPerEpisode': 128.1504983522209
'totalSteps': 19200, 'rewardStep': 0.6071198771026153, 'errorList': [], 'lossList': [0.0, -1.2740902990102767, 0.0, 4.028003172278404, 0.0, 0.0, 0.0], 'rewardMean': 0.7250742471033689, 'totalEpisodes': 210, 'stepsPerEpisode': 338, 'rewardPerEpisode': 260.37069043808464
'totalSteps': 20480, 'rewardStep': 0.6949926130308584, 'errorList': [], 'lossList': [0.0, -1.2423678755760192, 0.0, 2.928365609049797, 0.0, 0.0, 0.0], 'rewardMean': 0.7136743639628199, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 804.8195147722967
'totalSteps': 21760, 'rewardStep': 0.7105743495953296, 'errorList': [], 'lossList': [0.0, -1.2154039007425308, 0.0, 3.773034629225731, 0.0, 0.0, 0.0], 'rewardMean': 0.696019462609902, 'totalEpisodes': 211, 'stepsPerEpisode': 532, 'rewardPerEpisode': 430.51152663526096
'totalSteps': 23040, 'rewardStep': 0.6681752437951284, 'errorList': [], 'lossList': [0.0, -1.1990210992097854, 0.0, 3.0236505037546157, 0.0, 0.0, 0.0], 'rewardMean': 0.6855140332496027, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 943.2059906668342
'totalSteps': 24320, 'rewardStep': 0.789437330530079, 'errorList': [], 'lossList': [0.0, -1.1799948048591613, 0.0, 1.6329461567103862, 0.0, 0.0, 0.0], 'rewardMean': 0.7097445592548002, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.7091163100656
'totalSteps': 25600, 'rewardStep': 0.9828503507303358, 'errorList': [0.022229694462409248, 0.04103150331855031, 0.018047416979728642, 0.024600624679499992, 0.024188591713859453, 0.017237571699052018, 0.01891685399493503, 0.018190675464355068, 0.01772299470881017, 0.01747331215676087, 0.01812617394693183, 0.01862640155893888, 0.018444062587102125, 0.02631263946334746, 0.017905655461877248, 0.018336575815189653, 0.016782453011076257, 0.019088053231841186, 0.018264278193467758, 0.025578436748156526, 0.018090416418966398, 0.027935311139117378, 0.01803579471128715, 0.01894969020750363, 0.027444497974518894, 0.025585577328605592, 0.017900979362106124, 0.019263651172832283, 0.01880776049269351, 0.01800687010629371, 0.018160300083827456, 0.03549192825353038, 0.018688442148013046, 0.02456360873670022, 0.03254631145354189, 0.02290147147891779, 0.01841905691620893, 0.03497673377344216, 0.01827917255087656, 0.02581681835236407, 0.028706172178425433, 0.020465830950862893, 0.02161005707658347, 0.044194594889618986, 0.017780418534789014, 0.018267088054666033, 0.017207772172835544, 0.018584212257538313, 0.017638028790330523, 0.01832957602307434], 'lossList': [0.0, -1.1517983955144881, 0.0, 1.2279264303296804, 0.0, 0.0, 0.0], 'rewardMean': 0.7387459550304312, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.1586145532765, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=50.14
