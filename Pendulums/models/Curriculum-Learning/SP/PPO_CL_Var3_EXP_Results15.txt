#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 15
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.7179727547571633, 'errorList': [], 'lossList': [0.0, -1.4318319684267045, 0.0, 31.11344696998596, 0.0, 0.0, 0.0], 'rewardMean': 0.8067771729826652, 'totalEpisodes': 63, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.022129773641149
'totalSteps': 3840, 'rewardStep': 0.3674685895201403, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5871228812514028, 'totalEpisodes': 130, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.717718480148351
'totalSteps': 5120, 'rewardStep': 0.8094813842058426, 'errorList': [], 'lossList': [0.0, -1.4228231018781663, 0.0, 38.96951192855835, 0.0, 0.0, 0.0], 'rewardMean': 0.6315945818422908, 'totalEpisodes': 188, 'stepsPerEpisode': 63, 'rewardPerEpisode': 51.91359696891508
'totalSteps': 6400, 'rewardStep': 0.5574694134101894, 'errorList': [], 'lossList': [0.0, -1.4053197848796843, 0.0, 44.66239435195923, 0.0, 0.0, 0.0], 'rewardMean': 0.6192403871036072, 'totalEpisodes': 219, 'stepsPerEpisode': 35, 'rewardPerEpisode': 24.948450019664936
'totalSteps': 7680, 'rewardStep': 0.5499239286806015, 'errorList': [], 'lossList': [0.0, -1.3816691344976426, 0.0, 41.027211027145384, 0.0, 0.0, 0.0], 'rewardMean': 0.6093380359003208, 'totalEpisodes': 236, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.5917721560565605
'totalSteps': 8960, 'rewardStep': 0.7367129298385086, 'errorList': [], 'lossList': [0.0, -1.3658289164304733, 0.0, 37.668304488658904, 0.0, 0.0, 0.0], 'rewardMean': 0.625259897642594, 'totalEpisodes': 244, 'stepsPerEpisode': 101, 'rewardPerEpisode': 79.25592678500166
'totalSteps': 10240, 'rewardStep': 0.7092079993015021, 'errorList': [], 'lossList': [0.0, -1.3596265882253646, 0.0, 29.364725246429444, 0.0, 0.0, 0.0], 'rewardMean': 0.6345874644935838, 'totalEpisodes': 254, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.42184507821215
'totalSteps': 11520, 'rewardStep': 0.4783634246762375, 'errorList': [], 'lossList': [0.0, -1.351109303832054, 0.0, 31.338273956775666, 0.0, 0.0, 0.0], 'rewardMean': 0.6189650605118492, 'totalEpisodes': 259, 'stepsPerEpisode': 333, 'rewardPerEpisode': 259.07733145350176
'totalSteps': 12800, 'rewardStep': 0.757798601928719, 'errorList': [], 'lossList': [0.0, -1.3359134209156036, 0.0, 11.2351376837492, 0.0, 0.0, 0.0], 'rewardMean': 0.6051867615839044, 'totalEpisodes': 262, 'stepsPerEpisode': 219, 'rewardPerEpisode': 181.08697430996645
'totalSteps': 14080, 'rewardStep': 0.39049331688618194, 'errorList': [], 'lossList': [0.0, -1.321093856692314, 0.0, 4.945573214292526, 0.0, 0.0, 0.0], 'rewardMean': 0.5724388177968063, 'totalEpisodes': 264, 'stepsPerEpisode': 322, 'rewardPerEpisode': 205.33033675286725
'totalSteps': 15360, 'rewardStep': 0.7814963066413473, 'errorList': [], 'lossList': [0.0, -1.3131708830595017, 0.0, 9.427915489673614, 0.0, 0.0, 0.0], 'rewardMean': 0.6138415895089271, 'totalEpisodes': 266, 'stepsPerEpisode': 690, 'rewardPerEpisode': 562.9198720585061
'totalSteps': 16640, 'rewardStep': 0.948120362879502, 'errorList': [0.03653012441117167, 0.05983406408523487, 0.01618495179959846, 0.02130730485105138, 0.01420196600068548, 0.04805945842956378, 0.026947684984360423, 0.03652312049819752, 0.06916364978400512, 0.043818485725650516, 0.025069434972429423, 0.045316602017151895, 0.027634833810432666, 0.03348938764778835, 0.07461515956853024, 0.050509196534077476, 0.015569920556475223, 0.04196167434338848, 0.02881297339727867, 0.032882756609418674, 0.05906203397192736, 0.04325176322813621, 0.0544854264120009, 0.04883643257812232, 0.048940897878901525, 0.0659806641585745, 0.10757791956393338, 0.016649776976343577, 0.06056395238230315, 0.06050845746788783, 0.05265264711486636, 0.05032979649583351, 0.02077845577737685, 0.040640925406794794, 0.03730684770526641, 0.01704280601787408, 0.08006389408067582, 0.015291153413511395, 0.013877573263141733, 0.015219993787990922, 0.015592753380971082, 0.02295952757839662, 0.016316144122207072, 0.02853125776979924, 0.021440970801755225, 0.06874402937302147, 0.037724845487584376, 0.05338135687371672, 0.06752877925296905, 0.06343786328720576], 'lossList': [0.0, -1.310064811706543, 0.0, 5.4678555865585805, 0.0, 0.0, 0.0], 'rewardMean': 0.6719067668448633, 'totalEpisodes': 266, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.7937079002704, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=16640, timeSpent=49.7
