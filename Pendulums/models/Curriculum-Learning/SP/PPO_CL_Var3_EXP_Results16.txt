#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 16
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7626097709365603, 'errorList': [], 'lossList': [0.0, -1.4169733202457429, 0.0, 34.19180130004883, 0.0, 0.0, 0.0], 'rewardMean': 0.6606287447064577, 'totalEpisodes': 63, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.909816137613305
'totalSteps': 3840, 'rewardStep': 0.7660864701362816, 'errorList': [], 'lossList': [0.0, -1.4037377369403838, 0.0, 35.63134483337402, 0.0, 0.0, 0.0], 'rewardMean': 0.6957813198497323, 'totalEpisodes': 125, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.267722544743862
'totalSteps': 5120, 'rewardStep': 0.8966988111323905, 'errorList': [], 'lossList': [0.0, -1.3874078077077865, 0.0, 37.04357908248901, 0.0, 0.0, 0.0], 'rewardMean': 0.7460106926703969, 'totalEpisodes': 172, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.974938717166919
'totalSteps': 6400, 'rewardStep': 0.7998198824412017, 'errorList': [], 'lossList': [0.0, -1.3759554308652877, 0.0, 44.38858953475952, 0.0, 0.0, 0.0], 'rewardMean': 0.7567725306245578, 'totalEpisodes': 199, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.370184831778024
'totalSteps': 7680, 'rewardStep': 0.7470141216265502, 'errorList': [], 'lossList': [0.0, -1.3601784700155257, 0.0, 35.9637542104721, 0.0, 0.0, 0.0], 'rewardMean': 0.7551461291248899, 'totalEpisodes': 206, 'stepsPerEpisode': 154, 'rewardPerEpisode': 115.05826326341035
'totalSteps': 8960, 'rewardStep': 0.747386838789593, 'errorList': [], 'lossList': [0.0, -1.3461204206943511, 0.0, 31.233454263210298, 0.0, 0.0, 0.0], 'rewardMean': 0.7540376590769904, 'totalEpisodes': 212, 'stepsPerEpisode': 118, 'rewardPerEpisode': 95.5773658223188
'totalSteps': 10240, 'rewardStep': 0.8188165779674849, 'errorList': [], 'lossList': [0.0, -1.3318072032928467, 0.0, 19.322695503234865, 0.0, 0.0, 0.0], 'rewardMean': 0.7621350239383022, 'totalEpisodes': 219, 'stepsPerEpisode': 54, 'rewardPerEpisode': 47.34343693977077
'totalSteps': 11520, 'rewardStep': 0.7757419852454568, 'errorList': [], 'lossList': [0.0, -1.3176316386461258, 0.0, 12.112683769464493, 0.0, 0.0, 0.0], 'rewardMean': 0.7636469085279861, 'totalEpisodes': 223, 'stepsPerEpisode': 42, 'rewardPerEpisode': 32.76154120299856
'totalSteps': 12800, 'rewardStep': 0.8668301567006668, 'errorList': [], 'lossList': [0.0, -1.3065932470560073, 0.0, 11.662149887084961, 0.0, 0.0, 0.0], 'rewardMean': 0.7739652333452541, 'totalEpisodes': 230, 'stepsPerEpisode': 182, 'rewardPerEpisode': 155.40546803368358
'totalSteps': 14080, 'rewardStep': 0.9461431678230301, 'errorList': [17.297075295988794, 0.5531474346560942, 50.539226444674526, 1.286041352224028, 69.83274398377905, 18.278704121698134, 21.15498952022716, 14.013752945295199, 23.093689930110664, 36.12573981978484, 19.982525897894106, 3.268516684531131, 16.06043838110455, 10.155582719244943, 70.90622196832349, 21.37636201524334, 14.255265755924517, 12.122039137095364, 17.734104496511886, 8.582039937250743, 7.407881156798314, 1.8262865184099744, 16.1595055656186, 6.147650602114254, 5.0138593761722845, 21.91321854394828, 58.341658018389545, 28.74055149900548, 12.118095951225797, 0.9128260808726045, 25.509539226912842, 41.84463258603329, 15.476816143270986, 3.378848179335404, 3.678055355055862, 35.828726788939946, 48.73744572086013, 11.03277922084694, 17.857117851679128, 16.20527357829848, 20.379412827074674, 44.77455607131627, 14.903366985944974, 14.89177402295331, 10.193515548986387, 3.8014417234300533, 30.82188826152995, 8.474081469308246, 15.479110526105313, 24.527035861904473], 'lossList': [0.0, -1.2931146401166915, 0.0, 7.286467469930649, 0.0, 0.0, 0.0], 'rewardMean': 0.8127147782799217, 'totalEpisodes': 235, 'stepsPerEpisode': 99, 'rewardPerEpisode': 88.22278349697773, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.39613414878012093, 'errorList': [], 'lossList': [0.0, -1.2742153376340866, 0.0, 3.8458965310454367, 0.0, 0.0, 0.0], 'rewardMean': 0.7760672160642776, 'totalEpisodes': 239, 'stepsPerEpisode': 217, 'rewardPerEpisode': 161.94307337846993
'totalSteps': 16640, 'rewardStep': 0.4834673508850288, 'errorList': [], 'lossList': [0.0, -1.2865026712417602, 0.0, 4.312362035512924, 0.0, 0.0, 0.0], 'rewardMean': 0.7478053041391524, 'totalEpisodes': 243, 'stepsPerEpisode': 220, 'rewardPerEpisode': 167.48176174298672
'totalSteps': 17920, 'rewardStep': 0.766794611982764, 'errorList': [], 'lossList': [0.0, -1.2974731385707856, 0.0, 3.1515248239040377, 0.0, 0.0, 0.0], 'rewardMean': 0.7348148842241897, 'totalEpisodes': 247, 'stepsPerEpisode': 104, 'rewardPerEpisode': 89.45278494133056
'totalSteps': 19200, 'rewardStep': 0.7493419658138485, 'errorList': [], 'lossList': [0.0, -1.3006355053186416, 0.0, 4.329519987106323, 0.0, 0.0, 0.0], 'rewardMean': 0.7297670925614544, 'totalEpisodes': 249, 'stepsPerEpisode': 606, 'rewardPerEpisode': 435.0669006695094
'totalSteps': 20480, 'rewardStep': 0.7477214321257332, 'errorList': [], 'lossList': [0.0, -1.2852711546421052, 0.0, 3.807477509379387, 0.0, 0.0, 0.0], 'rewardMean': 0.7298378236113726, 'totalEpisodes': 253, 'stepsPerEpisode': 60, 'rewardPerEpisode': 52.957464978322335
'totalSteps': 21760, 'rewardStep': 0.7954557326084573, 'errorList': [], 'lossList': [0.0, -1.2668928170204163, 0.0, 4.280145224332809, 0.0, 0.0, 0.0], 'rewardMean': 0.7346447129932591, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 910.0479043679615
'totalSteps': 23040, 'rewardStep': 0.8800890980839032, 'errorList': [], 'lossList': [0.0, -1.2190030741691589, 0.0, 0.8687323396652937, 0.0, 0.0, 0.0], 'rewardMean': 0.740771965004901, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1120.860158321439
'totalSteps': 24320, 'rewardStep': 0.8145165283862965, 'errorList': [], 'lossList': [0.0, -1.165717002749443, 0.0, 0.6304905374348163, 0.0, 0.0, 0.0], 'rewardMean': 0.744649419318985, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.9069089325806
'totalSteps': 25600, 'rewardStep': 0.9763391325042703, 'errorList': [0.0422695127083472, 0.023263523585306388, 0.04007240695418408, 0.03967484118748912, 0.02952382096643181, 0.01980578468063333, 0.03381876483840589, 0.023326127535021918, 0.01018675382103956, 0.029717434691528346, 0.017703337551530956, 0.01834516977024903, 0.02605838148624875, 0.01699314687213295, 0.0378210040367493, 0.012703641698052893, 0.004851281968746578, 0.04077291023120089, 0.026056463659332823, 0.0195248082324807, 0.029352650273766795, 0.005015728148585882, 0.004630251775660721, 0.053893963808605345, 0.02673761025667538, 0.014725961892985561, 0.031659392633545454, 0.0251710471676916, 0.01136572012068134, 0.04812677240038534, 0.004843369625690849, 0.015772773552349, 0.04314770245933843, 0.04124396679735639, 0.004182468354881791, 0.03430848558400734, 0.02512780493854298, 0.03399474238847568, 0.0105877974253873, 0.03427159400162906, 0.02927948877787323, 0.020203627276418723, 0.015675793330590496, 0.023525281501964788, 0.03963038239986457, 0.023389710148016073, 0.009243639001447788, 0.035104033245702125, 0.021068900393983944, 0.028334693969963017], 'lossList': [0.0, -1.100828664302826, 0.0, 0.6397877505421639, 0.0, 0.0, 0.0], 'rewardMean': 0.7556003168993453, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1196.8910832540844, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=91.1
