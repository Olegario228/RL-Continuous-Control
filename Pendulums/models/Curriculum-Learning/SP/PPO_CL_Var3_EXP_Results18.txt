#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 18
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.763158208305067, 'errorList': [], 'lossList': [0.0, -1.4077130162715912, 0.0, 29.368029518127443, 0.0, 0.0, 0.0], 'rewardMean': 0.8106851584303033, 'totalEpisodes': 68, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.5629988446217282
'totalSteps': 3840, 'rewardStep': 0.6778869341364735, 'errorList': [], 'lossList': [0.0, -1.398750559091568, 0.0, 35.997126722335814, 0.0, 0.0, 0.0], 'rewardMean': 0.7664190836656933, 'totalEpisodes': 130, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.260922380225153
'totalSteps': 5120, 'rewardStep': 0.7337602898471925, 'errorList': [], 'lossList': [0.0, -1.3831986087560653, 0.0, 42.734806079864505, 0.0, 0.0, 0.0], 'rewardMean': 0.7582543852110681, 'totalEpisodes': 175, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.243072586398416
'totalSteps': 6400, 'rewardStep': 0.5452941581198689, 'errorList': [], 'lossList': [0.0, -1.3661753249168396, 0.0, 47.69353199005127, 0.0, 0.0, 0.0], 'rewardMean': 0.7156623397928283, 'totalEpisodes': 198, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.404348193576556
'totalSteps': 7680, 'rewardStep': 0.7367513658566296, 'errorList': [], 'lossList': [0.0, -1.3458330976963042, 0.0, 45.7970583152771, 0.0, 0.0, 0.0], 'rewardMean': 0.7191771774701285, 'totalEpisodes': 212, 'stepsPerEpisode': 81, 'rewardPerEpisode': 56.607832495966974
'totalSteps': 8960, 'rewardStep': 0.9219721442010884, 'errorList': [], 'lossList': [0.0, -1.3294615936279297, 0.0, 29.73369767189026, 0.0, 0.0, 0.0], 'rewardMean': 0.7481478870031228, 'totalEpisodes': 219, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.45841415612777
'totalSteps': 10240, 'rewardStep': 0.754319311855481, 'errorList': [], 'lossList': [0.0, -1.3224574136734009, 0.0, 28.90921392917633, 0.0, 0.0, 0.0], 'rewardMean': 0.7489193151096676, 'totalEpisodes': 225, 'stepsPerEpisode': 137, 'rewardPerEpisode': 106.0587001453843
'totalSteps': 11520, 'rewardStep': 0.46738678962645946, 'errorList': [], 'lossList': [0.0, -1.3236652433872222, 0.0, 12.213671705126762, 0.0, 0.0, 0.0], 'rewardMean': 0.7176379233893111, 'totalEpisodes': 228, 'stepsPerEpisode': 380, 'rewardPerEpisode': 295.38251199424104
'totalSteps': 12800, 'rewardStep': 0.8551224314389647, 'errorList': [], 'lossList': [0.0, -1.3291921043395996, 0.0, 10.417279226779938, 0.0, 0.0, 0.0], 'rewardMean': 0.7313863741942764, 'totalEpisodes': 232, 'stepsPerEpisode': 151, 'rewardPerEpisode': 134.65253948371225
'totalSteps': 14080, 'rewardStep': 0.8083328906867818, 'errorList': [], 'lossList': [0.0, -1.301622930765152, 0.0, 5.804860299229622, 0.0, 0.0, 0.0], 'rewardMean': 0.7263984524074008, 'totalEpisodes': 234, 'stepsPerEpisode': 316, 'rewardPerEpisode': 245.13378714639165
'totalSteps': 15360, 'rewardStep': 0.6704574854242911, 'errorList': [], 'lossList': [0.0, -1.288520827293396, 0.0, 4.9393657809495926, 0.0, 0.0, 0.0], 'rewardMean': 0.7171283801193231, 'totalEpisodes': 236, 'stepsPerEpisode': 409, 'rewardPerEpisode': 340.3193194087087
'totalSteps': 16640, 'rewardStep': 0.7827408369871096, 'errorList': [], 'lossList': [0.0, -1.27379487991333, 0.0, 2.8330656459927557, 0.0, 0.0, 0.0], 'rewardMean': 0.7276137704043866, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.3338375610551
'totalSteps': 17920, 'rewardStep': 0.8107650187857609, 'errorList': [], 'lossList': [0.0, -1.2258082985877992, 0.0, 1.4029985792189836, 0.0, 0.0, 0.0], 'rewardMean': 0.7353142432982436, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.7934891240354
'totalSteps': 19200, 'rewardStep': 0.9269128946614059, 'errorList': [], 'lossList': [0.0, -1.1828050124645233, 0.0, 1.2681518971174954, 0.0, 0.0, 0.0], 'rewardMean': 0.7734761169523973, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.1203814394696
'totalSteps': 20480, 'rewardStep': 0.9163749059370335, 'errorList': [], 'lossList': [0.0, -1.143098447918892, 0.0, 1.2973195216432214, 0.0, 0.0, 0.0], 'rewardMean': 0.7914384709604376, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1157.1124754145712
'totalSteps': 21760, 'rewardStep': 0.9235368242101932, 'errorList': [], 'lossList': [0.0, -1.1078605782985687, 0.0, 0.8144160041585564, 0.0, 0.0, 0.0], 'rewardMean': 0.791594938961348, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.94850588039
'totalSteps': 23040, 'rewardStep': 0.9006903181401741, 'errorList': [], 'lossList': [0.0, -1.0792802256345748, 0.0, 0.8673766081873328, 0.0, 0.0, 0.0], 'rewardMean': 0.8062320395898175, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1209.9446746991725
'totalSteps': 24320, 'rewardStep': 0.9121744897378968, 'errorList': [], 'lossList': [0.0, -1.0433651822805405, 0.0, 0.45357053112238643, 0.0, 0.0, 0.0], 'rewardMean': 0.8507108096009611, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.0148452436679
'totalSteps': 25600, 'rewardStep': 0.9565153747002437, 'errorList': [0.18795936599551535, 0.09285824041833318, 0.074941703423043, 0.12867779721973727, 0.17952357456403695, 0.09026965815357996, 0.16427881820262313, 0.08034813166052071, 0.15477851745127688, 0.13960535375599645, 0.21249918371508983, 0.08843118540410719, 0.09244996639472275, 0.21349528855937686, 0.12074023158096843, 0.11671412665039135, 0.1723218084549899, 0.15297681229865936, 0.11713074729622738, 0.20475102074984255, 0.12531305738927975, 0.22658429020843512, 0.09017185605315972, 0.10840247485707676, 0.1417529302538875, 0.0904306246719516, 0.08753435231141278, 0.11251779932810468, 0.09815644629016815, 0.11957023492088839, 0.11481039844089003, 0.08921747334512559, 0.17062657140565823, 0.1543090683549877, 0.1663143774280708, 0.10695715658307987, 0.19234524055218744, 0.14694912368261948, 0.08446376897433318, 0.11634192738206608, 0.12247880406347282, 0.12642151830030168, 0.16645166774482753, 0.18364001931987478, 0.1284600962276442, 0.1316960970539708, 0.1173525991862266, 0.20388512162341035, 0.21849690309676253, 0.1724806554754494], 'lossList': [0.0, -1.02140176653862, 0.0, 0.3331463711429387, 0.0, 0.0, 0.0], 'rewardMean': 0.860850103927089, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1203.7311476215855, 'successfulTests': 44
#maxSuccessfulTests=44, maxSuccessfulTestsAtStep=25600, timeSpent=72.15
