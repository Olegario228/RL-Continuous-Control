#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 19
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.7132692531718268, 'errorList': [], 'lossList': [0.0, -1.4238828146457672, 0.0, 31.3748783493042, 0.0, 0.0, 0.0], 'rewardMean': 0.7183100885255638, 'totalEpisodes': 66, 'stepsPerEpisode': 20, 'rewardPerEpisode': 17.368371935556823
'totalSteps': 3840, 'rewardStep': 0.5011799188957264, 'errorList': [], 'lossList': [0.0, -1.4080542677640915, 0.0, 35.764306421279905, 0.0, 0.0, 0.0], 'rewardMean': 0.645933365315618, 'totalEpisodes': 143, 'stepsPerEpisode': 2, 'rewardPerEpisode': 0.9907168205274348
'totalSteps': 5120, 'rewardStep': 0.6124878400311674, 'errorList': [], 'lossList': [0.0, -1.393862814307213, 0.0, 42.49551040649414, 0.0, 0.0, 0.0], 'rewardMean': 0.6375719839945053, 'totalEpisodes': 190, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.320889685664866
'totalSteps': 6400, 'rewardStep': 0.8422110099290339, 'errorList': [], 'lossList': [0.0, -1.3901020729541778, 0.0, 40.901223497390745, 0.0, 0.0, 0.0], 'rewardMean': 0.6784997891814111, 'totalEpisodes': 207, 'stepsPerEpisode': 35, 'rewardPerEpisode': 27.274797937197985
'totalSteps': 7680, 'rewardStep': 0.6293572853599181, 'errorList': [], 'lossList': [0.0, -1.374944927096367, 0.0, 39.19081476688385, 0.0, 0.0, 0.0], 'rewardMean': 0.6703093718778289, 'totalEpisodes': 217, 'stepsPerEpisode': 31, 'rewardPerEpisode': 20.40574752953481
'totalSteps': 8960, 'rewardStep': 0.5335221590952041, 'errorList': [], 'lossList': [0.0, -1.3585086005926132, 0.0, 33.11062963962555, 0.0, 0.0, 0.0], 'rewardMean': 0.650768341480311, 'totalEpisodes': 227, 'stepsPerEpisode': 112, 'rewardPerEpisode': 83.79137035537698
'totalSteps': 10240, 'rewardStep': 0.8765703211231686, 'errorList': [], 'lossList': [0.0, -1.3552323937416078, 0.0, 11.119265283346175, 0.0, 0.0, 0.0], 'rewardMean': 0.6789935889356682, 'totalEpisodes': 232, 'stepsPerEpisode': 44, 'rewardPerEpisode': 34.97381266162378
'totalSteps': 11520, 'rewardStep': 0.9050798694107041, 'errorList': [], 'lossList': [0.0, -1.346603569984436, 0.0, 8.651572343707084, 0.0, 0.0, 0.0], 'rewardMean': 0.7041142867662278, 'totalEpisodes': 235, 'stepsPerEpisode': 78, 'rewardPerEpisode': 69.83165628378879
'totalSteps': 12800, 'rewardStep': 0.8852122619046423, 'errorList': [], 'lossList': [0.0, -1.3454690223932266, 0.0, 39.14490035533905, 0.0, 0.0, 0.0], 'rewardMean': 0.7222240842800691, 'totalEpisodes': 238, 'stepsPerEpisode': 63, 'rewardPerEpisode': 56.86901160803819
'totalSteps': 14080, 'rewardStep': 0.6152446274344541, 'errorList': [], 'lossList': [0.0, -1.3398588824272155, 0.0, 16.342007904052736, 0.0, 0.0, 0.0], 'rewardMean': 0.7114134546355846, 'totalEpisodes': 239, 'stepsPerEpisode': 504, 'rewardPerEpisode': 385.6888002648736
'totalSteps': 15360, 'rewardStep': 0.7305048649878063, 'errorList': [], 'lossList': [0.0, -1.3432203942537309, 0.0, 7.6440426075458525, 0.0, 0.0, 0.0], 'rewardMean': 0.7131370158171826, 'totalEpisodes': 240, 'stepsPerEpisode': 784, 'rewardPerEpisode': 622.3282697255949
'totalSteps': 16640, 'rewardStep': 0.6598593794616222, 'errorList': [], 'lossList': [0.0, -1.3359554433822631, 0.0, 7.362573238015175, 0.0, 0.0, 0.0], 'rewardMean': 0.7290049618737722, 'totalEpisodes': 242, 'stepsPerEpisode': 138, 'rewardPerEpisode': 118.10792307744138
'totalSteps': 17920, 'rewardStep': 0.7054468391053359, 'errorList': [], 'lossList': [0.0, -1.3123477905988694, 0.0, 3.2676075434684755, 0.0, 0.0, 0.0], 'rewardMean': 0.738300861781189, 'totalEpisodes': 243, 'stepsPerEpisode': 529, 'rewardPerEpisode': 445.2799069330234
'totalSteps': 19200, 'rewardStep': 0.8676812723168305, 'errorList': [], 'lossList': [0.0, -1.2996188127994537, 0.0, 3.864552357196808, 0.0, 0.0, 0.0], 'rewardMean': 0.7408478880199686, 'totalEpisodes': 245, 'stepsPerEpisode': 89, 'rewardPerEpisode': 83.07193970980936
'totalSteps': 20480, 'rewardStep': 0.9347307656955632, 'errorList': [0.35115384861797067, 0.09971261474981055, 0.3969952471129246, 0.18293549487790373, 0.3393526690279055, 0.056367461805418, 0.2982943532351649, 0.12971976543246808, 0.23069736238828376, 0.11282865210491544, 0.22539456111404357, 0.3170016993095155, 0.10396172634979584, 0.10498411802396505, 0.12225097205719562, 0.09587806375835539, 0.09129826546812186, 0.1472130747222612, 0.10199609984941939, 0.23975678120386898, 0.08537420345910314, 0.11687238179778515, 0.12200043814915727, 0.42001662648002563, 0.04687555738252109, 0.10707611878662086, 0.10786951743054296, 0.1180415922134534, 0.11035178717750292, 0.18324509663312571, 0.11009303807139005, 0.1180717822289053, 0.2240439713715903, 0.15730499384548735, 0.21436874291267746, 0.09563682384575015, 0.2706189044717288, 0.29615423692039544, 0.07410464183493867, 0.09604591360612566, 0.11332234562475939, 0.2965652583226433, 0.23876089281485063, 0.11927412199984092, 0.08770832183875038, 0.10829347141928099, 0.10268537468996909, 0.06571208477443694, 0.08636747311892587, 0.19048127858314184], 'lossList': [0.0, -1.2925971353054047, 0.0, 1.5114786287397146, 0.0, 0.0, 0.0], 'rewardMean': 0.7713852360535332, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.4528382117558, 'successfulTests': 35
'totalSteps': 21760, 'rewardStep': 0.9009647433061773, 'errorList': [], 'lossList': [0.0, -1.290679686665535, 0.0, 0.8988928677141667, 0.0, 0.0, 0.0], 'rewardMean': 0.8081294944746304, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.6900246945506
'totalSteps': 23040, 'rewardStep': 0.8974820544629404, 'errorList': [], 'lossList': [0.0, -1.299538260102272, 0.0, 0.891654260829091, 0.0, 0.0, 0.0], 'rewardMean': 0.8102206678086077, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.9408670974615
'totalSteps': 24320, 'rewardStep': 0.9057705045482933, 'errorList': [], 'lossList': [0.0, -1.274924460053444, 0.0, 0.5182508259266615, 0.0, 0.0, 0.0], 'rewardMean': 0.8102897313223665, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.9010635633012
'totalSteps': 25600, 'rewardStep': 0.9846400728556229, 'errorList': [0.05575980184490574, 0.1025503099960721, 0.05412207987765597, 0.055726905812491005, 0.039950899412645305, 0.08195886106669238, 0.058882137176737355, 0.03797726040714205, 0.08586461069302687, 0.05486555162389467, 0.04004970755618379, 0.03832370167765717, 0.09213166606642179, 0.09871298636340312, 0.06534672046339039, 0.055905453821689696, 0.07200764657273849, 0.0726747577744332, 0.13222076618281298, 0.06905671087890447, 0.1317706900368721, 0.04253866363936476, 0.036088731807448905, 0.05296550381383349, 0.053603287310399914, 0.039040692414574925, 0.10020579330264749, 0.07039043913784303, 0.0438412772340506, 0.05172845075152696, 0.04016531496584494, 0.10385862761436898, 0.04243145345690606, 0.04565624550774877, 0.09007879046154228, 0.14852144385007293, 0.034139402187183844, 0.033383161945243774, 0.04359011643960203, 0.04250048827836644, 0.10804618523141402, 0.05642828252088993, 0.057798781741861004, 0.0515350869304308, 0.04719917521582454, 0.029393044953038933, 0.026262051613310227, 0.034979110821097066, 0.04305234626256522, 0.044687157369562205], 'lossList': [0.0, -1.240003415942192, 0.0, 0.44842111978679894, 0.0, 0.0, 0.0], 'rewardMean': 0.8202325124174645, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.0188504094654, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=92.93
