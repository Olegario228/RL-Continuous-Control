#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.7222292081069216, 'errorList': [], 'lossList': [0.0, -1.428145228624344, 0.0, 32.12753492355347, 0.0, 0.0, 0.0], 'rewardMean': 0.7689332749453155, 'totalEpisodes': 110, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.7827483024652016
'totalSteps': 3840, 'rewardStep': 0.8529912156883122, 'errorList': [], 'lossList': [0.0, -1.415665099620819, 0.0, 31.85055274963379, 0.0, 0.0, 0.0], 'rewardMean': 0.7969525885263143, 'totalEpisodes': 174, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.529725855730397
'totalSteps': 5120, 'rewardStep': 0.6812459553894965, 'errorList': [], 'lossList': [0.0, -1.3945237702131272, 0.0, 39.40666261672973, 0.0, 0.0, 0.0], 'rewardMean': 0.7680259302421099, 'totalEpisodes': 206, 'stepsPerEpisode': 27, 'rewardPerEpisode': 21.740475907881407
'totalSteps': 6400, 'rewardStep': 0.6404873785753099, 'errorList': [], 'lossList': [0.0, -1.378033093214035, 0.0, 40.46344096183777, 0.0, 0.0, 0.0], 'rewardMean': 0.7425182199087499, 'totalEpisodes': 221, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.931089971343113
'totalSteps': 7680, 'rewardStep': 0.9764975407624973, 'errorList': [11.633967783043017, 3.620779513575729, 11.192585774032594, 2.1788808065924705, 13.068365615123298, 2.1850757467009734, 4.425544155989497, 6.247741868156272, 5.123793977293382, 2.1044293603168205, 5.31356290257172, 19.379916060130167, 6.488266922313689, 2.8836529705201213, 5.339132082109895, 9.128567373040184, 14.154700790776717, 16.39739660351832, 1.725542835412071, 7.5393474562830045, 13.087919157319723, 8.086288618785407, 1.589519522908289, 1.2484346119772838, 16.150468452666008, 2.4994135268252484, 2.0820159859713505, 0.9037853632863685, 6.491605770695925, 2.5523691721127393, 0.5271483870380164, 4.375761630707186, 7.041299449620825, 13.287614630452376, 2.892445712565553, 20.3869639458421, 8.31132891472831, 10.756916428755787, 5.510312521547405, 8.599411770648176, 3.5837941997896516, 13.228942978273292, 0.8817571302041558, 14.177325766015626, 3.1393580407207153, 5.67557862152307, 1.01045934713693, 2.850552162048866, 14.8572305243444, 3.2390431452813453], 'lossList': [0.0, -1.3583534890413285, 0.0, 49.17630042076111, 0.0, 0.0, 0.0], 'rewardMean': 0.7815147733843745, 'totalEpisodes': 232, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.577103857896333, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.5191564890361431, 'errorList': [], 'lossList': [0.0, -1.3478222167491913, 0.0, 28.209370021820067, 0.0, 0.0, 0.0], 'rewardMean': 0.7440350184774843, 'totalEpisodes': 235, 'stepsPerEpisode': 134, 'rewardPerEpisode': 100.58740646284033
'totalSteps': 10240, 'rewardStep': 0.519588591776178, 'errorList': [], 'lossList': [0.0, -1.3454608684778213, 0.0, 23.44678183555603, 0.0, 0.0, 0.0], 'rewardMean': 0.715979215139821, 'totalEpisodes': 238, 'stepsPerEpisode': 247, 'rewardPerEpisode': 203.2959175611457
'totalSteps': 11520, 'rewardStep': 0.6504575711742966, 'errorList': [], 'lossList': [0.0, -1.352383673787117, 0.0, 10.682152470350266, 0.0, 0.0, 0.0], 'rewardMean': 0.708699032476985, 'totalEpisodes': 242, 'stepsPerEpisode': 300, 'rewardPerEpisode': 220.8133278345546
'totalSteps': 12800, 'rewardStep': 0.6963076992756527, 'errorList': [], 'lossList': [0.0, -1.338541379570961, 0.0, 24.780144691467285, 0.0, 0.0, 0.0], 'rewardMean': 0.7074598991568517, 'totalEpisodes': 247, 'stepsPerEpisode': 131, 'rewardPerEpisode': 99.66661339642671
'totalSteps': 14080, 'rewardStep': 0.7908214181859614, 'errorList': [], 'lossList': [0.0, -1.3302429383993148, 0.0, 5.837531642317772, 0.0, 0.0, 0.0], 'rewardMean': 0.704978306797077, 'totalEpisodes': 250, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.48031108099803
'totalSteps': 15360, 'rewardStep': 0.6961364361228206, 'errorList': [], 'lossList': [0.0, -1.329489752650261, 0.0, 4.918277667760849, 0.0, 0.0, 0.0], 'rewardMean': 0.7023690295986669, 'totalEpisodes': 252, 'stepsPerEpisode': 1009, 'rewardPerEpisode': 769.6256066186622
'totalSteps': 16640, 'rewardStep': 0.6677273580798306, 'errorList': [], 'lossList': [0.0, -1.3310192054510117, 0.0, 6.1090709915757175, 0.0, 0.0, 0.0], 'rewardMean': 0.6838426438378187, 'totalEpisodes': 253, 'stepsPerEpisode': 495, 'rewardPerEpisode': 377.77829291379334
'totalSteps': 17920, 'rewardStep': 0.8122117232463422, 'errorList': [], 'lossList': [0.0, -1.3278466665744781, 0.0, 3.1230648082494734, 0.0, 0.0, 0.0], 'rewardMean': 0.6969392206235032, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.7953820763234
'totalSteps': 19200, 'rewardStep': 0.9670838984185909, 'errorList': [0.21012427747903198, 0.1903217209820708, 0.21672427507093467, 0.20525546111341456, 0.18079813079608076, 0.170223095783406, 0.1969034556640438, 0.1706678554522364, 0.22849493218889996, 0.16572903688013352, 0.1916015344101611, 0.1668091796472681, 0.16101714651970622, 0.16446445177195732, 0.16677049999896418, 0.16349851472702503, 0.21940441595916038, 0.23901979776250382, 0.19547793663517712, 0.13488373194457107, 0.16276273590434198, 0.19913291611168973, 0.21910950730296608, 0.1934344844422742, 0.1865056610966163, 0.1496999611027291, 0.14154654541776113, 0.1693750980347831, 0.15898723406838317, 0.19772667522178006, 0.13058367235836804, 0.19212296701756568, 0.14874491986534166, 0.18155480474625615, 0.2273006383977521, 0.18094781799443338, 0.12481378710092521, 0.21885343107477306, 0.13071386574710409, 0.1897713460472792, 0.18775321417273755, 0.2012715779512285, 0.2087823079256406, 0.19647159030873032, 0.1531751202210237, 0.18002383701947397, 0.17273814215459998, 0.1588417645088883, 0.1498210677154132, 0.20878085531374888], 'lossList': [0.0, -1.3009610909223557, 0.0, 1.9458674457669258, 0.0, 0.0, 0.0], 'rewardMean': 0.7295988726078314, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.4400706100437, 'successfulTests': 38
'totalSteps': 20480, 'rewardStep': 0.9649224487093991, 'errorList': [0.26243030684050844, 0.1928632619098751, 0.17886285809713962, 0.1477709779687892, 0.1560672938095785, 0.16885390730635422, 0.17725493542459145, 0.14288738865362577, 0.169217413243625, 0.17697040998346827, 0.14344000555144285, 0.1772466770600777, 0.21348423074326436, 0.16575781644744197, 0.21341200813309114, 0.1887313063128697, 0.17355269233215806, 0.22300832532578985, 0.19628088468296512, 0.2096643293506404, 0.19260044933871173, 0.1929386437719117, 0.1922109172939431, 0.20882227619503244, 0.19769028128100055, 0.16264857894607124, 0.2055479593619189, 0.1568634949430531, 0.24590774434880994, 0.254916575917172, 0.15887523507395584, 0.12462929581887519, 0.1981447323650779, 0.13261035196767376, 0.1572450712907084, 0.21428718505568065, 0.1376581509450807, 0.18819741658319153, 0.14398506322279261, 0.2057568293355811, 0.18051590370495119, 0.20076839783012054, 0.15546512614296235, 0.1855190094730317, 0.23622216095750542, 0.17382986849271007, 0.16352651510855135, 0.16319523833236024, 0.17653393173451568, 0.20076740601444815], 'lossList': [0.0, -1.2696784579753875, 0.0, 1.384452757872641, 0.0, 0.0, 0.0], 'rewardMean': 0.7284413634025215, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1140.7081075195183, 'successfulTests': 36
'totalSteps': 21760, 'rewardStep': 0.8845632737923262, 'errorList': [], 'lossList': [0.0, -1.2217069232463837, 0.0, 0.8706091103330255, 0.0, 0.0, 0.0], 'rewardMean': 0.7649820418781399, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.5477209788125
'totalSteps': 23040, 'rewardStep': 0.9818550017168736, 'errorList': [0.0555300666136318, 0.13297055621423862, 0.06266250688088232, 0.07999739619347601, 0.19200303192371246, 0.09781284250310686, 0.12582256048322535, 0.14982373619356149, 0.0648844750395003, 0.06673866614823715, 0.16130711329943173, 0.1484645679935482, 0.13930916114208544, 0.20853980837912825, 0.14199087854159503, 0.1048843444222972, 0.19820983027947542, 0.21746828934648352, 0.11404004640629217, 0.17613879322319143, 0.16629773582191243, 0.09976727743789411, 0.25714112256292254, 0.05402614525683304, 0.12209349820628744, 0.128908899693702, 0.1867401151024281, 0.0756994233931043, 0.14288425026529303, 0.19063775369007435, 0.08062652321941581, 0.12290211598004017, 0.15205814141762092, 0.13188478221724087, 0.0870341905379359, 0.08490496689283235, 0.06848614350174685, 0.08377733953737167, 0.1926503544750991, 0.14099537616905078, 0.07729393835157967, 0.2516406148817432, 0.13127713914474662, 0.17898753448960752, 0.10814044289231385, 0.09309359625431184, 0.08458402583187041, 0.26324675058946795, 0.07030530460462303, 0.1508517708108553], 'lossList': [0.0, -1.1784290385246277, 0.0, 1.0855784070491792, 0.0, 0.0, 0.0], 'rewardMean': 0.8112086828722095, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.9641795705427, 'successfulTests': 45
'totalSteps': 24320, 'rewardStep': 0.9102444669271695, 'errorList': [], 'lossList': [0.0, -1.1507679671049118, 0.0, 0.6616243634559215, 0.0, 0.0, 0.0], 'rewardMean': 0.8371873724474967, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1206.9404148676529
'totalSteps': 25600, 'rewardStep': 0.9029910477119252, 'errorList': [], 'lossList': [0.0, -1.1310737884044648, 0.0, 0.408022689409554, 0.0, 0.0, 0.0], 'rewardMean': 0.8578557072911238, 'totalEpisodes': 253, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.136531251066
#maxSuccessfulTests=45, maxSuccessfulTestsAtStep=23040, timeSpent=126.35
