#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 20
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.7096741251873848, 'errorList': [], 'lossList': [0.0, -1.4417524123191834, 0.0, 27.890116758346558, 0.0, 0.0, 0.0], 'rewardMean': 0.8122473778841064, 'totalEpisodes': 59, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7096741251873848
'totalSteps': 3840, 'rewardStep': 0.6441853603700604, 'errorList': [], 'lossList': [0.0, -1.4319504278898239, 0.0, 30.18021780014038, 0.0, 0.0, 0.0], 'rewardMean': 0.7562267053794244, 'totalEpisodes': 137, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.7777412163525064
'totalSteps': 5120, 'rewardStep': 0.9478804092334786, 'errorList': [118.61777063542854, 71.62935905437156, 164.84292324229733, 92.18288719608779, 111.9230022130038, 39.95122123284638, 147.69230108729056, 172.20688369930448, 139.4706800003518, 220.15248370739712, 163.52144891032987, 184.96289684001408, 115.35979596797789, 100.94444929810793, 172.53657816577527, 190.132782389815, 197.45746453238718, 113.09242313009852, 112.79035094538587, 221.44785077531182, 195.60021596455604, 142.1513674035249, 71.77328883082356, 83.79946882041186, 219.8093161380869, 129.94654721210637, 230.5367999105958, 141.66760035525417, 208.78227053252368, 185.27575245631363, 199.61269650496774, 110.45404672255074, 217.78057511742676, 139.6602258025434, 186.1217348466065, 131.61037028273896, 111.05233338340338, 213.85021371392992, 123.85975091591148, 236.40832099625322, 232.32027418377726, 155.3824521097428, 178.0706104619421, 177.88794119120027, 59.45892668476692, 236.67621950583117, 82.37043121786922, 208.55528347867252, 128.4343665977105, 151.08596540164064], 'lossList': [0.0, -1.4054996192455291, 0.0, 32.072734394073485, 0.0, 0.0, 0.0], 'rewardMean': 0.8041401313429379, 'totalEpisodes': 191, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.459753977777376, 'successfulTests': 0
'totalSteps': 6400, 'rewardStep': 0.46620341254493103, 'errorList': [], 'lossList': [0.0, -1.3833501297235489, 0.0, 37.98736759185791, 0.0, 0.0, 0.0], 'rewardMean': 0.7365527875833365, 'totalEpisodes': 207, 'stepsPerEpisode': 129, 'rewardPerEpisode': 89.94841578058501
'totalSteps': 7680, 'rewardStep': 0.8346154422482552, 'errorList': [], 'lossList': [0.0, -1.3668154448270797, 0.0, 37.3546648311615, 0.0, 0.0, 0.0], 'rewardMean': 0.752896563360823, 'totalEpisodes': 220, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.836402406765195
'totalSteps': 8960, 'rewardStep': 0.7699041404881894, 'errorList': [], 'lossList': [0.0, -1.363255558013916, 0.0, 26.307197728157043, 0.0, 0.0, 0.0], 'rewardMean': 0.755326217236161, 'totalEpisodes': 229, 'stepsPerEpisode': 142, 'rewardPerEpisode': 114.26537090336267
'totalSteps': 10240, 'rewardStep': 0.6617032046663649, 'errorList': [], 'lossList': [0.0, -1.3666099560260774, 0.0, 23.475474028587342, 0.0, 0.0, 0.0], 'rewardMean': 0.7436233406649365, 'totalEpisodes': 235, 'stepsPerEpisode': 130, 'rewardPerEpisode': 101.69956454550623
'totalSteps': 11520, 'rewardStep': 0.28400138081196474, 'errorList': [], 'lossList': [0.0, -1.3583279252052307, 0.0, 21.06858894824982, 0.0, 0.0, 0.0], 'rewardMean': 0.6925542340146064, 'totalEpisodes': 239, 'stepsPerEpisode': 335, 'rewardPerEpisode': 231.74812010908119
'totalSteps': 12800, 'rewardStep': 0.7401729153818534, 'errorList': [], 'lossList': [0.0, -1.3566430562734604, 0.0, 7.015273996591568, 0.0, 0.0, 0.0], 'rewardMean': 0.697316102151331, 'totalEpisodes': 242, 'stepsPerEpisode': 350, 'rewardPerEpisode': 289.73202782295954
'totalSteps': 14080, 'rewardStep': 0.7624003678274123, 'errorList': [], 'lossList': [0.0, -1.364473260641098, 0.0, 6.421030938029289, 0.0, 0.0, 0.0], 'rewardMean': 0.6820740758759894, 'totalEpisodes': 244, 'stepsPerEpisode': 378, 'rewardPerEpisode': 329.1339931195255
'totalSteps': 15360, 'rewardStep': 0.866956434186989, 'errorList': [], 'lossList': [0.0, -1.3574324238300324, 0.0, 5.440477518439293, 0.0, 0.0, 0.0], 'rewardMean': 0.6978023067759499, 'totalEpisodes': 246, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.487172448509812
'totalSteps': 16640, 'rewardStep': 0.7507259594010955, 'errorList': [], 'lossList': [0.0, -1.3572111523151398, 0.0, 3.3540597210824488, 0.0, 0.0, 0.0], 'rewardMean': 0.7084563666790534, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.203628777994
'totalSteps': 17920, 'rewardStep': 0.8432397698170234, 'errorList': [], 'lossList': [0.0, -1.3495488703250884, 0.0, 1.6002656430006028, 0.0, 0.0, 0.0], 'rewardMean': 0.6979923027374079, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1028.50235541954
'totalSteps': 19200, 'rewardStep': 0.9032068494877514, 'errorList': [], 'lossList': [0.0, -1.309857932329178, 0.0, 1.7673280880600215, 0.0, 0.0, 0.0], 'rewardMean': 0.7416926464316899, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.7543428812687
'totalSteps': 20480, 'rewardStep': 0.8624547908075121, 'errorList': [], 'lossList': [0.0, -1.2720101052522659, 0.0, 0.9719592800363899, 0.0, 0.0, 0.0], 'rewardMean': 0.7444765812876156, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.717945002521
'totalSteps': 21760, 'rewardStep': 0.9862844582683344, 'errorList': [0.1188070733077307, 0.05479987582488366, 0.07923463718251118, 0.08633082102114835, 0.05065248079041225, 0.055750023599642554, 0.1325751504421972, 0.05445078565623545, 0.06862084574542429, 0.05568778736048655, 0.07577062442375135, 0.05255930263698554, 0.0567264561577129, 0.0527253176549587, 0.05463693623717428, 0.05797824172621588, 0.06558696965145372, 0.050510315495719244, 0.0538794506434118, 0.05470839234757829, 0.05846478364475946, 0.07952017195175128, 0.06556745693481632, 0.08188690055359478, 0.0665721610491432, 0.07083313865819026, 0.053618917463863124, 0.059392346519068835, 0.052269350572777575, 0.08673294448777798, 0.061339130502164135, 0.057178152071022934, 0.11467268991254331, 0.05343347426165258, 0.1463803192778603, 0.058350243995718504, 0.050573231994849484, 0.06755026240235883, 0.05107210556880303, 0.056951554104673625, 0.050610730187224895, 0.0626114441752538, 0.10699320628438264, 0.05257401630612356, 0.08127107920288698, 0.12949188251239924, 0.14362881551434, 0.059322416337519966, 0.058662308983239986, 0.05841510154910489], 'lossList': [0.0, -1.2321776521205903, 0.0, 0.9228563170135021, 0.0, 0.0, 0.0], 'rewardMean': 0.7661146130656302, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1163.633949032806, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=83.13
