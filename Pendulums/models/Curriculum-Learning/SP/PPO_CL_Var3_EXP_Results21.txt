#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 21
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8350738725532211, 'errorList': [], 'lossList': [0.0, -1.4203132659196853, 0.0, 33.551282691955564, 0.0, 0.0, 0.0], 'rewardMean': 0.7141258460666902, 'totalEpisodes': 55, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.06519848031306
'totalSteps': 3840, 'rewardStep': 0.6155005596010712, 'errorList': [], 'lossList': [0.0, -1.4139599734544754, 0.0, 37.98892225265503, 0.0, 0.0, 0.0], 'rewardMean': 0.6812507505781505, 'totalEpisodes': 124, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.877475422376381
'totalSteps': 5120, 'rewardStep': 0.9336757601620067, 'errorList': [88.6297283830134, 74.4498407497203, 107.85628371153878, 57.23166204390845, 70.64824213433202, 93.6404752049883, 126.66745519093786, 111.89758588765751, 93.13541072322752, 105.45732905590225, 24.82460369447781, 144.8645039904049, 102.22408981151546, 70.54013025650391, 11.621372120054549, 77.23569351806964, 127.18979251973397, 3.1181145907742955, 106.76672444348301, 31.390664043202758, 50.04716488573282, 79.86967762225386, 94.07758467658032, 4.25005031105981, 72.78222352376518, 83.48328692541207, 58.48698154613101, 103.29523520833921, 85.84640169328253, 87.07502223365306, 88.96581292743033, 139.69315326293545, 111.37419565099785, 57.322691121344434, 81.57169270664184, 146.56985283051287, 101.80102482560994, 69.7257220600186, 15.417094160458303, 143.82844030311583, 92.6206179888142, 119.45840244654411, 32.00980546112268, 26.279751630021334, 68.4247919226356, 86.61851436163246, 101.99248594188151, 85.82636786164863, 81.6163906260836, 85.97152630151776], 'lossList': [0.0, -1.395673707127571, 0.0, 37.2394918346405, 0.0, 0.0, 0.0], 'rewardMean': 0.7443570029741146, 'totalEpisodes': 173, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.025884197031337, 'successfulTests': 0
'totalSteps': 6400, 'rewardStep': 0.6836784875372952, 'errorList': [], 'lossList': [0.0, -1.3791085040569306, 0.0, 41.54396496772766, 0.0, 0.0, 0.0], 'rewardMean': 0.7322212998867507, 'totalEpisodes': 195, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.452637694290538
'totalSteps': 7680, 'rewardStep': 0.6175700055368476, 'errorList': [], 'lossList': [0.0, -1.375561586022377, 0.0, 29.007417027950286, 0.0, 0.0, 0.0], 'rewardMean': 0.7131127508284335, 'totalEpisodes': 204, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.430410776021763
'totalSteps': 8960, 'rewardStep': 0.5901648710138812, 'errorList': [], 'lossList': [0.0, -1.37810408949852, 0.0, 43.681646666526795, 0.0, 0.0, 0.0], 'rewardMean': 0.6955487679977832, 'totalEpisodes': 213, 'stepsPerEpisode': 120, 'rewardPerEpisode': 85.41170912739862
'totalSteps': 10240, 'rewardStep': 0.7204204417502533, 'errorList': [], 'lossList': [0.0, -1.3608828872442245, 0.0, 18.009181609153746, 0.0, 0.0, 0.0], 'rewardMean': 0.6986577272168419, 'totalEpisodes': 219, 'stepsPerEpisode': 57, 'rewardPerEpisode': 43.330532588700024
'totalSteps': 11520, 'rewardStep': 0.6935919416656819, 'errorList': [], 'lossList': [0.0, -1.358042922616005, 0.0, 14.27902125477791, 0.0, 0.0, 0.0], 'rewardMean': 0.6980948621556019, 'totalEpisodes': 223, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.3361982486541
'totalSteps': 12800, 'rewardStep': 0.7885355698857581, 'errorList': [], 'lossList': [0.0, -1.3461254441738129, 0.0, 11.25377411723137, 0.0, 0.0, 0.0], 'rewardMean': 0.7071389329286175, 'totalEpisodes': 228, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.942496480236027
'totalSteps': 14080, 'rewardStep': 0.5816864637190338, 'errorList': [], 'lossList': [0.0, -1.314295960664749, 0.0, 25.438754024505617, 0.0, 0.0, 0.0], 'rewardMean': 0.705989797342505, 'totalEpisodes': 232, 'stepsPerEpisode': 230, 'rewardPerEpisode': 171.93073780102705
'totalSteps': 15360, 'rewardStep': 0.8208017344353353, 'errorList': [], 'lossList': [0.0, -1.2981067276000977, 0.0, 7.244473748803139, 0.0, 0.0, 0.0], 'rewardMean': 0.7045625835307165, 'totalEpisodes': 234, 'stepsPerEpisode': 259, 'rewardPerEpisode': 212.88047672318467
'totalSteps': 16640, 'rewardStep': 0.6667130293439426, 'errorList': [], 'lossList': [0.0, -1.2618135607242584, 0.0, 4.419412615001201, 0.0, 0.0, 0.0], 'rewardMean': 0.7096838305050036, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.761971011607
'totalSteps': 17920, 'rewardStep': 0.8901851957989423, 'errorList': [], 'lossList': [0.0, -1.1979519379138948, 0.0, 3.5324027609825133, 0.0, 0.0, 0.0], 'rewardMean': 0.705334774068697, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.039107634768
'totalSteps': 19200, 'rewardStep': 0.929937793269737, 'errorList': [], 'lossList': [0.0, -1.1674096286296844, 0.0, 2.9245226661488415, 0.0, 0.0, 0.0], 'rewardMean': 0.7299607046419413, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.5749440893453
'totalSteps': 20480, 'rewardStep': 0.8011049173948828, 'errorList': [], 'lossList': [0.0, -1.1376645183563232, 0.0, 1.4392856204882265, 0.0, 0.0, 0.0], 'rewardMean': 0.7483141958277448, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.8427011721133
'totalSteps': 21760, 'rewardStep': 0.8714319629876698, 'errorList': [], 'lossList': [0.0, -1.0834719407558442, 0.0, 1.5451994233578443, 0.0, 0.0, 0.0], 'rewardMean': 0.7764409050251236, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.3498625501145
'totalSteps': 23040, 'rewardStep': 0.9994565733303357, 'errorList': [0.0633822807722231, 0.04897081687387206, 0.0602635988010277, 0.05826639668835418, 0.014990015703141183, 0.031373314753687986, 0.017825995717156998, 0.018111302600780498, 0.056309150880273016, 0.02018739208821417, 0.01725627440680201, 0.05662831100651562, 0.016902128897348146, 0.009092052787713056, 0.01356515019878521, 0.03365407239854194, 0.005722402949225035, 0.030439052467619493, 0.00929724018601695, 0.011242745319580994, 0.0076929404056671055, 0.024115491954160063, 0.017708462476148375, 0.04573471520749429, 0.04551528489024885, 0.023018878390273372, 0.03264961005743548, 0.022722711044661795, 0.04213931597716611, 0.028091210565922866, 0.05850850398173507, 0.07424711265054733, 0.01827213870680539, 0.018433604837916538, 0.02406408505077843, 0.01449518015237553, 0.03295312606276529, 0.03621598608330248, 0.02806444117486259, 0.04976907191649563, 0.011051782510082462, 0.10779223640025436, 0.012198273437930611, 0.0238386544448921, 0.038425285710277896, 0.0632602168149321, 0.018699427082655085, 0.008762803648802324, 0.04354955871343034, 0.021023739819665117], 'lossList': [0.0, -1.0396629917621611, 0.0, 1.1735753751918674, 0.0, 0.0, 0.0], 'rewardMean': 0.804344518183132, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.419229843843, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=86.74
