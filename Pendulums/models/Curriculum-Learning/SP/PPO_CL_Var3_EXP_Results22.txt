#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 22
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.6642933338938118, 'errorList': [], 'lossList': [0.0, -1.4258948928117752, 0.0, 29.398726930618285, 0.0, 0.0, 0.0], 'rewardMean': 0.5836970908989119, 'totalEpisodes': 62, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.541377980898207
'totalSteps': 3840, 'rewardStep': 0.7361581123692343, 'errorList': [], 'lossList': [0.0, -1.4054936653375625, 0.0, 39.48592733383179, 0.0, 0.0, 0.0], 'rewardMean': 0.6345174313890194, 'totalEpisodes': 128, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.106366354818947
'totalSteps': 5120, 'rewardStep': 0.8228172762981484, 'errorList': [], 'lossList': [0.0, -1.3898301011323928, 0.0, 38.42439284324646, 0.0, 0.0, 0.0], 'rewardMean': 0.6815923926163017, 'totalEpisodes': 178, 'stepsPerEpisode': 47, 'rewardPerEpisode': 34.96465835560522
'totalSteps': 6400, 'rewardStep': 0.6076232403551378, 'errorList': [], 'lossList': [0.0, -1.377533465027809, 0.0, 42.34480896949768, 0.0, 0.0, 0.0], 'rewardMean': 0.6667985621640689, 'totalEpisodes': 202, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.319890209278576
'totalSteps': 7680, 'rewardStep': 0.964405224356573, 'errorList': [2.163032023320493, 9.023533808525498, 4.732775653049245, 5.92321708878262, 8.66393180857342, 13.317465347131083, 2.957672238677923, 12.144811896319924, 14.848062694675246, 26.788055013430455, 2.503825450758315, 20.756544998717594, 2.1021344071971044, 12.952528806485745, 4.125333796474237, 16.507178724439335, 11.432175530833733, 20.939069539456007, 0.6252138449329844, 20.19835191161976, 11.228551216674468, 2.553994691110165, 8.405256385013786, 21.32362309822055, 8.266226788189423, 1.449945454937618, 11.91649387964864, 12.211550648442573, 1.5679657310134543, 0.3110008975997497, 16.586530142449764, 10.338998307294121, 21.492202342867188, 5.487916526231076, 13.022792644648167, 4.646126606592971, 0.8084711421926144, 12.979358050957936, 23.104618785695905, 19.546459191602338, 8.657501652451227, 1.478112388372063, 3.3678482878253098, 3.6751937407278175, 8.989888983191513, 0.23232863668355266, 8.443746211800264, 8.559994621440698, 4.161132407984547, 14.920277146490271], 'lossList': [0.0, -1.358378562927246, 0.0, 54.764103717803955, 0.0, 0.0, 0.0], 'rewardMean': 0.7163996725294863, 'totalEpisodes': 218, 'stepsPerEpisode': 63, 'rewardPerEpisode': 50.505760030265925, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.2999636341029971, 'errorList': [], 'lossList': [0.0, -1.3504833567142487, 0.0, 31.061859905719757, 0.0, 0.0, 0.0], 'rewardMean': 0.6569088098971306, 'totalEpisodes': 224, 'stepsPerEpisode': 135, 'rewardPerEpisode': 90.5584287396269
'totalSteps': 10240, 'rewardStep': 0.17890310758420414, 'errorList': [], 'lossList': [0.0, -1.3446289056539535, 0.0, 13.681361315250397, 0.0, 0.0, 0.0], 'rewardMean': 0.5971580971080148, 'totalEpisodes': 227, 'stepsPerEpisode': 388, 'rewardPerEpisode': 282.16256475615893
'totalSteps': 11520, 'rewardStep': 0.6950695902612145, 'errorList': [], 'lossList': [0.0, -1.3513592118024826, 0.0, 21.339162888526918, 0.0, 0.0, 0.0], 'rewardMean': 0.6080371519028147, 'totalEpisodes': 230, 'stepsPerEpisode': 407, 'rewardPerEpisode': 292.82794485360563
'totalSteps': 12800, 'rewardStep': 0.7884808855157067, 'errorList': [], 'lossList': [0.0, -1.3350785529613496, 0.0, 40.224085652828215, 0.0, 0.0, 0.0], 'rewardMean': 0.626081525264104, 'totalEpisodes': 232, 'stepsPerEpisode': 564, 'rewardPerEpisode': 456.6686501589652
'totalSteps': 14080, 'rewardStep': 0.8552359704847876, 'errorList': [], 'lossList': [0.0, -1.297904862165451, 0.0, 7.331999413371086, 0.0, 0.0, 0.0], 'rewardMean': 0.6612950375221816, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.5341201439517
'totalSteps': 15360, 'rewardStep': 0.8897267300533255, 'errorList': [], 'lossList': [0.0, -1.2725846219062804, 0.0, 3.613922513127327, 0.0, 0.0, 0.0], 'rewardMean': 0.6838383771381329, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1031.592185600469
'totalSteps': 16640, 'rewardStep': 0.9201495682526819, 'errorList': [], 'lossList': [0.0, -1.2658224654197694, 0.0, 3.217575601041317, 0.0, 0.0, 0.0], 'rewardMean': 0.7022375227264777, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1056.4627505850196
'totalSteps': 17920, 'rewardStep': 0.9167464899132672, 'errorList': [], 'lossList': [0.0, -1.2341279435157775, 0.0, 3.4592777337133884, 0.0, 0.0, 0.0], 'rewardMean': 0.7116304440879895, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.3949543876627
'totalSteps': 19200, 'rewardStep': 0.9378036810576168, 'errorList': [0.05738375196153527, 0.03919981424922131, 0.0909267501743276, 0.06662705669843824, 0.015011192678274974, 0.03478178579905134, 0.1282793684392476, 0.08577923410132221, 0.1285018139578124, 0.04959702545116323, 0.06483631914490796, 0.06185466806301262, 0.01569950285473811, 0.041310214211826436, 0.08312132523959086, 0.028023641701675154, 0.11507775588576438, 0.10476567085275693, 0.07130258368895626, 0.029539541712483585, 0.03336449799646496, 0.04528061635850257, 0.10669583693954733, 0.03450459081908755, 0.06710078925477113, 0.06510384817560719, 0.021436676809915744, 0.018000727131464847, 0.04110444088844545, 0.09183966500798314, 0.02733631427586332, 0.1287529415645257, 0.032424860230710374, 0.042607613471192796, 0.13701812534293667, 0.05421407153108681, 0.038339829310104795, 0.09434079646317792, 0.03228517296178159, 0.07695008793357375, 0.038935118454227535, 0.04251731838647329, 0.08095549059458672, 0.06289075762997655, 0.025479588195080116, 0.056389501210367565, 0.017880833985079978, 0.0350371171476305, 0.07261596677788881, 0.11060739234374355], 'lossList': [0.0, -1.206645935177803, 0.0, 1.9708505944162606, 0.0, 0.0, 0.0], 'rewardMean': 0.7446484881582375, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1120.503464221961, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=78.61
