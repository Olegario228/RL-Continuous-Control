#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 23
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.9385414953407986, 'errorList': [], 'lossList': [0.0, -1.4272979128360748, 0.0, 28.778411712646484, 0.0, 0.0, 0.0], 'rewardMean': 0.9187359555837695, 'totalEpisodes': 63, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.664009394590811
'totalSteps': 3840, 'rewardStep': 0.8194936877650889, 'errorList': [], 'lossList': [0.0, -1.4195672637224197, 0.0, 39.26473623275757, 0.0, 0.0, 0.0], 'rewardMean': 0.8856551996442094, 'totalEpisodes': 120, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.304203694814067
'totalSteps': 5120, 'rewardStep': 0.8649952739173533, 'errorList': [], 'lossList': [0.0, -1.40883327960968, 0.0, 48.68403589248657, 0.0, 0.0, 0.0], 'rewardMean': 0.8804902182124953, 'totalEpisodes': 159, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.217380791798181
'totalSteps': 6400, 'rewardStep': 0.9342665645509382, 'errorList': [64.9699185279686, 81.56163485502431, 101.55725986117258, 69.57231413631068, 70.76040270852161, 122.02027305071567, 93.50020700818038, 85.1957111303782, 73.94473066902025, 109.24366265752266, 130.86302676825966, 93.7922924591773, 97.12012649985988, 118.80600192889733, 54.07360501255002, 119.69910220844773, 17.439620906018455, 124.13954391089659, 111.0472866603573, 91.46249716993465, 67.9305628899285, 105.46837611077738, 15.68193715911278, 66.39825453274881, 45.25462673323246, 112.24323982144786, 15.9056767911344, 154.77987316684892, 73.22965275747703, 117.94221115032371, 14.755907816494767, 49.71553368591562, 33.66607756687919, 97.49923341560861, 63.56537073940652, 106.0357381015158, 146.02507662603617, 16.390036986193078, 115.50059070307816, 8.49226645374219, 105.92571294563736, 48.99508667211088, 6.203435357177842, 80.18384878358205, 128.18111978594726, 131.14397322352576, 90.04810664563016, 75.7429672376177, 10.91467328839636, 142.59585965584347], 'lossList': [0.0, -1.4028137761354447, 0.0, 43.52179386138916, 0.0, 0.0, 0.0], 'rewardMean': 0.8912454874801838, 'totalEpisodes': 180, 'stepsPerEpisode': 29, 'rewardPerEpisode': 19.7023859699043, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.6155145253007424, 'errorList': [], 'lossList': [0.0, -1.3782545363903045, 0.0, 38.81054783821106, 0.0, 0.0, 0.0], 'rewardMean': 0.8452903271169436, 'totalEpisodes': 194, 'stepsPerEpisode': 83, 'rewardPerEpisode': 50.90080473061893
'totalSteps': 8960, 'rewardStep': 0.8986962317126174, 'errorList': [], 'lossList': [0.0, -1.3559630995988845, 0.0, 37.89101650714874, 0.0, 0.0, 0.0], 'rewardMean': 0.8529197420591828, 'totalEpisodes': 204, 'stepsPerEpisode': 57, 'rewardPerEpisode': 48.294361663447454
'totalSteps': 10240, 'rewardStep': 0.8353846208325266, 'errorList': [], 'lossList': [0.0, -1.345049217939377, 0.0, 17.12765784740448, 0.0, 0.0, 0.0], 'rewardMean': 0.8507278519058508, 'totalEpisodes': 209, 'stepsPerEpisode': 67, 'rewardPerEpisode': 57.33946041511952
'totalSteps': 11520, 'rewardStep': 0.7206244708614411, 'errorList': [], 'lossList': [0.0, -1.3465876853466034, 0.0, 7.63674125790596, 0.0, 0.0, 0.0], 'rewardMean': 0.8362719206786942, 'totalEpisodes': 214, 'stepsPerEpisode': 51, 'rewardPerEpisode': 42.94049994668615
'totalSteps': 12800, 'rewardStep': 0.5211626949124459, 'errorList': [], 'lossList': [0.0, -1.337585647702217, 0.0, 8.266613781750202, 0.0, 0.0, 0.0], 'rewardMean': 0.8047609981020694, 'totalEpisodes': 215, 'stepsPerEpisode': 353, 'rewardPerEpisode': 255.60548121654944
'totalSteps': 14080, 'rewardStep': 0.8034229907326326, 'errorList': [], 'lossList': [0.0, -1.325807563662529, 0.0, 7.231330421566963, 0.0, 0.0, 0.0], 'rewardMean': 0.7952102555926585, 'totalEpisodes': 217, 'stepsPerEpisode': 75, 'rewardPerEpisode': 64.02360130425411
'totalSteps': 15360, 'rewardStep': 0.8002356140755227, 'errorList': [], 'lossList': [0.0, -1.316105034351349, 0.0, 4.8278109741210935, 0.0, 0.0, 0.0], 'rewardMean': 0.7813796674661309, 'totalEpisodes': 218, 'stepsPerEpisode': 221, 'rewardPerEpisode': 192.78556629854256
'totalSteps': 16640, 'rewardStep': 0.7518739847158004, 'errorList': [], 'lossList': [0.0, -1.2998722970485688, 0.0, 2.047607130408287, 0.0, 0.0, 0.0], 'rewardMean': 0.774617697161202, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1011.0633924402397
'totalSteps': 17920, 'rewardStep': 0.7999765463490113, 'errorList': [], 'lossList': [0.0, -1.265034459233284, 0.0, 1.2694012135267259, 0.0, 0.0, 0.0], 'rewardMean': 0.7681158244043679, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1030.5142157075784
'totalSteps': 19200, 'rewardStep': 0.8726704033768826, 'errorList': [], 'lossList': [0.0, -1.2339224940538407, 0.0, 1.957043470814824, 0.0, 0.0, 0.0], 'rewardMean': 0.7619562082869622, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.7433265612515
'totalSteps': 20480, 'rewardStep': 0.9266192968031776, 'errorList': [], 'lossList': [0.0, -1.2115039122104645, 0.0, 1.4753164338320495, 0.0, 0.0, 0.0], 'rewardMean': 0.7930666854372058, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.3839028578086
'totalSteps': 21760, 'rewardStep': 0.917832553608863, 'errorList': [], 'lossList': [0.0, -1.1782738029956819, 0.0, 0.7494781612418592, 0.0, 0.0, 0.0], 'rewardMean': 0.7949803176268302, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1160.3035240394322
'totalSteps': 23040, 'rewardStep': 0.8960488521565864, 'errorList': [], 'lossList': [0.0, -1.1499169969558716, 0.0, 0.8040760933980345, 0.0, 0.0, 0.0], 'rewardMean': 0.8010467407592362, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1211.661041882567
'totalSteps': 24320, 'rewardStep': 0.905874304914449, 'errorList': [], 'lossList': [0.0, -1.116112042069435, 0.0, 0.4409744849614799, 0.0, 0.0, 0.0], 'rewardMean': 0.819571724164537, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.5540691065423
'totalSteps': 25600, 'rewardStep': 0.955139674654743, 'errorList': [0.20061580022230027, 0.12686275274851977, 0.10158018262130573, 0.15621389174665326, 0.1917194358747259, 0.12191140282970996, 0.22850971016824742, 0.11858718207892004, 0.17486984557926677, 0.16876091021747508, 0.2158850186722031, 0.12545804462470325, 0.12787276799509492, 0.22291294051878266, 0.16408534123406684, 0.14774691646693697, 0.18846461151320987, 0.17500332663829815, 0.17565794142180693, 0.21039033370418891, 0.15300380838351316, 0.22358424084972317, 0.12707924190986886, 0.14964205769245237, 0.15637466449389037, 0.11912212017673748, 0.13553021664383808, 0.18024994392461594, 0.1262359975954759, 0.18628241375058402, 0.1851617472350751, 0.12694150873718257, 0.1870140737031393, 0.1764068315343217, 0.18465725113456105, 0.1317997303205086, 0.19876267863850122, 0.17032272342725235, 0.12367853021075938, 0.13679343307438419, 0.15103955709629552, 0.15273403226812865, 0.17834938964799976, 0.20168731242545143, 0.15612861806644046, 0.15889928093235325, 0.17763762255883817, 0.21331684285644045, 0.22369857713724403, 0.20225827701804136], 'lossList': [0.0, -1.097535337805748, 0.0, 0.29971308644860983, 0.0, 0.0, 0.0], 'rewardMean': 0.8629694221387668, 'totalEpisodes': 218, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1202.4910272199022, 'successfulTests': 40
#maxSuccessfulTests=40, maxSuccessfulTestsAtStep=25600, timeSpent=95.21
