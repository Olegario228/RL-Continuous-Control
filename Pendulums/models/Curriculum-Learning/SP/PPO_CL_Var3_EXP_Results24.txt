#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 24
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8785605869944857, 'errorList': [], 'lossList': [0.0, -1.4377107906341553, 0.0, 27.0487903547287, 0.0, 0.0, 0.0], 'rewardMean': 0.8139257256468834, 'totalEpisodes': 61, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.12739996197938
'totalSteps': 3840, 'rewardStep': 0.5010042105340886, 'errorList': [], 'lossList': [0.0, -1.428781441450119, 0.0, 31.1786949634552, 0.0, 0.0, 0.0], 'rewardMean': 0.7096185539426184, 'totalEpisodes': 141, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.4635973810505747
'totalSteps': 5120, 'rewardStep': 0.6926323789679267, 'errorList': [], 'lossList': [0.0, -1.4019851249456405, 0.0, 47.96598220825195, 0.0, 0.0, 0.0], 'rewardMean': 0.7053720101989456, 'totalEpisodes': 180, 'stepsPerEpisode': 15, 'rewardPerEpisode': 8.72009703812031
'totalSteps': 6400, 'rewardStep': 0.8757744032759847, 'errorList': [], 'lossList': [0.0, -1.3872130233049393, 0.0, 43.374296770095825, 0.0, 0.0, 0.0], 'rewardMean': 0.7394524888143534, 'totalEpisodes': 197, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.769896546498508
'totalSteps': 7680, 'rewardStep': 0.7582511518078667, 'errorList': [], 'lossList': [0.0, -1.3694883382320404, 0.0, 55.25076336860657, 0.0, 0.0, 0.0], 'rewardMean': 0.7425855993132723, 'totalEpisodes': 209, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.90441737219602
'totalSteps': 8960, 'rewardStep': 0.4698491190612058, 'errorList': [], 'lossList': [0.0, -1.355762243270874, 0.0, 33.78991672515869, 0.0, 0.0, 0.0], 'rewardMean': 0.7036232449915485, 'totalEpisodes': 215, 'stepsPerEpisode': 214, 'rewardPerEpisode': 146.4608553592377
'totalSteps': 10240, 'rewardStep': 0.9425428326646874, 'errorList': [13.30270521736545, 3.351835121244004, 36.38124440216203, 1.320211038539111, 44.987977145584075, 24.560962980082824, 9.099438258138473, 31.205788389157597, 15.602765871251298, 41.57868158394905, 22.23605299428053, 37.38900231805077, 1.419694115845005, 47.25882864111539, 54.55534893874804, 6.205003124717392, 10.831520268370367, 26.6758370877427, 66.64800031523342, 6.498216750945854, 51.96941551703107, 74.42449105256101, 32.2458951863105, 20.312677342732762, 38.399818104690986, 22.52708277690721, 9.845907435607689, 10.88023808747067, 1.2412052698999072, 20.028744466572746, 1.487309731901769, 59.28043789116499, 39.902042024257334, 8.754699860782747, 112.62719744774145, 1.9949560706946132, 46.48249514472415, 71.03974488020972, 2.8467961622515294, 63.096348020680594, 4.509446496415476, 24.463950894576218, 43.362931224807625, 28.807485868452332, 66.29674434501537, 6.910938837410419, 43.66676196913653, 82.50958073008678, 67.14555213036158, 12.997136406555487], 'lossList': [0.0, -1.3478244191408157, 0.0, 28.485677733421326, 0.0, 0.0, 0.0], 'rewardMean': 0.7334881934506908, 'totalEpisodes': 222, 'stepsPerEpisode': 16, 'rewardPerEpisode': 15.116588240858963, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6693908421471237, 'errorList': [], 'lossList': [0.0, -1.352621232867241, 0.0, 47.17248103618622, 0.0, 0.0, 0.0], 'rewardMean': 0.7263662655280723, 'totalEpisodes': 227, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2994088734021734
'totalSteps': 12800, 'rewardStep': 0.8898037863032909, 'errorList': [], 'lossList': [0.0, -1.358169143795967, 0.0, 36.98912706851959, 0.0, 0.0, 0.0], 'rewardMean': 0.7427100176055941, 'totalEpisodes': 232, 'stepsPerEpisode': 63, 'rewardPerEpisode': 58.83425810572453
'totalSteps': 14080, 'rewardStep': 0.9499367154903771, 'errorList': [7.003732749325992, 12.399846194678709, 7.580663435113742, 8.037915708754333, 8.212447388672302, 16.470519488225477, 22.086213084861203, 32.32700340386137, 6.631239154106577, 17.297288299469358, 3.965425317749872, 24.043711473977663, 14.00418520480812, 11.810471604264242, 8.11790820535146, 17.787163281271084, 22.444188003819992, 4.655118782481243, 12.685617239074892, 38.24453189655126, 2.393090660546573, 16.707542574778177, 21.586818349427343, 24.898611368909187, 11.27209135190927, 6.353747470056568, 9.004666431392376, 7.4018084276608755, 11.030085895305733, 12.208529812911085, 9.39821535486794, 5.170452618959047, 1.023467380704283, 15.529285570478308, 11.932869520357729, 11.246054574662926, 9.315289831169643, 17.991477410286826, 37.181150673220436, 33.301518698930245, 31.24012510863405, 20.640934559610194, 4.074712289980166, 46.237994793393916, 17.377613539822, 2.9059976120064044, 7.67420690804978, 35.02460672215568, 13.379380043344204, 5.116467538285477], 'lossList': [0.0, -1.3378726875782012, 0.0, 12.665302219390869, 0.0, 0.0, 0.0], 'rewardMean': 0.7627746027247038, 'totalEpisodes': 237, 'stepsPerEpisode': 113, 'rewardPerEpisode': 104.07377750162996, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.9106542511180723, 'errorList': [], 'lossList': [0.0, -1.3226953750848771, 0.0, 21.29655893802643, 0.0, 0.0, 0.0], 'rewardMean': 0.7659839691370623, 'totalEpisodes': 242, 'stepsPerEpisode': 171, 'rewardPerEpisode': 153.76894019811914
'totalSteps': 16640, 'rewardStep': 0.27101344259302657, 'errorList': [], 'lossList': [0.0, -1.333897340297699, 0.0, 5.3865683215856555, 0.0, 0.0, 0.0], 'rewardMean': 0.7429848923429562, 'totalEpisodes': 245, 'stepsPerEpisode': 266, 'rewardPerEpisode': 199.9686832661683
'totalSteps': 17920, 'rewardStep': 0.7265047030470202, 'errorList': [], 'lossList': [0.0, -1.3645219892263412, 0.0, 3.8902071851491926, 0.0, 0.0, 0.0], 'rewardMean': 0.7463721247508655, 'totalEpisodes': 247, 'stepsPerEpisode': 648, 'rewardPerEpisode': 562.2182182162318
'totalSteps': 19200, 'rewardStep': 0.7938017519438711, 'errorList': [], 'lossList': [0.0, -1.3580519771575927, 0.0, 3.4832093960046766, 0.0, 0.0, 0.0], 'rewardMean': 0.7381748596176541, 'totalEpisodes': 248, 'stepsPerEpisode': 434, 'rewardPerEpisode': 369.26023145283347
'totalSteps': 20480, 'rewardStep': 0.9643259758583411, 'errorList': [0.033661702370557066, 0.04620043797307117, 0.034816362589600025, 0.07773016099611789, 0.08072478086786017, 0.08707399435803431, 0.0649767206221095, 0.047996104483636594, 0.03487752161413396, 0.050721376645680195, 0.06693917893732508, 0.034822218347824896, 0.034796927980681416, 0.05068956755614431, 0.06302752409646027, 0.07040503192867423, 0.06307549320212107, 0.06810194736551661, 0.04657496912383819, 0.045847856790782786, 0.062489822074663776, 0.09257883159284652, 0.04599901713705342, 0.08822691945605186, 0.03513000462375129, 0.07020912093786667, 0.030732975085839675, 0.08054956606371133, 0.061022575488046925, 0.07365662263344383, 0.09765530593853033, 0.056500465802389585, 0.06228457125871784, 0.035279085943459895, 0.03036692842828263, 0.039862381475463796, 0.034696282336091976, 0.06547013951929093, 0.06880666446247531, 0.06223462562008894, 0.05049695690711478, 0.09000041209255137, 0.04427851632808991, 0.09871464579090553, 0.10653683364084647, 0.04724267667717084, 0.047791693207199365, 0.03241710700353737, 0.05451102264201857, 0.0836091659639253], 'lossList': [0.0, -1.3331200456619263, 0.0, 1.787983631491661, 0.0, 0.0, 0.0], 'rewardMean': 0.7587823420227017, 'totalEpisodes': 248, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.365508482001, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=100.29
