#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 25
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.8891961727514607, 'errorList': [], 'lossList': [0.0, -1.4062160605192184, 0.0, 31.873206996917723, 0.0, 0.0, 0.0], 'rewardMean': 0.6667094568634893, 'totalEpisodes': 90, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.808703798945416
'totalSteps': 3840, 'rewardStep': 0.8094460250162736, 'errorList': [], 'lossList': [0.0, -1.3891420370340348, 0.0, 35.50597594261169, 0.0, 0.0, 0.0], 'rewardMean': 0.7142883129144174, 'totalEpisodes': 144, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.869943562926616
'totalSteps': 5120, 'rewardStep': 0.7816457515373101, 'errorList': [], 'lossList': [0.0, -1.3823390609025956, 0.0, 42.62455695152283, 0.0, 0.0, 0.0], 'rewardMean': 0.7311276725701406, 'totalEpisodes': 178, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.853691191926757
'totalSteps': 6400, 'rewardStep': 0.8778419688483556, 'errorList': [], 'lossList': [0.0, -1.3847755497694016, 0.0, 43.48114273071289, 0.0, 0.0, 0.0], 'rewardMean': 0.7604705318257835, 'totalEpisodes': 195, 'stepsPerEpisode': 28, 'rewardPerEpisode': 23.689320296133186
'totalSteps': 7680, 'rewardStep': 0.9295782933487515, 'errorList': [], 'lossList': [0.0, -1.3755084121227263, 0.0, 42.01684631347656, 0.0, 0.0, 0.0], 'rewardMean': 0.7886551587462782, 'totalEpisodes': 206, 'stepsPerEpisode': 78, 'rewardPerEpisode': 64.65300582503463
'totalSteps': 8960, 'rewardStep': 0.6632233659090203, 'errorList': [], 'lossList': [0.0, -1.3756842696666718, 0.0, 21.311993808746337, 0.0, 0.0, 0.0], 'rewardMean': 0.7707363311980985, 'totalEpisodes': 212, 'stepsPerEpisode': 82, 'rewardPerEpisode': 70.49408016001767
'totalSteps': 10240, 'rewardStep': 0.44213161105679993, 'errorList': [], 'lossList': [0.0, -1.3865553450584411, 0.0, 9.330254129171372, 0.0, 0.0, 0.0], 'rewardMean': 0.7296607411804362, 'totalEpisodes': 218, 'stepsPerEpisode': 230, 'rewardPerEpisode': 162.90336311000613
'totalSteps': 11520, 'rewardStep': 0.6662109036689111, 'errorList': [], 'lossList': [0.0, -1.3746142214536667, 0.0, 18.220144946575164, 0.0, 0.0, 0.0], 'rewardMean': 0.7226107592347112, 'totalEpisodes': 224, 'stepsPerEpisode': 103, 'rewardPerEpisode': 86.24235145187689
'totalSteps': 12800, 'rewardStep': 0.8287358278105306, 'errorList': [], 'lossList': [0.0, -1.3591275113821029, 0.0, 7.645367067754268, 0.0, 0.0, 0.0], 'rewardMean': 0.733223266092293, 'totalEpisodes': 228, 'stepsPerEpisode': 362, 'rewardPerEpisode': 312.2934459308896
'totalSteps': 14080, 'rewardStep': 0.82465802283796, 'errorList': [], 'lossList': [0.0, -1.3430112516880035, 0.0, 23.71697658777237, 0.0, 0.0, 0.0], 'rewardMean': 0.7712667942785374, 'totalEpisodes': 231, 'stepsPerEpisode': 214, 'rewardPerEpisode': 178.96545322387325
'totalSteps': 15360, 'rewardStep': 0.547620084366902, 'errorList': [], 'lossList': [0.0, -1.3237265986204148, 0.0, 4.132472748756409, 0.0, 0.0, 0.0], 'rewardMean': 0.7371091854400815, 'totalEpisodes': 232, 'stepsPerEpisode': 698, 'rewardPerEpisode': 522.2643216616306
'totalSteps': 16640, 'rewardStep': 0.9623182197534677, 'errorList': [0.0558313533503575, 0.049032385119399134, 0.06510335101701632, 0.06487414335372536, 0.02998627902763426, 0.0937396332755129, 0.06258894867308545, 0.0647403267985289, 0.029286996962056385, 0.030396066453268568, 0.02805197108181517, 0.05183535500283768, 0.16273305930937437, 0.027417677122193417, 0.029046421901154498, 0.028568414171794326, 0.03072077609971098, 0.029209350403582694, 0.031913743522081615, 0.030289303876477713, 0.02691383153248584, 0.029303077894661803, 0.029625051631011947, 0.040471764560675305, 0.040923489942098455, 0.07490225518044409, 0.05673892383016307, 0.0712466288843479, 0.0536739246868667, 0.03602895661423782, 0.027236753041741116, 0.03728010670688071, 0.028972011480404552, 0.02829162019769887, 0.08366660736616938, 0.10989727244933056, 0.05055144790387763, 0.08341140187052429, 0.07111716987288477, 0.050859189934375236, 0.05188551649853926, 0.031228826789744916, 0.030621458846868215, 0.03141757675621556, 0.050677169217608675, 0.04790556007998456, 0.03544019107854876, 0.027388708008221806, 0.0881913787830437, 0.02791626544130399], 'lossList': [0.0, -1.318208366036415, 0.0, 6.175292084217071, 0.0, 0.0, 0.0], 'rewardMean': 0.752396404913801, 'totalEpisodes': 233, 'stepsPerEpisode': 599, 'rewardPerEpisode': 536.5918907002043, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=16640, timeSpent=54.18
