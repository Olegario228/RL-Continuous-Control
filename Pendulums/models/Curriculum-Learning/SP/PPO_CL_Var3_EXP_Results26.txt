#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 26
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.9389690596175113, 'errorList': [], 'lossList': [0.0, -1.436090161204338, 0.0, 28.057911891937255, 0.0, 0.0, 0.0], 'rewardMean': 0.8770095962294593, 'totalEpisodes': 93, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.970281724236486
'totalSteps': 3840, 'rewardStep': 0.6163773703675131, 'errorList': [], 'lossList': [0.0, -1.4354606914520263, 0.0, 37.0821874332428, 0.0, 0.0, 0.0], 'rewardMean': 0.7901321876088105, 'totalEpisodes': 140, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.8190867416081387
'totalSteps': 5120, 'rewardStep': 0.6749914623946164, 'errorList': [], 'lossList': [0.0, -1.4198239624500275, 0.0, 44.01804174423218, 0.0, 0.0, 0.0], 'rewardMean': 0.761347006305262, 'totalEpisodes': 164, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.893600436765323
'totalSteps': 6400, 'rewardStep': 0.6738886751040915, 'errorList': [], 'lossList': [0.0, -1.4056806683540344, 0.0, 45.521700086593626, 0.0, 0.0, 0.0], 'rewardMean': 0.7438553400650278, 'totalEpisodes': 178, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.312556694997444
'totalSteps': 7680, 'rewardStep': 0.890214325176044, 'errorList': [], 'lossList': [0.0, -1.3925734460353851, 0.0, 23.567714614868166, 0.0, 0.0, 0.0], 'rewardMean': 0.7682485042501973, 'totalEpisodes': 184, 'stepsPerEpisode': 114, 'rewardPerEpisode': 95.85506402879233
'totalSteps': 8960, 'rewardStep': 0.7037190376773542, 'errorList': [], 'lossList': [0.0, -1.390051422715187, 0.0, 35.81720753908157, 0.0, 0.0, 0.0], 'rewardMean': 0.7590300090255054, 'totalEpisodes': 190, 'stepsPerEpisode': 200, 'rewardPerEpisode': 162.52555423457176
'totalSteps': 10240, 'rewardStep': 0.5795752658441637, 'errorList': [], 'lossList': [0.0, -1.389138332605362, 0.0, 12.411857947707176, 0.0, 0.0, 0.0], 'rewardMean': 0.7365981661278376, 'totalEpisodes': 194, 'stepsPerEpisode': 322, 'rewardPerEpisode': 250.22839987308336
'totalSteps': 11520, 'rewardStep': 0.513308936426331, 'errorList': [], 'lossList': [0.0, -1.4036584901809692, 0.0, 8.644181065559387, 0.0, 0.0, 0.0], 'rewardMean': 0.711788251716559, 'totalEpisodes': 201, 'stepsPerEpisode': 14, 'rewardPerEpisode': 7.5321357840976475
'totalSteps': 12800, 'rewardStep': 0.6876196477529235, 'errorList': [], 'lossList': [0.0, -1.381717637181282, 0.0, 21.876323990821838, 0.0, 0.0, 0.0], 'rewardMean': 0.7093713913201954, 'totalEpisodes': 206, 'stepsPerEpisode': 152, 'rewardPerEpisode': 122.26324248715595
'totalSteps': 14080, 'rewardStep': 0.51860060622074, 'errorList': [], 'lossList': [0.0, -1.3626781976222992, 0.0, 5.456056247949601, 0.0, 0.0, 0.0], 'rewardMean': 0.679726438658129, 'totalEpisodes': 208, 'stepsPerEpisode': 420, 'rewardPerEpisode': 282.1436716227313
'totalSteps': 15360, 'rewardStep': 0.8759870736585845, 'errorList': [], 'lossList': [0.0, -1.3456636494398118, 0.0, 4.441443163752556, 0.0, 0.0, 0.0], 'rewardMean': 0.6734282400622361, 'totalEpisodes': 211, 'stepsPerEpisode': 63, 'rewardPerEpisode': 55.18116239590017
'totalSteps': 16640, 'rewardStep': 0.9074559283134576, 'errorList': [], 'lossList': [0.0, -1.3215066051483155, 0.0, 3.9699556577205657, 0.0, 0.0, 0.0], 'rewardMean': 0.7025360958568306, 'totalEpisodes': 215, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.204258077949984
'totalSteps': 17920, 'rewardStep': 0.33897270750497044, 'errorList': [], 'lossList': [0.0, -1.2805211281776427, 0.0, 3.5950866675376894, 0.0, 0.0, 0.0], 'rewardMean': 0.6689342203678661, 'totalEpisodes': 217, 'stepsPerEpisode': 355, 'rewardPerEpisode': 255.3392276583067
'totalSteps': 19200, 'rewardStep': 0.8817416469204827, 'errorList': [], 'lossList': [0.0, -1.2633126330375672, 0.0, 4.710275346040726, 0.0, 0.0, 0.0], 'rewardMean': 0.6897195175495051, 'totalEpisodes': 220, 'stepsPerEpisode': 261, 'rewardPerEpisode': 221.8007466437184
'totalSteps': 20480, 'rewardStep': 0.6783871937415211, 'errorList': [], 'lossList': [0.0, -1.2386674273014069, 0.0, 2.3526265624165537, 0.0, 0.0, 0.0], 'rewardMean': 0.6685368044060529, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.4634209517542
'totalSteps': 21760, 'rewardStep': 0.7970039141210964, 'errorList': [], 'lossList': [0.0, -1.1884982562065125, 0.0, 1.1419399577379226, 0.0, 0.0, 0.0], 'rewardMean': 0.6778652920504271, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.4105487765225
'totalSteps': 23040, 'rewardStep': 0.9356722772124745, 'errorList': [0.055551147306043755, 0.05856888992424826, 0.07171088267122515, 0.05015918066971684, 0.07473110206622303, 0.0940926772525727, 0.04313216058959209, 0.07870733174684831, 0.06424469770293086, 0.08065727007317249, 0.09252352244158954, 0.05154641361624476, 0.054636734168734054, 0.1170485332221414, 0.055333114151433646, 0.0432848039614223, 0.07095043103599788, 0.043219984836439324, 0.045311273851269505, 0.10691764887533702, 0.07419632376806032, 0.07433724975307511, 0.09013786451831125, 0.10019990920979778, 0.07648822173650228, 0.08241155530656394, 0.04671579302762633, 0.047012372650768026, 0.10297513966876741, 0.09855583511974171, 0.10732996711255514, 0.1050162282146892, 0.06410639249741984, 0.05966966678549217, 0.10473485726939573, 0.1025657820830788, 0.06136220676242539, 0.05233289165510888, 0.052371086538879924, 0.06075867373631708, 0.04223460082494077, 0.06345822262583488, 0.13518362867256353, 0.0934685266766104, 0.07928621851819002, 0.0399003818741248, 0.07764527789724454, 0.03881228138913669, 0.03657266749841618, 0.058294581649391206], 'lossList': [0.0, -1.1650486123561858, 0.0, 0.8554552655667067, 0.0, 0.0, 0.0], 'rewardMean': 0.7134749931872582, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.8811914039652, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=68.06
