#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 27
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.6476027380112682, 'errorList': [], 'lossList': [0.0, -1.4172860139608383, 0.0, 28.715751190185546, 0.0, 0.0, 0.0], 'rewardMean': 0.7316200398974888, 'totalEpisodes': 102, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.030005090184754
'totalSteps': 3840, 'rewardStep': 0.7482340169676284, 'errorList': [], 'lossList': [0.0, -1.39882743537426, 0.0, 38.56532205581665, 0.0, 0.0, 0.0], 'rewardMean': 0.737158032254202, 'totalEpisodes': 161, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.82549003801808
'totalSteps': 5120, 'rewardStep': 0.6869854191148762, 'errorList': [], 'lossList': [0.0, -1.378232520222664, 0.0, 36.44403657913208, 0.0, 0.0, 0.0], 'rewardMean': 0.7246148789693705, 'totalEpisodes': 201, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.672865870106214
'totalSteps': 6400, 'rewardStep': 0.5927466577082044, 'errorList': [], 'lossList': [0.0, -1.3641133105754852, 0.0, 35.65584355354309, 0.0, 0.0, 0.0], 'rewardMean': 0.6982412347171373, 'totalEpisodes': 222, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.09519791400584
'totalSteps': 7680, 'rewardStep': 0.8161425692335444, 'errorList': [], 'lossList': [0.0, -1.346716039776802, 0.0, 56.573152256011966, 0.0, 0.0, 0.0], 'rewardMean': 0.7178914571365386, 'totalEpisodes': 238, 'stepsPerEpisode': 57, 'rewardPerEpisode': 40.000354748635544
'totalSteps': 8960, 'rewardStep': 0.6373372014870885, 'errorList': [], 'lossList': [0.0, -1.3360969573259354, 0.0, 36.53421094417572, 0.0, 0.0, 0.0], 'rewardMean': 0.7063837063294743, 'totalEpisodes': 245, 'stepsPerEpisode': 133, 'rewardPerEpisode': 109.82952220234306
'totalSteps': 10240, 'rewardStep': 0.7595065338331008, 'errorList': [], 'lossList': [0.0, -1.3270975983142852, 0.0, 15.69374845266342, 0.0, 0.0, 0.0], 'rewardMean': 0.7130240597674276, 'totalEpisodes': 249, 'stepsPerEpisode': 143, 'rewardPerEpisode': 122.06381967965879
'totalSteps': 11520, 'rewardStep': 0.7515078330854018, 'errorList': [], 'lossList': [0.0, -1.3415500408411025, 0.0, 26.04948098897934, 0.0, 0.0, 0.0], 'rewardMean': 0.7173000345805358, 'totalEpisodes': 254, 'stepsPerEpisode': 99, 'rewardPerEpisode': 77.76497741630673
'totalSteps': 12800, 'rewardStep': 0.4159852264511192, 'errorList': [], 'lossList': [0.0, -1.3468625897169113, 0.0, 19.593196628093718, 0.0, 0.0, 0.0], 'rewardMean': 0.6871685537675941, 'totalEpisodes': 261, 'stepsPerEpisode': 169, 'rewardPerEpisode': 110.31320638256251
'totalSteps': 14080, 'rewardStep': 0.771028188575087, 'errorList': [], 'lossList': [0.0, -1.3575134682655334, 0.0, 11.590478444695473, 0.0, 0.0, 0.0], 'rewardMean': 0.6827076384467319, 'totalEpisodes': 268, 'stepsPerEpisode': 40, 'rewardPerEpisode': 32.16332192001476
'totalSteps': 15360, 'rewardStep': 0.7035684145774462, 'errorList': [], 'lossList': [0.0, -1.3641618198156358, 0.0, 13.061252764463426, 0.0, 0.0, 0.0], 'rewardMean': 0.6883042061033497, 'totalEpisodes': 273, 'stepsPerEpisode': 393, 'rewardPerEpisode': 322.6665742205498
'totalSteps': 16640, 'rewardStep': 0.6448553290747097, 'errorList': [], 'lossList': [0.0, -1.348817186355591, 0.0, 11.2574524974823, 0.0, 0.0, 0.0], 'rewardMean': 0.6779663373140578, 'totalEpisodes': 277, 'stepsPerEpisode': 201, 'rewardPerEpisode': 158.49152689654383
'totalSteps': 17920, 'rewardStep': 0.8813910425002958, 'errorList': [], 'lossList': [0.0, -1.3369125366210937, 0.0, 4.025032634139061, 0.0, 0.0, 0.0], 'rewardMean': 0.6974068996525997, 'totalEpisodes': 281, 'stepsPerEpisode': 101, 'rewardPerEpisode': 92.10318459436756
'totalSteps': 19200, 'rewardStep': 0.7662324096368325, 'errorList': [], 'lossList': [0.0, -1.3188210648298264, 0.0, 4.302950447797775, 0.0, 0.0, 0.0], 'rewardMean': 0.7147554748454625, 'totalEpisodes': 282, 'stepsPerEpisode': 543, 'rewardPerEpisode': 436.64181883887863
'totalSteps': 20480, 'rewardStep': 0.7085937666406754, 'errorList': [], 'lossList': [0.0, -1.2927178645133972, 0.0, 3.583411637544632, 0.0, 0.0, 0.0], 'rewardMean': 0.7040005945861758, 'totalEpisodes': 282, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 962.6065313966142
'totalSteps': 21760, 'rewardStep': 0.8777564297247514, 'errorList': [], 'lossList': [0.0, -1.2581491249799728, 0.0, 1.8411191464960575, 0.0, 0.0, 0.0], 'rewardMean': 0.7280425174099421, 'totalEpisodes': 282, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1034.7581456604723
'totalSteps': 23040, 'rewardStep': 0.9532496658565657, 'errorList': [1.694104671675314, 0.2049572512307925, 1.7973463425237843, 0.6844409873258009, 0.567661581601399, 5.520804108923764, 0.4111704942731864, 4.917303181882056, 6.213033526625824, 0.1399267351120196, 10.400553666126704, 0.7996461816574634, 2.4728896299305974, 4.4321299829447, 0.4072945341320996, 1.1351057939752496, 1.9012593141851173, 0.7649165934371367, 0.2508457260652003, 1.6470984739573113, 9.933566604952796, 5.223449861442228, 2.2775638215953498, 2.7960378271453235, 1.795387222851633, 2.6348799955645372, 1.875552555114266, 3.4167560877805565, 0.33713828553899705, 0.5640768891091752, 1.3683267899038138, 3.449205979141975, 2.229427555508439, 0.349896198598104, 9.488535720871523, 2.8172195637153257, 10.442322114320735, 1.8513352109830263, 1.197814039643633, 5.047591260398201, 0.37276759029832524, 0.13628018384561316, 3.742002535083804, 2.8845789506210227, 2.62982498368275, 0.5477653541453402, 0.7430834891961026, 0.7469246102361121, 4.115223402333608, 7.0678399598219555], 'lossList': [0.0, -1.2109827345609665, 0.0, 1.2036797833442687, 0.0, 0.0, 0.0], 'rewardMean': 0.7474168306122885, 'totalEpisodes': 282, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.7178314011237, 'successfulTests': 2
'totalSteps': 24320, 'rewardStep': 0.8948531394081711, 'errorList': [], 'lossList': [0.0, -1.1956488573551178, 0.0, 1.0831943422555923, 0.0, 0.0, 0.0], 'rewardMean': 0.7617513612445654, 'totalEpisodes': 285, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.01152458341938
'totalSteps': 25600, 'rewardStep': 0.8486613362495886, 'errorList': [], 'lossList': [0.0, -1.191413897871971, 0.0, 0.7556017587333917, 0.0, 0.0, 0.0], 'rewardMean': 0.8050189722244123, 'totalEpisodes': 286, 'stepsPerEpisode': 975, 'rewardPerEpisode': 876.502094779972
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=23040, timeSpent=72.96
