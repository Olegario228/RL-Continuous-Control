#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 28
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.8694748391520455, 'errorList': [], 'lossList': [0.0, -1.4334607708454132, 0.0, 25.448313536643983, 0.0, 0.0, 0.0], 'rewardMean': 0.6872070329462152, 'totalEpisodes': 101, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8694748391520455
'totalSteps': 3840, 'rewardStep': 0.6276639813800928, 'errorList': [], 'lossList': [0.0, -1.4292347258329392, 0.0, 37.81850336074829, 0.0, 0.0, 0.0], 'rewardMean': 0.667359349090841, 'totalEpisodes': 147, 'stepsPerEpisode': 15, 'rewardPerEpisode': 9.696224785078144
'totalSteps': 5120, 'rewardStep': 0.8694671106275162, 'errorList': [], 'lossList': [0.0, -1.4133347231149673, 0.0, 49.21305044174194, 0.0, 0.0, 0.0], 'rewardMean': 0.7178862894750099, 'totalEpisodes': 170, 'stepsPerEpisode': 20, 'rewardPerEpisode': 17.694912553106285
'totalSteps': 6400, 'rewardStep': 0.6098221036157256, 'errorList': [], 'lossList': [0.0, -1.4015112459659576, 0.0, 50.53082011222839, 0.0, 0.0, 0.0], 'rewardMean': 0.6962734523031531, 'totalEpisodes': 186, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.820366771986844
'totalSteps': 7680, 'rewardStep': 0.7436899956277084, 'errorList': [], 'lossList': [0.0, -1.402445494532585, 0.0, 52.34714289665222, 0.0, 0.0, 0.0], 'rewardMean': 0.7041762095239122, 'totalEpisodes': 198, 'stepsPerEpisode': 83, 'rewardPerEpisode': 60.718525365267496
'totalSteps': 8960, 'rewardStep': 0.9288268973401783, 'errorList': [], 'lossList': [0.0, -1.4035421550273894, 0.0, 47.73346311569214, 0.0, 0.0, 0.0], 'rewardMean': 0.736269164926236, 'totalEpisodes': 207, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.418285216375578
'totalSteps': 10240, 'rewardStep': 0.7780539219192515, 'errorList': [], 'lossList': [0.0, -1.4017584252357482, 0.0, 23.755976395606993, 0.0, 0.0, 0.0], 'rewardMean': 0.741492259550363, 'totalEpisodes': 213, 'stepsPerEpisode': 76, 'rewardPerEpisode': 59.16794201252012
'totalSteps': 11520, 'rewardStep': 0.7013078097638689, 'errorList': [], 'lossList': [0.0, -1.390511457324028, 0.0, 15.740963230133056, 0.0, 0.0, 0.0], 'rewardMean': 0.737027320685197, 'totalEpisodes': 220, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.328400236171703
'totalSteps': 12800, 'rewardStep': 0.7964660351913857, 'errorList': [], 'lossList': [0.0, -1.3799842190742493, 0.0, 19.850907397270202, 0.0, 0.0, 0.0], 'rewardMean': 0.7429711921358159, 'totalEpisodes': 227, 'stepsPerEpisode': 48, 'rewardPerEpisode': 40.79352419267953
'totalSteps': 14080, 'rewardStep': 0.7646938249123502, 'errorList': [], 'lossList': [0.0, -1.3960183370113373, 0.0, 11.890795369148254, 0.0, 0.0, 0.0], 'rewardMean': 0.7689466519530124, 'totalEpisodes': 232, 'stepsPerEpisode': 150, 'rewardPerEpisode': 124.4564202745576
'totalSteps': 15360, 'rewardStep': 0.5993280933807157, 'errorList': [], 'lossList': [0.0, -1.4094289642572404, 0.0, 10.8732909989357, 0.0, 0.0, 0.0], 'rewardMean': 0.7419319773758793, 'totalEpisodes': 237, 'stepsPerEpisode': 137, 'rewardPerEpisode': 99.39266076846509
'totalSteps': 16640, 'rewardStep': 0.7050242587763033, 'errorList': [], 'lossList': [0.0, -1.4236931252479552, 0.0, 6.88951552271843, 0.0, 0.0, 0.0], 'rewardMean': 0.7496680051155004, 'totalEpisodes': 241, 'stepsPerEpisode': 307, 'rewardPerEpisode': 257.57897973334764
'totalSteps': 17920, 'rewardStep': 0.7790092588332326, 'errorList': [], 'lossList': [0.0, -1.4340405333042145, 0.0, 6.362371671795845, 0.0, 0.0, 0.0], 'rewardMean': 0.740622219936072, 'totalEpisodes': 244, 'stepsPerEpisode': 23, 'rewardPerEpisode': 18.728954930546507
'totalSteps': 19200, 'rewardStep': 0.7232174189869345, 'errorList': [], 'lossList': [0.0, -1.4147082751989364, 0.0, 3.0657964438199996, 0.0, 0.0, 0.0], 'rewardMean': 0.7519617514731929, 'totalEpisodes': 245, 'stepsPerEpisode': 986, 'rewardPerEpisode': 766.759490447593
'totalSteps': 20480, 'rewardStep': 0.8588543754178332, 'errorList': [], 'lossList': [0.0, -1.394489779472351, 0.0, 3.3065016227960586, 0.0, 0.0, 0.0], 'rewardMean': 0.7634781894522054, 'totalEpisodes': 248, 'stepsPerEpisode': 89, 'rewardPerEpisode': 79.72848121695161
'totalSteps': 21760, 'rewardStep': 0.9162564989473975, 'errorList': [], 'lossList': [0.0, -1.3697467875480651, 0.0, 1.716253914386034, 0.0, 0.0, 0.0], 'rewardMean': 0.7622211496129273, 'totalEpisodes': 248, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.7834293515946
'totalSteps': 23040, 'rewardStep': 0.7884287038554205, 'errorList': [], 'lossList': [0.0, -1.3335613882541657, 0.0, 0.8705003389716148, 0.0, 0.0, 0.0], 'rewardMean': 0.7632586278065443, 'totalEpisodes': 248, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.727606446661
'totalSteps': 24320, 'rewardStep': 0.8901952371241586, 'errorList': [], 'lossList': [0.0, -1.2914393424987793, 0.0, 0.8941708599030972, 0.0, 0.0, 0.0], 'rewardMean': 0.7821473705425732, 'totalEpisodes': 248, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.2771009412156
'totalSteps': 25600, 'rewardStep': 0.9535116459095212, 'errorList': [0.031324894551380114, 0.015342528398413404, 0.015598310903008748, 0.020371283797826028, 0.02669024673566998, 0.01554133962395826, 0.022630530929997485, 0.015022753743368698, 0.024482727140914227, 0.022695123160993376, 0.029866643148078246, 0.014988667174278813, 0.014987140619330526, 0.03832146447286517, 0.017185884445486757, 0.018076685381212803, 0.02797960199529151, 0.0255078172373979, 0.016200389414645678, 0.029528686093444993, 0.01946989446456485, 0.029594653755138815, 0.014794515349054153, 0.015286654501978282, 0.01892992886819656, 0.015732500302478285, 0.01564229465852948, 0.016728796124219174, 0.01588153919313516, 0.016772454091921494, 0.016867093533121668, 0.015048914516523662, 0.027083793808605497, 0.025672347479981154, 0.027718608022879353, 0.016413341974649043, 0.026328680091259795, 0.024184753381533516, 0.01539310025183642, 0.01706427580390442, 0.019167260574829544, 0.01949255705439816, 0.022618130345888127, 0.03072966638598712, 0.020300692295286804, 0.021169210601129064, 0.016399133221258896, 0.035170312014462565, 0.03548364775273142, 0.02731691563085918], 'lossList': [0.0, -1.2597970962524414, 0.0, 0.7875446401722729, 0.0, 0.0, 0.0], 'rewardMean': 0.7978519316143868, 'totalEpisodes': 248, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1196.7153844219117, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=75.42
