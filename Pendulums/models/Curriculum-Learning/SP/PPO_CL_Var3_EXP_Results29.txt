#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 29
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.5265751664404702, 'errorList': [], 'lossList': [0.0, -1.4028390431404114, 0.0, 25.37913903236389, 0.0, 0.0, 0.0], 'rewardMean': 0.7238326502959387, 'totalEpisodes': 106, 'stepsPerEpisode': 23, 'rewardPerEpisode': 18.65809989146476
'totalSteps': 3840, 'rewardStep': 0.7849123307572726, 'errorList': [], 'lossList': [0.0, -1.3895184361934663, 0.0, 33.208479013443, 0.0, 0.0, 0.0], 'rewardMean': 0.74419254378305, 'totalEpisodes': 168, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.50868730425373
'totalSteps': 5120, 'rewardStep': 0.8898203425695577, 'errorList': [], 'lossList': [0.0, -1.3795693546533585, 0.0, 41.80059742927551, 0.0, 0.0, 0.0], 'rewardMean': 0.7805994934796769, 'totalEpisodes': 203, 'stepsPerEpisode': 44, 'rewardPerEpisode': 36.744016754076966
'totalSteps': 6400, 'rewardStep': 0.781570520488097, 'errorList': [], 'lossList': [0.0, -1.3742821788787842, 0.0, 43.64929437637329, 0.0, 0.0, 0.0], 'rewardMean': 0.780793698881361, 'totalEpisodes': 221, 'stepsPerEpisode': 36, 'rewardPerEpisode': 29.542565365008084
'totalSteps': 7680, 'rewardStep': 0.7531176622382196, 'errorList': [], 'lossList': [0.0, -1.364187279343605, 0.0, 33.55095668077469, 0.0, 0.0, 0.0], 'rewardMean': 0.7761810261075041, 'totalEpisodes': 230, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.222582048915452
'totalSteps': 8960, 'rewardStep': 0.7760718707187217, 'errorList': [], 'lossList': [0.0, -1.3472719657421113, 0.0, 34.23833721637726, 0.0, 0.0, 0.0], 'rewardMean': 0.7761654324805353, 'totalEpisodes': 237, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.504059658527762
'totalSteps': 10240, 'rewardStep': 0.9408680046993615, 'errorList': [6.7881261226626055, 24.81129130318621, 2.8378422142380706, 1.6817796036455934, 49.20311371716112, 138.25648506720447, 74.67380043452926, 38.77168096023329, 5.280235395291988, 99.81549877645804, 8.114599168315014, 131.55219314401998, 125.03908797748922, 94.34592826312382, 105.31014956879659, 57.32311295319833, 82.4446533233425, 102.3288385430025, 59.17009382321551, 20.563955112419446, 5.086242118415802, 48.105414324246475, 3.688063550140323, 44.29890967434251, 88.54142164823143, 70.60644868352024, 109.81279956540068, 17.84129007463738, 20.401938970816975, 78.45263115426268, 8.309581122090329, 56.82511547141849, 41.153096054063916, 99.51447582876298, 44.92522847240281, 68.42126369274715, 16.35716897656279, 11.394353910800668, 50.470077789247206, 125.78811521765745, 5.098137113669488, 12.351069099069312, 110.37229325293058, 0.6760050338935595, 92.0590555116775, 20.059984335851258, 98.55773071678227, 86.0745636841116, 4.169206582854808, 27.4326228572038], 'lossList': [0.0, -1.3423055464029312, 0.0, 8.776772890090943, 0.0, 0.0, 0.0], 'rewardMean': 0.7967532540078885, 'totalEpisodes': 244, 'stepsPerEpisode': 76, 'rewardPerEpisode': 65.35022959050308, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5324612809330165, 'errorList': [], 'lossList': [0.0, -1.339993765950203, 0.0, 7.983665861487388, 0.0, 0.0, 0.0], 'rewardMean': 0.7673874792217917, 'totalEpisodes': 247, 'stepsPerEpisode': 116, 'rewardPerEpisode': 85.21686773282563
'totalSteps': 12800, 'rewardStep': 0.8734347650336636, 'errorList': [], 'lossList': [0.0, -1.3215792709589005, 0.0, 29.010837545394896, 0.0, 0.0, 0.0], 'rewardMean': 0.7779922078029788, 'totalEpisodes': 253, 'stepsPerEpisode': 64, 'rewardPerEpisode': 58.59838358162451
'totalSteps': 14080, 'rewardStep': 0.7992043000087633, 'errorList': [], 'lossList': [0.0, -1.3034689819812775, 0.0, 6.980879698395729, 0.0, 0.0, 0.0], 'rewardMean': 0.7658036243887143, 'totalEpisodes': 256, 'stepsPerEpisode': 251, 'rewardPerEpisode': 210.6613094300879
'totalSteps': 15360, 'rewardStep': 0.5342994239353165, 'errorList': [], 'lossList': [0.0, -1.277585706114769, 0.0, 48.55259127140045, 0.0, 0.0, 0.0], 'rewardMean': 0.7665760501381991, 'totalEpisodes': 263, 'stepsPerEpisode': 148, 'rewardPerEpisode': 117.4405399704668
'totalSteps': 16640, 'rewardStep': 0.6377151917039582, 'errorList': [], 'lossList': [0.0, -1.2763187140226364, 0.0, 4.742644178271294, 0.0, 0.0, 0.0], 'rewardMean': 0.7518563362328675, 'totalEpisodes': 268, 'stepsPerEpisode': 88, 'rewardPerEpisode': 75.6430402532524
'totalSteps': 17920, 'rewardStep': 0.6976057965044228, 'errorList': [], 'lossList': [0.0, -1.2625004380941391, 0.0, 4.129570544958114, 0.0, 0.0, 0.0], 'rewardMean': 0.732634881626354, 'totalEpisodes': 271, 'stepsPerEpisode': 350, 'rewardPerEpisode': 303.7170973690856
'totalSteps': 19200, 'rewardStep': 0.8170057766211043, 'errorList': [], 'lossList': [0.0, -1.2483865189552308, 0.0, 3.206543555259705, 0.0, 0.0, 0.0], 'rewardMean': 0.7361784072396548, 'totalEpisodes': 275, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8170057766211043
'totalSteps': 20480, 'rewardStep': 0.894132408069298, 'errorList': [], 'lossList': [0.0, -1.2238197576999665, 0.0, 3.23339208483696, 0.0, 0.0, 0.0], 'rewardMean': 0.7502798818227626, 'totalEpisodes': 275, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.8542619003981
'totalSteps': 21760, 'rewardStep': 0.7288259092096245, 'errorList': [], 'lossList': [0.0, -1.20818350315094, 0.0, 2.335010287165642, 0.0, 0.0, 0.0], 'rewardMean': 0.7455552856718529, 'totalEpisodes': 277, 'stepsPerEpisode': 194, 'rewardPerEpisode': 161.47401068278324
'totalSteps': 23040, 'rewardStep': 0.7571939439488898, 'errorList': [], 'lossList': [0.0, -1.1968703496456146, 0.0, 3.796262364387512, 0.0, 0.0, 0.0], 'rewardMean': 0.7271878795968056, 'totalEpisodes': 278, 'stepsPerEpisode': 927, 'rewardPerEpisode': 714.5191284849801
'totalSteps': 24320, 'rewardStep': 0.7981522065813141, 'errorList': [], 'lossList': [0.0, -1.1780100786685943, 0.0, 0.6879334028810262, 0.0, 0.0, 0.0], 'rewardMean': 0.7537569721616354, 'totalEpisodes': 278, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.4087743914827
'totalSteps': 25600, 'rewardStep': 0.9901030097383986, 'errorList': [0.06107174182209693, 0.037103730790538955, 0.03508341304251711, 0.08645041391743746, 0.047952843889001014, 0.06256435203920392, 0.039772346665751496, 0.11676790143357647, 0.05903267812144335, 0.03550379425503323, 0.08255000196790217, 0.04180114148739301, 0.03734066181494024, 0.06775078907305943, 0.034988316290091845, 0.03886387078235877, 0.03919654351321882, 0.08537296591555275, 0.05495058839712421, 0.059241003849574446, 0.03805781376415042, 0.0403811547923165, 0.037834250651561284, 0.03768558731874573, 0.03923877944580414, 0.11927963429788908, 0.03483433826335999, 0.03562306135292507, 0.0369307786849054, 0.03669865248001155, 0.05900892999980075, 0.03762764992299373, 0.09223493410816723, 0.058255054225754256, 0.04143831955823948, 0.03545507119860138, 0.08002162259610605, 0.038349289258415666, 0.04069127322946959, 0.03859854094643664, 0.0362397509491058, 0.03469595592655384, 0.039170222394270746, 0.03622711954348096, 0.03971186758151263, 0.04638929507592313, 0.03876419917754259, 0.06909646622511942, 0.12713754574232167, 0.03762927283137599], 'lossList': [0.0, -1.1598507833480836, 0.0, 0.6934370846673846, 0.0, 0.0, 0.0], 'rewardMean': 0.765423796632109, 'totalEpisodes': 278, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.7310685054822, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=96.06
