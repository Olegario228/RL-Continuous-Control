#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 30
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8024836625627985, 'errorList': [], 'lossList': [0.0, -1.4055092269182206, 0.0, 32.976182765960694, 0.0, 0.0, 0.0], 'rewardMean': 0.7065768807523245, 'totalEpisodes': 59, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.766747907660898
'totalSteps': 3840, 'rewardStep': 0.4622984626741476, 'errorList': [], 'lossList': [0.0, -1.397236726284027, 0.0, 41.444424571990965, 0.0, 0.0, 0.0], 'rewardMean': 0.6251507413929321, 'totalEpisodes': 107, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.0530527499435083
'totalSteps': 5120, 'rewardStep': 0.5933988571046901, 'errorList': [], 'lossList': [0.0, -1.3921454232931136, 0.0, 53.292434272766116, 0.0, 0.0, 0.0], 'rewardMean': 0.6172127703208716, 'totalEpisodes': 150, 'stepsPerEpisode': 18, 'rewardPerEpisode': 10.154675261315527
'totalSteps': 6400, 'rewardStep': 0.8208530453070169, 'errorList': [], 'lossList': [0.0, -1.3833324086666108, 0.0, 39.20616234779358, 0.0, 0.0, 0.0], 'rewardMean': 0.6579408253181007, 'totalEpisodes': 163, 'stepsPerEpisode': 86, 'rewardPerEpisode': 63.90490754050196
'totalSteps': 7680, 'rewardStep': 0.7128399583439584, 'errorList': [], 'lossList': [0.0, -1.3658855825662612, 0.0, 20.504751323461534, 0.0, 0.0, 0.0], 'rewardMean': 0.6670906808224103, 'totalEpisodes': 171, 'stepsPerEpisode': 139, 'rewardPerEpisode': 105.21616375881997
'totalSteps': 8960, 'rewardStep': 0.6298689996451673, 'errorList': [], 'lossList': [0.0, -1.3520078736543655, 0.0, 14.552765872478485, 0.0, 0.0, 0.0], 'rewardMean': 0.6617732977970899, 'totalEpisodes': 176, 'stepsPerEpisode': 224, 'rewardPerEpisode': 166.3194459882979
'totalSteps': 10240, 'rewardStep': 0.1577510754788748, 'errorList': [], 'lossList': [0.0, -1.3641562473773956, 0.0, 10.565138479471207, 0.0, 0.0, 0.0], 'rewardMean': 0.598770520007313, 'totalEpisodes': 179, 'stepsPerEpisode': 222, 'rewardPerEpisode': 125.74356290609785
'totalSteps': 11520, 'rewardStep': 0.7352746497840399, 'errorList': [], 'lossList': [0.0, -1.371956604719162, 0.0, 35.58956536889076, 0.0, 0.0, 0.0], 'rewardMean': 0.6139376455380605, 'totalEpisodes': 183, 'stepsPerEpisode': 68, 'rewardPerEpisode': 60.56313975337369
'totalSteps': 12800, 'rewardStep': 0.9075044884492179, 'errorList': [], 'lossList': [0.0, -1.3565714067220689, 0.0, 13.93984126329422, 0.0, 0.0, 0.0], 'rewardMean': 0.6432943298291762, 'totalEpisodes': 186, 'stepsPerEpisode': 115, 'rewardPerEpisode': 102.51108324823419
'totalSteps': 14080, 'rewardStep': 0.5653120084045019, 'errorList': [], 'lossList': [0.0, -1.3402864587306977, 0.0, 9.966775663495063, 0.0, 0.0, 0.0], 'rewardMean': 0.6387585207754414, 'totalEpisodes': 186, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 962.9622058891109
'totalSteps': 15360, 'rewardStep': 0.6925317992294326, 'errorList': [], 'lossList': [0.0, -1.33488944709301, 0.0, 5.615483375191689, 0.0, 0.0, 0.0], 'rewardMean': 0.6277633344421047, 'totalEpisodes': 188, 'stepsPerEpisode': 264, 'rewardPerEpisode': 224.08793183993166
'totalSteps': 16640, 'rewardStep': 0.8176133574728198, 'errorList': [], 'lossList': [0.0, -1.3253998774290086, 0.0, 4.0544888037443165, 0.0, 0.0, 0.0], 'rewardMean': 0.663294823921972, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 940.1005357922581
'totalSteps': 17920, 'rewardStep': 0.8258692197908096, 'errorList': [], 'lossList': [0.0, -1.2762451875209808, 0.0, 2.387594729065895, 0.0, 0.0, 0.0], 'rewardMean': 0.6865418601905839, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.29164012865
'totalSteps': 19200, 'rewardStep': 0.9112861835389132, 'errorList': [], 'lossList': [0.0, -1.2333541882038117, 0.0, 1.9192653281986713, 0.0, 0.0, 0.0], 'rewardMean': 0.6955851740137736, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.7498408153933
'totalSteps': 20480, 'rewardStep': 0.9544888404605644, 'errorList': [0.045587773112045996, 0.03635710711801987, 0.05179361125554204, 0.09149320099645425, 0.0604235087386855, 0.0740161550531022, 0.03577718447909406, 0.08572161567699799, 0.036506036069893784, 0.06008671290072718, 0.08293364012428955, 0.06128739174383456, 0.07978586133615559, 0.04850146011312534, 0.06186032569359736, 0.08136301650032642, 0.0624582462099055, 0.08514779237147865, 0.04007696951586234, 0.07352407232748845, 0.07597141125839298, 0.04062462375196089, 0.06704521379196607, 0.054090977531839744, 0.041907027877704756, 0.04674827115668651, 0.062420300836638645, 0.0644220821887448, 0.08387119681059381, 0.06600761956369312, 0.03955140797765394, 0.06892021588342871, 0.08226804807899751, 0.06276325781924624, 0.03521797615123983, 0.06791940063304812, 0.07388186544676281, 0.08182019696249752, 0.0807914341128448, 0.05078525867365882, 0.06767220290975572, 0.07714190127832396, 0.06017047750411079, 0.06340653365259906, 0.06818074626676147, 0.06299301633750864, 0.0363820204630224, 0.1021421605533156, 0.0962706902536118, 0.0707482529739946], 'lossList': [0.0, -1.2229765343666077, 0.0, 1.4809220352396368, 0.0, 0.0, 0.0], 'rewardMean': 0.7197500622254343, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.4205667901042, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=64.76
