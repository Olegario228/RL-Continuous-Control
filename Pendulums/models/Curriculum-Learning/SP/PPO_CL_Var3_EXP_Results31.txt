#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 31
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.9660207330916386, 'errorList': [], 'lossList': [0.0, -1.4068424206972123, 0.0, 32.09506495475769, 0.0, 0.0, 0.0], 'rewardMean': 0.9646551425011742, 'totalEpisodes': 61, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.63391401284889
'totalSteps': 3840, 'rewardStep': 0.7145042506264329, 'errorList': [], 'lossList': [0.0, -1.3912183660268784, 0.0, 38.09987189292908, 0.0, 0.0, 0.0], 'rewardMean': 0.8812715118762604, 'totalEpisodes': 122, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.738908607210601
'totalSteps': 5120, 'rewardStep': 0.9109091110581581, 'errorList': [], 'lossList': [0.0, -1.3715529263019561, 0.0, 44.49749697685242, 0.0, 0.0, 0.0], 'rewardMean': 0.8886809116717348, 'totalEpisodes': 166, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.561196275925405
'totalSteps': 6400, 'rewardStep': 0.45619032380268965, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7445173823820531, 'totalEpisodes': 190, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.898334047744169
'totalSteps': 7680, 'rewardStep': 0.78132608997157, 'errorList': [], 'lossList': [0.0, -1.3578356724977494, 0.0, 37.231543545722964, 0.0, 0.0, 0.0], 'rewardMean': 0.7497757691805554, 'totalEpisodes': 200, 'stepsPerEpisode': 32, 'rewardPerEpisode': 28.223237455193054
'totalSteps': 8960, 'rewardStep': 0.7445400735850343, 'errorList': [], 'lossList': [0.0, -1.326866139769554, 0.0, 52.55880479812622, 0.0, 0.0, 0.0], 'rewardMean': 0.7491213072311154, 'totalEpisodes': 214, 'stepsPerEpisode': 111, 'rewardPerEpisode': 89.47323869996237
'totalSteps': 10240, 'rewardStep': 0.5769308112672236, 'errorList': [], 'lossList': [0.0, -1.3193912953138351, 0.0, 29.373305349349977, 0.0, 0.0, 0.0], 'rewardMean': 0.729989029901794, 'totalEpisodes': 218, 'stepsPerEpisode': 158, 'rewardPerEpisode': 118.96099842101923
'totalSteps': 11520, 'rewardStep': 0.7347373678364364, 'errorList': [], 'lossList': [0.0, -1.322592242360115, 0.0, 25.850395498275756, 0.0, 0.0, 0.0], 'rewardMean': 0.7304638636952582, 'totalEpisodes': 225, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.0505296655828
'totalSteps': 12800, 'rewardStep': 0.5968925645293988, 'errorList': [], 'lossList': [0.0, -1.298305140733719, 0.0, 12.398233324289322, 0.0, 0.0, 0.0], 'rewardMean': 0.6938241649571272, 'totalEpisodes': 230, 'stepsPerEpisode': 162, 'rewardPerEpisode': 117.96390716232585
'totalSteps': 14080, 'rewardStep': 0.609473396515509, 'errorList': [], 'lossList': [0.0, -1.2913446837663651, 0.0, 7.9576058208942415, 0.0, 0.0, 0.0], 'rewardMean': 0.6581694312995142, 'totalEpisodes': 235, 'stepsPerEpisode': 323, 'rewardPerEpisode': 238.4859784803497
'totalSteps': 15360, 'rewardStep': 0.950967508777621, 'errorList': [4.128058092523345, 1.0286160305811154, 8.64704049882133, 0.18725077443927327, 11.088339821551319, 3.7853680801825504, 4.909572930744556, 2.8544971277298585, 5.140106895872478, 7.449872916113239, 4.440164491863782, 0.18975181785429834, 3.3087005984402147, 2.958855700316089, 11.196438331031764, 4.236878306505132, 2.016633683458567, 2.427259311491114, 3.7536438265394416, 1.0894519312412771, 1.9514041369340338, 0.4050958332489437, 2.8459793556463633, 1.8774555999562623, 1.2089347364991299, 3.4936073316455487, 8.916883944412834, 4.228730603830378, 1.201443371675639, 0.2156261989051704, 4.38934804041842, 7.798790902150491, 2.108706763754842, 1.3463770411587854, 0.20593775067127376, 6.234845588224747, 7.705890938593448, 2.3802190212779255, 3.4053568510913017, 4.122555180015838, 5.428534365124869, 7.021440958412982, 4.1174010443458755, 1.8427313575356628, 2.0907260338555504, 0.3437684074417938, 6.5784111056666985, 1.6150206103434441, 3.6527367746024892, 4.671297268556586], 'lossList': [0.0, -1.2943262767791748, 0.0, 6.371558272242546, 0.0, 0.0, 0.0], 'rewardMean': 0.681815757114633, 'totalEpisodes': 239, 'stepsPerEpisode': 118, 'rewardPerEpisode': 104.46493556275537, 'successfulTests': 2
'totalSteps': 16640, 'rewardStep': 0.885529347755518, 'errorList': [], 'lossList': [0.0, -1.2775732284784318, 0.0, 24.859974250793456, 0.0, 0.0, 0.0], 'rewardMean': 0.6792777807843691, 'totalEpisodes': 242, 'stepsPerEpisode': 69, 'rewardPerEpisode': 60.107910281554396
'totalSteps': 17920, 'rewardStep': 0.8833335626126899, 'errorList': [], 'lossList': [0.0, -1.2653857362270355, 0.0, 6.183143779039383, 0.0, 0.0, 0.0], 'rewardMean': 0.721992104665369, 'totalEpisodes': 243, 'stepsPerEpisode': 587, 'rewardPerEpisode': 504.41022566491955
'totalSteps': 19200, 'rewardStep': 0.9890796110024713, 'errorList': [0.03159491079917002, 0.15594035282569707, 0.015036586706984656, 0.023447844702759183, 0.11125283429030894, 0.055948293023217126, 0.08562035268853972, 0.13331839833747952, 0.2130516170825185, 0.17335957763503831, 0.08967066281523386, 0.22851121008148, 0.14800478569355532, 0.021000121075438333, 0.03502822232043396, 0.13062626358986276, 0.007687356413840772, 0.15321966132406517, 0.09981960743740441, 0.10425637613319032, 0.06610673915793507, 0.09906625824463215, 0.035068325421425094, 0.10494466160701363, 0.09048315526426866, 0.01251106864455475, 0.19953839626994282, 0.12775111794893176, 0.09645335964074603, 0.12552610719841945, 0.06476891086424455, 0.10556515329823751, 0.215121852725427, 0.1238951955171175, 0.15748063607679855, 0.06540368701694332, 0.16683475290314, 0.0036055738134142517, 0.22713489906887568, 0.1627234159353783, 0.05870096122544796, 0.21108731072637937, 0.08280017611335562, 0.15531035621009268, 0.017232965465455493, 0.11846348049876723, 0.11807648999949256, 0.2512156044653169, 0.10746366379119295, 0.0261244653968762], 'lossList': [0.0, -1.25513012945652, 0.0, 4.687064582109452, 0.0, 0.0, 0.0], 'rewardMean': 0.7752810333853473, 'totalEpisodes': 244, 'stepsPerEpisode': 500, 'rewardPerEpisode': 387.1876703683185, 'successfulTests': 44
'totalSteps': 20480, 'rewardStep': 0.7188917564495989, 'errorList': [], 'lossList': [0.0, -1.2315778511762618, 0.0, 3.7869524037837983, 0.0, 0.0, 0.0], 'rewardMean': 0.7690376000331501, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.4171853920395
'totalSteps': 21760, 'rewardStep': 0.5956451219796425, 'errorList': [], 'lossList': [0.0, -1.2165891581773758, 0.0, 3.764249938726425, 0.0, 0.0, 0.0], 'rewardMean': 0.754148104872611, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 983.5396053551999
'totalSteps': 23040, 'rewardStep': 0.9179648987020758, 'errorList': [], 'lossList': [0.0, -1.2079514640569686, 0.0, 1.3283133809268475, 0.0, 0.0, 0.0], 'rewardMean': 0.7882515136160962, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.3870638429353
'totalSteps': 24320, 'rewardStep': 0.7791293958828394, 'errorList': [], 'lossList': [0.0, -1.1736325293779373, 0.0, 0.8357242107391357, 0.0, 0.0, 0.0], 'rewardMean': 0.7926907164207365, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.9144178018655
'totalSteps': 25600, 'rewardStep': 0.985088860077283, 'errorList': [0.05098647865866205, 0.07898021882968534, 0.05693229263672632, 0.04706108211910321, 0.051193512073411766, 0.04524936502851176, 0.08258433402063782, 0.06054561665256708, 0.05099604546348227, 0.16035708688809025, 0.04964387654304225, 0.046921587228431795, 0.0970035907329162, 0.049601869395626254, 0.0507494553912493, 0.054758677397626604, 0.044620239239853046, 0.05138002564151538, 0.04843277990553659, 0.047624121460141645, 0.05067812617156949, 0.05098381064042899, 0.089215581571642, 0.08302309714490588, 0.05073969974923316, 0.06946486343169958, 0.044578849059609287, 0.04820955160558157, 0.05097930894308184, 0.05290148820109899, 0.08022612463038056, 0.05338804711107556, 0.053211307412391086, 0.050813761816253446, 0.07929633433737866, 0.056070457645971024, 0.053369853655935544, 0.06230137954060524, 0.06170125830510125, 0.047828625275308165, 0.046158708753525825, 0.0512427147376022, 0.045818040754003725, 0.04838544765617956, 0.05204605386145114, 0.04737760432055914, 0.051301378848112825, 0.0510356367031284, 0.05747394658439575, 0.04977301114417928], 'lossList': [0.0, -1.108447603583336, 0.0, 0.866738806925714, 0.0, 0.0, 0.0], 'rewardMean': 0.831510345975525, 'totalEpisodes': 244, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.740850030552, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=110.03
