#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 32
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.6591489221803429, 'errorList': [], 'lossList': [0.0, -1.4026945447921753, 0.0, 34.57563591003418, 0.0, 0.0, 0.0], 'rewardMean': 0.7264594616802471, 'totalEpisodes': 63, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.296155408162948
'totalSteps': 3840, 'rewardStep': 0.5296313992632249, 'errorList': [], 'lossList': [0.0, -1.3872868126630784, 0.0, 43.57723355293274, 0.0, 0.0, 0.0], 'rewardMean': 0.6608501075412397, 'totalEpisodes': 118, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.528487755565733
'totalSteps': 5120, 'rewardStep': 0.5850151019475317, 'errorList': [], 'lossList': [0.0, -1.3833628189563751, 0.0, 49.21928413391113, 0.0, 0.0, 0.0], 'rewardMean': 0.6418913561428127, 'totalEpisodes': 158, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.2980417184260102
'totalSteps': 6400, 'rewardStep': 0.5650475426537925, 'errorList': [], 'lossList': [0.0, -1.372885667681694, 0.0, 33.30961152076721, 0.0, 0.0, 0.0], 'rewardMean': 0.6265225934450086, 'totalEpisodes': 176, 'stepsPerEpisode': 26, 'rewardPerEpisode': 18.39158623639455
'totalSteps': 7680, 'rewardStep': 0.9103310073648725, 'errorList': [], 'lossList': [0.0, -1.36632967710495, 0.0, 53.79524956703186, 0.0, 0.0, 0.0], 'rewardMean': 0.673823995764986, 'totalEpisodes': 188, 'stepsPerEpisode': 55, 'rewardPerEpisode': 38.53850906880915
'totalSteps': 8960, 'rewardStep': 0.5640945403730925, 'errorList': [], 'lossList': [0.0, -1.365944442152977, 0.0, 38.511128010749815, 0.0, 0.0, 0.0], 'rewardMean': 0.6581483592804298, 'totalEpisodes': 194, 'stepsPerEpisode': 134, 'rewardPerEpisode': 106.46831326639071
'totalSteps': 10240, 'rewardStep': 0.604138211045463, 'errorList': [], 'lossList': [0.0, -1.364410725235939, 0.0, 16.2802630931139, 0.0, 0.0, 0.0], 'rewardMean': 0.6513970907510589, 'totalEpisodes': 197, 'stepsPerEpisode': 106, 'rewardPerEpisode': 88.94210524098415
'totalSteps': 11520, 'rewardStep': 0.8287662596864983, 'errorList': [], 'lossList': [0.0, -1.3801781237125397, 0.0, 10.482981922626495, 0.0, 0.0, 0.0], 'rewardMean': 0.6711047761883299, 'totalEpisodes': 199, 'stepsPerEpisode': 808, 'rewardPerEpisode': 633.0121283367143
'totalSteps': 12800, 'rewardStep': 0.9618027333312266, 'errorList': [0.13702737305649768, 0.26907682007560435, 0.18187920115134268, 0.16841069274575896, 0.299258119229962, 0.11009928292443678, 0.28860631065025744, 0.1823584747736809, 0.09890505593375198, 0.1396842417184761, 0.4232714693524224, 0.5103488292531612, 0.2374755877919363, 0.09451827700382713, 0.1524373910437317, 0.30283870796036094, 0.5282040519103521, 0.13558305209516439, 0.6087379802266554, 0.4831015678750455, 0.2743716451920796, 0.27093238436709943, 0.1520720170110093, 0.14594989344747808, 0.3102441854932945, 0.16851446097588413, 0.09061228430191902, 0.0949063229729091, 0.20695141908897524, 0.4044680475567746, 0.4921832414411067, 0.1418077293288891, 0.4569462796049966, 0.0845250235647636, 0.5892019856998931, 0.2366445882223346, 0.3748762799044576, 0.14335077295015478, 0.20342749014321718, 0.046535213206486604, 0.4810157840602212, 0.22514550896353358, 0.2655755302572779, 0.3192573988915139, 0.19587348254958925, 0.19837687270693607, 0.05437020721492016, 0.2398254075420754, 0.2388982882273146, 0.07597125232733382], 'lossList': [0.0, -1.357569990158081, 0.0, 39.50432067871094, 0.0, 0.0, 0.0], 'rewardMean': 0.7001745719026196, 'totalEpisodes': 203, 'stepsPerEpisode': 371, 'rewardPerEpisode': 312.4297949440971, 'successfulTests': 23
'totalSteps': 14080, 'rewardStep': 0.8069948738440705, 'errorList': [], 'lossList': [0.0, -1.3420552879571914, 0.0, 6.261114285290241, 0.0, 0.0, 0.0], 'rewardMean': 0.7014970591690115, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1039.423211275117
'totalSteps': 15360, 'rewardStep': 0.5968453872930521, 'errorList': [], 'lossList': [0.0, -1.3440097218751907, 0.0, 56.066003365516664, 0.0, 0.0, 0.0], 'rewardMean': 0.6952667056802825, 'totalEpisodes': 205, 'stepsPerEpisode': 309, 'rewardPerEpisode': 215.97446176812278
'totalSteps': 16640, 'rewardStep': 0.9283035017940982, 'errorList': [], 'lossList': [0.0, -1.3589876037836075, 0.0, 50.51576986074448, 0.0, 0.0, 0.0], 'rewardMean': 0.7351339159333697, 'totalEpisodes': 207, 'stepsPerEpisode': 732, 'rewardPerEpisode': 514.6601197131336
'totalSteps': 17920, 'rewardStep': 0.8806609432463608, 'errorList': [], 'lossList': [0.0, -1.3597069323062896, 0.0, 27.078008499741554, 0.0, 0.0, 0.0], 'rewardMean': 0.7646985000632528, 'totalEpisodes': 208, 'stepsPerEpisode': 520, 'rewardPerEpisode': 378.9868335331439
'totalSteps': 19200, 'rewardStep': 0.9607474288267652, 'errorList': [0.39120584276234455, 0.238393357743644, 0.2259014798415989, 0.32994404451104054, 0.2176073747250742, 0.2282241084050249, 0.21803627775573178, 0.2885583161847395, 0.298801845116323, 0.29234563893043575, 0.24565432152903305, 0.3025084079766793, 0.3459375061461495, 0.3446688478230543, 0.27794280792600634, 0.28260958274898174, 0.25090559193264705, 0.25777964025487965, 0.2568707508803362, 0.24953926693261821, 0.32885768058650977, 0.29599014337453244, 0.2925985891442395, 0.25552444495252125, 0.20328354303949117, 0.26989667441649495, 0.25011560844399655, 0.2310033435763517, 0.33777789858441537, 0.26654022592481996, 0.3097280188678862, 0.28843497773224047, 0.355456856205184, 0.25866669872523523, 0.2278797797285034, 0.26871111499401473, 0.28051994746456027, 0.2905500490152866, 0.2784635022249062, 0.3274240744660919, 0.2507530612613576, 0.24104010933709002, 0.22156878835149146, 0.24366684368060276, 0.4194521547132858, 0.2592715604127626, 0.3371682484990739, 0.2353574819507026, 0.2710582709790526, 0.4080334769229039], 'lossList': [0.0, -1.3639302068948747, 0.0, 13.812147143483163, 0.0, 0.0, 0.0], 'rewardMean': 0.80426848868055, 'totalEpisodes': 209, 'stepsPerEpisode': 838, 'rewardPerEpisode': 680.7749904255134, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8125552770161912, 'errorList': [], 'lossList': [0.0, -1.3491459208726884, 0.0, 1.634475199431181, 0.0, 0.0, 0.0], 'rewardMean': 0.7944909156456819, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.2632650605707
'totalSteps': 21760, 'rewardStep': 0.6887154406443183, 'errorList': [], 'lossList': [0.0, -1.3377088445425034, 0.0, 1.189305177628994, 0.0, 0.0, 0.0], 'rewardMean': 0.8069530056728043, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 937.4061413556848
'totalSteps': 23040, 'rewardStep': 0.8396594155404334, 'errorList': [], 'lossList': [0.0, -1.3144716691970826, 0.0, 0.46524798616766927, 0.0, 0.0, 0.0], 'rewardMean': 0.8305051261223015, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 992.2755551212957
'totalSteps': 24320, 'rewardStep': 0.785921894898953, 'errorList': [], 'lossList': [0.0, -1.2881408375501633, 0.0, 0.8301934034749866, 0.0, 0.0, 0.0], 'rewardMean': 0.8262206896435469, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.069529186295
'totalSteps': 25600, 'rewardStep': 0.5771886526979122, 'errorList': [], 'lossList': [0.0, -1.265190868973732, 0.0, 0.5068714743852616, 0.0, 0.0, 0.0], 'rewardMean': 0.7877592815802156, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.6529163139155
#maxSuccessfulTests=23, maxSuccessfulTestsAtStep=12800, timeSpent=94.31
