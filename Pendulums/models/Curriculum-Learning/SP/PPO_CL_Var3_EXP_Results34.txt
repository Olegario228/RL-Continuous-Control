#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 34
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.6746502925382224, 'errorList': [], 'lossList': [0.0, -1.4030853879451752, 0.0, 32.521233854293826, 0.0, 0.0, 0.0], 'rewardMean': 0.5193601991569055, 'totalEpisodes': 70, 'stepsPerEpisode': 18, 'rewardPerEpisode': 12.534311446864209
'totalSteps': 3840, 'rewardStep': 0.3366146433662166, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4279874212615611, 'totalEpisodes': 134, 'stepsPerEpisode': 40, 'rewardPerEpisode': 23.674727398074722
'totalSteps': 5120, 'rewardStep': 0.7935564137013776, 'errorList': [], 'lossList': [0.0, -1.386728410720825, 0.0, 40.90963569641113, 0.0, 0.0, 0.0], 'rewardMean': 0.5011012197495244, 'totalEpisodes': 198, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7935564137013776
'totalSteps': 6400, 'rewardStep': 0.8995744056698124, 'errorList': [], 'lossList': [0.0, -1.3674358004331588, 0.0, 43.29765950202942, 0.0, 0.0, 0.0], 'rewardMean': 0.5675134174029057, 'totalEpisodes': 231, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.439881269035896
'totalSteps': 7680, 'rewardStep': 0.7103566337709265, 'errorList': [], 'lossList': [0.0, -1.3549008071422577, 0.0, 49.147833528518674, 0.0, 0.0, 0.0], 'rewardMean': 0.5879195911697659, 'totalEpisodes': 251, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.951031749501865
'totalSteps': 8960, 'rewardStep': 0.5115951584297961, 'errorList': [], 'lossList': [0.0, -1.3444978427886962, 0.0, 38.30851815700531, 0.0, 0.0, 0.0], 'rewardMean': 0.5783790370772696, 'totalEpisodes': 261, 'stepsPerEpisode': 148, 'rewardPerEpisode': 101.7393134630275
'totalSteps': 10240, 'rewardStep': 0.8747754116558285, 'errorList': [], 'lossList': [0.0, -1.3238534671068192, 0.0, 32.94120799064636, 0.0, 0.0, 0.0], 'rewardMean': 0.6113119675859983, 'totalEpisodes': 268, 'stepsPerEpisode': 81, 'rewardPerEpisode': 65.03024752858335
'totalSteps': 11520, 'rewardStep': 0.5079243336569865, 'errorList': [], 'lossList': [0.0, -1.30575348675251, 0.0, 9.135332697629929, 0.0, 0.0, 0.0], 'rewardMean': 0.6009732041930971, 'totalEpisodes': 274, 'stepsPerEpisode': 124, 'rewardPerEpisode': 93.08860947847349
'totalSteps': 12800, 'rewardStep': 0.8845754448539142, 'errorList': [], 'lossList': [0.0, -1.2992806231975556, 0.0, 34.76114772796631, 0.0, 0.0, 0.0], 'rewardMean': 0.6530237381009297, 'totalEpisodes': 283, 'stepsPerEpisode': 66, 'rewardPerEpisode': 57.900531408199804
'totalSteps': 14080, 'rewardStep': 0.6906001228682083, 'errorList': [], 'lossList': [0.0, -1.305358253121376, 0.0, 9.321701309084892, 0.0, 0.0, 0.0], 'rewardMean': 0.6546187211339283, 'totalEpisodes': 287, 'stepsPerEpisode': 282, 'rewardPerEpisode': 221.21996943709513
'totalSteps': 15360, 'rewardStep': 0.8156747054961727, 'errorList': [], 'lossList': [0.0, -1.3143959456682206, 0.0, 37.38456326007843, 0.0, 0.0, 0.0], 'rewardMean': 0.702524727346924, 'totalEpisodes': 295, 'stepsPerEpisode': 208, 'rewardPerEpisode': 166.32394879891976
'totalSteps': 16640, 'rewardStep': 0.4973198064006169, 'errorList': [], 'lossList': [0.0, -1.3019438290596008, 0.0, 16.43145133495331, 0.0, 0.0, 0.0], 'rewardMean': 0.718595243650364, 'totalEpisodes': 300, 'stepsPerEpisode': 125, 'rewardPerEpisode': 98.37172715305924
'totalSteps': 17920, 'rewardStep': 0.6514127487118021, 'errorList': [], 'lossList': [0.0, -1.273072761297226, 0.0, 6.684784977436066, 0.0, 0.0, 0.0], 'rewardMean': 0.7043808771514064, 'totalEpisodes': 302, 'stepsPerEpisode': 274, 'rewardPerEpisode': 222.14271479463238
'totalSteps': 19200, 'rewardStep': 0.6469424436407112, 'errorList': [], 'lossList': [0.0, -1.2392717278003693, 0.0, 4.818625287413597, 0.0, 0.0, 0.0], 'rewardMean': 0.6791176809484962, 'totalEpisodes': 302, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.9525378087635
'totalSteps': 20480, 'rewardStep': 0.8519159847094536, 'errorList': [], 'lossList': [0.0, -1.2084867948293685, 0.0, 3.2373644602298737, 0.0, 0.0, 0.0], 'rewardMean': 0.6932736160423489, 'totalEpisodes': 302, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.8910062916748
'totalSteps': 21760, 'rewardStep': 0.8596532714702891, 'errorList': [], 'lossList': [0.0, -1.1799905443191527, 0.0, 2.142178721949458, 0.0, 0.0, 0.0], 'rewardMean': 0.7280794273463984, 'totalEpisodes': 302, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.7306311261575
'totalSteps': 23040, 'rewardStep': 0.9153573719981654, 'errorList': [], 'lossList': [0.0, -1.1569798439741135, 0.0, 1.4440770369023084, 0.0, 0.0, 0.0], 'rewardMean': 0.732137623380632, 'totalEpisodes': 302, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.827189985942
'totalSteps': 24320, 'rewardStep': 0.9176102603262561, 'errorList': [], 'lossList': [0.0, -1.143606733083725, 0.0, 1.4953080574609339, 0.0, 0.0, 0.0], 'rewardMean': 0.773106216047559, 'totalEpisodes': 302, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1195.8889434040555
'totalSteps': 25600, 'rewardStep': 0.9887324156509497, 'errorList': [0.021356305014591574, 0.014569321705632726, 0.052864064727681515, 0.05429080404567131, 0.03286092197256729, 0.013405435871682796, 0.03220420081647327, 0.016799742246114396, 0.003997663328872039, 0.0037653037849169164, 0.02760002767826556, 0.04137805593938188, 0.022224954035796336, 0.018810977625084626, 0.006951510986234317, 0.025783350898455043, 0.026876570798924634, 0.04083045694115174, 0.052080606659431275, 0.01589312839525466, 0.004556898460849571, 0.010128166140549767, 0.02980824068989073, 0.014343814199547136, 0.016601131960473796, 0.027193853553967477, 0.006240398303823649, 0.03333938340576273, 0.04085204296894279, 0.0706800846575878, 0.057619742888388586, 0.0060404231668201094, 0.01305310506001432, 0.005839133688314327, 0.050631346964944104, 0.0267033293344707, 0.010889292501292213, 0.012772242766336775, 0.04103152085103965, 0.00738844312164458, 0.0036358590334428947, 0.05284584443912738, 0.011273902547221396, 0.040718308284517474, 0.017555096339318045, 0.012007410402655204, 0.001803866032536671, 0.006139137130031737, 0.024113578941250206, 0.011269576014766138], 'lossList': [0.0, -1.1200246018171311, 0.0, 1.2477511925064027, 0.0, 0.0, 0.0], 'rewardMean': 0.7835219131272626, 'totalEpisodes': 302, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1218.137443972906, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=76.7
