#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 35
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.6975312718446752, 'errorList': [], 'lossList': [0.0, -1.411143224835396, 0.0, 31.15531442642212, 0.0, 0.0, 0.0], 'rewardMean': 0.7486651161718192, 'totalEpisodes': 44, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.643443917453622
'totalSteps': 3840, 'rewardStep': 0.519936941187337, 'errorList': [], 'lossList': [0.0, -1.4060428822040558, 0.0, 42.41882816314697, 0.0, 0.0, 0.0], 'rewardMean': 0.6724223911769918, 'totalEpisodes': 107, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.933940655142428
'totalSteps': 5120, 'rewardStep': 0.8213667589507637, 'errorList': [], 'lossList': [0.0, -1.3941512393951416, 0.0, 40.6288002204895, 0.0, 0.0, 0.0], 'rewardMean': 0.7096584831204348, 'totalEpisodes': 143, 'stepsPerEpisode': 69, 'rewardPerEpisode': 55.221762537167066
'totalSteps': 6400, 'rewardStep': 0.5549350893399074, 'errorList': [], 'lossList': [0.0, -1.3779181742668152, 0.0, 43.17460056304932, 0.0, 0.0, 0.0], 'rewardMean': 0.6787138043643293, 'totalEpisodes': 158, 'stepsPerEpisode': 16, 'rewardPerEpisode': 8.217547890729652
'totalSteps': 7680, 'rewardStep': 0.7870928609577968, 'errorList': [], 'lossList': [0.0, -1.3590294182300569, 0.0, 34.791066579818725, 0.0, 0.0, 0.0], 'rewardMean': 0.6967769804632405, 'totalEpisodes': 165, 'stepsPerEpisode': 192, 'rewardPerEpisode': 145.89300422558526
'totalSteps': 8960, 'rewardStep': 0.8082853359641347, 'errorList': [], 'lossList': [0.0, -1.3460731625556945, 0.0, 23.442190591096878, 0.0, 0.0, 0.0], 'rewardMean': 0.7127067455347967, 'totalEpisodes': 170, 'stepsPerEpisode': 269, 'rewardPerEpisode': 190.82951501298322
'totalSteps': 10240, 'rewardStep': 0.6022991795612814, 'errorList': [], 'lossList': [0.0, -1.3475783634185792, 0.0, 26.53236691713333, 0.0, 0.0, 0.0], 'rewardMean': 0.6989057997881074, 'totalEpisodes': 173, 'stepsPerEpisode': 109, 'rewardPerEpisode': 83.09753626288715
'totalSteps': 11520, 'rewardStep': 0.4730662106251064, 'errorList': [], 'lossList': [0.0, -1.3350154733657837, 0.0, 25.903776520490645, 0.0, 0.0, 0.0], 'rewardMean': 0.6738125121033295, 'totalEpisodes': 175, 'stepsPerEpisode': 68, 'rewardPerEpisode': 45.94231142021324
'totalSteps': 12800, 'rewardStep': 0.4152557449532357, 'errorList': [], 'lossList': [0.0, -1.3013658356666564, 0.0, 24.053674585819245, 0.0, 0.0, 0.0], 'rewardMean': 0.64795683538832, 'totalEpisodes': 176, 'stepsPerEpisode': 816, 'rewardPerEpisode': 579.9048963153638
'totalSteps': 14080, 'rewardStep': 0.8428934274839687, 'errorList': [], 'lossList': [0.0, -1.2650193107128143, 0.0, 9.634079135507346, 0.0, 0.0, 0.0], 'rewardMean': 0.6522662820868206, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.1524820718826
'totalSteps': 15360, 'rewardStep': 0.40918908085640954, 'errorList': [], 'lossList': [0.0, -1.2237736123800278, 0.0, 12.635136476755143, 0.0, 0.0, 0.0], 'rewardMean': 0.623432062987994, 'totalEpisodes': 178, 'stepsPerEpisode': 597, 'rewardPerEpisode': 487.27137841305586
'totalSteps': 16640, 'rewardStep': 0.7805496541458142, 'errorList': [], 'lossList': [0.0, -1.2176171362400054, 0.0, 2.5058369188010694, 0.0, 0.0, 0.0], 'rewardMean': 0.6494933342838418, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 984.6935681555585
'totalSteps': 17920, 'rewardStep': 0.9003966662985512, 'errorList': [], 'lossList': [0.0, -1.2012074065208436, 0.0, 2.5317594176530838, 0.0, 0.0, 0.0], 'rewardMean': 0.6573963250186207, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.5409098908608
'totalSteps': 19200, 'rewardStep': 0.8637816252257347, 'errorList': [], 'lossList': [0.0, -1.1738068389892578, 0.0, 1.1269195214658976, 0.0, 0.0, 0.0], 'rewardMean': 0.6882809786072033, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.7860538535335
'totalSteps': 20480, 'rewardStep': 0.9003743877489303, 'errorList': [], 'lossList': [0.0, -1.1575808089971542, 0.0, 0.8466551113128662, 0.0, 0.0, 0.0], 'rewardMean': 0.6996091312863166, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.3531751899325
'totalSteps': 21760, 'rewardStep': 0.8860378363264384, 'errorList': [], 'lossList': [0.0, -1.1353919154405594, 0.0, 0.8776618536934256, 0.0, 0.0, 0.0], 'rewardMean': 0.707384381322547, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.8824927664139
'totalSteps': 23040, 'rewardStep': 0.9085667389163593, 'errorList': [], 'lossList': [0.0, -1.1027714258432388, 0.0, 0.8748949814774096, 0.0, 0.0, 0.0], 'rewardMean': 0.7380111372580549, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.058106710904
'totalSteps': 24320, 'rewardStep': 0.7913688434576435, 'errorList': [], 'lossList': [0.0, -1.0631599402427674, 0.0, 0.5480126432701945, 0.0, 0.0, 0.0], 'rewardMean': 0.7698414005413087, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1194.7248699286772
'totalSteps': 25600, 'rewardStep': 0.9654063644521487, 'errorList': [0.10691681870226911, 0.056944123623242915, 0.058549005507250014, 0.04994626480749854, 0.048100760796728856, 0.09389904577355078, 0.08373829406150031, 0.0775288730996107, 0.05948835063294646, 0.06707142268614343, 0.0633087335507972, 0.08538016545597601, 0.06080396214911388, 0.052850505784721336, 0.04661119905270486, 0.04063068749486838, 0.14608651960895183, 0.03322315890332651, 0.06890667397751665, 0.0296885484821501, 0.026822404224616632, 0.01710715908524333, 0.022284398426383092, 0.05107041504199376, 0.12422925760926526, 0.09740878236270971, 0.1312334528262394, 0.06082385994785679, 0.07947943318102525, 0.02000078241763751, 0.07622069400884526, 0.025718803329781933, 0.040484123105940045, 0.048252888369008554, 0.05100778124899988, 0.048480089511270356, 0.08619714465627983, 0.07464961519319951, 0.024059242181160602, 0.0373656952671703, 0.05932027347648347, 0.030252964370208545, 0.0735364599988241, 0.056593308041988366, 0.032434387365121754, 0.02325580096406714, 0.02421303377419329, 0.058212614845164545, 0.07409587036129744, 0.04146861918258232], 'lossList': [0.0, -1.0252582049369812, 0.0, 0.49636594377458093, 0.0, 0.0, 0.0], 'rewardMean': 0.8248564624911998, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1212.3446444855906, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=75.87
