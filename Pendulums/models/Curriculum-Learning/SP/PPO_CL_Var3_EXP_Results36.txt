#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 36
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.889910499907899, 'errorList': [], 'lossList': [0.0, -1.4242242187261582, 0.0, 31.42001588821411, 0.0, 0.0, 0.0], 'rewardMean': 0.7033522008036759, 'totalEpisodes': 64, 'stepsPerEpisode': 41, 'rewardPerEpisode': 29.376617868145544
'totalSteps': 3840, 'rewardStep': 0.9166595999760747, 'errorList': [], 'lossList': [0.0, -1.4150006920099258, 0.0, 33.710677194595334, 0.0, 0.0, 0.0], 'rewardMean': 0.7744546671944755, 'totalEpisodes': 132, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9166595999760747
'totalSteps': 5120, 'rewardStep': 0.5833507842548374, 'errorList': [], 'lossList': [0.0, -1.4031590348482132, 0.0, 40.95862850189209, 0.0, 0.0, 0.0], 'rewardMean': 0.726678696459566, 'totalEpisodes': 168, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.539303876629532
'totalSteps': 6400, 'rewardStep': 0.6803626215934863, 'errorList': [], 'lossList': [0.0, -1.3913658660650254, 0.0, 50.77305164337158, 0.0, 0.0, 0.0], 'rewardMean': 0.71741548148635, 'totalEpisodes': 187, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.84461997399933
'totalSteps': 7680, 'rewardStep': 0.6733895394163566, 'errorList': [], 'lossList': [0.0, -1.3832365876436234, 0.0, 41.80205480575562, 0.0, 0.0, 0.0], 'rewardMean': 0.7100778244746845, 'totalEpisodes': 195, 'stepsPerEpisode': 214, 'rewardPerEpisode': 176.53847055255108
'totalSteps': 8960, 'rewardStep': 0.7829286954130681, 'errorList': [], 'lossList': [0.0, -1.37847740650177, 0.0, 27.88618970870972, 0.0, 0.0, 0.0], 'rewardMean': 0.7204850917515964, 'totalEpisodes': 204, 'stepsPerEpisode': 133, 'rewardPerEpisode': 110.36115011315418
'totalSteps': 10240, 'rewardStep': 0.7577530630915026, 'errorList': [], 'lossList': [0.0, -1.3607239425182343, 0.0, 15.897094082832336, 0.0, 0.0, 0.0], 'rewardMean': 0.7251435881690846, 'totalEpisodes': 210, 'stepsPerEpisode': 55, 'rewardPerEpisode': 47.684780475721546
'totalSteps': 11520, 'rewardStep': 0.6342750285563489, 'errorList': [], 'lossList': [0.0, -1.3553553378582002, 0.0, 12.422885964512824, 0.0, 0.0, 0.0], 'rewardMean': 0.7150470815454474, 'totalEpisodes': 214, 'stepsPerEpisode': 65, 'rewardPerEpisode': 54.74098985953705
'totalSteps': 12800, 'rewardStep': 0.49367784155104677, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6905985515311667, 'totalEpisodes': 220, 'stepsPerEpisode': 182, 'rewardPerEpisode': 116.93736045822935
'totalSteps': 14080, 'rewardStep': 0.8651178720565132, 'errorList': [], 'lossList': [0.0, -1.352214987874031, 0.0, 14.069824489355087, 0.0, 0.0, 0.0], 'rewardMean': 0.688119288746028, 'totalEpisodes': 224, 'stepsPerEpisode': 98, 'rewardPerEpisode': 86.37896996431144
'totalSteps': 15360, 'rewardStep': 0.6142563423768078, 'errorList': [], 'lossList': [0.0, -1.3564141315221787, 0.0, 5.147266499400139, 0.0, 0.0, 0.0], 'rewardMean': 0.6578789629861015, 'totalEpisodes': 228, 'stepsPerEpisode': 204, 'rewardPerEpisode': 162.2280257732053
'totalSteps': 16640, 'rewardStep': 0.5204973655377718, 'errorList': [], 'lossList': [0.0, -1.363375797867775, 0.0, 5.463543259501457, 0.0, 0.0, 0.0], 'rewardMean': 0.6515936211143949, 'totalEpisodes': 231, 'stepsPerEpisode': 299, 'rewardPerEpisode': 235.44797380788737
'totalSteps': 17920, 'rewardStep': 0.6965489091616515, 'errorList': [], 'lossList': [0.0, -1.3534250420331955, 0.0, 2.662056007385254, 0.0, 0.0, 0.0], 'rewardMean': 0.6532122498712114, 'totalEpisodes': 236, 'stepsPerEpisode': 110, 'rewardPerEpisode': 91.00105608020183
'totalSteps': 19200, 'rewardStep': 0.7926846407967191, 'errorList': [], 'lossList': [0.0, -1.3490842974185944, 0.0, 2.438887487947941, 0.0, 0.0, 0.0], 'rewardMean': 0.6651417600092476, 'totalEpisodes': 238, 'stepsPerEpisode': 574, 'rewardPerEpisode': 498.67327667970636
'totalSteps': 20480, 'rewardStep': 0.7535558137888658, 'errorList': [], 'lossList': [0.0, -1.3313530635833741, 0.0, 2.730448195040226, 0.0, 0.0, 0.0], 'rewardMean': 0.6622044718468275, 'totalEpisodes': 239, 'stepsPerEpisode': 782, 'rewardPerEpisode': 688.081923818139
'totalSteps': 21760, 'rewardStep': 0.7450837124774596, 'errorList': [], 'lossList': [0.0, -1.2858382999897002, 0.0, 0.8380620940029622, 0.0, 0.0, 0.0], 'rewardMean': 0.6609375367854231, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.078929231972
'totalSteps': 23040, 'rewardStep': 0.9250592749018591, 'errorList': [], 'lossList': [0.0, -1.2579527819156646, 0.0, 1.0503267975524069, 0.0, 0.0, 0.0], 'rewardMean': 0.6900159614199742, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.4100393759209
'totalSteps': 24320, 'rewardStep': 0.8897592367640121, 'errorList': [], 'lossList': [0.0, -1.2230270737409592, 0.0, 0.8235972755774855, 0.0, 0.0, 0.0], 'rewardMean': 0.7296241009412707, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.0050856181954
'totalSteps': 25600, 'rewardStep': 0.9691140566810978, 'errorList': [0.10643379252669385, 0.07227723965224776, 0.08358237104194725, 0.13729933756571477, 0.1663438929602922, 0.08043446926408326, 0.08064533967366573, 0.07754282339197956, 0.08728366702811627, 0.10411891857796618, 0.09132058949420171, 0.08133629165078617, 0.08996490555590257, 0.08599714995252446, 0.26702922619340486, 0.07992326385704826, 0.12325328684294626, 0.08551408824959161, 0.11462998809765529, 0.2811145459858848, 0.2558436655142703, 0.08607523042987078, 0.10380594490214153, 0.08537747414028424, 0.0782849183323823, 0.07901542236345967, 0.07372399696691792, 0.14043541237919077, 0.0750921641210276, 0.11345005260243075, 0.07535975530945294, 0.2590299916939369, 0.09443630080057519, 0.23808035546579956, 0.2564160262444317, 0.08579915980813457, 0.11397639482868038, 0.07967873643548427, 0.07855112272268769, 0.08299378756437574, 0.08175920337930641, 0.07576400071448178, 0.08648957460806392, 0.08314574741642146, 0.14917646246673627, 0.15988433502540655, 0.08288647819893367, 0.15850492261143215, 0.08162387573921533, 0.11008384309716446], 'lossList': [0.0, -1.1637554943561554, 0.0, 0.6368376458063721, 0.0, 0.0, 0.0], 'rewardMean': 0.7771677224542758, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1205.100601126404, 'successfulTests': 44
#maxSuccessfulTests=44, maxSuccessfulTestsAtStep=25600, timeSpent=76.8
