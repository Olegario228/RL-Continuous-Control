#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 37
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.8382643103018286, 'errorList': [], 'lossList': [0.0, -1.442396947145462, 0.0, 30.723882837295534, 0.0, 0.0, 0.0], 'rewardMean': 0.6381474512538513, 'totalEpisodes': 55, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.537421741135046
'totalSteps': 3840, 'rewardStep': 0.5828364798379927, 'errorList': [], 'lossList': [0.0, -1.429957612156868, 0.0, 45.150425605773925, 0.0, 0.0, 0.0], 'rewardMean': 0.6197104607818984, 'totalEpisodes': 111, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.655683588545238
'totalSteps': 5120, 'rewardStep': 0.8594743572315696, 'errorList': [], 'lossList': [0.0, -1.411182218194008, 0.0, 51.57275527954101, 0.0, 0.0, 0.0], 'rewardMean': 0.6796514348943162, 'totalEpisodes': 154, 'stepsPerEpisode': 48, 'rewardPerEpisode': 36.4912983690223
'totalSteps': 6400, 'rewardStep': 0.5572599375189535, 'errorList': [], 'lossList': [0.0, -1.387949588894844, 0.0, 43.721706266403196, 0.0, 0.0, 0.0], 'rewardMean': 0.6551731354192437, 'totalEpisodes': 178, 'stepsPerEpisode': 25, 'rewardPerEpisode': 17.170900458817666
'totalSteps': 7680, 'rewardStep': 0.9133263831967287, 'errorList': [], 'lossList': [0.0, -1.3686423087120057, 0.0, 43.6732036781311, 0.0, 0.0, 0.0], 'rewardMean': 0.6981986767154913, 'totalEpisodes': 193, 'stepsPerEpisode': 57, 'rewardPerEpisode': 41.46793208164826
'totalSteps': 8960, 'rewardStep': 0.4825113064508124, 'errorList': [], 'lossList': [0.0, -1.3597262382507325, 0.0, 38.192225394248965, 0.0, 0.0, 0.0], 'rewardMean': 0.6673861952491086, 'totalEpisodes': 199, 'stepsPerEpisode': 136, 'rewardPerEpisode': 101.49662724459145
'totalSteps': 10240, 'rewardStep': 0.5143988132771128, 'errorList': [], 'lossList': [0.0, -1.3597354429960251, 0.0, 28.131098548173906, 0.0, 0.0, 0.0], 'rewardMean': 0.6482627725026091, 'totalEpisodes': 201, 'stepsPerEpisode': 126, 'rewardPerEpisode': 99.03504828326462
'totalSteps': 11520, 'rewardStep': 0.6797067159791015, 'errorList': [], 'lossList': [0.0, -1.3814450156688691, 0.0, 35.079156684875485, 0.0, 0.0, 0.0], 'rewardMean': 0.6517565439999972, 'totalEpisodes': 203, 'stepsPerEpisode': 784, 'rewardPerEpisode': 608.0617885155502
'totalSteps': 12800, 'rewardStep': 0.794101468761234, 'errorList': [], 'lossList': [0.0, -1.3702604728937149, 0.0, 53.206157631874085, 0.0, 0.0, 0.0], 'rewardMean': 0.6659910364761208, 'totalEpisodes': 207, 'stepsPerEpisode': 362, 'rewardPerEpisode': 281.3422917977473
'totalSteps': 14080, 'rewardStep': 0.6542501594698547, 'errorList': [], 'lossList': [0.0, -1.3526619774103166, 0.0, 3.6099896505475044, 0.0, 0.0, 0.0], 'rewardMean': 0.6876129932025188, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 807.7622886760038
'totalSteps': 15360, 'rewardStep': 0.9769329193612146, 'errorList': [1.1200953777349782, 1.0868081193137449, 1.0867800599616166, 1.1046072657480277, 1.0805024179776266, 1.078985800116216, 1.0767529010767551, 1.0826969713873322, 1.083419517896805, 1.0945825393592363, 1.0938375750496352, 1.1223209741993865, 1.129686908717094, 1.0732587692557523, 1.1199682056809181, 1.1262366617188817, 1.1162606420875265, 1.1277792754465834, 1.0901761863911195, 1.121589014147089, 1.1100751299881828, 1.134912704646983, 1.0924545483084753, 1.1043275269601072, 1.0930330422349732, 1.0978245338727115, 1.115671211186339, 1.1194938008503295, 1.0989499045538618, 1.1171991110374093, 1.0984603152590844, 1.1077000455436068, 1.1178982686052077, 1.0831741555206844, 1.113886763005594, 1.112367403433279, 1.1087042235906712, 1.0993544220771554, 1.1068267442236372, 1.1161385414343505, 1.08305625260552, 1.080760880131524, 1.1124684183441067, 1.0916305230489352, 1.1125708425328258, 1.1115942309220002, 1.0877829886938142, 1.1147068558971795, 1.1196783531153018, 1.097962471265222], 'lossList': [0.0, -1.329180423617363, 0.0, 30.35323970079422, 0.0, 0.0, 0.0], 'rewardMean': 0.7014798541084575, 'totalEpisodes': 211, 'stepsPerEpisode': 86, 'rewardPerEpisode': 77.44174534252608, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.8800443622828684, 'errorList': [], 'lossList': [0.0, -1.3364824092388152, 0.0, 8.375608565211296, 0.0, 0.0, 0.0], 'rewardMean': 0.731200642352945, 'totalEpisodes': 212, 'stepsPerEpisode': 122, 'rewardPerEpisode': 103.15431735115003
'totalSteps': 17920, 'rewardStep': 0.6951307280149284, 'errorList': [], 'lossList': [0.0, -1.3242558830976485, 0.0, 1.3757193233072758, 0.0, 0.0, 0.0], 'rewardMean': 0.7147662794312809, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 869.4909450120513
'totalSteps': 19200, 'rewardStep': 0.9474268441993327, 'errorList': [0.34250251109002094, 0.2273594868166406, 0.21181708038297198, 0.3194386756016933, 0.1857030011216226, 0.19928702047325356, 0.18481276346483932, 0.2776528123776417, 0.2980095582389723, 0.29259283091110405, 0.21768383403229644, 0.2476772480468548, 0.32072809981296635, 0.29289448071705626, 0.2643245504939923, 0.2890000331681468, 0.24064515057647587, 0.25250758401536133, 0.2470875374447572, 0.23417538056370163, 0.3473254645164239, 0.2975307957908413, 0.2883115731788406, 0.23513354820174737, 0.18675464200974304, 0.19279506743172659, 0.2465111584912744, 0.20720349036272417, 0.35426461421079275, 0.255516674211251, 0.3191792765572519, 0.28337832187280526, 0.3043360663982664, 0.25878078544180416, 0.22021189037270253, 0.26939625991277705, 0.24185825789832233, 0.28623888542476567, 0.28040865630769474, 0.3175454500828618, 0.23029058312984688, 0.22696630721155353, 0.1836988799498455, 0.22749779351470778, 0.38900758690863363, 0.2500435336406114, 0.30845572199828986, 0.21464457658068845, 0.2617110179149195, 0.3685911484113024], 'lossList': [0.0, -1.2749047046899795, 0.0, 2.7130889458954335, 0.0, 0.0, 0.0], 'rewardMean': 0.7537829700993188, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.8547013892348, 'successfulTests': 6
'totalSteps': 20480, 'rewardStep': 0.9200454847682936, 'errorList': [], 'lossList': [0.0, -1.2341000312566757, 0.0, 1.4540303578972817, 0.0, 0.0, 0.0], 'rewardMean': 0.7544548802564754, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.4189488383183
'totalSteps': 21760, 'rewardStep': 0.7957734507325607, 'errorList': [], 'lossList': [0.0, -1.2133852219581605, 0.0, 1.561415354013443, 0.0, 0.0, 0.0], 'rewardMean': 0.7857810946846501, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.6589810043606
'totalSteps': 23040, 'rewardStep': 0.9481287215868228, 'errorList': [0.09167774988386397, 0.07674479296911589, 0.1241845169231948, 0.13557608607269023, 0.2987638114379211, 0.02874388582046179, 0.08330666347470109, 0.14637468489814684, 0.05930560202437251, 0.13056789607964941, 0.16189592234206646, 0.32028196338057596, 0.3049808812373361, 0.3358584511663615, 0.2976967136365824, 0.03549352421354517, 0.43822362593306985, 0.30902758654503987, 0.18511467274511045, 0.32923892160723156, 0.30493810479852584, 0.12661619869262392, 0.40827040933479053, 0.08665679495209877, 0.2863527585154715, 0.14512281124431095, 0.3141564399219858, 0.12144789230830413, 0.10756011611902164, 0.26318298790827016, 0.11785538407926663, 0.19291175571329086, 0.18734036018425176, 0.12284736224783217, 0.13529125031840042, 0.08707725577684795, 0.09995796774480363, 0.09753559636640516, 0.3637069743890896, 0.13723927917883, 0.09436212129573067, 0.35053854892343994, 0.08469978989361386, 0.3158359093489812, 0.07917832519516249, 0.08400431945475145, 0.08611906912459646, 0.3806560419618989, 0.12042834932221837, 0.1520839103907485], 'lossList': [0.0, -1.1896079558134078, 0.0, 0.8019473231956362, 0.0, 0.0, 0.0], 'rewardMean': 0.8291540855156212, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.8541679767445, 'successfulTests': 33
'totalSteps': 24320, 'rewardStep': 0.776042510043748, 'errorList': [], 'lossList': [0.0, -1.1639326977729798, 0.0, 0.6600791762024164, 0.0, 0.0, 0.0], 'rewardMean': 0.8387876649220857, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.8891353698689
'totalSteps': 25600, 'rewardStep': 0.7578986262257603, 'errorList': [], 'lossList': [0.0, -1.1471581768989563, 0.0, 0.45077556785196066, 0.0, 0.0, 0.0], 'rewardMean': 0.8351673806685385, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.665407393186
#maxSuccessfulTests=33, maxSuccessfulTestsAtStep=23040, timeSpent=115.55
