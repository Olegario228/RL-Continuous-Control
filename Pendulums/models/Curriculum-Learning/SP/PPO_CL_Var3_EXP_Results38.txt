#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 38
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.8687042710558156, 'errorList': [], 'lossList': [0.0, -1.4421722090244293, 0.0, 31.263580923080443, 0.0, 0.0, 0.0], 'rewardMean': 0.8170150520014359, 'totalEpisodes': 63, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7081988648917021
'totalSteps': 3840, 'rewardStep': 0.46570295644851006, 'errorList': [], 'lossList': [0.0, -1.4434296721220017, 0.0, 38.654857006073, 0.0, 0.0, 0.0], 'rewardMean': 0.6999110201504606, 'totalEpisodes': 122, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.4499550392575948
'totalSteps': 5120, 'rewardStep': 0.9455391025534315, 'errorList': [], 'lossList': [0.0, -1.4401391118764877, 0.0, 42.49423614501953, 0.0, 0.0, 0.0], 'rewardMean': 0.7613180407512034, 'totalEpisodes': 156, 'stepsPerEpisode': 12, 'rewardPerEpisode': 11.163149589501947
'totalSteps': 6400, 'rewardStep': 0.9037952161147027, 'errorList': [], 'lossList': [0.0, -1.4272476172447204, 0.0, 47.36139825820923, 0.0, 0.0, 0.0], 'rewardMean': 0.7898134758239033, 'totalEpisodes': 177, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.670290718744365
'totalSteps': 7680, 'rewardStep': 0.8634379206949416, 'errorList': [], 'lossList': [0.0, -1.4058087641000747, 0.0, 37.63337952136993, 0.0, 0.0, 0.0], 'rewardMean': 0.8020842166357429, 'totalEpisodes': 190, 'stepsPerEpisode': 81, 'rewardPerEpisode': 61.82779961196402
'totalSteps': 8960, 'rewardStep': 0.9047479114170549, 'errorList': [], 'lossList': [0.0, -1.3940017062425614, 0.0, 33.65503245353699, 0.0, 0.0, 0.0], 'rewardMean': 0.816750458747359, 'totalEpisodes': 197, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.23631064756805
'totalSteps': 10240, 'rewardStep': 0.789804343499974, 'errorList': [], 'lossList': [0.0, -1.3850766569375992, 0.0, 21.224352734088896, 0.0, 0.0, 0.0], 'rewardMean': 0.8133821943414359, 'totalEpisodes': 201, 'stepsPerEpisode': 138, 'rewardPerEpisode': 115.72961414949147
'totalSteps': 11520, 'rewardStep': 0.4861832144861777, 'errorList': [], 'lossList': [0.0, -1.3782965463399888, 0.0, 10.14262923002243, 0.0, 0.0, 0.0], 'rewardMean': 0.777026752135296, 'totalEpisodes': 205, 'stepsPerEpisode': 111, 'rewardPerEpisode': 79.27206863279784
'totalSteps': 12800, 'rewardStep': 0.798093770452107, 'errorList': [], 'lossList': [0.0, -1.3829612493515016, 0.0, 7.228301122188568, 0.0, 0.0, 0.0], 'rewardMean': 0.7791334539669772, 'totalEpisodes': 208, 'stepsPerEpisode': 449, 'rewardPerEpisode': 372.1057255831643
'totalSteps': 14080, 'rewardStep': 0.7261214077640886, 'errorList': [], 'lossList': [0.0, -1.349023432135582, 0.0, 5.684289218783379, 0.0, 0.0, 0.0], 'rewardMean': 0.7752130114486804, 'totalEpisodes': 210, 'stepsPerEpisode': 243, 'rewardPerEpisode': 186.4526754698732
'totalSteps': 15360, 'rewardStep': 0.7526376410493346, 'errorList': [], 'lossList': [0.0, -1.332492356300354, 0.0, 3.1894254064559937, 0.0, 0.0, 0.0], 'rewardMean': 0.7636063484480322, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.048202595016
'totalSteps': 16640, 'rewardStep': 0.8306885646571054, 'errorList': [], 'lossList': [0.0, -1.3095776951313018, 0.0, 3.723430583178997, 0.0, 0.0, 0.0], 'rewardMean': 0.8001049092688918, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.5609580581122
'totalSteps': 17920, 'rewardStep': 0.8128245230939273, 'errorList': [], 'lossList': [0.0, -1.2847875547409058, 0.0, 2.0184086422622203, 0.0, 0.0, 0.0], 'rewardMean': 0.7868334513229414, 'totalEpisodes': 211, 'stepsPerEpisode': 433, 'rewardPerEpisode': 359.7473652047042
'totalSteps': 19200, 'rewardStep': 0.8067003550771809, 'errorList': [], 'lossList': [0.0, -1.2623991858959198, 0.0, 2.2635449141263964, 0.0, 0.0, 0.0], 'rewardMean': 0.7771239652191892, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1026.6892534623964
'totalSteps': 20480, 'rewardStep': 0.9423739711092091, 'errorList': [0.06847391101559204, 0.061681920973299637, 0.06565814103190687, 0.1019368837433954, 0.06655308284075939, 0.0759265288091424, 0.12452199015476303, 0.08274987708901442, 0.08595529770140613, 0.11171332380724702, 0.06203032078860637, 0.07463933363461839, 0.07959034776974588, 0.09935945125490839, 0.08356774128060053, 0.061233972581465596, 0.10503135600248374, 0.1255795350761832, 0.10618232576769084, 0.09979038078391278, 0.06328397985593555, 0.07694495367383052, 0.07407639653252258, 0.1084254980984286, 0.07071526890878051, 0.06733434639721038, 0.1052475910698643, 0.11062719539555256, 0.12225738339134067, 0.11666691826551905, 0.05808414847949288, 0.12636496784208864, 0.06390017376213784, 0.10454828158870608, 0.15045241706064288, 0.1047105828440994, 0.12406455260907649, 0.07585274211998848, 0.0631122568793831, 0.06119876213308462, 0.0775819077299719, 0.1048227114031421, 0.09070311997459674, 0.09456079645706549, 0.1099157513196347, 0.07173097478197288, 0.05879276555818236, 0.08223557670848156, 0.09690728087871336, 0.11521111140375428], 'lossList': [0.0, -1.223984591960907, 0.0, 1.0435617203265428, 0.0, 0.0, 0.0], 'rewardMean': 0.785017570260616, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.565072742221, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=64.61
