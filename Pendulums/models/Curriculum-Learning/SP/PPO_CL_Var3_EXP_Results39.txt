#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 39
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.6101217711141749, 'errorList': [], 'lossList': [0.0, -1.4223173969984055, 0.0, 31.072332878112793, 0.0, 0.0, 0.0], 'rewardMean': 0.6410185886287592, 'totalEpisodes': 64, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.8552956836891
'totalSteps': 3840, 'rewardStep': 0.6525256826562505, 'errorList': [], 'lossList': [0.0, -1.4133468091487884, 0.0, 36.58261087417603, 0.0, 0.0, 0.0], 'rewardMean': 0.6448542866379229, 'totalEpisodes': 132, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.5589446257818
'totalSteps': 5120, 'rewardStep': 0.44405040863036693, 'errorList': [], 'lossList': [0.0, -1.3915939658880234, 0.0, 43.06720705986023, 0.0, 0.0, 0.0], 'rewardMean': 0.594653317136034, 'totalEpisodes': 168, 'stepsPerEpisode': 43, 'rewardPerEpisode': 27.729662408727602
'totalSteps': 6400, 'rewardStep': 0.8570039167587764, 'errorList': [], 'lossList': [0.0, -1.3760404092073442, 0.0, 36.52870820999146, 0.0, 0.0, 0.0], 'rewardMean': 0.6471234370605824, 'totalEpisodes': 187, 'stepsPerEpisode': 36, 'rewardPerEpisode': 29.202427555987377
'totalSteps': 7680, 'rewardStep': 0.724627099287032, 'errorList': [], 'lossList': [0.0, -1.3678123790025711, 0.0, 49.29254757881164, 0.0, 0.0, 0.0], 'rewardMean': 0.660040714098324, 'totalEpisodes': 200, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.62125607243038
'totalSteps': 8960, 'rewardStep': 0.9123095449492041, 'errorList': [], 'lossList': [0.0, -1.3479025262594222, 0.0, 41.03715938568115, 0.0, 0.0, 0.0], 'rewardMean': 0.6960791185055927, 'totalEpisodes': 211, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.13942327254706
'totalSteps': 10240, 'rewardStep': 0.7763229504014393, 'errorList': [], 'lossList': [0.0, -1.3370637011528015, 0.0, 41.23093641757965, 0.0, 0.0, 0.0], 'rewardMean': 0.7061095974925735, 'totalEpisodes': 216, 'stepsPerEpisode': 150, 'rewardPerEpisode': 115.1669905624558
'totalSteps': 11520, 'rewardStep': 0.7682953226748493, 'errorList': [], 'lossList': [0.0, -1.3324459666013717, 0.0, 34.51568574666977, 0.0, 0.0, 0.0], 'rewardMean': 0.7130191225128264, 'totalEpisodes': 221, 'stepsPerEpisode': 101, 'rewardPerEpisode': 80.79306280914312
'totalSteps': 12800, 'rewardStep': 0.7559165440968513, 'errorList': [], 'lossList': [0.0, -1.3143036025762558, 0.0, 48.30499300956726, 0.0, 0.0, 0.0], 'rewardMean': 0.7173088646712289, 'totalEpisodes': 228, 'stepsPerEpisode': 64, 'rewardPerEpisode': 51.00285124755043
'totalSteps': 14080, 'rewardStep': 0.8867544858152908, 'errorList': [], 'lossList': [0.0, -1.3007718116044997, 0.0, 33.181943135261534, 0.0, 0.0, 0.0], 'rewardMean': 0.7387927726384236, 'totalEpisodes': 233, 'stepsPerEpisode': 172, 'rewardPerEpisode': 147.24922747241558
'totalSteps': 15360, 'rewardStep': 0.8990039632880432, 'errorList': [], 'lossList': [0.0, -1.2944853335618973, 0.0, 63.298395075798034, 0.0, 0.0, 0.0], 'rewardMean': 0.7676809918558105, 'totalEpisodes': 240, 'stepsPerEpisode': 51, 'rewardPerEpisode': 46.20152919178211
'totalSteps': 16640, 'rewardStep': 0.4833062736651312, 'errorList': [], 'lossList': [0.0, -1.3001322346925734, 0.0, 24.383613901138304, 0.0, 0.0, 0.0], 'rewardMean': 0.7507590509566985, 'totalEpisodes': 246, 'stepsPerEpisode': 166, 'rewardPerEpisode': 135.00124726321437
'totalSteps': 17920, 'rewardStep': 0.5375081194274626, 'errorList': [], 'lossList': [0.0, -1.3035382890701295, 0.0, 17.764747030735016, 0.0, 0.0, 0.0], 'rewardMean': 0.7601048220364081, 'totalEpisodes': 251, 'stepsPerEpisode': 141, 'rewardPerEpisode': 102.444667900113
'totalSteps': 19200, 'rewardStep': 0.8337752520472627, 'errorList': [], 'lossList': [0.0, -1.2982931125164032, 0.0, 10.733342376351356, 0.0, 0.0, 0.0], 'rewardMean': 0.7577819555652565, 'totalEpisodes': 253, 'stepsPerEpisode': 288, 'rewardPerEpisode': 234.3522369794284
'totalSteps': 20480, 'rewardStep': 0.88059108750109, 'errorList': [], 'lossList': [0.0, -1.29636867582798, 0.0, 12.66233186006546, 0.0, 0.0, 0.0], 'rewardMean': 0.7733783543866624, 'totalEpisodes': 256, 'stepsPerEpisode': 56, 'rewardPerEpisode': 46.62160824005876
'totalSteps': 21760, 'rewardStep': 0.8294118735434095, 'errorList': [], 'lossList': [0.0, -1.2896818733215332, 0.0, 7.343310869336128, 0.0, 0.0, 0.0], 'rewardMean': 0.765088587246083, 'totalEpisodes': 257, 'stepsPerEpisode': 687, 'rewardPerEpisode': 595.7205005597505
'totalSteps': 23040, 'rewardStep': 0.6916932255877144, 'errorList': [], 'lossList': [0.0, -1.274399973154068, 0.0, 3.1470389704406263, 0.0, 0.0, 0.0], 'rewardMean': 0.7566256147647106, 'totalEpisodes': 257, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1004.5223971946548
'totalSteps': 24320, 'rewardStep': 0.8025468415962503, 'errorList': [], 'lossList': [0.0, -1.273863525390625, 0.0, 1.6647528908401727, 0.0, 0.0, 0.0], 'rewardMean': 0.7600507666568506, 'totalEpisodes': 257, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.645595444292
'totalSteps': 25600, 'rewardStep': 0.9801960657908513, 'errorList': [0.03354070199574824, 0.015671253588618293, 0.01551590743705054, 0.01863697716452581, 0.014008439316618056, 0.013317130209222164, 0.013391458151632077, 0.015471402393389146, 0.02027565146051479, 0.01342251075099196, 0.013087618410409015, 0.025398865591615503, 0.02870339153085932, 0.01546949970839618, 0.026565999378568673, 0.041541308496624095, 0.025454391532945653, 0.03300561202011062, 0.02100324676427104, 0.028331843792159972, 0.022438540422800538, 0.019274506684275453, 0.03171472143123435, 0.02959691119052609, 0.01854694253114219, 0.015890412411229165, 0.028425562499630377, 0.04078975647548353, 0.01950053916528163, 0.016223813797900128, 0.03516813207182399, 0.023462115780746553, 0.049182074398740186, 0.03671582402182598, 0.04040288819304358, 0.02346753624088673, 0.017778343685643544, 0.02996677138294969, 0.026056375620436802, 0.018604852239350603, 0.03861431388737127, 0.0233326563391008, 0.02347143700447222, 0.01655648601430982, 0.021075447786913504, 0.035115612658992365, 0.023400069951362498, 0.025546902816867455, 0.034885062339136484, 0.05299678846357298], 'lossList': [0.0, -1.252517607808113, 0.0, 1.1502498307079077, 0.0, 0.0, 0.0], 'rewardMean': 0.7824787188262505, 'totalEpisodes': 257, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.0504125892978, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=78.69
