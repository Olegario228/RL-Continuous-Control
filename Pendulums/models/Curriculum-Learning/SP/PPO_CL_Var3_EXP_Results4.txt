#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.9273320514722232, 'errorList': [], 'lossList': [0.0, -1.4115724575519562, 0.0, 28.074505462646485, 0.0, 0.0, 0.0], 'rewardMean': 0.9242110928118152, 'totalEpisodes': 106, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.321484423478154
'totalSteps': 3840, 'rewardStep': 0.7529494994455842, 'errorList': [], 'lossList': [0.0, -1.412173730134964, 0.0, 31.168866443634034, 0.0, 0.0, 0.0], 'rewardMean': 0.8671238950230715, 'totalEpisodes': 165, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.06765161288823
'totalSteps': 5120, 'rewardStep': 0.7088407204142817, 'errorList': [], 'lossList': [0.0, -1.401294361948967, 0.0, 37.40291408538818, 0.0, 0.0, 0.0], 'rewardMean': 0.8275531013708741, 'totalEpisodes': 199, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.863232872531905
'totalSteps': 6400, 'rewardStep': 0.72368629260649, 'errorList': [], 'lossList': [0.0, -1.3878514617681503, 0.0, 38.20759306907654, 0.0, 0.0, 0.0], 'rewardMean': 0.8067797396179973, 'totalEpisodes': 211, 'stepsPerEpisode': 36, 'rewardPerEpisode': 24.341177624705782
'totalSteps': 7680, 'rewardStep': 0.8358085611437581, 'errorList': [], 'lossList': [0.0, -1.3716566008329392, 0.0, 44.25376608848572, 0.0, 0.0, 0.0], 'rewardMean': 0.8116178765389575, 'totalEpisodes': 221, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.514557782558118
'totalSteps': 8960, 'rewardStep': 0.8574459853704638, 'errorList': [], 'lossList': [0.0, -1.3609266555309296, 0.0, 22.621585764884948, 0.0, 0.0, 0.0], 'rewardMean': 0.8181647492291726, 'totalEpisodes': 230, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.284960977351847
'totalSteps': 10240, 'rewardStep': 0.9109400368194772, 'errorList': [], 'lossList': [0.0, -1.3390771412849427, 0.0, 11.259985706806184, 0.0, 0.0, 0.0], 'rewardMean': 0.8297616601779607, 'totalEpisodes': 236, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.550300523329152
'totalSteps': 11520, 'rewardStep': 0.9184910028562335, 'errorList': [], 'lossList': [0.0, -1.3262866574525833, 0.0, 16.30045026063919, 0.0, 0.0, 0.0], 'rewardMean': 0.8396204760311021, 'totalEpisodes': 241, 'stepsPerEpisode': 60, 'rewardPerEpisode': 53.09479176957597
'totalSteps': 12800, 'rewardStep': 0.8825988799692069, 'errorList': [], 'lossList': [0.0, -1.3419201201200486, 0.0, 11.188043676614761, 0.0, 0.0, 0.0], 'rewardMean': 0.8439183164249127, 'totalEpisodes': 246, 'stepsPerEpisode': 123, 'rewardPerEpisode': 97.96307147780159
'totalSteps': 14080, 'rewardStep': 0.9091448347077975, 'errorList': [], 'lossList': [0.0, -1.3712554425001144, 0.0, 5.346899753808975, 0.0, 0.0, 0.0], 'rewardMean': 0.8427237864805516, 'totalEpisodes': 251, 'stepsPerEpisode': 36, 'rewardPerEpisode': 30.440048731899637
'totalSteps': 15360, 'rewardStep': 0.6839369840246095, 'errorList': [], 'lossList': [0.0, -1.3875528055429458, 0.0, 12.478983198404313, 0.0, 0.0, 0.0], 'rewardMean': 0.8183842797357903, 'totalEpisodes': 255, 'stepsPerEpisode': 310, 'rewardPerEpisode': 265.9620244749768
'totalSteps': 16640, 'rewardStep': 0.6246090221429562, 'errorList': [], 'lossList': [0.0, -1.3772705990076064, 0.0, 7.818121733665467, 0.0, 0.0, 0.0], 'rewardMean': 0.8055502320055273, 'totalEpisodes': 258, 'stepsPerEpisode': 422, 'rewardPerEpisode': 357.7163640684298
'totalSteps': 17920, 'rewardStep': 0.7749728830110755, 'errorList': [], 'lossList': [0.0, -1.3432767552137375, 0.0, 3.996158295869827, 0.0, 0.0, 0.0], 'rewardMean': 0.8121634482652068, 'totalEpisodes': 261, 'stepsPerEpisode': 62, 'rewardPerEpisode': 52.7819809904315
'totalSteps': 19200, 'rewardStep': 0.8661989142134268, 'errorList': [], 'lossList': [0.0, -1.3191184848546982, 0.0, 4.331911613345146, 0.0, 0.0, 0.0], 'rewardMean': 0.8264147104259006, 'totalEpisodes': 264, 'stepsPerEpisode': 188, 'rewardPerEpisode': 156.6447109773344
'totalSteps': 20480, 'rewardStep': 0.9060986122780091, 'errorList': [], 'lossList': [0.0, -1.302360447049141, 0.0, 2.5842454558610917, 0.0, 0.0, 0.0], 'rewardMean': 0.8334437155393255, 'totalEpisodes': 268, 'stepsPerEpisode': 31, 'rewardPerEpisode': 27.487078985929383
'totalSteps': 21760, 'rewardStep': 0.9213921003261701, 'errorList': [], 'lossList': [0.0, -1.2734343206882477, 0.0, 1.7167631737887858, 0.0, 0.0, 0.0], 'rewardMean': 0.8398383270348961, 'totalEpisodes': 270, 'stepsPerEpisode': 571, 'rewardPerEpisode': 514.7374360717035
'totalSteps': 23040, 'rewardStep': 0.8880794934023629, 'errorList': [], 'lossList': [0.0, -1.239791852235794, 0.0, 1.9004854103922844, 0.0, 0.0, 0.0], 'rewardMean': 0.8375522726931848, 'totalEpisodes': 272, 'stepsPerEpisode': 174, 'rewardPerEpisode': 153.37134603231578
'totalSteps': 24320, 'rewardStep': 0.5612684776750669, 'errorList': [], 'lossList': [0.0, -1.2305876767635346, 0.0, 2.065464523434639, 0.0, 0.0, 0.0], 'rewardMean': 0.8018300201750682, 'totalEpisodes': 273, 'stepsPerEpisode': 725, 'rewardPerEpisode': 615.2265549400433
'totalSteps': 25600, 'rewardStep': 0.9749410038910622, 'errorList': [1.0265202346320337, 0.34928870685038577, 0.137926806481519, 0.6911027452018257, 0.8894381961306698, 0.4250394880757197, 0.23541860871762427, 1.2713185754625373, 0.45422077074072303, 0.15141881304969254, 0.3582440927538117, 1.2326996036626443, 1.098921163004417, 0.1303085016316111, 0.27249631605096025, 0.1623094427263092, 0.6672294704048757, 1.460876008132564, 0.5644179888214937, 0.25792735605145867, 0.14991578421058419, 0.3794297839150574, 0.0965590220349948, 0.5536221630206757, 0.2208104845794725, 0.9317243910111965, 1.04357604113487, 0.12286310318709884, 0.3472235218721758, 0.8286046386968278, 0.11081057584933943, 0.806979636519832, 0.3202961359754971, 1.0396537782587858, 0.7018631406457193, 0.23245986184191664, 0.7840456811699974, 1.3421352494345336, 0.8323382113760969, 0.6348411451030505, 0.5968130338444821, 1.8155514641058386, 0.26422728038159243, 0.6956360334935169, 0.3647920989178119, 0.1810046620472132, 0.8397998931748534, 0.7326585720112956, 0.6384459857254562, 0.31666999134015167], 'lossList': [0.0, -1.2328037244081498, 0.0, 1.5111880299448968, 0.0, 0.0, 0.0], 'rewardMean': 0.8110642325672537, 'totalEpisodes': 275, 'stepsPerEpisode': 162, 'rewardPerEpisode': 147.30448492248286, 'successfulTests': 9
#maxSuccessfulTests=9, maxSuccessfulTestsAtStep=25600, timeSpent=68.6
