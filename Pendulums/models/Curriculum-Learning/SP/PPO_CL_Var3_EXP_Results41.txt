#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 41
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7482241978653908, 'errorList': [], 'lossList': [0.0, -1.410132218003273, 0.0, 33.21440967559815, 0.0, 0.0, 0.0], 'rewardMean': 0.653435958170873, 'totalEpisodes': 42, 'stepsPerEpisode': 39, 'rewardPerEpisode': 30.46358916803779
'totalSteps': 3840, 'rewardStep': 0.5282906119617269, 'errorList': [], 'lossList': [0.0, -1.3940466278791428, 0.0, 44.10303933143616, 0.0, 0.0, 0.0], 'rewardMean': 0.6117208427678243, 'totalEpisodes': 99, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.523251377107342
'totalSteps': 5120, 'rewardStep': 0.9293576042762821, 'errorList': [], 'lossList': [0.0, -1.3771327674388885, 0.0, 46.27882736206055, 0.0, 0.0, 0.0], 'rewardMean': 0.6911300331449388, 'totalEpisodes': 143, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.195580351574843
'totalSteps': 6400, 'rewardStep': 0.7574366214143752, 'errorList': [], 'lossList': [0.0, -1.3622209966182708, 0.0, 44.049912719726564, 0.0, 0.0, 0.0], 'rewardMean': 0.7043913507988261, 'totalEpisodes': 161, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4767010221624635
'totalSteps': 7680, 'rewardStep': 0.7791238081931686, 'errorList': [], 'lossList': [0.0, -1.359587226510048, 0.0, 37.136529779434206, 0.0, 0.0, 0.0], 'rewardMean': 0.7168467603645499, 'totalEpisodes': 168, 'stepsPerEpisode': 78, 'rewardPerEpisode': 57.45721013542456
'totalSteps': 8960, 'rewardStep': 0.45817434087481385, 'errorList': [], 'lossList': [0.0, -1.3595535951852797, 0.0, 26.002061855793, 0.0, 0.0, 0.0], 'rewardMean': 0.6798935575803019, 'totalEpisodes': 176, 'stepsPerEpisode': 118, 'rewardPerEpisode': 78.80809538362676
'totalSteps': 10240, 'rewardStep': 0.7876353536316731, 'errorList': [], 'lossList': [0.0, -1.3524759495258332, 0.0, 17.766630145311357, 0.0, 0.0, 0.0], 'rewardMean': 0.6933612820867232, 'totalEpisodes': 182, 'stepsPerEpisode': 64, 'rewardPerEpisode': 53.08673558577019
'totalSteps': 11520, 'rewardStep': 0.7224452015620161, 'errorList': [], 'lossList': [0.0, -1.3527789813280107, 0.0, 13.901641784906387, 0.0, 0.0, 0.0], 'rewardMean': 0.6965928286950891, 'totalEpisodes': 189, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.82759948245056
'totalSteps': 12800, 'rewardStep': 0.46787230642340577, 'errorList': [], 'lossList': [0.0, -1.336739045381546, 0.0, 8.182827829122543, 0.0, 0.0, 0.0], 'rewardMean': 0.6737207764679207, 'totalEpisodes': 192, 'stepsPerEpisode': 499, 'rewardPerEpisode': 376.58089101096743
'totalSteps': 14080, 'rewardStep': 0.749765130652322, 'errorList': [], 'lossList': [0.0, -1.317661167383194, 0.0, 6.512061337530613, 0.0, 0.0, 0.0], 'rewardMean': 0.6928325176855175, 'totalEpisodes': 193, 'stepsPerEpisode': 1212, 'rewardPerEpisode': 1000.1263052005513
'totalSteps': 15360, 'rewardStep': 0.5220352999875617, 'errorList': [], 'lossList': [0.0, -1.29152936398983, 0.0, 6.007767794430256, 0.0, 0.0, 0.0], 'rewardMean': 0.6702136278977345, 'totalEpisodes': 194, 'stepsPerEpisode': 957, 'rewardPerEpisode': 776.6573127936991
'totalSteps': 16640, 'rewardStep': 0.860654570613954, 'errorList': [], 'lossList': [0.0, -1.2723831236362457, 0.0, 3.866688331961632, 0.0, 0.0, 0.0], 'rewardMean': 0.7034500237629573, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 783.119831051939
'totalSteps': 17920, 'rewardStep': 0.8217200848402527, 'errorList': [], 'lossList': [0.0, -1.244329382777214, 0.0, 2.277873111516237, 0.0, 0.0, 0.0], 'rewardMean': 0.6926862718193543, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1004.7152942170612
'totalSteps': 19200, 'rewardStep': 0.8726335224832928, 'errorList': [], 'lossList': [0.0, -1.2077216470241547, 0.0, 1.2403006079792975, 0.0, 0.0, 0.0], 'rewardMean': 0.7042059619262461, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.5584467242263
'totalSteps': 20480, 'rewardStep': 0.772266028083662, 'errorList': [], 'lossList': [0.0, -1.154308866262436, 0.0, 1.4181343756616116, 0.0, 0.0, 0.0], 'rewardMean': 0.7035201839152954, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.248358647677
'totalSteps': 21760, 'rewardStep': 0.857594960053141, 'errorList': [], 'lossList': [0.0, -1.0832058346271516, 0.0, 1.029908917658031, 0.0, 0.0, 0.0], 'rewardMean': 0.743462245833128, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.895490935182
'totalSteps': 23040, 'rewardStep': 0.9645202597114586, 'errorList': [0.16658909618010978, 0.1577267377854866, 0.17776246331329748, 0.14819646446827506, 0.18418003331785718, 0.19999501671040512, 0.14644217574676213, 0.18973863176192352, 0.1755005987171342, 0.18577486579660943, 0.20223382334969767, 0.19726896835002816, 0.1644212817032956, 0.21963499678668927, 0.19942061374946302, 0.13371990005855233, 0.1806615977623323, 0.1261038797766084, 0.17282695611423457, 0.2123581717585269, 0.21297369903163846, 0.17657510089248796, 0.27766424199062606, 0.20513406098752968, 0.17830230735314728, 0.19822678906810115, 0.18855707125457014, 0.14661869914354855, 0.20451451049718106, 0.20320587274802163, 0.2113662676755543, 0.2090735452898679, 0.16486027313305462, 0.2163275331787461, 0.20975152844528727, 0.20545687334614993, 0.18383054873575613, 0.1655951323605444, 0.1990021000274335, 0.21176995497205586, 0.13644597609432, 0.17244601268534035, 0.2371443588923171, 0.22234594135287922, 0.18885867614429283, 0.13338193093997164, 0.18591305159866806, 0.15064131811009487, 0.15043790981159205, 0.17036216609457622], 'lossList': [0.0, -1.035183070898056, 0.0, 0.5101832112297415, 0.0, 0.0, 0.0], 'rewardMean': 0.7611507364411066, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.3300754533202, 'successfulTests': 34
'totalSteps': 24320, 'rewardStep': 0.9189717420177513, 'errorList': [], 'lossList': [0.0, -0.988597826063633, 0.0, 0.4218049499578774, 0.0, 0.0, 0.0], 'rewardMean': 0.7808033904866802, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1160.864410014113
'totalSteps': 25600, 'rewardStep': 0.982062988307484, 'errorList': [0.14636994404787101, 0.19478898569297917, 0.3080355598067409, 0.2743157162915772, 0.07186712668701095, 0.10980872322034158, 0.11241005637936057, 0.1677254673998547, 0.04333464463046484, 0.2145924720185367, 0.06273622238650964, 0.13519014535576274, 0.06032007190972508, 0.015275823109724451, 0.2027880199167772, 0.12069139240271504, 0.03314967777354968, 0.24185397964033417, 0.2011248487435691, 0.126445289140378, 0.22330222664173277, 0.03304315451315741, 0.03633152667834154, 0.24313350213498455, 0.06192341534986031, 0.09132403423780025, 0.23874412638293488, 0.17020870949800954, 0.05840295715629549, 0.05480107908467174, 0.03179930758491449, 0.022355664938317175, 0.17513132377197008, 0.18974494284018192, 0.030409410831844154, 0.07451492741552186, 0.020856055332562816, 0.17475179275671135, 0.0277879822356851, 0.22651903966822898, 0.08696777755155563, 0.0717174880042872, 0.07367462918326226, 0.09272952463228676, 0.11154890041708497, 0.0631643744504558, 0.016713988379017594, 0.2104856898012699, 0.09578523877331659, 0.18859129103997999], 'lossList': [0.0, -0.9363970628380776, 0.0, 0.539985326398164, 0.0, 0.0, 0.0], 'rewardMean': 0.8322224586750882, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1224.9487289489284, 'successfulTests': 39
#maxSuccessfulTests=39, maxSuccessfulTestsAtStep=25600, timeSpent=96.88
