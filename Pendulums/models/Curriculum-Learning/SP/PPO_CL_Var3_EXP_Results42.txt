#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 42
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.9439254048106082, 'errorList': [], 'lossList': [0.0, -1.418771071434021, 0.0, 32.36124071121216, 0.0, 0.0, 0.0], 'rewardMean': 0.7108555900167032, 'totalEpisodes': 55, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.560499846380118
'totalSteps': 3840, 'rewardStep': 0.7523298778047383, 'errorList': [], 'lossList': [0.0, -1.3942130494117737, 0.0, 41.66363657951355, 0.0, 0.0, 0.0], 'rewardMean': 0.7246803526127149, 'totalEpisodes': 112, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.215447710477328
'totalSteps': 5120, 'rewardStep': 0.8914912161712341, 'errorList': [], 'lossList': [0.0, -1.3765044099092483, 0.0, 49.69095912933349, 0.0, 0.0, 0.0], 'rewardMean': 0.7663830685023447, 'totalEpisodes': 153, 'stepsPerEpisode': 23, 'rewardPerEpisode': 14.826572221928235
'totalSteps': 6400, 'rewardStep': 0.531220377309602, 'errorList': [], 'lossList': [0.0, -1.367238673567772, 0.0, 48.2960853099823, 0.0, 0.0, 0.0], 'rewardMean': 0.7193505302637961, 'totalEpisodes': 175, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.970146759990435
'totalSteps': 7680, 'rewardStep': 0.9759349984819747, 'errorList': [23.436505947524907, 100.07151236772695, 92.45282779129964, 122.7132137055973, 99.32114949018526, 52.6353859272549, 61.81730296464064, 12.847203910039648, 32.541690561013844, 11.918232421427373, 50.819266647316894, 92.93800441525653, 75.63533148939312, 29.811083316744117, 11.21867900970111, 16.662632150302457, 45.60562802320201, 95.97324885104963, 68.28065248707803, 11.954354549826032, 1.9245554264541171, 96.78579372939542, 74.58918159237327, 92.5657896604013, 30.129887892770984, 71.2137861453657, 33.639525373573306, 123.19848781901354, 64.45115679181843, 96.36912434404104, 22.30989992819223, 114.87564872796008, 1.7516057137752878, 120.57234960379509, 34.31216372272348, 55.9314431547822, 2.4096336062440984, 66.21553233240519, 82.43448780079325, 119.59428676530113, 10.04626032458015, 27.21547202282748, 102.4882797403379, 100.5665915485315, 56.49135257440389, 42.993294779701735, 114.12827043489911, 6.5049452910488865, 65.17809217148964, 5.879937861121737], 'lossList': [0.0, -1.3581287837028504, 0.0, 54.16376145362854, 0.0, 0.0, 0.0], 'rewardMean': 0.7621146083001592, 'totalEpisodes': 192, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.26690396565181, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.6114824535010722, 'errorList': [], 'lossList': [0.0, -1.3533685332536698, 0.0, 36.846346054077145, 0.0, 0.0, 0.0], 'rewardMean': 0.7405957290431467, 'totalEpisodes': 200, 'stepsPerEpisode': 90, 'rewardPerEpisode': 66.62976294774299
'totalSteps': 10240, 'rewardStep': 0.7191788954422274, 'errorList': [], 'lossList': [0.0, -1.3456477749347686, 0.0, 16.536593823432924, 0.0, 0.0, 0.0], 'rewardMean': 0.737918624843032, 'totalEpisodes': 204, 'stepsPerEpisode': 178, 'rewardPerEpisode': 139.34098667331264
'totalSteps': 11520, 'rewardStep': 0.5996333542384912, 'errorList': [], 'lossList': [0.0, -1.3377823501825332, 0.0, 9.286327421665192, 0.0, 0.0, 0.0], 'rewardMean': 0.7225535947758607, 'totalEpisodes': 209, 'stepsPerEpisode': 138, 'rewardPerEpisode': 96.83973698503621
'totalSteps': 12800, 'rewardStep': 0.8570429291265951, 'errorList': [], 'lossList': [0.0, -1.3218037068843842, 0.0, 10.80366403579712, 0.0, 0.0, 0.0], 'rewardMean': 0.7360025282109341, 'totalEpisodes': 215, 'stepsPerEpisode': 101, 'rewardPerEpisode': 87.5078893673423
'totalSteps': 14080, 'rewardStep': 0.788308331040207, 'errorList': [], 'lossList': [0.0, -1.32036712706089, 0.0, 5.927282203435897, 0.0, 0.0, 0.0], 'rewardMean': 0.7670547837926749, 'totalEpisodes': 219, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.04570988648821
'totalSteps': 15360, 'rewardStep': 0.9027290048446241, 'errorList': [], 'lossList': [0.0, -1.3266205549240113, 0.0, 4.072516810297966, 0.0, 0.0, 0.0], 'rewardMean': 0.7629351437960767, 'totalEpisodes': 223, 'stepsPerEpisode': 58, 'rewardPerEpisode': 50.71820085042191
'totalSteps': 16640, 'rewardStep': 0.6390658555453823, 'errorList': [], 'lossList': [0.0, -1.3145494091510772, 0.0, 3.231878105401993, 0.0, 0.0, 0.0], 'rewardMean': 0.7516087415701411, 'totalEpisodes': 225, 'stepsPerEpisode': 241, 'rewardPerEpisode': 184.96912574690828
'totalSteps': 17920, 'rewardStep': 0.4510584651623267, 'errorList': [], 'lossList': [0.0, -1.2978700709342956, 0.0, 3.022626483440399, 0.0, 0.0, 0.0], 'rewardMean': 0.7075654664692503, 'totalEpisodes': 226, 'stepsPerEpisode': 1187, 'rewardPerEpisode': 849.3443166301089
'totalSteps': 19200, 'rewardStep': 0.8878058986974147, 'errorList': [], 'lossList': [0.0, -1.276298503279686, 0.0, 2.78359116345644, 0.0, 0.0, 0.0], 'rewardMean': 0.7432240186080316, 'totalEpisodes': 227, 'stepsPerEpisode': 102, 'rewardPerEpisode': 86.28921185790324
'totalSteps': 20480, 'rewardStep': 0.8871783871557907, 'errorList': [], 'lossList': [0.0, -1.2359167784452438, 0.0, 2.0511687737703324, 0.0, 0.0, 0.0], 'rewardMean': 0.7343483574754132, 'totalEpisodes': 227, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.243657047399
'totalSteps': 21760, 'rewardStep': 0.9028186468764074, 'errorList': [], 'lossList': [0.0, -1.1795041555166244, 0.0, 1.3836901338398457, 0.0, 0.0, 0.0], 'rewardMean': 0.7634819768129467, 'totalEpisodes': 227, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.8866025428642
'totalSteps': 23040, 'rewardStep': 0.9521928484357602, 'errorList': [0.10590657790572566, 0.1379017648435137, 0.08982282723492979, 0.15695562898262033, 0.17355254231575828, 0.14131087999148537, 0.1920703636605773, 0.13547308851946846, 0.15776473619583745, 0.1601356096653471, 0.208423086012481, 0.05558524437098568, 0.09502704348201842, 0.1496150304411781, 0.14332174126073452, 0.1246644520386565, 0.08469450352489519, 0.16679151770854886, 0.18218814176503506, 0.08086522296837917, 0.18121820659364538, 0.1374444913115399, 0.09809789242386394, 0.10182222007635353, 0.078887282251032, 0.09800721041391067, 0.08185728421548735, 0.11661267094155553, 0.2291634273947286, 0.1419792217977705, 0.07351591157535815, 0.12149017115821671, 0.09653803508251609, 0.15603730985432274, 0.17817257872917813, 0.10496371338510634, 0.18593444044542146, 0.09096237819764347, 0.07233289763008921, 0.13545014234678193, 0.06892682534217691, 0.10831335476929635, 0.12265449349126517, 0.26186327433758866, 0.26442497522791863, 0.1633370806393781, 0.0988250439167356, 0.16545913814138483, 0.12234606621526631, 0.16214112802215036], 'lossList': [0.0, -1.1261867475509644, 0.0, 0.7864061106368899, 0.0, 0.0, 0.0], 'rewardMean': 0.7867833721123, 'totalEpisodes': 227, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1167.8555525670806, 'successfulTests': 46
'totalSteps': 24320, 'rewardStep': 0.9573060267799002, 'errorList': [0.04361713657881123, 0.12402688832656834, 0.04866114731730703, 0.023347324777508414, 0.022047814291283496, 0.035610555442701856, 0.09894017070331024, 0.07186710949811034, 0.052145407235616316, 0.0726987227690798, 0.0869158866824789, 0.07976827977066456, 0.03889103915111824, 0.066377090574318, 0.09729616816228027, 0.06572718343470504, 0.06882285116244416, 0.0547684369006687, 0.06069550381622422, 0.1468447086991356, 0.101770771934517, 0.06369768229119295, 0.07622575267403978, 0.07466832664061121, 0.13503619087496702, 0.09471613268583969, 0.057767300556646375, 0.07566437192297655, 0.10154592056299731, 0.048066004083729526, 0.17692568069204637, 0.04789342620686295, 0.05648216187951729, 0.017151887976769684, 0.031480043849400154, 0.04917545689217319, 0.10038109810348746, 0.09589564526226214, 0.06365018847913595, 0.08027845080905385, 0.11163947146032718, 0.09343263568765016, 0.0870998594761751, 0.14220797391941065, 0.0653784315069715, 0.02155066375380144, 0.05509419767833475, 0.0322845529875233, 0.10137359349791117, 0.04231221334241519], 'lossList': [0.0, -1.097589066028595, 0.0, 0.5860423570498824, 0.0, 0.0, 0.0], 'rewardMean': 0.8225506393664409, 'totalEpisodes': 227, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.4762145135346, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=113.78
