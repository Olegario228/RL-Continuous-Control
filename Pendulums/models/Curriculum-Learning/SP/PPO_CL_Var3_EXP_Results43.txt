#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 43
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.4392844773303958, 'errorList': [], 'lossList': [0.0, -1.4229393792152405, 0.0, 32.63551666259766, 0.0, 0.0, 0.0], 'rewardMean': 0.6487482929429678, 'totalEpisodes': 56, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.585727231442572
'totalSteps': 3840, 'rewardStep': 0.684234956136421, 'errorList': [], 'lossList': [0.0, -1.4125013065338134, 0.0, 42.00492520332337, 0.0, 0.0, 0.0], 'rewardMean': 0.6605771806741189, 'totalEpisodes': 118, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.057219846032554
'totalSteps': 5120, 'rewardStep': 0.9102425362501018, 'errorList': [], 'lossList': [0.0, -1.390095156431198, 0.0, 45.32015028953552, 0.0, 0.0, 0.0], 'rewardMean': 0.7229935195681146, 'totalEpisodes': 166, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.591567814306895
'totalSteps': 6400, 'rewardStep': 0.9092935774131944, 'errorList': [], 'lossList': [0.0, -1.368481032848358, 0.0, 47.855820446014405, 0.0, 0.0, 0.0], 'rewardMean': 0.7602535311371306, 'totalEpisodes': 201, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.671500236025983
'totalSteps': 7680, 'rewardStep': 0.9717603507503662, 'errorList': [132.74965874599528, 207.6157415805102, 149.44435341369683, 171.0811686726683, 217.9115845745467, 170.64746822749524, 171.05944983754657, 109.17049469151364, 231.68087814110916, 210.044933114107, 221.94348842724423, 259.96224137465737, 117.6406323657229, 236.37867858123994, 201.27886654000767, 190.73987242724704, 199.06525943399657, 80.27438932342494, 218.71965230532263, 169.67654372862805, 181.1418753301913, 180.69052098331147, 54.8187289406472, 197.95326919172865, 207.9829193608711, 159.45234920002557, 150.73862561220977, 268.7666759638334, 197.91930573153354, 193.5800336704008, 235.25071835320588, 143.79862543832223, 234.7055143032498, 199.18271178136237, 110.62838627721045, 268.368516322154, 273.27911062985294, 115.77578318501729, 168.69426079505988, 234.09839476403937, 209.81110322705058, 192.48253230270834, 226.22631122342926, 148.48998911183276, 240.20635268880255, 161.59938496537464, 236.66426310944962, 229.4362147300149, 250.2913844586195, 245.80038961027418], 'lossList': [0.0, -1.33726240336895, 0.0, 44.0479612827301, 0.0, 0.0, 0.0], 'rewardMean': 0.7955046677393365, 'totalEpisodes': 218, 'stepsPerEpisode': 14, 'rewardPerEpisode': 13.106125717017722, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.7406670091135916, 'errorList': [], 'lossList': [0.0, -1.3148722577095031, 0.0, 43.92101240158081, 0.0, 0.0, 0.0], 'rewardMean': 0.7876707165070872, 'totalEpisodes': 230, 'stepsPerEpisode': 41, 'rewardPerEpisode': 30.914727662240377
'totalSteps': 10240, 'rewardStep': 0.48908989179301876, 'errorList': [], 'lossList': [0.0, -1.3123352485895157, 0.0, 19.686842727661134, 0.0, 0.0, 0.0], 'rewardMean': 0.7503481134178286, 'totalEpisodes': 239, 'stepsPerEpisode': 189, 'rewardPerEpisode': 129.77294491917345
'totalSteps': 11520, 'rewardStep': 0.6795304878926883, 'errorList': [], 'lossList': [0.0, -1.3135415410995483, 0.0, 14.910040533542633, 0.0, 0.0, 0.0], 'rewardMean': 0.7424794883594797, 'totalEpisodes': 245, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.47895273232143
'totalSteps': 12800, 'rewardStep': 0.6557193603412556, 'errorList': [], 'lossList': [0.0, -1.3242892742156982, 0.0, 14.385828105211258, 0.0, 0.0, 0.0], 'rewardMean': 0.7338034755576573, 'totalEpisodes': 249, 'stepsPerEpisode': 62, 'rewardPerEpisode': 48.447228798258074
'totalSteps': 14080, 'rewardStep': 0.5620806482517305, 'errorList': [], 'lossList': [0.0, -1.3209942054748536, 0.0, 5.11806298494339, 0.0, 0.0, 0.0], 'rewardMean': 0.7041903295272764, 'totalEpisodes': 251, 'stepsPerEpisode': 307, 'rewardPerEpisode': 222.99741364300712
'totalSteps': 15360, 'rewardStep': 0.7092774759436165, 'errorList': [], 'lossList': [0.0, -1.2961610436439515, 0.0, 7.990958794355392, 0.0, 0.0, 0.0], 'rewardMean': 0.7311896293885984, 'totalEpisodes': 254, 'stepsPerEpisode': 152, 'rewardPerEpisode': 129.03798915847062
'totalSteps': 16640, 'rewardStep': 0.7868020549120961, 'errorList': [], 'lossList': [0.0, -1.2806737112998963, 0.0, 6.313049529790878, 0.0, 0.0, 0.0], 'rewardMean': 0.7414463392661659, 'totalEpisodes': 257, 'stepsPerEpisode': 84, 'rewardPerEpisode': 73.0425080905955
'totalSteps': 17920, 'rewardStep': 0.7968148268865922, 'errorList': [], 'lossList': [0.0, -1.2716864722967147, 0.0, 3.2588330319523813, 0.0, 0.0, 0.0], 'rewardMean': 0.730103568329815, 'totalEpisodes': 257, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 996.1566243092376
'totalSteps': 19200, 'rewardStep': 0.9300084410941085, 'errorList': [0.08947416497321949, 0.0899288710185599, 0.11024473721629517, 0.10371761040909482, 0.1435089011854543, 0.10170072191574477, 0.07016312537574182, 0.091987595829858, 0.09015614232248724, 0.06908410357350384, 0.07170796839673313, 0.094190044470767, 0.0873053609398969, 0.08977981787059493, 0.09235023147716515, 0.1455250597287696, 0.08803031659221458, 0.071316982403962, 0.14890224123574497, 0.13967861309699062, 0.07913988601463623, 0.12990202982459373, 0.11244336892310469, 0.075775877288708, 0.11275124904732783, 0.10928445501143576, 0.12112845307552608, 0.07672691552762305, 0.0744987806848679, 0.07500880836570542, 0.14380342203037141, 0.10870495849551337, 0.07778087363285881, 0.1570685246224325, 0.08647991177269726, 0.09411148530398643, 0.09307127826610734, 0.07068233589405157, 0.11141692543616552, 0.10531367080339393, 0.09603534154641329, 0.11338697271704197, 0.09681751448710452, 0.07465844555222687, 0.14616719233470246, 0.09078174725308713, 0.1548309778293717, 0.06897158150504946, 0.09359573930594728, 0.1469828628868318], 'lossList': [0.0, -1.26671927690506, 0.0, 2.024228466153145, 0.0, 0.0, 0.0], 'rewardMean': 0.7321750546979064, 'totalEpisodes': 257, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.5482802297304, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=82.85
