#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 44
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.7665353838405435, 'errorList': [], 'lossList': [0.0, -1.409446073770523, 0.0, 32.6730126285553, 0.0, 0.0, 0.0], 'rewardMean': 0.7449431538599223, 'totalEpisodes': 58, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.13755298938835
'totalSteps': 3840, 'rewardStep': 0.9106760607574856, 'errorList': [], 'lossList': [0.0, -1.405803741812706, 0.0, 31.5032981300354, 0.0, 0.0, 0.0], 'rewardMean': 0.8001874561591101, 'totalEpisodes': 133, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.714914152906168
'totalSteps': 5120, 'rewardStep': 0.9506414077344125, 'errorList': [], 'lossList': [0.0, -1.398267070055008, 0.0, 40.73427406311035, 0.0, 0.0, 0.0], 'rewardMean': 0.8378009440529357, 'totalEpisodes': 166, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.812237844221116
'totalSteps': 6400, 'rewardStep': 0.7682114201420669, 'errorList': [], 'lossList': [0.0, -1.387780680656433, 0.0, 41.73134909629822, 0.0, 0.0, 0.0], 'rewardMean': 0.8238830392707619, 'totalEpisodes': 178, 'stepsPerEpisode': 35, 'rewardPerEpisode': 26.598741719706734
'totalSteps': 7680, 'rewardStep': 0.7976515422907472, 'errorList': [], 'lossList': [0.0, -1.3779228854179382, 0.0, 39.6416930437088, 0.0, 0.0, 0.0], 'rewardMean': 0.8195111231074262, 'totalEpisodes': 184, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.4281531987243
'totalSteps': 8960, 'rewardStep': 0.8286690910642346, 'errorList': [], 'lossList': [0.0, -1.3696313387155532, 0.0, 47.77149796962738, 0.0, 0.0, 0.0], 'rewardMean': 0.8208194042441131, 'totalEpisodes': 191, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.378999489284844
'totalSteps': 10240, 'rewardStep': 0.4909297817668147, 'errorList': [], 'lossList': [0.0, -1.3546966290473939, 0.0, 14.691707206368447, 0.0, 0.0, 0.0], 'rewardMean': 0.7795832014344508, 'totalEpisodes': 192, 'stepsPerEpisode': 1097, 'rewardPerEpisode': 739.2455633878193
'totalSteps': 11520, 'rewardStep': 0.8003149953430174, 'errorList': [], 'lossList': [0.0, -1.3303542840480804, 0.0, 18.27147445678711, 0.0, 0.0, 0.0], 'rewardMean': 0.7818867340909582, 'totalEpisodes': 197, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.607057825249899
'totalSteps': 12800, 'rewardStep': 0.867805818446596, 'errorList': [], 'lossList': [0.0, -1.318797124028206, 0.0, 26.193400428295135, 0.0, 0.0, 0.0], 'rewardMean': 0.790478642526522, 'totalEpisodes': 200, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.59266173573343
'totalSteps': 14080, 'rewardStep': 0.5343318986008536, 'errorList': [], 'lossList': [0.0, -1.3118795031309127, 0.0, 7.771610770821571, 0.0, 0.0, 0.0], 'rewardMean': 0.7715767399986773, 'totalEpisodes': 201, 'stepsPerEpisode': 1100, 'rewardPerEpisode': 870.3291674543922
'totalSteps': 15360, 'rewardStep': 0.9349974822547433, 'errorList': [0.27523365444917736, 0.28559070698858524, 0.26723545358858475, 0.2844532124405456, 0.28822128755586873, 0.26940426030632747, 0.288687572236417, 0.27743827373350377, 0.27985215539538016, 0.2693838691197645, 0.38573642466371516, 0.287262577297451, 0.2562491620578345, 0.2804797187660022, 0.28874421018762253, 0.2880154137567209, 0.27930138353506967, 0.2928991903656966, 0.29120578538726, 0.2749647693027205, 0.28875001125155114, 0.309088394568986, 0.2794568734685702, 0.3026524555665575, 0.28509091752670945, 0.2739831814394662, 0.283128482944283, 0.29186909989692456, 0.28528857691526877, 0.269034797500945, 0.2739215730836422, 0.30409053709799155, 0.2750881400598781, 0.285857239179302, 0.29024690300399536, 0.28133357637800616, 0.2901764840345314, 0.29107847654591545, 0.2714960820008115, 0.29253854233559207, 0.3350354661343003, 0.28351121190756406, 0.2889386136966949, 0.2873330832119309, 0.2867211123850756, 0.28173989346945405, 0.2852025747711229, 0.28883175660570964, 0.28474938805368716, 0.27603758951153207], 'lossList': [0.0, -1.2998576819896699, 0.0, 4.846563184857368, 0.0, 0.0, 0.0], 'rewardMean': 0.7884229498400973, 'totalEpisodes': 202, 'stepsPerEpisode': 1259, 'rewardPerEpisode': 1004.8051702601574, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6482656978827245, 'errorList': [], 'lossList': [0.0, -1.255146479010582, 0.0, 1.3996504271030425, 0.0, 0.0, 0.0], 'rewardMean': 0.7621819135526211, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 999.2677770821639
'totalSteps': 17920, 'rewardStep': 0.8013433120403018, 'errorList': [], 'lossList': [0.0, -1.2203983056545258, 0.0, 1.7245250822231173, 0.0, 0.0, 0.0], 'rewardMean': 0.7472521039832101, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.5525108853037
'totalSteps': 19200, 'rewardStep': 0.6833384615708755, 'errorList': [], 'lossList': [0.0, -1.2098426628112793, 0.0, 2.2301442375779152, 0.0, 0.0, 0.0], 'rewardMean': 0.7387648081260909, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 966.5955467975392
'totalSteps': 20480, 'rewardStep': 0.965649246189861, 'errorList': [0.13264669162656254, 0.1081261698284762, 0.24778168265079628, 0.2591790481537254, 0.15809016572502788, 0.20634314599609788, 0.16987436859234464, 0.33566267844587244, 0.095997173787917, 0.10838408656784895, 0.296132426074241, 0.10907609065604795, 0.10039966229835322, 0.37495038379802637, 0.12717400613773117, 0.1936631911411497, 0.09197170590546792, 0.0519277120974327, 0.11915598606053007, 0.08803941219267956, 0.06259868650165927, 0.1672883670207742, 0.24479794863908474, 0.36237465552119247, 0.21994290815864617, 0.3399021952338458, 0.14866077812166037, 0.12283045512267023, 0.47627482471670896, 0.16818316467385616, 0.17749690925938205, 0.09576211087522843, 0.0648028726337046, 0.14098815179991736, 0.11394898633786642, 0.07503234637242395, 0.26428329542847023, 0.3522869373420793, 0.34326884656099377, 0.3126379942908312, 0.11842240105592103, 0.09511340667647922, 0.2886692912035375, 0.20025561598328143, 0.28457670101834537, 0.06494219483503337, 0.15425169954662066, 0.22661591796737723, 0.08481801655357353, 0.10848596587202866], 'lossList': [0.0, -1.1936118817329406, 0.0, 1.0386970715224744, 0.0, 0.0, 0.0], 'rewardMean': 0.7555645785160023, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.0546729690932, 'successfulTests': 31
'totalSteps': 21760, 'rewardStep': 0.8458178325252586, 'errorList': [], 'lossList': [0.0, -1.1541449397802352, 0.0, 0.5978330480307341, 0.0, 0.0, 0.0], 'rewardMean': 0.7572794526621047, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.114829582354
'totalSteps': 23040, 'rewardStep': 0.9465961392834762, 'errorList': [0.06889999719740957, 0.2583548240209153, 0.057185276356109316, 0.21373706942823306, 0.10784960498747703, 0.14486497762676212, 0.1519487099977759, 0.1724790638258735, 0.09706977115413948, 0.09172068308467425, 0.05713200139851551, 0.05525956538078902, 0.1357620416696632, 0.09815140127307366, 0.07972194927382871, 0.01644377242244846, 0.14954491176491122, 0.03508733870327208, 0.16688662620404135, 0.1323684714049228, 0.1847472144921953, 0.10228599490468233, 0.0839964976901665, 0.13905344434046515, 0.2980076944876104, 0.01895857862886026, 0.09961069242188962, 0.10097666023094409, 0.06993145003810348, 0.16391017696780336, 0.04265195510565481, 0.34223869523724987, 0.272842211082433, 0.31017680644168416, 0.0648248851719687, 0.017217523224825147, 0.04695653138864022, 0.21291932564371163, 0.20487021111259604, 0.14566475846921445, 0.17377025196908946, 0.19936645451517224, 0.24251745050113752, 0.2379696118739229, 0.16832123349607403, 0.1368018823321358, 0.16823647390601762, 0.025520770189362964, 0.011040381708924199, 0.06785674811645927], 'lossList': [0.0, -1.1262154537439346, 0.0, 0.3638963468745351, 0.0, 0.0, 0.0], 'rewardMean': 0.8028460884137708, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.2854577772148, 'successfulTests': 40
'totalSteps': 24320, 'rewardStep': 0.9466648090191739, 'errorList': [0.11086089006266253, 0.03831454240250548, 0.13949265828838808, 0.1169851896013118, 0.08446083116673041, 0.07148616801795865, 0.09015948505172977, 0.04583093291204114, 0.02887842335727988, 0.028668837331279083, 0.0847094463391041, 0.10396851140500378, 0.075424174784089, 0.057228401250146055, 0.029641926742152543, 0.07341692299770242, 0.055555978542352925, 0.09488939949361884, 0.12913233020210077, 0.04832813529698007, 0.023318196609746617, 0.06293917455535944, 0.13577713800730912, 0.08425589747849044, 0.02869915278916789, 0.06684196090204822, 0.036362885136205524, 0.07787985150232551, 0.10282560963639512, 0.19022496673685427, 0.16243697407857696, 0.07010341061460909, 0.05343103108366572, 0.08136756493373769, 0.09799296440087069, 0.08776311797097423, 0.08303524314800365, 0.0448123690406265, 0.0891554823270175, 0.03523095801642994, 0.02071241653603776, 0.12063198808369771, 0.027706674957939787, 0.13323687382925942, 0.0606312043471219, 0.0635374819579967, 0.04317337303892059, 0.06569585773024243, 0.054256480490528544, 0.0882225620418368], 'lossList': [0.0, -1.1101324898004532, 0.0, 0.2933742164541036, 0.0, 0.0, 0.0], 'rewardMean': 0.8174810697813865, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1178.6298441999352, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=135.05
