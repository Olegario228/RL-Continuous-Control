#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 45
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.6654529829935494, 'errorList': [], 'lossList': [0.0, -1.4291463470458985, 0.0, 34.144946632385256, 0.0, 0.0, 0.0], 'rewardMean': 0.7901368067871888, 'totalEpisodes': 40, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.774510030098664
'totalSteps': 3840, 'rewardStep': 0.8381841433688346, 'errorList': [], 'lossList': [0.0, -1.4165940797328949, 0.0, 38.21921126365662, 0.0, 0.0, 0.0], 'rewardMean': 0.8061525856477374, 'totalEpisodes': 109, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.9406567923039457
'totalSteps': 5120, 'rewardStep': 0.5591814636785162, 'errorList': [], 'lossList': [0.0, -1.3992516964673996, 0.0, 39.862582225799564, 0.0, 0.0, 0.0], 'rewardMean': 0.7444098051554321, 'totalEpisodes': 153, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.0812441385638625
'totalSteps': 6400, 'rewardStep': 0.7104776505102541, 'errorList': [], 'lossList': [0.0, -1.385967322587967, 0.0, 46.350538682937625, 0.0, 0.0, 0.0], 'rewardMean': 0.7376233742263965, 'totalEpisodes': 169, 'stepsPerEpisode': 202, 'rewardPerEpisode': 150.6066917064718
'totalSteps': 7680, 'rewardStep': 0.8239790553602954, 'errorList': [], 'lossList': [0.0, -1.3769776326417924, 0.0, 31.083121309280397, 0.0, 0.0, 0.0], 'rewardMean': 0.752015987748713, 'totalEpisodes': 178, 'stepsPerEpisode': 68, 'rewardPerEpisode': 50.035244287038736
'totalSteps': 8960, 'rewardStep': 0.9263491332690446, 'errorList': [], 'lossList': [0.0, -1.3691083407402038, 0.0, 35.162473216056824, 0.0, 0.0, 0.0], 'rewardMean': 0.7769207228230461, 'totalEpisodes': 186, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.148430314018103
'totalSteps': 10240, 'rewardStep': 0.7084885710163603, 'errorList': [], 'lossList': [0.0, -1.3800756514072419, 0.0, 30.3319637465477, 0.0, 0.0, 0.0], 'rewardMean': 0.7683667038472104, 'totalEpisodes': 190, 'stepsPerEpisode': 109, 'rewardPerEpisode': 86.13231480171042
'totalSteps': 11520, 'rewardStep': 0.7451933418305101, 'errorList': [], 'lossList': [0.0, -1.3681280064582824, 0.0, 19.867943776845934, 0.0, 0.0, 0.0], 'rewardMean': 0.7657918858453548, 'totalEpisodes': 195, 'stepsPerEpisode': 68, 'rewardPerEpisode': 61.88167871746193
'totalSteps': 12800, 'rewardStep': 0.8134500706843453, 'errorList': [], 'lossList': [0.0, -1.3583698332309724, 0.0, 10.76436686873436, 0.0, 0.0, 0.0], 'rewardMean': 0.7705577043292539, 'totalEpisodes': 198, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.4521317116651504
'totalSteps': 14080, 'rewardStep': 0.7351466079258697, 'errorList': [], 'lossList': [0.0, -1.365534669160843, 0.0, 6.32296455681324, 0.0, 0.0, 0.0], 'rewardMean': 0.752590302063758, 'totalEpisodes': 200, 'stepsPerEpisode': 225, 'rewardPerEpisode': 170.82553360492994
'totalSteps': 15360, 'rewardStep': 0.4902708339059073, 'errorList': [], 'lossList': [0.0, -1.3799681544303894, 0.0, 6.603568916916847, 0.0, 0.0, 0.0], 'rewardMean': 0.7350720871549938, 'totalEpisodes': 202, 'stepsPerEpisode': 257, 'rewardPerEpisode': 196.64786921647564
'totalSteps': 16640, 'rewardStep': 0.9719578665327446, 'errorList': [0.17194326288795492, 0.180677464572885, 0.1964849758845261, 0.18750312430804192, 0.22141187780827698, 0.1860385105815234, 0.21810166371734035, 0.22924219244763266, 0.19910649855672383, 0.1865213774856283, 0.19635320955375235, 0.23942499808158219, 0.18674836160395636, 0.22169828439578482, 0.2245867666871271, 0.18978856677358377, 0.19378380738731318, 0.28800390343127696, 0.16770759241112454, 0.1784117075429355, 0.17733598354277413, 0.19241000504163389, 0.17849401005219873, 0.19422192081430992, 0.1750146756145182, 0.20858476345175556, 0.1978441036254803, 0.18111847344797577, 0.1856669227470699, 0.18006219305533544, 0.1835822913997648, 0.1897618653711967, 0.182455812968557, 0.17614887817957078, 0.20825615356784993, 0.17080328914552745, 0.18697710921826832, 0.2058595249549253, 0.19568864149631154, 0.19137996447671357, 0.17259921722986657, 0.1643059902378833, 0.20597637430210097, 0.19209581921531563, 0.17883999633949935, 0.19177502351928233, 0.17496993175112246, 0.18015186391448912, 0.17207931080411557, 0.18838536523268032], 'lossList': [0.0, -1.3714864164590836, 0.0, 3.7964340218901635, 0.0, 0.0, 0.0], 'rewardMean': 0.7484494594713847, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.911033989649, 'successfulTests': 39
'totalSteps': 17920, 'rewardStep': 0.8366496070113659, 'errorList': [], 'lossList': [0.0, -1.3365749108791352, 0.0, 1.296487531736493, 0.0, 0.0, 0.0], 'rewardMean': 0.7761962738046698, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.4159590348554
'totalSteps': 19200, 'rewardStep': 0.8735504593576177, 'errorList': [], 'lossList': [0.0, -1.2970355713367463, 0.0, 1.110888742879033, 0.0, 0.0, 0.0], 'rewardMean': 0.7925035546894061, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.2650326148505
'totalSteps': 20480, 'rewardStep': 0.9054912975085723, 'errorList': [], 'lossList': [0.0, -1.260909903049469, 0.0, 1.4899351151287556, 0.0, 0.0, 0.0], 'rewardMean': 0.8006547789042339, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.9178763862444
'totalSteps': 21760, 'rewardStep': 0.9736142375445779, 'errorList': [0.1285173958743447, 0.10493585694843785, 0.1265308978929365, 0.15463395545557143, 0.16671690420654786, 0.18754269815977043, 0.12224735913681066, 0.1354319932640101, 0.11731993254114051, 0.13071433380969733, 0.11971736716599955, 0.11986834279941144, 0.16280000415822718, 0.1513369312388397, 0.13313213598114287, 0.22302499873522957, 0.1018215943092024, 0.13268169920311462, 0.11386283993881581, 0.12290078170405586, 0.2374317803946841, 0.14610956734842473, 0.15805481133640834, 0.15674751134758327, 0.11396886004221349, 0.1274510816861552, 0.12488977132549721, 0.10821261579770623, 0.20043802303534694, 0.18759727976331486, 0.13523247484139403, 0.12453117498787289, 0.10999876546734996, 0.11486701102184114, 0.15503375055351884, 0.16492473562495477, 0.16584530610340878, 0.11244362969559604, 0.13528827552489597, 0.14713094253119258, 0.12757952142047532, 0.14256938746061998, 0.1643191346615832, 0.14301627372632592, 0.288264657375869, 0.12971887608058597, 0.12676984367122848, 0.15157006790162247, 0.11077697253128985, 0.1812240536868223], 'lossList': [0.0, -1.2288140779733658, 0.0, 1.045004154779017, 0.0, 0.0, 0.0], 'rewardMean': 0.805381289331787, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.0626679867967, 'successfulTests': 46
'totalSteps': 23040, 'rewardStep': 0.8815822202084815, 'errorList': [], 'lossList': [0.0, -1.1936950391530992, 0.0, 0.6799814949184656, 0.0, 0.0, 0.0], 'rewardMean': 0.8226906542509994, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1167.736356189153
'totalSteps': 24320, 'rewardStep': 0.7027850569667303, 'errorList': [], 'lossList': [0.0, -1.1633533668518066, 0.0, 0.26393947066739204, 0.0, 0.0, 0.0], 'rewardMean': 0.8184498257646211, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.446562823884
'totalSteps': 25600, 'rewardStep': 0.9569666235448802, 'errorList': [0.17722150173602202, 0.06601069737031037, 0.0659878812153689, 0.18586496597299448, 0.10604288735325075, 0.06970354686827188, 0.16351031152237505, 0.11245622350863708, 0.06632781849841059, 0.08447067790002644, 0.0703161368950534, 0.08403680123235212, 0.08871032855242329, 0.1309437055961329, 0.0645218495592758, 0.1609330039278209, 0.10747396195306061, 0.07619700007245404, 0.25919771897877075, 0.08527005095590108, 0.134282261941585, 0.07246764384214577, 0.3240267175574668, 0.1065507138312932, 0.15608929383170694, 0.33458544075982627, 0.12910715387072377, 0.1458037668028469, 0.10232392079497558, 0.15437708481794427, 0.18313792150708313, 0.1262449447894405, 0.08246433594541107, 0.07630844917674216, 0.19008874177171003, 0.12824475220905562, 0.11114667660221796, 0.16933800404491012, 0.1052330106603087, 0.17158877751563031, 0.10660061859963504, 0.10140948661158648, 0.15486057788152077, 0.23469727105811908, 0.22608631211330868, 0.20057567556138325, 0.11437240063331149, 0.09961405918123922, 0.12847871270479164, 0.12215134052558033], 'lossList': [0.0, -1.1267498016357422, 0.0, 0.36540104636922477, 0.0, 0.0, 0.0], 'rewardMean': 0.8328014810506748, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.3217629476296, 'successfulTests': 44
#maxSuccessfulTests=46, maxSuccessfulTestsAtStep=21760, timeSpent=118.41
