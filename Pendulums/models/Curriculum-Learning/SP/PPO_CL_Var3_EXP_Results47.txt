#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 47
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.876867640123686, 'errorList': [], 'lossList': [0.0, -1.418892312645912, 0.0, 33.17849097251892, 0.0, 0.0, 0.0], 'rewardMean': 0.6899842440138488, 'totalEpisodes': 46, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.422162668203752
'totalSteps': 3840, 'rewardStep': 0.6871475447775891, 'errorList': [], 'lossList': [0.0, -1.3885026544332504, 0.0, 37.62542426109314, 0.0, 0.0, 0.0], 'rewardMean': 0.6890386776017623, 'totalEpisodes': 108, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.748690705880948
'totalSteps': 5120, 'rewardStep': 0.6493710055663398, 'errorList': [], 'lossList': [0.0, -1.3678237229585648, 0.0, 45.993075675964356, 0.0, 0.0, 0.0], 'rewardMean': 0.6791217595929067, 'totalEpisodes': 170, 'stepsPerEpisode': 34, 'rewardPerEpisode': 18.514928483745877
'totalSteps': 6400, 'rewardStep': 0.5997832921211091, 'errorList': [], 'lossList': [0.0, -1.3567077726125718, 0.0, 43.78812803268433, 0.0, 0.0, 0.0], 'rewardMean': 0.6632540660985471, 'totalEpisodes': 195, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.722953552186254
'totalSteps': 7680, 'rewardStep': 0.24153439808353122, 'errorList': [], 'lossList': [0.0, -1.33791328728199, 0.0, 42.66299593925476, 0.0, 0.0, 0.0], 'rewardMean': 0.5929674547627112, 'totalEpisodes': 206, 'stepsPerEpisode': 284, 'rewardPerEpisode': 210.0001703146508
'totalSteps': 8960, 'rewardStep': 0.503532769402378, 'errorList': [], 'lossList': [0.0, -1.3327746540307999, 0.0, 16.807423803806305, 0.0, 0.0, 0.0], 'rewardMean': 0.5801910711398064, 'totalEpisodes': 215, 'stepsPerEpisode': 133, 'rewardPerEpisode': 97.73208388977463
'totalSteps': 10240, 'rewardStep': 0.6861767063099253, 'errorList': [], 'lossList': [0.0, -1.3428062629699706, 0.0, 14.453501921892165, 0.0, 0.0, 0.0], 'rewardMean': 0.5934392755360713, 'totalEpisodes': 220, 'stepsPerEpisode': 109, 'rewardPerEpisode': 94.07987541295374
'totalSteps': 11520, 'rewardStep': 0.9950476120104622, 'errorList': [0.6850525319888997, 3.3350415582883954, 3.229889835965966, 0.11352133208253584, 2.7069585968954484, 1.5571682423175013, 2.000233766824797, 2.00189665342824, 0.3859445387137893, 0.5792551247578616, 1.1501034012209064, 2.9631069964984524, 0.09789785347720287, 1.9975986699576431, 0.20592408813269508, 1.2653614027570965, 4.129610802310952, 1.2102561469394375, 1.7079269380089475, 0.6574399514572147, 4.656266204426516, 0.4347138512423754, 3.575745722002895, 0.882227728483753, 1.9573370460638704, 5.8732937448306055, 1.8518236913795134, 2.785475126234628, 1.2445449274934528, 1.2977084140020152, 6.575700280224416, 2.790847478998852, 0.09689361627789828, 0.7813641624995565, 1.2423692104247344, 4.792188059285468, 3.4874112902201637, 3.2430155893276624, 1.3651651866698917, 2.4486858766652073, 3.8653295645994885, 5.389077745040539, 0.09273916596358435, 2.9673012820350855, 1.5274611993522618, 1.0682533520726905, 3.0000112517193926, 3.5625365990940723, 3.7612079170480723, 2.3789601248662597], 'lossList': [0.0, -1.3527782833576203, 0.0, 13.435234454870224, 0.0, 0.0, 0.0], 'rewardMean': 0.6380624240332259, 'totalEpisodes': 224, 'stepsPerEpisode': 111, 'rewardPerEpisode': 97.64416852939789, 'successfulTests': 4
'totalSteps': 12800, 'rewardStep': 0.8743868287803986, 'errorList': [], 'lossList': [0.0, -1.3503178930282593, 0.0, 8.122222600579262, 0.0, 0.0, 0.0], 'rewardMean': 0.6616948645079431, 'totalEpisodes': 226, 'stepsPerEpisode': 315, 'rewardPerEpisode': 266.28976137834553
'totalSteps': 14080, 'rewardStep': 0.8484179302826717, 'errorList': [], 'lossList': [0.0, -1.3463699841499328, 0.0, 5.502774515748024, 0.0, 0.0, 0.0], 'rewardMean': 0.696226572745809, 'totalEpisodes': 228, 'stepsPerEpisode': 657, 'rewardPerEpisode': 576.8288591940741
'totalSteps': 15360, 'rewardStep': 0.4854919992431224, 'errorList': [], 'lossList': [0.0, -1.3269107669591904, 0.0, 22.344039140939714, 0.0, 0.0, 0.0], 'rewardMean': 0.6570890086577529, 'totalEpisodes': 230, 'stepsPerEpisode': 480, 'rewardPerEpisode': 325.6347056805937
'totalSteps': 16640, 'rewardStep': 0.8177996848480951, 'errorList': [], 'lossList': [0.0, -1.3048494410514833, 0.0, 3.314907765984535, 0.0, 0.0, 0.0], 'rewardMean': 0.6701542226648034, 'totalEpisodes': 231, 'stepsPerEpisode': 1268, 'rewardPerEpisode': 1050.2226654987085
'totalSteps': 17920, 'rewardStep': 0.7683703771559023, 'errorList': [], 'lossList': [0.0, -1.2539034593105316, 0.0, 1.471431438922882, 0.0, 0.0, 0.0], 'rewardMean': 0.6820541598237597, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 968.151530897008
'totalSteps': 19200, 'rewardStep': 0.9278882412304157, 'errorList': [], 'lossList': [0.0, -1.2064015698432922, 0.0, 1.696861951649189, 0.0, 0.0, 0.0], 'rewardMean': 0.7148646547346902, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.033911338702
'totalSteps': 20480, 'rewardStep': 0.965969251380482, 'errorList': [0.08786238142129076, 0.16040508152583688, 0.1218292207676817, 0.08876656993509266, 0.1185162190358851, 0.11342254339825403, 0.16078481179494156, 0.0944152510600864, 0.12849747752691515, 0.11964075176746314, 0.10061535103375296, 0.13283635695097668, 0.10262362429561177, 0.11026579167353892, 0.11421479868661713, 0.09830474485063256, 0.10313461061775006, 0.10709109809650173, 0.09180838001689098, 0.11816226303966122, 0.10008349959240585, 0.09052674198863762, 0.1923306710565912, 0.1401523630305271, 0.11511129037109029, 0.11879345372576021, 0.0882037620005456, 0.0814872144575936, 0.1368641489769754, 0.11379574140418543, 0.09378199251296569, 0.0877912423990858, 0.19258024877823385, 0.09396673110384922, 0.11276283758647057, 0.10618637319717382, 0.11208015175126129, 0.10093966548745252, 0.11280484962500668, 0.1451136429001643, 0.09842638825792856, 0.2064038261123981, 0.10145307203170875, 0.22249459924262993, 0.09243051089553268, 0.12206077805070592, 0.09919609529216782, 0.0839960094191901, 0.08916310616626053, 0.10442163341607102], 'lossList': [0.0, -1.1474480962753295, 0.0, 1.7092439563572406, 0.0, 0.0, 0.0], 'rewardMean': 0.7873081400643853, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.3045419192215, 'successfulTests': 48
'totalSteps': 21760, 'rewardStep': 0.9328703910005562, 'errorList': [0.16764068369917293, 0.18851370807091652, 0.11036254175355947, 0.19630335848875344, 0.19478835572852118, 0.17409988373572832, 0.20483999506542538, 0.20292108870914455, 0.1402305299604683, 0.11486619040973926, 0.14863282340698739, 0.19816145188265535, 0.24465374749318541, 0.1485675116119853, 0.1469419026446739, 0.18615614909423264, 0.15894529374464705, 0.1255508927716353, 0.10723316878306423, 0.11779808875334447, 0.17171129588973752, 0.17766789483058976, 0.18628537938892129, 0.21365132286903632, 0.19415650558655367, 0.16210562608789894, 0.1532252621151607, 0.18145460466852104, 0.12946901592781562, 0.22573174050055178, 0.21177349413472707, 0.2358299773648098, 0.2055095009527348, 0.19597388945479108, 0.17110564245764692, 0.12347564522217477, 0.18160163254008213, 0.22916966350723694, 0.2410716577260571, 0.16286930999618324, 0.22876087960003072, 0.1397285013943348, 0.14684449982026146, 0.23106655469231518, 0.21251683183210526, 0.1826301026277706, 0.11119580617606149, 0.16193152672527747, 0.11399312691701034, 0.18518279510817812], 'lossList': [0.0, -1.0895905232429504, 0.0, 1.1441379939951002, 0.0, 0.0, 0.0], 'rewardMean': 0.8302419022242031, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.3750751870052, 'successfulTests': 37
'totalSteps': 23040, 'rewardStep': 0.9643709018609435, 'errorList': [0.1707596358454053, 0.20037198627512237, 0.19293580158169438, 0.16640857629739478, 0.17520495577840692, 0.18648483644469455, 0.21719940724698802, 0.17612365305885025, 0.21493728134362508, 0.14709031527185795, 0.22610222245737416, 0.15127578252351856, 0.14089392973746548, 0.17213133495380392, 0.23990065965694113, 0.1067337781907775, 0.16969905612032715, 0.22457437900919186, 0.18616651160550662, 0.19272954458477592, 0.1861998252184908, 0.17974341815223357, 0.26588744841814005, 0.1907544401558783, 0.20354651090553869, 0.14757350105705552, 0.1692989506912402, 0.2024347473905187, 0.16510857306793048, 0.12067838257841022, 0.1759145725214781, 0.21946133885459376, 0.15901607592261732, 0.24176048582242343, 0.21512156557480733, 0.25144924547571507, 0.11957219955545521, 0.16615231232618802, 0.23395607727928375, 0.20518507029954028, 0.1916830320061767, 0.13843288820268002, 0.2618659869707312, 0.2028406925898917, 0.16778615291512966, 0.15807245747930265, 0.22390912313343095, 0.19732071447836663, 0.14975873747448032, 0.13732248094349847], 'lossList': [0.0, -1.0571925556659698, 0.0, 0.8245856383349747, 0.0, 0.0, 0.0], 'rewardMean': 0.858061321779305, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.2685545366953, 'successfulTests': 32
'totalSteps': 24320, 'rewardStep': 0.8944378269561405, 'errorList': [], 'lossList': [0.0, -1.036626648902893, 0.0, 0.5441528515983373, 0.0, 0.0, 0.0], 'rewardMean': 0.8480003432738729, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.7826165601375
'totalSteps': 25600, 'rewardStep': 0.876352757445498, 'errorList': [], 'lossList': [0.0, -1.0231832057237624, 0.0, 0.40504390746355057, 0.0, 0.0, 0.0], 'rewardMean': 0.8481969361403827, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1196.1706394611535
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=20480, timeSpent=140.19
