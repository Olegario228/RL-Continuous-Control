#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 48
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5074567484059298, 'errorList': [], 'lossList': [0.0, -1.4346552640199661, 0.0, 31.487408094406128, 0.0, 0.0, 0.0], 'rewardMean': 0.7031935821163351, 'totalEpisodes': 43, 'stepsPerEpisode': 36, 'rewardPerEpisode': 25.791717276094065
'totalSteps': 3840, 'rewardStep': 0.6395846343102944, 'errorList': [], 'lossList': [0.0, -1.440692386031151, 0.0, 36.896410655975345, 0.0, 0.0, 0.0], 'rewardMean': 0.6819905995143216, 'totalEpisodes': 114, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.138378702745308
'totalSteps': 5120, 'rewardStep': 0.6474202344973511, 'errorList': [], 'lossList': [0.0, -1.4158725446462632, 0.0, 39.07303683280945, 0.0, 0.0, 0.0], 'rewardMean': 0.673348008260079, 'totalEpisodes': 160, 'stepsPerEpisode': 28, 'rewardPerEpisode': 20.96024834716693
'totalSteps': 6400, 'rewardStep': 0.9533957988875436, 'errorList': [33.292188557700015, 43.06966421769828, 52.86815550206885, 30.09738303257026, 38.57087546359425, 64.53207234567559, 45.273608489311215, 22.989854212400566, 30.384509519430544, 32.27721217966967, 56.511228861724454, 42.01995928228532, 24.021493736005105, 60.859151648108046, 25.146830920467956, 63.271349324125225, 4.49863530483286, 50.1294388408997, 56.547284948319344, 17.64192732510938, 29.661324599487752, 34.67211089957727, 10.621670356397738, 30.519952203775436, 4.077888163140609, 46.38231862278308, 13.220181359052456, 93.413910539728, 13.85719397572627, 44.321588836137344, 1.4923320569661502, 23.68771122403831, 2.070838471524705, 52.02900249963887, 5.200748510696231, 39.01890022805223, 87.50027983695617, 3.98950942004322, 61.99004452931779, 5.5078821671457865, 49.48819575271423, 5.592751565090419, 8.712027305603822, 19.263351006047735, 75.20545485870727, 51.326082782740805, 43.276467609776766, 19.583377852819112, 3.98325955885845, 84.65564380066809], 'lossList': [0.0, -1.3923079735040664, 0.0, 44.132417125701906, 0.0, 0.0, 0.0], 'rewardMean': 0.7293575663855719, 'totalEpisodes': 188, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.885487739720943, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.9111745708302307, 'errorList': [], 'lossList': [0.0, -1.379525230526924, 0.0, 38.57701281547546, 0.0, 0.0, 0.0], 'rewardMean': 0.7596604004596818, 'totalEpisodes': 203, 'stepsPerEpisode': 52, 'rewardPerEpisode': 45.15884450187811
'totalSteps': 8960, 'rewardStep': 0.8903721592930338, 'errorList': [], 'lossList': [0.0, -1.3614974981546402, 0.0, 21.048558239936828, 0.0, 0.0, 0.0], 'rewardMean': 0.7783335088644464, 'totalEpisodes': 208, 'stepsPerEpisode': 51, 'rewardPerEpisode': 41.33673796305936
'totalSteps': 10240, 'rewardStep': 0.8051359641440324, 'errorList': [], 'lossList': [0.0, -1.3519181597232819, 0.0, 28.787032566070557, 0.0, 0.0, 0.0], 'rewardMean': 0.7816838157743945, 'totalEpisodes': 213, 'stepsPerEpisode': 162, 'rewardPerEpisode': 142.01499999388807
'totalSteps': 11520, 'rewardStep': 0.37092727506014955, 'errorList': [], 'lossList': [0.0, -1.3489467644691466, 0.0, 8.06206253528595, 0.0, 0.0, 0.0], 'rewardMean': 0.7360442001394785, 'totalEpisodes': 215, 'stepsPerEpisode': 453, 'rewardPerEpisode': 289.26966400903996
'totalSteps': 12800, 'rewardStep': 0.8394107418071894, 'errorList': [], 'lossList': [0.0, -1.3453767585754395, 0.0, 8.956839709877968, 0.0, 0.0, 0.0], 'rewardMean': 0.7463808543062495, 'totalEpisodes': 217, 'stepsPerEpisode': 204, 'rewardPerEpisode': 164.51685737878938
'totalSteps': 14080, 'rewardStep': 0.8877419512997364, 'errorList': [], 'lossList': [0.0, -1.320323292016983, 0.0, 7.290461606532335, 0.0, 0.0, 0.0], 'rewardMean': 0.745262007853549, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.3741336739142
'totalSteps': 15360, 'rewardStep': 0.8424530851561027, 'errorList': [], 'lossList': [0.0, -1.2886168956756592, 0.0, 4.939297388643026, 0.0, 0.0, 0.0], 'rewardMean': 0.7787616415285663, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.07875811417
'totalSteps': 16640, 'rewardStep': 0.847323259879616, 'errorList': [], 'lossList': [0.0, -1.267873501777649, 0.0, 4.3500819306075575, 0.0, 0.0, 0.0], 'rewardMean': 0.7995355040854986, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.3756658283314
'totalSteps': 17920, 'rewardStep': 0.8954885362295599, 'errorList': [], 'lossList': [0.0, -1.2329334396123885, 0.0, 3.3911399897560477, 0.0, 0.0, 0.0], 'rewardMean': 0.8243423342587194, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.701897907911
'totalSteps': 19200, 'rewardStep': 0.6287256429496825, 'errorList': [], 'lossList': [0.0, -1.199914818406105, 0.0, 2.535810671299696, 0.0, 0.0, 0.0], 'rewardMean': 0.7918753186649333, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.9936620299575
'totalSteps': 20480, 'rewardStep': 0.9332713842592277, 'errorList': [0.4273717190815084, 0.40083501663627885, 0.3940790804361221, 0.4040323912984048, 0.3864359296882466, 0.411371674990014, 0.4748657499271457, 0.38310505797439953, 0.3463450420761661, 0.38022707756187757, 0.3757725400469202, 0.3611684759546324, 0.43434840192030233, 0.4063096941287729, 0.3856234402508646, 0.44131520567061255, 0.47391606297433214, 0.4039863753553898, 0.41913187843502686, 0.3742172023237801, 0.39617190533443775, 0.3556855283639348, 0.4361971577718633, 0.43465983402816494, 0.4438077682760641, 0.41351971850908076, 0.3791108040014737, 0.3937834207241413, 0.37094383434937156, 0.3995182269174164, 0.4927435022656944, 0.39843226880600574, 0.3774224061545646, 0.4157762767390213, 0.36668226834488216, 0.3996503968334818, 0.40242226031924994, 0.4599226801908306, 0.3975218756294273, 0.3875946011313136, 0.39505355387831936, 0.3954652047662655, 0.39746474076479926, 0.4409464908216615, 0.3629946977434996, 0.37195210901810255, 0.44821805853250746, 0.3874019090830035, 0.4339078585721171, 0.3793580214139134], 'lossList': [0.0, -1.1842664629220963, 0.0, 1.2479461392760276, 0.0, 0.0, 0.0], 'rewardMean': 0.794085000007833, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.3125335839718, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8317326186053831, 'errorList': [], 'lossList': [0.0, -1.1526521146297455, 0.0, 1.2538885965943336, 0.0, 0.0, 0.0], 'rewardMean': 0.7882210459390679, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.7543940031526
'totalSteps': 23040, 'rewardStep': 0.8578671601596862, 'errorList': [], 'lossList': [0.0, -1.1360494774580001, 0.0, 0.7741356182843446, 0.0, 0.0, 0.0], 'rewardMean': 0.7934941655406333, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.025836239338
'totalSteps': 24320, 'rewardStep': 0.846277277661018, 'errorList': [], 'lossList': [0.0, -1.0997070568799971, 0.0, 0.377231189198792, 0.0, 0.0, 0.0], 'rewardMean': 0.8410291658007202, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.3606514199457
'totalSteps': 25600, 'rewardStep': 0.9880728916327548, 'errorList': [0.08403809087574844, 0.1477732339353547, 0.14615230394979123, 0.0879693326440262, 0.07815782320957196, 0.12119091847691806, 0.09232966133596794, 0.07801131062519784, 0.10410364580787666, 0.08395394505460546, 0.14371448171261023, 0.10754723852019771, 0.1783388429236097, 0.09288190812042982, 0.06735420967585064, 0.09787448562594252, 0.14802962540207268, 0.10074101086317949, 0.11193602972434417, 0.10502287592386442, 0.0870377442861981, 0.11991595142776826, 0.16400665296990052, 0.08898052057757716, 0.11261421952652592, 0.10825893609474188, 0.13432574733821795, 0.07775411808451858, 0.11594155940146547, 0.11324324377510196, 0.12893281210214855, 0.12826230217872142, 0.14838533085184852, 0.07110093699144932, 0.1023557918640146, 0.11586890378556242, 0.12645930951614928, 0.1504710776633273, 0.1515366881817954, 0.12217862928724332, 0.0749293504980643, 0.13250510677446836, 0.11463893609625933, 0.08700870968042163, 0.1488080186743206, 0.09472750805450432, 0.1069414880917143, 0.096530792842685, 0.11303516300631775, 0.15448842059790743], 'lossList': [0.0, -1.064814178943634, 0.0, 0.26842043481767175, 0.0, 0.0, 0.0], 'rewardMean': 0.8558953807832766, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.0407670159943, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=122.23
