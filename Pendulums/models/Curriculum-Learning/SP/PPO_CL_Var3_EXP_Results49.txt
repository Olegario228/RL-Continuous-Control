#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 49
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8889655191016573, 'errorList': [], 'lossList': [0.0, -1.431295011639595, 0.0, 30.587614250183105, 0.0, 0.0, 0.0], 'rewardMean': 0.8191281917004691, 'totalEpisodes': 49, 'stepsPerEpisode': 22, 'rewardPerEpisode': 20.272315212272314
'totalSteps': 3840, 'rewardStep': 0.491544903315067, 'errorList': [], 'lossList': [0.0, -1.4166989344358445, 0.0, 36.910330114364626, 0.0, 0.0, 0.0], 'rewardMean': 0.7099337622386684, 'totalEpisodes': 118, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.513687546058678
'totalSteps': 5120, 'rewardStep': 0.6604892991969031, 'errorList': [], 'lossList': [0.0, -1.3919975000619889, 0.0, 43.19394141197205, 0.0, 0.0, 0.0], 'rewardMean': 0.697572646478227, 'totalEpisodes': 175, 'stepsPerEpisode': 45, 'rewardPerEpisode': 35.004292783004296
'totalSteps': 6400, 'rewardStep': 0.49804727889113276, 'errorList': [], 'lossList': [0.0, -1.3716224646568298, 0.0, 43.5677485370636, 0.0, 0.0, 0.0], 'rewardMean': 0.6576675729608082, 'totalEpisodes': 199, 'stepsPerEpisode': 37, 'rewardPerEpisode': 20.536315007122095
'totalSteps': 7680, 'rewardStep': 0.7812354052285548, 'errorList': [], 'lossList': [0.0, -1.3494628745317458, 0.0, 49.25032845497131, 0.0, 0.0, 0.0], 'rewardMean': 0.6782622116720992, 'totalEpisodes': 213, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.20806653413093
'totalSteps': 8960, 'rewardStep': 0.8598587967168574, 'errorList': [], 'lossList': [0.0, -1.337212448120117, 0.0, 46.60758185386658, 0.0, 0.0, 0.0], 'rewardMean': 0.7042045809642075, 'totalEpisodes': 226, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.098116509542505
'totalSteps': 10240, 'rewardStep': 0.6738530661757525, 'errorList': [], 'lossList': [0.0, -1.3211675423383713, 0.0, 30.111167621612548, 0.0, 0.0, 0.0], 'rewardMean': 0.7004106416156507, 'totalEpisodes': 232, 'stepsPerEpisode': 306, 'rewardPerEpisode': 246.9978715497658
'totalSteps': 11520, 'rewardStep': 0.9268264712123488, 'errorList': [], 'lossList': [0.0, -1.2995322012901307, 0.0, 26.26740585565567, 0.0, 0.0, 0.0], 'rewardMean': 0.7255679560152838, 'totalEpisodes': 237, 'stepsPerEpisode': 154, 'rewardPerEpisode': 128.03789623744873
'totalSteps': 12800, 'rewardStep': 0.9703227305166792, 'errorList': [23.077304635142525, 40.15074980663776, 18.744931004283274, 79.44399622354469, 105.36707499482759, 98.87518078657882, 103.87468601235902, 80.06849224968205, 104.9072339025698, 0.12026039736703294, 0.6153482613874783, 70.8273764942274, 68.4762657117996, 65.83466623498258, 78.31258661449839, 73.60480703661317, 24.89933119475753, 62.338274195751175, 84.99137312977058, 47.060774439749224, 19.529938272996898, 41.93338714046278, 10.399503274327182, 32.994174110076585, 75.13904589284779, 6.618852074549622, 35.78387542577256, 85.2050118589242, 11.407798251356116, 85.12389213767862, 20.69983363530856, 19.03886357878156, 38.54964505280032, 3.9105725509706595, 87.40891294084584, 41.66909438715016, 138.53281040555086, 64.13932890287043, 16.39803173526586, 124.18715639818798, 32.879808998087086, 134.91550037072355, 11.862751818246593, 62.13596167076786, 103.20641610606928, 71.01003621296049, 71.34458453959601, 1.852659503150786, 15.309305071982566, 97.54410829844889], 'lossList': [0.0, -1.3025579142570496, 0.0, 27.97386755347252, 0.0, 0.0, 0.0], 'rewardMean': 0.7500434334654233, 'totalEpisodes': 244, 'stepsPerEpisode': 66, 'rewardPerEpisode': 60.182198387623956, 'successfulTests': 1
'totalSteps': 14080, 'rewardStep': 0.33599651651154555, 'errorList': [], 'lossList': [0.0, -1.3120440989732742, 0.0, 7.987045868039131, 0.0, 0.0, 0.0], 'rewardMean': 0.70871399868665, 'totalEpisodes': 248, 'stepsPerEpisode': 135, 'rewardPerEpisode': 87.34110088563645
'totalSteps': 15360, 'rewardStep': 0.6343262397988926, 'errorList': [], 'lossList': [0.0, -1.3307585263252257, 0.0, 9.092494457960129, 0.0, 0.0, 0.0], 'rewardMean': 0.6832500707563733, 'totalEpisodes': 254, 'stepsPerEpisode': 263, 'rewardPerEpisode': 228.71423201519087
'totalSteps': 16640, 'rewardStep': 0.6808741751893667, 'errorList': [], 'lossList': [0.0, -1.3334631860256194, 0.0, 5.650312924385071, 0.0, 0.0, 0.0], 'rewardMean': 0.7021829979438033, 'totalEpisodes': 259, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.06598430955933
'totalSteps': 17920, 'rewardStep': 0.6339990957090573, 'errorList': [], 'lossList': [0.0, -1.3193722331523896, 0.0, 4.917444167137146, 0.0, 0.0, 0.0], 'rewardMean': 0.6995339775950187, 'totalEpisodes': 262, 'stepsPerEpisode': 491, 'rewardPerEpisode': 366.06045131701904
'totalSteps': 19200, 'rewardStep': 0.9453094950944909, 'errorList': [0.4178634366386884, 0.4216302938171545, 0.3977536748772177, 0.46238630080265647, 0.4222571933944371, 0.4612217373320191, 0.3589866674651167, 0.3762007162945414, 0.4528889819745323, 0.5924676195472096, 0.4844383709543915, 0.3914355255960643, 0.5112812803484416, 0.44698424766907263, 0.3409702054816382, 0.41780629914979905, 0.5315199620179895, 0.43571943189964435, 0.36516407991280797, 0.5338680344531239, 0.5064444362446133, 0.38078986358004885, 0.5504042063579232, 0.562634645827824, 0.6946712357773683, 0.44367321558559203, 0.4854555681866361, 0.3518670192814172, 0.6823502197108606, 0.5538184567696472, 0.4676510865069323, 0.4532650074577538, 0.7449731191751857, 0.48874049919297885, 0.5298923346280473, 0.3455289696119338, 0.6083075286878598, 0.48395392093475365, 0.46185930672871817, 0.4069373084074172, 0.49804942634853827, 0.5581348948665259, 0.58011586759928, 0.5575522713126082, 0.4086906419887743, 0.40193792018318564, 0.5541270545763579, 0.49122658723791557, 0.46824822726066345, 0.4147216648339245], 'lossList': [0.0, -1.3053519576787949, 0.0, 4.120035909414291, 0.0, 0.0, 0.0], 'rewardMean': 0.7442601992153546, 'totalEpisodes': 264, 'stepsPerEpisode': 303, 'rewardPerEpisode': 265.2450817018454, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8316606261731423, 'errorList': [], 'lossList': [0.0, -1.2971338874101639, 0.0, 3.4417746019363404, 0.0, 0.0, 0.0], 'rewardMean': 0.7493027213098132, 'totalEpisodes': 266, 'stepsPerEpisode': 67, 'rewardPerEpisode': 55.17123126031612
'totalSteps': 21760, 'rewardStep': 0.8805037626762439, 'errorList': [], 'lossList': [0.0, -1.2646584790945052, 0.0, 2.4958678582310676, 0.0, 0.0, 0.0], 'rewardMean': 0.751367217905752, 'totalEpisodes': 266, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.7370111966638
'totalSteps': 23040, 'rewardStep': 0.9199563791469072, 'errorList': [], 'lossList': [0.0, -1.2254598289728165, 0.0, 1.501689044237137, 0.0, 0.0, 0.0], 'rewardMean': 0.7759775492028675, 'totalEpisodes': 266, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.1213036038275
'totalSteps': 24320, 'rewardStep': 0.8966913523787682, 'errorList': [], 'lossList': [0.0, -1.1989876925945282, 0.0, 1.0451671801134943, 0.0, 0.0, 0.0], 'rewardMean': 0.7729640373195095, 'totalEpisodes': 266, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1190.6784436544397
'totalSteps': 25600, 'rewardStep': 0.9781162392717939, 'errorList': [0.027610116882911914, 0.023621278499766866, 0.022364888403028298, 0.029794075729386516, 0.036574882437344414, 0.022239036678917162, 0.025472480004736408, 0.023715241478253442, 0.01861324894500528, 0.020306318196453732, 0.01960077031852687, 0.0195529853425135, 0.03762205885544485, 0.047098828633625744, 0.023467328053496585, 0.020737177018590017, 0.02074048258131628, 0.024570186472058178, 0.03845547024452319, 0.022104903424438726, 0.024212860442483695, 0.02969421433861708, 0.024496006270534915, 0.019274260199145382, 0.030875137143567648, 0.021668787995581088, 0.020954325069367317, 0.02428286218086409, 0.031100505096054325, 0.03303730211301999, 0.01880000036240065, 0.020238589866339828, 0.023384366776730493, 0.039820275687603594, 0.022591077368560874, 0.027583344625388945, 0.02884683581496228, 0.020532117272233807, 0.024342323873854, 0.040708608194550445, 0.021041487141985254, 0.024261158013309598, 0.01962288210658918, 0.024983609034879343, 0.018946471658159637, 0.021149597207496696, 0.044063285350783875, 0.019515245821394408, 0.025018806534920647, 0.02634880477181736], 'lossList': [0.0, -1.1585213750600816, 0.0, 0.8419587500393391, 0.0, 0.0, 0.0], 'rewardMean': 0.7737433881950209, 'totalEpisodes': 266, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1204.9359933736798, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=121.78
