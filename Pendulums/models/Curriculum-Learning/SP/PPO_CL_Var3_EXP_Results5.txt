#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 5
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.9505184564838223, 'errorList': [], 'lossList': [0.0, -1.4163918429613114, 0.0, 33.258000745773316, 0.0, 0.0, 0.0], 'rewardMean': 0.7805942777128364, 'totalEpisodes': 71, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.43583381005741
'totalSteps': 3840, 'rewardStep': 0.5462513782672888, 'errorList': [], 'lossList': [0.0, -1.4142683404684067, 0.0, 41.83135885238647, 0.0, 0.0, 0.0], 'rewardMean': 0.7024799778976538, 'totalEpisodes': 133, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.713106308349271
'totalSteps': 5120, 'rewardStep': 0.3814243999477311, 'errorList': [], 'lossList': [0.0, -1.4090848207473754, 0.0, 37.528572454452515, 0.0, 0.0, 0.0], 'rewardMean': 0.6222160834101731, 'totalEpisodes': 168, 'stepsPerEpisode': 65, 'rewardPerEpisode': 34.63362465496211
'totalSteps': 6400, 'rewardStep': 0.9241606720567118, 'errorList': [], 'lossList': [0.0, -1.4092967545986175, 0.0, 42.38732748985291, 0.0, 0.0, 0.0], 'rewardMean': 0.6826050011394809, 'totalEpisodes': 182, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.507189843580253
'totalSteps': 7680, 'rewardStep': 0.9484895054114026, 'errorList': [53.53838560868173, 43.02973366461645, 31.150718958617325, 25.459783201796967, 52.8832541095583, 53.14424353440407, 41.65596656127317, 32.14361968228639, 4.7953106178646285, 37.96282839780649, 33.822606646108646, 4.538476996937225, 28.300991421517022, 29.912041810400353, 69.69697377620123, 25.540584637890156, 22.472658242046734, 55.88506097787221, 68.90260218099318, 31.04031052279607, 46.20223605576306, 26.650154166227665, 16.219474219760205, 43.70283438328465, 36.09850130187024, 21.675130260260616, 27.522165482538174, 19.431153033808588, 31.660666017534563, 13.859093961528037, 15.53020890612219, 47.1775100197773, 9.70348381735264, 9.784698804188745, 20.756688811640498, 11.871396079902345, 28.583729480389717, 56.16351079188069, 48.635466466536364, 60.107764013083376, 55.722815247761545, 14.31457060944544, 57.48869097490552, 49.631354289211835, 30.236708302106294, 60.882851131823486, 5.931847136502349, 19.410695410250273, 36.77598929547178, 15.528294885954022], 'lossList': [0.0, -1.4055381935834885, 0.0, 30.372490725517274, 0.0, 0.0, 0.0], 'rewardMean': 0.7269190851848011, 'totalEpisodes': 189, 'stepsPerEpisode': 69, 'rewardPerEpisode': 58.76990694502813, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.606211618610335, 'errorList': [], 'lossList': [0.0, -1.391567867398262, 0.0, 51.46150251865387, 0.0, 0.0, 0.0], 'rewardMean': 0.7096751613884488, 'totalEpisodes': 198, 'stepsPerEpisode': 270, 'rewardPerEpisode': 215.5696772149571
'totalSteps': 10240, 'rewardStep': 0.5971196527791887, 'errorList': [], 'lossList': [0.0, -1.3890139788389206, 0.0, 47.59869815826416, 0.0, 0.0, 0.0], 'rewardMean': 0.6956057228122914, 'totalEpisodes': 206, 'stepsPerEpisode': 108, 'rewardPerEpisode': 78.6993588439681
'totalSteps': 11520, 'rewardStep': 0.7224607812768916, 'errorList': [], 'lossList': [0.0, -1.3932782703638076, 0.0, 46.10814496517182, 0.0, 0.0, 0.0], 'rewardMean': 0.698589618197247, 'totalEpisodes': 215, 'stepsPerEpisode': 71, 'rewardPerEpisode': 55.67627799810662
'totalSteps': 12800, 'rewardStep': 0.1105773765786493, 'errorList': [], 'lossList': [0.0, -1.3903980338573456, 0.0, 16.153565092086794, 0.0, 0.0, 0.0], 'rewardMean': 0.6397883940353872, 'totalEpisodes': 221, 'stepsPerEpisode': 263, 'rewardPerEpisode': 186.70868644347212
'totalSteps': 14080, 'rewardStep': 0.7560893335833796, 'errorList': [], 'lossList': [0.0, -1.3696717011928559, 0.0, 24.58573339700699, 0.0, 0.0, 0.0], 'rewardMean': 0.6543303174995401, 'totalEpisodes': 227, 'stepsPerEpisode': 214, 'rewardPerEpisode': 176.5341878144214
'totalSteps': 15360, 'rewardStep': 0.8263467875541337, 'errorList': [], 'lossList': [0.0, -1.3532722580432892, 0.0, 20.60789826154709, 0.0, 0.0, 0.0], 'rewardMean': 0.6419131506065712, 'totalEpisodes': 234, 'stepsPerEpisode': 68, 'rewardPerEpisode': 61.92007554650364
'totalSteps': 16640, 'rewardStep': 0.5698299006359793, 'errorList': [], 'lossList': [0.0, -1.3572140634059906, 0.0, 6.242516374588012, 0.0, 0.0, 0.0], 'rewardMean': 0.6442710028434403, 'totalEpisodes': 239, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5698299006359793
'totalSteps': 17920, 'rewardStep': 0.9149995253782721, 'errorList': [], 'lossList': [0.0, -1.3719201636314393, 0.0, 4.636410716176033, 0.0, 0.0, 0.0], 'rewardMean': 0.6976285153864944, 'totalEpisodes': 244, 'stepsPerEpisode': 289, 'rewardPerEpisode': 251.57599062124623
'totalSteps': 19200, 'rewardStep': 0.976456240797852, 'errorList': [1.9672659306643434, 23.54550514423514, 11.115626085705333, 37.85244691576209, 15.35402167574003, 26.804407027166967, 6.443326288224153, 11.920187182850283, 8.719032977654804, 1.2825603628355158, 5.945367155711592, 8.230910241279323, 22.060376810687146, 2.9600710814590454, 14.158746892302396, 0.2033464488479192, 13.797899091291642, 5.73867884010181, 15.099193764472657, 2.97841058554987, 0.5349839656290214, 2.0821353346881573, 23.170040123462005, 4.790682077698433, 8.320803386259929, 13.89825684074682, 8.195026027279447, 2.541012065774795, 13.672373253546475, 14.302805648331777, 2.3860690276736927, 34.022591242499516, 6.646940953301091, 3.07176003560689, 15.259932298547701, 18.42578257669116, 17.047348292481832, 7.449533006273135, 15.420546596132615, 19.401341134315228, 0.22651135655335555, 15.503405440482963, 12.08142859032694, 2.623950273551827, 7.58592003789428, 2.3982171663860155, 1.868544290133814, 15.445458243359083, 20.48038963884299, 6.294534992643263], 'lossList': [0.0, -1.3617619663476943, 0.0, 3.7184352147579194, 0.0, 0.0, 0.0], 'rewardMean': 0.7028580722606084, 'totalEpisodes': 249, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.763305659768224, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.6157136325221211, 'errorList': [], 'lossList': [0.0, -1.3433903861045837, 0.0, 2.8937773171067236, 0.0, 0.0, 0.0], 'rewardMean': 0.6695804849716802, 'totalEpisodes': 252, 'stepsPerEpisode': 258, 'rewardPerEpisode': 211.34843504773468
'totalSteps': 21760, 'rewardStep': 0.9788352562801068, 'errorList': [10.803294915266102, 1.4073572452379326, 6.19363839938192, 0.40476093283339426, 2.137861066822653, 7.539530172016476, 2.4281207920909287, 2.6026507508770225, 0.9143108999858042, 0.2602135460642201, 3.8265834847725424, 2.492779128645356, 2.2134477392641174, 0.9316401231099712, 3.784170306813001, 0.0520433992057857, 7.28327645332531, 0.4582932571271852, 1.235237866265291, 3.2067151169428194, 0.6360223765645341, 0.9802892069108802, 4.923216330874372, 1.3477210568494309, 5.805323808129976, 10.219623190302274, 0.21437395718670427, 0.5476845233428845, 2.30530823804093, 7.496501849722347, 0.21085713144079604, 0.38462960442478183, 2.865586283995142, 0.22070236525667225, 0.16705541957721803, 0.4426482572524405, 1.8708835615679562, 1.1760054141332625, 3.9690825414067903, 0.09784511907968112, 2.2651766618002322, 1.06061153479735, 0.45994425985571813, 9.111627374599893, 1.1378089688733881, 2.6588838598171516, 0.09759366590940598, 2.176626586146733, 3.8045328647030163, 1.469390320385335], 'lossList': [0.0, -1.3469394493103026, 0.0, 2.8289224028587343, 0.0, 0.0, 0.0], 'rewardMean': 0.7068428487386574, 'totalEpisodes': 257, 'stepsPerEpisode': 61, 'rewardPerEpisode': 56.18434395362506, 'successfulTests': 4
'totalSteps': 23040, 'rewardStep': 0.7697888302631261, 'errorList': [], 'lossList': [0.0, -1.3427952027320862, 0.0, 1.9266353875398636, 0.0, 0.0, 0.0], 'rewardMean': 0.724109766487051, 'totalEpisodes': 259, 'stepsPerEpisode': 360, 'rewardPerEpisode': 324.0410505048863
'totalSteps': 24320, 'rewardStep': 0.7501228489407226, 'errorList': [], 'lossList': [0.0, -1.3365447807312012, 0.0, 2.414438316822052, 0.0, 0.0, 0.0], 'rewardMean': 0.7268759732534342, 'totalEpisodes': 260, 'stepsPerEpisode': 435, 'rewardPerEpisode': 389.37400502784124
'totalSteps': 25600, 'rewardStep': 0.9258810724701988, 'errorList': [], 'lossList': [0.0, -1.3179917687177658, 0.0, 1.301424627304077, 0.0, 0.0, 0.0], 'rewardMean': 0.808406342842589, 'totalEpisodes': 261, 'stepsPerEpisode': 799, 'rewardPerEpisode': 730.7803693681228
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=21760, timeSpent=107.0
