#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 50
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.677536732116674, 'errorList': [], 'lossList': [0.0, -1.4211325913667678, 0.0, 28.26951174736023, 0.0, 0.0, 0.0], 'rewardMean': 0.5608797365460958, 'totalEpisodes': 91, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.3865346057192405
'totalSteps': 3840, 'rewardStep': 0.9632202832022562, 'errorList': [], 'lossList': [0.0, -1.4253103560209275, 0.0, 35.03272225379944, 0.0, 0.0, 0.0], 'rewardMean': 0.6949932520981492, 'totalEpisodes': 147, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.296726679947404
'totalSteps': 5120, 'rewardStep': 0.6235017376501981, 'errorList': [], 'lossList': [0.0, -1.424164120554924, 0.0, 45.75804698944092, 0.0, 0.0, 0.0], 'rewardMean': 0.6771203734861615, 'totalEpisodes': 186, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.33396962338513
'totalSteps': 6400, 'rewardStep': 0.5746518320236521, 'errorList': [], 'lossList': [0.0, -1.4174419581890105, 0.0, 46.94808163642883, 0.0, 0.0, 0.0], 'rewardMean': 0.6566266651936596, 'totalEpisodes': 202, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.1453872332238184
'totalSteps': 7680, 'rewardStep': 0.7902974957749513, 'errorList': [], 'lossList': [0.0, -1.4093779730796814, 0.0, 43.97521054267883, 0.0, 0.0, 0.0], 'rewardMean': 0.6789051369572082, 'totalEpisodes': 210, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.81006580306026
'totalSteps': 8960, 'rewardStep': 0.6934449620864451, 'errorList': [], 'lossList': [0.0, -1.4033630663156509, 0.0, 42.83051953315735, 0.0, 0.0, 0.0], 'rewardMean': 0.6809822548328136, 'totalEpisodes': 216, 'stepsPerEpisode': 118, 'rewardPerEpisode': 88.46613109381502
'totalSteps': 10240, 'rewardStep': 0.8596209155014264, 'errorList': [], 'lossList': [0.0, -1.409466676712036, 0.0, 39.989265241622924, 0.0, 0.0, 0.0], 'rewardMean': 0.7033120874163901, 'totalEpisodes': 223, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.15079565266222
'totalSteps': 11520, 'rewardStep': 0.1512630117232734, 'errorList': [], 'lossList': [0.0, -1.4175484883785248, 0.0, 38.52095174789429, 0.0, 0.0, 0.0], 'rewardMean': 0.641973301228266, 'totalEpisodes': 229, 'stepsPerEpisode': 230, 'rewardPerEpisode': 155.54967723658095
'totalSteps': 12800, 'rewardStep': 0.7777647568381282, 'errorList': [], 'lossList': [0.0, -1.4012663793563842, 0.0, 37.85604187250137, 0.0, 0.0, 0.0], 'rewardMean': 0.6555524467892522, 'totalEpisodes': 234, 'stepsPerEpisode': 89, 'rewardPerEpisode': 76.79272700048303
'totalSteps': 14080, 'rewardStep': 0.7551350418901981, 'errorList': [], 'lossList': [0.0, -1.4066492581367493, 0.0, 23.047553375959396, 0.0, 0.0, 0.0], 'rewardMean': 0.6866436768807204, 'totalEpisodes': 237, 'stepsPerEpisode': 75, 'rewardPerEpisode': 66.02900859167427
'totalSteps': 15360, 'rewardStep': 0.8246852943937362, 'errorList': [], 'lossList': [0.0, -1.3962843984365463, 0.0, 36.55767191886902, 0.0, 0.0, 0.0], 'rewardMean': 0.7013585331084264, 'totalEpisodes': 241, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.257294321702837
'totalSteps': 16640, 'rewardStep': 0.8514377827382312, 'errorList': [], 'lossList': [0.0, -1.367426366209984, 0.0, 11.895958045721054, 0.0, 0.0, 0.0], 'rewardMean': 0.6901802830620241, 'totalEpisodes': 243, 'stepsPerEpisode': 159, 'rewardPerEpisode': 139.33636412501724
'totalSteps': 17920, 'rewardStep': 0.5538122480645206, 'errorList': [], 'lossList': [0.0, -1.3422592979669572, 0.0, 20.357552309036254, 0.0, 0.0, 0.0], 'rewardMean': 0.6832113341034562, 'totalEpisodes': 248, 'stepsPerEpisode': 141, 'rewardPerEpisode': 97.68067389771976
'totalSteps': 19200, 'rewardStep': 0.5238752974287236, 'errorList': [], 'lossList': [0.0, -1.335632530450821, 0.0, 11.038963836431503, 0.0, 0.0, 0.0], 'rewardMean': 0.6781336806439635, 'totalEpisodes': 253, 'stepsPerEpisode': 236, 'rewardPerEpisode': 181.13194220818036
'totalSteps': 20480, 'rewardStep': 0.8009608842149835, 'errorList': [], 'lossList': [0.0, -1.3501219218969345, 0.0, 7.938891931772232, 0.0, 0.0, 0.0], 'rewardMean': 0.6792000194879666, 'totalEpisodes': 257, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.022138331601365
'totalSteps': 21760, 'rewardStep': 0.9333268166917696, 'errorList': [0.3236703205861404, 0.23036515938986196, 0.32621613271621586, 0.4006197417115116, 0.44413011090468996, 0.20813389652475145, 0.3089668269181264, 0.32913987249809235, 0.3019027463470259, 0.21973955015067956, 0.30667474206143563, 0.2986031175773237, 0.2554735321019178, 0.2273121622066021, 0.33980485707623115, 0.24401216681244797, 0.2205846160469899, 0.31963315595649555, 0.2755287724312091, 0.32699136911977744, 0.17199036184935937, 0.3895892670630732, 0.41073806275823205, 0.3175640455526265, 0.21075698809789686, 0.29988635272065106, 0.29788563558275377, 0.2624508915054579, 0.2299667549936473, 0.20377728686735802, 0.23283044333314892, 0.30328080930989704, 0.26360725433851456, 0.24576305327451153, 0.23166976956839183, 0.26085544623676105, 0.44885219544445576, 0.27777105201831387, 0.3573513462515227, 0.3867645228229715, 0.30927243446834934, 0.3761176919050121, 0.4150498133001809, 0.35664714888789445, 0.17500621803245564, 0.32949422030244735, 0.340218477067505, 0.24993338889501288, 0.2685038051186461, 0.22031552392323123], 'lossList': [0.0, -1.3478299915790557, 0.0, 14.784877228736878, 0.0, 0.0, 0.0], 'rewardMean': 0.7031882049484991, 'totalEpisodes': 259, 'stepsPerEpisode': 60, 'rewardPerEpisode': 53.744709488717135, 'successfulTests': 2
'totalSteps': 23040, 'rewardStep': 0.49320922977934, 'errorList': [], 'lossList': [0.0, -1.3216322910785676, 0.0, 1.8064365389943122, 0.0, 0.0, 0.0], 'rewardMean': 0.6665470363762904, 'totalEpisodes': 259, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 915.122295124811
'totalSteps': 24320, 'rewardStep': 0.7051759697613176, 'errorList': [], 'lossList': [0.0, -1.2903965109586715, 0.0, 1.316327583938837, 0.0, 0.0, 0.0], 'rewardMean': 0.721938332180095, 'totalEpisodes': 259, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1071.416638312801
'totalSteps': 25600, 'rewardStep': 0.8605884099815372, 'errorList': [], 'lossList': [0.0, -1.2564812022447587, 0.0, 0.8127811712771654, 0.0, 0.0, 0.0], 'rewardMean': 0.7302206974944357, 'totalEpisodes': 259, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.8009364358809
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=21760, timeSpent=77.27
