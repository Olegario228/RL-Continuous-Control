#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 51
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.8989500689240484, 'errorList': [], 'lossList': [0.0, -1.4116384720802306, 0.0, 33.763583269119266, 0.0, 0.0, 0.0], 'rewardMean': 0.8570001008827279, 'totalEpisodes': 93, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.234103600938287
'totalSteps': 3840, 'rewardStep': 0.848831602304971, 'errorList': [], 'lossList': [0.0, -1.3912552511692047, 0.0, 35.97776488304138, 0.0, 0.0, 0.0], 'rewardMean': 0.8542772680234756, 'totalEpisodes': 151, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.019614708982623
'totalSteps': 5120, 'rewardStep': 0.7051403763914644, 'errorList': [], 'lossList': [0.0, -1.3776376670598984, 0.0, 40.23932681083679, 0.0, 0.0, 0.0], 'rewardMean': 0.8169930451154728, 'totalEpisodes': 188, 'stepsPerEpisode': 27, 'rewardPerEpisode': 21.046124350233733
'totalSteps': 6400, 'rewardStep': 0.9167874589988305, 'errorList': [], 'lossList': [0.0, -1.358153225183487, 0.0, 48.04398934364319, 0.0, 0.0, 0.0], 'rewardMean': 0.8369519278921442, 'totalEpisodes': 206, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.05298328166646
'totalSteps': 7680, 'rewardStep': 0.5504859035972338, 'errorList': [], 'lossList': [0.0, -1.3517267894744873, 0.0, 40.950552520751955, 0.0, 0.0, 0.0], 'rewardMean': 0.7892075905096592, 'totalEpisodes': 212, 'stepsPerEpisode': 116, 'rewardPerEpisode': 85.91114032088291
'totalSteps': 8960, 'rewardStep': 0.5390623295686621, 'errorList': [], 'lossList': [0.0, -1.3382596629858017, 0.0, 29.39096995592117, 0.0, 0.0, 0.0], 'rewardMean': 0.7534725532323738, 'totalEpisodes': 220, 'stepsPerEpisode': 119, 'rewardPerEpisode': 86.31954125343172
'totalSteps': 10240, 'rewardStep': 0.8355272794036915, 'errorList': [], 'lossList': [0.0, -1.3321618658304215, 0.0, 11.987664035558701, 0.0, 0.0, 0.0], 'rewardMean': 0.7637293940037886, 'totalEpisodes': 227, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.183382304761025
'totalSteps': 11520, 'rewardStep': 0.6528626564366123, 'errorList': [], 'lossList': [0.0, -1.3339651250839233, 0.0, 9.225914813280106, 0.0, 0.0, 0.0], 'rewardMean': 0.7514108676074357, 'totalEpisodes': 233, 'stepsPerEpisode': 15, 'rewardPerEpisode': 9.733401283360443
'totalSteps': 12800, 'rewardStep': 0.9276852027050145, 'errorList': [], 'lossList': [0.0, -1.3197792291641235, 0.0, 7.678793969154358, 0.0, 0.0, 0.0], 'rewardMean': 0.7690383011171936, 'totalEpisodes': 240, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.722745137018344
'totalSteps': 14080, 'rewardStep': 0.7807937717257883, 'errorList': [], 'lossList': [0.0, -1.32374642431736, 0.0, 6.788552483320236, 0.0, 0.0, 0.0], 'rewardMean': 0.7656126650056317, 'totalEpisodes': 242, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.88469313278175
'totalSteps': 15360, 'rewardStep': 0.9093406021635158, 'errorList': [], 'lossList': [0.0, -1.3452053898572922, 0.0, 4.848070363402367, 0.0, 0.0, 0.0], 'rewardMean': 0.7666517183295785, 'totalEpisodes': 243, 'stepsPerEpisode': 408, 'rewardPerEpisode': 357.6697927469454
'totalSteps': 16640, 'rewardStep': 0.679576130541722, 'errorList': [], 'lossList': [0.0, -1.3188993012905121, 0.0, 2.562390333414078, 0.0, 0.0, 0.0], 'rewardMean': 0.7497261711532536, 'totalEpisodes': 243, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 919.4838874252514
'totalSteps': 17920, 'rewardStep': 0.8347627332431198, 'errorList': [], 'lossList': [0.0, -1.2940242719650268, 0.0, 3.4335672563314437, 0.0, 0.0, 0.0], 'rewardMean': 0.7626884068384191, 'totalEpisodes': 245, 'stepsPerEpisode': 147, 'rewardPerEpisode': 132.12820161962088
'totalSteps': 19200, 'rewardStep': 0.8638592744443753, 'errorList': [], 'lossList': [0.0, -1.2605536985397339, 0.0, 2.1492518314719202, 0.0, 0.0, 0.0], 'rewardMean': 0.7573955883829735, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1029.2180181992946
'totalSteps': 20480, 'rewardStep': 0.6661026295792893, 'errorList': [], 'lossList': [0.0, -1.205360068678856, 0.0, 1.387534105628729, 0.0, 0.0, 0.0], 'rewardMean': 0.7689572609811791, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.2800778146054
'totalSteps': 21760, 'rewardStep': 0.7887406034262292, 'errorList': [], 'lossList': [0.0, -1.1379698151350022, 0.0, 0.7311154070496559, 0.0, 0.0, 0.0], 'rewardMean': 0.7939250883669358, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.3483158793633
'totalSteps': 23040, 'rewardStep': 0.92704651559513, 'errorList': [], 'lossList': [0.0, -1.1111353760957718, 0.0, 1.1862045166268944, 0.0, 0.0, 0.0], 'rewardMean': 0.8030770119860795, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.433810416021
'totalSteps': 24320, 'rewardStep': 0.8570253653392242, 'errorList': [], 'lossList': [0.0, -1.0883295887708664, 0.0, 0.7160244590044021, 0.0, 0.0, 0.0], 'rewardMean': 0.8234932828763408, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1183.4174341758157
'totalSteps': 25600, 'rewardStep': 0.9906577783350926, 'errorList': [0.1674790451408261, 0.12149099381312592, 0.15751706249989678, 0.1327306860566103, 0.1660313971428382, 0.12078869479569072, 0.13859728382796246, 0.13137319353280044, 0.14305128158573252, 0.15067937107210386, 0.25922975483608257, 0.11066281136702727, 0.1454226393188679, 0.23364583363401578, 0.15701080192469918, 0.15088484876473351, 0.15496132537338891, 0.14380568781135167, 0.13822290676234414, 0.11169566869256131, 0.15544173866765218, 0.12849197335215592, 0.15743118876353576, 0.1503859671279746, 0.16139246057987447, 0.18883320394983077, 0.17573854679798415, 0.14326022831710403, 0.1776297253037855, 0.14720206376369624, 0.150942143427891, 0.1855958618485994, 0.1573192938389348, 0.14481851409844537, 0.1655487815414525, 0.1385739108873893, 0.2467707165805312, 0.1494334286408661, 0.13510321348404786, 0.17945946175598218, 0.2276089951137916, 0.19285391830710993, 0.1371536712229606, 0.16571114196780612, 0.20250083509748612, 0.24796770154230813, 0.13861350995112842, 0.15009424507611857, 0.1525001894611275, 0.1300857956877079], 'lossList': [0.0, -1.0363828986883163, 0.0, 0.6303576335497201, 0.0, 0.0, 0.0], 'rewardMean': 0.8297905404393486, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1222.1610192193814, 'successfulTests': 44
#maxSuccessfulTests=44, maxSuccessfulTestsAtStep=25600, timeSpent=77.15
