#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 52
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.6694966183307659, 'errorList': [], 'lossList': [0.0, -1.4244207990169526, 0.0, 31.660795917510985, 0.0, 0.0, 0.0], 'rewardMean': 0.7425669800572376, 'totalEpisodes': 91, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.115751012353351
'totalSteps': 3840, 'rewardStep': 0.6207427229704663, 'errorList': [], 'lossList': [0.0, -1.4135747349262238, 0.0, 41.11281509399414, 0.0, 0.0, 0.0], 'rewardMean': 0.7019588943616473, 'totalEpisodes': 152, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.57512255130596
'totalSteps': 5120, 'rewardStep': 0.6759780243385672, 'errorList': [], 'lossList': [0.0, -1.3946299922466279, 0.0, 44.465948057174685, 0.0, 0.0, 0.0], 'rewardMean': 0.6954636768558773, 'totalEpisodes': 190, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.212852804282958
'totalSteps': 6400, 'rewardStep': 0.6223676559141798, 'errorList': [], 'lossList': [0.0, -1.3828406488895417, 0.0, 48.54432775497437, 0.0, 0.0, 0.0], 'rewardMean': 0.6808444726675378, 'totalEpisodes': 209, 'stepsPerEpisode': 28, 'rewardPerEpisode': 22.739626947040875
'totalSteps': 7680, 'rewardStep': 0.975673690586117, 'errorList': [23.14091490508497, 4.156830486377295, 8.663945707181755, 43.49648210770472, 49.66067180421299, 5.277272806749349, 5.739609869910839, 3.00816900131851, 68.10811022902392, 89.5189210478024, 29.96523401707425, 31.866355899400286, 11.054596993874359, 5.11958919269222, 37.861025117961496, 20.706486169651193, 53.60033985819102, 28.215842292946927, 29.47934874586243, 81.08457047352447, 50.1522712903948, 10.313167047013316, 2.0480091782259, 26.74647954335455, 45.4348137653721, 34.032712190078925, 8.089875295978864, 7.267326871282231, 16.092958944215656, 24.58103687953377, 19.71607241792354, 55.11174222704137, 85.81684527182857, 46.87503458822902, 4.111449553289862, 41.90691721044832, 19.068984011003668, 60.480619370536466, 82.87051485282245, 76.35330992863759, 5.333825878177395, 14.614235939696117, 26.133235911567454, 30.96835518500707, 50.37274240205931, 11.618678775553168, 0.9218055027402068, 45.94929581592689, 12.163220266233335, 19.917158746309315], 'lossList': [0.0, -1.375983985066414, 0.0, 46.201435546875, 0.0, 0.0, 0.0], 'rewardMean': 0.7299826756539676, 'totalEpisodes': 225, 'stepsPerEpisode': 60, 'rewardPerEpisode': 46.69842473603823, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.5208956035325603, 'errorList': [], 'lossList': [0.0, -1.3683188712596894, 0.0, 22.060250803232194, 0.0, 0.0, 0.0], 'rewardMean': 0.700113093922338, 'totalEpisodes': 233, 'stepsPerEpisode': 135, 'rewardPerEpisode': 98.951153724006
'totalSteps': 10240, 'rewardStep': 0.8383859455980271, 'errorList': [], 'lossList': [0.0, -1.3557236540317534, 0.0, 17.528393857479095, 0.0, 0.0, 0.0], 'rewardMean': 0.7173972003817991, 'totalEpisodes': 238, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.696078512551368
'totalSteps': 11520, 'rewardStep': 0.7603653222526977, 'errorList': [], 'lossList': [0.0, -1.3284032446146012, 0.0, 20.5797741997242, 0.0, 0.0, 0.0], 'rewardMean': 0.7221714361452323, 'totalEpisodes': 242, 'stepsPerEpisode': 162, 'rewardPerEpisode': 129.9559869513224
'totalSteps': 12800, 'rewardStep': 0.6471551866054697, 'errorList': [], 'lossList': [0.0, -1.3225520932674408, 0.0, 16.629415390491484, 0.0, 0.0, 0.0], 'rewardMean': 0.7146698111912559, 'totalEpisodes': 247, 'stepsPerEpisode': 395, 'rewardPerEpisode': 287.11561587908045
'totalSteps': 14080, 'rewardStep': 0.8549139931577884, 'errorList': [], 'lossList': [0.0, -1.338950328230858, 0.0, 8.244432498812676, 0.0, 0.0, 0.0], 'rewardMean': 0.7185974763286639, 'totalEpisodes': 250, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.919154208753454
'totalSteps': 15360, 'rewardStep': 0.8141399081045844, 'errorList': [], 'lossList': [0.0, -1.3480339270830155, 0.0, 24.86153519809246, 0.0, 0.0, 0.0], 'rewardMean': 0.7330618053060458, 'totalEpisodes': 251, 'stepsPerEpisode': 1140, 'rewardPerEpisode': 906.0336188576485
'totalSteps': 16640, 'rewardStep': 0.7206180968418403, 'errorList': [], 'lossList': [0.0, -1.3303701132535934, 0.0, 3.218135113865137, 0.0, 0.0, 0.0], 'rewardMean': 0.7430493426931831, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1016.0198658784242
'totalSteps': 17920, 'rewardStep': 0.8665409923899572, 'errorList': [], 'lossList': [0.0, -1.2857363283634187, 0.0, 3.0532338343560697, 0.0, 0.0, 0.0], 'rewardMean': 0.7621056394983221, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.7796593275123
'totalSteps': 19200, 'rewardStep': 0.9140897462356878, 'errorList': [], 'lossList': [0.0, -1.2434679853916168, 0.0, 2.532524045296013, 0.0, 0.0, 0.0], 'rewardMean': 0.791277848530473, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1136.5074278867285
'totalSteps': 20480, 'rewardStep': 0.931553614882334, 'errorList': [0.08239850439931883, 0.13772927693317769, 0.14491297216694368, 0.15775708211905395, 0.11347848398200666, 0.1293872594589611, 0.0989089024100994, 0.10177029349723031, 0.0810716619361366, 0.11727109559103226, 0.12160724330650304, 0.09144135532276229, 0.14219730456081814, 0.12187707423326978, 0.1254745574243651, 0.1334502605433769, 0.11038999198716747, 0.14141318006948034, 0.11300322568100683, 0.10343896503144764, 0.11548611257105677, 0.13497783619274215, 0.13443084759057575, 0.1371349132261444, 0.12408281349593957, 0.17079411079134743, 0.13442099040024755, 0.13724790779597057, 0.13759866894938313, 0.10298758307784826, 0.09343135793614414, 0.13224364236883035, 0.13459377329281044, 0.09994879251497377, 0.10229502478047124, 0.12689607536843248, 0.09644434609549882, 0.11356935080717605, 0.10933992972694521, 0.15342498531739163, 0.14782795079597139, 0.09415406938353557, 0.11576348351441566, 0.14404913860705326, 0.11953012324278009, 0.12977471550775455, 0.08591235651023932, 0.09605343113724357, 0.14338074594469227, 0.11606185750151754], 'lossList': [0.0, -1.1985065364837646, 0.0, 2.1190089567750694, 0.0, 0.0, 0.0], 'rewardMean': 0.7868658409600947, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.9513879816404, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=90.27
