#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 53
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.8349061645593677, 'errorList': [], 'lossList': [0.0, -1.426509758234024, 0.0, 28.112055702209474, 0.0, 0.0, 0.0], 'rewardMean': 0.6699226956498763, 'totalEpisodes': 87, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.102750710927527
'totalSteps': 3840, 'rewardStep': 0.5406042028498647, 'errorList': [], 'lossList': [0.0, -1.421123366355896, 0.0, 43.77659216880798, 0.0, 0.0, 0.0], 'rewardMean': 0.6268165313832057, 'totalEpisodes': 145, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.346708806633268
'totalSteps': 5120, 'rewardStep': 0.6805860184619775, 'errorList': [], 'lossList': [0.0, -1.4151862126588821, 0.0, 43.47664681434631, 0.0, 0.0, 0.0], 'rewardMean': 0.6402589031528987, 'totalEpisodes': 171, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.500509872065161
'totalSteps': 6400, 'rewardStep': 0.8814134724082937, 'errorList': [], 'lossList': [0.0, -1.3979138070344925, 0.0, 47.07249210357666, 0.0, 0.0, 0.0], 'rewardMean': 0.6884898170039777, 'totalEpisodes': 183, 'stepsPerEpisode': 161, 'rewardPerEpisode': 132.64917318331078
'totalSteps': 7680, 'rewardStep': 0.8387442001949605, 'errorList': [], 'lossList': [0.0, -1.3824194765090942, 0.0, 52.91415759086609, 0.0, 0.0, 0.0], 'rewardMean': 0.7135322142024748, 'totalEpisodes': 195, 'stepsPerEpisode': 83, 'rewardPerEpisode': 63.44717776537728
'totalSteps': 8960, 'rewardStep': 0.8643018897116822, 'errorList': [], 'lossList': [0.0, -1.3828194665908813, 0.0, 27.776908264160156, 0.0, 0.0, 0.0], 'rewardMean': 0.7350707392752188, 'totalEpisodes': 200, 'stepsPerEpisode': 104, 'rewardPerEpisode': 85.6481610809338
'totalSteps': 10240, 'rewardStep': 0.5835717904160949, 'errorList': [], 'lossList': [0.0, -1.374402732849121, 0.0, 46.43003266811371, 0.0, 0.0, 0.0], 'rewardMean': 0.7161333706678283, 'totalEpisodes': 205, 'stepsPerEpisode': 247, 'rewardPerEpisode': 188.11034250539376
'totalSteps': 11520, 'rewardStep': 0.8244376271363006, 'errorList': [], 'lossList': [0.0, -1.367597323656082, 0.0, 74.12940715789794, 0.0, 0.0, 0.0], 'rewardMean': 0.7281671769421031, 'totalEpisodes': 211, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.627060042027744
'totalSteps': 12800, 'rewardStep': 0.6793106142887148, 'errorList': [], 'lossList': [0.0, -1.3746494019031525, 0.0, 36.508334023952486, 0.0, 0.0, 0.0], 'rewardMean': 0.7232815206767642, 'totalEpisodes': 214, 'stepsPerEpisode': 211, 'rewardPerEpisode': 177.15909093592623
'totalSteps': 14080, 'rewardStep': 0.7436251255361666, 'errorList': [], 'lossList': [0.0, -1.3750025963783263, 0.0, 42.80192054271698, 0.0, 0.0, 0.0], 'rewardMean': 0.7471501105563423, 'totalEpisodes': 218, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.217165466130629
'totalSteps': 15360, 'rewardStep': 0.9355638676082841, 'errorList': [0.5899348871961058, 0.7589492554382457, 0.8256302208883106, 0.7841871241512216, 0.4855245974761552, 0.6753218722133273, 0.6633647193298193, 0.48992788230760875, 0.6920163442673634, 0.5882499966824517, 0.5565087761951593, 0.6128148544937925, 0.4441618966612628, 0.39133249176323737, 0.7413873015597443, 0.4061049240404757, 0.7196237020399683, 0.46895029764695034, 0.6395124609310366, 0.3656416935177682, 0.765017569451707, 0.4006112896854655, 0.7416895259822767, 0.3901476710441111, 0.7818180039980809, 0.5842362396445421, 0.6897624259110375, 0.6027011571356816, 0.6091790656714091, 0.8606736639024255, 0.8367031439968696, 0.7150390630006693, 0.5166984297780818, 0.553072398178261, 0.8002329607436699, 0.4860795635346733, 0.4951260941103725, 0.5755146653547291, 0.8392534554051405, 0.6286915746874097, 0.7533669158167583, 0.7372008241119077, 0.528298614795754, 0.7307210601854269, 0.4449524477115711, 0.5941597516013954, 0.6434524766414315, 0.635628071959742, 0.7203683856781979, 0.47508282853356254], 'lossList': [0.0, -1.371166352033615, 0.0, 28.929747681617737, 0.0, 0.0, 0.0], 'rewardMean': 0.7572158808612339, 'totalEpisodes': 221, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.8771442534221339, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5825821458828003, 'errorList': [], 'lossList': [0.0, -1.3573469305038453, 0.0, 8.689659502506256, 0.0, 0.0, 0.0], 'rewardMean': 0.7614136751645275, 'totalEpisodes': 222, 'stepsPerEpisode': 666, 'rewardPerEpisode': 401.2443410960145
'totalSteps': 17920, 'rewardStep': 0.7427343973325993, 'errorList': [], 'lossList': [0.0, -1.3367836737632752, 0.0, 3.1769375923275947, 0.0, 0.0, 0.0], 'rewardMean': 0.7676285130515897, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 944.8698900240344
'totalSteps': 19200, 'rewardStep': 0.9915632673894713, 'errorList': [0.02105687704140968, 0.03784763740017827, 0.030216284903674142, 0.04053701847477567, 0.048872642907995534, 0.0246947574329599, 0.028448419515515612, 0.050092574087612175, 0.03614751720253919, 0.02934118135807822, 0.025482509628863528, 0.035161357073204474, 0.030566264612866333, 0.02830246122419991, 0.025845847126426142, 0.06872086707345366, 0.03545544560573626, 0.028156775412457015, 0.04745383655760638, 0.040153771208236655, 0.03634216425328222, 0.04738809325446776, 0.0182511525226539, 0.03273926537693607, 0.01934476720656789, 0.028826040677446797, 0.02209469000697126, 0.029116084079324617, 0.030613559177949896, 0.02620591106599492, 0.02911399449266604, 0.03106248625793024, 0.034602131164053994, 0.051343302093115016, 0.04131939588043326, 0.019071316528640515, 0.04627588759794972, 0.028409025682594333, 0.01913206872284142, 0.030785702186992517, 0.023296165877465285, 0.029096430917862598, 0.022653204797103157, 0.02655491768265423, 0.055512235373863586, 0.0428591507042676, 0.07825859321868629, 0.028136504350262683, 0.029137148113304735, 0.04711745233951438], 'lossList': [0.0, -1.3235739415884018, 0.0, 2.3176176288723944, 0.0, 0.0, 0.0], 'rewardMean': 0.7786434925497074, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 972.3897048466927, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=84.96
