#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 54
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.9306501053633052, 'errorList': [], 'lossList': [0.0, -1.4160368621349335, 0.0, 28.05533133983612, 0.0, 0.0, 0.0], 'rewardMean': 0.9258701197573562, 'totalEpisodes': 102, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.525082493869839
'totalSteps': 3840, 'rewardStep': 0.8267935298614786, 'errorList': [], 'lossList': [0.0, -1.4187470662593842, 0.0, 35.16865797996521, 0.0, 0.0, 0.0], 'rewardMean': 0.8928445897920637, 'totalEpisodes': 168, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.00364741131655
'totalSteps': 5120, 'rewardStep': 0.9759604312970358, 'errorList': [], 'lossList': [0.0, -1.4159402787685393, 0.0, 35.306510581970215, 0.0, 0.0, 0.0], 'rewardMean': 0.9136235501683068, 'totalEpisodes': 199, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.147008874902602
'totalSteps': 6400, 'rewardStep': 0.6233316489210587, 'errorList': [], 'lossList': [0.0, -1.4102362549304963, 0.0, 36.814600009918216, 0.0, 0.0, 0.0], 'rewardMean': 0.8555651699188571, 'totalEpisodes': 214, 'stepsPerEpisode': 36, 'rewardPerEpisode': 23.10337869047502
'totalSteps': 7680, 'rewardStep': 0.7508886112940802, 'errorList': [], 'lossList': [0.0, -1.4045913046598435, 0.0, 42.88888935565949, 0.0, 0.0, 0.0], 'rewardMean': 0.8381190768147276, 'totalEpisodes': 224, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.70544399979474
'totalSteps': 8960, 'rewardStep': 0.6009880406556666, 'errorList': [], 'lossList': [0.0, -1.386050242781639, 0.0, 32.50643619537354, 0.0, 0.0, 0.0], 'rewardMean': 0.8042432145062903, 'totalEpisodes': 230, 'stepsPerEpisode': 112, 'rewardPerEpisode': 81.54184591602224
'totalSteps': 10240, 'rewardStep': 0.977859921979204, 'errorList': [1.9783175787584004, 15.551147276684098, 13.62581629615594, 8.513177342705516, 12.413096547185622, 7.635785571374581, 0.8821183963219371, 7.415078226153335, 2.821099130373279, 13.130745463506553, 6.409816980990683, 15.312577078266056, 12.270454808137563, 24.205557368074533, 26.4511555266311, 23.036211279036152, 0.45026748861916893, 4.890945375725914, 32.38481707175124, 2.270275690543569, 19.446608582209887, 49.835892089887025, 9.263072857568387, 5.942121374997966, 11.235232223865209, 39.48272149205635, 1.0867524658701269, 2.2865261143482796, 10.71372821366397, 4.235334372648063, 13.252985664579358, 29.593950509901752, 16.96253094188019, 24.415291702861012, 88.2591026021712, 3.302795982545503, 22.805398525070032, 40.12873785658831, 4.204967999831206, 37.05731982985107, 4.3807731740754745, 5.783628728408195, 13.106898390993756, 10.765789592767135, 40.358020084133706, 3.151904300125605, 14.1007563115503, 50.299715232127056, 33.04757334758471, 0.15590466570604117], 'lossList': [0.0, -1.3799682080745697, 0.0, 18.79305354475975, 0.0, 0.0, 0.0], 'rewardMean': 0.8259453029404046, 'totalEpisodes': 237, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.7456729617891655, 'successfulTests': 1
'totalSteps': 11520, 'rewardStep': 0.5729637731197899, 'errorList': [], 'lossList': [0.0, -1.3922506886720658, 0.0, 10.325296469926833, 0.0, 0.0, 0.0], 'rewardMean': 0.7978362440714474, 'totalEpisodes': 241, 'stepsPerEpisode': 193, 'rewardPerEpisode': 134.30222538346575
'totalSteps': 12800, 'rewardStep': 0.7632209478770076, 'errorList': [], 'lossList': [0.0, -1.383503761291504, 0.0, 25.312455580234527, 0.0, 0.0, 0.0], 'rewardMean': 0.7943747144520035, 'totalEpisodes': 247, 'stepsPerEpisode': 63, 'rewardPerEpisode': 51.475465386794895
'totalSteps': 14080, 'rewardStep': 0.9212206946541419, 'errorList': [], 'lossList': [0.0, -1.3797871100902557, 0.0, 6.62399300634861, 0.0, 0.0, 0.0], 'rewardMean': 0.7943877705022768, 'totalEpisodes': 252, 'stepsPerEpisode': 129, 'rewardPerEpisode': 116.77240563528403
'totalSteps': 15360, 'rewardStep': 0.9003239854430208, 'errorList': [], 'lossList': [0.0, -1.3903740853071214, 0.0, 21.394667398929595, 0.0, 0.0, 0.0], 'rewardMean': 0.7913551585102484, 'totalEpisodes': 255, 'stepsPerEpisode': 14, 'rewardPerEpisode': 13.546630359733157
'totalSteps': 16640, 'rewardStep': 0.4976031847110771, 'errorList': [], 'lossList': [0.0, -1.3634475004673003, 0.0, 4.393299490511417, 0.0, 0.0, 0.0], 'rewardMean': 0.7584361239952082, 'totalEpisodes': 256, 'stepsPerEpisode': 327, 'rewardPerEpisode': 242.93399998623536
'totalSteps': 17920, 'rewardStep': 0.39574453465100806, 'errorList': [], 'lossList': [0.0, -1.3357696491479873, 0.0, 9.925633759498597, 0.0, 0.0, 0.0], 'rewardMean': 0.7004145343306056, 'totalEpisodes': 258, 'stepsPerEpisode': 262, 'rewardPerEpisode': 184.92374455437724
'totalSteps': 19200, 'rewardStep': 0.8846092138884942, 'errorList': [], 'lossList': [0.0, -1.3316930705308914, 0.0, 12.940596572160722, 0.0, 0.0, 0.0], 'rewardMean': 0.7265422908273491, 'totalEpisodes': 262, 'stepsPerEpisode': 80, 'rewardPerEpisode': 73.27380459909477
'totalSteps': 20480, 'rewardStep': 0.9641555632339613, 'errorList': [0.367014885930613, 0.40326641735499114, 0.505313059718277, 0.3717862617469869, 0.2947785129588227, 0.4176485497638181, 0.4117905484280665, 0.31548594700928784, 0.3416565345009261, 0.3311342185929215, 0.34206134581866315, 0.27328145817243266, 0.26364243143297766, 0.345447307404914, 0.45200781330040773, 0.4385470203383947, 0.27709526483357544, 0.4150606255212972, 0.4970168153069755, 0.39274874481161687, 0.26693154776714095, 0.33581122725890616, 0.2634051643148614, 0.31123097102670794, 0.4354223224802752, 0.40483672314894775, 0.33942516288479085, 0.36598438565498115, 0.29157281098312393, 0.24617773478060068, 0.2925234121381228, 0.5349770104429163, 0.40339901048503035, 0.45917845993348627, 0.3680449670442153, 0.42139624325093233, 0.2884611195000025, 0.29063140062749493, 0.30896780424500014, 0.3631983246453185, 0.24439555115479653, 0.40545588489820594, 0.5183068540214917, 0.36462650049667356, 0.357837399712942, 0.46949628215851824, 0.2667227077060895, 0.3080902897527335, 0.33509682214797887, 0.30601648518902214], 'lossList': [0.0, -1.3024662095308304, 0.0, 2.166593820825219, 0.0, 0.0, 0.0], 'rewardMean': 0.7478689860213371, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.6596697463906, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.3961778986109763, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6692197694800454, 'totalEpisodes': 263, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.6185442484388
'totalSteps': 23040, 'rewardStep': 0.9030105447698544, 'errorList': [], 'lossList': [0.0, -1.2785312116146088, 0.0, 3.590453841686249, 0.0, 0.0, 0.0], 'rewardMean': 0.7022244466450519, 'totalEpisodes': 263, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.2517049429961
'totalSteps': 24320, 'rewardStep': 0.8102769099422339, 'errorList': [], 'lossList': [0.0, -1.280419285297394, 0.0, 1.020504012517631, 0.0, 0.0, 0.0], 'rewardMean': 0.7069300428515743, 'totalEpisodes': 263, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1140.5360581975988
'totalSteps': 25600, 'rewardStep': 0.9592881807566764, 'errorList': [0.23139637636106342, 0.12066648080583134, 0.1612260726656793, 0.21243536744321193, 0.1367363112019147, 0.13931903714900515, 0.23174348728515576, 0.11713611842781343, 0.15238395780319697, 0.1466209199670857, 0.17418291352566342, 0.14579646768211188, 0.12647411419859592, 0.13245734204502518, 0.10285171146863184, 0.20749987994363975, 0.16899244986865714, 0.14145484041375325, 0.14269060562177044, 0.13444075117984217, 0.16163844027814345, 0.12680756197990706, 0.15629313457770863, 0.13179003010488965, 0.14178526107840364, 0.13884796362435003, 0.11577895076848864, 0.1610511562113264, 0.17832805641114133, 0.14473148191929133, 0.13597988973743733, 0.1307840681860566, 0.18282382180211756, 0.18446695180882872, 0.15959409223739351, 0.16581606966868495, 0.1401983508887475, 0.19427044630604726, 0.13866595330092538, 0.15706291082893853, 0.1512384018432008, 0.1442386379935506, 0.13904526284218172, 0.15976299514612421, 0.12214635104742706, 0.1634943263201324, 0.15979090083197192, 0.15126385365890166, 0.11590584876287116, 0.14890269962240368], 'lossList': [0.0, -1.2681550306081772, 0.0, 0.5640811906382441, 0.0, 0.0, 0.0], 'rewardMean': 0.7107367914618278, 'totalEpisodes': 263, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.7364855354015, 'successfulTests': 46
#maxSuccessfulTests=46, maxSuccessfulTestsAtStep=25600, timeSpent=119.91
