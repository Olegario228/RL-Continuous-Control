#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 55
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.7302736225840617, 'errorList': [], 'lossList': [0.0, -1.3956418693065644, 0.0, 32.290983057022096, 0.0, 0.0, 0.0], 'rewardMean': 0.6704718607629561, 'totalEpisodes': 53, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.3883473937346515
'totalSteps': 3840, 'rewardStep': 0.7126976202315216, 'errorList': [], 'lossList': [0.0, -1.381633734703064, 0.0, 41.47727374076843, 0.0, 0.0, 0.0], 'rewardMean': 0.6845471139191446, 'totalEpisodes': 114, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.8182392178176334
'totalSteps': 5120, 'rewardStep': 0.7323908282129264, 'errorList': [], 'lossList': [0.0, -1.362175953388214, 0.0, 46.10284862518311, 0.0, 0.0, 0.0], 'rewardMean': 0.69650804249259, 'totalEpisodes': 153, 'stepsPerEpisode': 44, 'rewardPerEpisode': 35.07026921292844
'totalSteps': 6400, 'rewardStep': 0.8221340697705366, 'errorList': [], 'lossList': [0.0, -1.3507837331295014, 0.0, 35.13737263679504, 0.0, 0.0, 0.0], 'rewardMean': 0.7216332479481793, 'totalEpisodes': 167, 'stepsPerEpisode': 127, 'rewardPerEpisode': 98.2593363754111
'totalSteps': 7680, 'rewardStep': 0.8250623725526697, 'errorList': [], 'lossList': [0.0, -1.3289812231063842, 0.0, 50.85146970748902, 0.0, 0.0, 0.0], 'rewardMean': 0.7388714353822611, 'totalEpisodes': 180, 'stepsPerEpisode': 41, 'rewardPerEpisode': 34.37949150571753
'totalSteps': 8960, 'rewardStep': 0.6749646365542985, 'errorList': [], 'lossList': [0.0, -1.3116889703273773, 0.0, 34.543799710273746, 0.0, 0.0, 0.0], 'rewardMean': 0.7297418926925522, 'totalEpisodes': 186, 'stepsPerEpisode': 270, 'rewardPerEpisode': 185.69603891634983
'totalSteps': 10240, 'rewardStep': 0.871798565558678, 'errorList': [], 'lossList': [0.0, -1.3184262228012085, 0.0, 81.32612211227416, 0.0, 0.0, 0.0], 'rewardMean': 0.7474989768008178, 'totalEpisodes': 197, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.874523529644629
'totalSteps': 11520, 'rewardStep': 0.8227439659762811, 'errorList': [], 'lossList': [0.0, -1.319496488571167, 0.0, 51.87755108833313, 0.0, 0.0, 0.0], 'rewardMean': 0.755859531153647, 'totalEpisodes': 203, 'stepsPerEpisode': 67, 'rewardPerEpisode': 58.55629326409293
'totalSteps': 12800, 'rewardStep': 0.8227117271973765, 'errorList': [], 'lossList': [0.0, -1.3125920242071152, 0.0, 21.16874689936638, 0.0, 0.0, 0.0], 'rewardMean': 0.76254475075802, 'totalEpisodes': 204, 'stepsPerEpisode': 817, 'rewardPerEpisode': 703.0015872265616
'totalSteps': 14080, 'rewardStep': 0.6952278995049652, 'errorList': [], 'lossList': [0.0, -1.296219739317894, 0.0, 10.722864499390125, 0.0, 0.0, 0.0], 'rewardMean': 0.7710005308143316, 'totalEpisodes': 206, 'stepsPerEpisode': 545, 'rewardPerEpisode': 451.44306953893084
'totalSteps': 15360, 'rewardStep': 0.4356623511162167, 'errorList': [], 'lossList': [0.0, -1.3142428439855576, 0.0, 11.818139653205872, 0.0, 0.0, 0.0], 'rewardMean': 0.741539403667547, 'totalEpisodes': 209, 'stepsPerEpisode': 279, 'rewardPerEpisode': 221.39591552556035
'totalSteps': 16640, 'rewardStep': 0.5989021923287815, 'errorList': [], 'lossList': [0.0, -1.3059933245182038, 0.0, 6.472406111955642, 0.0, 0.0, 0.0], 'rewardMean': 0.7301598608772729, 'totalEpisodes': 211, 'stepsPerEpisode': 829, 'rewardPerEpisode': 528.1851093201313
'totalSteps': 17920, 'rewardStep': 0.9157630972333499, 'errorList': [], 'lossList': [0.0, -1.2926870495080949, 0.0, 5.296940599679947, 0.0, 0.0, 0.0], 'rewardMean': 0.7484970877793155, 'totalEpisodes': 214, 'stepsPerEpisode': 40, 'rewardPerEpisode': 31.591392025565472
'totalSteps': 19200, 'rewardStep': 0.9940983310805911, 'errorList': [1.022910362306861, 0.38960440699748367, 1.361724408142759, 0.18443987429133785, 0.9370198531282485, 0.48379142955379517, 1.0930245940895387, 0.7214505912467155, 0.4212790745050742, 0.6141581518647434, 0.613158758381382, 0.3102557873228497, 0.17602020476646868, 0.9962116063852885, 0.7531183241964193, 0.22784943386155698, 0.2677748009525403, 0.9284695340136998, 0.8402417026001923, 0.8574769040732438, 0.37423338315950266, 0.602306175781801, 0.6545733173638688, 0.7487884526893435, 1.2827119858961318, 0.19267633251780691, 0.509614305935391, 0.1572552150281922, 0.15025145931406797, 0.467895267412606, 1.0490121632450118, 0.7889393572019573, 0.6941937203570551, 1.3492151789042885, 0.7528493077602271, 0.48398789184936214, 0.3573061417182798, 0.7879160316677598, 0.3370524237997937, 0.9057401396580933, 0.2924021454660382, 0.3103466311622851, 0.30999216888167963, 0.6153942656499449, 0.1530877950000472, 1.0175897164461423, 0.9877337783594671, 0.6629745230557775, 0.5319866022748257, 0.7608183217937007], 'lossList': [0.0, -1.2888317853212357, 0.0, 5.178896380066871, 0.0, 0.0, 0.0], 'rewardMean': 0.7656935139103208, 'totalEpisodes': 218, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.434064762706416, 'successfulTests': 6
'totalSteps': 20480, 'rewardStep': 0.651205987328489, 'errorList': [], 'lossList': [0.0, -1.265866330265999, 0.0, 3.462941790819168, 0.0, 0.0, 0.0], 'rewardMean': 0.7483078753879029, 'totalEpisodes': 219, 'stepsPerEpisode': 738, 'rewardPerEpisode': 602.1761880537537
'totalSteps': 21760, 'rewardStep': 0.6741697060933063, 'errorList': [], 'lossList': [0.0, -1.2493255835771562, 0.0, 5.593933393955231, 0.0, 0.0, 0.0], 'rewardMean': 0.7482283823418034, 'totalEpisodes': 221, 'stepsPerEpisode': 491, 'rewardPerEpisode': 362.37316822389414
'totalSteps': 23040, 'rewardStep': 0.6976921275870075, 'errorList': [], 'lossList': [0.0, -1.2241505420207976, 0.0, 1.3657276324927807, 0.0, 0.0, 0.0], 'rewardMean': 0.7308177385446365, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.3646431734828
'totalSteps': 24320, 'rewardStep': 0.6968110069100089, 'errorList': [], 'lossList': [0.0, -1.1932975298166275, 0.0, 0.8092400075495243, 0.0, 0.0, 0.0], 'rewardMean': 0.7182244426380093, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.9756166764873
'totalSteps': 25600, 'rewardStep': 0.8921549374442466, 'errorList': [], 'lossList': [0.0, -1.1580975204706192, 0.0, 0.9934726034104824, 0.0, 0.0, 0.0], 'rewardMean': 0.7251687636626963, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.8098670616637
#maxSuccessfulTests=6, maxSuccessfulTestsAtStep=19200, timeSpent=78.28
