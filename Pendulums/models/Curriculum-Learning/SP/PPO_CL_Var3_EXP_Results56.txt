#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 56
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.9501950317525638, 'errorList': [], 'lossList': [0.0, -1.4035588657855989, 0.0, 34.08474109649658, 0.0, 0.0, 0.0], 'rewardMean': 0.9567422918316368, 'totalEpisodes': 50, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.52968458765971
'totalSteps': 3840, 'rewardStep': 0.904709341238384, 'errorList': [], 'lossList': [0.0, -1.3936301070451735, 0.0, 47.814743251800536, 0.0, 0.0, 0.0], 'rewardMean': 0.9393979749672191, 'totalEpisodes': 103, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.904709341238384
'totalSteps': 5120, 'rewardStep': 0.8976627562209684, 'errorList': [], 'lossList': [0.0, -1.3837013000249863, 0.0, 52.16990119934082, 0.0, 0.0, 0.0], 'rewardMean': 0.9289641702806565, 'totalEpisodes': 141, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.244404696306529
'totalSteps': 6400, 'rewardStep': 0.7497477059489632, 'errorList': [], 'lossList': [0.0, -1.3777342706918716, 0.0, 55.21613814353943, 0.0, 0.0, 0.0], 'rewardMean': 0.8931208774143178, 'totalEpisodes': 161, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.38695209159651
'totalSteps': 7680, 'rewardStep': 0.823800240788745, 'errorList': [], 'lossList': [0.0, -1.3637917071580887, 0.0, 31.691829175949096, 0.0, 0.0, 0.0], 'rewardMean': 0.8815674379767223, 'totalEpisodes': 164, 'stepsPerEpisode': 805, 'rewardPerEpisode': 587.0713167249338
'totalSteps': 8960, 'rewardStep': 0.7551401545951034, 'errorList': [], 'lossList': [0.0, -1.338770351409912, 0.0, 29.01416881799698, 0.0, 0.0, 0.0], 'rewardMean': 0.8635063974936339, 'totalEpisodes': 170, 'stepsPerEpisode': 60, 'rewardPerEpisode': 49.05378500154168
'totalSteps': 10240, 'rewardStep': 0.8744193390648075, 'errorList': [], 'lossList': [0.0, -1.3353383493423463, 0.0, 23.64154509663582, 0.0, 0.0, 0.0], 'rewardMean': 0.8648705151900307, 'totalEpisodes': 171, 'stepsPerEpisode': 88, 'rewardPerEpisode': 77.66649220117563
'totalSteps': 11520, 'rewardStep': 0.6436558394174767, 'errorList': [], 'lossList': [0.0, -1.3287347996234893, 0.0, 43.49049397468567, 0.0, 0.0, 0.0], 'rewardMean': 0.840291106770858, 'totalEpisodes': 174, 'stepsPerEpisode': 487, 'rewardPerEpisode': 362.59268797183535
'totalSteps': 12800, 'rewardStep': 0.7142840899094935, 'errorList': [], 'lossList': [0.0, -1.3178697645664215, 0.0, 30.41852536201477, 0.0, 0.0, 0.0], 'rewardMean': 0.8276904050847216, 'totalEpisodes': 177, 'stepsPerEpisode': 581, 'rewardPerEpisode': 443.9739816207575
'totalSteps': 14080, 'rewardStep': 0.9195963039978285, 'errorList': [], 'lossList': [0.0, -1.286103612780571, 0.0, 4.529990013837814, 0.0, 0.0, 0.0], 'rewardMean': 0.8233210802934334, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 936.6370182927267
'totalSteps': 15360, 'rewardStep': 0.8326623005813156, 'errorList': [], 'lossList': [0.0, -1.2536681360006332, 0.0, 16.447235607504844, 0.0, 0.0, 0.0], 'rewardMean': 0.8115678071763085, 'totalEpisodes': 179, 'stepsPerEpisode': 612, 'rewardPerEpisode': 445.50345974239195
'totalSteps': 16640, 'rewardStep': 0.5539147500941559, 'errorList': [], 'lossList': [0.0, -1.239008384346962, 0.0, 2.6920780259370805, 0.0, 0.0, 0.0], 'rewardMean': 0.7764883480618858, 'totalEpisodes': 179, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 911.694211294118
'totalSteps': 17920, 'rewardStep': 0.6889966447588444, 'errorList': [], 'lossList': [0.0, -1.2237089473009108, 0.0, 4.322756913602352, 0.0, 0.0, 0.0], 'rewardMean': 0.7556217369156732, 'totalEpisodes': 180, 'stepsPerEpisode': 996, 'rewardPerEpisode': 739.0426996602822
'totalSteps': 19200, 'rewardStep': 0.7422536330545855, 'errorList': [], 'lossList': [0.0, -1.2199421912431716, 0.0, 1.627119659408927, 0.0, 0.0, 0.0], 'rewardMean': 0.7548723296262356, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 989.2889406272072
'totalSteps': 20480, 'rewardStep': 0.6088732516721216, 'errorList': [], 'lossList': [0.0, -1.2079560548067092, 0.0, 1.3514519712328912, 0.0, 0.0, 0.0], 'rewardMean': 0.7333796307145732, 'totalEpisodes': 181, 'stepsPerEpisode': 1235, 'rewardPerEpisode': 931.7400822383106
'totalSteps': 21760, 'rewardStep': 0.9096278871054773, 'errorList': [], 'lossList': [0.0, -1.1886996507644654, 0.0, 1.0321468715369702, 0.0, 0.0, 0.0], 'rewardMean': 0.7488284039656106, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1034.6062868262047
'totalSteps': 23040, 'rewardStep': 0.9170881706058509, 'errorList': [], 'lossList': [0.0, -1.1671607083082198, 0.0, 1.0349840802699328, 0.0, 0.0, 0.0], 'rewardMean': 0.753095287119715, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.1658206757313
'totalSteps': 24320, 'rewardStep': 0.8093548187285623, 'errorList': [], 'lossList': [0.0, -1.1386135709285736, 0.0, 0.6159905081987381, 0.0, 0.0, 0.0], 'rewardMean': 0.7696651850508236, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.4108516613005
'totalSteps': 25600, 'rewardStep': 0.9793831796320326, 'errorList': [0.10354735771251283, 0.057654254869948574, 0.1103118007823361, 0.12835986014686696, 0.08875133143552048, 0.10880863325623083, 0.11696645345358611, 0.2025816266277791, 0.2170834159891261, 0.16615554225184326, 0.034476951967349484, 0.022764550171364525, 0.13131427272622204, 0.05641181667692556, 0.044325263061707566, 0.16005175415860812, 0.03429770765479935, 0.03582823929555192, 0.05381101213819877, 0.062228107508837824, 0.023153983038058844, 0.07007932716882068, 0.11520322265511236, 0.33022278010787903, 0.16338375066142363, 0.1076430488282035, 0.10930863601886058, 0.2057293422200319, 0.0406864607459453, 0.057861452598685366, 0.15569712800669572, 0.0664735025839216, 0.23905217307138749, 0.3098878119170173, 0.08082314813174658, 0.1616774602650247, 0.2156985542079324, 0.0783083294229566, 0.15633760903435567, 0.08341771986608403, 0.019621197558084856, 0.07115658323583307, 0.05201615437362762, 0.04922836372842717, 0.12172934537091282, 0.21879711102937685, 0.03837560314396797, 0.24956371941003966, 0.06423968817796619, 0.034157445544296554], 'lossList': [0.0, -1.0929445511102676, 0.0, 0.4557823924534023, 0.0, 0.0, 0.0], 'rewardMean': 0.7961750940230774, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.1999504099674, 'successfulTests': 41
#maxSuccessfulTests=41, maxSuccessfulTestsAtStep=25600, timeSpent=79.15
