#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 57
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.6044418367036246, 'errorList': [], 'lossList': [0.0, -1.4276759654283524, 0.0, 34.91709354400635, 0.0, 0.0, 0.0], 'rewardMean': 0.6991059189418879, 'totalEpisodes': 53, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.252602028107212
'totalSteps': 3840, 'rewardStep': 0.7348425375186729, 'errorList': [], 'lossList': [0.0, -1.4096539628505707, 0.0, 43.97882703781128, 0.0, 0.0, 0.0], 'rewardMean': 0.7110181251341495, 'totalEpisodes': 101, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.012429640422037
'totalSteps': 5120, 'rewardStep': 0.6110924937883466, 'errorList': [], 'lossList': [0.0, -1.387373326420784, 0.0, 45.0478041267395, 0.0, 0.0, 0.0], 'rewardMean': 0.6860367172976989, 'totalEpisodes': 148, 'stepsPerEpisode': 32, 'rewardPerEpisode': 16.614154439423338
'totalSteps': 6400, 'rewardStep': 0.8163368969388822, 'errorList': [], 'lossList': [0.0, -1.3652990543842316, 0.0, 44.305406951904295, 0.0, 0.0, 0.0], 'rewardMean': 0.7120967532259355, 'totalEpisodes': 172, 'stepsPerEpisode': 45, 'rewardPerEpisode': 31.926245688719977
'totalSteps': 7680, 'rewardStep': 0.9192373612678222, 'errorList': [], 'lossList': [0.0, -1.3476346033811568, 0.0, 43.41499723911286, 0.0, 0.0, 0.0], 'rewardMean': 0.7466201878995834, 'totalEpisodes': 184, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.79673549499253
'totalSteps': 8960, 'rewardStep': 0.531425682716803, 'errorList': [], 'lossList': [0.0, -1.3479174232482911, 0.0, 39.23955560207367, 0.0, 0.0, 0.0], 'rewardMean': 0.7158781157306147, 'totalEpisodes': 190, 'stepsPerEpisode': 135, 'rewardPerEpisode': 104.34267014705287
'totalSteps': 10240, 'rewardStep': 0.6362507138997144, 'errorList': [], 'lossList': [0.0, -1.3457156711816787, 0.0, 21.75808639883995, 0.0, 0.0, 0.0], 'rewardMean': 0.7059246905017522, 'totalEpisodes': 193, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.698568115259825
'totalSteps': 11520, 'rewardStep': 0.959525982578499, 'errorList': [1.234562625178698, 1.2917825683980169, 1.570561319799869, 1.429144957006114, 1.4219587147450459, 1.3037307429233074, 1.506123558187565, 1.3285756974723997, 1.9201918220248766, 1.2954036664630977, 1.1939364238747059, 1.3925885052454443, 1.09560384042867, 1.400201665456096, 1.4241148346506696, 1.5495169906652457, 1.6294806138258455, 1.2836911604677144, 1.1141045594315298, 1.3460527030767677, 1.4287418973730062, 0.9142473590870446, 1.1388668533965798, 1.2036044729777613, 1.2756279671983364, 1.4999027618495113, 1.2851587023484428, 1.4377598616893497, 1.0305046891682685, 1.2617401309031293, 1.238886478133113, 1.1202332141424614, 1.152255769024789, 1.0903357437668486, 0.9165922966357096, 1.4997684138224487, 0.9474481440323095, 1.5390778649291317, 1.3865972814004952, 1.222911370436255, 1.4117982391258332, 1.302558403851285, 1.0094043020193975, 1.244516915838405, 1.203143045738852, 1.0425525743955975, 1.0251621282534713, 0.9678831215550637, 1.1907968968483862, 1.3652855438899139], 'lossList': [0.0, -1.327152760028839, 0.0, 25.212071981430054, 0.0, 0.0, 0.0], 'rewardMean': 0.7341026118436129, 'totalEpisodes': 195, 'stepsPerEpisode': 457, 'rewardPerEpisode': 374.1093560305151, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.9326634395577275, 'errorList': [3.2851403459574406, 2.6283636308916987, 2.6609618918984523, 2.8625580610506134, 3.1925556877270944, 3.0628180360920134, 2.788807848591494, 3.731318969511746, 2.693591950468791, 3.585213364641546, 3.5595778835165777, 3.517710646745245, 3.142877046266141, 2.5247480345796354, 2.96769844410995, 2.5896518112012776, 3.2305034702421587, 3.0470448396511194, 2.5336807931064262, 2.8520579760601743, 2.5787378047853546, 2.7533572289828765, 3.0427080960713178, 3.5045553478597995, 2.5730560414243495, 2.4703665540785393, 3.086925888064833, 2.8169574229038643, 3.464889391409606, 3.744611105721597, 2.707945411097814, 3.2304225429008295, 2.864188294059682, 2.788532187118181, 2.8084476119823374, 3.687544260722295, 2.810226438745936, 3.041874637345265, 2.8761482158264573, 2.4930489487454675, 3.5832596276288546, 3.7189860961751786, 3.4601637805353653, 3.4156020950441355, 2.6144337138527907, 3.188647887063034, 2.499462604164038, 2.6618114658021423, 3.230317150309211, 2.87284095032941], 'lossList': [0.0, -1.3168062794208526, 0.0, 57.3338736486435, 0.0, 0.0, 0.0], 'rewardMean': 0.7539586946150244, 'totalEpisodes': 198, 'stepsPerEpisode': 346, 'rewardPerEpisode': 278.2026081658278, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.6797484520051517, 'errorList': [], 'lossList': [0.0, -1.3220748436450958, 0.0, 71.47174984931945, 0.0, 0.0, 0.0], 'rewardMean': 0.7425565396975244, 'totalEpisodes': 202, 'stepsPerEpisode': 118, 'rewardPerEpisode': 95.6499661229729
'totalSteps': 15360, 'rewardStep': 0.9421978790969238, 'errorList': [2.028070324544783, 2.3942015125729887, 0.9220139338850766, 3.667089861167384, 0.7486022940718217, 0.232280207468659, 4.577996965234243, 5.663456243779592, 3.489827598281963, 0.3692610268511273, 2.920621316056274, 3.079792892066128, 0.6410394630303081, 5.251298957583391, 2.636447332856861, 1.7707641540040218, 1.74732052980194, 6.0096429950666455, 2.1997066915940433, 1.4448196950568635, 5.173258969565587, 1.961474956507679, 0.5924932489038325, 3.4666961469752238, 3.4072649325154307, 5.533776692811816, 3.4995922597377915, 2.809552644401032, 8.418604199392831, 0.3627760951117987, 2.4762314224534165, 0.3537862527845361, 6.520296567892455, 1.1940437397639525, 1.1876781412582291, 3.481952111395873, 4.177096342276997, 0.6076603332625877, 1.5512894698652249, 3.8710526651747235, 2.5491958223107867, 2.1161979156825352, 5.093776097960003, 4.349880341477111, 7.260767810350889, 5.989365001706594, 3.976585512261107, 4.8893950439424625, 3.6315176383137557, 1.0068867181747418], 'lossList': [0.0, -1.326118502020836, 0.0, 62.57021678447723, 0.0, 0.0, 0.0], 'rewardMean': 0.7763321439368542, 'totalEpisodes': 207, 'stepsPerEpisode': 71, 'rewardPerEpisode': 60.959980771907645, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6034849262759386, 'errorList': [], 'lossList': [0.0, -1.335922001004219, 0.0, 16.804828947782518, 0.0, 0.0, 0.0], 'rewardMean': 0.7631963828125808, 'totalEpisodes': 209, 'stepsPerEpisode': 445, 'rewardPerEpisode': 356.442589723297
'totalSteps': 17920, 'rewardStep': 0.7089569154188852, 'errorList': [], 'lossList': [0.0, -1.3398976302146912, 0.0, 13.345638499259948, 0.0, 0.0, 0.0], 'rewardMean': 0.7729828249756349, 'totalEpisodes': 212, 'stepsPerEpisode': 506, 'rewardPerEpisode': 364.9390442220488
'totalSteps': 19200, 'rewardStep': 0.9202779113289165, 'errorList': [], 'lossList': [0.0, -1.3415533846616745, 0.0, 7.0512811595201494, 0.0, 0.0, 0.0], 'rewardMean': 0.7833769264146382, 'totalEpisodes': 214, 'stepsPerEpisode': 115, 'rewardPerEpisode': 104.80155590424968
'totalSteps': 20480, 'rewardStep': 0.8400736328419567, 'errorList': [], 'lossList': [0.0, -1.3533278185129165, 0.0, 4.954836498498917, 0.0, 0.0, 0.0], 'rewardMean': 0.7754605535720517, 'totalEpisodes': 217, 'stepsPerEpisode': 264, 'rewardPerEpisode': 236.85240137526196
'totalSteps': 21760, 'rewardStep': 0.4660428488652188, 'errorList': [], 'lossList': [0.0, -1.3595635360479355, 0.0, 2.375521809756756, 0.0, 0.0, 0.0], 'rewardMean': 0.7689222701868933, 'totalEpisodes': 219, 'stepsPerEpisode': 495, 'rewardPerEpisode': 340.87563493283
'totalSteps': 23040, 'rewardStep': 0.8034432986883924, 'errorList': [], 'lossList': [0.0, -1.3633757215738296, 0.0, 1.6814566296339035, 0.0, 0.0, 0.0], 'rewardMean': 0.785641528665761, 'totalEpisodes': 220, 'stepsPerEpisode': 209, 'rewardPerEpisode': 175.60108758065954
'totalSteps': 24320, 'rewardStep': 0.6769260860746953, 'errorList': [], 'lossList': [0.0, -1.349149879217148, 0.0, 1.272387108206749, 0.0, 0.0, 0.0], 'rewardMean': 0.7573815390153806, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.69903153859
'totalSteps': 25600, 'rewardStep': 0.24793865967433254, 'errorList': [], 'lossList': [0.0, -1.342914384007454, 0.0, 9.171881934404373, 0.0, 0.0, 0.0], 'rewardMean': 0.6889090610270412, 'totalEpisodes': 222, 'stepsPerEpisode': 429, 'rewardPerEpisode': 327.16758987629885
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=120.68
