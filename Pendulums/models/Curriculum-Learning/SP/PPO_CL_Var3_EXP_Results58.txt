#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 58
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.7757481073737381, 'errorList': [], 'lossList': [0.0, -1.4274780064821244, 0.0, 34.65782874107361, 0.0, 0.0, 0.0], 'rewardMean': 0.5600516494595965, 'totalEpisodes': 48, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.482461528495576
'totalSteps': 3840, 'rewardStep': 0.997592964505021, 'errorList': [], 'lossList': [0.0, -1.4213621497154236, 0.0, 45.973002948760985, 0.0, 0.0, 0.0], 'rewardMean': 0.705898754474738, 'totalEpisodes': 100, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.458060402057361
'totalSteps': 5120, 'rewardStep': 0.5951198514194793, 'errorList': [], 'lossList': [0.0, -1.4153923463821412, 0.0, 51.07319574356079, 0.0, 0.0, 0.0], 'rewardMean': 0.6782040287109233, 'totalEpisodes': 125, 'stepsPerEpisode': 13, 'rewardPerEpisode': 7.02724372655427
'totalSteps': 6400, 'rewardStep': 0.8006541932366795, 'errorList': [], 'lossList': [0.0, -1.4008186435699463, 0.0, 54.883461771011355, 0.0, 0.0, 0.0], 'rewardMean': 0.7026940616160745, 'totalEpisodes': 139, 'stepsPerEpisode': 156, 'rewardPerEpisode': 107.40644185547255
'totalSteps': 7680, 'rewardStep': 0.7397294869416542, 'errorList': [], 'lossList': [0.0, -1.380845823287964, 0.0, 37.30033202171326, 0.0, 0.0, 0.0], 'rewardMean': 0.7088666325036712, 'totalEpisodes': 146, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.9112247971707
'totalSteps': 8960, 'rewardStep': 0.6879741167931007, 'errorList': [], 'lossList': [0.0, -1.3797702586650848, 0.0, 35.26846485137939, 0.0, 0.0, 0.0], 'rewardMean': 0.7058819874021612, 'totalEpisodes': 151, 'stepsPerEpisode': 474, 'rewardPerEpisode': 298.8951847935012
'totalSteps': 10240, 'rewardStep': 0.7654148077773528, 'errorList': [], 'lossList': [0.0, -1.3805913180112839, 0.0, 21.322889746427535, 0.0, 0.0, 0.0], 'rewardMean': 0.71332358994906, 'totalEpisodes': 155, 'stepsPerEpisode': 84, 'rewardPerEpisode': 70.21593022301451
'totalSteps': 11520, 'rewardStep': 0.35332886961764265, 'errorList': [], 'lossList': [0.0, -1.3614074075222016, 0.0, 12.316624267101288, 0.0, 0.0, 0.0], 'rewardMean': 0.6733241765789025, 'totalEpisodes': 159, 'stepsPerEpisode': 158, 'rewardPerEpisode': 110.61811634949204
'totalSteps': 12800, 'rewardStep': 0.48507057065395737, 'errorList': [], 'lossList': [0.0, -1.3473678153753281, 0.0, 9.183688757121564, 0.0, 0.0, 0.0], 'rewardMean': 0.654498815986408, 'totalEpisodes': 160, 'stepsPerEpisode': 1256, 'rewardPerEpisode': 883.1875811877449
'totalSteps': 14080, 'rewardStep': 0.4929380814535294, 'errorList': [], 'lossList': [0.0, -1.3266108375787735, 0.0, 10.432130726575851, 0.0, 0.0, 0.0], 'rewardMean': 0.6693571049772156, 'totalEpisodes': 166, 'stepsPerEpisode': 161, 'rewardPerEpisode': 107.19260572911985
'totalSteps': 15360, 'rewardStep': 0.8572400687323843, 'errorList': [], 'lossList': [0.0, -1.3046592611074448, 0.0, 9.066771357059478, 0.0, 0.0, 0.0], 'rewardMean': 0.6775063011130802, 'totalEpisodes': 173, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.783343545515821
'totalSteps': 16640, 'rewardStep': 0.7743746605040726, 'errorList': [], 'lossList': [0.0, -1.2915675473213195, 0.0, 5.096781895160675, 0.0, 0.0, 0.0], 'rewardMean': 0.6551844707129852, 'totalEpisodes': 179, 'stepsPerEpisode': 56, 'rewardPerEpisode': 46.25293429646445
'totalSteps': 17920, 'rewardStep': 0.5014707054512102, 'errorList': [], 'lossList': [0.0, -1.2856363117694856, 0.0, 3.547099474668503, 0.0, 0.0, 0.0], 'rewardMean': 0.6458195561161583, 'totalEpisodes': 183, 'stepsPerEpisode': 310, 'rewardPerEpisode': 237.38071088240406
'totalSteps': 19200, 'rewardStep': 0.898195043645014, 'errorList': [], 'lossList': [0.0, -1.2993029236793519, 0.0, 1.6358462223410606, 0.0, 0.0, 0.0], 'rewardMean': 0.6555736411569917, 'totalEpisodes': 185, 'stepsPerEpisode': 283, 'rewardPerEpisode': 226.83130061276313
'totalSteps': 20480, 'rewardStep': 0.8864247483166049, 'errorList': [], 'lossList': [0.0, -1.2846299135684967, 0.0, 2.044884299337864, 0.0, 0.0, 0.0], 'rewardMean': 0.6702431672944869, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.739344278448
'totalSteps': 21760, 'rewardStep': 0.8265775138603318, 'errorList': [], 'lossList': [0.0, -1.2580117440223695, 0.0, 0.6837527544796467, 0.0, 0.0, 0.0], 'rewardMean': 0.68410350700121, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1001.4112927586832
'totalSteps': 23040, 'rewardStep': 0.7313791320440066, 'errorList': [], 'lossList': [0.0, -1.2318814998865129, 0.0, 0.5427555208280682, 0.0, 0.0, 0.0], 'rewardMean': 0.6806999394278753, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.594649643715
'totalSteps': 24320, 'rewardStep': 0.8618758719161577, 'errorList': [], 'lossList': [0.0, -1.1973309087753297, 0.0, 1.095124617293477, 0.0, 0.0, 0.0], 'rewardMean': 0.7315546396577269, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.7075856336344
'totalSteps': 25600, 'rewardStep': 0.9318130997552704, 'errorList': [0.040547175976381834, 0.07806400459861502, 0.015884256583775958, 0.016531438054022258, 0.034352791236419516, 0.06910357219930131, 0.09634238993185774, 0.03831006949835033, 0.059974623998755146, 0.017848332087331872, 0.022898280830791234, 0.041717664425565605, 0.05137580307625985, 0.12694716847138798, 0.023984605967074545, 0.023051679492861313, 0.015789328288816063, 0.065894220922597, 0.09041774235504851, 0.12478979653644058, 0.15175719795950415, 0.07650803018776532, 0.06119655564047244, 0.023484128259685502, 0.12101025879901046, 0.02430665673410821, 0.07201437294785383, 0.08932020934405478, 0.04860044197657866, 0.029407680662668453, 0.08565242541300738, 0.12211313198485577, 0.07143788446953338, 0.05852939797760406, 0.016728490726225596, 0.033484255557498566, 0.12083560407709534, 0.029784831394194793, 0.0709331976006302, 0.048943815937114055, 0.04285245169293932, 0.03590828189474814, 0.06039237291529854, 0.09358862407459924, 0.10540257193906631, 0.05938232464180062, 0.028403934191823105, 0.029018930509206276, 0.020903987600166662, 0.04492895131316399], 'lossList': [0.0, -1.17956827044487, 0.0, 0.8154862947762013, 0.0, 0.0, 0.0], 'rewardMean': 0.7762288925678582, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.5491080114025, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.28
