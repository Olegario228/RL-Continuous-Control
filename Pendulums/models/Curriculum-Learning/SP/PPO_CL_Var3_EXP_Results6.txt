#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 6
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.8607299555741099, 'errorList': [], 'lossList': [0.0, -1.4052551448345185, 0.0, 29.45562455177307, 0.0, 0.0, 0.0], 'rewardMean': 0.9120097537424099, 'totalEpisodes': 70, 'stepsPerEpisode': 34, 'rewardPerEpisode': 22.705007735169175
'totalSteps': 3840, 'rewardStep': 0.6114143584140197, 'errorList': [], 'lossList': [0.0, -1.3804853063821794, 0.0, 33.52563512802124, 0.0, 0.0, 0.0], 'rewardMean': 0.8118112886329465, 'totalEpisodes': 134, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.79435370487966
'totalSteps': 5120, 'rewardStep': 0.8434816328660554, 'errorList': [], 'lossList': [0.0, -1.358589089512825, 0.0, 43.19199169158936, 0.0, 0.0, 0.0], 'rewardMean': 0.8197288746912237, 'totalEpisodes': 171, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.403708021112238
'totalSteps': 6400, 'rewardStep': 0.6653117982062645, 'errorList': [], 'lossList': [0.0, -1.3456967157125472, 0.0, 46.22633786201477, 0.0, 0.0, 0.0], 'rewardMean': 0.7888454593942319, 'totalEpisodes': 190, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.281163040678524
'totalSteps': 7680, 'rewardStep': 0.8360667054970581, 'errorList': [], 'lossList': [0.0, -1.3383966588974, 0.0, 27.6142827129364, 0.0, 0.0, 0.0], 'rewardMean': 0.7967156670780362, 'totalEpisodes': 193, 'stepsPerEpisode': 51, 'rewardPerEpisode': 43.40303371088349
'totalSteps': 8960, 'rewardStep': 0.714056405303079, 'errorList': [], 'lossList': [0.0, -1.3150492477416993, 0.0, 53.052332706451416, 0.0, 0.0, 0.0], 'rewardMean': 0.7849072011101852, 'totalEpisodes': 201, 'stepsPerEpisode': 106, 'rewardPerEpisode': 88.72246609575308
'totalSteps': 10240, 'rewardStep': 0.7260977989784885, 'errorList': [], 'lossList': [0.0, -1.3065797930955887, 0.0, 24.949567830562593, 0.0, 0.0, 0.0], 'rewardMean': 0.7775560258437231, 'totalEpisodes': 206, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.9529170281871
'totalSteps': 11520, 'rewardStep': 0.6491494238792923, 'errorList': [], 'lossList': [0.0, -1.3175001698732376, 0.0, 13.87382977962494, 0.0, 0.0, 0.0], 'rewardMean': 0.763288625625453, 'totalEpisodes': 210, 'stepsPerEpisode': 189, 'rewardPerEpisode': 161.57835098175426
'totalSteps': 12800, 'rewardStep': 0.42820195447244924, 'errorList': [], 'lossList': [0.0, -1.294294040799141, 0.0, 6.959962955713272, 0.0, 0.0, 0.0], 'rewardMean': 0.7297799585101525, 'totalEpisodes': 213, 'stepsPerEpisode': 219, 'rewardPerEpisode': 148.0479844662245
'totalSteps': 14080, 'rewardStep': 0.5030272066468682, 'errorList': [], 'lossList': [0.0, -1.2508340400457383, 0.0, 7.40507404923439, 0.0, 0.0, 0.0], 'rewardMean': 0.6837537239837685, 'totalEpisodes': 216, 'stepsPerEpisode': 559, 'rewardPerEpisode': 389.7378649309165
'totalSteps': 15360, 'rewardStep': 0.858105535093688, 'errorList': [], 'lossList': [0.0, -1.210509677529335, 0.0, 5.300862855911255, 0.0, 0.0, 0.0], 'rewardMean': 0.6834912819357262, 'totalEpisodes': 221, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.816732087174675
'totalSteps': 16640, 'rewardStep': 0.5454869861409672, 'errorList': [], 'lossList': [0.0, -1.199225201010704, 0.0, 5.563677888512611, 0.0, 0.0, 0.0], 'rewardMean': 0.676898544708421, 'totalEpisodes': 223, 'stepsPerEpisode': 396, 'rewardPerEpisode': 277.51016478803575
'totalSteps': 17920, 'rewardStep': 0.8128114691963096, 'errorList': [], 'lossList': [0.0, -1.1992833805084229, 0.0, 2.9873986145853997, 0.0, 0.0, 0.0], 'rewardMean': 0.6738315283414464, 'totalEpisodes': 226, 'stepsPerEpisode': 135, 'rewardPerEpisode': 118.40546795152997
'totalSteps': 19200, 'rewardStep': 0.6657522124763746, 'errorList': [], 'lossList': [0.0, -1.1942015671730042, 0.0, 6.063770461082458, 0.0, 0.0, 0.0], 'rewardMean': 0.6738755697684574, 'totalEpisodes': 227, 'stepsPerEpisode': 962, 'rewardPerEpisode': 671.2787952363841
'totalSteps': 20480, 'rewardStep': 0.713871009264877, 'errorList': [], 'lossList': [0.0, -1.179928520321846, 0.0, 2.3481102389097215, 0.0, 0.0, 0.0], 'rewardMean': 0.6616560001452394, 'totalEpisodes': 229, 'stepsPerEpisode': 95, 'rewardPerEpisode': 78.99823759598283
'totalSteps': 21760, 'rewardStep': 0.6584225763025109, 'errorList': [], 'lossList': [0.0, -1.1638016748428344, 0.0, 29.845595782995225, 0.0, 0.0, 0.0], 'rewardMean': 0.6560926172451825, 'totalEpisodes': 230, 'stepsPerEpisode': 820, 'rewardPerEpisode': 577.0404799198275
'totalSteps': 23040, 'rewardStep': 0.8433797777276975, 'errorList': [], 'lossList': [0.0, -1.1292023849487305, 0.0, 1.391878790706396, 0.0, 0.0, 0.0], 'rewardMean': 0.6678208151201035, 'totalEpisodes': 230, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.7695792806712
'totalSteps': 24320, 'rewardStep': 0.7797997870072007, 'errorList': [], 'lossList': [0.0, -1.1005971372127532, 0.0, 0.8212143394351006, 0.0, 0.0, 0.0], 'rewardMean': 0.6808858514328944, 'totalEpisodes': 230, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1047.8748411315173
'totalSteps': 25600, 'rewardStep': 0.9413969435928641, 'errorList': [0.1728677096032319, 0.17151719425215212, 0.1710934455920063, 0.16307967072321733, 0.18287120773133517, 0.15831478505292937, 0.16561630557550278, 0.16356235556760446, 0.16673686830486512, 0.1688663205731653, 0.25875171111680645, 0.15892347742092067, 0.16835489070898343, 0.2331783620003703, 0.17149689593715406, 0.16813559922243715, 0.1679943995681385, 0.16689579633396806, 0.16438636738337772, 0.1582340926544959, 0.20727210729216156, 0.16458538685611154, 0.1713113201130327, 0.1686883149402035, 0.16970197520790303, 0.1781250914111551, 0.17540611959647695, 0.18166526183665938, 0.17477996732396667, 0.16813285456711846, 0.1956366150578722, 0.20681541254237415, 0.17449226540863452, 0.16716672084390913, 0.1726590323305108, 0.1650417787739822, 0.26377377971911087, 0.16847752617340225, 0.16434075387950187, 0.22110942086104185, 0.2467005845912341, 0.1905378210272639, 0.16500323151856297, 0.1735125332869885, 0.18295801633541955, 0.26429761973162913, 0.16249596701171903, 0.1882437309139622, 0.16911277091903792, 0.162350393359491], 'lossList': [0.0, -1.0462124615907669, 0.0, 0.740735643953085, 0.0, 0.0, 0.0], 'rewardMean': 0.7322053503449357, 'totalEpisodes': 230, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.3969090321966, 'successfulTests': 42
#maxSuccessfulTests=42, maxSuccessfulTestsAtStep=25600, timeSpent=70.67
