#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 60
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.5170741040895259, 'errorList': [], 'lossList': [0.0, -1.3924900728464127, 0.0, 34.090847129821775, 0.0, 0.0, 0.0], 'rewardMean': 0.6584365322942445, 'totalEpisodes': 40, 'stepsPerEpisode': 21, 'rewardPerEpisode': 13.95070996609736
'totalSteps': 3840, 'rewardStep': 0.6427079368116719, 'errorList': [], 'lossList': [0.0, -1.3746562039852142, 0.0, 45.42275875091553, 0.0, 0.0, 0.0], 'rewardMean': 0.653193667133387, 'totalEpisodes': 91, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.1315864104434055
'totalSteps': 5120, 'rewardStep': 0.4593624080235167, 'errorList': [], 'lossList': [0.0, -1.3701713573932648, 0.0, 44.58146996498108, 0.0, 0.0, 0.0], 'rewardMean': 0.6047358523559194, 'totalEpisodes': 132, 'stepsPerEpisode': 33, 'rewardPerEpisode': 23.80584788134633
'totalSteps': 6400, 'rewardStep': 0.4659895000647555, 'errorList': [], 'lossList': [0.0, -1.3701929926872254, 0.0, 37.676395092010495, 0.0, 0.0, 0.0], 'rewardMean': 0.5769865818976866, 'totalEpisodes': 146, 'stepsPerEpisode': 108, 'rewardPerEpisode': 67.684653922063
'totalSteps': 7680, 'rewardStep': 0.5711267813478472, 'errorList': [], 'lossList': [0.0, -1.3510325145721436, 0.0, 35.8866567325592, 0.0, 0.0, 0.0], 'rewardMean': 0.5760099484727134, 'totalEpisodes': 153, 'stepsPerEpisode': 192, 'rewardPerEpisode': 144.65104973368858
'totalSteps': 8960, 'rewardStep': 0.8836630615374219, 'errorList': [], 'lossList': [0.0, -1.3364574229717254, 0.0, 17.5560950255394, 0.0, 0.0, 0.0], 'rewardMean': 0.6199603931962432, 'totalEpisodes': 158, 'stepsPerEpisode': 45, 'rewardPerEpisode': 39.31809306908487
'totalSteps': 10240, 'rewardStep': 0.8861981449055514, 'errorList': [], 'lossList': [0.0, -1.338989698290825, 0.0, 28.9904967212677, 0.0, 0.0, 0.0], 'rewardMean': 0.6532401121599067, 'totalEpisodes': 162, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.040651713346515
'totalSteps': 11520, 'rewardStep': 0.8160649110467435, 'errorList': [], 'lossList': [0.0, -1.3316455614566802, 0.0, 35.36481995582581, 0.0, 0.0, 0.0], 'rewardMean': 0.6713317564806665, 'totalEpisodes': 165, 'stepsPerEpisode': 68, 'rewardPerEpisode': 61.86551796529088
'totalSteps': 12800, 'rewardStep': 0.4229767302512845, 'errorList': [], 'lossList': [0.0, -1.320123743414879, 0.0, 24.15709878206253, 0.0, 0.0, 0.0], 'rewardMean': 0.6464962538577282, 'totalEpisodes': 167, 'stepsPerEpisode': 814, 'rewardPerEpisode': 631.2295325372577
'totalSteps': 14080, 'rewardStep': 0.6575415767848545, 'errorList': [], 'lossList': [0.0, -1.3004111647605896, 0.0, 11.401344593167305, 0.0, 0.0, 0.0], 'rewardMean': 0.6322705154863173, 'totalEpisodes': 168, 'stepsPerEpisode': 1047, 'rewardPerEpisode': 841.4816384564917
'totalSteps': 15360, 'rewardStep': 0.8725966462248225, 'errorList': [], 'lossList': [0.0, -1.271972067952156, 0.0, 6.460367234945298, 0.0, 0.0, 0.0], 'rewardMean': 0.667822769699847, 'totalEpisodes': 169, 'stepsPerEpisode': 31, 'rewardPerEpisode': 28.303707201352836
'totalSteps': 16640, 'rewardStep': 0.9180058718311136, 'errorList': [], 'lossList': [0.0, -1.2469404101371766, 0.0, 3.813084155023098, 0.0, 0.0, 0.0], 'rewardMean': 0.6953525632017912, 'totalEpisodes': 169, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.3780796945514
'totalSteps': 17920, 'rewardStep': 0.9418622139114323, 'errorList': [0.1409621405457385, 0.1945775505143988, 0.13374876671105573, 0.1444984107608551, 0.1256094319604512, 0.1297366670252216, 0.14516658297897309, 0.12866719419334421, 0.12546261814235302, 0.14327414520907703, 0.13573438416407008, 0.14229884527975037, 0.1372693705342207, 0.14430523825742927, 0.12637626798426535, 0.14994187918041726, 0.13501300548381143, 0.1364664362722414, 0.14198595824348556, 0.16395791661500686, 0.14049966805447742, 0.139343808213709, 0.1815273821205903, 0.13582073023999722, 0.13944715892419685, 0.14222636221605894, 0.12702182241885696, 0.14004376287042092, 0.14629316861454236, 0.13991039686785386, 0.18540602811559898, 0.14693603787121426, 0.14970510514562949, 0.1385819072482226, 0.13056681976548629, 0.11855326609320178, 0.13635452363626677, 0.14324779952804673, 0.1930524713367488, 0.14160501356092686, 0.14268487095023596, 0.14530295382638198, 0.12315213186924698, 0.14610079218283778, 0.13955171076137848, 0.14372415293330557, 0.16175960854014118, 0.17489569345678224, 0.1267611059556376, 0.16255368172082277], 'lossList': [0.0, -1.2094761961698532, 0.0, 3.119609268307686, 0.0, 0.0, 0.0], 'rewardMean': 0.7436025437905827, 'totalEpisodes': 169, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.3573953380585, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=17920, timeSpent=63.01
