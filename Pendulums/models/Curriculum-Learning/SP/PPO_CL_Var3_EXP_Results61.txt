#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 61
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7078135190481404, 'errorList': [], 'lossList': [0.0, -1.4164519822597503, 0.0, 32.490110654830936, 0.0, 0.0, 0.0], 'rewardMean': 0.6123037103737966, 'totalEpisodes': 45, 'stepsPerEpisode': 39, 'rewardPerEpisode': 27.779619825223225
'totalSteps': 3840, 'rewardStep': 0.5522593167677796, 'errorList': [], 'lossList': [0.0, -1.401089954972267, 0.0, 45.44668203353882, 0.0, 0.0, 0.0], 'rewardMean': 0.5922889125051243, 'totalEpisodes': 104, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.67635914622564
'totalSteps': 5120, 'rewardStep': 0.4822069532858867, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5482561288174292, 'totalEpisodes': 153, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.5632818949915475
'totalSteps': 6400, 'rewardStep': 0.652316058050783, 'errorList': [], 'lossList': [0.0, -1.3857671731710435, 0.0, 46.58245834350586, 0.0, 0.0, 0.0], 'rewardMean': 0.5655994503563216, 'totalEpisodes': 196, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.381647079845637
'totalSteps': 7680, 'rewardStep': 0.4247152503979046, 'errorList': [], 'lossList': [0.0, -1.373132838010788, 0.0, 40.171316413879396, 0.0, 0.0, 0.0], 'rewardMean': 0.5454731360765478, 'totalEpisodes': 214, 'stepsPerEpisode': 171, 'rewardPerEpisode': 121.6194436400241
'totalSteps': 8960, 'rewardStep': 0.72954799979187, 'errorList': [], 'lossList': [0.0, -1.3543363845348357, 0.0, 42.673399152755735, 0.0, 0.0, 0.0], 'rewardMean': 0.568482494040963, 'totalEpisodes': 228, 'stepsPerEpisode': 118, 'rewardPerEpisode': 95.81229652326851
'totalSteps': 10240, 'rewardStep': 0.7542703836744886, 'errorList': [], 'lossList': [0.0, -1.338867462873459, 0.0, 24.57952269554138, 0.0, 0.0, 0.0], 'rewardMean': 0.5891255928891325, 'totalEpisodes': 237, 'stepsPerEpisode': 54, 'rewardPerEpisode': 40.90035818241601
'totalSteps': 11520, 'rewardStep': 0.6302784015271988, 'errorList': [], 'lossList': [0.0, -1.3362274581193925, 0.0, 15.623557916879655, 0.0, 0.0, 0.0], 'rewardMean': 0.5932408737529391, 'totalEpisodes': 244, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.327830958776984
'totalSteps': 12800, 'rewardStep': 0.3696303386003351, 'errorList': [], 'lossList': [0.0, -1.319325481057167, 0.0, 35.69528804779053, 0.0, 0.0, 0.0], 'rewardMean': 0.5785245174430274, 'totalEpisodes': 250, 'stepsPerEpisode': 277, 'rewardPerEpisode': 196.7456395732329
'totalSteps': 14080, 'rewardStep': 0.4918289172269644, 'errorList': [], 'lossList': [0.0, -1.3001389497518538, 0.0, 22.092507326602934, 0.0, 0.0, 0.0], 'rewardMean': 0.5569260572609097, 'totalEpisodes': 254, 'stepsPerEpisode': 232, 'rewardPerEpisode': 168.84688436278012
'totalSteps': 15360, 'rewardStep': 0.9241257169313208, 'errorList': [], 'lossList': [0.0, -1.2853214079141617, 0.0, 7.838941457867622, 0.0, 0.0, 0.0], 'rewardMean': 0.5941126972772639, 'totalEpisodes': 257, 'stepsPerEpisode': 535, 'rewardPerEpisode': 447.2922473171781
'totalSteps': 16640, 'rewardStep': 0.7276585776446584, 'errorList': [], 'lossList': [0.0, -1.2472388064861297, 0.0, 5.044569399356842, 0.0, 0.0, 0.0], 'rewardMean': 0.618657859713141, 'totalEpisodes': 258, 'stepsPerEpisode': 320, 'rewardPerEpisode': 264.8525132700681
'totalSteps': 17920, 'rewardStep': 0.8105855401356411, 'errorList': [], 'lossList': [0.0, -1.2188635385036468, 0.0, 10.13261368393898, 0.0, 0.0, 0.0], 'rewardMean': 0.6514957183981165, 'totalEpisodes': 261, 'stepsPerEpisode': 52, 'rewardPerEpisode': 44.91903900416258
'totalSteps': 19200, 'rewardStep': 0.8785269378192764, 'errorList': [], 'lossList': [0.0, -1.1754712766408921, 0.0, 2.4011076490581034, 0.0, 0.0, 0.0], 'rewardMean': 0.6741168063749657, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1029.135117310092
'totalSteps': 20480, 'rewardStep': 0.841677934042918, 'errorList': [], 'lossList': [0.0, -1.0938052761554717, 0.0, 1.7498343412578106, 0.0, 0.0, 0.0], 'rewardMean': 0.7158130747394671, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1116.0395935316767
'totalSteps': 21760, 'rewardStep': 0.8302230964392433, 'errorList': [], 'lossList': [0.0, -1.0305913132429123, 0.0, 1.5392975187674165, 0.0, 0.0, 0.0], 'rewardMean': 0.7258805844042046, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.905172476091
'totalSteps': 23040, 'rewardStep': 0.9748034763849688, 'errorList': [0.08020934236153045, 0.13764123288392338, 0.02189771825733424, 0.017841623876945583, 0.07677440078130307, 0.05295071388789536, 0.07254831323335437, 0.07035707855421906, 0.028634806836715536, 0.0358556968969499, 0.06972741708296798, 0.032069425394120894, 0.032478211786645936, 0.06288804445012398, 0.03747125325301121, 0.013711429909774956, 0.07280925372371405, 0.0959150191638216, 0.03806954044934912, 0.06426808653466419, 0.08479958784920211, 0.025586220301086572, 0.014468777683882359, 0.04245865654657937, 0.09123723072433212, 0.05964314464049801, 0.04564977657941319, 0.023872802474486147, 0.07720788507439255, 0.017287873201170298, 0.09760507546231917, 0.09478023288048228, 0.0928932901879626, 0.0806886980438999, 0.10920785963283929, 0.0667186628850347, 0.07691866131338757, 0.03232514262527691, 0.007817123639857664, 0.06422857267274416, 0.05853798810698258, 0.08836501806283945, 0.10139294646306048, 0.0665009809562325, 0.059811959802506455, 0.014547638748338022, 0.046832266062677326, 0.06447255166209695, 0.042904029436105294, 0.02328493352495064], 'lossList': [0.0, -0.9946423187851906, 0.0, 1.2475259956344962, 0.0, 0.0, 0.0], 'rewardMean': 0.7479338936752524, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.5402545200159, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=72.56
