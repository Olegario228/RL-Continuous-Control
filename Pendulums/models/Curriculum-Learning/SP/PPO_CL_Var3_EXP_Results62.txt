#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 62
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.7978266001930225, 'errorList': [], 'lossList': [0.0, -1.4357883965969085, 0.0, 29.126384649276734, 0.0, 0.0, 0.0], 'rewardMean': 0.6179285961994483, 'totalEpisodes': 47, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.67969768992093
'totalSteps': 3840, 'rewardStep': 0.9454828460726646, 'errorList': [], 'lossList': [0.0, -1.422179057598114, 0.0, 41.44708109855652, 0.0, 0.0, 0.0], 'rewardMean': 0.727113346157187, 'totalEpisodes': 100, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9454828460726646
'totalSteps': 5120, 'rewardStep': 0.9667041785653614, 'errorList': [], 'lossList': [0.0, -1.4045304822921754, 0.0, 42.3613906955719, 0.0, 0.0, 0.0], 'rewardMean': 0.7870110542592306, 'totalEpisodes': 151, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.482534206511515
'totalSteps': 6400, 'rewardStep': 0.5560585613505321, 'errorList': [], 'lossList': [0.0, -1.3902455699443816, 0.0, 47.528762340545654, 0.0, 0.0, 0.0], 'rewardMean': 0.7408205556774908, 'totalEpisodes': 168, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.666666423265283
'totalSteps': 7680, 'rewardStep': 0.8428109788233986, 'errorList': [], 'lossList': [0.0, -1.373098937869072, 0.0, 49.46338775634766, 0.0, 0.0, 0.0], 'rewardMean': 0.7578189595351422, 'totalEpisodes': 186, 'stepsPerEpisode': 58, 'rewardPerEpisode': 44.02765327330009
'totalSteps': 8960, 'rewardStep': 0.5251410183686109, 'errorList': [], 'lossList': [0.0, -1.37282277405262, 0.0, 30.93483186721802, 0.0, 0.0, 0.0], 'rewardMean': 0.7245792536542092, 'totalEpisodes': 193, 'stepsPerEpisode': 135, 'rewardPerEpisode': 106.26652177774598
'totalSteps': 10240, 'rewardStep': 0.320441133029553, 'errorList': [], 'lossList': [0.0, -1.371404643058777, 0.0, 12.250835382938385, 0.0, 0.0, 0.0], 'rewardMean': 0.6740619885761272, 'totalEpisodes': 197, 'stepsPerEpisode': 494, 'rewardPerEpisode': 356.7666070027607
'totalSteps': 11520, 'rewardStep': 0.795321675442152, 'errorList': [], 'lossList': [0.0, -1.3599114000797272, 0.0, 32.572950985431675, 0.0, 0.0, 0.0], 'rewardMean': 0.6875352871167967, 'totalEpisodes': 204, 'stepsPerEpisode': 215, 'rewardPerEpisode': 176.30709042549972
'totalSteps': 12800, 'rewardStep': 0.8906018276684062, 'errorList': [], 'lossList': [0.0, -1.3513487082719804, 0.0, 27.71264306306839, 0.0, 0.0, 0.0], 'rewardMean': 0.7078419411719576, 'totalEpisodes': 208, 'stepsPerEpisode': 339, 'rewardPerEpisode': 274.40884950233465
'totalSteps': 14080, 'rewardStep': 0.816740118404117, 'errorList': [], 'lossList': [0.0, -1.3366389697790146, 0.0, 7.606482962667942, 0.0, 0.0, 0.0], 'rewardMean': 0.7457128937917819, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.0148488300579
'totalSteps': 15360, 'rewardStep': 0.8166323781555029, 'errorList': [], 'lossList': [0.0, -1.303215324282646, 0.0, 2.8436534766852857, 0.0, 0.0, 0.0], 'rewardMean': 0.7475934715880299, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 916.3180577854149
'totalSteps': 16640, 'rewardStep': 0.8672848848038714, 'errorList': [], 'lossList': [0.0, -1.2783752107620239, 0.0, 2.9909211088716985, 0.0, 0.0, 0.0], 'rewardMean': 0.7397736754611506, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.4553860204712
'totalSteps': 17920, 'rewardStep': 0.8065228855012071, 'errorList': [], 'lossList': [0.0, -1.249991996884346, 0.0, 2.708764655813575, 0.0, 0.0, 0.0], 'rewardMean': 0.7237555461547351, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.5727822408655
'totalSteps': 19200, 'rewardStep': 0.9057106540592688, 'errorList': [], 'lossList': [0.0, -1.2173039281368256, 0.0, 1.2313707377389074, 0.0, 0.0, 0.0], 'rewardMean': 0.7587207554256088, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.3401413973113
'totalSteps': 20480, 'rewardStep': 0.9377311881095796, 'errorList': [0.1469955300045094, 0.08718483844407206, 0.31024804693737956, 0.11376586752336988, 0.2632664850148343, 0.2235804741408668, 0.2168019480019984, 0.2569153672419908, 0.13996885710194684, 0.1539102470881344, 0.17085709372523114, 0.18649016114561354, 0.19920969087285742, 0.1907485051327391, 0.259929069112216, 0.1069293806300044, 0.21936654736020073, 0.10713058383259554, 0.14943034579367906, 0.1260107784904541, 0.11494996721476687, 0.2127777870189868, 0.2508883277843572, 0.1601906808568836, 0.08889636843403656, 0.1603089977904372, 0.22095417606904466, 0.20121100445989626, 0.1666666160897797, 0.2735846101959978, 0.18491711471039524, 0.18072184106429132, 0.1630510879020612, 0.15372023898626938, 0.14878820359219566, 0.14421934143503687, 0.21198201092592442, 0.1588171148422218, 0.1915063020410745, 0.157329451252439, 0.1483106034689126, 0.21326440578626862, 0.20725431270174574, 0.27438783787376475, 0.1403304363106852, 0.24625990151083899, 0.11682087934845102, 0.23518177954363745, 0.1231624922212005, 0.11667265534420578], 'lossList': [0.0, -1.1755680131912232, 0.0, 1.1605064569413661, 0.0, 0.0, 0.0], 'rewardMean': 0.7682127763542269, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.1900644527245, 'successfulTests': 32
'totalSteps': 21760, 'rewardStep': 0.9179062401753997, 'errorList': [], 'lossList': [0.0, -1.1366754412651061, 0.0, 0.9721438026428223, 0.0, 0.0, 0.0], 'rewardMean': 0.8074892985349058, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.9988080753983
'totalSteps': 23040, 'rewardStep': 0.9775599919529533, 'errorList': [0.20892198359893166, 0.11691278500896639, 0.03797065554639894, 0.20163014434124668, 0.13269421922842692, 0.18091062871894123, 0.08248730450258628, 0.14439851152473493, 0.12787806035346963, 0.08450980461213317, 0.04265870999769073, 0.022484329115059078, 0.07770719647275717, 0.11386337040860924, 0.08636251509756049, 0.060157161405828725, 0.14502047402369994, 0.03812297582943001, 0.1242682745469172, 0.04687894230621549, 0.09852247470374609, 0.1281632563966217, 0.06502780208076808, 0.16073423221653677, 0.06383871566698072, 0.1579655764867514, 0.06323038906492115, 0.07568812377706806, 0.10865105873027948, 0.11906252079090247, 0.12210130032922194, 0.08750471391684982, 0.08193694947171677, 0.04465424448015059, 0.059261377352246236, 0.0941417552171043, 0.17433807658703215, 0.1020612644490351, 0.09654652591655591, 0.14743450121690768, 0.06386469527739884, 0.07579066523841806, 0.09759043238129564, 0.05844333541868422, 0.08620521026439101, 0.15401580422446046, 0.08264996771589504, 0.043044796846344095, 0.04373964443889714, 0.11268404801123375], 'lossList': [0.0, -1.1066558074951172, 0.0, 0.8611947564780712, 0.0, 0.0, 0.0], 'rewardMean': 0.8732011844272458, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.899893441922, 'successfulTests': 48
'totalSteps': 24320, 'rewardStep': 0.9226070760777341, 'errorList': [], 'lossList': [0.0, -1.0736007684469222, 0.0, 0.6562789681181311, 0.0, 0.0, 0.0], 'rewardMean': 0.8859297244908039, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.9164290000701
'totalSteps': 25600, 'rewardStep': 0.9108465009473937, 'errorList': [], 'lossList': [0.0, -1.0465097576379776, 0.0, 0.4498889276292175, 0.0, 0.0, 0.0], 'rewardMean': 0.8879541918187026, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1207.8084259292464
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=23040, timeSpent=103.07
