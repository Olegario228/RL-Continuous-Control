#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 63
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.4572724812874049, 'errorList': [], 'lossList': [0.0, -1.4263487428426742, 0.0, 33.78507239341736, 0.0, 0.0, 0.0], 'rewardMean': 0.6112991571172305, 'totalEpisodes': 56, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.7293178422761
'totalSteps': 3840, 'rewardStep': 0.5978035720450192, 'errorList': [], 'lossList': [0.0, -1.4285183203220368, 0.0, 45.77487323760986, 0.0, 0.0, 0.0], 'rewardMean': 0.6068006287598268, 'totalEpisodes': 109, 'stepsPerEpisode': 19, 'rewardPerEpisode': 11.565202831585854
'totalSteps': 5120, 'rewardStep': 0.5504350982342842, 'errorList': [], 'lossList': [0.0, -1.4302875685691834, 0.0, 56.58948165893555, 0.0, 0.0, 0.0], 'rewardMean': 0.5927092461284411, 'totalEpisodes': 152, 'stepsPerEpisode': 60, 'rewardPerEpisode': 48.18062332032843
'totalSteps': 6400, 'rewardStep': 0.9829020576337462, 'errorList': [], 'lossList': [0.0, -1.4214339900016784, 0.0, 51.35945359230041, 0.0, 0.0, 0.0], 'rewardMean': 0.6707478084295021, 'totalEpisodes': 174, 'stepsPerEpisode': 18, 'rewardPerEpisode': 12.59286981722448
'totalSteps': 7680, 'rewardStep': 0.889834313174607, 'errorList': [], 'lossList': [0.0, -1.4134771925210954, 0.0, 43.009040451049806, 0.0, 0.0, 0.0], 'rewardMean': 0.7072622258870195, 'totalEpisodes': 187, 'stepsPerEpisode': 80, 'rewardPerEpisode': 61.79718860394051
'totalSteps': 8960, 'rewardStep': 0.7682666535929599, 'errorList': [], 'lossList': [0.0, -1.4049225497245788, 0.0, 28.72168949842453, 0.0, 0.0, 0.0], 'rewardMean': 0.7159771441307253, 'totalEpisodes': 193, 'stepsPerEpisode': 96, 'rewardPerEpisode': 73.92874902517565
'totalSteps': 10240, 'rewardStep': 0.8340534324725486, 'errorList': [], 'lossList': [0.0, -1.3972861224412918, 0.0, 19.784993667006493, 0.0, 0.0, 0.0], 'rewardMean': 0.7307366801734533, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 989.2930774353973
'totalSteps': 11520, 'rewardStep': 0.6441449393711031, 'errorList': [], 'lossList': [0.0, -1.376490620970726, 0.0, 12.894962853491306, 0.0, 0.0, 0.0], 'rewardMean': 0.7211153756398588, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 983.9226661758923
'totalSteps': 12800, 'rewardStep': 0.5326296454880727, 'errorList': [], 'lossList': [0.0, -1.3417393457889557, 0.0, 8.81032076716423, 0.0, 0.0, 0.0], 'rewardMean': 0.7022668026246802, 'totalEpisodes': 195, 'stepsPerEpisode': 169, 'rewardPerEpisode': 134.1022210500268
'totalSteps': 14080, 'rewardStep': 0.8449569250149972, 'errorList': [], 'lossList': [0.0, -1.3429019045829773, 0.0, 3.6071051801741123, 0.0, 0.0, 0.0], 'rewardMean': 0.7102299118314743, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 863.0437254419645
'totalSteps': 15360, 'rewardStep': 0.5878606331552454, 'errorList': [], 'lossList': [0.0, -1.3433751809597014, 0.0, 17.77721860229969, 0.0, 0.0, 0.0], 'rewardMean': 0.7232887270182584, 'totalEpisodes': 196, 'stepsPerEpisode': 412, 'rewardPerEpisode': 315.96861667126973
'totalSteps': 16640, 'rewardStep': 0.8827839305940862, 'errorList': [], 'lossList': [0.0, -1.3496855479478835, 0.0, 2.683169845044613, 0.0, 0.0, 0.0], 'rewardMean': 0.751786762873165, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1011.8289598043848
'totalSteps': 17920, 'rewardStep': 0.673762058070183, 'errorList': [], 'lossList': [0.0, -1.3544705951213836, 0.0, 11.002721060216427, 0.0, 0.0, 0.0], 'rewardMean': 0.764119458856755, 'totalEpisodes': 197, 'stepsPerEpisode': 364, 'rewardPerEpisode': 307.6163388226114
'totalSteps': 19200, 'rewardStep': 0.7875598512778661, 'errorList': [], 'lossList': [0.0, -1.3556113564968109, 0.0, 1.6939904017746448, 0.0, 0.0, 0.0], 'rewardMean': 0.7445852382211668, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 982.1001322087077
'totalSteps': 20480, 'rewardStep': 0.9111111058704343, 'errorList': [], 'lossList': [0.0, -1.3161598658561706, 0.0, 1.2239611310139298, 0.0, 0.0, 0.0], 'rewardMean': 0.7467129174907496, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.5635491050323
'totalSteps': 21760, 'rewardStep': 0.8881658547889723, 'errorList': [], 'lossList': [0.0, -1.2635467678308487, 0.0, 0.6590711005404591, 0.0, 0.0, 0.0], 'rewardMean': 0.7587028376103508, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.8084822630037
'totalSteps': 23040, 'rewardStep': 0.8482614366989742, 'errorList': [], 'lossList': [0.0, -1.2286063921451569, 0.0, 0.667091000508517, 0.0, 0.0, 0.0], 'rewardMean': 0.7601236380329933, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.5863896018677
'totalSteps': 24320, 'rewardStep': 0.9257446091154593, 'errorList': [], 'lossList': [0.0, -1.180317189693451, 0.0, 0.6189722645655275, 0.0, 0.0, 0.0], 'rewardMean': 0.7882836050074291, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1185.1833152696033
'totalSteps': 25600, 'rewardStep': 0.9510227330965098, 'errorList': [0.07329994428594919, 0.10103017096086023, 0.05460076104789712, 0.05419136799128424, 0.0663462660297981, 0.10866332123273771, 0.07046494075130767, 0.06035367998366205, 0.05516837384883879, 0.06431259557592427, 0.08458435217046317, 0.08543055912276794, 0.10548823453946314, 0.11998631397811618, 0.08869653246950604, 0.06942741403346298, 0.0568917920790388, 0.0731949087457156, 0.11094957524296067, 0.09748934895209402, 0.10552461901610685, 0.07583427247559274, 0.08863003276117258, 0.053761349386089896, 0.10915617825010661, 0.09229077862873274, 0.0861236112718989, 0.10803317754138633, 0.05115219244424078, 0.07107511174860602, 0.05667933586983642, 0.10225562797015883, 0.07984535851304966, 0.05097627463294782, 0.05770580227401142, 0.07080648326649326, 0.11304211572858688, 0.054651789624968576, 0.08360175136532526, 0.059512853158429156, 0.08139134869181791, 0.07614755557396675, 0.09720977364404795, 0.12663284418814436, 0.10219744211011031, 0.0776497388498384, 0.07462568332288375, 0.05630912099933616, 0.08211342760768245, 0.09844652964285228], 'lossList': [0.0, -1.1469559109210967, 0.0, 0.47615284062922003, 0.0, 0.0, 0.0], 'rewardMean': 0.8301229137682729, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1210.728971984413, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.19
