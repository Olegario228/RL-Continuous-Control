#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 65
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.6495944543901311, 'errorList': [], 'lossList': [0.0, -1.4252878773212432, 0.0, 31.748319387435913, 0.0, 0.0, 0.0], 'rewardMean': 0.7725880227991491, 'totalEpisodes': 40, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.08333068940843
'totalSteps': 3840, 'rewardStep': 0.950616716119862, 'errorList': [], 'lossList': [0.0, -1.4006886476278304, 0.0, 40.4208588886261, 0.0, 0.0, 0.0], 'rewardMean': 0.8319309205727201, 'totalEpisodes': 102, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.635919380088887
'totalSteps': 5120, 'rewardStep': 0.9308833471125031, 'errorList': [], 'lossList': [0.0, -1.3651414513587952, 0.0, 44.25161343574524, 0.0, 0.0, 0.0], 'rewardMean': 0.8566690272076658, 'totalEpisodes': 159, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.690831531370048
'totalSteps': 6400, 'rewardStep': 0.6137068329044554, 'errorList': [], 'lossList': [0.0, -1.349018878340721, 0.0, 44.843581447601316, 0.0, 0.0, 0.0], 'rewardMean': 0.8080765883470237, 'totalEpisodes': 180, 'stepsPerEpisode': 141, 'rewardPerEpisode': 92.16804987679373
'totalSteps': 7680, 'rewardStep': 0.8068497008715322, 'errorList': [], 'lossList': [0.0, -1.3414081782102585, 0.0, 32.062220540046695, 0.0, 0.0, 0.0], 'rewardMean': 0.8078721071011085, 'totalEpisodes': 187, 'stepsPerEpisode': 47, 'rewardPerEpisode': 36.44134901723668
'totalSteps': 8960, 'rewardStep': 0.423513112130632, 'errorList': [], 'lossList': [0.0, -1.3198858946561813, 0.0, 30.8215318775177, 0.0, 0.0, 0.0], 'rewardMean': 0.7529636792481832, 'totalEpisodes': 195, 'stepsPerEpisode': 156, 'rewardPerEpisode': 103.73510037075931
'totalSteps': 10240, 'rewardStep': 0.588810462151974, 'errorList': [], 'lossList': [0.0, -1.3232657605409621, 0.0, 24.412701358795164, 0.0, 0.0, 0.0], 'rewardMean': 0.732444527111157, 'totalEpisodes': 201, 'stepsPerEpisode': 409, 'rewardPerEpisode': 275.7014512021683
'totalSteps': 11520, 'rewardStep': 0.777891227985581, 'errorList': [], 'lossList': [0.0, -1.3082819449901582, 0.0, 60.80877212524414, 0.0, 0.0, 0.0], 'rewardMean': 0.7374941605416486, 'totalEpisodes': 209, 'stepsPerEpisode': 67, 'rewardPerEpisode': 58.20152133200422
'totalSteps': 12800, 'rewardStep': 0.8940534650682576, 'errorList': [], 'lossList': [0.0, -1.2989298570156098, 0.0, 18.27152712106705, 0.0, 0.0, 0.0], 'rewardMean': 0.7531500909943094, 'totalEpisodes': 212, 'stepsPerEpisode': 60, 'rewardPerEpisode': 53.696480998449196
'totalSteps': 14080, 'rewardStep': 0.5401992400597759, 'errorList': [], 'lossList': [0.0, -1.292217777967453, 0.0, 13.513843301534653, 0.0, 0.0, 0.0], 'rewardMean': 0.7176118558794704, 'totalEpisodes': 216, 'stepsPerEpisode': 140, 'rewardPerEpisode': 113.94746447019028
'totalSteps': 15360, 'rewardStep': 0.4576010042115408, 'errorList': [], 'lossList': [0.0, -1.2917845225334168, 0.0, 8.567171111106873, 0.0, 0.0, 0.0], 'rewardMean': 0.6984125108616114, 'totalEpisodes': 221, 'stepsPerEpisode': 305, 'rewardPerEpisode': 241.2998025759793
'totalSteps': 16640, 'rewardStep': 0.7489681714980075, 'errorList': [], 'lossList': [0.0, -1.3132091826200485, 0.0, 5.2599666184186935, 0.0, 0.0, 0.0], 'rewardMean': 0.678247656399426, 'totalEpisodes': 225, 'stepsPerEpisode': 211, 'rewardPerEpisode': 166.42783610091922
'totalSteps': 17920, 'rewardStep': 0.8710503879237589, 'errorList': [], 'lossList': [0.0, -1.3244953954219818, 0.0, 4.957377113699913, 0.0, 0.0, 0.0], 'rewardMean': 0.6722643604805516, 'totalEpisodes': 228, 'stepsPerEpisode': 224, 'rewardPerEpisode': 192.86611233172093
'totalSteps': 19200, 'rewardStep': 0.7497804958825263, 'errorList': [], 'lossList': [0.0, -1.310129671692848, 0.0, 2.554709108173847, 0.0, 0.0, 0.0], 'rewardMean': 0.6858717267783587, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.8250524630869
'totalSteps': 20480, 'rewardStep': 0.68608143945165, 'errorList': [], 'lossList': [0.0, -1.2894231110811234, 0.0, 2.9131098222732543, 0.0, 0.0, 0.0], 'rewardMean': 0.6737949006363705, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 954.9167836934741
'totalSteps': 21760, 'rewardStep': 0.8632804936179709, 'errorList': [], 'lossList': [0.0, -1.2710842382907868, 0.0, 1.1151321110129357, 0.0, 0.0, 0.0], 'rewardMean': 0.7177716387851043, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.6814658196456
'totalSteps': 23040, 'rewardStep': 0.8282533596223033, 'errorList': [], 'lossList': [0.0, -1.2357945400476456, 0.0, 0.923761162981391, 0.0, 0.0, 0.0], 'rewardMean': 0.7417159285321373, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.9142535064393
'totalSteps': 24320, 'rewardStep': 0.7889136541075608, 'errorList': [], 'lossList': [0.0, -1.1870170211791993, 0.0, 0.6638448989018798, 0.0, 0.0, 0.0], 'rewardMean': 0.7428181711443352, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.3791622583412
'totalSteps': 25600, 'rewardStep': 0.9379164674142716, 'errorList': [0.08688494990259361, 0.0386650651801775, 0.06491905771894713, 0.021749966604327618, 0.04554813617010761, 0.07268647443004887, 0.034263295095011595, 0.05366914211031149, 0.023310291304653324, 0.02505995244411185, 0.06723526388877778, 0.04813408221516373, 0.07059051423980907, 0.0245207599195217, 0.022027054893435153, 0.027708326690013302, 0.061899604325282506, 0.026765366502367653, 0.05137207678142003, 0.04794571687961676, 0.041460468831175386, 0.0317357333104525, 0.03640842057657012, 0.02188080268190735, 0.09939746611558423, 0.033461308535214056, 0.0524931669292766, 0.06519499078722198, 0.052187100690617015, 0.03864599011093265, 0.027315188452789452, 0.04647421282186017, 0.03556053630628017, 0.055406443982998946, 0.04917994472491196, 0.02148045441086439, 0.06522285329542615, 0.02663164294678922, 0.04531532200313279, 0.03233389872018004, 0.026069958167197087, 0.021437669697506784, 0.026464339908565577, 0.023481015658110106, 0.040915058961966976, 0.0414697664502365, 0.03579215906642831, 0.030948557914432386, 0.02654285111283712, 0.04683466441081602], 'lossList': [0.0, -1.135504629611969, 0.0, 0.6106713078543544, 0.0, 0.0, 0.0], 'rewardMean': 0.7472044713789365, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1193.6624292849106, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.9
