#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 66
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.5432248959753556, 'errorList': [], 'lossList': [0.0, -1.410274434685707, 0.0, 33.13806102752685, 0.0, 0.0, 0.0], 'rewardMean': 0.5509363072258553, 'totalEpisodes': 38, 'stepsPerEpisode': 33, 'rewardPerEpisode': 23.713865271687833
'totalSteps': 3840, 'rewardStep': 0.9367375289226597, 'errorList': [], 'lossList': [0.0, -1.3790841460227967, 0.0, 44.24176128387451, 0.0, 0.0, 0.0], 'rewardMean': 0.6795367144581235, 'totalEpisodes': 93, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.301068154091762
'totalSteps': 5120, 'rewardStep': 0.8317661374221642, 'errorList': [], 'lossList': [0.0, -1.3460815262794494, 0.0, 43.024815006256105, 0.0, 0.0, 0.0], 'rewardMean': 0.7175940701991337, 'totalEpisodes': 145, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.831109726950599
'totalSteps': 6400, 'rewardStep': 0.5341938100117989, 'errorList': [], 'lossList': [0.0, -1.3277093482017517, 0.0, 46.2642484664917, 0.0, 0.0, 0.0], 'rewardMean': 0.6809140181616667, 'totalEpisodes': 175, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.270468511379687
'totalSteps': 7680, 'rewardStep': 0.7859123491371112, 'errorList': [], 'lossList': [0.0, -1.3110853385925294, 0.0, 30.989646985530854, 0.0, 0.0, 0.0], 'rewardMean': 0.6984137399909075, 'totalEpisodes': 182, 'stepsPerEpisode': 100, 'rewardPerEpisode': 78.60429207760971
'totalSteps': 8960, 'rewardStep': 0.6396026233120369, 'errorList': [], 'lossList': [0.0, -1.2880462938547135, 0.0, 33.959352707862855, 0.0, 0.0, 0.0], 'rewardMean': 0.690012151893926, 'totalEpisodes': 189, 'stepsPerEpisode': 329, 'rewardPerEpisode': 254.27905398302332
'totalSteps': 10240, 'rewardStep': 0.8130860442367989, 'errorList': [], 'lossList': [0.0, -1.2754442167282105, 0.0, 14.198675589561462, 0.0, 0.0, 0.0], 'rewardMean': 0.7053963884367851, 'totalEpisodes': 193, 'stepsPerEpisode': 57, 'rewardPerEpisode': 50.188334114426965
'totalSteps': 11520, 'rewardStep': 0.7237889155387666, 'errorList': [], 'lossList': [0.0, -1.2728898429870605, 0.0, 12.852430469691754, 0.0, 0.0, 0.0], 'rewardMean': 0.7074400025592275, 'totalEpisodes': 195, 'stepsPerEpisode': 41, 'rewardPerEpisode': 34.05711585262927
'totalSteps': 12800, 'rewardStep': 0.807383221647983, 'errorList': [], 'lossList': [0.0, -1.2639487606287003, 0.0, 9.690827689766884, 0.0, 0.0, 0.0], 'rewardMean': 0.7174343244681031, 'totalEpisodes': 198, 'stepsPerEpisode': 86, 'rewardPerEpisode': 72.79921569223231
'totalSteps': 14080, 'rewardStep': 0.7926817203988622, 'errorList': [], 'lossList': [0.0, -1.2574423140287398, 0.0, 5.433542323112488, 0.0, 0.0, 0.0], 'rewardMean': 0.7408377246603537, 'totalEpisodes': 201, 'stepsPerEpisode': 94, 'rewardPerEpisode': 83.14318417822167
'totalSteps': 15360, 'rewardStep': 0.8159738584785639, 'errorList': [], 'lossList': [0.0, -1.255437713265419, 0.0, 4.063872238993644, 0.0, 0.0, 0.0], 'rewardMean': 0.7681126209106746, 'totalEpisodes': 203, 'stepsPerEpisode': 77, 'rewardPerEpisode': 62.603546498444416
'totalSteps': 16640, 'rewardStep': 0.8936222523391836, 'errorList': [], 'lossList': [0.0, -1.24094778239727, 0.0, 3.338085905611515, 0.0, 0.0, 0.0], 'rewardMean': 0.7638010932523269, 'totalEpisodes': 204, 'stepsPerEpisode': 625, 'rewardPerEpisode': 516.5590960534947
'totalSteps': 17920, 'rewardStep': 0.7628874313250575, 'errorList': [], 'lossList': [0.0, -1.2006566101312637, 0.0, 2.9965088314563038, 0.0, 0.0, 0.0], 'rewardMean': 0.7569132226426163, 'totalEpisodes': 204, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.3998301575336
'totalSteps': 19200, 'rewardStep': 0.9287280941882463, 'errorList': [], 'lossList': [0.0, -1.1617227619886399, 0.0, 1.9420768073946237, 0.0, 0.0, 0.0], 'rewardMean': 0.7963666510602609, 'totalEpisodes': 204, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.7300669352398
'totalSteps': 20480, 'rewardStep': 0.855954215575914, 'errorList': [], 'lossList': [0.0, -1.1158237433433533, 0.0, 1.4986485803872347, 0.0, 0.0, 0.0], 'rewardMean': 0.8033708377041412, 'totalEpisodes': 204, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.8205282027977
'totalSteps': 21760, 'rewardStep': 0.8818917486017857, 'errorList': [], 'lossList': [0.0, -1.048949971795082, 0.0, 1.0347552253492176, 0.0, 0.0, 0.0], 'rewardMean': 0.8275997502331162, 'totalEpisodes': 204, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.9675939678832
'totalSteps': 23040, 'rewardStep': 0.9485772689991266, 'errorList': [0.15126263533432083, 0.1465827051878915, 0.15578847471587876, 0.159852591055214, 0.15827882187439646, 0.1642156180797881, 0.1476534968326215, 0.16041105114077037, 0.15510605652540826, 0.15868893860397446, 0.16471677226730835, 0.192036154028933, 0.15073682587454826, 0.17053592971251108, 0.20161180173177848, 0.13939355779697424, 0.15706718098426906, 0.13606207227331749, 0.16828300086525524, 0.16836657341938674, 0.18448921911040095, 0.15439617325542954, 0.24538284380796443, 0.16615862332336842, 0.15493295064271165, 0.1660621903639786, 0.1772920196899091, 0.14285130213621156, 0.16405849339177228, 0.1654947701846643, 0.1685088862036052, 0.1674660796688185, 0.1502066495040537, 0.22022888366606816, 0.16710961927815732, 0.16511120057090103, 0.1614054511762588, 0.15167951111374997, 0.1940294826788702, 0.1896571713302542, 0.1398026107482578, 0.15382954161352036, 0.17772406602985505, 0.18037558710408003, 0.1600450107978058, 0.1389373093638073, 0.1589079563806271, 0.14542256044991025, 0.14540766543811093, 0.15398729285372356], 'lossList': [0.0, -1.0133036011457444, 0.0, 0.45001467117108407, 0.0, 0.0, 0.0], 'rewardMean': 0.841148872709349, 'totalEpisodes': 204, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1157.8631785298112, 'successfulTests': 47
'totalSteps': 24320, 'rewardStep': 0.9041040964334752, 'errorList': [], 'lossList': [0.0, -0.9816766059398652, 0.0, 0.3079744485672563, 0.0, 0.0, 0.0], 'rewardMean': 0.8591803907988197, 'totalEpisodes': 204, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.4684707652757
'totalSteps': 25600, 'rewardStep': 0.9714941140204022, 'errorList': [0.12641600058344896, 0.1256374019695324, 0.21514244552704545, 0.191355432232652, 0.07099128218186801, 0.060286457282595546, 0.09967831739530116, 0.12080015333281192, 0.031034180050901725, 0.15349296907373974, 0.03835983473170721, 0.08029997154940341, 0.036030649026423056, 0.024916193040721717, 0.1480497547302499, 0.07477379326519913, 0.027594990869206495, 0.18103443169797598, 0.14050860324269251, 0.08547584186813752, 0.15748263216658112, 0.02747962429315162, 0.027591112723019413, 0.19554696634661703, 0.03645105921732814, 0.07015160858800508, 0.16125183472086668, 0.11651746850597991, 0.03934368042388036, 0.021863461917969305, 0.027370982132059053, 0.022377847650297506, 0.13752815483258218, 0.1451092694978787, 0.026033153256056703, 0.061778807446199686, 0.0186339705110846, 0.13734736447788723, 0.02444532301417406, 0.15721925256789945, 0.07184310362212726, 0.06508073132848007, 0.0528333022878259, 0.07898739625471199, 0.05711948669957116, 0.03725621954784742, 0.022233438611870947, 0.15692826414345773, 0.07927879989613967, 0.13851606978285863], 'lossList': [0.0, -0.9331100025773048, 0.0, 0.38945253723300993, 0.0, 0.0, 0.0], 'rewardMean': 0.8755914800360616, 'totalEpisodes': 204, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1216.3418324011095, 'successfulTests': 49
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=25600, timeSpent=102.76
