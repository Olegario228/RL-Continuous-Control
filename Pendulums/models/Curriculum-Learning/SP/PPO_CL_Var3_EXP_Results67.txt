#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 67
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.882189953260028, 'errorList': [], 'lossList': [0.0, -1.4186855059862138, 0.0, 34.06128373146057, 0.0, 0.0, 0.0], 'rewardMean': 0.6799878642414131, 'totalEpisodes': 46, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.442261382374312
'totalSteps': 3840, 'rewardStep': 0.6027548793190614, 'errorList': [], 'lossList': [0.0, -1.387686047554016, 0.0, 46.14287220954895, 0.0, 0.0, 0.0], 'rewardMean': 0.6542435359339626, 'totalEpisodes': 103, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.50215711743669
'totalSteps': 5120, 'rewardStep': 0.7171336244127022, 'errorList': [], 'lossList': [0.0, -1.3622875964641572, 0.0, 46.60943950653076, 0.0, 0.0, 0.0], 'rewardMean': 0.6699660580536475, 'totalEpisodes': 153, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.849941362096267
'totalSteps': 6400, 'rewardStep': 0.6256140730373646, 'errorList': [], 'lossList': [0.0, -1.3489997738599777, 0.0, 48.90054200172424, 0.0, 0.0, 0.0], 'rewardMean': 0.661095661050391, 'totalEpisodes': 179, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.890087375692676
'totalSteps': 7680, 'rewardStep': 0.8935914498049872, 'errorList': [], 'lossList': [0.0, -1.3334001755714417, 0.0, 45.30979224205017, 0.0, 0.0, 0.0], 'rewardMean': 0.699844959176157, 'totalEpisodes': 192, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.189839672035676
'totalSteps': 8960, 'rewardStep': 0.4314190676681337, 'errorList': [], 'lossList': [0.0, -1.3194010424613953, 0.0, 40.32900330543518, 0.0, 0.0, 0.0], 'rewardMean': 0.6614984032464394, 'totalEpisodes': 200, 'stepsPerEpisode': 135, 'rewardPerEpisode': 96.19383520488981
'totalSteps': 10240, 'rewardStep': 0.6452868943194586, 'errorList': [], 'lossList': [0.0, -1.3184450602531432, 0.0, 18.064285545349122, 0.0, 0.0, 0.0], 'rewardMean': 0.6594719646305668, 'totalEpisodes': 205, 'stepsPerEpisode': 193, 'rewardPerEpisode': 164.9606140623309
'totalSteps': 11520, 'rewardStep': 0.3150629221546089, 'errorList': [], 'lossList': [0.0, -1.3434134697914124, 0.0, 9.57532745718956, 0.0, 0.0, 0.0], 'rewardMean': 0.6212042932443492, 'totalEpisodes': 210, 'stepsPerEpisode': 137, 'rewardPerEpisode': 85.96599791103165
'totalSteps': 12800, 'rewardStep': 0.9837322101827455, 'errorList': [1.038398376278106, 1.1938658607326202, 1.675904773933414, 0.2953271020753695, 4.960533521633776, 0.6514951110550036, 2.1101552044243004, 0.36639707714243996, 0.8722058578954385, 0.15910217663488246, 6.222927234359092, 8.841264603279505, 1.9595894617129839, 0.9383908559591166, 2.8936691644873282, 5.577815883218543, 6.509464767159693, 1.8703582875928673, 8.268656851951137, 6.579620153653397, 4.21730236055435, 3.575516932000332, 3.0804627440982055, 2.49424540792455, 2.559596320811151, 2.2971246222570367, 0.9324304738175941, 0.8287267966362214, 1.9652743822113885, 6.833869689783577, 6.934097568289206, 2.2054093793375698, 3.02030275423435, 0.30436523503923324, 7.649543738357901, 2.2348554908442915, 5.0566665536516515, 0.23508291202516926, 1.9757012206582854, 0.21003105971906297, 6.901889634321725, 3.333937770870272, 1.0533075687719413, 1.80502938256696, 1.1652491048477343, 0.316080807565961, 0.36655827680302405, 2.3614990162620804, 1.4392007334303856, 0.4793329753461718], 'lossList': [0.0, -1.3577862507104874, 0.0, 23.261816787719727, 0.0, 0.0, 0.0], 'rewardMean': 0.6574570849381889, 'totalEpisodes': 215, 'stepsPerEpisode': 66, 'rewardPerEpisode': 60.64584325642271, 'successfulTests': 1
'totalSteps': 14080, 'rewardStep': 0.693775749926967, 'errorList': [], 'lossList': [0.0, -1.3584331333637238, 0.0, 6.942329899668693, 0.0, 0.0, 0.0], 'rewardMean': 0.6790560824086057, 'totalEpisodes': 217, 'stepsPerEpisode': 616, 'rewardPerEpisode': 509.74679363735225
'totalSteps': 15360, 'rewardStep': 0.8592727265504951, 'errorList': [], 'lossList': [0.0, -1.354893058538437, 0.0, 7.26519198179245, 0.0, 0.0, 0.0], 'rewardMean': 0.6767643597376525, 'totalEpisodes': 220, 'stepsPerEpisode': 286, 'rewardPerEpisode': 241.94755470420077
'totalSteps': 16640, 'rewardStep': 0.9448271217784756, 'errorList': [0.10506228442183743, 0.17011042301277296, 0.015599765090256712, 0.10212442140974892, 0.23543004812805662, 0.0987993611124367, 0.04430602197199482, 0.1300992073300602, 0.0073859270135230625, 0.07419979347653995, 0.054770551097563965, 0.024257202384259408, 0.05949857957105324, 0.03715887533440362, 0.04646760289638253, 0.039227791634069704, 0.03526598951607158, 0.054156741561102825, 0.11935819331229784, 0.22413689517503996, 0.16916080327022964, 0.047388709202597384, 0.13941834631359978, 0.06392883428925712, 0.16599436337347506, 0.1797831784179764, 0.06137213854559924, 0.0071071114129092824, 0.06542043013405517, 0.15797686436237046, 0.13529113057547146, 0.0840562750352732, 0.3284329132111835, 0.12449736729458419, 0.007411234487629585, 0.14645872492635853, 0.07009425285242157, 0.13347150475365038, 0.03194594015951604, 0.06497009756030701, 0.2857650401777108, 0.12392852922928252, 0.19910275797637522, 0.19045802458853536, 0.09379136442426687, 0.08315471963409389, 0.09058795589787072, 0.09742255228648049, 0.15989240077462835, 0.06448727569545229], 'lossList': [0.0, -1.3504746788740158, 0.0, 5.45166323184967, 0.0, 0.0, 0.0], 'rewardMean': 0.7109715839835938, 'totalEpisodes': 222, 'stepsPerEpisode': 98, 'rewardPerEpisode': 89.16251614984552, 'successfulTests': 46
'totalSteps': 17920, 'rewardStep': 0.8490126181073914, 'errorList': [], 'lossList': [0.0, -1.340602649450302, 0.0, 2.6070442512631415, 0.0, 0.0, 0.0], 'rewardMean': 0.7241594833530627, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.6340057628533
'totalSteps': 19200, 'rewardStep': 0.9052701391779289, 'errorList': [], 'lossList': [0.0, -1.317662160396576, 0.0, 22.8645201086998, 0.0, 0.0, 0.0], 'rewardMean': 0.7521250899671192, 'totalEpisodes': 223, 'stepsPerEpisode': 759, 'rewardPerEpisode': 663.1694241005608
'totalSteps': 20480, 'rewardStep': 0.9433572115507821, 'errorList': [0.1466332506817146, 0.17124586937119993, 0.14794615413708512, 0.16194436488224873, 0.14580982337103623, 0.1637065686357728, 0.16088470701498428, 0.15337931734930552, 0.16329459530099863, 0.15026629922584728, 0.1524572199291346, 0.17194553868645193, 0.15412001799757036, 0.1484406928102015, 0.17787844500190556, 0.1615255653861768, 0.160168412685077, 0.1628028932610779, 0.1509542140782313, 0.15484413596494892, 0.16556813720936603, 0.1775723399420764, 0.15316738836279084, 0.1707082744755832, 0.15075999425593858, 0.16808403173542466, 0.1600197727589075, 0.157267716726419, 0.20146399254189049, 0.16186534053554932, 0.1941108216128856, 0.16591827458725186, 0.16831070409164584, 0.1557412969614428, 0.15916671302806878, 0.16800146307710034, 0.1876554324437668, 0.1521988410037895, 0.16404604226331979, 0.17509590332657635, 0.16273068118079737, 0.15090704808192434, 0.17015702768648666, 0.16174057718190266, 0.16026058305518903, 0.16752227279115628, 0.16295438372408028, 0.1879948468584639, 0.16717492967158434, 0.14947445304594212], 'lossList': [0.0, -1.2886328095197677, 0.0, 1.838807639554143, 0.0, 0.0, 0.0], 'rewardMean': 0.7571016661416986, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.7940257022726, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.9338805364253371, 'errorList': [0.07964285438468058, 0.09398269785221317, 0.0690227865430033, 0.09335640670472786, 0.09095293329327803, 0.08707086399541443, 0.09581723470676774, 0.0986787583661858, 0.07114249943519939, 0.06929667108776857, 0.07639087005033382, 0.093891501964905, 0.10902067804241171, 0.07271304955408688, 0.07793297095483334, 0.09578723764774845, 0.0803059086823412, 0.07093333773214898, 0.06854611894351735, 0.0689023353353922, 0.08601786754626983, 0.08515722773880738, 0.09163252274796282, 0.10005570104933231, 0.09198679212014618, 0.08516371871048783, 0.07983804982313757, 0.0881685407056941, 0.07118208380492877, 0.10145623774378575, 0.09940896220213849, 0.10750727310765254, 0.10214624202319288, 0.09341712899559564, 0.0850340925567328, 0.06996055109742853, 0.08851528530326838, 0.1015764857093413, 0.10881073669227759, 0.0839442791152441, 0.10524053084620756, 0.07271033029601323, 0.07715390674180427, 0.10548409235365942, 0.09757548055238008, 0.08531494143533923, 0.0688242304527256, 0.08323901136090112, 0.07019151067079504, 0.08874122100619394], 'lossList': [0.0, -1.2566927134990693, 0.0, 0.9722208497673273, 0.0, 0.0, 0.0], 'rewardMean': 0.807347813017419, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.8520942413675, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=133.2
