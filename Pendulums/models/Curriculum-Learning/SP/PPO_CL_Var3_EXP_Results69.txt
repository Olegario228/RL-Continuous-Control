#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 69
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8865912068878231, 'errorList': [], 'lossList': [0.0, -1.4207831889390945, 0.0, 31.075489950180053, 0.0, 0.0, 0.0], 'rewardMean': 0.804971065383562, 'totalEpisodes': 49, 'stepsPerEpisode': 22, 'rewardPerEpisode': 20.28347625343498
'totalSteps': 3840, 'rewardStep': 0.6971048733933799, 'errorList': [], 'lossList': [0.0, -1.415855719447136, 0.0, 42.42235758781433, 0.0, 0.0, 0.0], 'rewardMean': 0.7690156680535013, 'totalEpisodes': 112, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.290416828837715
'totalSteps': 5120, 'rewardStep': 0.8878469576461703, 'errorList': [], 'lossList': [0.0, -1.3997554880380632, 0.0, 39.59910278320312, 0.0, 0.0, 0.0], 'rewardMean': 0.7987234904516685, 'totalEpisodes': 166, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.555914552734517
'totalSteps': 6400, 'rewardStep': 0.537574997856637, 'errorList': [], 'lossList': [0.0, -1.3822977119684219, 0.0, 42.01802682876587, 0.0, 0.0, 0.0], 'rewardMean': 0.7464937919326622, 'totalEpisodes': 189, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.690257142915712
'totalSteps': 7680, 'rewardStep': 0.7905707420618908, 'errorList': [], 'lossList': [0.0, -1.3682300245761871, 0.0, 47.0886563205719, 0.0, 0.0, 0.0], 'rewardMean': 0.7538399502875336, 'totalEpisodes': 208, 'stepsPerEpisode': 35, 'rewardPerEpisode': 30.528369856521714
'totalSteps': 8960, 'rewardStep': 0.6163569363822972, 'errorList': [], 'lossList': [0.0, -1.3558337515592576, 0.0, 38.8745978307724, 0.0, 0.0, 0.0], 'rewardMean': 0.7341995197296427, 'totalEpisodes': 220, 'stepsPerEpisode': 213, 'rewardPerEpisode': 152.26751602208424
'totalSteps': 10240, 'rewardStep': 0.8756244928051259, 'errorList': [], 'lossList': [0.0, -1.3479446256160736, 0.0, 16.752856085300447, 0.0, 0.0, 0.0], 'rewardMean': 0.7518776413640782, 'totalEpisodes': 227, 'stepsPerEpisode': 53, 'rewardPerEpisode': 46.40180231266558
'totalSteps': 11520, 'rewardStep': 0.6745574876501115, 'errorList': [], 'lossList': [0.0, -1.3448291224241258, 0.0, 6.86912550330162, 0.0, 0.0, 0.0], 'rewardMean': 0.7432865131736375, 'totalEpisodes': 233, 'stepsPerEpisode': 98, 'rewardPerEpisode': 80.84737582364511
'totalSteps': 12800, 'rewardStep': 0.5928670737574876, 'errorList': [], 'lossList': [0.0, -1.348057741522789, 0.0, 25.502632780075075, 0.0, 0.0, 0.0], 'rewardMean': 0.7282445692320224, 'totalEpisodes': 241, 'stepsPerEpisode': 64, 'rewardPerEpisode': 42.937563656923594
'totalSteps': 14080, 'rewardStep': 0.3665173834995805, 'errorList': [], 'lossList': [0.0, -1.3501369732618331, 0.0, 6.3572474211454395, 0.0, 0.0, 0.0], 'rewardMean': 0.6925612151940503, 'totalEpisodes': 246, 'stepsPerEpisode': 211, 'rewardPerEpisode': 147.81115231950355
'totalSteps': 15360, 'rewardStep': 0.7758846515469076, 'errorList': [], 'lossList': [0.0, -1.3604928427934646, 0.0, 18.226830108165743, 0.0, 0.0, 0.0], 'rewardMean': 0.6814905596599588, 'totalEpisodes': 250, 'stepsPerEpisode': 384, 'rewardPerEpisode': 315.7692674061362
'totalSteps': 16640, 'rewardStep': 0.6804818396071186, 'errorList': [], 'lossList': [0.0, -1.3513298493623733, 0.0, 8.375467145442963, 0.0, 0.0, 0.0], 'rewardMean': 0.6798282562813326, 'totalEpisodes': 252, 'stepsPerEpisode': 121, 'rewardPerEpisode': 103.11988975213232
'totalSteps': 17920, 'rewardStep': 0.8178581980864168, 'errorList': [], 'lossList': [0.0, -1.343163362145424, 0.0, 4.4608905917406085, 0.0, 0.0, 0.0], 'rewardMean': 0.6728293803253573, 'totalEpisodes': 256, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.843666231809078
'totalSteps': 19200, 'rewardStep': 0.8653543457103812, 'errorList': [], 'lossList': [0.0, -1.3384820050001145, 0.0, 3.9605803591012956, 0.0, 0.0, 0.0], 'rewardMean': 0.7056073151107317, 'totalEpisodes': 258, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.683111689391993
'totalSteps': 20480, 'rewardStep': 0.8116169005146094, 'errorList': [], 'lossList': [0.0, -1.329368988275528, 0.0, 1.2177116605639458, 0.0, 0.0, 0.0], 'rewardMean': 0.7077119309560036, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1006.0840025597685
'totalSteps': 21760, 'rewardStep': 0.9148491002277088, 'errorList': [], 'lossList': [0.0, -1.3090856719017028, 0.0, 1.687862347587943, 0.0, 0.0, 0.0], 'rewardMean': 0.7375611473405448, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.6370092435686
'totalSteps': 23040, 'rewardStep': 0.8972014288590974, 'errorList': [], 'lossList': [0.0, -1.3022687226533889, 0.0, 1.269432302042842, 0.0, 0.0, 0.0], 'rewardMean': 0.739718840945942, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.6370078199288
'totalSteps': 24320, 'rewardStep': 0.8893203317088155, 'errorList': [], 'lossList': [0.0, -1.2913594657182694, 0.0, 1.0077748460322618, 0.0, 0.0, 0.0], 'rewardMean': 0.7611951253518123, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.836718236579
'totalSteps': 25600, 'rewardStep': 0.9493814801403371, 'errorList': [0.052943286385342424, 0.04185527400766388, 0.039173917040457765, 0.06591385940231778, 0.051425963793520546, 0.037840566982675176, 0.03578807338506358, 0.025667598504083195, 0.040264387225277525, 0.06271703017332565, 0.05349356996477124, 0.03715905869039574, 0.037215608988743036, 0.07270042943207959, 0.039138132477908975, 0.07364798623770089, 0.0379789680266189, 0.0421368344601017, 0.05804790226527914, 0.08156400244051247, 0.10170985884991926, 0.07754677389797855, 0.0761312910122094, 0.04027373377310052, 0.07762669243363922, 0.04588173448743303, 0.09237765659776816, 0.07365763565396016, 0.06544194464347516, 0.07234705395036518, 0.05457826613688826, 0.08147838337716963, 0.09412109486961162, 0.055668566146859964, 0.11161090887157335, 0.05558223049926939, 0.06130467193027481, 0.05532121801454501, 0.03938271910562697, 0.055451791994162294, 0.07602295040877834, 0.08520556876351529, 0.04736545064770978, 0.06959831303948968, 0.08195071406403005, 0.09695677871058651, 0.031036337117536075, 0.0335585303015489, 0.08559415361970572, 0.10778847038439529], 'lossList': [0.0, -1.2578334665298463, 0.0, 0.5996513104811311, 0.0, 0.0, 0.0], 'rewardMean': 0.7968465659900972, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.2368618505345, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.85
