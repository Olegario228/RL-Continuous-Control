#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 7
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.7050888266403081, 'errorList': [], 'lossList': [0.0, -1.4275062119960784, 0.0, 30.3682705783844, 0.0, 0.0, 0.0], 'rewardMean': 0.7494294139102297, 'totalEpisodes': 75, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.813350418736201
'totalSteps': 3840, 'rewardStep': 0.5917369175149011, 'errorList': [], 'lossList': [0.0, -1.429237903356552, 0.0, 40.395541219711305, 0.0, 0.0, 0.0], 'rewardMean': 0.6968652484451202, 'totalEpisodes': 134, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.400408503775631
'totalSteps': 5120, 'rewardStep': 0.8501299697740297, 'errorList': [], 'lossList': [0.0, -1.4078372609615326, 0.0, 47.268625526428224, 0.0, 0.0, 0.0], 'rewardMean': 0.7351814287773475, 'totalEpisodes': 181, 'stepsPerEpisode': 48, 'rewardPerEpisode': 37.537025654591886
'totalSteps': 6400, 'rewardStep': 0.5684294717399455, 'errorList': [], 'lossList': [0.0, -1.37949380338192, 0.0, 40.64346192359924, 0.0, 0.0, 0.0], 'rewardMean': 0.7018310373698671, 'totalEpisodes': 203, 'stepsPerEpisode': 25, 'rewardPerEpisode': 18.70835856268387
'totalSteps': 7680, 'rewardStep': 0.8193079443592656, 'errorList': [], 'lossList': [0.0, -1.3577997702360154, 0.0, 46.77574294090271, 0.0, 0.0, 0.0], 'rewardMean': 0.7214105218681001, 'totalEpisodes': 216, 'stepsPerEpisode': 56, 'rewardPerEpisode': 38.29760290264862
'totalSteps': 8960, 'rewardStep': 0.6190716109979146, 'errorList': [], 'lossList': [0.0, -1.352544873356819, 0.0, 31.43900382757187, 0.0, 0.0, 0.0], 'rewardMean': 0.7067906774580736, 'totalEpisodes': 223, 'stepsPerEpisode': 134, 'rewardPerEpisode': 108.46717513166054
'totalSteps': 10240, 'rewardStep': 0.4037454478316769, 'errorList': [], 'lossList': [0.0, -1.3542831724882125, 0.0, 14.46444526553154, 0.0, 0.0, 0.0], 'rewardMean': 0.668910023754774, 'totalEpisodes': 227, 'stepsPerEpisode': 139, 'rewardPerEpisode': 104.1174918705211
'totalSteps': 11520, 'rewardStep': 0.23038982528360297, 'errorList': [], 'lossList': [0.0, -1.3615895110368728, 0.0, 19.631246793270112, 0.0, 0.0, 0.0], 'rewardMean': 0.6201855572579773, 'totalEpisodes': 234, 'stepsPerEpisode': 156, 'rewardPerEpisode': 99.46475622701668
'totalSteps': 12800, 'rewardStep': 0.6647402075029043, 'errorList': [], 'lossList': [0.0, -1.3621303176879882, 0.0, 20.13015592455864, 0.0, 0.0, 0.0], 'rewardMean': 0.62464102228247, 'totalEpisodes': 238, 'stepsPerEpisode': 373, 'rewardPerEpisode': 309.08783317884064
'totalSteps': 14080, 'rewardStep': 0.8567362221218637, 'errorList': [], 'lossList': [0.0, -1.3578694725036622, 0.0, 6.021477496623993, 0.0, 0.0, 0.0], 'rewardMean': 0.6309376443766412, 'totalEpisodes': 242, 'stepsPerEpisode': 49, 'rewardPerEpisode': 40.16339090427667
'totalSteps': 15360, 'rewardStep': 0.8758918505830571, 'errorList': [], 'lossList': [0.0, -1.3679695439338684, 0.0, 19.335179600715637, 0.0, 0.0, 0.0], 'rewardMean': 0.6480179467709162, 'totalEpisodes': 245, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.526739635514543
'totalSteps': 16640, 'rewardStep': 0.8591431764486821, 'errorList': [], 'lossList': [0.0, -1.3485212165117264, 0.0, 3.5076608723402023, 0.0, 0.0, 0.0], 'rewardMean': 0.6747585726642943, 'totalEpisodes': 247, 'stepsPerEpisode': 112, 'rewardPerEpisode': 91.00485221567106
'totalSteps': 17920, 'rewardStep': 0.7295959812033066, 'errorList': [], 'lossList': [0.0, -1.3214204978942872, 0.0, 2.9831997388601303, 0.0, 0.0, 0.0], 'rewardMean': 0.6627051738072219, 'totalEpisodes': 248, 'stepsPerEpisode': 949, 'rewardPerEpisode': 789.7430601985476
'totalSteps': 19200, 'rewardStep': 0.7580339865246407, 'errorList': [], 'lossList': [0.0, -1.3065790665149688, 0.0, 3.2306199568510054, 0.0, 0.0, 0.0], 'rewardMean': 0.6816656252856914, 'totalEpisodes': 249, 'stepsPerEpisode': 274, 'rewardPerEpisode': 214.6224937179211
'totalSteps': 20480, 'rewardStep': 0.8205173220029067, 'errorList': [], 'lossList': [0.0, -1.2842556780576706, 0.0, 1.3292881652712822, 0.0, 0.0, 0.0], 'rewardMean': 0.6817865630500556, 'totalEpisodes': 249, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 946.8667439529949
'totalSteps': 21760, 'rewardStep': 0.8811012107342483, 'errorList': [], 'lossList': [0.0, -1.2593590092658997, 0.0, 3.379979177415371, 0.0, 0.0, 0.0], 'rewardMean': 0.7079895230236889, 'totalEpisodes': 250, 'stepsPerEpisode': 149, 'rewardPerEpisode': 131.66535393922416
'totalSteps': 23040, 'rewardStep': 0.9528828095589089, 'errorList': [0.04916161281722939, 0.0743709762192428, 0.05701129429035619, 0.03190829684442007, 0.13260395559265878, 0.1630904121474112, 0.1241299232274061, 0.1494072021830436, 0.18346602270061302, 0.09133968808918426, 0.2419634086823072, 0.041312150204933035, 0.07974125071910979, 0.13802771989042864, 0.0529684301918033, 0.04030405408590172, 0.0565966841442027, 0.03381733921766505, 0.10219523742843577, 0.05493405762247255, 0.2410770890440693, 0.15932472808250508, 0.06778931467130402, 0.09386589617912204, 0.054517285931870885, 0.08302345671025677, 0.057395219322594095, 0.10999121531557401, 0.11528023761002289, 0.03306655330577446, 0.05074461792831504, 0.11580844416520956, 0.06610055863640596, 0.05023148132762622, 0.23550981058111942, 0.09108909265103664, 0.24779384634942395, 0.051701945447835844, 0.04296541926089709, 0.1554442526674175, 0.05844295969814875, 0.08667872034273373, 0.12458812179646733, 0.2193046030894618, 0.2052842263449048, 0.0366459722373887, 0.037279258606297286, 0.03316165074660708, 0.13144569013028606, 0.19570802168179263], 'lossList': [0.0, -1.2266908252239228, 0.0, 1.2368719471618532, 0.0, 0.0, 0.0], 'rewardMean': 0.7629032591964121, 'totalEpisodes': 250, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.0124577495421, 'successfulTests': 44
'totalSteps': 24320, 'rewardStep': 0.9295838441475547, 'errorList': [], 'lossList': [0.0, -1.1997460037469865, 0.0, 0.9945604276657104, 0.0, 0.0, 0.0], 'rewardMean': 0.8328226610828073, 'totalEpisodes': 250, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1185.4072010612624
'totalSteps': 25600, 'rewardStep': 0.838395215361853, 'errorList': [], 'lossList': [0.0, -1.1729652255773544, 0.0, 0.5476122849993408, 0.0, 0.0, 0.0], 'rewardMean': 0.8501881618687023, 'totalEpisodes': 250, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.101704459982
#maxSuccessfulTests=44, maxSuccessfulTestsAtStep=23040, timeSpent=70.32
