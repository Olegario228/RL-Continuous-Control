#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 70
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.5415161640295375, 'errorList': [], 'lossList': [0.0, -1.4319399166107178, 0.0, 31.148243837356567, 0.0, 0.0, 0.0], 'rewardMean': 0.7281683973051828, 'totalEpisodes': 29, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.362490787771272
'totalSteps': 3840, 'rewardStep': 0.45795169731872837, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5930600473119556, 'totalEpisodes': 91, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.237621153942089
'totalSteps': 5120, 'rewardStep': 0.4633902891681502, 'errorList': [], 'lossList': [0.0, -1.4070594501495362, 0.0, 45.760583839416505, 0.0, 0.0, 0.0], 'rewardMean': 0.5671260956831945, 'totalEpisodes': 149, 'stepsPerEpisode': 2, 'rewardPerEpisode': 0.9468728578614061
'totalSteps': 6400, 'rewardStep': 0.5711104127827905, 'errorList': [], 'lossList': [0.0, -1.386560532450676, 0.0, 50.01980516433716, 0.0, 0.0, 0.0], 'rewardMean': 0.5677901485331273, 'totalEpisodes': 198, 'stepsPerEpisode': 20, 'rewardPerEpisode': 12.491478032395078
'totalSteps': 7680, 'rewardStep': 0.8480358354135302, 'errorList': [], 'lossList': [0.0, -1.3610794430971145, 0.0, 44.52297649383545, 0.0, 0.0, 0.0], 'rewardMean': 0.607825246658899, 'totalEpisodes': 218, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.274274304096785
'totalSteps': 8960, 'rewardStep': 0.5410647925618008, 'errorList': [], 'lossList': [0.0, -1.3360198879241942, 0.0, 33.49777801513672, 0.0, 0.0, 0.0], 'rewardMean': 0.5994801898967618, 'totalEpisodes': 230, 'stepsPerEpisode': 100, 'rewardPerEpisode': 74.43590571326422
'totalSteps': 10240, 'rewardStep': 0.5182356543758062, 'errorList': [], 'lossList': [0.0, -1.3294635283946992, 0.0, 26.622572021484373, 0.0, 0.0, 0.0], 'rewardMean': 0.5904530192833223, 'totalEpisodes': 241, 'stepsPerEpisode': 108, 'rewardPerEpisode': 84.77287094770308
'totalSteps': 11520, 'rewardStep': 0.5764616325827815, 'errorList': [], 'lossList': [0.0, -1.317102478146553, 0.0, 33.63726941108703, 0.0, 0.0, 0.0], 'rewardMean': 0.5890538806132682, 'totalEpisodes': 249, 'stepsPerEpisode': 67, 'rewardPerEpisode': 55.03664329478383
'totalSteps': 12800, 'rewardStep': 0.770093907524096, 'errorList': [], 'lossList': [0.0, -1.2995116168260574, 0.0, 13.197487270832061, 0.0, 0.0, 0.0], 'rewardMean': 0.5745812083075951, 'totalEpisodes': 255, 'stepsPerEpisode': 93, 'rewardPerEpisode': 77.62035274067489
'totalSteps': 14080, 'rewardStep': 0.7573481037723512, 'errorList': [], 'lossList': [0.0, -1.3002540653944015, 0.0, 9.422434412837028, 0.0, 0.0, 0.0], 'rewardMean': 0.5961644022818764, 'totalEpisodes': 258, 'stepsPerEpisode': 574, 'rewardPerEpisode': 485.3742169205139
'totalSteps': 15360, 'rewardStep': 0.5747058828736485, 'errorList': [], 'lossList': [0.0, -1.3232121485471726, 0.0, 7.9438902795314785, 0.0, 0.0, 0.0], 'rewardMean': 0.6078398208373683, 'totalEpisodes': 261, 'stepsPerEpisode': 265, 'rewardPerEpisode': 218.50721836300332
'totalSteps': 16640, 'rewardStep': 0.8777284727335815, 'errorList': [], 'lossList': [0.0, -1.3436479485034942, 0.0, 6.548723742961884, 0.0, 0.0, 0.0], 'rewardMean': 0.6498174983788536, 'totalEpisodes': 262, 'stepsPerEpisode': 322, 'rewardPerEpisode': 285.3824508756915
'totalSteps': 17920, 'rewardStep': 0.8908038271436179, 'errorList': [], 'lossList': [0.0, -1.333396973013878, 0.0, 2.1690244030952455, 0.0, 0.0, 0.0], 'rewardMean': 0.6925588521764003, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.984832833937
'totalSteps': 19200, 'rewardStep': 0.8970976388929696, 'errorList': [], 'lossList': [0.0, -1.2956758987903596, 0.0, 2.3508818963170053, 0.0, 0.0, 0.0], 'rewardMean': 0.7251575747874183, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.824240399464
'totalSteps': 20480, 'rewardStep': 0.9247448136123724, 'errorList': [], 'lossList': [0.0, -1.2718196988105774, 0.0, 1.86764297734946, 0.0, 0.0, 0.0], 'rewardMean': 0.7328284726073026, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.372705195967
'totalSteps': 21760, 'rewardStep': 0.8827750780207939, 'errorList': [], 'lossList': [0.0, -1.251840842962265, 0.0, 1.4655223151668906, 0.0, 0.0, 0.0], 'rewardMean': 0.7669995011532019, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.7771684986337
'totalSteps': 23040, 'rewardStep': 0.910134070919727, 'errorList': [], 'lossList': [0.0, -1.2305486100912093, 0.0, 1.0218092977628113, 0.0, 0.0, 0.0], 'rewardMean': 0.8061893428075939, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.6681591225956
'totalSteps': 24320, 'rewardStep': 0.7587681438017175, 'errorList': [], 'lossList': [0.0, -1.1927433037757873, 0.0, 0.5441797594726085, 0.0, 0.0, 0.0], 'rewardMean': 0.8244199939294876, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.4862599386684
'totalSteps': 25600, 'rewardStep': 0.9469726732139395, 'errorList': [0.13341290219648083, 0.06885547061802405, 0.06812535908454082, 0.05276991075862713, 0.057449204291149074, 0.08763767859496228, 0.11792003026216542, 0.032021183763384774, 0.11188515964788479, 0.07030355145318619, 0.10198950054958633, 0.04322380668775589, 0.05859820607243397, 0.06561470345478322, 0.07390340823956913, 0.056326010683564985, 0.05532062237163777, 0.048424820160574686, 0.06797182355762398, 0.131114646881008, 0.061578981025917946, 0.08386946934806486, 0.0742348591230033, 0.06790258465093742, 0.044250363259775716, 0.03308087980415884, 0.11148416396294089, 0.04576045404376618, 0.05422703375464244, 0.07607881198722292, 0.07215234487559746, 0.09491509618681548, 0.066342308471186, 0.11571224435655904, 0.17719938553605274, 0.1575131403869773, 0.07202538772561655, 0.03872990843188372, 0.12797722521775604, 0.10894130958513369, 0.12508370913821143, 0.06340786775514688, 0.10301695888011037, 0.1008965169373241, 0.10478634002478442, 0.05707601023101023, 0.0578599881143194, 0.0915351335844235, 0.09793304306983312, 0.0674676229272481], 'lossList': [0.0, -1.1550644910335541, 0.0, 0.6342974282242357, 0.0, 0.0, 0.0], 'rewardMean': 0.8421078704984719, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.8411215951562, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=79.51
