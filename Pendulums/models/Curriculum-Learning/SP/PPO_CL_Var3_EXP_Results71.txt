#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 71
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.6337720848042706, 'errorList': [], 'lossList': [0.0, -1.4235347676277161, 0.0, 31.31130311012268, 0.0, 0.0, 0.0], 'rewardMean': 0.6134749521922149, 'totalEpisodes': 26, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.46641669977859
'totalSteps': 3840, 'rewardStep': 0.42043322860947885, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5169540904008469, 'totalEpisodes': 74, 'stepsPerEpisode': 45, 'rewardPerEpisode': 32.511179146667985
'totalSteps': 5120, 'rewardStep': 0.9851525360097755, 'errorList': [], 'lossList': [0.0, -1.4155068510770799, 0.0, 54.086224727630615, 0.0, 0.0, 0.0], 'rewardMean': 0.6105937795226326, 'totalEpisodes': 147, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.643168280469386
'totalSteps': 6400, 'rewardStep': 0.623808484889942, 'errorList': [], 'lossList': [0.0, -1.4040253299474716, 0.0, 43.63893404006958, 0.0, 0.0, 0.0], 'rewardMean': 0.6127962304171842, 'totalEpisodes': 193, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.173343160157054
'totalSteps': 7680, 'rewardStep': 0.8178221841029335, 'errorList': [], 'lossList': [0.0, -1.3929404306411743, 0.0, 46.5004047203064, 0.0, 0.0, 0.0], 'rewardMean': 0.6420856523722912, 'totalEpisodes': 211, 'stepsPerEpisode': 113, 'rewardPerEpisode': 89.31465878351439
'totalSteps': 8960, 'rewardStep': 0.8166412900422002, 'errorList': [], 'lossList': [0.0, -1.3709222650527955, 0.0, 47.65487010002136, 0.0, 0.0, 0.0], 'rewardMean': 0.6639051070810298, 'totalEpisodes': 225, 'stepsPerEpisode': 118, 'rewardPerEpisode': 98.35422388279116
'totalSteps': 10240, 'rewardStep': 0.7598383432478069, 'errorList': [], 'lossList': [0.0, -1.3564803856611252, 0.0, 43.65388014793396, 0.0, 0.0, 0.0], 'rewardMean': 0.6745643555440051, 'totalEpisodes': 233, 'stepsPerEpisode': 55, 'rewardPerEpisode': 47.95721230637586
'totalSteps': 11520, 'rewardStep': 0.540494519898238, 'errorList': [], 'lossList': [0.0, -1.3484034651517869, 0.0, 38.0178681564331, 0.0, 0.0, 0.0], 'rewardMean': 0.6611573719794284, 'totalEpisodes': 238, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.77031854537405
'totalSteps': 12800, 'rewardStep': 0.23374008315146327, 'errorList': [], 'lossList': [0.0, -1.331949292421341, 0.0, 13.920821576118469, 0.0, 0.0, 0.0], 'rewardMean': 0.6252135983365588, 'totalEpisodes': 243, 'stepsPerEpisode': 171, 'rewardPerEpisode': 103.97181815403684
'totalSteps': 14080, 'rewardStep': 0.6974234424571525, 'errorList': [], 'lossList': [0.0, -1.3109675562381744, 0.0, 12.416555830836296, 0.0, 0.0, 0.0], 'rewardMean': 0.631578734101847, 'totalEpisodes': 246, 'stepsPerEpisode': 233, 'rewardPerEpisode': 195.59287233685225
'totalSteps': 15360, 'rewardStep': 0.711842378288197, 'errorList': [], 'lossList': [0.0, -1.2866846776008607, 0.0, 6.763739756941796, 0.0, 0.0, 0.0], 'rewardMean': 0.6607196490697188, 'totalEpisodes': 249, 'stepsPerEpisode': 362, 'rewardPerEpisode': 282.59681977785317
'totalSteps': 16640, 'rewardStep': 0.6790884852725905, 'errorList': [], 'lossList': [0.0, -1.2642514955997468, 0.0, 9.66376069366932, 0.0, 0.0, 0.0], 'rewardMean': 0.6865851747360299, 'totalEpisodes': 251, 'stepsPerEpisode': 558, 'rewardPerEpisode': 454.6385262802237
'totalSteps': 17920, 'rewardStep': 0.8546914447997866, 'errorList': [], 'lossList': [0.0, -1.2565448087453843, 0.0, 8.599436283111572, 0.0, 0.0, 0.0], 'rewardMean': 0.673539065615031, 'totalEpisodes': 254, 'stepsPerEpisode': 354, 'rewardPerEpisode': 320.0718744093583
'totalSteps': 19200, 'rewardStep': 0.8744850343885614, 'errorList': [], 'lossList': [0.0, -1.2550324946641922, 0.0, 5.214434441924095, 0.0, 0.0, 0.0], 'rewardMean': 0.6986067205648929, 'totalEpisodes': 256, 'stepsPerEpisode': 225, 'rewardPerEpisode': 192.67006735039638
'totalSteps': 20480, 'rewardStep': 0.45007950492351917, 'errorList': [], 'lossList': [0.0, -1.2100097274780273, 0.0, 5.196213941574097, 0.0, 0.0, 0.0], 'rewardMean': 0.6618324526469517, 'totalEpisodes': 257, 'stepsPerEpisode': 666, 'rewardPerEpisode': 501.1248157358505
'totalSteps': 21760, 'rewardStep': 0.7790236478365685, 'errorList': [], 'lossList': [0.0, -1.182546494603157, 0.0, 5.340628272891045, 0.0, 0.0, 0.0], 'rewardMean': 0.6580706884263884, 'totalEpisodes': 259, 'stepsPerEpisode': 344, 'rewardPerEpisode': 304.92841912743415
'totalSteps': 23040, 'rewardStep': 0.8798582425002208, 'errorList': [], 'lossList': [0.0, -1.1573144745826722, 0.0, 1.1572700749337672, 0.0, 0.0, 0.0], 'rewardMean': 0.6700726783516299, 'totalEpisodes': 259, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.1809757119938
'totalSteps': 24320, 'rewardStep': 0.749337935864363, 'errorList': [], 'lossList': [0.0, -1.1336148631572724, 0.0, 0.8162126586586237, 0.0, 0.0, 0.0], 'rewardMean': 0.6909570199482422, 'totalEpisodes': 259, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.4233316689342
'totalSteps': 25600, 'rewardStep': 0.9534125807264479, 'errorList': [0.07223420992346961, 0.08911303978230713, 0.07468285102094743, 0.08167790788072918, 0.10914891073518744, 0.11532373761211515, 0.10137346782500747, 0.07816669029739103, 0.08799620581460903, 0.08882064474971474, 0.0655385968321569, 0.0514130751239912, 0.10072238655503711, 0.06796373871236007, 0.12610474859569318, 0.09428241857772927, 0.09238279126947552, 0.07275085479659052, 0.07613617953197704, 0.12756810963144558, 0.12055449725920254, 0.062015341689125536, 0.12363636926178859, 0.08007856660353713, 0.10888591560700579, 0.09085838975224171, 0.08525486793512227, 0.08305251841875665, 0.0937534549768085, 0.0902495979287122, 0.12083826202029899, 0.12860751769806666, 0.06334777702774477, 0.11660220927209024, 0.12692818624008412, 0.07307825051250882, 0.07069701157083576, 0.06070821138377718, 0.0764105770697994, 0.0515683987132086, 0.08071950817312332, 0.06843401030330766, 0.08218514013996389, 0.059622686519243025, 0.11592258369397114, 0.11282968617523115, 0.09314584767115407, 0.10944521891148021, 0.05533541465158897, 0.09400813236872685], 'lossList': [0.0, -1.09654791533947, 0.0, 0.7089443764090538, 0.0, 0.0, 0.0], 'rewardMean': 0.7629242697057408, 'totalEpisodes': 259, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.0983871109302, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=78.76
