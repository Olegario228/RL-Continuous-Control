#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 72
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.5815627286745225, 'errorList': [], 'lossList': [0.0, -1.4398863965272903, 0.0, 31.239028248786926, 0.0, 0.0, 0.0], 'rewardMean': 0.5423317882892671, 'totalEpisodes': 34, 'stepsPerEpisode': 46, 'rewardPerEpisode': 33.313871160391734
'totalSteps': 3840, 'rewardStep': 0.5252741050951933, 'errorList': [], 'lossList': [0.0, -1.4372903025150299, 0.0, 47.28749708175659, 0.0, 0.0, 0.0], 'rewardMean': 0.5366458938912425, 'totalEpisodes': 84, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.041897872013939
'totalSteps': 5120, 'rewardStep': 0.7022055671722135, 'errorList': [], 'lossList': [0.0, -1.4208237236738206, 0.0, 44.2977276134491, 0.0, 0.0, 0.0], 'rewardMean': 0.5780358122114853, 'totalEpisodes': 142, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.937679079999313
'totalSteps': 6400, 'rewardStep': 0.5117333847808115, 'errorList': [], 'lossList': [0.0, -1.398428117632866, 0.0, 45.89895133972168, 0.0, 0.0, 0.0], 'rewardMean': 0.5647753267253506, 'totalEpisodes': 187, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5117333847808115
'totalSteps': 7680, 'rewardStep': 0.7692551367519738, 'errorList': [], 'lossList': [0.0, -1.3848064118623733, 0.0, 42.410850038528444, 0.0, 0.0, 0.0], 'rewardMean': 0.5988552950631211, 'totalEpisodes': 205, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.408558377003676
'totalSteps': 8960, 'rewardStep': 0.3657845674687403, 'errorList': [], 'lossList': [0.0, -1.3713286727666856, 0.0, 32.42166838645935, 0.0, 0.0, 0.0], 'rewardMean': 0.5655594768353523, 'totalEpisodes': 216, 'stepsPerEpisode': 136, 'rewardPerEpisode': 95.20136344824833
'totalSteps': 10240, 'rewardStep': 0.11079764466916153, 'errorList': [], 'lossList': [0.0, -1.3658161211013793, 0.0, 13.855681369304657, 0.0, 0.0, 0.0], 'rewardMean': 0.5087142478145785, 'totalEpisodes': 220, 'stepsPerEpisode': 169, 'rewardPerEpisode': 116.49666205495292
'totalSteps': 11520, 'rewardStep': 0.8527273151915009, 'errorList': [], 'lossList': [0.0, -1.371882165670395, 0.0, 23.948040943145752, 0.0, 0.0, 0.0], 'rewardMean': 0.54693792196757, 'totalEpisodes': 223, 'stepsPerEpisode': 469, 'rewardPerEpisode': 388.79663239790074
'totalSteps': 12800, 'rewardStep': 0.9163777223509874, 'errorList': [], 'lossList': [0.0, -1.3739608711004256, 0.0, 31.549375622272493, 0.0, 0.0, 0.0], 'rewardMean': 0.5838819020059117, 'totalEpisodes': 229, 'stepsPerEpisode': 74, 'rewardPerEpisode': 65.82815482170075
'totalSteps': 14080, 'rewardStep': 0.7319590766782205, 'errorList': [], 'lossList': [0.0, -1.3775713354349137, 0.0, 9.745119706988335, 0.0, 0.0, 0.0], 'rewardMean': 0.6067677248833325, 'totalEpisodes': 230, 'stepsPerEpisode': 640, 'rewardPerEpisode': 542.194742138225
'totalSteps': 15360, 'rewardStep': 0.6265534823556638, 'errorList': [], 'lossList': [0.0, -1.3800161212682724, 0.0, 19.56964785695076, 0.0, 0.0, 0.0], 'rewardMean': 0.6112668002514468, 'totalEpisodes': 231, 'stepsPerEpisode': 1140, 'rewardPerEpisode': 808.8959767743008
'totalSteps': 16640, 'rewardStep': 0.8369272450558609, 'errorList': [], 'lossList': [0.0, -1.3681002342700959, 0.0, 4.299855330586434, 0.0, 0.0, 0.0], 'rewardMean': 0.6424321142475135, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.9730754441091
'totalSteps': 17920, 'rewardStep': 0.8384524284050178, 'errorList': [], 'lossList': [0.0, -1.3252515417337418, 0.0, 2.3349171330034735, 0.0, 0.0, 0.0], 'rewardMean': 0.6560568003707938, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.1914812182472
'totalSteps': 19200, 'rewardStep': 0.9544701682114686, 'errorList': [0.11040466971477035, 0.0872438666829999, 0.10029973490906922, 0.10998997291548539, 0.07352127726915393, 0.06416518052277069, 0.0813781531665484, 0.06269118515575932, 0.1382625361038346, 0.06294847252855884, 0.0904093231398009, 0.05944310803512496, 0.05016900287623534, 0.0496372512387998, 0.07853791149072427, 0.05304289812012245, 0.10120133777913723, 0.14144754104902946, 0.09230054922049145, 0.04111446132374384, 0.059821951192645696, 0.09670154943218537, 0.10458674989067612, 0.09208555775126337, 0.08543645801142137, 0.06852963727241365, 0.031213883987506793, 0.07092130364428931, 0.046310845214507394, 0.10794479279603321, 0.03822572539153639, 0.08312211734643284, 0.05482355154505406, 0.06293977144414056, 0.133899551827041, 0.06375225896797956, 0.0527240626744197, 0.11176104132260288, 0.04475345970850928, 0.09277308866126505, 0.08310005499222062, 0.11167723780118856, 0.10860855065270111, 0.08093415365043484, 0.03498364500531482, 0.07305537411088883, 0.06474278847971288, 0.05172403386174695, 0.07523083496232692, 0.11146528873095926], 'lossList': [0.0, -1.2779057770967484, 0.0, 2.3080334039777517, 0.0, 0.0, 0.0], 'rewardMean': 0.7003304787138596, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.9900166810548, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=65.44
