#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 74
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.5976367997217933, 'errorList': [], 'lossList': [0.0, -1.4164778184890747, 0.0, 31.196950120925905, 0.0, 0.0, 0.0], 'rewardMean': 0.6734638320105372, 'totalEpisodes': 35, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.650079338539811
'totalSteps': 3840, 'rewardStep': 0.48830447401226146, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5808841530113993, 'totalEpisodes': 89, 'stepsPerEpisode': 17, 'rewardPerEpisode': 10.867615972143943
'totalSteps': 5120, 'rewardStep': 0.8040173635494844, 'errorList': [], 'lossList': [0.0, -1.4105748635530473, 0.0, 48.30002889633179, 0.0, 0.0, 0.0], 'rewardMean': 0.6255107951190163, 'totalEpisodes': 158, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.3032193789077344
'totalSteps': 6400, 'rewardStep': 0.8977635530980572, 'errorList': [], 'lossList': [0.0, -1.3959996151924132, 0.0, 47.75704173088074, 0.0, 0.0, 0.0], 'rewardMean': 0.6708862547821898, 'totalEpisodes': 200, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.71912599958842
'totalSteps': 7680, 'rewardStep': 0.5073616773576426, 'errorList': [], 'lossList': [0.0, -1.3768999034166336, 0.0, 48.29509471893311, 0.0, 0.0, 0.0], 'rewardMean': 0.6475256008643973, 'totalEpisodes': 228, 'stepsPerEpisode': 34, 'rewardPerEpisode': 22.214770215455314
'totalSteps': 8960, 'rewardStep': 0.7223828204502001, 'errorList': [], 'lossList': [0.0, -1.3602507412433624, 0.0, 42.40555423736572, 0.0, 0.0, 0.0], 'rewardMean': 0.6568827533126227, 'totalEpisodes': 244, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.676372394557858
'totalSteps': 10240, 'rewardStep': 0.8789802265764629, 'errorList': [], 'lossList': [0.0, -1.3487110662460327, 0.0, 28.605086119174956, 0.0, 0.0, 0.0], 'rewardMean': 0.6815602503419382, 'totalEpisodes': 252, 'stepsPerEpisode': 38, 'rewardPerEpisode': 29.705243744811977
'totalSteps': 11520, 'rewardStep': 0.892136052404883, 'errorList': [], 'lossList': [0.0, -1.3346662539243699, 0.0, 16.423007829189302, 0.0, 0.0, 0.0], 'rewardMean': 0.7026178305482327, 'totalEpisodes': 255, 'stepsPerEpisode': 482, 'rewardPerEpisode': 403.25689983004133
'totalSteps': 12800, 'rewardStep': 0.8639945806653052, 'errorList': [], 'lossList': [0.0, -1.3272623246908188, 0.0, 24.27849452495575, 0.0, 0.0, 0.0], 'rewardMean': 0.7140882021848352, 'totalEpisodes': 261, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.18811076201752
'totalSteps': 14080, 'rewardStep': 0.9808277883434832, 'errorList': [9.404644097738547, 7.604078275578302, 6.749036997496964, 9.267617178244151, 6.921952462616894, 9.022793504929414, 7.643824187040308, 8.32583325094093, 6.157267061164241, 7.9788703783876995, 9.591122039393275, 7.655683715136964, 6.051909591058989, 4.66069278846694, 6.527825606663975, 6.914839069094184, 7.987591604813619, 6.85980122252549, 5.063740535099294, 8.764857875407259, 8.436077374234834, 8.332748143985876, 7.830138864002891, 4.001118182144011, 7.98950502850653, 8.720818356240173, 10.022097566279172, 7.757155996732177, 7.982758024690019, 8.851504124700982, 7.24967108812832, 9.954214068632895, 5.530731616961839, 5.895455336741996, 8.078118809895217, 6.643521552542566, 7.728827892646701, 9.195958261738713, 8.186993545513298, 7.558392243266343, 6.9634009715601355, 6.93089637407669, 8.935404641929702, 9.87495473954116, 8.798996535985031, 5.435936207283485, 9.18609462239292, 3.4829151049196687, 9.8565318868081, 7.789542929908874], 'lossList': [0.0, -1.3159464752674104, 0.0, 11.430528662204743, 0.0, 0.0, 0.0], 'rewardMean': 0.7524073010470043, 'totalEpisodes': 267, 'stepsPerEpisode': 42, 'rewardPerEpisode': 40.14875649335855, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7258162249230609, 'errorList': [], 'lossList': [0.0, -1.3100944948196411, 0.0, 26.468721795082093, 0.0, 0.0, 0.0], 'rewardMean': 0.7761584761380841, 'totalEpisodes': 272, 'stepsPerEpisode': 579, 'rewardPerEpisode': 460.6356788999858
'totalSteps': 16640, 'rewardStep': 0.5597224505839813, 'errorList': [], 'lossList': [0.0, -1.3100736463069915, 0.0, 6.9714102363586425, 0.0, 0.0, 0.0], 'rewardMean': 0.7833002737952561, 'totalEpisodes': 276, 'stepsPerEpisode': 235, 'rewardPerEpisode': 194.9199603848773
'totalSteps': 17920, 'rewardStep': 0.7448019627549007, 'errorList': [], 'lossList': [0.0, -1.3038521713018418, 0.0, 7.646359957456589, 0.0, 0.0, 0.0], 'rewardMean': 0.7773787337157977, 'totalEpisodes': 280, 'stepsPerEpisode': 64, 'rewardPerEpisode': 54.79821729440029
'totalSteps': 19200, 'rewardStep': 0.8924886465057533, 'errorList': [], 'lossList': [0.0, -1.2907219475507736, 0.0, 3.4195036023855208, 0.0, 0.0, 0.0], 'rewardMean': 0.7768512430565673, 'totalEpisodes': 281, 'stepsPerEpisode': 375, 'rewardPerEpisode': 295.2435386326407
'totalSteps': 20480, 'rewardStep': 0.8494916048948571, 'errorList': [], 'lossList': [0.0, -1.2622739428281784, 0.0, 3.602222011387348, 0.0, 0.0, 0.0], 'rewardMean': 0.8110642358102889, 'totalEpisodes': 281, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 840.500117647729
'totalSteps': 21760, 'rewardStep': 0.6219959420152945, 'errorList': [], 'lossList': [0.0, -1.2457341641187667, 0.0, 3.035180421471596, 0.0, 0.0, 0.0], 'rewardMean': 0.8010255479667983, 'totalEpisodes': 283, 'stepsPerEpisode': 181, 'rewardPerEpisode': 146.1688105335583
'totalSteps': 23040, 'rewardStep': 0.8464781310214158, 'errorList': [], 'lossList': [0.0, -1.2456831473112107, 0.0, 4.288112049102783, 0.0, 0.0, 0.0], 'rewardMean': 0.7977753384112936, 'totalEpisodes': 284, 'stepsPerEpisode': 1108, 'rewardPerEpisode': 843.8569082011977
'totalSteps': 24320, 'rewardStep': 0.7676792758688552, 'errorList': [], 'lossList': [0.0, -1.237507427930832, 0.0, 1.0261598315089941, 0.0, 0.0, 0.0], 'rewardMean': 0.7853296607576907, 'totalEpisodes': 284, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.9625674341764
'totalSteps': 25600, 'rewardStep': 0.9409223515780147, 'errorList': [0.04376732296094067, 0.02160789532834241, 0.022129569763515898, 0.02133298951441091, 0.027449657252648314, 0.028227684843723466, 0.02833363115902188, 0.08361308487167406, 0.055018676007544995, 0.034486315385375, 0.08771328873550989, 0.04610470945772381, 0.053510608790096933, 0.062446700521843576, 0.04663835745478559, 0.07022046220391387, 0.0515511643629755, 0.0783631931911916, 0.041326330213008224, 0.04826542719452908, 0.06571905429857627, 0.0220356112653753, 0.022888872209437888, 0.020984114771795626, 0.05047023510542325, 0.04549146623372382, 0.047757194349682085, 0.05031656274190385, 0.08296654193096037, 0.037743354255856884, 0.07359836305081005, 0.03537357963084057, 0.09391955625223516, 0.10528157765517375, 0.045509551094653856, 0.029904622548028992, 0.04688904116650813, 0.05754142237604361, 0.025228705509325343, 0.044062113020278036, 0.06100862760105029, 0.04891011541871861, 0.05069069523352244, 0.029651470991349366, 0.06849801720048661, 0.04894312196241788, 0.05005476543982332, 0.02734384572302033, 0.09082315358872432, 0.04793196239338046], 'lossList': [0.0, -1.2160753393173218, 0.0, 1.0821036747843027, 0.0, 0.0, 0.0], 'rewardMean': 0.7930224378489618, 'totalEpisodes': 284, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.2155384329014, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=100.64
