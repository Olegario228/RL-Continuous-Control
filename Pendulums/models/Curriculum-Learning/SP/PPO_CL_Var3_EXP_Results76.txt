#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 76
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.7334200784615094, 'errorList': [], 'lossList': [0.0, -1.4172225195169448, 0.0, 31.659447507858275, 0.0, 0.0, 0.0], 'rewardMean': 0.7742351056514584, 'totalEpisodes': 79, 'stepsPerEpisode': 35, 'rewardPerEpisode': 25.78772045718443
'totalSteps': 3840, 'rewardStep': 0.9232388691252468, 'errorList': [], 'lossList': [0.0, -1.4005122351646424, 0.0, 40.95268602371216, 0.0, 0.0, 0.0], 'rewardMean': 0.8239030268093878, 'totalEpisodes': 135, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.840380086417051
'totalSteps': 5120, 'rewardStep': 0.5679546904810076, 'errorList': [], 'lossList': [0.0, -1.3837905025482178, 0.0, 50.70364635467529, 0.0, 0.0, 0.0], 'rewardMean': 0.7599159427272928, 'totalEpisodes': 176, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.950004813282176
'totalSteps': 6400, 'rewardStep': 0.591306089218137, 'errorList': [], 'lossList': [0.0, -1.375857846736908, 0.0, 51.581365003585816, 0.0, 0.0, 0.0], 'rewardMean': 0.7261939720254617, 'totalEpisodes': 196, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.267140051835894
'totalSteps': 7680, 'rewardStep': 0.7556435065694177, 'errorList': [], 'lossList': [0.0, -1.3683776783943176, 0.0, 39.396444544792175, 0.0, 0.0, 0.0], 'rewardMean': 0.7311022277827877, 'totalEpisodes': 206, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.423760543971785
'totalSteps': 8960, 'rewardStep': 0.7856622854759197, 'errorList': [], 'lossList': [0.0, -1.3552955877780914, 0.0, 30.708206882476805, 0.0, 0.0, 0.0], 'rewardMean': 0.7388965217389494, 'totalEpisodes': 214, 'stepsPerEpisode': 276, 'rewardPerEpisode': 222.77480542944738
'totalSteps': 10240, 'rewardStep': 0.7632965503914497, 'errorList': [], 'lossList': [0.0, -1.3437632870674134, 0.0, 11.537108608484267, 0.0, 0.0, 0.0], 'rewardMean': 0.741946525320512, 'totalEpisodes': 219, 'stepsPerEpisode': 59, 'rewardPerEpisode': 48.48962894461588
'totalSteps': 11520, 'rewardStep': 0.5084451373151472, 'errorList': [], 'lossList': [0.0, -1.3413072127103804, 0.0, 11.70995600938797, 0.0, 0.0, 0.0], 'rewardMean': 0.7160019266532491, 'totalEpisodes': 223, 'stepsPerEpisode': 100, 'rewardPerEpisode': 78.96698594452225
'totalSteps': 12800, 'rewardStep': 0.7518840983032928, 'errorList': [], 'lossList': [0.0, -1.305055215358734, 0.0, 18.802259695529937, 0.0, 0.0, 0.0], 'rewardMean': 0.7195901438182536, 'totalEpisodes': 231, 'stepsPerEpisode': 117, 'rewardPerEpisode': 94.65820315736795
'totalSteps': 14080, 'rewardStep': 0.4504550238364262, 'errorList': [], 'lossList': [0.0, -1.2747094875574112, 0.0, 6.734692543745041, 0.0, 0.0, 0.0], 'rewardMean': 0.6831306329177554, 'totalEpisodes': 234, 'stepsPerEpisode': 230, 'rewardPerEpisode': 168.20021818712402
'totalSteps': 15360, 'rewardStep': 0.6587396024924868, 'errorList': [], 'lossList': [0.0, -1.2609985089302063, 0.0, 5.5867746496200565, 0.0, 0.0, 0.0], 'rewardMean': 0.6756625853208531, 'totalEpisodes': 237, 'stepsPerEpisode': 471, 'rewardPerEpisode': 378.6681395367979
'totalSteps': 16640, 'rewardStep': 0.6166585711257644, 'errorList': [], 'lossList': [0.0, -1.248444680571556, 0.0, 4.186556592583656, 0.0, 0.0, 0.0], 'rewardMean': 0.645004555520905, 'totalEpisodes': 239, 'stepsPerEpisode': 376, 'rewardPerEpisode': 308.6757235929824
'totalSteps': 17920, 'rewardStep': 0.7059703936926444, 'errorList': [], 'lossList': [0.0, -1.2499306601285936, 0.0, 3.82687073469162, 0.0, 0.0, 0.0], 'rewardMean': 0.6588061258420687, 'totalEpisodes': 241, 'stepsPerEpisode': 728, 'rewardPerEpisode': 610.1612442669401
'totalSteps': 19200, 'rewardStep': 0.918778852934717, 'errorList': [], 'lossList': [0.0, -1.221996460556984, 0.0, 1.7037877605110407, 0.0, 0.0, 0.0], 'rewardMean': 0.6915534022137266, 'totalEpisodes': 241, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.5243032820451
'totalSteps': 20480, 'rewardStep': 0.824585738363222, 'errorList': [], 'lossList': [0.0, -1.1539163899421692, 0.0, 1.9559804056584835, 0.0, 0.0, 0.0], 'rewardMean': 0.698447625393107, 'totalEpisodes': 241, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.5449643486254
'totalSteps': 21760, 'rewardStep': 0.8760804435195452, 'errorList': [], 'lossList': [0.0, -1.0765979653596878, 0.0, 1.1691327144578099, 0.0, 0.0, 0.0], 'rewardMean': 0.7074894411974696, 'totalEpisodes': 241, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1178.0856541027097
'totalSteps': 23040, 'rewardStep': 0.9850299781358275, 'errorList': [0.052911948597492826, 0.05108174004527949, 0.060692399351536815, 0.04323904910873021, 0.06600221593782053, 0.07979883975408551, 0.039460400779367764, 0.07133573569171305, 0.05925825986169802, 0.06771910764876378, 0.08145237062329944, 0.05052065204804127, 0.04886767308099114, 0.0941322383811786, 0.04592896977468359, 0.03940571300521546, 0.06355413559432277, 0.0395713043844707, 0.04108277437638856, 0.08949419640875204, 0.07409122697760386, 0.06163427800091347, 0.09952219322139455, 0.08416964848601255, 0.06300284220800334, 0.0768984618019057, 0.05121422784505816, 0.04470485775435662, 0.08209798532095189, 0.08255481349364474, 0.0893837162353174, 0.08706730207157255, 0.05292127545872999, 0.04994614560058271, 0.0870316404974129, 0.08305706172906513, 0.06268389949301478, 0.05133834883472516, 0.050708736496980385, 0.06711755914354117, 0.03917475617056892, 0.05553238290888498, 0.10938332793542214, 0.08995868648734537, 0.07017394931610706, 0.03758360693651399, 0.06753947719579148, 0.039785946123503835, 0.037751891434619586, 0.055105507843318816], 'lossList': [0.0, -1.041271002292633, 0.0, 0.7614300143904984, 0.0, 0.0, 0.0], 'rewardMean': 0.7296627839719073, 'totalEpisodes': 241, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.30743587875, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=75.1
