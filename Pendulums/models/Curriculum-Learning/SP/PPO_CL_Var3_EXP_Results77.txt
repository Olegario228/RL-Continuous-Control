#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 77
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9710371722726007, 'errorList': [], 'lossList': [0.0, -1.4123701751232147, 0.0, 27.789157934188843, 0.0, 0.0, 0.0], 'rewardMean': 0.893337257028155, 'totalEpisodes': 92, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.800467654634333
'totalSteps': 3840, 'rewardStep': 0.7454238271471918, 'errorList': [], 'lossList': [0.0, -1.3879153448343278, 0.0, 40.75541717529297, 0.0, 0.0, 0.0], 'rewardMean': 0.8440327804011672, 'totalEpisodes': 155, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.2077741667288664
'totalSteps': 5120, 'rewardStep': 0.97638390275119, 'errorList': [], 'lossList': [0.0, -1.3721607720851898, 0.0, 43.18104571342468, 0.0, 0.0, 0.0], 'rewardMean': 0.877120560988673, 'totalEpisodes': 197, 'stepsPerEpisode': 48, 'rewardPerEpisode': 37.54105647901315
'totalSteps': 6400, 'rewardStep': 0.47024742961359844, 'errorList': [], 'lossList': [0.0, -1.3533035498857497, 0.0, 47.28813391685486, 0.0, 0.0, 0.0], 'rewardMean': 0.7957459347136581, 'totalEpisodes': 216, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.356612330959237
'totalSteps': 7680, 'rewardStep': 0.8358116182080341, 'errorList': [], 'lossList': [0.0, -1.344005972146988, 0.0, 47.552979917526244, 0.0, 0.0, 0.0], 'rewardMean': 0.8024235486293874, 'totalEpisodes': 227, 'stepsPerEpisode': 56, 'rewardPerEpisode': 40.60655362257686
'totalSteps': 8960, 'rewardStep': 0.3759753090802492, 'errorList': [], 'lossList': [0.0, -1.337645702958107, 0.0, 31.602333512306213, 0.0, 0.0, 0.0], 'rewardMean': 0.741502371550939, 'totalEpisodes': 231, 'stepsPerEpisode': 135, 'rewardPerEpisode': 100.41491170194125
'totalSteps': 10240, 'rewardStep': 0.6760976533033749, 'errorList': [], 'lossList': [0.0, -1.3203885269165039, 0.0, 23.0712460398674, 0.0, 0.0, 0.0], 'rewardMean': 0.7333267817699936, 'totalEpisodes': 236, 'stepsPerEpisode': 218, 'rewardPerEpisode': 179.4396559613606
'totalSteps': 11520, 'rewardStep': 0.6573774403978594, 'errorList': [], 'lossList': [0.0, -1.3067849528789521, 0.0, 28.71433005332947, 0.0, 0.0, 0.0], 'rewardMean': 0.7248879660619786, 'totalEpisodes': 240, 'stepsPerEpisode': 398, 'rewardPerEpisode': 346.3039177442331
'totalSteps': 12800, 'rewardStep': 0.8321115073359229, 'errorList': [], 'lossList': [0.0, -1.3151319116353988, 0.0, 51.4404377579689, 0.0, 0.0, 0.0], 'rewardMean': 0.735610320189373, 'totalEpisodes': 246, 'stepsPerEpisode': 75, 'rewardPerEpisode': 65.56408501971922
'totalSteps': 14080, 'rewardStep': 0.6032183825659582, 'errorList': [], 'lossList': [0.0, -1.323161124587059, 0.0, 21.00735750436783, 0.0, 0.0, 0.0], 'rewardMean': 0.7143684242675981, 'totalEpisodes': 254, 'stepsPerEpisode': 137, 'rewardPerEpisode': 112.52208156636021
'totalSteps': 15360, 'rewardStep': 0.9117113840252057, 'errorList': [], 'lossList': [0.0, -1.3266446614265441, 0.0, 13.535700734853744, 0.0, 0.0, 0.0], 'rewardMean': 0.7084358454428584, 'totalEpisodes': 259, 'stepsPerEpisode': 54, 'rewardPerEpisode': 44.58031855836769
'totalSteps': 16640, 'rewardStep': 0.6270296418922727, 'errorList': [], 'lossList': [0.0, -1.3475586980581284, 0.0, 7.960913220643997, 0.0, 0.0, 0.0], 'rewardMean': 0.6965964269173666, 'totalEpisodes': 264, 'stepsPerEpisode': 104, 'rewardPerEpisode': 77.03598627361623
'totalSteps': 17920, 'rewardStep': 0.9178284643086346, 'errorList': [], 'lossList': [0.0, -1.3629150873422622, 0.0, 8.234966766834258, 0.0, 0.0, 0.0], 'rewardMean': 0.690740883073111, 'totalEpisodes': 268, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.34529723890754
'totalSteps': 19200, 'rewardStep': 0.7592773225227376, 'errorList': [], 'lossList': [0.0, -1.3357128328084946, 0.0, 3.265750115513802, 0.0, 0.0, 0.0], 'rewardMean': 0.7196438723640248, 'totalEpisodes': 269, 'stepsPerEpisode': 475, 'rewardPerEpisode': 359.6800258649817
'totalSteps': 20480, 'rewardStep': 0.8972250464819935, 'errorList': [], 'lossList': [0.0, -1.293795450925827, 0.0, 2.867767888754606, 0.0, 0.0, 0.0], 'rewardMean': 0.7257852151914207, 'totalEpisodes': 269, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.147376587475
'totalSteps': 21760, 'rewardStep': 0.9115017957939984, 'errorList': [], 'lossList': [0.0, -1.2408856630325318, 0.0, 1.6260287493467331, 0.0, 0.0, 0.0], 'rewardMean': 0.7793378638627957, 'totalEpisodes': 269, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1119.0611768830076
'totalSteps': 23040, 'rewardStep': 0.9772376540517037, 'errorList': [0.12567078933129444, 0.14003189360904567, 0.1551639578397779, 0.13235583146593416, 0.14795112441943836, 0.18815184731400247, 0.14560354052511884, 0.20256929641144136, 0.2322017895488918, 0.1418335334674444, 0.2940667652321408, 0.14243193567072643, 0.14471211698457448, 0.2151962882998974, 0.1367585283176272, 0.12876561019846516, 0.14894282464015138, 0.1314801022572503, 0.14457206666271682, 0.14310097365602392, 0.2647916251387897, 0.19206046191101978, 0.12610153382196784, 0.13717368346368605, 0.13167212062586883, 0.15020328111108236, 0.14148041507577422, 0.14273841320929279, 0.1429907086296671, 0.13520013281697824, 0.14274959224765002, 0.1783953318866047, 0.12638487814842023, 0.13784786462117096, 0.25837885834684626, 0.13170302027444494, 0.2762907128479642, 0.12646774064400632, 0.13181040973583508, 0.19213204479039625, 0.14324688199973312, 0.14438581132112005, 0.14441377876927144, 0.15773111453040387, 0.15651975185140868, 0.13410936888952976, 0.13500982441084342, 0.13148991204044688, 0.17175710952786943, 0.24366844248392913], 'lossList': [0.0, -1.2011876487731934, 0.0, 1.7073105942457913, 0.0, 0.0, 0.0], 'rewardMean': 0.8094518639376286, 'totalEpisodes': 269, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.6744403376608, 'successfulTests': 42
'totalSteps': 24320, 'rewardStep': 0.8907464640265589, 'errorList': [], 'lossList': [0.0, -1.1768503677845001, 0.0, 0.9060560288280248, 0.0, 0.0, 0.0], 'rewardMean': 0.8327887663004987, 'totalEpisodes': 269, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.1850972340171
'totalSteps': 25600, 'rewardStep': 0.903302457338054, 'errorList': [], 'lossList': [0.0, -1.1474046063423158, 0.0, 0.5555369880609214, 0.0, 0.0, 0.0], 'rewardMean': 0.8399078613007118, 'totalEpisodes': 269, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1163.843343478674
#maxSuccessfulTests=42, maxSuccessfulTestsAtStep=23040, timeSpent=83.18
