#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 78
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.4801983239972004, 'errorList': [], 'lossList': [0.0, -1.4312279999256134, 0.0, 27.36785617828369, 0.0, 0.0, 0.0], 'rewardMean': 0.4925687753687926, 'totalEpisodes': 88, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.3718429730647426
'totalSteps': 3840, 'rewardStep': 0.9137049668636766, 'errorList': [], 'lossList': [0.0, -1.424384434223175, 0.0, 39.872304124832155, 0.0, 0.0, 0.0], 'rewardMean': 0.6329475058670874, 'totalEpisodes': 144, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.064563930366097
'totalSteps': 5120, 'rewardStep': 0.7730053103697063, 'errorList': [], 'lossList': [0.0, -1.404407475590706, 0.0, 48.035390586853026, 0.0, 0.0, 0.0], 'rewardMean': 0.6679619569927421, 'totalEpisodes': 171, 'stepsPerEpisode': 75, 'rewardPerEpisode': 56.39713848638091
'totalSteps': 6400, 'rewardStep': 0.584604958115197, 'errorList': [], 'lossList': [0.0, -1.3798295372724534, 0.0, 52.93486614227295, 0.0, 0.0, 0.0], 'rewardMean': 0.6512905572172331, 'totalEpisodes': 188, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.7760198385203
'totalSteps': 7680, 'rewardStep': 0.7431733332444903, 'errorList': [], 'lossList': [0.0, -1.3675182729959487, 0.0, 44.185068454742435, 0.0, 0.0, 0.0], 'rewardMean': 0.6666043532217759, 'totalEpisodes': 197, 'stepsPerEpisode': 82, 'rewardPerEpisode': 60.617006027342605
'totalSteps': 8960, 'rewardStep': 0.9922509275249781, 'errorList': [47.42643182336657, 14.644815030084686, 44.50134286722636, 4.998100478886314, 57.43702840686671, 65.27730798793765, 25.894278906406615, 47.44986061387205, 70.70437163896659, 2.616036077026583, 57.22704677650262, 33.21112299390281, 19.20341456215133, 112.665149133814, 117.43105220042926, 42.10181592357839, 2.810310416413746, 10.1700541866074, 8.964946067241154, 49.87348494718563, 47.42829235237732, 47.18366997931149, 114.5544533987249, 2.088913040812359, 21.90925441495361, 48.82659832466072, 61.434558572306514, 3.9411374785567053, 0.8276701287599489, 19.631129951595778, 21.02173238850114, 30.428402143718344, 40.68425219437299, 94.39787798164967, 74.28543453616976, 78.15355671838735, 62.59310890299141, 26.599418312809025, 32.25079831148426, 43.20878533645477, 35.13540647766204, 20.074612960562803, 5.371879424371422, 77.99906582996434, 12.214638569671289, 35.322318017895114, 87.44570804292307, 12.473153909462267, 7.181893274936939, 4.491704775504296], 'lossList': [0.0, -1.359233148097992, 0.0, 22.534713215827942, 0.0, 0.0, 0.0], 'rewardMean': 0.7131252924079476, 'totalEpisodes': 203, 'stepsPerEpisode': 47, 'rewardPerEpisode': 40.836272189198425, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.8361907996736916, 'errorList': [], 'lossList': [0.0, -1.3511921113729477, 0.0, 34.972814948558806, 0.0, 0.0, 0.0], 'rewardMean': 0.7285084808161657, 'totalEpisodes': 209, 'stepsPerEpisode': 32, 'rewardPerEpisode': 25.187005555643662
'totalSteps': 11520, 'rewardStep': 0.7356899657218658, 'errorList': [], 'lossList': [0.0, -1.3459083086252213, 0.0, 12.788016757965089, 0.0, 0.0, 0.0], 'rewardMean': 0.7293064235834658, 'totalEpisodes': 213, 'stepsPerEpisode': 70, 'rewardPerEpisode': 56.49869432550786
'totalSteps': 12800, 'rewardStep': 0.6856576967533133, 'errorList': [], 'lossList': [0.0, -1.3265657114982605, 0.0, 23.166632957458496, 0.0, 0.0, 0.0], 'rewardMean': 0.7249415509004505, 'totalEpisodes': 214, 'stepsPerEpisode': 815, 'rewardPerEpisode': 575.2958865395003
'totalSteps': 14080, 'rewardStep': 0.6007530076679962, 'errorList': [], 'lossList': [0.0, -1.2934473496675492, 0.0, 9.829718455076218, 0.0, 0.0, 0.0], 'rewardMean': 0.7345229289932116, 'totalEpisodes': 216, 'stepsPerEpisode': 394, 'rewardPerEpisode': 276.2895581774557
'totalSteps': 15360, 'rewardStep': 0.7553501007080171, 'errorList': [], 'lossList': [0.0, -1.2637466448545456, 0.0, 15.099463583230973, 0.0, 0.0, 0.0], 'rewardMean': 0.7620381066642932, 'totalEpisodes': 217, 'stepsPerEpisode': 1128, 'rewardPerEpisode': 802.9944786661723
'totalSteps': 16640, 'rewardStep': 0.5259520486842665, 'errorList': [], 'lossList': [0.0, -1.2503394609689713, 0.0, 3.638500935435295, 0.0, 0.0, 0.0], 'rewardMean': 0.7232628148463522, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 878.608596129322
'totalSteps': 17920, 'rewardStep': 0.8220730229850141, 'errorList': [], 'lossList': [0.0, -1.2330669105052947, 0.0, 2.200854332149029, 0.0, 0.0, 0.0], 'rewardMean': 0.7281695861078831, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.5409094769016
'totalSteps': 19200, 'rewardStep': 0.8752557190804029, 'errorList': [], 'lossList': [0.0, -1.1728423726558685, 0.0, 1.6488727282732725, 0.0, 0.0, 0.0], 'rewardMean': 0.7572346622044037, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.7547670828335
'totalSteps': 20480, 'rewardStep': 0.9044919613953725, 'errorList': [], 'lossList': [0.0, -1.1303554558753968, 0.0, 1.8781421107426286, 0.0, 0.0, 0.0], 'rewardMean': 0.7733665250194918, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.7058869674038
'totalSteps': 21760, 'rewardStep': 0.9058797988571375, 'errorList': [], 'lossList': [0.0, -1.1027402794361114, 0.0, 0.9578240850381553, 0.0, 0.0, 0.0], 'rewardMean': 0.7647294121527077, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.74844506173
'totalSteps': 23040, 'rewardStep': 0.8951154359665826, 'errorList': [], 'lossList': [0.0, -1.0761566746234894, 0.0, 1.1324785183370114, 0.0, 0.0, 0.0], 'rewardMean': 0.7706218757819968, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1207.368042795448
'totalSteps': 24320, 'rewardStep': 0.8916869740946658, 'errorList': [], 'lossList': [0.0, -1.0354939305782318, 0.0, 0.5712098732031882, 0.0, 0.0, 0.0], 'rewardMean': 0.7862215766192768, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.2344980632859
'totalSteps': 25600, 'rewardStep': 0.9663354960735289, 'errorList': [0.18068316441321255, 0.11407320747295865, 0.09250758721794772, 0.14087776316839115, 0.172417939168669, 0.11010801015082443, 0.21051669124791092, 0.10627116012933296, 0.15756895230274684, 0.15610220269696712, 0.19485237484380766, 0.11438077227964982, 0.11435371274390393, 0.20343331056496688, 0.149991612778334, 0.13295780567656057, 0.16927606545067062, 0.15787911956745015, 0.16298619761951366, 0.18932616765112353, 0.13805889492391588, 0.20035102819023012, 0.11527746798024609, 0.13778467757513335, 0.14153071923146307, 0.10848233161405091, 0.12487443748563347, 0.16656799962841853, 0.11359868289532243, 0.1708453132349302, 0.1712787849360121, 0.11682237287471464, 0.16832543214081516, 0.15943324612431906, 0.16638256748704203, 0.11878815708900757, 0.1786740421242961, 0.153471200033414, 0.10983246954334473, 0.12373388851130272, 0.13556224102994985, 0.13725129927496194, 0.16035700625449328, 0.18363896915703473, 0.14099504586922332, 0.14314327818912229, 0.16287835300317405, 0.19257194095031563, 0.20138134045503178, 0.1839083242641786], 'lossList': [0.0, -1.013520844578743, 0.0, 0.40642328687012197, 0.0, 0.0, 0.0], 'rewardMean': 0.8142893565512985, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.9316112154927, 'successfulTests': 46
#maxSuccessfulTests=46, maxSuccessfulTestsAtStep=25600, timeSpent=101.18
