#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 79
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.81899321112296, 'errorList': [], 'lossList': [0.0, -1.3976136940717696, 0.0, 31.545468473434447, 0.0, 0.0, 0.0], 'rewardMean': 0.8700416726371836, 'totalEpisodes': 89, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.52223405552315
'totalSteps': 3840, 'rewardStep': 0.6885580505178963, 'errorList': [], 'lossList': [0.0, -1.3816916251182556, 0.0, 41.89755058288574, 0.0, 0.0, 0.0], 'rewardMean': 0.8095471319307546, 'totalEpisodes': 145, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.714965897647371
'totalSteps': 5120, 'rewardStep': 0.9589084100825995, 'errorList': [], 'lossList': [0.0, -1.3699335879087449, 0.0, 48.582374057769776, 0.0, 0.0, 0.0], 'rewardMean': 0.8468874514687158, 'totalEpisodes': 173, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.52748618808974
'totalSteps': 6400, 'rewardStep': 0.8570192158685075, 'errorList': [], 'lossList': [0.0, -1.3561247527599334, 0.0, 46.3897901725769, 0.0, 0.0, 0.0], 'rewardMean': 0.8489138043486741, 'totalEpisodes': 187, 'stepsPerEpisode': 37, 'rewardPerEpisode': 31.75581901024673
'totalSteps': 7680, 'rewardStep': 0.49420547188918573, 'errorList': [], 'lossList': [0.0, -1.3456153053045272, 0.0, 37.291800594329835, 0.0, 0.0, 0.0], 'rewardMean': 0.7897957489387594, 'totalEpisodes': 194, 'stepsPerEpisode': 79, 'rewardPerEpisode': 50.14496750414433
'totalSteps': 8960, 'rewardStep': 0.8582670562372368, 'errorList': [], 'lossList': [0.0, -1.336778022646904, 0.0, 39.707383708953856, 0.0, 0.0, 0.0], 'rewardMean': 0.7995773642671133, 'totalEpisodes': 202, 'stepsPerEpisode': 304, 'rewardPerEpisode': 242.1412689904385
'totalSteps': 10240, 'rewardStep': 0.7680605188276295, 'errorList': [], 'lossList': [0.0, -1.3379991322755813, 0.0, 16.12202308177948, 0.0, 0.0, 0.0], 'rewardMean': 0.7956377585871778, 'totalEpisodes': 206, 'stepsPerEpisode': 497, 'rewardPerEpisode': 407.51765613051674
'totalSteps': 11520, 'rewardStep': 0.9238931196324881, 'errorList': [], 'lossList': [0.0, -1.3170783519744873, 0.0, 11.986370087862015, 0.0, 0.0, 0.0], 'rewardMean': 0.809888354258879, 'totalEpisodes': 210, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.592327778510924
'totalSteps': 12800, 'rewardStep': 0.8736541202598233, 'errorList': [], 'lossList': [0.0, -1.3028271985054016, 0.0, 22.89531094789505, 0.0, 0.0, 0.0], 'rewardMean': 0.8162649308589733, 'totalEpisodes': 216, 'stepsPerEpisode': 66, 'rewardPerEpisode': 56.19404792347187
'totalSteps': 14080, 'rewardStep': 0.4189325937593993, 'errorList': [], 'lossList': [0.0, -1.3025199669599532, 0.0, 5.773422132134438, 0.0, 0.0, 0.0], 'rewardMean': 0.7660491768197726, 'totalEpisodes': 221, 'stepsPerEpisode': 155, 'rewardPerEpisode': 105.53289275078568
'totalSteps': 15360, 'rewardStep': 0.7575237263658045, 'errorList': [], 'lossList': [0.0, -1.2931867450475694, 0.0, 35.08492988109589, 0.0, 0.0, 0.0], 'rewardMean': 0.7599022283440571, 'totalEpisodes': 228, 'stepsPerEpisode': 209, 'rewardPerEpisode': 186.325721317478
'totalSteps': 16640, 'rewardStep': 0.674762255245806, 'errorList': [], 'lossList': [0.0, -1.2833629089593888, 0.0, 25.704126007556916, 0.0, 0.0, 0.0], 'rewardMean': 0.758522648816848, 'totalEpisodes': 231, 'stepsPerEpisode': 88, 'rewardPerEpisode': 75.61927078808378
'totalSteps': 17920, 'rewardStep': 0.8854409329080037, 'errorList': [], 'lossList': [0.0, -1.2522324579954147, 0.0, 2.818787932395935, 0.0, 0.0, 0.0], 'rewardMean': 0.7511759010993885, 'totalEpisodes': 235, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.2345632147376655
'totalSteps': 19200, 'rewardStep': 0.5496594378610229, 'errorList': [], 'lossList': [0.0, -1.2176084041595459, 0.0, 3.5962877160310747, 0.0, 0.0, 0.0], 'rewardMean': 0.7204399232986399, 'totalEpisodes': 236, 'stepsPerEpisode': 754, 'rewardPerEpisode': 582.3893870646907
'totalSteps': 20480, 'rewardStep': 0.9095475060306853, 'errorList': [], 'lossList': [0.0, -1.1983014416694642, 0.0, 2.8649528512358664, 0.0, 0.0, 0.0], 'rewardMean': 0.7619741267127901, 'totalEpisodes': 239, 'stepsPerEpisode': 34, 'rewardPerEpisode': 27.097032048630506
'totalSteps': 21760, 'rewardStep': 0.6988963856305744, 'errorList': [], 'lossList': [0.0, -1.1799209886789321, 0.0, 2.4224872040748595, 0.0, 0.0, 0.0], 'rewardMean': 0.7460370596521237, 'totalEpisodes': 240, 'stepsPerEpisode': 283, 'rewardPerEpisode': 221.70512011599124
'totalSteps': 23040, 'rewardStep': 0.9180531731938785, 'errorList': [], 'lossList': [0.0, -1.1708650302886963, 0.0, 2.651192912310362, 0.0, 0.0, 0.0], 'rewardMean': 0.7610363250887485, 'totalEpisodes': 242, 'stepsPerEpisode': 236, 'rewardPerEpisode': 215.20916905994522
'totalSteps': 24320, 'rewardStep': 0.8058805344155882, 'errorList': [], 'lossList': [0.0, -1.1498894745111465, 0.0, 0.7829233113676309, 0.0, 0.0, 0.0], 'rewardMean': 0.7492350665670585, 'totalEpisodes': 242, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.5684936337734
'totalSteps': 25600, 'rewardStep': 0.9689484374088039, 'errorList': [0.016906601871216714, 0.011534873541780611, 0.006264880671369766, 0.01753157692560156, 0.012260330027460183, 0.006641678276045426, 0.008546795828318998, 0.006298466738373444, 0.010503229919996852, 0.012738416446692621, 0.007619015411072324, 0.009799389520944543, 0.0095832010516371, 0.006209439818917972, 0.009276243452995653, 0.025726468948576435, 0.009383399037353485, 0.014270109186650193, 0.007426098905296034, 0.025529268666680358, 0.014225563957022569, 0.007230746041532432, 0.024951540810574026, 0.009163004184569636, 0.00896074135821149, 0.01125588478757738, 0.029335022021371473, 0.025631114150815735, 0.01935188223328443, 0.019022832813667376, 0.017121483568304976, 0.024049215395287055, 0.034721314871395084, 0.02674396789600516, 0.039506483139876376, 0.009646748167101581, 0.006534457647624068, 0.018182745470768195, 0.010807246437832123, 0.0068439388776105415, 0.026581100608304153, 0.02459993733030366, 0.009543916951690686, 0.017845046900667592, 0.009595224278381668, 0.03205974904128186, 0.007605067516004538, 0.007924182004117345, 0.029593667282154082, 0.0405740791820054], 'lossList': [0.0, -1.1092797243595123, 0.0, 0.861706525683403, 0.0, 0.0, 0.0], 'rewardMean': 0.7587644982819566, 'totalEpisodes': 242, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.8640603946362, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=83.0
