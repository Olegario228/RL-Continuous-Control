#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 81
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6650994867372976, 'errorList': [], 'lossList': [0.0, -1.4136366140842438, 0.0, 35.65210670471191, 0.0, 0.0, 0.0], 'rewardMean': 0.8141945193240037, 'totalEpisodes': 46, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.90446512247082
'totalSteps': 3840, 'rewardStep': 0.43173632196603706, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6229654206450204, 'totalEpisodes': 94, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.681577968984566
'totalSteps': 5120, 'rewardStep': 0.7470492372330285, 'errorList': [], 'lossList': [0.0, -1.4015705090761186, 0.0, 42.98262942314148, 0.0, 0.0, 0.0], 'rewardMean': 0.647782183962622, 'totalEpisodes': 157, 'stepsPerEpisode': 23, 'rewardPerEpisode': 16.866361726597017
'totalSteps': 6400, 'rewardStep': 0.879542094850873, 'errorList': [], 'lossList': [0.0, -1.3820140635967255, 0.0, 48.363873901367185, 0.0, 0.0, 0.0], 'rewardMean': 0.6864088357773305, 'totalEpisodes': 195, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.540221625357749
'totalSteps': 7680, 'rewardStep': 0.480489818361832, 'errorList': [], 'lossList': [0.0, -1.3533622306585311, 0.0, 44.171521520614625, 0.0, 0.0, 0.0], 'rewardMean': 0.6569918332894021, 'totalEpisodes': 209, 'stepsPerEpisode': 128, 'rewardPerEpisode': 91.51464985759132
'totalSteps': 8960, 'rewardStep': 0.7733637588019272, 'errorList': [], 'lossList': [0.0, -1.3209059792757034, 0.0, 39.737125544548036, 0.0, 0.0, 0.0], 'rewardMean': 0.6715383239784678, 'totalEpisodes': 221, 'stepsPerEpisode': 111, 'rewardPerEpisode': 92.65609478140767
'totalSteps': 10240, 'rewardStep': 0.7611376226326335, 'errorList': [], 'lossList': [0.0, -1.298905646800995, 0.0, 32.504600489139555, 0.0, 0.0, 0.0], 'rewardMean': 0.6814938016067084, 'totalEpisodes': 226, 'stepsPerEpisode': 54, 'rewardPerEpisode': 46.54826212702124
'totalSteps': 11520, 'rewardStep': 0.8574454833627244, 'errorList': [], 'lossList': [0.0, -1.2923291736841203, 0.0, 18.62290547132492, 0.0, 0.0, 0.0], 'rewardMean': 0.6990889697823099, 'totalEpisodes': 232, 'stepsPerEpisode': 36, 'rewardPerEpisode': 28.749065897052546
'totalSteps': 12800, 'rewardStep': 0.526688901375775, 'errorList': [], 'lossList': [0.0, -1.271431034207344, 0.0, 29.644264187812805, 0.0, 0.0, 0.0], 'rewardMean': 0.6554289047288165, 'totalEpisodes': 240, 'stepsPerEpisode': 153, 'rewardPerEpisode': 102.58081024093198
'totalSteps': 14080, 'rewardStep': 0.6604205449127813, 'errorList': [], 'lossList': [0.0, -1.2443222373723983, 0.0, 17.45715195417404, 0.0, 0.0, 0.0], 'rewardMean': 0.654961010546365, 'totalEpisodes': 245, 'stepsPerEpisode': 230, 'rewardPerEpisode': 179.8973253502475
'totalSteps': 15360, 'rewardStep': 0.8634439881298512, 'errorList': [], 'lossList': [0.0, -1.2113134253025055, 0.0, 7.819819545745849, 0.0, 0.0, 0.0], 'rewardMean': 0.6981317771627464, 'totalEpisodes': 248, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.967186161243085
'totalSteps': 16640, 'rewardStep': 0.7579610344922308, 'errorList': [], 'lossList': [0.0, -1.1866121715307236, 0.0, 5.7029408884048465, 0.0, 0.0, 0.0], 'rewardMean': 0.7307542484153657, 'totalEpisodes': 250, 'stepsPerEpisode': 455, 'rewardPerEpisode': 374.6618075583047
'totalSteps': 17920, 'rewardStep': 0.7779492614399, 'errorList': [], 'lossList': [0.0, -1.162593513727188, 0.0, 3.6351380433142184, 0.0, 0.0, 0.0], 'rewardMean': 0.733844250836053, 'totalEpisodes': 251, 'stepsPerEpisode': 148, 'rewardPerEpisode': 126.7717223671261
'totalSteps': 19200, 'rewardStep': 0.6785708473122691, 'errorList': [], 'lossList': [0.0, -1.1329724419116973, 0.0, 3.3114672353863717, 0.0, 0.0, 0.0], 'rewardMean': 0.7137471260821926, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 981.3771828887561
'totalSteps': 20480, 'rewardStep': 0.7862795857772122, 'errorList': [], 'lossList': [0.0, -1.1162498056888581, 0.0, 2.9791673111915586, 0.0, 0.0, 0.0], 'rewardMean': 0.7443261028237305, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 986.6703114310418
'totalSteps': 21760, 'rewardStep': 0.7370881201132413, 'errorList': [], 'lossList': [0.0, -1.1081758171319962, 0.0, 1.4379364103078842, 0.0, 0.0, 0.0], 'rewardMean': 0.7406985389548619, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.6429579825874
'totalSteps': 23040, 'rewardStep': 0.9339820504991724, 'errorList': [0.08574429061943284, 0.11055291067108984, 0.06034668098192751, 0.06570581088706229, 0.08148614381871802, 0.05389537324026706, 0.08803463467195406, 0.08639551569773239, 0.07296660941665942, 0.05269086002451323, 0.08832666686379334, 0.07515777578055559, 0.05098006054681455, 0.08364334061130989, 0.0521540033873888, 0.059871614717260616, 0.05485288284815997, 0.08451818944410576, 0.05765886556683814, 0.08577739865084653, 0.07269951587421163, 0.06245948894427741, 0.06613951470577346, 0.05381614641817196, 0.09877663181048404, 0.056123933257485555, 0.06593968506218825, 0.071680681861089, 0.09555254444057172, 0.06988339463842452, 0.09969881780301762, 0.09746775320996348, 0.05672278056224863, 0.05600556163900996, 0.05793349086190319, 0.07190939214310808, 0.09595706848675485, 0.07306633606039648, 0.06139017307295783, 0.08979786213667777, 0.057448766259317274, 0.0886300506083567, 0.05780859177207894, 0.0887929762925528, 0.08410847300587009, 0.06721277099483954, 0.07495680295728502, 0.06650374243576265, 0.05383413848567952, 0.0649813247396178], 'lossList': [0.0, -1.1008006101846695, 0.0, 0.8239265850186348, 0.0, 0.0, 0.0], 'rewardMean': 0.7579829817415158, 'totalEpisodes': 251, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.6178993633737, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=72.97
