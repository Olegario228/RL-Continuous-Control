#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 83
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.5673835305781508, 'errorList': [], 'lossList': [0.0, -1.4297985976934433, 0.0, 33.603269004821776, 0.0, 0.0, 0.0], 'rewardMean': 0.45586936106180287, 'totalEpisodes': 45, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5673835305781508
'totalSteps': 3840, 'rewardStep': 0.5808597577006026, 'errorList': [], 'lossList': [0.0, -1.4292005091905593, 0.0, 46.30855237007141, 0.0, 0.0, 0.0], 'rewardMean': 0.49753282660806947, 'totalEpisodes': 92, 'stepsPerEpisode': 17, 'rewardPerEpisode': 10.68127976744589
'totalSteps': 5120, 'rewardStep': 0.6541922275275557, 'errorList': [], 'lossList': [0.0, -1.4146679812669753, 0.0, 48.19960619926453, 0.0, 0.0, 0.0], 'rewardMean': 0.536697676837941, 'totalEpisodes': 130, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.586623270936476
'totalSteps': 6400, 'rewardStep': 0.8950624194976907, 'errorList': [], 'lossList': [0.0, -1.4019094860553742, 0.0, 39.100474948883054, 0.0, 0.0, 0.0], 'rewardMean': 0.608370625369891, 'totalEpisodes': 146, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.228231310156271
'totalSteps': 7680, 'rewardStep': 0.9663517827666035, 'errorList': [], 'lossList': [0.0, -1.3750123471021651, 0.0, 58.7161294555664, 0.0, 0.0, 0.0], 'rewardMean': 0.6680341516026763, 'totalEpisodes': 159, 'stepsPerEpisode': 84, 'rewardPerEpisode': 67.6934192702484
'totalSteps': 8960, 'rewardStep': 0.7783307811902063, 'errorList': [], 'lossList': [0.0, -1.3519975227117538, 0.0, 65.23934419631958, 0.0, 0.0, 0.0], 'rewardMean': 0.6837908129723235, 'totalEpisodes': 171, 'stepsPerEpisode': 107, 'rewardPerEpisode': 83.2768884528607
'totalSteps': 10240, 'rewardStep': 0.899955583783079, 'errorList': [], 'lossList': [0.0, -1.3602738845348359, 0.0, 44.25992219924927, 0.0, 0.0, 0.0], 'rewardMean': 0.7108114093236679, 'totalEpisodes': 178, 'stepsPerEpisode': 28, 'rewardPerEpisode': 23.63584558504387
'totalSteps': 11520, 'rewardStep': 0.7891426977869644, 'errorList': [], 'lossList': [0.0, -1.3622340208292008, 0.0, 70.41037782669068, 0.0, 0.0, 0.0], 'rewardMean': 0.7195148858195898, 'totalEpisodes': 186, 'stepsPerEpisode': 29, 'rewardPerEpisode': 20.02062996057864
'totalSteps': 12800, 'rewardStep': 0.7290326404788516, 'errorList': [], 'lossList': [0.0, -1.3466514617204666, 0.0, 41.493719124794005, 0.0, 0.0, 0.0], 'rewardMean': 0.720466661285516, 'totalEpisodes': 194, 'stepsPerEpisode': 188, 'rewardPerEpisode': 156.2535020635451
'totalSteps': 14080, 'rewardStep': 0.5261745000165524, 'errorList': [], 'lossList': [0.0, -1.3345781028270722, 0.0, 16.584517349004745, 0.0, 0.0, 0.0], 'rewardMean': 0.7386485921326258, 'totalEpisodes': 200, 'stepsPerEpisode': 232, 'rewardPerEpisode': 177.39379867845918
'totalSteps': 15360, 'rewardStep': 0.7617620006084047, 'errorList': [], 'lossList': [0.0, -1.3263161259889602, 0.0, 9.555765085220337, 0.0, 0.0, 0.0], 'rewardMean': 0.7580864391356511, 'totalEpisodes': 205, 'stepsPerEpisode': 223, 'rewardPerEpisode': 184.9421878720208
'totalSteps': 16640, 'rewardStep': 0.9114322033160234, 'errorList': [], 'lossList': [0.0, -1.3288700878620148, 0.0, 7.82351818561554, 0.0, 0.0, 0.0], 'rewardMean': 0.7911436836971932, 'totalEpisodes': 212, 'stepsPerEpisode': 63, 'rewardPerEpisode': 51.58867429756124
'totalSteps': 17920, 'rewardStep': 0.8518781298278639, 'errorList': [], 'lossList': [0.0, -1.3417548775672912, 0.0, 5.4831811761856075, 0.0, 0.0, 0.0], 'rewardMean': 0.8109122739272239, 'totalEpisodes': 216, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.302492601560992
'totalSteps': 19200, 'rewardStep': 0.8231343020499772, 'errorList': [], 'lossList': [0.0, -1.3246737295389175, 0.0, 9.19766127228737, 0.0, 0.0, 0.0], 'rewardMean': 0.8037194621824527, 'totalEpisodes': 221, 'stepsPerEpisode': 30, 'rewardPerEpisode': 27.089083422395607
'totalSteps': 20480, 'rewardStep': 0.8777089629292525, 'errorList': [], 'lossList': [0.0, -1.3057819509506225, 0.0, 5.3302970945835115, 0.0, 0.0, 0.0], 'rewardMean': 0.7948551801987176, 'totalEpisodes': 222, 'stepsPerEpisode': 759, 'rewardPerEpisode': 629.6623485529102
'totalSteps': 21760, 'rewardStep': 0.7742721795006016, 'errorList': [], 'lossList': [0.0, -1.291936265230179, 0.0, 4.359564391970634, 0.0, 0.0, 0.0], 'rewardMean': 0.7944493200297571, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.2306792382649
'totalSteps': 23040, 'rewardStep': 0.8640745869056422, 'errorList': [], 'lossList': [0.0, -1.2418877816200256, 0.0, 1.6608373825252056, 0.0, 0.0, 0.0], 'rewardMean': 0.7908612203420133, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.936329557676
'totalSteps': 24320, 'rewardStep': 0.7531607352795203, 'errorList': [], 'lossList': [0.0, -1.1879960405826568, 0.0, 1.4276056388765574, 0.0, 0.0, 0.0], 'rewardMean': 0.787263024091269, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.1045976366613
'totalSteps': 25600, 'rewardStep': 0.9733129968846724, 'errorList': [0.06006636136909127, 0.072764451983241, 0.05211726053262144, 0.05441611276330656, 0.057397563272024076, 0.0650923390602206, 0.051157025635879295, 0.049881397054346514, 0.05104910906094754, 0.05787385336301017, 0.06226189557429136, 0.06476447497912034, 0.06719146344044892, 0.05787892089313739, 0.06340091246762958, 0.0599648509875176, 0.05131383427699331, 0.053742758955630666, 0.07695162644705072, 0.051411445045528845, 0.051954889138034074, 0.05298814155940217, 0.052804959911218356, 0.05217539431898658, 0.0730024105979575, 0.06650329385113128, 0.063609896471901, 0.057653191775810915, 0.0503250099115388, 0.05379070590501876, 0.051170630628864094, 0.05160305454917976, 0.051493814589328876, 0.05049410130956818, 0.054308298661680805, 0.05882259814710851, 0.07516639695414162, 0.05061094216265707, 0.0613648243955524, 0.050155774431639474, 0.0650227226006645, 0.05775202274498596, 0.05440142968245446, 0.06121581048080329, 0.07038452953663381, 0.05576941266338003, 0.0594832513409869, 0.05008638156373929, 0.06435461134215886, 0.07178557975287155], 'lossList': [0.0, -1.1604266631603242, 0.0, 1.0901502406597137, 0.0, 0.0, 0.0], 'rewardMean': 0.811691059731851, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.6111089674773, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.82
