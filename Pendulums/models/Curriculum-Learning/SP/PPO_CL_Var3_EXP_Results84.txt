#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 84
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7832765124035721, 'errorList': [], 'lossList': [0.0, -1.404241160750389, 0.0, 32.634816579818725, 0.0, 0.0, 0.0], 'rewardMean': 0.5736733090895804, 'totalEpisodes': 51, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.078718681121924
'totalSteps': 3840, 'rewardStep': 0.5275368098131541, 'errorList': [], 'lossList': [0.0, -1.38846813082695, 0.0, 41.21722487449646, 0.0, 0.0, 0.0], 'rewardMean': 0.5582944759974383, 'totalEpisodes': 110, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.000188088709596
'totalSteps': 5120, 'rewardStep': 0.9742575356100023, 'errorList': [], 'lossList': [0.0, -1.365030802488327, 0.0, 51.01211908340454, 0.0, 0.0, 0.0], 'rewardMean': 0.6622852409005793, 'totalEpisodes': 150, 'stepsPerEpisode': 46, 'rewardPerEpisode': 40.53271149548061
'totalSteps': 6400, 'rewardStep': 0.5287377425858165, 'errorList': [], 'lossList': [0.0, -1.3593467009067535, 0.0, 46.65417855262756, 0.0, 0.0, 0.0], 'rewardMean': 0.6355757412376267, 'totalEpisodes': 166, 'stepsPerEpisode': 35, 'rewardPerEpisode': 25.57058116860821
'totalSteps': 7680, 'rewardStep': 0.7543910834237082, 'errorList': [], 'lossList': [0.0, -1.3504188758134843, 0.0, 42.82097478866577, 0.0, 0.0, 0.0], 'rewardMean': 0.6553782982686402, 'totalEpisodes': 178, 'stepsPerEpisode': 29, 'rewardPerEpisode': 22.971529767586777
'totalSteps': 8960, 'rewardStep': 0.780142367734558, 'errorList': [], 'lossList': [0.0, -1.3384348672628403, 0.0, 49.57522701263428, 0.0, 0.0, 0.0], 'rewardMean': 0.6732017367637714, 'totalEpisodes': 191, 'stepsPerEpisode': 112, 'rewardPerEpisode': 83.22548383931567
'totalSteps': 10240, 'rewardStep': 0.8456084481271907, 'errorList': [], 'lossList': [0.0, -1.3317106872797013, 0.0, 17.772529512643814, 0.0, 0.0, 0.0], 'rewardMean': 0.6947525756841988, 'totalEpisodes': 197, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.59883595681043
'totalSteps': 11520, 'rewardStep': 0.5456940511230264, 'errorList': [], 'lossList': [0.0, -1.3152448868751525, 0.0, 7.695432166457176, 0.0, 0.0, 0.0], 'rewardMean': 0.6781905173996241, 'totalEpisodes': 199, 'stepsPerEpisode': 365, 'rewardPerEpisode': 236.8748830164672
'totalSteps': 12800, 'rewardStep': 0.7065163927064608, 'errorList': [], 'lossList': [0.0, -1.2804896581172942, 0.0, 19.97764399290085, 0.0, 0.0, 0.0], 'rewardMean': 0.6810231049303078, 'totalEpisodes': 206, 'stepsPerEpisode': 64, 'rewardPerEpisode': 45.458241880335855
'totalSteps': 14080, 'rewardStep': 0.8326592328644072, 'errorList': [], 'lossList': [0.0, -1.256254763007164, 0.0, 5.789390890002251, 0.0, 0.0, 0.0], 'rewardMean': 0.7278820176391896, 'totalEpisodes': 212, 'stepsPerEpisode': 86, 'rewardPerEpisode': 71.17369644880091
'totalSteps': 15360, 'rewardStep': 0.621480921909108, 'errorList': [], 'lossList': [0.0, -1.2435629403591155, 0.0, 13.391423501968383, 0.0, 0.0, 0.0], 'rewardMean': 0.7117024585897432, 'totalEpisodes': 216, 'stepsPerEpisode': 220, 'rewardPerEpisode': 169.41894394596366
'totalSteps': 16640, 'rewardStep': 0.4734339020196086, 'errorList': [], 'lossList': [0.0, -1.2298724818229676, 0.0, 4.379371995925903, 0.0, 0.0, 0.0], 'rewardMean': 0.7062921678103887, 'totalEpisodes': 218, 'stepsPerEpisode': 757, 'rewardPerEpisode': 624.6222569481023
'totalSteps': 17920, 'rewardStep': 0.7818570874681411, 'errorList': [], 'lossList': [0.0, -1.2207478487491608, 0.0, 3.377732890844345, 0.0, 0.0, 0.0], 'rewardMean': 0.6870521229962024, 'totalEpisodes': 219, 'stepsPerEpisode': 116, 'rewardPerEpisode': 103.32531587792514
'totalSteps': 19200, 'rewardStep': 0.7365016419183689, 'errorList': [], 'lossList': [0.0, -1.2087168258428573, 0.0, 2.3525801102817057, 0.0, 0.0, 0.0], 'rewardMean': 0.7078285129294578, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 918.639645003751
'totalSteps': 20480, 'rewardStep': 0.7514023306609909, 'errorList': [], 'lossList': [0.0, -1.149970400929451, 0.0, 2.157496534585953, 0.0, 0.0, 0.0], 'rewardMean': 0.7075296376531861, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1028.7675021357525
'totalSteps': 21760, 'rewardStep': 0.835606989830656, 'errorList': [], 'lossList': [0.0, -1.0987434375286103, 0.0, 1.6925966642796992, 0.0, 0.0, 0.0], 'rewardMean': 0.7130760998627959, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.012597616988
'totalSteps': 23040, 'rewardStep': 0.9187730701399367, 'errorList': [], 'lossList': [0.0, -1.0528470504283904, 0.0, 1.0197780791670084, 0.0, 0.0, 0.0], 'rewardMean': 0.7203925620640704, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.4248636059829
'totalSteps': 24320, 'rewardStep': 0.9261167055904186, 'errorList': [], 'lossList': [0.0, -1.0108522170782088, 0.0, 0.9517209322750568, 0.0, 0.0, 0.0], 'rewardMean': 0.7584348275108097, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1195.8410223108083
'totalSteps': 25600, 'rewardStep': 0.9595674062547749, 'errorList': [0.03664677190700199, 0.09591998570217868, 0.03981443966870835, 0.06128194400897579, 0.05523021202419622, 0.0380369876674145, 0.044273060223062245, 0.04014688979427514, 0.03942618069993859, 0.03906048702186689, 0.040042761014949586, 0.04104939297859849, 0.04068612397031784, 0.05120241422143477, 0.039770619933268886, 0.04038017966724, 0.03739733907747943, 0.05849295296046469, 0.040384523163174, 0.04860354702082743, 0.04017184538084365, 0.05937495740433155, 0.03988442676865227, 0.041698511533590714, 0.05823258154596694, 0.05617328069345893, 0.03953030235660481, 0.04547089126096044, 0.03734960542591547, 0.039735833438471285, 0.04019050335889697, 0.07666159581995333, 0.05216273327467832, 0.05044275432123786, 0.06751706747500577, 0.04008476957400421, 0.04140260584001497, 0.08828045988569039, 0.04027099871185491, 0.05746433245578917, 0.07598030086203408, 0.03702904416055576, 0.03613077013626382, 0.10114712531165221, 0.03918403463027053, 0.045526720161506905, 0.038085797951283785, 0.04091542899113563, 0.03975294785528903, 0.04045766173491179], 'lossList': [0.0, -0.9863352611660957, 0.0, 0.5880676253326237, 0.0, 0.0, 0.0], 'rewardMean': 0.783739928865641, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1195.9357829874295, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.1
