#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 86
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.5294477877890668, 'errorList': [], 'lossList': [0.0, -1.4069126331806183, 0.0, 32.76018811225891, 0.0, 0.0, 0.0], 'rewardMean': 0.5231208447442598, 'totalEpisodes': 38, 'stepsPerEpisode': 33, 'rewardPerEpisode': 23.23266264674354
'totalSteps': 3840, 'rewardStep': 0.8004831200180205, 'errorList': [], 'lossList': [0.0, -1.3893519443273545, 0.0, 50.042002506256104, 0.0, 0.0, 0.0], 'rewardMean': 0.61557493650218, 'totalEpisodes': 88, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.102481330894864
'totalSteps': 5120, 'rewardStep': 0.641434112309066, 'errorList': [], 'lossList': [0.0, -1.3706833112239838, 0.0, 54.352016830444335, 0.0, 0.0, 0.0], 'rewardMean': 0.6220397304539015, 'totalEpisodes': 132, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.960214807745393
'totalSteps': 6400, 'rewardStep': 0.7668312580640624, 'errorList': [], 'lossList': [0.0, -1.3585833019018174, 0.0, 55.12412689208984, 0.0, 0.0, 0.0], 'rewardMean': 0.6509980359759336, 'totalEpisodes': 156, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.208571053782899
'totalSteps': 7680, 'rewardStep': 0.6164543155657394, 'errorList': [], 'lossList': [0.0, -1.3451932817697525, 0.0, 24.35914801955223, 0.0, 0.0, 0.0], 'rewardMean': 0.6452407492409012, 'totalEpisodes': 163, 'stepsPerEpisode': 174, 'rewardPerEpisode': 125.80177072106201
'totalSteps': 8960, 'rewardStep': 0.7395191307572915, 'errorList': [], 'lossList': [0.0, -1.313604355454445, 0.0, 40.82986471176147, 0.0, 0.0, 0.0], 'rewardMean': 0.6587090894575285, 'totalEpisodes': 173, 'stepsPerEpisode': 110, 'rewardPerEpisode': 91.31942064168031
'totalSteps': 10240, 'rewardStep': 0.707780146552377, 'errorList': [], 'lossList': [0.0, -1.2987178015708922, 0.0, 21.106420866250993, 0.0, 0.0, 0.0], 'rewardMean': 0.6648429715943845, 'totalEpisodes': 179, 'stepsPerEpisode': 55, 'rewardPerEpisode': 45.24542830043757
'totalSteps': 11520, 'rewardStep': 0.7259585683156659, 'errorList': [], 'lossList': [0.0, -1.293520695567131, 0.0, 24.23401558637619, 0.0, 0.0, 0.0], 'rewardMean': 0.6716335934523047, 'totalEpisodes': 186, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.933116708954682
'totalSteps': 12800, 'rewardStep': 0.5773646842417237, 'errorList': [], 'lossList': [0.0, -1.262022751569748, 0.0, 21.74224076986313, 0.0, 0.0, 0.0], 'rewardMean': 0.6622067025312466, 'totalEpisodes': 191, 'stepsPerEpisode': 165, 'rewardPerEpisode': 126.16981722328227
'totalSteps': 14080, 'rewardStep': 0.7429183400305619, 'errorList': [], 'lossList': [0.0, -1.2435479658842086, 0.0, 13.889259142875671, 0.0, 0.0, 0.0], 'rewardMean': 0.6848191463643575, 'totalEpisodes': 195, 'stepsPerEpisode': 232, 'rewardPerEpisode': 198.17821451174441
'totalSteps': 15360, 'rewardStep': 0.657670997305387, 'errorList': [], 'lossList': [0.0, -1.2208880949020386, 0.0, 5.120563976764679, 0.0, 0.0, 0.0], 'rewardMean': 0.6976414673159895, 'totalEpisodes': 197, 'stepsPerEpisode': 524, 'rewardPerEpisode': 411.0479160031012
'totalSteps': 16640, 'rewardStep': 0.6796113666845143, 'errorList': [], 'lossList': [0.0, -1.193404684662819, 0.0, 4.153216106295585, 0.0, 0.0, 0.0], 'rewardMean': 0.685554291982639, 'totalEpisodes': 199, 'stepsPerEpisode': 215, 'rewardPerEpisode': 165.13926347446346
'totalSteps': 17920, 'rewardStep': 0.735973814631461, 'errorList': [], 'lossList': [0.0, -1.179829602241516, 0.0, 10.649656618833541, 0.0, 0.0, 0.0], 'rewardMean': 0.6950082622148784, 'totalEpisodes': 203, 'stepsPerEpisode': 97, 'rewardPerEpisode': 83.13369908541159
'totalSteps': 19200, 'rewardStep': 0.7101955818184598, 'errorList': [], 'lossList': [0.0, -1.1614116710424423, 0.0, 4.646015338301659, 0.0, 0.0, 0.0], 'rewardMean': 0.6893446945903181, 'totalEpisodes': 205, 'stepsPerEpisode': 495, 'rewardPerEpisode': 383.25239677894797
'totalSteps': 20480, 'rewardStep': 0.2139042607170658, 'errorList': [], 'lossList': [0.0, -1.1551255667209626, 0.0, 2.497815788984299, 0.0, 0.0, 0.0], 'rewardMean': 0.6490896891054507, 'totalEpisodes': 206, 'stepsPerEpisode': 642, 'rewardPerEpisode': 503.7544886191379
'totalSteps': 21760, 'rewardStep': 0.7297076898983771, 'errorList': [], 'lossList': [0.0, -1.1363045650720596, 0.0, 1.3522517496347428, 0.0, 0.0, 0.0], 'rewardMean': 0.6481085450195593, 'totalEpisodes': 207, 'stepsPerEpisode': 1273, 'rewardPerEpisode': 1022.5017429248761
'totalSteps': 23040, 'rewardStep': 0.8734437218962366, 'errorList': [], 'lossList': [0.0, -1.1022407084703445, 0.0, 1.0923136214911937, 0.0, 0.0, 0.0], 'rewardMean': 0.6646749025539453, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.953525470891
'totalSteps': 24320, 'rewardStep': 0.7528543552840359, 'errorList': [], 'lossList': [0.0, -1.068178095817566, 0.0, 0.6431143063306809, 0.0, 0.0, 0.0], 'rewardMean': 0.6673644812507824, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.397921318295
'totalSteps': 25600, 'rewardStep': 0.9774596973587181, 'errorList': [0.17231967950566637, 0.08494510204409586, 0.15174150176742232, 0.11442448054457702, 0.15141987979129848, 0.09571316379890486, 0.11989064094515166, 0.10897567743213309, 0.12754270253302494, 0.1395776039433689, 0.18201764126493236, 0.07742641254579065, 0.13147957280840197, 0.20879046264960524, 0.15105023553311503, 0.1400884756360512, 0.1502217163852263, 0.1286481806233869, 0.1238543742933759, 0.07526912405718102, 0.09691719833326361, 0.10262480847479703, 0.15164786980482, 0.139967496523245, 0.15387688657948514, 0.20514259877022048, 0.18500560631217586, 0.0921396055083867, 0.1628313562057825, 0.1346699285966191, 0.10982151608804656, 0.14811792381558317, 0.14703223250820732, 0.130508390824954, 0.16650576792167046, 0.11998249856879023, 0.16895243229572454, 0.13866958018982534, 0.11406066102715016, 0.11878285754546714, 0.15786895084876618, 0.18472909571485216, 0.11739336828259377, 0.1665517538569494, 0.22698150135236383, 0.17163871152816954, 0.12648020071664737, 0.11010544493171703, 0.1433183119292575, 0.11219626442290284], 'lossList': [0.0, -1.0219926419854164, 0.0, 0.7195186272263527, 0.0, 0.0, 0.0], 'rewardMean': 0.7073739825624819, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.7127580391152, 'successfulTests': 47
#maxSuccessfulTests=47, maxSuccessfulTestsAtStep=25600, timeSpent=81.32
