#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 87
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.9226579163850485, 'errorList': [], 'lossList': [0.0, -1.4297674459218979, 0.0, 31.148033123016358, 0.0, 0.0, 0.0], 'rewardMean': 0.6803442542954612, 'totalEpisodes': 43, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.330926038734456
'totalSteps': 3840, 'rewardStep': 0.8099881087909944, 'errorList': [], 'lossList': [0.0, -1.3952327185869218, 0.0, 42.99413730621338, 0.0, 0.0, 0.0], 'rewardMean': 0.7235588724606389, 'totalEpisodes': 98, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.6081736808398939
'totalSteps': 5120, 'rewardStep': 0.9629916138481502, 'errorList': [], 'lossList': [0.0, -1.358093450665474, 0.0, 43.40493559837341, 0.0, 0.0, 0.0], 'rewardMean': 0.7834170578075167, 'totalEpisodes': 152, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.26686091595404
'totalSteps': 6400, 'rewardStep': 0.8455164848163965, 'errorList': [], 'lossList': [0.0, -1.331855723261833, 0.0, 42.5859836101532, 0.0, 0.0, 0.0], 'rewardMean': 0.7958369432092927, 'totalEpisodes': 185, 'stepsPerEpisode': 27, 'rewardPerEpisode': 18.67385247331028
'totalSteps': 7680, 'rewardStep': 0.842646741349346, 'errorList': [], 'lossList': [0.0, -1.3160558277368546, 0.0, 45.09357454299927, 0.0, 0.0, 0.0], 'rewardMean': 0.8036385762326348, 'totalEpisodes': 201, 'stepsPerEpisode': 59, 'rewardPerEpisode': 44.040214179205286
'totalSteps': 8960, 'rewardStep': 0.4703385633147633, 'errorList': [], 'lossList': [0.0, -1.3075573563575744, 0.0, 25.224691870212556, 0.0, 0.0, 0.0], 'rewardMean': 0.7560242886729389, 'totalEpisodes': 207, 'stepsPerEpisode': 134, 'rewardPerEpisode': 106.62580025177067
'totalSteps': 10240, 'rewardStep': 0.714309363004563, 'errorList': [], 'lossList': [0.0, -1.297954753637314, 0.0, 17.895775426626205, 0.0, 0.0, 0.0], 'rewardMean': 0.750809922964392, 'totalEpisodes': 210, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.853824774045208
'totalSteps': 11520, 'rewardStep': 0.8445502407750854, 'errorList': [], 'lossList': [0.0, -1.30126828789711, 0.0, 18.817806941270828, 0.0, 0.0, 0.0], 'rewardMean': 0.7612255138322468, 'totalEpisodes': 213, 'stepsPerEpisode': 544, 'rewardPerEpisode': 372.1345436404595
'totalSteps': 12800, 'rewardStep': 0.6933768979555678, 'errorList': [], 'lossList': [0.0, -1.3017126852273941, 0.0, 24.59171852827072, 0.0, 0.0, 0.0], 'rewardMean': 0.7544406522445788, 'totalEpisodes': 215, 'stepsPerEpisode': 564, 'rewardPerEpisode': 435.6491171492997
'totalSteps': 14080, 'rewardStep': 0.8346502128531842, 'errorList': [], 'lossList': [0.0, -1.2757907557487487, 0.0, 5.269925893694162, 0.0, 0.0, 0.0], 'rewardMean': 0.79410261430931, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 973.9273451405401
'totalSteps': 15360, 'rewardStep': 0.7784428701628505, 'errorList': [], 'lossList': [0.0, -1.2545351618528366, 0.0, 7.839888271093368, 0.0, 0.0, 0.0], 'rewardMean': 0.77968110968709, 'totalEpisodes': 216, 'stepsPerEpisode': 1019, 'rewardPerEpisode': 795.9989866064735
'totalSteps': 16640, 'rewardStep': 0.7359591183357204, 'errorList': [], 'lossList': [0.0, -1.2376479691267013, 0.0, 3.7795947948098183, 0.0, 0.0, 0.0], 'rewardMean': 0.7722782106415627, 'totalEpisodes': 216, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.1307606073294
'totalSteps': 17920, 'rewardStep': 0.6509895289317507, 'errorList': [], 'lossList': [0.0, -1.2213476651906967, 0.0, 4.400379568636417, 0.0, 0.0, 0.0], 'rewardMean': 0.7410780021499227, 'totalEpisodes': 217, 'stepsPerEpisode': 852, 'rewardPerEpisode': 690.5415733499166
'totalSteps': 19200, 'rewardStep': 0.7821160318439501, 'errorList': [], 'lossList': [0.0, -1.2185195022821427, 0.0, 2.406496660113335, 0.0, 0.0, 0.0], 'rewardMean': 0.7347379568526782, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 970.5472458439546
'totalSteps': 20480, 'rewardStep': 0.8541814451569234, 'errorList': [], 'lossList': [0.0, -1.210588383078575, 0.0, 1.45568669334054, 0.0, 0.0, 0.0], 'rewardMean': 0.7358914272334358, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.7561300411996
'totalSteps': 21760, 'rewardStep': 0.9163328134551547, 'errorList': [], 'lossList': [0.0, -1.1823639678955078, 0.0, 0.904714302122593, 0.0, 0.0, 0.0], 'rewardMean': 0.780490852247475, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.4547562272091
'totalSteps': 23040, 'rewardStep': 0.9382969358131227, 'errorList': [0.18910849154792622, 0.13924884476457478, 0.15043001859122573, 0.20269809720833912, 0.13804877664047918, 0.2262617290120357, 0.16343595423325505, 0.20319153079082183, 0.21296671879890475, 0.15265415182350547, 0.2622702203626996, 0.100802677675756, 0.1558204294669192, 0.20203377402402575, 0.16619515081077232, 0.18976376872526013, 0.15335096997154032, 0.23216436815428165, 0.19111702302018233, 0.12708530172776372, 0.255024172355162, 0.2124245308734842, 0.18161425626749225, 0.1737275637469592, 0.14880549467405763, 0.16685124045659858, 0.1416755035265426, 0.2023969831947109, 0.21756325731721088, 0.19741088974183468, 0.119664415422989, 0.17211103685846377, 0.17937311448742632, 0.20679728751785517, 0.2528140378436768, 0.1866815533997253, 0.257395285402019, 0.1717425393496949, 0.1429836260072933, 0.2073846599673644, 0.0972042713188881, 0.09846899481305138, 0.20792138871657237, 0.17050100945788563, 0.19394143700881614, 0.2100783807947629, 0.16487042472325258, 0.22304327881889902, 0.19310740680748742, 0.22576138854166963], 'lossList': [0.0, -1.1320906889438629, 0.0, 0.37582653732970356, 0.0, 0.0, 0.0], 'rewardMean': 0.802889609528331, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.650472242045, 'successfulTests': 31
'totalSteps': 24320, 'rewardStep': 0.9579182794911615, 'errorList': [0.04907761137720483, 0.08799536795372959, 0.07392228652383333, 0.038965697760484225, 0.035746830923124924, 0.06086059009723223, 0.1153098594055673, 0.08060719410943566, 0.07598373029550452, 0.08297726689011092, 0.0835505661833651, 0.0772351022870125, 0.04140492481379362, 0.08942950711680568, 0.06783547904004937, 0.08018477033651504, 0.04764583612606625, 0.07336954468274297, 0.08279657790863788, 0.10104967284132817, 0.08189089995656146, 0.0715009033356952, 0.09317772263284008, 0.06827136563345118, 0.13402534245933478, 0.10476665656984267, 0.06883131270505075, 0.08723740600532486, 0.10083090409582396, 0.0688856779559715, 0.14544917788801073, 0.07306366175620366, 0.07106220233481353, 0.039593188036821686, 0.05578287325605913, 0.04311115249582961, 0.08021835619324563, 0.09003211110926083, 0.07208751575704184, 0.07945926308371129, 0.12253409472398631, 0.09601519801066424, 0.07335666549823189, 0.09593165858240954, 0.07900120669055302, 0.03762467841967117, 0.04906656809636395, 0.03923662968494568, 0.08905269990145855, 0.06542422068221894], 'lossList': [0.0, -1.0916917300224305, 0.0, 0.46382849213667215, 0.0, 0.0, 0.0], 'rewardMean': 0.8142264133999385, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.5709128629665, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=97.0
