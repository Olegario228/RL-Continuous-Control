#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 89
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.6437125868282558, 'errorList': [], 'lossList': [0.0, -1.4189856410026551, 0.0, 35.32847622871399, 0.0, 0.0, 0.0], 'rewardMean': 0.6578139964857996, 'totalEpisodes': 45, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.20471769693251
'totalSteps': 3840, 'rewardStep': 0.7530460577818632, 'errorList': [], 'lossList': [0.0, -1.4111442905664444, 0.0, 48.59512340545654, 0.0, 0.0, 0.0], 'rewardMean': 0.6895580169178208, 'totalEpisodes': 95, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.1765735254516962
'totalSteps': 5120, 'rewardStep': 0.8729427666814762, 'errorList': [], 'lossList': [0.0, -1.3987477511167525, 0.0, 62.748912448883054, 0.0, 0.0, 0.0], 'rewardMean': 0.7354042043587347, 'totalEpisodes': 139, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.25970971724089
'totalSteps': 6400, 'rewardStep': 0.2362483742680332, 'errorList': [], 'lossList': [0.0, -1.399583945274353, 0.0, 52.08800436019897, 0.0, 0.0, 0.0], 'rewardMean': 0.6355730383405944, 'totalEpisodes': 164, 'stepsPerEpisode': 105, 'rewardPerEpisode': 63.78638847783599
'totalSteps': 7680, 'rewardStep': 0.7771499080258635, 'errorList': [], 'lossList': [0.0, -1.3879507744312287, 0.0, 40.950974206924435, 0.0, 0.0, 0.0], 'rewardMean': 0.6591691832881392, 'totalEpisodes': 177, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.5112631250894
'totalSteps': 8960, 'rewardStep': 0.9276772755417104, 'errorList': [], 'lossList': [0.0, -1.3657142609357833, 0.0, 35.52142884254456, 0.0, 0.0, 0.0], 'rewardMean': 0.6975274821815065, 'totalEpisodes': 188, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.142733942441009
'totalSteps': 10240, 'rewardStep': 0.761891091053296, 'errorList': [], 'lossList': [0.0, -1.3551689863204956, 0.0, 21.976456074714662, 0.0, 0.0, 0.0], 'rewardMean': 0.7055729332904802, 'totalEpisodes': 194, 'stepsPerEpisode': 161, 'rewardPerEpisode': 127.99091724856117
'totalSteps': 11520, 'rewardStep': 0.7519255689528948, 'errorList': [], 'lossList': [0.0, -1.3461116713285446, 0.0, 12.699466544389725, 0.0, 0.0, 0.0], 'rewardMean': 0.7107232261418596, 'totalEpisodes': 198, 'stepsPerEpisode': 217, 'rewardPerEpisode': 167.39013747542123
'totalSteps': 12800, 'rewardStep': 0.8416650487038826, 'errorList': [], 'lossList': [0.0, -1.325490835905075, 0.0, 12.612508006095887, 0.0, 0.0, 0.0], 'rewardMean': 0.7238174083980619, 'totalEpisodes': 202, 'stepsPerEpisode': 67, 'rewardPerEpisode': 59.422236278938435
'totalSteps': 14080, 'rewardStep': 0.40831640113423673, 'errorList': [], 'lossList': [0.0, -1.288051387667656, 0.0, 4.421019351482391, 0.0, 0.0, 0.0], 'rewardMean': 0.6974575078971512, 'totalEpisodes': 206, 'stepsPerEpisode': 264, 'rewardPerEpisode': 184.78665253021825
'totalSteps': 15360, 'rewardStep': 0.87422688299355, 'errorList': [], 'lossList': [0.0, -1.2565688639879227, 0.0, 24.97727600336075, 0.0, 0.0, 0.0], 'rewardMean': 0.7205089375136806, 'totalEpisodes': 211, 'stepsPerEpisode': 31, 'rewardPerEpisode': 28.225665851856714
'totalSteps': 16640, 'rewardStep': 0.7057322829042799, 'errorList': [], 'lossList': [0.0, -1.246015860438347, 0.0, 7.206541858315468, 0.0, 0.0, 0.0], 'rewardMean': 0.7157775600259224, 'totalEpisodes': 214, 'stepsPerEpisode': 109, 'rewardPerEpisode': 95.72199592178276
'totalSteps': 17920, 'rewardStep': 0.6677467197257456, 'errorList': [], 'lossList': [0.0, -1.2222711557149888, 0.0, 3.7603536742925643, 0.0, 0.0, 0.0], 'rewardMean': 0.6952579553303492, 'totalEpisodes': 216, 'stepsPerEpisode': 451, 'rewardPerEpisode': 379.6534452727271
'totalSteps': 19200, 'rewardStep': 0.764330173360889, 'errorList': [], 'lossList': [0.0, -1.218738361597061, 0.0, 2.424919922351837, 0.0, 0.0, 0.0], 'rewardMean': 0.7480661352396349, 'totalEpisodes': 218, 'stepsPerEpisode': 246, 'rewardPerEpisode': 205.31261538024003
'totalSteps': 20480, 'rewardStep': 0.9328650020661284, 'errorList': [0.06700282356873727, 0.19444638627730457, 0.04521591520933194, 0.12223014131685013, 0.4862762245671602, 0.7578882703620083, 0.07060951803503167, 0.13497373930698112, 0.33347283826079155, 0.27398738718534843, 0.419851110787727, 0.17152976412673873, 0.5642693616045096, 0.048081674179105326, 0.3512578906837472, 0.22265529720192193, 0.23233624143937692, 0.5894547396341631, 0.2954304575096621, 0.5532605790830266, 0.07380788279287565, 0.12130293534788883, 0.11340473380663307, 0.6844480948523359, 0.04753418852644585, 0.3200596238748901, 0.10557130451655476, 0.49961590814950135, 0.4119135336004033, 0.25338811886885737, 0.32606554632194273, 0.22069119925534977, 0.05626689349177971, 0.330962676944141, 0.21270142794366337, 0.2598806273945378, 0.04976405336018835, 0.1589063122163416, 0.7855681268377778, 0.055733436105010026, 0.04133783152791934, 0.21165899037142222, 0.25369679000446127, 0.03382332664508064, 0.07750505428743538, 0.40813275360458173, 0.299304596346762, 0.18470945238034847, 0.3347613106904895, 0.06551013686688077], 'lossList': [0.0, -1.230247113108635, 0.0, 2.492483711242676, 0.0, 0.0, 0.0], 'rewardMean': 0.7636376446436614, 'totalEpisodes': 221, 'stepsPerEpisode': 67, 'rewardPerEpisode': 56.052554682163596, 'successfulTests': 22
'totalSteps': 21760, 'rewardStep': 0.9056067449257774, 'errorList': [], 'lossList': [0.0, -1.2248281985521317, 0.0, 1.6861887155473232, 0.0, 0.0, 0.0], 'rewardMean': 0.761430591582068, 'totalEpisodes': 223, 'stepsPerEpisode': 160, 'rewardPerEpisode': 142.90831383447698
'totalSteps': 23040, 'rewardStep': 0.8727159950635311, 'errorList': [], 'lossList': [0.0, -1.2087192618846894, 0.0, 1.337261136174202, 0.0, 0.0, 0.0], 'rewardMean': 0.7725130819830917, 'totalEpisodes': 224, 'stepsPerEpisode': 533, 'rewardPerEpisode': 447.7995205803232
'totalSteps': 24320, 'rewardStep': 0.709290668845586, 'errorList': [], 'lossList': [0.0, -1.186258438229561, 0.0, 1.2865097916126251, 0.0, 0.0, 0.0], 'rewardMean': 0.7682495919723606, 'totalEpisodes': 225, 'stepsPerEpisode': 555, 'rewardPerEpisode': 482.5044118334722
'totalSteps': 25600, 'rewardStep': 0.9859461272888781, 'errorList': [0.020067144198577865, 0.01642781596725284, 0.016306584725901463, 0.018781297186073347, 0.023148628039247002, 0.06415014882254809, 0.03928448327258855, 0.01289060716699487, 0.021280208795892354, 0.01564571112057133, 0.026247595073422957, 0.039219434050734255, 0.03363204049568333, 0.018432226010702123, 0.021485950814757095, 0.018161976880643905, 0.02548677986743177, 0.01937827136190601, 0.02416306841808099, 0.04585898163539927, 0.025006326937968545, 0.026809390110109257, 0.016371318672664687, 0.016688633718509233, 0.016317027286505845, 0.02830220938515116, 0.012327310350533608, 0.012791148114198592, 0.02144497900900851, 0.02228803553182905, 0.03299934280622682, 0.025196874031520333, 0.08304231209352117, 0.019474655436767976, 0.025005804396114696, 0.04308793905837693, 0.020238293946070095, 0.04424933439873314, 0.0324400591327136, 0.01769124802853128, 0.014401648056495336, 0.021708713719652117, 0.01267594048285269, 0.027041012565048025, 0.038538770864045, 0.013717274585898744, 0.012854114458191101, 0.02163203183164103, 0.02746132494076166, 0.015733063692207373], 'lossList': [0.0, -1.1465076261758804, 0.0, 0.6193280026689172, 0.0, 0.0, 0.0], 'rewardMean': 0.7826776998308602, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.3453356015916, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=99.18
