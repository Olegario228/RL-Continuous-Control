#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 90
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.5416751228893167, 'errorList': [], 'lossList': [0.0, -1.4275838589668275, 0.0, 31.07675305366516, 0.0, 0.0, 0.0], 'rewardMean': 0.718628357048742, 'totalEpisodes': 29, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.407464735751145
'totalSteps': 3840, 'rewardStep': 0.3807124494471852, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5496704032479636, 'totalEpisodes': 83, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.0311258801552383
'totalSteps': 5120, 'rewardStep': 0.46311286639569177, 'errorList': [], 'lossList': [0.0, -1.417803671360016, 0.0, 46.55870698928833, 0.0, 0.0, 0.0], 'rewardMean': 0.5323588958775092, 'totalEpisodes': 149, 'stepsPerEpisode': 2, 'rewardPerEpisode': 0.9464966063812201
'totalSteps': 6400, 'rewardStep': 0.520078323227308, 'errorList': [], 'lossList': [0.0, -1.4109403365850448, 0.0, 42.56321599960327, 0.0, 0.0, 0.0], 'rewardMean': 0.5303121337691423, 'totalEpisodes': 186, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.329243997636357
'totalSteps': 7680, 'rewardStep': 0.9602377711529075, 'errorList': [], 'lossList': [0.0, -1.382875611782074, 0.0, 47.13356245040894, 0.0, 0.0, 0.0], 'rewardMean': 0.591730081966823, 'totalEpisodes': 209, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.9074022400068955
'totalSteps': 8960, 'rewardStep': 0.6445806072612342, 'errorList': [], 'lossList': [0.0, -1.354119954109192, 0.0, 47.12738591194153, 0.0, 0.0, 0.0], 'rewardMean': 0.5983363976286245, 'totalEpisodes': 217, 'stepsPerEpisode': 97, 'rewardPerEpisode': 63.78212253530287
'totalSteps': 10240, 'rewardStep': 0.4747043552302594, 'errorList': [], 'lossList': [0.0, -1.3447241306304931, 0.0, 32.5730943775177, 0.0, 0.0, 0.0], 'rewardMean': 0.5845995040288061, 'totalEpisodes': 223, 'stepsPerEpisode': 108, 'rewardPerEpisode': 77.73528212983852
'totalSteps': 11520, 'rewardStep': 0.8164447550470507, 'errorList': [], 'lossList': [0.0, -1.3405684322118758, 0.0, 36.89343026161194, 0.0, 0.0, 0.0], 'rewardMean': 0.6077840291306306, 'totalEpisodes': 228, 'stepsPerEpisode': 67, 'rewardPerEpisode': 58.66559126680447
'totalSteps': 12800, 'rewardStep': 0.9532304111965639, 'errorList': [3.0488855571432687, 25.460319301690166, 8.994743632062544, 7.138994812815878, 8.2068304305832, 16.930323039446577, 24.9752386296318, 10.638376010246978, 16.086110709020303, 23.027639291310877, 7.941676020839563, 4.9305853761647915, 16.97853979987779, 3.5818907517541265, 21.694869785101115, 16.66735584125156, 20.264895223429157, 13.900717789063012, 15.867214379289603, 0.6116545614718683, 19.45023690723756, 10.112065224310248, 17.79185892076122, 13.313409994031847, 15.541803976319283, 24.662018121247293, 3.8268259911202422, 9.674815126077144, 29.31400737287212, 22.387994016578144, 5.770953958388305, 13.476173825071665, 9.087255132262806, 8.557555366134652, 19.878501410080776, 25.677650534401174, 8.878019338017332, 19.94158506899285, 23.965590864738395, 10.614011806824577, 12.515940526193331, 12.488967582031568, 7.233364250219516, 21.30268249542954, 16.082583139971725, 17.627360952198448, 16.549814176528795, 17.95624333853147, 7.113077026800791, 12.249936077150625], 'lossList': [0.0, -1.337462174296379, 0.0, 12.741549096107484, 0.0, 0.0, 0.0], 'rewardMean': 0.6135489111294701, 'totalEpisodes': 232, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.249973984273112, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.5557862051503932, 'errorList': [], 'lossList': [0.0, -1.3370490020513535, 0.0, 21.517010660171508, 0.0, 0.0, 0.0], 'rewardMean': 0.6149600193555778, 'totalEpisodes': 239, 'stepsPerEpisode': 57, 'rewardPerEpisode': 39.067990159265186
'totalSteps': 15360, 'rewardStep': 0.573783859159275, 'errorList': [], 'lossList': [0.0, -1.3136814939975738, 0.0, 7.339358410835266, 0.0, 0.0, 0.0], 'rewardMean': 0.6342671603267869, 'totalEpisodes': 244, 'stepsPerEpisode': 391, 'rewardPerEpisode': 326.4721146032824
'totalSteps': 16640, 'rewardStep': 0.713515951523326, 'errorList': [], 'lossList': [0.0, -1.310274791121483, 0.0, 4.040025829076767, 0.0, 0.0, 0.0], 'rewardMean': 0.667547510534401, 'totalEpisodes': 250, 'stepsPerEpisode': 101, 'rewardPerEpisode': 78.5188473088759
'totalSteps': 17920, 'rewardStep': 0.9649878777831821, 'errorList': [0.5489450790757872, 1.64218127393268, 1.1502783552806999, 1.3707485467998628, 0.5897445570325587, 0.12906573387265724, 1.5202063336139235, 1.6437801935508427, 1.3851397738228481, 0.812089827033542, 0.24443243966856462, 0.30021488548654224, 1.1319722841560145, 1.5155733946648642, 0.7056683365313091, 0.12224859028467866, 2.224665175070311, 2.119934611259391, 0.24461116234644972, 1.1247790835862992, 1.8365658647416183, 1.8146955766965365, 0.1934252375182998, 2.816811266136561, 0.6287875510585904, 0.18949808025143242, 0.7309807787816925, 0.45319733378098703, 2.071610846954688, 0.3247601301116387, 0.5574895136016521, 0.9935807485605543, 0.7228735142146847, 0.22186676243956363, 1.616114708525935, 1.6440495171055949, 1.181575522885702, 0.14123070834024076, 0.2238933106729183, 0.4842444544189527, 0.6851569491672956, 0.1978739919578735, 0.31212146323333695, 0.14461172983231751, 2.5106174080573225, 1.6212203724998813, 0.18214301294910903, 0.4453717330524031, 1.3731605383437584, 1.1415293488034528], 'lossList': [0.0, -1.322470703125, 0.0, 3.496709094643593, 0.0, 0.0, 0.0], 'rewardMean': 0.71773501167315, 'totalEpisodes': 253, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.93920234397761, 'successfulTests': 8
'totalSteps': 19200, 'rewardStep': 0.9125803702209957, 'errorList': [], 'lossList': [0.0, -1.3140030205249786, 0.0, 4.852951785326004, 0.0, 0.0, 0.0], 'rewardMean': 0.7569852163725187, 'totalEpisodes': 255, 'stepsPerEpisode': 132, 'rewardPerEpisode': 119.11268513090401
'totalSteps': 20480, 'rewardStep': 0.4966557633947799, 'errorList': [], 'lossList': [0.0, -1.3066057765483856, 0.0, 3.366138017773628, 0.0, 0.0, 0.0], 'rewardMean': 0.7106270155967059, 'totalEpisodes': 256, 'stepsPerEpisode': 501, 'rewardPerEpisode': 329.2893579829622
'totalSteps': 21760, 'rewardStep': 0.7123189905654896, 'errorList': [], 'lossList': [0.0, -1.3015238976478576, 0.0, 8.68820002436638, 0.0, 0.0, 0.0], 'rewardMean': 0.7174008539271316, 'totalEpisodes': 258, 'stepsPerEpisode': 413, 'rewardPerEpisode': 328.80961758264135
'totalSteps': 23040, 'rewardStep': 0.47016244209389335, 'errorList': [], 'lossList': [0.0, -1.2838962066173554, 0.0, 2.8357602274417877, 0.0, 0.0, 0.0], 'rewardMean': 0.716946662613495, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 920.5148366499053
'totalSteps': 24320, 'rewardStep': 0.7068427588935466, 'errorList': [], 'lossList': [0.0, -1.2468080669641495, 0.0, 1.1260541193187237, 0.0, 0.0, 0.0], 'rewardMean': 0.7059864629981446, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.93950219755
'totalSteps': 25600, 'rewardStep': 0.879982335778459, 'errorList': [], 'lossList': [0.0, -1.2167996114492416, 0.0, 0.9706057634949684, 0.0, 0.0, 0.0], 'rewardMean': 0.6986616554563341, 'totalEpisodes': 258, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.3840814673501
#maxSuccessfulTests=8, maxSuccessfulTestsAtStep=17920, timeSpent=98.21
