#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 91
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7002489002645322, 'errorList': [], 'lossList': [0.0, -1.4202158731222152, 0.0, 30.792507972717285, 0.0, 0.0, 0.0], 'rewardMean': 0.6294483093704437, 'totalEpisodes': 26, 'stepsPerEpisode': 64, 'rewardPerEpisode': 49.855720442419674
'totalSteps': 3840, 'rewardStep': 0.8261938237832954, 'errorList': [], 'lossList': [0.0, -1.4079683244228363, 0.0, 62.64463157653809, 0.0, 0.0, 0.0], 'rewardMean': 0.6950301475080609, 'totalEpisodes': 74, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.172512817892244
'totalSteps': 5120, 'rewardStep': 0.6467212126333503, 'errorList': [], 'lossList': [0.0, -1.3784416407346725, 0.0, 63.45081699371338, 0.0, 0.0, 0.0], 'rewardMean': 0.6829529137893833, 'totalEpisodes': 116, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.44331388017091
'totalSteps': 6400, 'rewardStep': 0.9877245216826039, 'errorList': [], 'lossList': [0.0, -1.353225412964821, 0.0, 58.902230949401854, 0.0, 0.0, 0.0], 'rewardMean': 0.7439072353680274, 'totalEpisodes': 154, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.398449334704518
'totalSteps': 7680, 'rewardStep': 0.8876290148544944, 'errorList': [], 'lossList': [0.0, -1.337766905426979, 0.0, 44.62431626319885, 0.0, 0.0, 0.0], 'rewardMean': 0.7678608652824385, 'totalEpisodes': 167, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.44633680233212
'totalSteps': 8960, 'rewardStep': 0.8363101057512277, 'errorList': [], 'lossList': [0.0, -1.3180053699016572, 0.0, 45.46526811599731, 0.0, 0.0, 0.0], 'rewardMean': 0.7776393282065512, 'totalEpisodes': 180, 'stepsPerEpisode': 63, 'rewardPerEpisode': 55.83977387702302
'totalSteps': 10240, 'rewardStep': 0.27851317734135433, 'errorList': [], 'lossList': [0.0, -1.3188467866182327, 0.0, 32.546072659492495, 0.0, 0.0, 0.0], 'rewardMean': 0.7152485593484017, 'totalEpisodes': 187, 'stepsPerEpisode': 260, 'rewardPerEpisode': 185.45585545298206
'totalSteps': 11520, 'rewardStep': 0.7024168676257911, 'errorList': [], 'lossList': [0.0, -1.3063073152303695, 0.0, 22.645636944770814, 0.0, 0.0, 0.0], 'rewardMean': 0.7138228158236672, 'totalEpisodes': 198, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.80533570599654
'totalSteps': 12800, 'rewardStep': 0.5413759690469295, 'errorList': [], 'lossList': [0.0, -1.27782894551754, 0.0, 11.164351941347123, 0.0, 0.0, 0.0], 'rewardMean': 0.6965781311459934, 'totalEpisodes': 202, 'stepsPerEpisode': 192, 'rewardPerEpisode': 139.60950744185607
'totalSteps': 14080, 'rewardStep': 0.8087792894333166, 'errorList': [], 'lossList': [0.0, -1.2541194307804107, 0.0, 10.394297028779983, 0.0, 0.0, 0.0], 'rewardMean': 0.7215912882416895, 'totalEpisodes': 208, 'stepsPerEpisode': 101, 'rewardPerEpisode': 87.5251152814878
'totalSteps': 15360, 'rewardStep': 0.7358771258013018, 'errorList': [], 'lossList': [0.0, -1.2572687739133834, 0.0, 4.423177148103714, 0.0, 0.0, 0.0], 'rewardMean': 0.7251541107953666, 'totalEpisodes': 214, 'stepsPerEpisode': 86, 'rewardPerEpisode': 67.79114200162597
'totalSteps': 16640, 'rewardStep': 0.3574769980895823, 'errorList': [], 'lossList': [0.0, -1.2292860794067382, 0.0, 5.007155978679657, 0.0, 0.0, 0.0], 'rewardMean': 0.6782824282259952, 'totalEpisodes': 219, 'stepsPerEpisode': 183, 'rewardPerEpisode': 119.6295167999148
'totalSteps': 17920, 'rewardStep': 0.6574183349732994, 'errorList': [], 'lossList': [0.0, -1.193830144405365, 0.0, 4.298096124529838, 0.0, 0.0, 0.0], 'rewardMean': 0.6793521404599903, 'totalEpisodes': 222, 'stepsPerEpisode': 201, 'rewardPerEpisode': 163.58498240943354
'totalSteps': 19200, 'rewardStep': 0.7634153641100616, 'errorList': [], 'lossList': [0.0, -1.1691492182016372, 0.0, 4.141872918009758, 0.0, 0.0, 0.0], 'rewardMean': 0.6569212247027358, 'totalEpisodes': 224, 'stepsPerEpisode': 518, 'rewardPerEpisode': 425.91444179715717
'totalSteps': 20480, 'rewardStep': 0.7003816712694365, 'errorList': [], 'lossList': [0.0, -1.133147587776184, 0.0, 3.500855197161436, 0.0, 0.0, 0.0], 'rewardMean': 0.6381964903442301, 'totalEpisodes': 224, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.2118291699305
'totalSteps': 21760, 'rewardStep': 0.7550798355729319, 'errorList': [], 'lossList': [0.0, -1.072391374707222, 0.0, 2.005708223879337, 0.0, 0.0, 0.0], 'rewardMean': 0.6300734633264005, 'totalEpisodes': 224, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.9622603116468
'totalSteps': 23040, 'rewardStep': 0.9303804557721785, 'errorList': [0.07310398037252827, 0.069417913703468, 0.07803080934332537, 0.0650179551303484, 0.0804957386973055, 0.08554043707925577, 0.06441506326176408, 0.08210358137148493, 0.07711826570778575, 0.08018502121420791, 0.08460717798489051, 0.07173771404600153, 0.07366509744532738, 0.08821203971636245, 0.06927041133240512, 0.06441499731455866, 0.07909519513945351, 0.06441577295875288, 0.06813426433301965, 0.08765212031813055, 0.08160646913619042, 0.07551102047197224, 0.08960947301752194, 0.08726989916402011, 0.07581941982389001, 0.08343952710424568, 0.07226595211908614, 0.06650736562485002, 0.08205541863987965, 0.08677986602502616, 0.08911428941490511, 0.08760839473356315, 0.07243147597082877, 0.07111696976896523, 0.08620769133341195, 0.08395070779773123, 0.07706886559388666, 0.0736591681721818, 0.07197020298320882, 0.07815247740979467, 0.06488817851290749, 0.07657047428643193, 0.09522543717531588, 0.0876437387129787, 0.0820936837338545, 0.06409169607226269, 0.08117923411165553, 0.0683372379529508, 0.0684356234493105, 0.07540174000583061], 'lossList': [0.0, -1.0384407371282578, 0.0, 1.2689211606234312, 0.0, 0.0, 0.0], 'rewardMean': 0.6952601911694829, 'totalEpisodes': 224, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.01087483576, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=75.32
