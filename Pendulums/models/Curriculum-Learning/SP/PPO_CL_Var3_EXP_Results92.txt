#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 92
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.5624347352732346, 'errorList': [], 'lossList': [0.0, -1.4405849313735961, 0.0, 31.24658574104309, 0.0, 0.0, 0.0], 'rewardMean': 0.5201102552480164, 'totalEpisodes': 34, 'stepsPerEpisode': 46, 'rewardPerEpisode': 33.2499803799034
'totalSteps': 3840, 'rewardStep': 0.9078136321695918, 'errorList': [], 'lossList': [0.0, -1.4296553921699524, 0.0, 43.68299040794373, 0.0, 0.0, 0.0], 'rewardMean': 0.6493447142218748, 'totalEpisodes': 96, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.507458636256949
'totalSteps': 5120, 'rewardStep': 0.9065513252620494, 'errorList': [], 'lossList': [0.0, -1.4086597728729249, 0.0, 43.35621869087219, 0.0, 0.0, 0.0], 'rewardMean': 0.7136463669819184, 'totalEpisodes': 148, 'stepsPerEpisode': 29, 'rewardPerEpisode': 22.14222217720193
'totalSteps': 6400, 'rewardStep': 0.9554461714667399, 'errorList': [], 'lossList': [0.0, -1.3767988115549088, 0.0, 38.165606355667116, 0.0, 0.0, 0.0], 'rewardMean': 0.7620063278788827, 'totalEpisodes': 175, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.231725010795651
'totalSteps': 7680, 'rewardStep': 0.8723775781116004, 'errorList': [], 'lossList': [0.0, -1.3446222150325775, 0.0, 51.83778904914856, 0.0, 0.0, 0.0], 'rewardMean': 0.7804015362510023, 'totalEpisodes': 198, 'stepsPerEpisode': 61, 'rewardPerEpisode': 49.175548384998876
'totalSteps': 8960, 'rewardStep': 0.787337315704248, 'errorList': [], 'lossList': [0.0, -1.3254693681001664, 0.0, 35.29716787338257, 0.0, 0.0, 0.0], 'rewardMean': 0.7813923618871803, 'totalEpisodes': 210, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.35757540644421
'totalSteps': 10240, 'rewardStep': 0.5219456859654703, 'errorList': [], 'lossList': [0.0, -1.3170108687877655, 0.0, 17.104404230117797, 0.0, 0.0, 0.0], 'rewardMean': 0.7489615273969665, 'totalEpisodes': 217, 'stepsPerEpisode': 10, 'rewardPerEpisode': 5.294860873312619
'totalSteps': 11520, 'rewardStep': 0.9335212402669569, 'errorList': [47.47410270032626, 7.974062867847994, 11.652652588841185, 5.78537031128418, 14.32463228485153, 81.88886285397261, 20.755030409804494, 11.407210174313086, 54.14761184572839, 82.63063067188067, 56.67868410228387, 70.31640869908125, 63.399463359697016, 27.174973003858014, 50.78604232281937, 36.77493954646476, 6.579688513677983, 54.40060568999729, 48.324953924758816, 59.80906848520607, 89.99158860847481, 17.68260782260063, 20.42131896844301, 10.272378780829515, 70.89148976284399, 16.790313475589723, 32.446081516497, 29.081649879257164, 33.81625270649028, 83.89965942581517, 34.67440445879619, 81.11444494664545, 30.555622637243513, 22.67111128316595, 53.24286936396358, 36.49236073782936, 51.3347445517439, 88.10750096772612, 52.36906739922508, 50.22613672192632, 6.705607248824804, 93.30421365711732, 20.396860974226367, 4.027025837599557, 47.96599738263438, 44.77560425217617, 96.80807164456088, 60.19234206969829, 39.52716888914882, 72.55347767249879], 'lossList': [0.0, -1.3365437906980515, 0.0, 7.497449769973755, 0.0, 0.0, 0.0], 'rewardMean': 0.7694681621602988, 'totalEpisodes': 225, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.420544195360566, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.850678242300725, 'errorList': [], 'lossList': [0.0, -1.3457019764184952, 0.0, 10.760946754217148, 0.0, 0.0, 0.0], 'rewardMean': 0.7775891701743414, 'totalEpisodes': 232, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.353832961565489
'totalSteps': 14080, 'rewardStep': 0.797757119529554, 'errorList': [], 'lossList': [0.0, -1.3169787043333054, 0.0, 5.851131810545922, 0.0, 0.0, 0.0], 'rewardMean': 0.8095863046050169, 'totalEpisodes': 236, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.88895854214868
'totalSteps': 15360, 'rewardStep': 0.9646587806899025, 'errorList': [14.069056411855723, 1.252391042359433, 26.981324224289782, 42.99665202449823, 32.93619799096257, 3.804861672444321, 28.074315227991057, 5.596477781261542, 6.818845378585268, 19.719988506873097, 22.639542983160602, 1.8642675538321394, 9.97062130994009, 3.2673149046801937, 7.002322764491312, 9.776622834421998, 9.680396876432726, 5.404610491327622, 11.641697760301472, 8.329361723192662, 10.238720877166907, 2.392686761294291, 2.4014316100727022, 5.998485958488783, 24.31682265734088, 5.437813857983447, 9.890422450552855, 17.466805525685245, 12.97898229290326, 6.77411162986693, 11.093125448799942, 7.426431301513296, 0.290960331787903, 4.9543577473389995, 24.05926078380207, 10.133369460783179, 5.38106550827243, 2.7261936468572876, 15.255813902283032, 4.031411331814487, 29.859528252668266, 22.719300960097435, 31.985334833953793, 34.895506147996095, 34.33643455898977, 1.4966186589870838, 6.11627416788774, 29.18300488579811, 16.82520005611032, 3.727758801247694], 'lossList': [0.0, -1.279256243109703, 0.0, 9.653064754009247, 0.0, 0.0, 0.0], 'rewardMean': 0.8498087091466837, 'totalEpisodes': 242, 'stepsPerEpisode': 53, 'rewardPerEpisode': 44.29538169685348, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.905391346760068, 'errorList': [], 'lossList': [0.0, -1.2664679700136185, 0.0, 3.223560315966606, 0.0, 0.0, 0.0], 'rewardMean': 0.8495664806057313, 'totalEpisodes': 247, 'stepsPerEpisode': 77, 'rewardPerEpisode': 63.84081071604759
'totalSteps': 17920, 'rewardStep': 0.8863329382112488, 'errorList': [], 'lossList': [0.0, -1.245177344083786, 0.0, 3.7181565231084823, 0.0, 0.0, 0.0], 'rewardMean': 0.8475446419006513, 'totalEpisodes': 250, 'stepsPerEpisode': 312, 'rewardPerEpisode': 282.3349071686172
'totalSteps': 19200, 'rewardStep': 0.9040728498047209, 'errorList': [], 'lossList': [0.0, -1.213658971786499, 0.0, 2.4291104230284692, 0.0, 0.0, 0.0], 'rewardMean': 0.8424073097344493, 'totalEpisodes': 252, 'stepsPerEpisode': 589, 'rewardPerEpisode': 510.11527199607775
'totalSteps': 20480, 'rewardStep': 0.5718535182453122, 'errorList': [], 'lossList': [0.0, -1.1985575115680696, 0.0, 1.4520246693491936, 0.0, 0.0, 0.0], 'rewardMean': 0.8123549037478206, 'totalEpisodes': 255, 'stepsPerEpisode': 281, 'rewardPerEpisode': 226.39689257082992
'totalSteps': 21760, 'rewardStep': 0.627725536223608, 'errorList': [], 'lossList': [0.0, -1.1730005556344987, 0.0, 1.4759480753540992, 0.0, 0.0, 0.0], 'rewardMean': 0.7963937257997566, 'totalEpisodes': 257, 'stepsPerEpisode': 251, 'rewardPerEpisode': 197.8880650402665
'totalSteps': 23040, 'rewardStep': 0.904761318847872, 'errorList': [], 'lossList': [0.0, -1.1541663599014282, 0.0, 1.4320948354899883, 0.0, 0.0, 0.0], 'rewardMean': 0.8346752890879967, 'totalEpisodes': 259, 'stepsPerEpisode': 320, 'rewardPerEpisode': 287.6088417271709
'totalSteps': 24320, 'rewardStep': 0.9680949654297525, 'errorList': [1.5321299466116973, 1.8762920924134279, 2.790009826159646, 1.5931842070525406, 1.354537061737182, 0.5311327123498482, 1.5195109450860491, 0.8998798182743564, 2.178742350314272, 0.7781146837370908, 1.4959798788899672, 2.4326268978414824, 2.1638062122560933, 0.8829235277637432, 0.7038115886969815, 0.9064603751126864, 1.0088375569522532, 0.5382616314964012, 1.8762170101534366, 0.8728914950721399, 1.0844458852759207, 0.8152942803293224, 1.2706407836459483, 1.031116687148759, 0.7948306542721003, 0.7125768523878443, 1.2671320027556343, 0.6712912810767385, 0.6108969286521981, 0.6509544167886236, 1.251426258276963, 0.5207382649717279, 0.5477955157362773, 1.5619992967096055, 2.7596335600633264, 0.6562109940723451, 1.8041488831978483, 0.8904171656326834, 0.7412679077944263, 1.313579782729645, 0.7768886337190911, 0.4969944596233385, 2.2780371994477244, 0.8354192938214491, 2.237735014652596, 0.9617489486441171, 0.6755538471282869, 2.6880520334399507, 0.6843999185814488, 0.8590435220202415], 'lossList': [0.0, -1.146016994714737, 0.0, 1.6543745666742324, 0.0, 0.0, 0.0], 'rewardMean': 0.8381326616042764, 'totalEpisodes': 259, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.5258652127666, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.9188686545414317, 'errorList': [], 'lossList': [0.0, -1.1122954440116883, 0.0, 1.0753525918722153, 0.0, 0.0, 0.0], 'rewardMean': 0.8449517028283472, 'totalEpisodes': 261, 'stepsPerEpisode': 280, 'rewardPerEpisode': 256.2381631554309
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=119.22
