#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 93
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.4953586799105156, 'errorList': [], 'lossList': [0.0, -1.4254937756061554, 0.0, 32.99380061149597, 0.0, 0.0, 0.0], 'rewardMean': 0.6767853942330276, 'totalEpisodes': 38, 'stepsPerEpisode': 33, 'rewardPerEpisode': 25.283914890136348
'totalSteps': 3840, 'rewardStep': 0.6491617460789154, 'errorList': [], 'lossList': [0.0, -1.42000563621521, 0.0, 44.1002880859375, 0.0, 0.0, 0.0], 'rewardMean': 0.6675775115149903, 'totalEpisodes': 94, 'stepsPerEpisode': 15, 'rewardPerEpisode': 9.181841336366135
'totalSteps': 5120, 'rewardStep': 0.7108541265869713, 'errorList': [], 'lossList': [0.0, -1.4079846668243408, 0.0, 43.50414716720581, 0.0, 0.0, 0.0], 'rewardMean': 0.6783966652829856, 'totalEpisodes': 136, 'stepsPerEpisode': 20, 'rewardPerEpisode': 13.801927770105255
'totalSteps': 6400, 'rewardStep': 0.8606868336387085, 'errorList': [], 'lossList': [0.0, -1.3875353562831878, 0.0, 46.592971086502075, 0.0, 0.0, 0.0], 'rewardMean': 0.7148546989541302, 'totalEpisodes': 166, 'stepsPerEpisode': 49, 'rewardPerEpisode': 35.59720869115418
'totalSteps': 7680, 'rewardStep': 0.9027342293452592, 'errorList': [], 'lossList': [0.0, -1.3712809580564498, 0.0, 39.38096630573273, 0.0, 0.0, 0.0], 'rewardMean': 0.7461679540193183, 'totalEpisodes': 183, 'stepsPerEpisode': 53, 'rewardPerEpisode': 44.01956424347972
'totalSteps': 8960, 'rewardStep': 0.8423647629092875, 'errorList': [], 'lossList': [0.0, -1.362440865635872, 0.0, 24.162047531604767, 0.0, 0.0, 0.0], 'rewardMean': 0.7599103552893139, 'totalEpisodes': 190, 'stepsPerEpisode': 43, 'rewardPerEpisode': 31.721116807156115
'totalSteps': 10240, 'rewardStep': 0.8266499160043858, 'errorList': [], 'lossList': [0.0, -1.3554960054159164, 0.0, 15.264452015161515, 0.0, 0.0, 0.0], 'rewardMean': 0.7682528003786979, 'totalEpisodes': 194, 'stepsPerEpisode': 176, 'rewardPerEpisode': 147.90146575239953
'totalSteps': 11520, 'rewardStep': 0.44880102378495346, 'errorList': [], 'lossList': [0.0, -1.3388909351825715, 0.0, 10.598896884322166, 0.0, 0.0, 0.0], 'rewardMean': 0.7327581585349485, 'totalEpisodes': 197, 'stepsPerEpisode': 429, 'rewardPerEpisode': 339.75462302623947
'totalSteps': 12800, 'rewardStep': 0.7521424252706924, 'errorList': [], 'lossList': [0.0, -1.341773691177368, 0.0, 9.732854641079904, 0.0, 0.0, 0.0], 'rewardMean': 0.7346965852085229, 'totalEpisodes': 201, 'stepsPerEpisode': 182, 'rewardPerEpisode': 161.3226018183678
'totalSteps': 14080, 'rewardStep': 0.654193876696594, 'errorList': [], 'lossList': [0.0, -1.3573301082849503, 0.0, 5.615508375167846, 0.0, 0.0, 0.0], 'rewardMean': 0.7142947620226282, 'totalEpisodes': 203, 'stepsPerEpisode': 550, 'rewardPerEpisode': 439.6003996305025
'totalSteps': 15360, 'rewardStep': 0.8283214040941236, 'errorList': [], 'lossList': [0.0, -1.37336878657341, 0.0, 5.217745475172997, 0.0, 0.0, 0.0], 'rewardMean': 0.7475910344409891, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 889.1360631852707
'totalSteps': 16640, 'rewardStep': 0.8420337183269261, 'errorList': [], 'lossList': [0.0, -1.340223742723465, 0.0, 3.841577297002077, 0.0, 0.0, 0.0], 'rewardMean': 0.7668782316657901, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.4843948879304
'totalSteps': 17920, 'rewardStep': 0.9239352994982983, 'errorList': [], 'lossList': [0.0, -1.3108711075782775, 0.0, 1.789328931942582, 0.0, 0.0, 0.0], 'rewardMean': 0.7881863489569229, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1065.107105689055
'totalSteps': 19200, 'rewardStep': 0.8359994789666025, 'errorList': [], 'lossList': [0.0, -1.2813545620441438, 0.0, 1.441642475426197, 0.0, 0.0, 0.0], 'rewardMean': 0.7857176134897121, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.2187225511107
'totalSteps': 20480, 'rewardStep': 0.9655031203241321, 'errorList': [0.21181472902274512, 0.12756558784423086, 0.23175036699689844, 0.19145886739158746, 0.22100088819918773, 0.15575177224643894, 0.2126383239043357, 0.16822947168260624, 0.17365732443262963, 0.20043202032808524, 0.18279816162713372, 0.16314925172844147, 0.15939548628032188, 0.18109655889887985, 0.1629305756976388, 0.1338761464646542, 0.187441041270221, 0.2180704966772384, 0.19146748860377205, 0.23709573969677405, 0.16774566911905975, 0.16868973744198004, 0.3375691764442669, 0.1868419244667045, 0.15123242086731384, 0.14805378263002034, 0.19250887319470347, 0.20775973392702882, 0.20474115730559037, 0.21279693425774227, 0.1437864191554249, 0.20906831472766002, 0.21219975607056896, 0.1829166946698169, 0.24890514763670588, 0.19036628922820784, 0.20931506093310404, 0.22668742904163985, 0.1253733024871269, 0.13850397770342074, 0.16092364899795203, 0.2585252123967467, 0.17460562769057486, 0.19106230253415496, 0.2023209836868875, 0.2516412571396061, 0.13649829858082418, 0.1622004484962268, 0.20946867902465524, 0.19298320321817558], 'lossList': [0.0, -1.2399239146709442, 0.0, 0.8494167633354663, 0.0, 0.0, 0.0], 'rewardMean': 0.7919945025875996, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.356884052864, 'successfulTests': 30
'totalSteps': 21760, 'rewardStep': 0.9034946346399004, 'errorList': [], 'lossList': [0.0, -1.206437623500824, 0.0, 0.7842715707421303, 0.0, 0.0, 0.0], 'rewardMean': 0.7981074897606609, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.4252599544145
'totalSteps': 23040, 'rewardStep': 0.8554924603849423, 'errorList': [], 'lossList': [0.0, -1.1949744427204132, 0.0, 0.5452472963556647, 0.0, 0.0, 0.0], 'rewardMean': 0.8009917441987164, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.8199696854415
'totalSteps': 24320, 'rewardStep': 0.9186885725830967, 'errorList': [], 'lossList': [0.0, -1.1573709720373153, 0.0, 0.49081675259396434, 0.0, 0.0, 0.0], 'rewardMean': 0.8479804990785308, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.960173357234
'totalSteps': 25600, 'rewardStep': 0.9557213948528723, 'errorList': [0.09391077232242623, 0.039934428579846366, 0.040247765973100265, 0.05807826853881788, 0.08769533918014713, 0.0419266057960064, 0.0861662196286773, 0.03516143624748822, 0.07370335380028496, 0.07004431382930898, 0.10594438592784199, 0.035790741768393156, 0.036831694513747855, 0.11381677675028694, 0.0582143247493381, 0.04907284761953983, 0.08430009801189865, 0.07223887123224308, 0.05510186962642738, 0.10181430556218275, 0.05683949169508351, 0.11180870171363758, 0.03684445963891575, 0.04857543884055362, 0.06650808398334737, 0.043354537719883915, 0.04199016542763569, 0.05367577441361244, 0.04463618806628356, 0.05684674346050779, 0.05515456899920263, 0.0367136607563041, 0.08335885290243893, 0.07386125949403145, 0.08067746794281838, 0.048642462319042276, 0.09362145588827255, 0.06846398227846129, 0.038825634318905934, 0.05636882333869178, 0.05316396624589854, 0.05668438009929566, 0.07929839899608722, 0.09588972858621536, 0.058106439029068, 0.05882488965354304, 0.05486431812760466, 0.1042714987247292, 0.11227860420092216, 0.08942221387235744], 'lossList': [0.0, -1.1242502617835999, 0.0, 0.3649940748233348, 0.0, 0.0, 0.0], 'rewardMean': 0.8683383960367488, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1209.7618107845683, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=102.93
