#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 95
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.7462180285128702, 'errorList': [], 'lossList': [0.0, -1.4407058680057525, 0.0, 28.2712348985672, 0.0, 0.0, 0.0], 'rewardMean': 0.8305193295468491, 'totalEpisodes': 21, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.29115005263383
'totalSteps': 3840, 'rewardStep': 0.9366941582048341, 'errorList': [], 'lossList': [0.0, -1.428864740729332, 0.0, 56.4460884475708, 0.0, 0.0, 0.0], 'rewardMean': 0.8659109390995109, 'totalEpisodes': 70, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9366941582048341
'totalSteps': 5120, 'rewardStep': 0.9199507154614719, 'errorList': [], 'lossList': [0.0, -1.4214142698049546, 0.0, 45.12312005996704, 0.0, 0.0, 0.0], 'rewardMean': 0.8794208831900011, 'totalEpisodes': 115, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.8307153284993967
'totalSteps': 6400, 'rewardStep': 0.7417052676266817, 'errorList': [], 'lossList': [0.0, -1.4121475023031236, 0.0, 45.58733066558838, 0.0, 0.0, 0.0], 'rewardMean': 0.8518777600773373, 'totalEpisodes': 131, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.388776918232914
'totalSteps': 7680, 'rewardStep': 0.6199367040648032, 'errorList': [], 'lossList': [0.0, -1.3953418284654617, 0.0, 46.1927481174469, 0.0, 0.0, 0.0], 'rewardMean': 0.8132209174085815, 'totalEpisodes': 141, 'stepsPerEpisode': 69, 'rewardPerEpisode': 46.92752574046084
'totalSteps': 8960, 'rewardStep': 0.7145900860755683, 'errorList': [], 'lossList': [0.0, -1.3784473741054535, 0.0, 22.09690981864929, 0.0, 0.0, 0.0], 'rewardMean': 0.7991307986467225, 'totalEpisodes': 147, 'stepsPerEpisode': 277, 'rewardPerEpisode': 196.3852291137934
'totalSteps': 10240, 'rewardStep': 0.6354728605743061, 'errorList': [], 'lossList': [0.0, -1.3840093249082566, 0.0, 40.29956718444824, 0.0, 0.0, 0.0], 'rewardMean': 0.7786735563876704, 'totalEpisodes': 153, 'stepsPerEpisode': 109, 'rewardPerEpisode': 91.1286200424121
'totalSteps': 11520, 'rewardStep': 0.40377380376989686, 'errorList': [], 'lossList': [0.0, -1.36471184194088, 0.0, 19.143990912437438, 0.0, 0.0, 0.0], 'rewardMean': 0.737018028319029, 'totalEpisodes': 156, 'stepsPerEpisode': 334, 'rewardPerEpisode': 242.7424331414177
'totalSteps': 12800, 'rewardStep': 0.8374496430388478, 'errorList': [], 'lossList': [0.0, -1.3326326185464858, 0.0, 15.801726032495498, 0.0, 0.0, 0.0], 'rewardMean': 0.7470611897910109, 'totalEpisodes': 159, 'stepsPerEpisode': 112, 'rewardPerEpisode': 99.32696439331175
'totalSteps': 14080, 'rewardStep': 0.7718961054228977, 'errorList': [], 'lossList': [0.0, -1.309593071937561, 0.0, 13.486719709038734, 0.0, 0.0, 0.0], 'rewardMean': 0.7327687372752177, 'totalEpisodes': 160, 'stepsPerEpisode': 371, 'rewardPerEpisode': 303.787958405432
'totalSteps': 15360, 'rewardStep': 0.7649260025740032, 'errorList': [], 'lossList': [0.0, -1.299706683754921, 0.0, 9.657884716391564, 0.0, 0.0, 0.0], 'rewardMean': 0.734639534681331, 'totalEpisodes': 161, 'stepsPerEpisode': 276, 'rewardPerEpisode': 225.3329331910534
'totalSteps': 16640, 'rewardStep': 0.5942232358890407, 'errorList': [], 'lossList': [0.0, -1.2816152060031891, 0.0, 3.814455561339855, 0.0, 0.0, 0.0], 'rewardMean': 0.7003924424497517, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.8764872143628
'totalSteps': 17920, 'rewardStep': 0.8983238797402139, 'errorList': [], 'lossList': [0.0, -1.2578775882720947, 0.0, 1.5771099090576173, 0.0, 0.0, 0.0], 'rewardMean': 0.698229758877626, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 923.639998935275
'totalSteps': 19200, 'rewardStep': 0.9186036188148148, 'errorList': [], 'lossList': [0.0, -1.2462112694978713, 0.0, 1.8144426765292883, 0.0, 0.0, 0.0], 'rewardMean': 0.7159195939964392, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.4591175026546
'totalSteps': 20480, 'rewardStep': 0.953201598443534, 'errorList': [0.11139262293153042, 0.10614385861248303, 0.11300491376351783, 0.14205464184708572, 0.14007832497278, 0.10337161214632766, 0.14579933377087295, 0.11435969518951082, 0.10239391646879782, 0.14627101782642413, 0.15436464342329034, 0.10828391279221425, 0.1311036972410141, 0.21407131279729621, 0.1639996717945204, 0.1561774601123182, 0.07123615307742122, 0.096231147844416, 0.12894941577158, 0.0962359440540878, 0.21562937196881368, 0.15823238381128482, 0.12150892820745846, 0.13468830510120922, 0.08468505676817685, 0.16413173265470152, 0.14730097638421777, 0.09697457672173741, 0.112140510629673, 0.12643200042586442, 0.16469528726157326, 0.1538682895454754, 0.1821997852550808, 0.11138261836618124, 0.1146276632577451, 0.12353415069047108, 0.13366478026924422, 0.1372883677808343, 0.19619360996170684, 0.10331577033175927, 0.11978517216033789, 0.09695605202486363, 0.13576113941462056, 0.1627413434713699, 0.19108389355050565, 0.14451178166417084, 0.11280220291718003, 0.11066096485714402, 0.14126951537386626, 0.08495027591117624], 'lossList': [0.0, -1.2430319970846175, 0.0, 1.4883825505897403, 0.0, 0.0, 0.0], 'rewardMean': 0.7492460834343123, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.222438284119, 'successfulTests': 48
'totalSteps': 21760, 'rewardStep': 0.9040662070527407, 'errorList': [], 'lossList': [0.0, -1.2303836381435393, 0.0, 0.9596359248831868, 0.0, 0.0, 0.0], 'rewardMean': 0.7681936955320295, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.7812993444434
'totalSteps': 23040, 'rewardStep': 0.8959609928931868, 'errorList': [], 'lossList': [0.0, -1.2096540385484695, 0.0, 0.7811838147044182, 0.0, 0.0, 0.0], 'rewardMean': 0.7942425087639177, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.7700413342175
'totalSteps': 24320, 'rewardStep': 0.8097973561988061, 'errorList': [], 'lossList': [0.0, -1.172013835310936, 0.0, 0.6251082151010633, 0.0, 0.0, 0.0], 'rewardMean': 0.8348448640068085, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.3950940849809
'totalSteps': 25600, 'rewardStep': 0.972326123844091, 'errorList': [0.08575649001390065, 0.03605285592465376, 0.06194035092696315, 0.025138250850580145, 0.05049504714392923, 0.07137949007561557, 0.03515738817711675, 0.053235168683551465, 0.023015321308323313, 0.02344046147679862, 0.06443924276240975, 0.04858613103873584, 0.06893982947394639, 0.021435650259990946, 0.020415883562191543, 0.025641705595975037, 0.06742962659037573, 0.023218487317133742, 0.0491324118089281, 0.04584958714588075, 0.04408022853938568, 0.02764408098696216, 0.03516511943029211, 0.025310606347779506, 0.0979263353338442, 0.036948101925742834, 0.057438875537289706, 0.0634830508424245, 0.06636873684424945, 0.03783863457781184, 0.025846287098810972, 0.04467875806534766, 0.032310198431095935, 0.05545538975549409, 0.056065889812312926, 0.019181578283040752, 0.06421467638873023, 0.0244903334261477, 0.042652802491488166, 0.028618153760086598, 0.023988438637037732, 0.025072345493313566, 0.025016752638746302, 0.021818847300762258, 0.04269474334952293, 0.03739379329232326, 0.03230953015739907, 0.029782445593559418, 0.025887864144837507, 0.043947574545278], 'lossList': [0.0, -1.131689619421959, 0.0, 0.5181622902303934, 0.0, 0.0, 0.0], 'rewardMean': 0.8483325120873328, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1214.3806325630778, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=103.93
