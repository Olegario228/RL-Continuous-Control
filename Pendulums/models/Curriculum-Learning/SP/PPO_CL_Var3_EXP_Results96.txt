#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 96
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.6469884393179501, 'errorList': [], 'lossList': [0.0, -1.4157128411531448, 0.0, 32.00838450908661, 0.0, 0.0, 0.0], 'rewardMean': 0.6200831294490547, 'totalEpisodes': 21, 'stepsPerEpisode': 107, 'rewardPerEpisode': 80.35614022102332
'totalSteps': 3840, 'rewardStep': 0.8067563051209821, 'errorList': [], 'lossList': [0.0, -1.3849610728025437, 0.0, 72.94749822616578, 0.0, 0.0, 0.0], 'rewardMean': 0.6823075213396971, 'totalEpisodes': 69, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.4213744272262794
'totalSteps': 5120, 'rewardStep': 0.7187589361424533, 'errorList': [], 'lossList': [0.0, -1.3673974776268005, 0.0, 66.79200113296508, 0.0, 0.0, 0.0], 'rewardMean': 0.6914203750403862, 'totalEpisodes': 111, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.9231216095384562
'totalSteps': 6400, 'rewardStep': 0.9306073175324299, 'errorList': [], 'lossList': [0.0, -1.3631926882266998, 0.0, 66.09191547393799, 0.0, 0.0, 0.0], 'rewardMean': 0.7392577635387949, 'totalEpisodes': 140, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.467223472088758
'totalSteps': 7680, 'rewardStep': 0.4684970636574774, 'errorList': [], 'lossList': [0.0, -1.360783210992813, 0.0, 48.67233703613281, 0.0, 0.0, 0.0], 'rewardMean': 0.6941309802252421, 'totalEpisodes': 153, 'stepsPerEpisode': 36, 'rewardPerEpisode': 21.803540928727003
'totalSteps': 8960, 'rewardStep': 0.7429274985980265, 'errorList': [], 'lossList': [0.0, -1.3591615128517152, 0.0, 49.56416186332703, 0.0, 0.0, 0.0], 'rewardMean': 0.7011019114213541, 'totalEpisodes': 167, 'stepsPerEpisode': 117, 'rewardPerEpisode': 98.43177956275315
'totalSteps': 10240, 'rewardStep': 0.729133477027522, 'errorList': [], 'lossList': [0.0, -1.3448438602685928, 0.0, 19.331692094802857, 0.0, 0.0, 0.0], 'rewardMean': 0.7046058571221251, 'totalEpisodes': 176, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.050286310827666
'totalSteps': 11520, 'rewardStep': 0.6854381067739234, 'errorList': [], 'lossList': [0.0, -1.3252583158016205, 0.0, 21.761055293083192, 0.0, 0.0, 0.0], 'rewardMean': 0.702476107083436, 'totalEpisodes': 182, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.277161604289363
'totalSteps': 12800, 'rewardStep': 0.08073881395851568, 'errorList': [], 'lossList': [0.0, -1.3107362294197082, 0.0, 12.551923352479935, 0.0, 0.0, 0.0], 'rewardMean': 0.640302377770944, 'totalEpisodes': 186, 'stepsPerEpisode': 169, 'rewardPerEpisode': 109.82063675140279
'totalSteps': 14080, 'rewardStep': 0.9299298702068661, 'errorList': [], 'lossList': [0.0, -1.2883221685886384, 0.0, 22.376228466033936, 0.0, 0.0, 0.0], 'rewardMean': 0.6739775828336148, 'totalEpisodes': 193, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.377798491055202
'totalSteps': 15360, 'rewardStep': 0.9911611371132767, 'errorList': [13.005904652032177, 20.28465222238821, 6.963489557771113, 5.735821519249384, 18.058822187850133, 11.353689917447804, 9.968772510868499, 12.614571700967216, 26.045290936302283, 9.073831189749207, 2.955639138263348, 3.808516987068347, 37.77997341447657, 12.378632900026753, 0.7791821862381865, 0.46451169920450835, 8.739404771759293, 37.205824199884326, 47.38285774614487, 1.6060405217805205, 5.069767144097112, 0.5399903116968862, 1.967564233528618, 2.7125933074181687, 0.5856151299130443, 36.10557430758754, 7.550700094138957, 3.0329516130885783, 1.2012883762107782, 7.406516278012199, 6.299287132377792, 43.93782004060619, 5.88031760065912, 8.879921982869671, 2.2379619590466873, 10.3300532143297, 1.3841248342560954, 1.7782558691454202, 3.584369975280431, 1.1996220948704324, 22.20555475001774, 1.007978104279104, 0.045599533438178415, 17.188889547543877, 0.8039238991629155, 5.720234352558324, 49.922275157242446, 4.676532957095575, 28.696702172792186, 4.75350258904814], 'lossList': [0.0, -1.2885992729663849, 0.0, 6.054252140522003, 0.0, 0.0, 0.0], 'rewardMean': 0.7083948526131474, 'totalEpisodes': 197, 'stepsPerEpisode': 248, 'rewardPerEpisode': 217.04036364123328, 'successfulTests': 1
'totalSteps': 16640, 'rewardStep': 0.7373723444778484, 'errorList': [], 'lossList': [0.0, -1.2842075663805008, 0.0, 6.018238681554794, 0.0, 0.0, 0.0], 'rewardMean': 0.7014564565488339, 'totalEpisodes': 200, 'stepsPerEpisode': 123, 'rewardPerEpisode': 94.6343468752312
'totalSteps': 17920, 'rewardStep': 0.7734038876440991, 'errorList': [], 'lossList': [0.0, -1.274391473531723, 0.0, 4.217209821939468, 0.0, 0.0, 0.0], 'rewardMean': 0.7069209516989985, 'totalEpisodes': 204, 'stepsPerEpisode': 49, 'rewardPerEpisode': 42.24420728345091
'totalSteps': 19200, 'rewardStep': 0.575708301518769, 'errorList': [], 'lossList': [0.0, -1.258533445596695, 0.0, 3.7697819298505784, 0.0, 0.0, 0.0], 'rewardMean': 0.6714310500976324, 'totalEpisodes': 205, 'stepsPerEpisode': 688, 'rewardPerEpisode': 538.4418820280365
'totalSteps': 20480, 'rewardStep': 0.6695903391683129, 'errorList': [], 'lossList': [0.0, -1.238054822087288, 0.0, 3.5909897863864897, 0.0, 0.0, 0.0], 'rewardMean': 0.691540377648716, 'totalEpisodes': 208, 'stepsPerEpisode': 120, 'rewardPerEpisode': 102.20101318768587
'totalSteps': 21760, 'rewardStep': 0.6855545389493025, 'errorList': [], 'lossList': [0.0, -1.2218901020288468, 0.0, 3.0541897308826447, 0.0, 0.0, 0.0], 'rewardMean': 0.6858030816838436, 'totalEpisodes': 209, 'stepsPerEpisode': 528, 'rewardPerEpisode': 457.68402397949086
'totalSteps': 23040, 'rewardStep': 0.8773968907254958, 'errorList': [], 'lossList': [0.0, -1.2043800818920136, 0.0, 1.134148220717907, 0.0, 0.0, 0.0], 'rewardMean': 0.700629423053641, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.8171168727117
'totalSteps': 24320, 'rewardStep': 0.8988554952662153, 'errorList': [], 'lossList': [0.0, -1.1701824551820754, 0.0, 0.8602268039435148, 0.0, 0.0, 0.0], 'rewardMean': 0.7219711619028703, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.1294777004716
'totalSteps': 25600, 'rewardStep': 0.9726611150382263, 'errorList': [0.028610031935951064, 0.01350722527098999, 0.026290728474059714, 0.011695338381026743, 0.015112698512225969, 0.013664656112578449, 0.012155345591538606, 0.009083276325512599, 0.015665754772329118, 0.02053479929959388, 0.039861616857207245, 0.012135635706299556, 0.01745778116229504, 0.023897620418848155, 0.026004875668214935, 0.021459438617112518, 0.022981874741570575, 0.01621473078606328, 0.01413262594532321, 0.009923696575443158, 0.026742878893784516, 0.004911755110755711, 0.02662158707563677, 0.021521340645879357, 0.02443441445936854, 0.04361019445994731, 0.033965410541862175, 0.02207874036266831, 0.017833573574565352, 0.019250246595516732, 0.015689930556119863, 0.014982482218775624, 0.019440990678644764, 0.01711767912831278, 0.02929128258750692, 0.012833293726956336, 0.039805338037577964, 0.02053699693519066, 0.010417221186900425, 0.028098952619936106, 0.03496478585278495, 0.02570448557780468, 0.011033348859695992, 0.029028347089158416, 0.05332289643591206, 0.03871620194531784, 0.015743048009290328, 0.015164800263051207, 0.022961572933223345, 0.014824740120764016], 'lossList': [0.0, -1.1028981626033783, 0.0, 0.7765636697411538, 0.0, 0.0, 0.0], 'rewardMean': 0.8111633920108412, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.9854968220957, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=101.43
