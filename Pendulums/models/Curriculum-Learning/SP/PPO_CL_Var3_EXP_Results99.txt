#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 99
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_EXP_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.7875461818658345, 'errorList': [], 'lossList': [0.0, -1.4240337616205216, 0.0, 27.43349347114563, 0.0, 0.0, 0.0], 'rewardMean': 0.7684185230825578, 'totalEpisodes': 25, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.221584635815056
'totalSteps': 3840, 'rewardStep': 0.9541607278584687, 'errorList': [], 'lossList': [0.0, -1.4251420521736144, 0.0, 49.72544771194458, 0.0, 0.0, 0.0], 'rewardMean': 0.8303325913411946, 'totalEpisodes': 84, 'stepsPerEpisode': 27, 'rewardPerEpisode': 21.953121567196256
'totalSteps': 5120, 'rewardStep': 0.9499581908002125, 'errorList': [], 'lossList': [0.0, -1.4172796016931535, 0.0, 56.36465885162354, 0.0, 0.0, 0.0], 'rewardMean': 0.8602389912059492, 'totalEpisodes': 125, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.921847621255262
'totalSteps': 6400, 'rewardStep': 0.7228196103359972, 'errorList': [], 'lossList': [0.0, -1.3988798940181733, 0.0, 63.999151935577395, 0.0, 0.0, 0.0], 'rewardMean': 0.8327551150319588, 'totalEpisodes': 155, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.532202138200208
'totalSteps': 7680, 'rewardStep': 0.6537440890826105, 'errorList': [], 'lossList': [0.0, -1.3891080993413925, 0.0, 52.92410441398621, 0.0, 0.0, 0.0], 'rewardMean': 0.8029199440404007, 'totalEpisodes': 174, 'stepsPerEpisode': 40, 'rewardPerEpisode': 28.94149467841126
'totalSteps': 8960, 'rewardStep': 0.6390929954440154, 'errorList': [], 'lossList': [0.0, -1.3760907709598542, 0.0, 42.397909564971926, 0.0, 0.0, 0.0], 'rewardMean': 0.7795160942409171, 'totalEpisodes': 188, 'stepsPerEpisode': 113, 'rewardPerEpisode': 72.24620108034894
'totalSteps': 10240, 'rewardStep': 0.23351754322396157, 'errorList': [], 'lossList': [0.0, -1.3561006993055345, 0.0, 13.887678997516632, 0.0, 0.0, 0.0], 'rewardMean': 0.7112662753637977, 'totalEpisodes': 196, 'stepsPerEpisode': 152, 'rewardPerEpisode': 92.43357604127253
'totalSteps': 11520, 'rewardStep': 0.6482806208797314, 'errorList': [], 'lossList': [0.0, -1.3333079046010972, 0.0, 21.42470685005188, 0.0, 0.0, 0.0], 'rewardMean': 0.7042678693100126, 'totalEpisodes': 201, 'stepsPerEpisode': 276, 'rewardPerEpisode': 221.08855415658974
'totalSteps': 12800, 'rewardStep': 0.8175347984426438, 'errorList': [], 'lossList': [0.0, -1.3180132138729095, 0.0, 32.51330758571625, 0.0, 0.0, 0.0], 'rewardMean': 0.7155945622232757, 'totalEpisodes': 208, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.91252947136009
'totalSteps': 14080, 'rewardStep': 0.8031453663011292, 'errorList': [], 'lossList': [0.0, -1.309282466173172, 0.0, 7.928696412444115, 0.0, 0.0, 0.0], 'rewardMean': 0.7209800124234604, 'totalEpisodes': 212, 'stepsPerEpisode': 130, 'rewardPerEpisode': 105.61811506765656
'totalSteps': 15360, 'rewardStep': 0.8389830915690996, 'errorList': [], 'lossList': [0.0, -1.2963544648885728, 0.0, 21.422367750406266, 0.0, 0.0, 0.0], 'rewardMean': 0.7261237033937868, 'totalEpisodes': 216, 'stepsPerEpisode': 238, 'rewardPerEpisode': 201.74043730296316
'totalSteps': 16640, 'rewardStep': 0.49385155122229857, 'errorList': [], 'lossList': [0.0, -1.2889911824464797, 0.0, 3.603373706936836, 0.0, 0.0, 0.0], 'rewardMean': 0.68009278573017, 'totalEpisodes': 220, 'stepsPerEpisode': 242, 'rewardPerEpisode': 197.59936412701677
'totalSteps': 17920, 'rewardStep': 0.5152052341578535, 'errorList': [], 'lossList': [0.0, -1.3036163914203645, 0.0, 3.54850578635931, 0.0, 0.0, 0.0], 'rewardMean': 0.636617490065934, 'totalEpisodes': 222, 'stepsPerEpisode': 824, 'rewardPerEpisode': 680.641074404061
'totalSteps': 19200, 'rewardStep': 0.879936286090305, 'errorList': [], 'lossList': [0.0, -1.3055001044273375, 0.0, 2.5056482788920404, 0.0, 0.0, 0.0], 'rewardMean': 0.6523291576413649, 'totalEpisodes': 226, 'stepsPerEpisode': 71, 'rewardPerEpisode': 64.2134377817328
'totalSteps': 20480, 'rewardStep': 0.9258648130947301, 'errorList': [], 'lossList': [0.0, -1.3002445083856582, 0.0, 3.014462589919567, 0.0, 0.0, 0.0], 'rewardMean': 0.6795412300425767, 'totalEpisodes': 228, 'stepsPerEpisode': 122, 'rewardPerEpisode': 107.5319223226134
'totalSteps': 21760, 'rewardStep': 0.9252707447299385, 'errorList': [], 'lossList': [0.0, -1.2875546944141387, 0.0, 2.006623786985874, 0.0, 0.0, 0.0], 'rewardMean': 0.7081590049711691, 'totalEpisodes': 229, 'stepsPerEpisode': 767, 'rewardPerEpisode': 673.6504893104403
'totalSteps': 23040, 'rewardStep': 0.8237605571281483, 'errorList': [], 'lossList': [0.0, -1.2584377014636994, 0.0, 1.8219241508841515, 0.0, 0.0, 0.0], 'rewardMean': 0.7671833063615877, 'totalEpisodes': 230, 'stepsPerEpisode': 479, 'rewardPerEpisode': 399.68377375002575
'totalSteps': 24320, 'rewardStep': 0.6204314990324835, 'errorList': [], 'lossList': [0.0, -1.2430743765830994, 0.0, 1.4335733765363694, 0.0, 0.0, 0.0], 'rewardMean': 0.7643983941768631, 'totalEpisodes': 230, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.7631372727183
'totalSteps': 25600, 'rewardStep': 0.9918179566918479, 'errorList': [0.04571547233534033, 0.04298452922186411, 0.042893406782043945, 0.06472764763878663, 0.04731962542749611, 0.04303603845011314, 0.04294203644202341, 0.042891564549852386, 0.0476311287550659, 0.06008283725410875, 0.05126089635030896, 0.04308398967583391, 0.042625368099874814, 0.04291222027533448, 0.042887900319188293, 0.06992609777696389, 0.04291324726340244, 0.042492509089665, 0.042691160768226724, 0.0825797428427446, 0.04258289839226847, 0.04270058266092976, 0.07734393392336925, 0.042969402249001344, 0.04282329289339252, 0.046518896735191204, 0.08904531607416609, 0.07238820673044875, 0.056825782258581876, 0.0648983272614501, 0.05742033855251532, 0.07986847616722283, 0.0867904759837558, 0.04230938855081252, 0.10575270831193305, 0.042601093174355766, 0.04280357976637282, 0.04247526886495058, 0.043012392792750716, 0.04285945966571133, 0.07307589574689104, 0.08566556830872472, 0.042687107271481776, 0.0653188292584771, 0.0426170552035659, 0.04230911399722314, 0.042814974919205315, 0.042712116448996876, 0.04231028491558496, 0.10062975948449206], 'lossList': [0.0, -1.2113689029216765, 0.0, 0.8667388071119785, 0.0, 0.0, 0.0], 'rewardMean': 0.7818267100017835, 'totalEpisodes': 230, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.4699824222762, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=83.67
