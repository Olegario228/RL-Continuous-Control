#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.9097811076438134, 'errorList': [], 'lossList': [0.0, -1.4027168262004852, 0.0, 32.89802464485169, 0.0, 0.0, 0.0], 'rewardMean': 0.6770019243096655, 'totalEpisodes': 62, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.477717698127814
'totalSteps': 3840, 'rewardStep': 0.9318336191102454, 'errorList': [], 'lossList': [0.0, -1.379273959994316, 0.0, 48.26119411468506, 0.0, 0.0, 0.0], 'rewardMean': 0.7619458225765255, 'totalEpisodes': 98, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.75613997533984
'totalSteps': 5120, 'rewardStep': 0.45741354102556026, 'errorList': [], 'lossList': [0.0, -1.3602352195978165, 0.0, 69.97273042678833, 0.0, 0.0, 0.0], 'rewardMean': 0.6858127521887842, 'totalEpisodes': 131, 'stepsPerEpisode': 13, 'rewardPerEpisode': 6.289999437381487
'totalSteps': 6400, 'rewardStep': 0.6519400599265622, 'errorList': [], 'lossList': [0.0, -1.3484804356098175, 0.0, 68.85624952316284, 0.0, 0.0, 0.0], 'rewardMean': 0.6790382137363398, 'totalEpisodes': 154, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.45326515242812
'totalSteps': 7680, 'rewardStep': 0.9361225451017355, 'errorList': [5.5970789570579775, 5.918556591955841, 5.973245257494632, 5.988239854435373, 5.952840057183568, 5.846359167241726, 6.074294000490278, 5.891457044524499, 5.639752157488523, 5.497973809722518, 6.08790818515381, 5.952879825219631, 3.676193174977241, 6.092411084773004, 5.911959304602244, 2.998135294441937, 5.488231793317833, 5.9632254900096235, 6.11243208622137, 5.499453052498791, 5.972285299022289, 6.030020529397826, 5.543897802633731, 5.963974723901354, 6.093022935623647, 6.13182645081919, 6.01020460151325, 6.134354379847789, 2.0293555425577736, 6.1181673682143956, 2.78321807488042, 5.999606420049355, 6.0320503472128255, 6.083232587894133, 5.844888879526791, 5.949952483900432, 6.099349885730886, 2.3179945330005265, 6.031363969385199, 6.10494605508182, 6.013605369379324, 6.1412132634130385, 5.916016935984126, 6.522954485985109, 5.967269961198093, 5.914038670468724, 5.7844894116502745, 6.108903136146406, 5.753078737414447, 5.9318502591572875], 'lossList': [0.0, -1.3431105333566666, 0.0, 50.67437753677368, 0.0, 0.0, 0.0], 'rewardMean': 0.7218856022972391, 'totalEpisodes': 170, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.84686530588956, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.8106721087497197, 'errorList': [], 'lossList': [0.0, -1.3417603826522828, 0.0, 40.11269049167633, 0.0, 0.0, 0.0], 'rewardMean': 0.7345693889333077, 'totalEpisodes': 178, 'stepsPerEpisode': 97, 'rewardPerEpisode': 80.99927353872053
'totalSteps': 10240, 'rewardStep': 0.6869176616691395, 'errorList': [], 'lossList': [0.0, -1.3337282943725586, 0.0, 22.550260009765626, 0.0, 0.0, 0.0], 'rewardMean': 0.7286129230252867, 'totalEpisodes': 184, 'stepsPerEpisode': 108, 'rewardPerEpisode': 85.83289981626709
'totalSteps': 11520, 'rewardStep': 0.738544176609402, 'errorList': [], 'lossList': [0.0, -1.3151999151706695, 0.0, 37.7379772901535, 0.0, 0.0, 0.0], 'rewardMean': 0.729716395645744, 'totalEpisodes': 190, 'stepsPerEpisode': 68, 'rewardPerEpisode': 58.27788099123836
'totalSteps': 12800, 'rewardStep': 0.777920868250728, 'errorList': [], 'lossList': [0.0, -1.2977608889341354, 0.0, 25.39345513820648, 0.0, 0.0, 0.0], 'rewardMean': 0.7345368429062423, 'totalEpisodes': 194, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.45226589805416
'totalSteps': 14080, 'rewardStep': 0.7951093496064829, 'errorList': [], 'lossList': [0.0, -1.2877064967155456, 0.0, 23.077194917201997, 0.0, 0.0, 0.0], 'rewardMean': 0.7696255037693389, 'totalEpisodes': 196, 'stepsPerEpisode': 214, 'rewardPerEpisode': 176.3213413655491
'totalSteps': 15360, 'rewardStep': 0.5509020292440883, 'errorList': [], 'lossList': [0.0, -1.2672637265920639, 0.0, 6.509987403154373, 0.0, 0.0, 0.0], 'rewardMean': 0.7337375959293665, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 832.5940571607573
'totalSteps': 16640, 'rewardStep': 0.8447691234555659, 'errorList': [], 'lossList': [0.0, -1.2500142413377762, 0.0, 4.579665597081185, 0.0, 0.0, 0.0], 'rewardMean': 0.7250311463638984, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1022.836863103836
'totalSteps': 17920, 'rewardStep': 0.9117591282242569, 'errorList': [], 'lossList': [0.0, -1.2262840366363525, 0.0, 3.7510769867897036, 0.0, 0.0, 0.0], 'rewardMean': 0.7704657050837682, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.1774292038772
'totalSteps': 19200, 'rewardStep': 0.9466941829318839, 'errorList': [0.08758314630056044, 0.08762731169619153, 0.11623145475008953, 0.1653325302695998, 0.11665730060358352, 0.11924556219513177, 0.09988541440954908, 0.12652343222058385, 0.08830866934759053, 0.12399685198936715, 0.12971034021295338, 0.1306332222434688, 0.19543385385328174, 0.08689372256402515, 0.10693761321432559, 0.10606968168710738, 0.10879204932560836, 0.08141126665884406, 0.12520761757711407, 0.08631010313612582, 0.10885928270233172, 0.1124830448102287, 0.11365294609164758, 0.09945169131145512, 0.0861047331366232, 0.1034100098081141, 0.10368448120547469, 0.0885078344559148, 0.11818563759523708, 0.11645106294948333, 0.13121360749023112, 0.11005287431680395, 0.13632922687932603, 0.09997920352194203, 0.1493281554529258, 0.14843872323003054, 0.1364184012175284, 0.08408108523727931, 0.1481721180284715, 0.09688533384624354, 0.11246543475397723, 0.09572512064500942, 0.10721049035112247, 0.1066249976374706, 0.16487212802524262, 0.09930803892609455, 0.11480548317473264, 0.12408934391566676, 0.09321340379454919, 0.11170864435384797], 'lossList': [0.0, -1.1797081536054612, 0.0, 3.217126977369189, 0.0, 0.0, 0.0], 'rewardMean': 0.7999411173843003, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.6617981875231, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=72.36
