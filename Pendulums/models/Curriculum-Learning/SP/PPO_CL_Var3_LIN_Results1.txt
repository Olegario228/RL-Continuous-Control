#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.729810941603775, 'errorList': [], 'lossList': [0.0, -1.4127692806720733, 0.0, 36.36322093963623, 0.0, 0.0, 0.0], 'rewardMean': 0.7724305372225913, 'totalEpisodes': 62, 'stepsPerEpisode': 39, 'rewardPerEpisode': 31.92926956906632
'totalSteps': 3840, 'rewardStep': 0.7448013975697034, 'errorList': [], 'lossList': [0.0, -1.3941191601753236, 0.0, 50.34375738143921, 0.0, 0.0, 0.0], 'rewardMean': 0.7632208240049619, 'totalEpisodes': 95, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.84024634776313
'totalSteps': 5120, 'rewardStep': 0.8674556549388145, 'errorList': [], 'lossList': [0.0, -1.3841342836618424, 0.0, 62.28579284667969, 0.0, 0.0, 0.0], 'rewardMean': 0.789279531738425, 'totalEpisodes': 119, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.457855526587313
'totalSteps': 6400, 'rewardStep': 0.47435779654850485, 'errorList': [], 'lossList': [0.0, -1.3829724007844926, 0.0, 78.55417942047119, 0.0, 0.0, 0.0], 'rewardMean': 0.726295184700441, 'totalEpisodes': 141, 'stepsPerEpisode': 82, 'rewardPerEpisode': 63.06765469319437
'totalSteps': 7680, 'rewardStep': 0.3982803523586831, 'errorList': [], 'lossList': [0.0, -1.384220640063286, 0.0, 31.554995968341828, 0.0, 0.0, 0.0], 'rewardMean': 0.6716260459768146, 'totalEpisodes': 146, 'stepsPerEpisode': 212, 'rewardPerEpisode': 162.74696279142805
'totalSteps': 8960, 'rewardStep': 0.6236711631214825, 'errorList': [], 'lossList': [0.0, -1.374676643013954, 0.0, 32.45815206050873, 0.0, 0.0, 0.0], 'rewardMean': 0.6647753484260529, 'totalEpisodes': 154, 'stepsPerEpisode': 67, 'rewardPerEpisode': 49.12554030185492
'totalSteps': 10240, 'rewardStep': 0.7696412308483012, 'errorList': [], 'lossList': [0.0, -1.3649534887075425, 0.0, 18.391631572246553, 0.0, 0.0, 0.0], 'rewardMean': 0.6778835837288341, 'totalEpisodes': 157, 'stepsPerEpisode': 55, 'rewardPerEpisode': 47.34472075558248
'totalSteps': 11520, 'rewardStep': 0.5297510654277544, 'errorList': [], 'lossList': [0.0, -1.3688140881061555, 0.0, 21.79346004128456, 0.0, 0.0, 0.0], 'rewardMean': 0.6614244150287141, 'totalEpisodes': 160, 'stepsPerEpisode': 200, 'rewardPerEpisode': 163.80779476063256
'totalSteps': 12800, 'rewardStep': 0.34812561559878963, 'errorList': [], 'lossList': [0.0, -1.38731074988842, 0.0, 10.219862045645714, 0.0, 0.0, 0.0], 'rewardMean': 0.6300945350857216, 'totalEpisodes': 163, 'stepsPerEpisode': 324, 'rewardPerEpisode': 246.5838461375783
'totalSteps': 14080, 'rewardStep': 0.5153807855690428, 'errorList': [], 'lossList': [0.0, -1.3969543880224229, 0.0, 14.154000856876372, 0.0, 0.0, 0.0], 'rewardMean': 0.6001276003584851, 'totalEpisodes': 168, 'stepsPerEpisode': 179, 'rewardPerEpisode': 134.2275659066957
'totalSteps': 15360, 'rewardStep': 0.5571928759526267, 'errorList': [], 'lossList': [0.0, -1.394257554411888, 0.0, 11.041352574825288, 0.0, 0.0, 0.0], 'rewardMean': 0.5828657937933703, 'totalEpisodes': 172, 'stepsPerEpisode': 409, 'rewardPerEpisode': 332.2889393581569
'totalSteps': 16640, 'rewardStep': 0.7421155851597322, 'errorList': [], 'lossList': [0.0, -1.4091637444496155, 0.0, 7.097185130715371, 0.0, 0.0, 0.0], 'rewardMean': 0.5825972125523732, 'totalEpisodes': 174, 'stepsPerEpisode': 551, 'rewardPerEpisode': 458.73418409468866
'totalSteps': 17920, 'rewardStep': 0.5648846457551246, 'errorList': [], 'lossList': [0.0, -1.4184599709510803, 0.0, 5.230767913460731, 0.0, 0.0, 0.0], 'rewardMean': 0.5523401116340043, 'totalEpisodes': 175, 'stepsPerEpisode': 317, 'rewardPerEpisode': 243.55372152566284
'totalSteps': 19200, 'rewardStep': 0.7027711909707282, 'errorList': [], 'lossList': [0.0, -1.4039766585826874, 0.0, 1.4780314806103707, 0.0, 0.0, 0.0], 'rewardMean': 0.5751814510762265, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 848.035838906929
'totalSteps': 20480, 'rewardStep': 0.6990501041773677, 'errorList': [], 'lossList': [0.0, -1.3784752476215363, 0.0, 1.0891517595201732, 0.0, 0.0, 0.0], 'rewardMean': 0.605258426258095, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.1700413229256
'totalSteps': 21760, 'rewardStep': 0.786778902402184, 'errorList': [], 'lossList': [0.0, -1.347774948477745, 0.0, 0.943699142113328, 0.0, 0.0, 0.0], 'rewardMean': 0.6215692001861651, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.5893804861398
'totalSteps': 23040, 'rewardStep': 0.9717215852708659, 'errorList': [0.17457317853942347, 0.2894859174363018, 0.17152487880999567, 0.17755046858892307, 0.13469061016007083, 0.22089520340720606, 0.15920826358806844, 0.20978187051791522, 0.1955063212128551, 0.1605668691359181, 0.21829093208961284, 0.18801612111078675, 0.2235764882022286, 0.16236001339008144, 0.14258954939600446, 0.16493709860206146, 0.25015522032299214, 0.24052999824183402, 0.22234476589305183, 0.16630071421394113, 0.1242786936020644, 0.17950431244620194, 0.16810384405915677, 0.19856997961500836, 0.15892599982048447, 0.2667356567908453, 0.1947486649004843, 0.19054340315102997, 0.16147724587514245, 0.1592117356894074, 0.22260400065060393, 0.19615839606769553, 0.24301478700797582, 0.15897825952697556, 0.19483730674118574, 0.11830197857915334, 0.20361277219941773, 0.15657907264787618, 0.2124728048272564, 0.25390137525993817, 0.20174361946237296, 0.19292948831474874, 0.21380201332063417, 0.15515331196112295, 0.22083171414658953, 0.2649982704709711, 0.2597356665106007, 0.1624215480199539, 0.2151356002761568, 0.23063528915990583], 'lossList': [0.0, -1.3213227444887161, 0.0, 0.61766676325351, 0.0, 0.0, 0.0], 'rewardMean': 0.6417772356284216, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.4810077458535, 'successfulTests': 29
'totalSteps': 24320, 'rewardStep': 0.8322426331914493, 'errorList': [], 'lossList': [0.0, -1.2914053630828857, 0.0, 0.5714927024766803, 0.0, 0.0, 0.0], 'rewardMean': 0.6720263924047911, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1136.965725662721
'totalSteps': 25600, 'rewardStep': 0.995234762202352, 'errorList': [0.22300628676396694, 0.25320458761965003, 0.2673603683178403, 0.29715395295775515, 0.26574962846666034, 0.32120329110575463, 0.19629337050541804, 0.21427403585813257, 0.27522469882496026, 0.19522337022337496, 0.19313706174920164, 0.26491607388527594, 0.4072570017800123, 0.28461151441290244, 0.21646844772638657, 0.25310205622472554, 0.2113372342333452, 0.36880837849615555, 0.4909030978156869, 0.28034174815078594, 0.2757667594013109, 0.27573716636508616, 0.2361575682949883, 0.28895731995420815, 0.2761704450917267, 0.23927947980172853, 0.27741949156949497, 0.2537455493772486, 0.3272988798062846, 0.2530646513545143, 0.26314756623638463, 0.24039273308716158, 0.2456363199635311, 0.23465785440234363, 0.2230272674671203, 0.21477093134784841, 0.31094198216731445, 0.26037421565593366, 0.2689104610621705, 0.22285095015777, 0.2786285918970011, 0.2147670780773632, 0.30035558624785386, 0.2587637839196038, 0.25494388026762344, 0.2428912835852848, 0.2671421821659432, 0.2734902947554994, 0.27755580843990973, 0.22662108375371653], 'lossList': [0.0, -1.2435640609264373, 0.0, 0.622105306237936, 0.0, 0.0, 0.0], 'rewardMean': 0.7367373070651473, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1205.137005284722, 'successfulTests': 3
#maxSuccessfulTests=29, maxSuccessfulTestsAtStep=23040, timeSpent=86.18
