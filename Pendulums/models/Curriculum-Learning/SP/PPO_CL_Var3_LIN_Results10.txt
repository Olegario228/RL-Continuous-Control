#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 10
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.8898650759872552, 'errorList': [], 'lossList': [0.0, -1.4199781000614167, 0.0, 23.981454558372498, 0.0, 0.0, 0.0], 'rewardMean': 0.844832018243109, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 361.53706155408275
'totalSteps': 3840, 'rewardStep': 0.4712097278764746, 'errorList': [], 'lossList': [0.0, -1.4221240544319154, 0.0, 45.397081499099734, 0.0, 0.0, 0.0], 'rewardMean': 0.7202912547875643, 'totalEpisodes': 19, 'stepsPerEpisode': 165, 'rewardPerEpisode': 117.83135994442748
'totalSteps': 5120, 'rewardStep': 0.6367638564646318, 'errorList': [], 'lossList': [0.0, -1.4165802198648452, 0.0, 52.28372433662415, 0.0, 0.0, 0.0], 'rewardMean': 0.6994094052068311, 'totalEpisodes': 29, 'stepsPerEpisode': 103, 'rewardPerEpisode': 71.37177428338076
'totalSteps': 6400, 'rewardStep': 0.6572662873729835, 'errorList': [], 'lossList': [0.0, -1.4116294550895692, 0.0, 124.07774158477783, 0.0, 0.0, 0.0], 'rewardMean': 0.6909807816400616, 'totalEpisodes': 68, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.206217307472329
'totalSteps': 7680, 'rewardStep': 0.6713456269745157, 'errorList': [], 'lossList': [0.0, -1.3957020181417465, 0.0, 84.85819620132446, 0.0, 0.0, 0.0], 'rewardMean': 0.6877082558624706, 'totalEpisodes': 100, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.227946413617186
'totalSteps': 8960, 'rewardStep': 0.8504723079670289, 'errorList': [], 'lossList': [0.0, -1.376050932407379, 0.0, 67.01567565917969, 0.0, 0.0, 0.0], 'rewardMean': 0.710960263305979, 'totalEpisodes': 122, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.616977091793853
'totalSteps': 10240, 'rewardStep': 0.8866593745995951, 'errorList': [], 'lossList': [0.0, -1.3594693344831468, 0.0, 52.874841947555545, 0.0, 0.0, 0.0], 'rewardMean': 0.7329226522176809, 'totalEpisodes': 137, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.410814813895326
'totalSteps': 11520, 'rewardStep': 0.7883667493359972, 'errorList': [], 'lossList': [0.0, -1.349964166879654, 0.0, 49.82743087768554, 0.0, 0.0, 0.0], 'rewardMean': 0.7390831074530494, 'totalEpisodes': 145, 'stepsPerEpisode': 70, 'rewardPerEpisode': 53.65553138647528
'totalSteps': 12800, 'rewardStep': 0.3115206189635371, 'errorList': [], 'lossList': [0.0, -1.3470134484767913, 0.0, 34.1717316365242, 0.0, 0.0, 0.0], 'rewardMean': 0.6963268586040982, 'totalEpisodes': 149, 'stepsPerEpisode': 534, 'rewardPerEpisode': 378.52298514152767
'totalSteps': 14080, 'rewardStep': 0.5740783186767132, 'errorList': [], 'lossList': [0.0, -1.3291720014810562, 0.0, 26.411121850013732, 0.0, 0.0, 0.0], 'rewardMean': 0.6737547944218732, 'totalEpisodes': 155, 'stepsPerEpisode': 70, 'rewardPerEpisode': 58.85144720655151
'totalSteps': 15360, 'rewardStep': 0.784091962451135, 'errorList': [], 'lossList': [0.0, -1.3150114983320236, 0.0, 38.365399856567386, 0.0, 0.0, 0.0], 'rewardMean': 0.6631774830682613, 'totalEpisodes': 163, 'stepsPerEpisode': 72, 'rewardPerEpisode': 61.2040006219369
'totalSteps': 16640, 'rewardStep': 0.9530332841953612, 'errorList': [7.616434324648229, 28.194507546454645, 5.189527272343578, 33.2484173188895, 14.267029829099558, 1.3223112740116794, 9.473431032016856, 12.576430245955335, 11.08885512331615, 19.73201029556323, 24.88761141449755, 30.485715412613843, 17.46178054852203, 29.171531296745048, 18.43892289737365, 21.429502291871888, 14.280152826377956, 27.680176052450406, 2.969981159608131, 22.309923054836382, 5.644667369608901, 28.743071105869276, 3.5960975713779644, 15.152230548906635, 6.734556529616682, 8.834172760715765, 24.064379529786407, 25.73247525444958, 12.766199265712247, 20.205659056812795, 11.212238924780618, 4.974823769104584, 25.80210353731736, 19.084307680518286, 11.889156021725118, 0.5013660284432199, 16.130859031307537, 10.20302802011954, 24.243236708703865, 28.589284486984486, 5.349161213651983, 3.478629428546076, 13.050305549266024, 7.859883825603939, 2.7328356560073193, 0.9720858495302801, 2.476599120077636, 22.06564200623339, 17.763573844162057, 19.170660318486455], 'lossList': [0.0, -1.3120923709869385, 0.0, 18.09200355052948, 0.0, 0.0, 0.0], 'rewardMean': 0.7113598387001498, 'totalEpisodes': 168, 'stepsPerEpisode': 48, 'rewardPerEpisode': 44.344208808734734, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.8881385617812861, 'errorList': [], 'lossList': [0.0, -1.305611332654953, 0.0, 22.210254938602446, 0.0, 0.0, 0.0], 'rewardMean': 0.7364973092318152, 'totalEpisodes': 174, 'stepsPerEpisode': 86, 'rewardPerEpisode': 72.55955164895576
'totalSteps': 19200, 'rewardStep': 0.9177376705781441, 'errorList': [], 'lossList': [0.0, -1.2972125262022018, 0.0, 34.217663638591766, 0.0, 0.0, 0.0], 'rewardMean': 0.7625444475523314, 'totalEpisodes': 179, 'stepsPerEpisode': 32, 'rewardPerEpisode': 26.23632530274208
'totalSteps': 20480, 'rewardStep': 0.8823142297675781, 'errorList': [], 'lossList': [0.0, -1.312194700241089, 0.0, 10.12055373251438, 0.0, 0.0, 0.0], 'rewardMean': 0.7836413078316375, 'totalEpisodes': 183, 'stepsPerEpisode': 269, 'rewardPerEpisode': 227.6637123486725
'totalSteps': 21760, 'rewardStep': 0.8554297521299747, 'errorList': [], 'lossList': [0.0, -1.3421176517009734, 0.0, 9.873885545134545, 0.0, 0.0, 0.0], 'rewardMean': 0.7841370522479323, 'totalEpisodes': 187, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.531963942361
'totalSteps': 23040, 'rewardStep': 0.2700209154240581, 'errorList': [], 'lossList': [0.0, -1.3614440619945527, 0.0, 4.415016462802887, 0.0, 0.0, 0.0], 'rewardMean': 0.7224732063303784, 'totalEpisodes': 191, 'stepsPerEpisode': 150, 'rewardPerEpisode': 101.48458936929343
'totalSteps': 24320, 'rewardStep': 0.6060578801111294, 'errorList': [], 'lossList': [0.0, -1.3515970700979232, 0.0, 3.6625914561748503, 0.0, 0.0, 0.0], 'rewardMean': 0.7042423194078917, 'totalEpisodes': 192, 'stepsPerEpisode': 1256, 'rewardPerEpisode': 1088.607881640879
'totalSteps': 25600, 'rewardStep': 0.6421339043602153, 'errorList': [], 'lossList': [0.0, -1.3443437880277633, 0.0, 3.737475402057171, 0.0, 0.0, 0.0], 'rewardMean': 0.7373036479475596, 'totalEpisodes': 196, 'stepsPerEpisode': 612, 'rewardPerEpisode': 514.2006242762025
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=68.54
