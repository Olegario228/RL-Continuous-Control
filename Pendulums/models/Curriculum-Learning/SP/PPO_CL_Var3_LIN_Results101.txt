#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 101
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.6279702724192402, 'errorList': [], 'lossList': [0.0, -1.4109414905309676, 0.0, 33.19052656173706, 0.0, 0.0, 0.0], 'rewardMean': 0.7215102026303237, 'totalEpisodes': 58, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.87773834302827
'totalSteps': 3840, 'rewardStep': 0.7348647931939242, 'errorList': [], 'lossList': [0.0, -1.3851399284601211, 0.0, 44.86552929878235, 0.0, 0.0, 0.0], 'rewardMean': 0.7259617328181905, 'totalEpisodes': 78, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.964519596112634
'totalSteps': 5120, 'rewardStep': 0.6129455397880395, 'errorList': [], 'lossList': [0.0, -1.3669174164533615, 0.0, 42.97277173042298, 0.0, 0.0, 0.0], 'rewardMean': 0.6977076845606528, 'totalEpisodes': 89, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.832456561274866
'totalSteps': 6400, 'rewardStep': 0.5008923736723495, 'errorList': [], 'lossList': [0.0, -1.3654531639814378, 0.0, 23.11517776250839, 0.0, 0.0, 0.0], 'rewardMean': 0.6583446223829921, 'totalEpisodes': 94, 'stepsPerEpisode': 83, 'rewardPerEpisode': 68.19559935488776
'totalSteps': 7680, 'rewardStep': 0.4125855601008808, 'errorList': [], 'lossList': [0.0, -1.3608508521318436, 0.0, 20.885075098872186, 0.0, 0.0, 0.0], 'rewardMean': 0.6173847786693069, 'totalEpisodes': 96, 'stepsPerEpisode': 418, 'rewardPerEpisode': 331.5327173650788
'totalSteps': 8960, 'rewardStep': 0.6514422709613132, 'errorList': [], 'lossList': [0.0, -1.3506841498613358, 0.0, 95.03656051635743, 0.0, 0.0, 0.0], 'rewardMean': 0.6222501347110221, 'totalEpisodes': 108, 'stepsPerEpisode': 118, 'rewardPerEpisode': 83.0251878635227
'totalSteps': 10240, 'rewardStep': 0.7638163903215589, 'errorList': [], 'lossList': [0.0, -1.3560233104228974, 0.0, 55.673047361373904, 0.0, 0.0, 0.0], 'rewardMean': 0.6399459166623391, 'totalEpisodes': 119, 'stepsPerEpisode': 30, 'rewardPerEpisode': 26.33835892592808
'totalSteps': 11520, 'rewardStep': 0.5862888702021721, 'errorList': [], 'lossList': [0.0, -1.3484937262535095, 0.0, 58.7837961769104, 0.0, 0.0, 0.0], 'rewardMean': 0.6339840226112095, 'totalEpisodes': 127, 'stepsPerEpisode': 236, 'rewardPerEpisode': 189.09028889992277
'totalSteps': 12800, 'rewardStep': 0.37796620447328166, 'errorList': [], 'lossList': [0.0, -1.3311607140302657, 0.0, 61.81348030090332, 0.0, 0.0, 0.0], 'rewardMean': 0.6083822407974167, 'totalEpisodes': 137, 'stepsPerEpisode': 149, 'rewardPerEpisode': 104.7053539554044
'totalSteps': 14080, 'rewardStep': 0.34500632379154395, 'errorList': [], 'lossList': [0.0, -1.3110985046625137, 0.0, 12.664782677888871, 0.0, 0.0, 0.0], 'rewardMean': 0.5613778598924304, 'totalEpisodes': 141, 'stepsPerEpisode': 541, 'rewardPerEpisode': 395.84526200434726
'totalSteps': 15360, 'rewardStep': 0.7269000847721595, 'errorList': [], 'lossList': [0.0, -1.2853220146894455, 0.0, 7.30166194319725, 0.0, 0.0, 0.0], 'rewardMean': 0.5712708411277225, 'totalEpisodes': 142, 'stepsPerEpisode': 1257, 'rewardPerEpisode': 886.1079738551895
'totalSteps': 16640, 'rewardStep': 0.8379272026847079, 'errorList': [], 'lossList': [0.0, -1.2625183379650116, 0.0, 5.477490286231041, 0.0, 0.0, 0.0], 'rewardMean': 0.5815770820768007, 'totalEpisodes': 145, 'stepsPerEpisode': 99, 'rewardPerEpisode': 82.01550344477475
'totalSteps': 17920, 'rewardStep': 0.5293871267872556, 'errorList': [], 'lossList': [0.0, -1.2461787450313568, 0.0, 4.721744846105576, 0.0, 0.0, 0.0], 'rewardMean': 0.5732212407767223, 'totalEpisodes': 146, 'stepsPerEpisode': 295, 'rewardPerEpisode': 223.30565641937798
'totalSteps': 19200, 'rewardStep': 0.7435462901626452, 'errorList': [], 'lossList': [0.0, -1.2089189827442168, 0.0, 2.285925151705742, 0.0, 0.0, 0.0], 'rewardMean': 0.597486632425752, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 931.1383082752596
'totalSteps': 20480, 'rewardStep': 0.8342646528368952, 'errorList': [], 'lossList': [0.0, -1.1507502967119216, 0.0, 1.699973908662796, 0.0, 0.0, 0.0], 'rewardMean': 0.6396545416993533, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.8578149692746
'totalSteps': 21760, 'rewardStep': 0.7583955562276941, 'errorList': [], 'lossList': [0.0, -1.1033897244930266, 0.0, 1.6580552913248539, 0.0, 0.0, 0.0], 'rewardMean': 0.6503498702259913, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.9997106449864
'totalSteps': 23040, 'rewardStep': 0.9169983254463767, 'errorList': [], 'lossList': [0.0, -1.0745450001955033, 0.0, 0.7110935056582093, 0.0, 0.0, 0.0], 'rewardMean': 0.6656680637384732, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.5176417099935
'totalSteps': 24320, 'rewardStep': 0.9097064387839975, 'errorList': [], 'lossList': [0.0, -1.0381094872951508, 0.0, 0.7811967376992106, 0.0, 0.0, 0.0], 'rewardMean': 0.6980098205966557, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.0319241663278
'totalSteps': 25600, 'rewardStep': 0.9883740507408413, 'errorList': [0.0370981358376407, 0.032326249601807716, 0.0916115754264829, 0.03408564708729803, 0.03174713211151733, 0.14747427934260543, 0.06841585110990786, 0.09879778118162713, 0.12996845982372227, 0.04068403550856958, 0.03196633646195601, 0.07602699300093638, 0.0706714532214198, 0.035342869455991584, 0.05916980195977844, 0.06740062134062633, 0.09923043174375117, 0.045655073203662654, 0.21308507319824732, 0.11270536991133699, 0.09212629572367687, 0.03861593732186346, 0.08191253744975471, 0.15536753066376333, 0.044786202036676034, 0.03776171514548817, 0.09988130289588461, 0.13695407296651474, 0.07572985673643633, 0.034008099191431045, 0.09868436399647895, 0.03944553654306344, 0.03015980896292845, 0.1394895949872125, 0.16474730096646298, 0.05461742117480373, 0.19765518816917474, 0.05813018696832769, 0.0956809128543091, 0.16592016225315026, 0.03513819017047978, 0.04779191894129758, 0.07070924363654328, 0.15748646947809955, 0.05425714044943418, 0.10783996821281722, 0.0472186446540048, 0.0741350405978046, 0.1564677509444565, 0.03255797131245945], 'lossList': [0.0, -0.9869134545326232, 0.0, 0.8665875651501119, 0.0, 0.0, 0.0], 'rewardMean': 0.7590506052234117, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1210.6212078769627, 'successfulTests': 49
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=25600, timeSpent=83.48
