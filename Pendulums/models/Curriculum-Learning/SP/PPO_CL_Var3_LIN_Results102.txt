#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 102
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.8677119605166249, 'errorList': [], 'lossList': [0.0, -1.4230572813749314, 0.0, 35.04852080345154, 0.0, 0.0, 0.0], 'rewardMean': 0.8416746511501672, 'totalEpisodes': 69, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.805192587121237
'totalSteps': 3840, 'rewardStep': 0.8692171245684007, 'errorList': [], 'lossList': [0.0, -1.404564123749733, 0.0, 41.19440719604492, 0.0, 0.0, 0.0], 'rewardMean': 0.8508554756229117, 'totalEpisodes': 91, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.523405378384941
'totalSteps': 5120, 'rewardStep': 0.7035181891678184, 'errorList': [], 'lossList': [0.0, -1.3741665583848954, 0.0, 42.646214666366575, 0.0, 0.0, 0.0], 'rewardMean': 0.8140211540091384, 'totalEpisodes': 103, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.89816317842407
'totalSteps': 6400, 'rewardStep': 0.5542942165412488, 'errorList': [], 'lossList': [0.0, -1.3644150823354722, 0.0, 52.02884764671326, 0.0, 0.0, 0.0], 'rewardMean': 0.7620757665155604, 'totalEpisodes': 113, 'stepsPerEpisode': 51, 'rewardPerEpisode': 38.845203900642176
'totalSteps': 7680, 'rewardStep': 0.7927732243422723, 'errorList': [], 'lossList': [0.0, -1.3538743507862092, 0.0, 67.3966577911377, 0.0, 0.0, 0.0], 'rewardMean': 0.7671920094866791, 'totalEpisodes': 123, 'stepsPerEpisode': 55, 'rewardPerEpisode': 39.27615177545551
'totalSteps': 8960, 'rewardStep': 0.5341923040757579, 'errorList': [], 'lossList': [0.0, -1.353469830751419, 0.0, 88.75912796020508, 0.0, 0.0, 0.0], 'rewardMean': 0.733906337285119, 'totalEpisodes': 133, 'stepsPerEpisode': 16, 'rewardPerEpisode': 9.221030209821135
'totalSteps': 10240, 'rewardStep': 0.5969412335137592, 'errorList': [], 'lossList': [0.0, -1.346153244972229, 0.0, 89.26220046997071, 0.0, 0.0, 0.0], 'rewardMean': 0.716785699313699, 'totalEpisodes': 146, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.72729349336193
'totalSteps': 11520, 'rewardStep': 0.4163491501987704, 'errorList': [], 'lossList': [0.0, -1.3537196439504624, 0.0, 81.02750938415528, 0.0, 0.0, 0.0], 'rewardMean': 0.6834038605231513, 'totalEpisodes': 156, 'stepsPerEpisode': 65, 'rewardPerEpisode': 40.1269077443353
'totalSteps': 12800, 'rewardStep': 0.8802711443017598, 'errorList': [], 'lossList': [0.0, -1.3644120800495148, 0.0, 60.24666121482849, 0.0, 0.0, 0.0], 'rewardMean': 0.7030905889010122, 'totalEpisodes': 164, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.42403542317685
'totalSteps': 14080, 'rewardStep': 0.7309366184993025, 'errorList': [], 'lossList': [0.0, -1.3725506353378296, 0.0, 32.20607043504715, 0.0, 0.0, 0.0], 'rewardMean': 0.6946205165725714, 'totalEpisodes': 168, 'stepsPerEpisode': 194, 'rewardPerEpisode': 168.11642248287296
'totalSteps': 15360, 'rewardStep': 0.6791727152590175, 'errorList': [], 'lossList': [0.0, -1.3791729480028152, 0.0, 18.621273255348207, 0.0, 0.0, 0.0], 'rewardMean': 0.6757665920468108, 'totalEpisodes': 174, 'stepsPerEpisode': 101, 'rewardPerEpisode': 79.45167541597208
'totalSteps': 16640, 'rewardStep': 0.9471179029670026, 'errorList': [1.392094568664544, 2.7636787598689216, 1.7691788335665704, 1.3791642702467193, 1.0112004418635052, 1.7680470926323397, 2.1579035551253227, 1.322382368572412, 1.555720220502206, 2.0005089567472107, 1.9527350748262564, 1.5933711495954708, 1.4174678460883485, 5.5139189126974, 2.029757917944855, 2.1363775429037215, 2.5345976804197625, 0.8973849911805515, 1.3230528374475612, 2.1787498443926827, 1.5796381671956738, 2.1747939356267962, 1.4686030449066372, 3.316719814536026, 2.1247311100871658, 1.7200667072730695, 3.9697747892558866, 2.3505064284054313, 1.7496157317454906, 1.5292225197869422, 1.132495028095398, 0.8613874607014934, 1.3177060409476042, 2.5065936997492893, 1.5941489015312256, 1.5695433167284794, 1.5764134414404778, 3.6000730362335096, 3.007840617151773, 1.43746797038568, 1.6803964474985347, 4.548059670470091, 2.6512653242580106, 1.5441603430638768, 1.8956100710709594, 1.541669427546872, 1.2584569136875077, 2.211301462130812, 2.6203541717714027, 1.8765266193592585], 'lossList': [0.0, -1.3723099827766418, 0.0, 7.111397862434387, 0.0, 0.0, 0.0], 'rewardMean': 0.683556669886671, 'totalEpisodes': 178, 'stepsPerEpisode': 63, 'rewardPerEpisode': 54.645432019137985, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7727565005720417, 'errorList': [], 'lossList': [0.0, -1.3852034920454026, 0.0, 4.862195283174515, 0.0, 0.0, 0.0], 'rewardMean': 0.6904805010270932, 'totalEpisodes': 181, 'stepsPerEpisode': 243, 'rewardPerEpisode': 200.08004417670875
'totalSteps': 19200, 'rewardStep': 0.6625832446066364, 'errorList': [], 'lossList': [0.0, -1.380969111919403, 0.0, 14.058063856959343, 0.0, 0.0, 0.0], 'rewardMean': 0.7013094038336319, 'totalEpisodes': 183, 'stepsPerEpisode': 755, 'rewardPerEpisode': 509.8837964269773
'totalSteps': 20480, 'rewardStep': 0.8943671635652435, 'errorList': [], 'lossList': [0.0, -1.3555686837434768, 0.0, 5.55809451520443, 0.0, 0.0, 0.0], 'rewardMean': 0.7114687977559292, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.9543525005322
'totalSteps': 21760, 'rewardStep': 0.8666792643867001, 'errorList': [], 'lossList': [0.0, -1.3186622714996339, 0.0, 2.1745290413498877, 0.0, 0.0, 0.0], 'rewardMean': 0.7447174937870233, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.7088197069872
'totalSteps': 23040, 'rewardStep': 0.932129216614274, 'errorList': [0.5163830350243862, 0.554940196849317, 0.5715911711561092, 0.5208530924045895, 0.6041961005205071, 0.615959568642404, 0.5519310010907289, 0.5343236324278043, 0.5393511694768756, 0.5517252265070051, 0.5684545053921668, 0.568740645640978, 0.5459568772782581, 0.5382930187065124, 0.5491736152191445, 0.5843142667009331, 0.6097752281992083, 0.5680283842453413, 0.5378240571374961, 0.5535132536621625, 0.5915281164612304, 0.5384906003317901, 0.551497788479315, 0.5285416062382164, 0.5490460086069824, 0.5368324526722416, 0.5558106333458249, 0.5481079989784288, 0.5427702232889734, 0.584603538941293, 0.5990957709890289, 0.5961159982060432, 0.590418855993498, 0.5720673538451119, 0.5838642991621042, 0.5485428260934408, 0.5253624759062606, 0.5765823355103505, 0.549509352197971, 0.5312655947864281, 0.5876860231210408, 0.5916045851960067, 0.5517637467561218, 0.5677824571305149, 0.5432163380239634, 0.5286126013697926, 0.5921294980058223, 0.5770280422099942, 0.569866544117658, 0.5407322483534649], 'lossList': [0.0, -1.2707372963428498, 0.0, 1.9523913089185954, 0.0, 0.0, 0.0], 'rewardMean': 0.7782362920970749, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.518974267394, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.6417801792461502, 'errorList': [], 'lossList': [0.0, -1.2368326950073243, 0.0, 0.9539244982600212, 0.0, 0.0, 0.0], 'rewardMean': 0.8007793950018127, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 939.8850148965605
'totalSteps': 25600, 'rewardStep': 0.8025214560619671, 'errorList': [], 'lossList': [0.0, -1.20943640768528, 0.0, 1.4964908044040204, 0.0, 0.0, 0.0], 'rewardMean': 0.7930044261778335, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.64653323532
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=103.38
