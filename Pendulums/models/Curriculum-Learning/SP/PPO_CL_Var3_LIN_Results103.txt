#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 103
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.8045622318630474, 'errorList': [], 'lossList': [0.0, -1.4344084495306015, 0.0, 32.77730741500854, 0.0, 0.0, 0.0], 'rewardMean': 0.6547507293017161, 'totalEpisodes': 62, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.2598920029179403
'totalSteps': 3840, 'rewardStep': 0.9655019965342626, 'errorList': [], 'lossList': [0.0, -1.4281811356544494, 0.0, 44.55497407913208, 0.0, 0.0, 0.0], 'rewardMean': 0.7583344850458982, 'totalEpisodes': 83, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.691033599747795
'totalSteps': 5120, 'rewardStep': 0.8436379577864129, 'errorList': [], 'lossList': [0.0, -1.4176088505983353, 0.0, 42.57130168437958, 0.0, 0.0, 0.0], 'rewardMean': 0.7796603532310269, 'totalEpisodes': 94, 'stepsPerEpisode': 71, 'rewardPerEpisode': 51.670021636736095
'totalSteps': 6400, 'rewardStep': 0.9099322506440543, 'errorList': [], 'lossList': [0.0, -1.4030905646085738, 0.0, 34.38492841243744, 0.0, 0.0, 0.0], 'rewardMean': 0.8057147327136324, 'totalEpisodes': 101, 'stepsPerEpisode': 23, 'rewardPerEpisode': 14.506008341080195
'totalSteps': 7680, 'rewardStep': 0.8854363249674948, 'errorList': [], 'lossList': [0.0, -1.3918689483404159, 0.0, 61.542703762054444, 0.0, 0.0, 0.0], 'rewardMean': 0.8190016647559428, 'totalEpisodes': 108, 'stepsPerEpisode': 98, 'rewardPerEpisode': 76.24857111117554
'totalSteps': 8960, 'rewardStep': 0.6041441173164331, 'errorList': [], 'lossList': [0.0, -1.38445292532444, 0.0, 67.82444318771363, 0.0, 0.0, 0.0], 'rewardMean': 0.7883077294074414, 'totalEpisodes': 115, 'stepsPerEpisode': 124, 'rewardPerEpisode': 83.97472972834184
'totalSteps': 10240, 'rewardStep': 0.7869850510041351, 'errorList': [], 'lossList': [0.0, -1.3910332745313645, 0.0, 30.874056718349458, 0.0, 0.0, 0.0], 'rewardMean': 0.7881423946070281, 'totalEpisodes': 123, 'stepsPerEpisode': 66, 'rewardPerEpisode': 56.931811858316266
'totalSteps': 11520, 'rewardStep': 0.8529920270526172, 'errorList': [], 'lossList': [0.0, -1.4001831012964248, 0.0, 40.350680766105654, 0.0, 0.0, 0.0], 'rewardMean': 0.7953479093232046, 'totalEpisodes': 130, 'stepsPerEpisode': 74, 'rewardPerEpisode': 63.414924736673655
'totalSteps': 12800, 'rewardStep': 0.7891830715879171, 'errorList': [], 'lossList': [0.0, -1.4187636500597, 0.0, 30.037020001411438, 0.0, 0.0, 0.0], 'rewardMean': 0.7947314255496759, 'totalEpisodes': 138, 'stepsPerEpisode': 132, 'rewardPerEpisode': 116.01811869589426
'totalSteps': 14080, 'rewardStep': 0.6384042644513747, 'errorList': [], 'lossList': [0.0, -1.4264806520938873, 0.0, 6.638161240816117, 0.0, 0.0, 0.0], 'rewardMean': 0.8080779293207749, 'totalEpisodes': 143, 'stepsPerEpisode': 93, 'rewardPerEpisode': 71.00917398058168
'totalSteps': 15360, 'rewardStep': 0.6106403460274628, 'errorList': [], 'lossList': [0.0, -1.4344885742664337, 0.0, 7.543300226926804, 0.0, 0.0, 0.0], 'rewardMean': 0.7886857407372164, 'totalEpisodes': 149, 'stepsPerEpisode': 150, 'rewardPerEpisode': 111.34963518079557
'totalSteps': 16640, 'rewardStep': 0.8683392580112148, 'errorList': [], 'lossList': [0.0, -1.4517183554172517, 0.0, 5.925867519378662, 0.0, 0.0, 0.0], 'rewardMean': 0.7789694668849118, 'totalEpisodes': 152, 'stepsPerEpisode': 167, 'rewardPerEpisode': 124.53575803358147
'totalSteps': 17920, 'rewardStep': 0.7357620534799235, 'errorList': [], 'lossList': [0.0, -1.4578832304477691, 0.0, 5.282080945372582, 0.0, 0.0, 0.0], 'rewardMean': 0.7681818764542627, 'totalEpisodes': 153, 'stepsPerEpisode': 614, 'rewardPerEpisode': 431.0293050854756
'totalSteps': 19200, 'rewardStep': 0.8879501739540334, 'errorList': [], 'lossList': [0.0, -1.4430506712198257, 0.0, 2.801492733359337, 0.0, 0.0, 0.0], 'rewardMean': 0.7659836687852607, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.3873906008192
'totalSteps': 20480, 'rewardStep': 0.9333767402090737, 'errorList': [0.016806876514434903, 0.012768177119181033, 0.013010096609112856, 0.027925800465650776, 0.014156038821307417, 0.015478324237526996, 0.04002917211886235, 0.021386775093928022, 0.014561714742448174, 0.035022463646436984, 0.01204804426557588, 0.0136551054948734, 0.015905411956921283, 0.023322385906685734, 0.01565900415722054, 0.012894680532193973, 0.027253486103650908, 0.040866572676144367, 0.02907871628663055, 0.02448633605477872, 0.013315999634102507, 0.016589379830493328, 0.017698629880103598, 0.031498920095193235, 0.011995588828110786, 0.01047794167284321, 0.02775606203609397, 0.03200546751178686, 0.038779263308627035, 0.030579249463674005, 0.011353243920690873, 0.040084059963349766, 0.012848889951899351, 0.026388825362502127, 0.051964896143146054, 0.029697069480649197, 0.03927693760485456, 0.013119576452363989, 0.013617325253636252, 0.013174563616102248, 0.013335154325356143, 0.027775318033028584, 0.025645937880814918, 0.02141810536581036, 0.033298886510464434, 0.019820340381490543, 0.011607591280214614, 0.01610554990337387, 0.024020742035171575, 0.03271777915231155], 'lossList': [0.0, -1.3935788786411285, 0.0, 1.8042888191342354, 0.0, 0.0, 0.0], 'rewardMean': 0.7707777103094184, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1120.9903455887206, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=71.5
