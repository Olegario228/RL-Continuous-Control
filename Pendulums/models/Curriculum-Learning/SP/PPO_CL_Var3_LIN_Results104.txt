#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 104
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.6554047496523715, 'errorList': [], 'lossList': [0.0, -1.4110942471027375, 0.0, 34.86728666305542, 0.0, 0.0, 0.0], 'rewardMean': 0.7882474419018893, 'totalEpisodes': 71, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.845453137351655
'totalSteps': 3840, 'rewardStep': 0.9624873261179736, 'errorList': [], 'lossList': [0.0, -1.4101241290569306, 0.0, 38.97295625686645, 0.0, 0.0, 0.0], 'rewardMean': 0.8463274033072508, 'totalEpisodes': 95, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.20788903515851
'totalSteps': 5120, 'rewardStep': 0.8514820923777927, 'errorList': [], 'lossList': [0.0, -1.396373896598816, 0.0, 35.544749217033385, 0.0, 0.0, 0.0], 'rewardMean': 0.8476160755748863, 'totalEpisodes': 109, 'stepsPerEpisode': 40, 'rewardPerEpisode': 31.75989944839247
'totalSteps': 6400, 'rewardStep': 0.8432612745921945, 'errorList': [], 'lossList': [0.0, -1.3914973723888397, 0.0, 39.1855890083313, 0.0, 0.0, 0.0], 'rewardMean': 0.8467451153783478, 'totalEpisodes': 121, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.34861888489435
'totalSteps': 7680, 'rewardStep': 0.6398038004441571, 'errorList': [], 'lossList': [0.0, -1.3846351158618928, 0.0, 59.628284549713136, 0.0, 0.0, 0.0], 'rewardMean': 0.8122548962226493, 'totalEpisodes': 131, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.07929760594027
'totalSteps': 8960, 'rewardStep': 0.7638614023867636, 'errorList': [], 'lossList': [0.0, -1.3767208236455917, 0.0, 109.41594804763794, 0.0, 0.0, 0.0], 'rewardMean': 0.8053415399603799, 'totalEpisodes': 152, 'stepsPerEpisode': 123, 'rewardPerEpisode': 96.20517582687295
'totalSteps': 10240, 'rewardStep': 0.7810304631615369, 'errorList': [], 'lossList': [0.0, -1.3810008370876312, 0.0, 69.30333530426026, 0.0, 0.0, 0.0], 'rewardMean': 0.8023026553605246, 'totalEpisodes': 167, 'stepsPerEpisode': 53, 'rewardPerEpisode': 45.081863908509476
'totalSteps': 11520, 'rewardStep': 0.8152616068266557, 'errorList': [], 'lossList': [0.0, -1.3678024983406067, 0.0, 23.354600677490236, 0.0, 0.0, 0.0], 'rewardMean': 0.8037425388567614, 'totalEpisodes': 177, 'stepsPerEpisode': 82, 'rewardPerEpisode': 71.99809422949704
'totalSteps': 12800, 'rewardStep': 0.5321270726611702, 'errorList': [], 'lossList': [0.0, -1.3482067859172822, 0.0, 13.09639753460884, 0.0, 0.0, 0.0], 'rewardMean': 0.7765809922372023, 'totalEpisodes': 182, 'stepsPerEpisode': 64, 'rewardPerEpisode': 44.46288257224533
'totalSteps': 14080, 'rewardStep': 0.8202884593136972, 'errorList': [], 'lossList': [0.0, -1.3509845638275146, 0.0, 6.485819777846336, 0.0, 0.0, 0.0], 'rewardMean': 0.7665008247534313, 'totalEpisodes': 189, 'stepsPerEpisode': 101, 'rewardPerEpisode': 87.31084231503804
'totalSteps': 15360, 'rewardStep': 0.6975320965075649, 'errorList': [], 'lossList': [0.0, -1.3599455446004867, 0.0, 25.153047094345094, 0.0, 0.0, 0.0], 'rewardMean': 0.7707135594389506, 'totalEpisodes': 197, 'stepsPerEpisode': 73, 'rewardPerEpisode': 56.56066179660594
'totalSteps': 16640, 'rewardStep': 0.19394821241611615, 'errorList': [], 'lossList': [0.0, -1.345949245095253, 0.0, 5.270371267199517, 0.0, 0.0, 0.0], 'rewardMean': 0.6938596480687649, 'totalEpisodes': 203, 'stepsPerEpisode': 186, 'rewardPerEpisode': 129.9142494996722
'totalSteps': 17920, 'rewardStep': 0.5109885788939177, 'errorList': [], 'lossList': [0.0, -1.3273908162117005, 0.0, 5.908301498293877, 0.0, 0.0, 0.0], 'rewardMean': 0.6598102967203774, 'totalEpisodes': 205, 'stepsPerEpisode': 417, 'rewardPerEpisode': 334.2367080031357
'totalSteps': 19200, 'rewardStep': 0.49168956524992585, 'errorList': [], 'lossList': [0.0, -1.3127063757181168, 0.0, 10.534937438964844, 0.0, 0.0, 0.0], 'rewardMean': 0.6246531257861505, 'totalEpisodes': 206, 'stepsPerEpisode': 901, 'rewardPerEpisode': 649.4384002870023
'totalSteps': 20480, 'rewardStep': 0.7845895647969914, 'errorList': [], 'lossList': [0.0, -1.2767095184326172, 0.0, 4.260740537345409, 0.0, 0.0, 0.0], 'rewardMean': 0.639131702221434, 'totalEpisodes': 207, 'stepsPerEpisode': 1273, 'rewardPerEpisode': 942.8617648669066
'totalSteps': 21760, 'rewardStep': 0.8117190005564175, 'errorList': [], 'lossList': [0.0, -1.2436187326908112, 0.0, 2.3056965667009353, 0.0, 0.0, 0.0], 'rewardMean': 0.6439174620383993, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.2887329024111
'totalSteps': 23040, 'rewardStep': 0.793054366731967, 'errorList': [], 'lossList': [0.0, -1.2221452921628952, 0.0, 1.5257072606682778, 0.0, 0.0, 0.0], 'rewardMean': 0.6451198523954422, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1071.0788136576525
'totalSteps': 24320, 'rewardStep': 0.9062930260900506, 'errorList': [], 'lossList': [0.0, -1.2135554832220077, 0.0, 1.2095413867756724, 0.0, 0.0, 0.0], 'rewardMean': 0.6542229943217819, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.9170185345192
'totalSteps': 25600, 'rewardStep': 0.9590371200705751, 'errorList': [0.07139786455804265, 0.05675278015126631, 0.05261730861312226, 0.06185267644275033, 0.05518322621982221, 0.050534975304273026, 0.052192050391401165, 0.052962509997111606, 0.053624351622795684, 0.051744080990624386, 0.04891046896990697, 0.05755703315030935, 0.06106273809938427, 0.052313089023746755, 0.062259606126524585, 0.0837526744295098, 0.06122745661364505, 0.06359997639689423, 0.05556329927183039, 0.0756056935384541, 0.05579042092694581, 0.05389632917426921, 0.07685927173161618, 0.0617064203510496, 0.05479973123320632, 0.05400793014905332, 0.0814377859123576, 0.08231743345419, 0.06812644795361904, 0.06392718943443802, 0.0681807516973031, 0.07173741838058457, 0.09900781335371579, 0.06559324182162878, 0.10094737486237575, 0.05683838628615717, 0.053722085518474544, 0.061307279438925597, 0.060323101549067254, 0.05526629761611762, 0.0836023467475069, 0.07082643259223859, 0.057605136727060975, 0.06162774279832628, 0.054791340979465745, 0.06476097278017257, 0.057963976661231424, 0.059181328577502114, 0.06435378008304111, 0.10772764869752113], 'lossList': [0.0, -1.1822465497255326, 0.0, 1.0915829554386436, 0.0, 0.0, 0.0], 'rewardMean': 0.6969139990627224, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.780544769132, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.76
