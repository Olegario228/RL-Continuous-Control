#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 108
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.6562399990817833, 'errorList': [], 'lossList': [0.0, -1.4107922834157944, 0.0, 23.24505614042282, 0.0, 0.0, 0.0], 'rewardMean': 0.5002975953136191, 'totalEpisodes': 15, 'stepsPerEpisode': 24, 'rewardPerEpisode': 16.91882157098735
'totalSteps': 3840, 'rewardStep': 0.878424244372732, 'errorList': [], 'lossList': [0.0, -1.3951434141397476, 0.0, 32.7716037273407, 0.0, 0.0, 0.0], 'rewardMean': 0.6263398116666568, 'totalEpisodes': 25, 'stepsPerEpisode': 197, 'rewardPerEpisode': 140.47175915406982
'totalSteps': 5120, 'rewardStep': 0.9169851149619604, 'errorList': [], 'lossList': [0.0, -1.395826326608658, 0.0, 39.18379016876221, 0.0, 0.0, 0.0], 'rewardMean': 0.6990011374904826, 'totalEpisodes': 31, 'stepsPerEpisode': 74, 'rewardPerEpisode': 61.3534391133007
'totalSteps': 6400, 'rewardStep': 0.7575132939322899, 'errorList': [], 'lossList': [0.0, -1.3933589512109756, 0.0, 36.65238921642303, 0.0, 0.0, 0.0], 'rewardMean': 0.7107035687788441, 'totalEpisodes': 39, 'stepsPerEpisode': 20, 'rewardPerEpisode': 17.627628327750365
'totalSteps': 7680, 'rewardStep': 0.8043964510539758, 'errorList': [], 'lossList': [0.0, -1.3877955597639084, 0.0, 73.76018165588378, 0.0, 0.0, 0.0], 'rewardMean': 0.7263190491580328, 'totalEpisodes': 50, 'stepsPerEpisode': 85, 'rewardPerEpisode': 62.31913136595467
'totalSteps': 8960, 'rewardStep': 0.8542244455449718, 'errorList': [], 'lossList': [0.0, -1.3694322627782822, 0.0, 71.82784517288208, 0.0, 0.0, 0.0], 'rewardMean': 0.7445912486418812, 'totalEpisodes': 58, 'stepsPerEpisode': 105, 'rewardPerEpisode': 81.54774713840364
'totalSteps': 10240, 'rewardStep': 0.9667244324483879, 'errorList': [133.83492318843938, 209.81560074777946, 49.50131101734682, 197.20803981747747, 170.1721653246429, 230.4223785913541, 144.84985873721712, 187.52391777401826, 178.4844259696546, 98.12026787951933, 229.20078456848543, 150.11989033291542, 164.00543817052983, 196.58080767350276, 201.57069426551172, 177.86743020499534, 218.88827743467652, 102.2794255268591, 169.8075210610404, 209.38109273958221, 191.7605321482423, 169.1534159732457, 256.69154904377405, 242.1299250330754, 88.72022736275514, 205.1284293014367, 15.699374852291024, 169.42210938760155, 136.6810706127594, 194.40904623291678, 235.3392988285102, 189.8391405264091, 84.78941866094603, 229.72279863321663, 168.04534393358475, 236.01686307930828, 218.98636992230797, 64.82270453887891, 97.33279187531848, 152.03252825230982, 159.62804120556538, 22.95108199496381, 225.00388192364684, 250.64397744067765, 232.2115171483689, 283.31694804523704, 190.50450749405292, 84.73910641380654, 11.929679620413578, 222.58017113925516], 'lossList': [0.0, -1.3604932397603988, 0.0, 139.78865613937378, 0.0, 0.0, 0.0], 'rewardMean': 0.7723578966176945, 'totalEpisodes': 86, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.824477836087002, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.7142244588024044, 'errorList': [], 'lossList': [0.0, -1.360642490386963, 0.0, 50.192627029418944, 0.0, 0.0, 0.0], 'rewardMean': 0.765898625749329, 'totalEpisodes': 102, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.416075488556398
'totalSteps': 12800, 'rewardStep': 0.5571965461564281, 'errorList': [], 'lossList': [0.0, -1.3555545204877852, 0.0, 27.315906620025636, 0.0, 0.0, 0.0], 'rewardMean': 0.7450284177900388, 'totalEpisodes': 112, 'stepsPerEpisode': 63, 'rewardPerEpisode': 50.75616867434797
'totalSteps': 14080, 'rewardStep': 0.8866151807789702, 'errorList': [], 'lossList': [0.0, -1.3445412969589234, 0.0, 13.588601455688476, 0.0, 0.0, 0.0], 'rewardMean': 0.7992544167133904, 'totalEpisodes': 119, 'stepsPerEpisode': 30, 'rewardPerEpisode': 26.805270039144045
'totalSteps': 15360, 'rewardStep': 0.7388208911557982, 'errorList': [], 'lossList': [0.0, -1.3441588878631592, 0.0, 11.323787672519684, 0.0, 0.0, 0.0], 'rewardMean': 0.8075125059207918, 'totalEpisodes': 123, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.38616461752677
'totalSteps': 16640, 'rewardStep': 0.7795625385187464, 'errorList': [], 'lossList': [0.0, -1.3525927394628525, 0.0, 7.9881578373909, 0.0, 0.0, 0.0], 'rewardMean': 0.7976263353353933, 'totalEpisodes': 125, 'stepsPerEpisode': 450, 'rewardPerEpisode': 381.50936095947594
'totalSteps': 17920, 'rewardStep': 0.8849182232385756, 'errorList': [], 'lossList': [0.0, -1.3587780541181564, 0.0, 7.235519570112229, 0.0, 0.0, 0.0], 'rewardMean': 0.7944196461630549, 'totalEpisodes': 127, 'stepsPerEpisode': 243, 'rewardPerEpisode': 220.01427572292994
'totalSteps': 19200, 'rewardStep': 0.8935468407270092, 'errorList': [], 'lossList': [0.0, -1.3433627092838287, 0.0, 3.778359811902046, 0.0, 0.0, 0.0], 'rewardMean': 0.8080230008425268, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.2624043966746
'totalSteps': 20480, 'rewardStep': 0.9172777168208712, 'errorList': [], 'lossList': [0.0, -1.2927640056610108, 0.0, 2.8949565870314835, 0.0, 0.0, 0.0], 'rewardMean': 0.8193111274192162, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.32185887361
'totalSteps': 21760, 'rewardStep': 0.887692463676899, 'errorList': [], 'lossList': [0.0, -1.2484363263845444, 0.0, 1.7197109071910381, 0.0, 0.0, 0.0], 'rewardMean': 0.822657929232409, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.3620296603863
'totalSteps': 23040, 'rewardStep': 0.8428970711682584, 'errorList': [], 'lossList': [0.0, -1.2016965395212174, 0.0, 1.744607119448483, 0.0, 0.0, 0.0], 'rewardMean': 0.810275193104396, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.6517481858127
'totalSteps': 24320, 'rewardStep': 0.9165431280844952, 'errorList': [], 'lossList': [0.0, -1.1444176787137985, 0.0, 1.3115265466459096, 0.0, 0.0, 0.0], 'rewardMean': 0.8305070600326052, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1196.774804741551
'totalSteps': 25600, 'rewardStep': 0.9621274460108454, 'errorList': [0.06740189028215826, 0.07736225641569996, 0.06084265694023016, 0.06183214250564943, 0.06706780206268068, 0.09127759762189203, 0.09373290839661115, 0.06702192284613467, 0.06869255182229426, 0.06481465927252597, 0.0708080647616013, 0.0715059908216912, 0.0834860002646899, 0.11872957645299613, 0.07209754143053586, 0.06679548034682327, 0.06100012883875698, 0.08137223630848787, 0.0807651238571858, 0.11224275067661219, 0.12529510154669227, 0.08613613148922465, 0.0836611482186344, 0.06000918778058276, 0.07886601121964361, 0.07380199920028947, 0.07099450473051461, 0.09975492650508207, 0.0684996975841129, 0.06409499232167576, 0.08506884220691614, 0.11180922011062515, 0.08521849091543994, 0.07270035543982438, 0.06245108624713592, 0.06651540926094247, 0.08049180253460438, 0.058732481969830004, 0.069445661325497, 0.07084028935493908, 0.07081266392300478, 0.06706083760876745, 0.08576488013344831, 0.10629722596055734, 0.0765726937061783, 0.07953485639658976, 0.06754295993198144, 0.06261773587603407, 0.07064250701371798, 0.07639418469093508], 'lossList': [0.0, -1.1175960165262222, 0.0, 1.005393572356552, 0.0, 0.0, 0.0], 'rewardMean': 0.8710001500180468, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1214.094600807307, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=103.17
