#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 109
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8668798749507471, 'errorList': [], 'lossList': [0.0, -1.4117257112264634, 0.0, 21.038055136203766, 0.0, 0.0, 0.0], 'rewardMean': 0.6154749903631679, 'totalEpisodes': 22, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.890734974447913
'totalSteps': 3840, 'rewardStep': 0.6612135509686957, 'errorList': [], 'lossList': [0.0, -1.4167081612348555, 0.0, 32.21520848274231, 0.0, 0.0, 0.0], 'rewardMean': 0.6307211772316772, 'totalEpisodes': 36, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.219546951084595
'totalSteps': 5120, 'rewardStep': 0.771808125352496, 'errorList': [], 'lossList': [0.0, -1.40687546312809, 0.0, 30.374097685813904, 0.0, 0.0, 0.0], 'rewardMean': 0.6659929142618819, 'totalEpisodes': 46, 'stepsPerEpisode': 40, 'rewardPerEpisode': 33.97381021081459
'totalSteps': 6400, 'rewardStep': 0.6994452091494109, 'errorList': [], 'lossList': [0.0, -1.3839966601133347, 0.0, 27.51808170795441, 0.0, 0.0, 0.0], 'rewardMean': 0.6726833732393878, 'totalEpisodes': 52, 'stepsPerEpisode': 349, 'rewardPerEpisode': 226.29280878793932
'totalSteps': 7680, 'rewardStep': 0.7700979774525465, 'errorList': [], 'lossList': [0.0, -1.3743495762348175, 0.0, 49.39519094467163, 0.0, 0.0, 0.0], 'rewardMean': 0.6889191406082475, 'totalEpisodes': 60, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.82543173312738
'totalSteps': 8960, 'rewardStep': 0.9106999090487682, 'errorList': [], 'lossList': [0.0, -1.3731466853618621, 0.0, 93.15218584060669, 0.0, 0.0, 0.0], 'rewardMean': 0.7206021075283219, 'totalEpisodes': 70, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.658630547593519
'totalSteps': 10240, 'rewardStep': 0.7726416293280444, 'errorList': [], 'lossList': [0.0, -1.3749327009916306, 0.0, 116.56083024978638, 0.0, 0.0, 0.0], 'rewardMean': 0.7271070477532872, 'totalEpisodes': 92, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.99313999951394
'totalSteps': 11520, 'rewardStep': 0.6309136849335372, 'errorList': [], 'lossList': [0.0, -1.3736999833583832, 0.0, 31.12864538669586, 0.0, 0.0, 0.0], 'rewardMean': 0.7164188963288705, 'totalEpisodes': 105, 'stepsPerEpisode': 88, 'rewardPerEpisode': 73.27675003332597
'totalSteps': 12800, 'rewardStep': 0.7073544221853344, 'errorList': [], 'lossList': [0.0, -1.3652552312612534, 0.0, 18.832409064769745, 0.0, 0.0, 0.0], 'rewardMean': 0.7155124489145168, 'totalEpisodes': 113, 'stepsPerEpisode': 65, 'rewardPerEpisode': 54.0416092236718
'totalSteps': 14080, 'rewardStep': 0.8506840799315284, 'errorList': [], 'lossList': [0.0, -1.3551951557397843, 0.0, 11.376547338962554, 0.0, 0.0, 0.0], 'rewardMean': 0.7641738463301109, 'totalEpisodes': 118, 'stepsPerEpisode': 164, 'rewardPerEpisode': 136.74318548012923
'totalSteps': 15360, 'rewardStep': 0.5729118234026034, 'errorList': [], 'lossList': [0.0, -1.3587801355123519, 0.0, 25.7450430393219, 0.0, 0.0, 0.0], 'rewardMean': 0.7347770411752965, 'totalEpisodes': 126, 'stepsPerEpisode': 163, 'rewardPerEpisode': 134.14138521046408
'totalSteps': 16640, 'rewardStep': 0.6148870745691579, 'errorList': [], 'lossList': [0.0, -1.3629829168319703, 0.0, 5.836153391599655, 0.0, 0.0, 0.0], 'rewardMean': 0.7301443935353428, 'totalEpisodes': 131, 'stepsPerEpisode': 113, 'rewardPerEpisode': 98.01239333836763
'totalSteps': 17920, 'rewardStep': 0.8390860501581883, 'errorList': [], 'lossList': [0.0, -1.360196288228035, 0.0, 5.6919577896595, 0.0, 0.0, 0.0], 'rewardMean': 0.736872186015912, 'totalEpisodes': 135, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.836135076495928
'totalSteps': 19200, 'rewardStep': 0.859131138707057, 'errorList': [], 'lossList': [0.0, -1.3524984633922577, 0.0, 5.507473055124283, 0.0, 0.0, 0.0], 'rewardMean': 0.7528407789716766, 'totalEpisodes': 140, 'stepsPerEpisode': 100, 'rewardPerEpisode': 86.13232026362901
'totalSteps': 20480, 'rewardStep': 0.9805336902365474, 'errorList': [1.1692493107789796, 0.5278123565648428, 0.8287699293638686, 1.0599889332331194, 1.5605490479441333, 0.9860925523625659, 0.7554220309638748, 1.0761922175803968, 0.4944687556308929, 1.012610900309021, 0.9099522671193078, 1.2317896306967857, 2.1630569265809756, 0.4252789295292247, 1.1327483984264668, 1.210631616520778, 1.1035857221813323, 1.330974639818288, 0.7862899531774036, 0.8867159975721158, 1.275954899753274, 0.49421663830117873, 1.400200184156557, 0.940366100152638, 0.8889163086631823, 1.1837397167689205, 1.3886390858198054, 1.1822790856421803, 0.7000691083006808, 0.9841821023442782, 0.6721148192956647, 0.917717936030016, 0.6979934772628908, 1.0864720928539986, 2.02015920368662, 0.8839503588521918, 0.5712488813895501, 1.0714218762238807, 1.2250173970462424, 1.2857031903823735, 0.8640073612738187, 1.609175572634325, 1.5655343597502256, 0.5262401330836234, 1.2734862765006192, 0.8662060803017283, 1.5018894855103129, 0.9706982540025075, 1.8071522458022495, 0.975221299581617], 'lossList': [0.0, -1.3459041303396224, 0.0, 5.534687532186508, 0.0, 0.0, 0.0], 'rewardMean': 0.7738843502500766, 'totalEpisodes': 143, 'stepsPerEpisode': 82, 'rewardPerEpisode': 70.24010915402148, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.7082531653400789, 'errorList': [], 'lossList': [0.0, -1.3433174264431, 0.0, 3.5424964171648026, 0.0, 0.0, 0.0], 'rewardMean': 0.7536396758792077, 'totalEpisodes': 145, 'stepsPerEpisode': 53, 'rewardPerEpisode': 42.63337401053079
'totalSteps': 23040, 'rewardStep': 0.7335391502467734, 'errorList': [], 'lossList': [0.0, -1.3278001844882965, 0.0, 3.380425199866295, 0.0, 0.0, 0.0], 'rewardMean': 0.7497294279710806, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 868.5204829756499
'totalSteps': 24320, 'rewardStep': 0.832489517834963, 'errorList': [], 'lossList': [0.0, -1.3002605146169663, 0.0, 0.9924502040445805, 0.0, 0.0, 0.0], 'rewardMean': 0.7698870112612233, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.012069156807
'totalSteps': 25600, 'rewardStep': 0.9880585760168611, 'errorList': [0.026561474793155494, 0.026437648909279042, 0.04006912955033811, 0.026099514694536807, 0.02543380166257421, 0.025502906865304577, 0.03790587551894386, 0.037330424088565343, 0.0717334327701404, 0.02521897194979517, 0.026278379269461093, 0.06543973292115039, 0.024641185220567963, 0.03271000069241627, 0.03816402649287392, 0.030213654834830433, 0.025252379230308625, 0.026157605341654837, 0.026751053049163455, 0.02560020109098646, 0.026452536935901765, 0.029983254592575167, 0.025552749183853373, 0.02524811616362133, 0.04488662688216622, 0.02555082538824242, 0.053650252038153864, 0.026899233888714166, 0.03809131988360866, 0.026019272436778526, 0.04561845140547857, 0.044047149525505404, 0.03078660549522905, 0.05967523440722032, 0.025990743112152416, 0.03273367249590497, 0.02632099162730556, 0.024841083592184215, 0.025418536941635168, 0.026949675935511175, 0.02470570699019015, 0.025489875245142492, 0.036815067070957073, 0.02615935707392966, 0.025075997221070036, 0.02672577365457955, 0.02498895497894196, 0.02471979759817786, 0.058287025496625365, 0.02585081336143755], 'lossList': [0.0, -1.277363977432251, 0.0, 0.8657626035809517, 0.0, 0.0, 0.0], 'rewardMean': 0.7979574266443759, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.5760365672058, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=100.7
