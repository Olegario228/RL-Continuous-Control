#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 11
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.6292785906709307, 'errorList': [], 'lossList': [0.0, -1.4211455363035201, 0.0, 29.649145467281343, 0.0, 0.0, 0.0], 'rewardMean': 0.5730362461851918, 'totalEpisodes': 14, 'stepsPerEpisode': 98, 'rewardPerEpisode': 74.78078422102293
'totalSteps': 3840, 'rewardStep': 0.6266620423868475, 'errorList': [], 'lossList': [0.0, -1.4146272426843642, 0.0, 36.26358347415924, 0.0, 0.0, 0.0], 'rewardMean': 0.5909115115857437, 'totalEpisodes': 23, 'stepsPerEpisode': 380, 'rewardPerEpisode': 245.42028753524852
'totalSteps': 5120, 'rewardStep': 0.9500646313255702, 'errorList': [283.34746652755445, 317.79897079770035, 228.39042110146755, 287.6713226548947, 284.18769085053435, 305.64714530136814, 282.7241959085192, 303.2985811702086, 281.23181148714417, 281.5832487687689, 274.1116912777927, 264.7289123315528, 294.15420827553896, 322.78342865927925, 312.87306011375176, 275.39502315445185, 309.3599425390309, 294.2724467399276, 307.6061567515304, 227.2904519306385, 247.0617036807174, 303.46272796386387, 322.4419744783873, 284.6114498540991, 308.40945716860386, 325.9102568323003, 316.0853897387611, 312.171674760386, 303.3634739723813, 322.8257370515018, 280.9488612235976, 295.8474523187427, 296.1397515127231, 307.6663191225026, 263.2194561866997, 321.3344476119742, 277.96164558477614, 294.9526153477464, 280.96657135675224, 238.59103299509923, 303.7088804547134, 311.9929064923701, 303.00793292273465, 290.3991860420432, 307.82330137175535, 318.83370706822006, 271.9691911318372, 287.0740452387654, 285.79290206859395, 278.1705189614292], 'lossList': [0.0, -1.4051838332414628, 0.0, 53.54252501487732, 0.0, 0.0, 0.0], 'rewardMean': 0.6806997915207004, 'totalEpisodes': 37, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.82068577368575, 'successfulTests': 0
'totalSteps': 6400, 'rewardStep': 0.6375521835400415, 'errorList': [], 'lossList': [0.0, -1.392816408276558, 0.0, 133.04282947540284, 0.0, 0.0, 0.0], 'rewardMean': 0.6720702699245686, 'totalEpisodes': 71, 'stepsPerEpisode': 40, 'rewardPerEpisode': 30.809285066563888
'totalSteps': 7680, 'rewardStep': 0.8564235382425816, 'errorList': [], 'lossList': [0.0, -1.3857695174217224, 0.0, 47.816514377594, 0.0, 0.0, 0.0], 'rewardMean': 0.7027958146442375, 'totalEpisodes': 84, 'stepsPerEpisode': 25, 'rewardPerEpisode': 17.06301993485533
'totalSteps': 8960, 'rewardStep': 0.8749758983632544, 'errorList': [], 'lossList': [0.0, -1.3770608806610107, 0.0, 74.04492875099182, 0.0, 0.0, 0.0], 'rewardMean': 0.7273929694612399, 'totalEpisodes': 99, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.613309516397031
'totalSteps': 10240, 'rewardStep': 0.7513768990004954, 'errorList': [], 'lossList': [0.0, -1.376296222805977, 0.0, 15.687343850135804, 0.0, 0.0, 0.0], 'rewardMean': 0.7303909606536467, 'totalEpisodes': 105, 'stepsPerEpisode': 54, 'rewardPerEpisode': 47.554427031523076
'totalSteps': 11520, 'rewardStep': 0.342329584228804, 'errorList': [], 'lossList': [0.0, -1.3697335886955262, 0.0, 18.658361439704894, 0.0, 0.0, 0.0], 'rewardMean': 0.6872730299397753, 'totalEpisodes': 110, 'stepsPerEpisode': 186, 'rewardPerEpisode': 136.26494167465268
'totalSteps': 12800, 'rewardStep': 0.7622125869472374, 'errorList': [], 'lossList': [0.0, -1.3498503011465073, 0.0, 12.253313089609145, 0.0, 0.0, 0.0], 'rewardMean': 0.6947669856405215, 'totalEpisodes': 113, 'stepsPerEpisode': 575, 'rewardPerEpisode': 471.02602392453474
'totalSteps': 14080, 'rewardStep': 0.7734087633881054, 'errorList': [], 'lossList': [0.0, -1.3352432370185852, 0.0, 6.468482871651649, 0.0, 0.0, 0.0], 'rewardMean': 0.7204284718093867, 'totalEpisodes': 115, 'stepsPerEpisode': 561, 'rewardPerEpisode': 478.37817553903926
'totalSteps': 15360, 'rewardStep': 0.5706650639631672, 'errorList': [], 'lossList': [0.0, -1.3341831880807877, 0.0, 4.447927974760533, 0.0, 0.0, 0.0], 'rewardMean': 0.7145671191386105, 'totalEpisodes': 116, 'stepsPerEpisode': 499, 'rewardPerEpisode': 362.69152373416216
'totalSteps': 16640, 'rewardStep': 0.809817483927018, 'errorList': [], 'lossList': [0.0, -1.3103832805156708, 0.0, 2.9779559329152105, 0.0, 0.0, 0.0], 'rewardMean': 0.7328826632926275, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 842.5835712554069
'totalSteps': 17920, 'rewardStep': 0.8996323351653113, 'errorList': [], 'lossList': [0.0, -1.2790737628936768, 0.0, 2.783753657191992, 0.0, 0.0, 0.0], 'rewardMean': 0.7278394336766016, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.3689747962653
'totalSteps': 19200, 'rewardStep': 0.8819474633922962, 'errorList': [], 'lossList': [0.0, -1.2463932079076767, 0.0, 2.19854621887207, 0.0, 0.0, 0.0], 'rewardMean': 0.7522789616618272, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.5036671449543
'totalSteps': 20480, 'rewardStep': 0.8844314245592139, 'errorList': [], 'lossList': [0.0, -1.1777932411432266, 0.0, 1.3207528316229582, 0.0, 0.0, 0.0], 'rewardMean': 0.7550797502934903, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.8265927858838
'totalSteps': 21760, 'rewardStep': 0.8308380945023208, 'errorList': [], 'lossList': [0.0, -1.121364126801491, 0.0, 1.0468751546926796, 0.0, 0.0, 0.0], 'rewardMean': 0.750665969907397, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.347486001255
'totalSteps': 23040, 'rewardStep': 0.9880567078056621, 'errorList': [0.04971834185202068, 0.044407227245214856, 0.03649556516589013, 0.040411198984752005, 0.0663768561873512, 0.04053815950576625, 0.042223702287480076, 0.05430571335387208, 0.03195247073637422, 0.05044821252052176, 0.03425658230712859, 0.030512549429308776, 0.04196168178024907, 0.039939923520870266, 0.0424913982020447, 0.03934149778092054, 0.05368724937290338, 0.030991659344843613, 0.041610678224243546, 0.03931006133778238, 0.061537719697669975, 0.043077256502979916, 0.033223178410943084, 0.05385333581428981, 0.04419108258093597, 0.030641105289362244, 0.053757481961987555, 0.028614046530179973, 0.030246023034735794, 0.03917840168646409, 0.033460961316730445, 0.03063974240059354, 0.05775102653691943, 0.05220458710591802, 0.03197689038903297, 0.0294009156422074, 0.03757376018981147, 0.03667197651485933, 0.049131761419594716, 0.033725306521559424, 0.039834810960971606, 0.054729458884538736, 0.054091076459468465, 0.07906788606273973, 0.03494785298350976, 0.04405515405560147, 0.03280185132894754, 0.03830921604031274, 0.030722547985182976, 0.03399970606537999], 'lossList': [0.0, -1.089699030518532, 0.0, 0.7432170476112515, 0.0, 0.0, 0.0], 'rewardMean': 0.7743339507879136, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1200.041367143097, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=81.01
