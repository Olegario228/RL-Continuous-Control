#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 111
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7873731395362751, 'errorList': [], 'lossList': [0.0, -1.4375958174467087, 0.0, 33.16258251667023, 0.0, 0.0, 0.0], 'rewardMean': 0.6520835206178639, 'totalEpisodes': 13, 'stepsPerEpisode': 81, 'rewardPerEpisode': 67.56436950637566
'totalSteps': 3840, 'rewardStep': 0.9044303714555234, 'errorList': [], 'lossList': [0.0, -1.4491905975341797, 0.0, 26.885989027023314, 0.0, 0.0, 0.0], 'rewardMean': 0.7361991375637503, 'totalEpisodes': 16, 'stepsPerEpisode': 544, 'rewardPerEpisode': 397.4931379628941
'totalSteps': 5120, 'rewardStep': 0.654519709426612, 'errorList': [], 'lossList': [0.0, -1.443871141076088, 0.0, 25.842873306274413, 0.0, 0.0, 0.0], 'rewardMean': 0.7157792805294658, 'totalEpisodes': 21, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.994892056926203
'totalSteps': 6400, 'rewardStep': 0.6913976989605684, 'errorList': [], 'lossList': [0.0, -1.4303424578905106, 0.0, 49.614586439132694, 0.0, 0.0, 0.0], 'rewardMean': 0.7109029642156863, 'totalEpisodes': 26, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.282621343277834
'totalSteps': 7680, 'rewardStep': 0.6800769602312966, 'errorList': [], 'lossList': [0.0, -1.4018088120222092, 0.0, 10.411289115846158, 0.0, 0.0, 0.0], 'rewardMean': 0.7057652968849547, 'totalEpisodes': 26, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 955.5258968279275
'totalSteps': 8960, 'rewardStep': 0.3824054129692077, 'errorList': [], 'lossList': [0.0, -1.374522904753685, 0.0, 65.97367002487182, 0.0, 0.0, 0.0], 'rewardMean': 0.6595710277541337, 'totalEpisodes': 32, 'stepsPerEpisode': 443, 'rewardPerEpisode': 292.85297704359033
'totalSteps': 10240, 'rewardStep': 0.7604637668248955, 'errorList': [], 'lossList': [0.0, -1.3767384856939315, 0.0, 232.33489868164062, 0.0, 0.0, 0.0], 'rewardMean': 0.6721826201379789, 'totalEpisodes': 58, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.20391445628699
'totalSteps': 11520, 'rewardStep': 0.6978258543767376, 'errorList': [], 'lossList': [0.0, -1.3741006845235824, 0.0, 91.35895483016968, 0.0, 0.0, 0.0], 'rewardMean': 0.6750318683867298, 'totalEpisodes': 79, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6978258543767376
'totalSteps': 12800, 'rewardStep': 0.6509480217117367, 'errorList': [], 'lossList': [0.0, -1.368834131360054, 0.0, 55.01430575370789, 0.0, 0.0, 0.0], 'rewardMean': 0.6726234837192305, 'totalEpisodes': 97, 'stepsPerEpisode': 53, 'rewardPerEpisode': 38.45229116658683
'totalSteps': 14080, 'rewardStep': 0.5797209084378141, 'errorList': [], 'lossList': [0.0, -1.352154797911644, 0.0, 18.746433639526366, 0.0, 0.0, 0.0], 'rewardMean': 0.6789161843930667, 'totalEpisodes': 105, 'stepsPerEpisode': 292, 'rewardPerEpisode': 207.12643102282559
'totalSteps': 15360, 'rewardStep': 0.46109031441211823, 'errorList': [], 'lossList': [0.0, -1.3398576253652572, 0.0, 12.508430211544036, 0.0, 0.0, 0.0], 'rewardMean': 0.646287901880651, 'totalEpisodes': 107, 'stepsPerEpisode': 696, 'rewardPerEpisode': 447.20454535365275
'totalSteps': 16640, 'rewardStep': 0.8107964334666334, 'errorList': [], 'lossList': [0.0, -1.3208493047952652, 0.0, 25.0922616648674, 0.0, 0.0, 0.0], 'rewardMean': 0.6369245080817619, 'totalEpisodes': 110, 'stepsPerEpisode': 67, 'rewardPerEpisode': 57.24006597535999
'totalSteps': 17920, 'rewardStep': 0.7658549027670076, 'errorList': [], 'lossList': [0.0, -1.2919179701805115, 0.0, 8.088454829454422, 0.0, 0.0, 0.0], 'rewardMean': 0.6480580274158017, 'totalEpisodes': 112, 'stepsPerEpisode': 97, 'rewardPerEpisode': 74.25018912391664
'totalSteps': 19200, 'rewardStep': 0.9591322325048615, 'errorList': [0.2876601834352677, 0.28783043933364333, 0.27151835884260017, 0.2826878584091276, 0.20698942178131965, 0.22280297557350046, 0.3709031535015554, 0.19181401513252608, 0.39680353575823907, 0.26460606602054104, 0.23677122037137058, 0.28254767310390677, 0.265350429132193, 0.34141134570865683, 0.21209225311298266, 0.3758964776696765, 0.2962206355643341, 0.3087872827719334, 0.3371199728891631, 0.32152815744863406, 0.3481644990130545, 0.4316652873661788, 0.2754359087065908, 0.3044720596788358, 0.3889673507061825, 0.2766796205490564, 0.36260941940893904, 0.3761971866568039, 0.26699716367128246, 0.27848535632949273, 0.30872879450835555, 0.3379282743415199, 0.41810106267197983, 0.27711759958777693, 0.3141129063328535, 0.21697084921840562, 0.23519002059041766, 0.1924715146837724, 0.38552821283128275, 0.27595028470801924, 0.19912340699582257, 0.4484503249126112, 0.3298235042246708, 0.22986270805802908, 0.34559660103225115, 0.22706609626333463, 0.25925406672179385, 0.23651126912652587, 0.26691289356401715, 0.24649485196314136], 'lossList': [0.0, -1.2631156837940216, 0.0, 5.1206973981857296, 0.0, 0.0, 0.0], 'rewardMean': 0.6748314807702309, 'totalEpisodes': 113, 'stepsPerEpisode': 285, 'rewardPerEpisode': 240.75082186057733, 'successfulTests': 3
'totalSteps': 20480, 'rewardStep': 0.5432548001016144, 'errorList': [], 'lossList': [0.0, -1.2398086750507356, 0.0, 9.57978774189949, 0.0, 0.0, 0.0], 'rewardMean': 0.6611492647572627, 'totalEpisodes': 114, 'stepsPerEpisode': 674, 'rewardPerEpisode': 547.3072441715537
'totalSteps': 21760, 'rewardStep': 0.7381473873283573, 'errorList': [], 'lossList': [0.0, -1.2252985900640487, 0.0, 2.7377523627877234, 0.0, 0.0, 0.0], 'rewardMean': 0.6967234621931777, 'totalEpisodes': 114, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 977.2270208371074
'totalSteps': 23040, 'rewardStep': 0.9062552078139761, 'errorList': [], 'lossList': [0.0, -1.2122434121370316, 0.0, 2.017125113531947, 0.0, 0.0, 0.0], 'rewardMean': 0.7113026062920857, 'totalEpisodes': 114, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.8448103586954
'totalSteps': 24320, 'rewardStep': 0.7734313077145303, 'errorList': [], 'lossList': [0.0, -1.1696275216341019, 0.0, 1.5504104382544757, 0.0, 0.0, 0.0], 'rewardMean': 0.7188631516258649, 'totalEpisodes': 114, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.9601426488068
'totalSteps': 25600, 'rewardStep': 0.9819468861974004, 'errorList': [0.07505057833477304, 0.08063561716054211, 0.09153942110271177, 0.07376285061191176, 0.08158083763970372, 0.13191858471735596, 0.09990221692513672, 0.09596348741642036, 0.13030259463771152, 0.08946428799490705, 0.08048035259945267, 0.10188873263313492, 0.10419258649308988, 0.08620787695821072, 0.08524661347158556, 0.10089490019832302, 0.10311085030191203, 0.08192094718842795, 0.10737514170683593, 0.1099153680195325, 0.0982127639074508, 0.0856578121343217, 0.0884443813203805, 0.13420439439149975, 0.08615195891568353, 0.09037263224673828, 0.11927138409396366, 0.10118770989692434, 0.10236065028916118, 0.08368353478551831, 0.10972112642816284, 0.07269307099792467, 0.07814063949704114, 0.12797901502581674, 0.10136454069905605, 0.08835434660155968, 0.16198777967112038, 0.08773331982288511, 0.09492505915159292, 0.1391082248397313, 0.08607319472100276, 0.08500524945785246, 0.08749740854290823, 0.10198070972997415, 0.0911049264817075, 0.10706342254161418, 0.085840631807107, 0.09338052255483001, 0.10152124995545571, 0.08050087879404487], 'lossList': [0.0, -1.1212372994422912, 0.0, 1.5515828068181872, 0.0, 0.0, 0.0], 'rewardMean': 0.7519630380744313, 'totalEpisodes': 114, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.6378602216832, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=100.45
