#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 112
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.7679971592949902, 'errorList': [], 'lossList': [0.0, -1.4488781630992889, 0.0, 28.004402940273284, 0.0, 0.0, 0.0], 'rewardMean': 0.6030138757504321, 'totalEpisodes': 13, 'stepsPerEpisode': 716, 'rewardPerEpisode': 479.52531561194826
'totalSteps': 3840, 'rewardStep': 0.9259427578946928, 'errorList': [], 'lossList': [0.0, -1.460046471953392, 0.0, 31.21583839893341, 0.0, 0.0, 0.0], 'rewardMean': 0.7106568364651856, 'totalEpisodes': 19, 'stepsPerEpisode': 77, 'rewardPerEpisode': 58.85299330842239
'totalSteps': 5120, 'rewardStep': 0.4737492650270577, 'errorList': [], 'lossList': [0.0, -1.4316585940122604, 0.0, 29.849744110107423, 0.0, 0.0, 0.0], 'rewardMean': 0.6514299436056536, 'totalEpisodes': 26, 'stepsPerEpisode': 67, 'rewardPerEpisode': 41.544418040849905
'totalSteps': 6400, 'rewardStep': 0.9558438899324674, 'errorList': [], 'lossList': [0.0, -1.415112103819847, 0.0, 53.292527871131895, 0.0, 0.0, 0.0], 'rewardMean': 0.7123127328710164, 'totalEpisodes': 32, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.9401218294707396
'totalSteps': 7680, 'rewardStep': 0.7978862368246128, 'errorList': [], 'lossList': [0.0, -1.4016071051359176, 0.0, 79.82998735427856, 0.0, 0.0, 0.0], 'rewardMean': 0.7265749835299492, 'totalEpisodes': 42, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.41787969380381
'totalSteps': 8960, 'rewardStep': 0.7230845061658664, 'errorList': [], 'lossList': [0.0, -1.3881351816654206, 0.0, 92.0080212020874, 0.0, 0.0, 0.0], 'rewardMean': 0.7260763439065088, 'totalEpisodes': 51, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.250477975511327
'totalSteps': 10240, 'rewardStep': 0.49658681460635506, 'errorList': [], 'lossList': [0.0, -1.3691970884799958, 0.0, 155.80035987854004, 0.0, 0.0, 0.0], 'rewardMean': 0.6973901527439895, 'totalEpisodes': 79, 'stepsPerEpisode': 2, 'rewardPerEpisode': 0.9947402309032627
'totalSteps': 11520, 'rewardStep': 0.4886252318168379, 'errorList': [], 'lossList': [0.0, -1.3581354528665543, 0.0, 92.33905670166016, 0.0, 0.0, 0.0], 'rewardMean': 0.6741940504187505, 'totalEpisodes': 108, 'stepsPerEpisode': 68, 'rewardPerEpisode': 44.17769945824521
'totalSteps': 12800, 'rewardStep': 0.8922465706674988, 'errorList': [], 'lossList': [0.0, -1.3573064690828323, 0.0, 45.92485978126526, 0.0, 0.0, 0.0], 'rewardMean': 0.6959993024436253, 'totalEpisodes': 123, 'stepsPerEpisode': 143, 'rewardPerEpisode': 121.84181553175243
'totalSteps': 14080, 'rewardStep': 0.6862474198736216, 'errorList': [], 'lossList': [0.0, -1.3507938641309738, 0.0, 13.141187572479248, 0.0, 0.0, 0.0], 'rewardMean': 0.7208209852104002, 'totalEpisodes': 128, 'stepsPerEpisode': 189, 'rewardPerEpisode': 144.42621545233
'totalSteps': 15360, 'rewardStep': 0.5421599266072326, 'errorList': [], 'lossList': [0.0, -1.3401237350702286, 0.0, 39.73201575756073, 0.0, 0.0, 0.0], 'rewardMean': 0.6982372619416244, 'totalEpisodes': 134, 'stepsPerEpisode': 313, 'rewardPerEpisode': 217.48792547679054
'totalSteps': 16640, 'rewardStep': 0.4831430774422308, 'errorList': [], 'lossList': [0.0, -1.3223228365182877, 0.0, 7.829403700232506, 0.0, 0.0, 0.0], 'rewardMean': 0.653957293896378, 'totalEpisodes': 135, 'stepsPerEpisode': 1037, 'rewardPerEpisode': 710.3401772279914
'totalSteps': 17920, 'rewardStep': 0.5198100159337181, 'errorList': [], 'lossList': [0.0, -1.2791799569129945, 0.0, 7.046701073646545, 0.0, 0.0, 0.0], 'rewardMean': 0.6585633689870442, 'totalEpisodes': 140, 'stepsPerEpisode': 217, 'rewardPerEpisode': 174.8199031108854
'totalSteps': 19200, 'rewardStep': 0.8107070091086833, 'errorList': [], 'lossList': [0.0, -1.2404588234424592, 0.0, 7.176474592685699, 0.0, 0.0, 0.0], 'rewardMean': 0.6440496809046657, 'totalEpisodes': 142, 'stepsPerEpisode': 761, 'rewardPerEpisode': 580.1059332379891
'totalSteps': 20480, 'rewardStep': 0.6736238073478784, 'errorList': [], 'lossList': [0.0, -1.20161341547966, 0.0, 5.7689209091663365, 0.0, 0.0, 0.0], 'rewardMean': 0.6316234379569923, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 918.0706939271704
'totalSteps': 21760, 'rewardStep': 0.883375434292939, 'errorList': [], 'lossList': [0.0, -1.1498529207706452, 0.0, 2.7497595125436782, 0.0, 0.0, 0.0], 'rewardMean': 0.6476525307696995, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1047.8502923503681
'totalSteps': 23040, 'rewardStep': 0.9861914127497984, 'errorList': [0.23883387634489683, 0.24713115706510322, 0.2254904690319574, 0.2458538645099939, 0.26735806668213985, 0.25372543915983914, 0.2434722620431781, 0.3748193633703095, 0.24311703077369537, 0.23409525902596062, 0.24106889790924133, 0.2554495256719054, 0.22549628778201733, 0.25334557534308505, 0.23323988163284226, 0.2412077965143198, 0.22882736675536075, 0.22919504107273486, 0.23798836245634358, 0.23203265910629156, 0.3199657296442907, 0.24451989073380936, 0.24634902090066743, 0.24549296772350707, 0.28351196656724026, 0.25268989671333725, 0.2441235272342496, 0.2454626985184752, 0.226687669010776, 0.326057300300295, 0.24277384997131282, 0.319317759319067, 0.3466441585706386, 0.26554081817905756, 0.25500015318331803, 0.31604783225073374, 0.24002262160553425, 0.2377598592615647, 0.3079897182510335, 0.22857870658180532, 0.24180474340793845, 0.23332238372461456, 0.2508492700786497, 0.2515883292365284, 0.2527662597563149, 0.24926824934681716, 0.24144286010617239, 0.25864480595179995, 0.2408885736470414, 0.24128127179155962], 'lossList': [0.0, -1.0998059463500978, 0.0, 2.3500789165496827, 0.0, 0.0, 0.0], 'rewardMean': 0.6966129905840439, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.4984310408715, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.830072170950001, 'errorList': [], 'lossList': [0.0, -1.0927940928936004, 0.0, 1.1651040148735046, 0.0, 0.0, 0.0], 'rewardMean': 0.7307576844973602, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.5991902241838
'totalSteps': 25600, 'rewardStep': 0.8145855517802004, 'errorList': [], 'lossList': [0.0, -1.0920388680696487, 0.0, 1.0326723187789322, 0.0, 0.0, 0.0], 'rewardMean': 0.7229915826086304, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.8351765493906
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=79.5
