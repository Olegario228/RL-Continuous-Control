#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 113
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.5639500100620956, 'errorList': [], 'lossList': [0.0, -1.4194619470834733, 0.0, 24.037607184648515, 0.0, 0.0, 0.0], 'rewardMean': 0.6646379215045758, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 29.513492278018283
'totalSteps': 3840, 'rewardStep': 0.8985545199830751, 'errorList': [], 'lossList': [0.0, -1.406170080304146, 0.0, 31.464187099933625, 0.0, 0.0, 0.0], 'rewardMean': 0.7426101209974089, 'totalEpisodes': 19, 'stepsPerEpisode': 494, 'rewardPerEpisode': 351.95132067973844
'totalSteps': 5120, 'rewardStep': 0.8304878584436022, 'errorList': [], 'lossList': [0.0, -1.3896444648504258, 0.0, 43.61024086475372, 0.0, 0.0, 0.0], 'rewardMean': 0.7645795553589572, 'totalEpisodes': 23, 'stepsPerEpisode': 76, 'rewardPerEpisode': 70.76484530546955
'totalSteps': 6400, 'rewardStep': 0.48057310344259635, 'errorList': [], 'lossList': [0.0, -1.3848826563358307, 0.0, 40.41246608734131, 0.0, 0.0, 0.0], 'rewardMean': 0.707778264975685, 'totalEpisodes': 27, 'stepsPerEpisode': 25, 'rewardPerEpisode': 16.949829951989955
'totalSteps': 7680, 'rewardStep': 0.9684794435422343, 'errorList': [], 'lossList': [0.0, -1.3831685996055603, 0.0, 46.18801910877228, 0.0, 0.0, 0.0], 'rewardMean': 0.7512284614034432, 'totalEpisodes': 32, 'stepsPerEpisode': 58, 'rewardPerEpisode': 50.05237333827305
'totalSteps': 8960, 'rewardStep': 0.9963241691110867, 'errorList': [], 'lossList': [0.0, -1.3751857137680055, 0.0, 57.72965094566345, 0.0, 0.0, 0.0], 'rewardMean': 0.7862421339331066, 'totalEpisodes': 39, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.13906685283103
'totalSteps': 10240, 'rewardStep': 0.7474914496577171, 'errorList': [], 'lossList': [0.0, -1.3666399711370467, 0.0, 178.92204174041748, 0.0, 0.0, 0.0], 'rewardMean': 0.7813982983986829, 'totalEpisodes': 65, 'stepsPerEpisode': 77, 'rewardPerEpisode': 70.33171821137805
'totalSteps': 11520, 'rewardStep': 0.6994934395053267, 'errorList': [], 'lossList': [0.0, -1.3625907021760941, 0.0, 93.85412551879882, 0.0, 0.0, 0.0], 'rewardMean': 0.7722977585216433, 'totalEpisodes': 88, 'stepsPerEpisode': 84, 'rewardPerEpisode': 62.174348699472645
'totalSteps': 12800, 'rewardStep': 0.3367246729420015, 'errorList': [], 'lossList': [0.0, -1.3558350545167923, 0.0, 23.564381256103516, 0.0, 0.0, 0.0], 'rewardMean': 0.7287404499636791, 'totalEpisodes': 101, 'stepsPerEpisode': 189, 'rewardPerEpisode': 147.7499653052483
'totalSteps': 14080, 'rewardStep': 0.7920939710134969, 'errorList': [], 'lossList': [0.0, -1.3387121671438218, 0.0, 19.628208603858948, 0.0, 0.0, 0.0], 'rewardMean': 0.7314172637703231, 'totalEpisodes': 108, 'stepsPerEpisode': 116, 'rewardPerEpisode': 82.79994650288022
'totalSteps': 15360, 'rewardStep': 0.47757515772963327, 'errorList': [], 'lossList': [0.0, -1.3353229570388794, 0.0, 10.792609750032424, 0.0, 0.0, 0.0], 'rewardMean': 0.722779778537077, 'totalEpisodes': 113, 'stepsPerEpisode': 4, 'rewardPerEpisode': 1.8897677267529387
'totalSteps': 16640, 'rewardStep': 0.8388230740814299, 'errorList': [], 'lossList': [0.0, -1.335555973649025, 0.0, 8.661235800981522, 0.0, 0.0, 0.0], 'rewardMean': 0.7168066339469125, 'totalEpisodes': 117, 'stepsPerEpisode': 106, 'rewardPerEpisode': 90.67998707206563
'totalSteps': 17920, 'rewardStep': 0.6388230284098668, 'errorList': [], 'lossList': [0.0, -1.3288337683677673, 0.0, 6.460951216220856, 0.0, 0.0, 0.0], 'rewardMean': 0.6976401509435389, 'totalEpisodes': 120, 'stepsPerEpisode': 380, 'rewardPerEpisode': 311.749253222341
'totalSteps': 19200, 'rewardStep': 0.9186036145080151, 'errorList': [], 'lossList': [0.0, -1.3310416847467423, 0.0, 4.909148952960968, 0.0, 0.0, 0.0], 'rewardMean': 0.7414432020500807, 'totalEpisodes': 123, 'stepsPerEpisode': 292, 'rewardPerEpisode': 228.1033444251813
'totalSteps': 20480, 'rewardStep': 0.7048452451390057, 'errorList': [], 'lossList': [0.0, -1.3145316070318223, 0.0, 4.289363233447075, 0.0, 0.0, 0.0], 'rewardMean': 0.7150797822097579, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.8127581079082
'totalSteps': 21760, 'rewardStep': 0.8571277854920532, 'errorList': [], 'lossList': [0.0, -1.2904254668951034, 0.0, 3.590973760485649, 0.0, 0.0, 0.0], 'rewardMean': 0.7011601438478545, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 985.5727356561281
'totalSteps': 23040, 'rewardStep': 0.7593617164215979, 'errorList': [], 'lossList': [0.0, -1.25274758040905, 0.0, 1.3211721021682024, 0.0, 0.0, 0.0], 'rewardMean': 0.7023471705242426, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.525578007321
'totalSteps': 24320, 'rewardStep': 0.848609395676467, 'errorList': [], 'lossList': [0.0, -1.1977162539958954, 0.0, 1.264954975619912, 0.0, 0.0, 0.0], 'rewardMean': 0.7172587661413568, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.5071259221888
'totalSteps': 25600, 'rewardStep': 0.9546020823146368, 'errorList': [0.013517356668164573, 0.03216896708418549, 0.015588687782955355, 0.012478427324853441, 0.006996014713776615, 0.023470032751668438, 0.03516864002287002, 0.015122910378174533, 0.03498155187090119, 0.00237287919373052, 0.014252397265968341, 0.017939209444933185, 0.026013651474901738, 0.03334938719726805, 0.01673916762022268, 0.008441013280992988, 0.015607546927793455, 0.01884699087759925, 0.03971751969463144, 0.040719784877662035, 0.05073108704744975, 0.022692722579156565, 0.01660593173579017, 0.010515509305938031, 0.03668136552496913, 0.021670103138235742, 0.019821327853295934, 0.02194841157869139, 0.022241484738185546, 0.029028254469732073, 0.0359441046784274, 0.03678352006314156, 0.021755636350906868, 0.023543794820139737, 0.011293090551618904, 0.012869304785254663, 0.03980714574424923, 0.02893665761315009, 0.027126591639049714, 0.019082572170514803, 0.017279752995940558, 0.014281984123454699, 0.01493659165321786, 0.021280993285556857, 0.031616096707630624, 0.014985829702280459, 0.009752527181418313, 0.015275661470155187, 0.015020513203113638, 0.029130446018624214], 'lossList': [0.0, -1.1597702538967132, 0.0, 1.10996948171407, 0.0, 0.0, 0.0], 'rewardMean': 0.7790465070786203, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.871305147292, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.56
