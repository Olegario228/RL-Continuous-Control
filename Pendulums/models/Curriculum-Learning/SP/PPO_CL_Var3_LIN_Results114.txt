#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 114
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.948546003512413, 'errorList': [], 'lossList': [0.0, -1.416555632352829, 0.0, 28.938043496608735, 0.0, 0.0, 0.0], 'rewardMean': 0.8102307048278782, 'totalEpisodes': 14, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.449968617512
'totalSteps': 3840, 'rewardStep': 0.5990974133965994, 'errorList': [], 'lossList': [0.0, -1.4211472630500794, 0.0, 24.476557970046997, 0.0, 0.0, 0.0], 'rewardMean': 0.7398529410174519, 'totalEpisodes': 17, 'stepsPerEpisode': 185, 'rewardPerEpisode': 133.86640577392012
'totalSteps': 5120, 'rewardStep': 0.5715622286773228, 'errorList': [], 'lossList': [0.0, -1.4175656294822694, 0.0, 26.738029721975327, 0.0, 0.0, 0.0], 'rewardMean': 0.6977802629324197, 'totalEpisodes': 19, 'stepsPerEpisode': 280, 'rewardPerEpisode': 202.42020737346948
'totalSteps': 6400, 'rewardStep': 0.7708944952300659, 'errorList': [], 'lossList': [0.0, -1.3948996937274933, 0.0, 45.745790271759034, 0.0, 0.0, 0.0], 'rewardMean': 0.712403109391949, 'totalEpisodes': 23, 'stepsPerEpisode': 722, 'rewardPerEpisode': 498.265039788816
'totalSteps': 7680, 'rewardStep': 0.7273880459903905, 'errorList': [], 'lossList': [0.0, -1.3780433267354966, 0.0, 69.09529044151306, 0.0, 0.0, 0.0], 'rewardMean': 0.7149005988250225, 'totalEpisodes': 30, 'stepsPerEpisode': 35, 'rewardPerEpisode': 27.007368486350373
'totalSteps': 8960, 'rewardStep': 0.7378879239114796, 'errorList': [], 'lossList': [0.0, -1.3647793656587601, 0.0, 133.41286706924438, 0.0, 0.0, 0.0], 'rewardMean': 0.7181845024088022, 'totalEpisodes': 45, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.86148885545288
'totalSteps': 10240, 'rewardStep': 0.718716915145535, 'errorList': [], 'lossList': [0.0, -1.358917189836502, 0.0, 153.76859672546388, 0.0, 0.0, 0.0], 'rewardMean': 0.7182510540008937, 'totalEpisodes': 81, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.681307111847726
'totalSteps': 11520, 'rewardStep': 0.44998789686003204, 'errorList': [], 'lossList': [0.0, -1.3589812856912613, 0.0, 95.32466674804688, 0.0, 0.0, 0.0], 'rewardMean': 0.688444036540798, 'totalEpisodes': 103, 'stepsPerEpisode': 83, 'rewardPerEpisode': 58.92837780488713
'totalSteps': 12800, 'rewardStep': 0.4016817009911003, 'errorList': [], 'lossList': [0.0, -1.3505386716127397, 0.0, 64.53876481056213, 0.0, 0.0, 0.0], 'rewardMean': 0.6597678029858283, 'totalEpisodes': 116, 'stepsPerEpisode': 63, 'rewardPerEpisode': 37.79182666299798
'totalSteps': 14080, 'rewardStep': 0.8398813660845865, 'errorList': [], 'lossList': [0.0, -1.334571236371994, 0.0, 40.73414662361145, 0.0, 0.0, 0.0], 'rewardMean': 0.6765643989799526, 'totalEpisodes': 124, 'stepsPerEpisode': 49, 'rewardPerEpisode': 33.57710481295421
'totalSteps': 15360, 'rewardStep': 0.9423156138950314, 'errorList': [17.556295900364695, 71.77243816052426, 2.7868500340767572, 7.134391256858342, 15.203676364536495, 6.224599656690024, 14.492702273136599, 11.91587733448628, 31.2116063725405, 5.715924876383995, 26.585846991836974, 7.7802653436832685, 5.801745867201665, 58.73683418523337, 48.78443572220771, 52.050923865797394, 35.54629850472568, 45.628826626649435, 32.28518126238145, 2.3902228137218877, 3.646176855656927, 21.577037270320705, 15.357830796822428, 20.400990879704295, 26.727892717707, 6.440630093552272, 34.23588998585443, 3.105139811390411, 18.7383095495061, 53.005680697922095, 20.169633330526093, 73.38055790320743, 10.101260608230112, 15.411719389413829, 3.635405101612497, 42.00918105207749, 11.836286869189866, 41.121296237518656, 42.78672431310265, 5.118994238430233, 10.579326888773316, 32.792951335680854, 10.421304018233167, 17.297391073918963, 76.25580228745484, 40.041286958992934, 44.80039226441148, 4.109967438509904, 27.314168357344457, 43.677794559854505], 'lossList': [0.0, -1.3163202768564224, 0.0, 48.5247162103653, 0.0, 0.0, 0.0], 'rewardMean': 0.6759413600182145, 'totalEpisodes': 133, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.95387676136079, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7703635303027447, 'errorList': [], 'lossList': [0.0, -1.3064684361219405, 0.0, 16.461637856960298, 0.0, 0.0, 0.0], 'rewardMean': 0.6930679717088288, 'totalEpisodes': 140, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7703635303027447
'totalSteps': 17920, 'rewardStep': 0.716442058683014, 'errorList': [], 'lossList': [0.0, -1.3024990630149842, 0.0, 12.587065335512161, 0.0, 0.0, 0.0], 'rewardMean': 0.7075559547093979, 'totalEpisodes': 145, 'stepsPerEpisode': 34, 'rewardPerEpisode': 23.386732792441027
'totalSteps': 19200, 'rewardStep': 0.8995268648593338, 'errorList': [], 'lossList': [0.0, -1.2828283059597014, 0.0, 7.9394962072372435, 0.0, 0.0, 0.0], 'rewardMean': 0.7204191916723246, 'totalEpisodes': 150, 'stepsPerEpisode': 123, 'rewardPerEpisode': 113.90093475669674
'totalSteps': 20480, 'rewardStep': 0.28290894879152184, 'errorList': [], 'lossList': [0.0, -1.2668521213531494, 0.0, 9.614020929336547, 0.0, 0.0, 0.0], 'rewardMean': 0.6759712819524378, 'totalEpisodes': 154, 'stepsPerEpisode': 313, 'rewardPerEpisode': 232.8122334666536
'totalSteps': 21760, 'rewardStep': 0.6058374475524183, 'errorList': [], 'lossList': [0.0, -1.2470438915491104, 0.0, 6.564390890002251, 0.0, 0.0, 0.0], 'rewardMean': 0.6627662343165318, 'totalEpisodes': 159, 'stepsPerEpisode': 132, 'rewardPerEpisode': 110.29891929706979
'totalSteps': 23040, 'rewardStep': 0.7946253050286343, 'errorList': [], 'lossList': [0.0, -1.2326525807380677, 0.0, 3.498454788327217, 0.0, 0.0, 0.0], 'rewardMean': 0.6703570733048417, 'totalEpisodes': 161, 'stepsPerEpisode': 555, 'rewardPerEpisode': 474.06290654878165
'totalSteps': 24320, 'rewardStep': 0.837974327274146, 'errorList': [], 'lossList': [0.0, -1.251649352312088, 0.0, 3.6066465854644774, 0.0, 0.0, 0.0], 'rewardMean': 0.7091557163462531, 'totalEpisodes': 165, 'stepsPerEpisode': 73, 'rewardPerEpisode': 65.84940713370541
'totalSteps': 25600, 'rewardStep': 0.45405700199518295, 'errorList': [], 'lossList': [0.0, -1.2517866718769073, 0.0, 3.8429126769304274, 0.0, 0.0, 0.0], 'rewardMean': 0.7143932464466614, 'totalEpisodes': 166, 'stepsPerEpisode': 672, 'rewardPerEpisode': 466.7042424866062
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.29
