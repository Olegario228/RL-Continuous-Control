#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 115
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.935447641059388, 'errorList': [], 'lossList': [0.0, -1.4406198197603226, 0.0, 28.536350152492524, 0.0, 0.0, 0.0], 'rewardMean': 0.9155146161337775, 'totalEpisodes': 8, 'stepsPerEpisode': 532, 'rewardPerEpisode': 372.4881249228318
'totalSteps': 3840, 'rewardStep': 0.7028717185942279, 'errorList': [], 'lossList': [0.0, -1.4359855443239211, 0.0, 32.85164448261261, 0.0, 0.0, 0.0], 'rewardMean': 0.844633650287261, 'totalEpisodes': 13, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.84997303270066
'totalSteps': 5120, 'rewardStep': 0.7158608013540811, 'errorList': [], 'lossList': [0.0, -1.4326461845636367, 0.0, 30.73803386449814, 0.0, 0.0, 0.0], 'rewardMean': 0.812440438053966, 'totalEpisodes': 15, 'stepsPerEpisode': 271, 'rewardPerEpisode': 213.93548100960388
'totalSteps': 6400, 'rewardStep': 0.9028851442097546, 'errorList': [], 'lossList': [0.0, -1.4196176946163177, 0.0, 17.201318042278288, 0.0, 0.0, 0.0], 'rewardMean': 0.8305293792851238, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 970.8563687437282
'totalSteps': 7680, 'rewardStep': 0.8569371938846466, 'errorList': [], 'lossList': [0.0, -1.3976581227779388, 0.0, 36.879248806238174, 0.0, 0.0, 0.0], 'rewardMean': 0.8349306817183776, 'totalEpisodes': 17, 'stepsPerEpisode': 298, 'rewardPerEpisode': 218.10324411930895
'totalSteps': 8960, 'rewardStep': 0.5768457590722507, 'errorList': [], 'lossList': [0.0, -1.373760735988617, 0.0, 65.15814268112183, 0.0, 0.0, 0.0], 'rewardMean': 0.7980614070546451, 'totalEpisodes': 22, 'stepsPerEpisode': 269, 'rewardPerEpisode': 198.2723048838802
'totalSteps': 10240, 'rewardStep': 0.9089317176648907, 'errorList': [], 'lossList': [0.0, -1.3720614153146744, 0.0, 330.7572679901123, 0.0, 0.0, 0.0], 'rewardMean': 0.8119201958809259, 'totalEpisodes': 53, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.38776396414782
'totalSteps': 11520, 'rewardStep': 0.6310654748667539, 'errorList': [], 'lossList': [0.0, -1.366705697774887, 0.0, 207.0652379989624, 0.0, 0.0, 0.0], 'rewardMean': 0.7918252268793512, 'totalEpisodes': 86, 'stepsPerEpisode': 43, 'rewardPerEpisode': 37.36362305026101
'totalSteps': 12800, 'rewardStep': 0.5859719108361809, 'errorList': [], 'lossList': [0.0, -1.359552600979805, 0.0, 108.76644897460938, 0.0, 0.0, 0.0], 'rewardMean': 0.7712398952750341, 'totalEpisodes': 110, 'stepsPerEpisode': 44, 'rewardPerEpisode': 33.596256982140396
'totalSteps': 14080, 'rewardStep': 0.7261810101607031, 'errorList': [], 'lossList': [0.0, -1.3641661602258681, 0.0, 58.54940469741821, 0.0, 0.0, 0.0], 'rewardMean': 0.7542998371702877, 'totalEpisodes': 135, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7261810101607031
'totalSteps': 15360, 'rewardStep': 0.8953006989009428, 'errorList': [], 'lossList': [0.0, -1.3620905303955078, 0.0, 30.22067618370056, 0.0, 0.0, 0.0], 'rewardMean': 0.7502851429544433, 'totalEpisodes': 152, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8953006989009428
'totalSteps': 16640, 'rewardStep': 0.4166265655318324, 'errorList': [], 'lossList': [0.0, -1.3637623238563537, 0.0, 25.95583321094513, 0.0, 0.0, 0.0], 'rewardMean': 0.7216606276482038, 'totalEpisodes': 159, 'stepsPerEpisode': 392, 'rewardPerEpisode': 324.48956157065675
'totalSteps': 17920, 'rewardStep': 0.7472079154245169, 'errorList': [], 'lossList': [0.0, -1.3730660980939866, 0.0, 19.2462451338768, 0.0, 0.0, 0.0], 'rewardMean': 0.7247953390552472, 'totalEpisodes': 164, 'stepsPerEpisode': 101, 'rewardPerEpisode': 66.79118375495476
'totalSteps': 19200, 'rewardStep': 0.8145076510705228, 'errorList': [], 'lossList': [0.0, -1.3713101196289061, 0.0, 14.689659111499786, 0.0, 0.0, 0.0], 'rewardMean': 0.7159575897413241, 'totalEpisodes': 170, 'stepsPerEpisode': 54, 'rewardPerEpisode': 44.99827767397451
'totalSteps': 20480, 'rewardStep': 0.912837669624652, 'errorList': [], 'lossList': [0.0, -1.37602284014225, 0.0, 11.61436461687088, 0.0, 0.0, 0.0], 'rewardMean': 0.7215476373153247, 'totalEpisodes': 174, 'stepsPerEpisode': 137, 'rewardPerEpisode': 113.39279059404807
'totalSteps': 21760, 'rewardStep': 0.7372894505205724, 'errorList': [], 'lossList': [0.0, -1.3716492384672165, 0.0, 7.924524849653244, 0.0, 0.0, 0.0], 'rewardMean': 0.7375920064601569, 'totalEpisodes': 176, 'stepsPerEpisode': 359, 'rewardPerEpisode': 284.7402669604609
'totalSteps': 23040, 'rewardStep': 0.41049107458207523, 'errorList': [], 'lossList': [0.0, -1.3536657285690308, 0.0, 9.070943779945374, 0.0, 0.0, 0.0], 'rewardMean': 0.6877479421518753, 'totalEpisodes': 177, 'stepsPerEpisode': 988, 'rewardPerEpisode': 710.6385278642655
'totalSteps': 24320, 'rewardStep': 0.9832207028071032, 'errorList': [0.6325468506411958, 1.0463694500052148, 0.5829276811290877, 0.8009921647767364, 0.6884746776479552, 0.6667357964107162, 0.6574278987239794, 0.8739067307796349, 0.7390903507579737, 0.7560676720534133, 0.8320497748046448, 0.6781510402404413, 0.7827890338630605, 0.7406399250554618, 0.9567168513278688, 0.68437424696962, 0.7977271512593226, 0.9643156617040028, 0.7922365644435435, 0.6961549813478489, 0.7548540690569889, 1.277078873322046, 0.7087019965650586, 0.6369976066921883, 0.6721449856796288, 0.6416499943501864, 0.7952762181838271, 1.0105113624977253, 1.0750324706534438, 0.6846580391203365, 0.7139718851546453, 0.9936829974960121, 0.6566818884620929, 0.99413464385524, 0.7496158650200582, 0.6187366719369395, 0.6746548088435251, 0.9324000819410193, 0.8024758286157547, 0.6181956321956648, 0.7640202200746361, 0.8300733926350556, 0.7061965543426204, 0.7412599155810212, 0.7068729390883086, 0.7721489557517457, 0.7163128301444681, 0.6412619729979705, 1.2587040041878652, 0.6722400196080828], 'lossList': [0.0, -1.3219116431474687, 0.0, 7.6520895314216615, 0.0, 0.0, 0.0], 'rewardMean': 0.7229634649459102, 'totalEpisodes': 178, 'stepsPerEpisode': 83, 'rewardPerEpisode': 74.480562400963, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.6931597523815851, 'errorList': [], 'lossList': [0.0, -1.313243589401245, 0.0, 5.294658573865891, 0.0, 0.0, 0.0], 'rewardMean': 0.7336822491004507, 'totalEpisodes': 180, 'stepsPerEpisode': 239, 'rewardPerEpisode': 197.56031805808144
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.92
