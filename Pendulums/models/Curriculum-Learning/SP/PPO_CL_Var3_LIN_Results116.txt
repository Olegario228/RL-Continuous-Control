#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 116
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7223450575647381, 'errorList': [], 'lossList': [0.0, -1.439827200770378, 0.0, 34.576351432800294, 0.0, 0.0, 0.0], 'rewardMean': 0.6404963880205465, 'totalEpisodes': 12, 'stepsPerEpisode': 77, 'rewardPerEpisode': 64.44799284495753
'totalSteps': 3840, 'rewardStep': 0.8611558072507501, 'errorList': [], 'lossList': [0.0, -1.4454904770851136, 0.0, 27.64357543826103, 0.0, 0.0, 0.0], 'rewardMean': 0.7140495277639477, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 963.0302696029449
'totalSteps': 5120, 'rewardStep': 0.6770713491626539, 'errorList': [], 'lossList': [0.0, -1.4277153140306473, 0.0, 22.56341418683529, 0.0, 0.0, 0.0], 'rewardMean': 0.7048049831136243, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 969.0859409909783
'totalSteps': 6400, 'rewardStep': 0.740717948438379, 'errorList': [], 'lossList': [0.0, -1.4245449900627136, 0.0, 28.830415542125703, 0.0, 0.0, 0.0], 'rewardMean': 0.7119875761785752, 'totalEpisodes': 13, 'stepsPerEpisode': 303, 'rewardPerEpisode': 228.97419217079423
'totalSteps': 7680, 'rewardStep': 0.8102500319830371, 'errorList': [], 'lossList': [0.0, -1.4031074112653732, 0.0, 16.41807113468647, 0.0, 0.0, 0.0], 'rewardMean': 0.7283646521459856, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.486011345566
'totalSteps': 8960, 'rewardStep': 0.7231767384692934, 'errorList': [], 'lossList': [0.0, -1.3717872655391694, 0.0, 81.25457085609436, 0.0, 0.0, 0.0], 'rewardMean': 0.7276235216207437, 'totalEpisodes': 18, 'stepsPerEpisode': 112, 'rewardPerEpisode': 89.92760505241633
'totalSteps': 10240, 'rewardStep': 0.4675673644887056, 'errorList': [], 'lossList': [0.0, -1.367299056649208, 0.0, 310.23971405029295, 0.0, 0.0, 0.0], 'rewardMean': 0.695116501979239, 'totalEpisodes': 46, 'stepsPerEpisode': 68, 'rewardPerEpisode': 52.42392290365199
'totalSteps': 11520, 'rewardStep': 0.5611593165301811, 'errorList': [], 'lossList': [0.0, -1.3653957623243331, 0.0, 144.42526636123657, 0.0, 0.0, 0.0], 'rewardMean': 0.680232370262677, 'totalEpisodes': 70, 'stepsPerEpisode': 91, 'rewardPerEpisode': 75.78790108870623
'totalSteps': 12800, 'rewardStep': 0.6676892278100345, 'errorList': [], 'lossList': [0.0, -1.3618545454740525, 0.0, 108.03643770217896, 0.0, 0.0, 0.0], 'rewardMean': 0.6789780560174127, 'totalEpisodes': 101, 'stepsPerEpisode': 68, 'rewardPerEpisode': 52.47424768349183
'totalSteps': 14080, 'rewardStep': 0.9852158749654487, 'errorList': [2.9716323730924783, 3.9824630802782686, 4.110530971385427, 3.3736192282418256, 3.424213198266681, 4.300022013411372, 3.1767814738344105, 3.8041325514982867, 3.5558326952745083, 4.15656371425538, 3.576727975388593, 3.2771733558224847, 4.2769980520730195, 3.9143240203327565, 2.6951239152696895, 4.101830434237465, 3.2985584007568476, 3.390647549007897, 1.9978367945124136, 4.2916016069057275, 4.034918029898673, 2.2727225008130096, 4.310962957058533, 4.094672529024116, 4.310534849006341, 4.08676059902853, 2.7781350259579045, 2.0431766423879294, 2.826503597090174, 4.265055225485506, 3.8850742421923026, 3.994479631323565, 1.739521158292267, 1.5922623710457136, 4.3290840518624485, 0.7254199167833485, 3.213251139099446, 4.216641882689599, 3.553087722833877, 3.5901424445515033, 4.082119619826158, 3.366174061150903, 4.216918808764865, 4.363303987307119, 3.193297628742665, 4.325971638899068, 3.491750181982312, 3.3308976702233393, 4.252337010927115, 2.1924452312752694], 'lossList': [0.0, -1.3570033234357834, 0.0, 38.4066939163208, 0.0, 0.0, 0.0], 'rewardMean': 0.7216348716663221, 'totalEpisodes': 114, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.455346404478483, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7622164371163248, 'errorList': [], 'lossList': [0.0, -1.3527546858787536, 0.0, 33.483398303985595, 0.0, 0.0, 0.0], 'rewardMean': 0.7256220096214808, 'totalEpisodes': 124, 'stepsPerEpisode': 51, 'rewardPerEpisode': 42.84196407859456
'totalSteps': 16640, 'rewardStep': 0.16866231632066997, 'errorList': [], 'lossList': [0.0, -1.333202110528946, 0.0, 26.43963625907898, 0.0, 0.0, 0.0], 'rewardMean': 0.6563726605284728, 'totalEpisodes': 130, 'stepsPerEpisode': 255, 'rewardPerEpisode': 139.7382141786899
'totalSteps': 17920, 'rewardStep': 0.46458295194694443, 'errorList': [], 'lossList': [0.0, -1.3112361520528792, 0.0, 20.882658569812776, 0.0, 0.0, 0.0], 'rewardMean': 0.6351238208069019, 'totalEpisodes': 134, 'stepsPerEpisode': 299, 'rewardPerEpisode': 201.92427379987535
'totalSteps': 19200, 'rewardStep': 0.67348304846583, 'errorList': [], 'lossList': [0.0, -1.2932492285966872, 0.0, 8.346645812988282, 0.0, 0.0, 0.0], 'rewardMean': 0.628400330809647, 'totalEpisodes': 137, 'stepsPerEpisode': 535, 'rewardPerEpisode': 438.1317920960377
'totalSteps': 20480, 'rewardStep': 0.44150320495546524, 'errorList': [], 'lossList': [0.0, -1.275934037566185, 0.0, 5.93071347117424, 0.0, 0.0, 0.0], 'rewardMean': 0.5915256481068898, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 884.1002369078848
'totalSteps': 21760, 'rewardStep': 0.7588002648192613, 'errorList': [], 'lossList': [0.0, -1.2590111047029495, 0.0, 3.1043560048937797, 0.0, 0.0, 0.0], 'rewardMean': 0.5950880007418866, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.4442928441565
'totalSteps': 23040, 'rewardStep': 0.9053045079044334, 'errorList': [], 'lossList': [0.0, -1.234225886464119, 0.0, 3.145342119410634, 0.0, 0.0, 0.0], 'rewardMean': 0.6388617150834593, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.6420488910844
'totalSteps': 24320, 'rewardStep': 0.781853642762883, 'errorList': [], 'lossList': [0.0, -1.1962477606534958, 0.0, 2.721970986649394, 0.0, 0.0, 0.0], 'rewardMean': 0.6609311477067294, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.9979320113355
'totalSteps': 25600, 'rewardStep': 0.9907631908105058, 'errorList': [0.10162793334681294, 0.1026288358897927, 0.10438490835519847, 0.10138225731467611, 0.10263567775530369, 0.1595768224043727, 0.10940025525877439, 0.10520055232063692, 0.14814980734423414, 0.10418267191721736, 0.10253373067077762, 0.11084381657414019, 0.11176080433938776, 0.10358414143601023, 0.1031227167621978, 0.11026891752829891, 0.12831242679237748, 0.1028317668461731, 0.10695543921988207, 0.14212625698060488, 0.10560797003779812, 0.10339790919350068, 0.10372176224021558, 0.15849343482588013, 0.10347911263487815, 0.10429438450568243, 0.11550151710771224, 0.10607945093815827, 0.11474881054765404, 0.10312869171782635, 0.12012689119181963, 0.10126394303458483, 0.10204191494774648, 0.15865551211367396, 0.1059577980325882, 0.10389763882333955, 0.1890555659708013, 0.10594682661148473, 0.1050124572049824, 0.16767757401772546, 0.10354923145209627, 0.1033338002236619, 0.10354638555869859, 0.10615281770386482, 0.10441016984866822, 0.135164231242914, 0.10340986862831526, 0.1047634857642792, 0.10604830241712057, 0.10259029718230962], 'lossList': [0.0, -1.1539647388458252, 0.0, 2.1779337695240972, 0.0, 0.0, 0.0], 'rewardMean': 0.6932385440067766, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1202.970074501376, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=102.8
