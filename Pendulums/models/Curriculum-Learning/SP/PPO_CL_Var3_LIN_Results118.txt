#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 118
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.552263153208042, 'errorList': [], 'lossList': [0.0, -1.4216731637716293, 0.0, 27.539268375635146, 0.0, 0.0, 0.0], 'rewardMean': 0.7052376308817908, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 30.50349211061752
'totalSteps': 3840, 'rewardStep': 0.9357067542537549, 'errorList': [], 'lossList': [0.0, -1.4181635421514511, 0.0, 26.246895228624343, 0.0, 0.0, 0.0], 'rewardMean': 0.7820606720057789, 'totalEpisodes': 18, 'stepsPerEpisode': 491, 'rewardPerEpisode': 371.52942256824974
'totalSteps': 5120, 'rewardStep': 0.7506139809167653, 'errorList': [], 'lossList': [0.0, -1.4144538581371306, 0.0, 27.98465123772621, 0.0, 0.0, 0.0], 'rewardMean': 0.7741989992335254, 'totalEpisodes': 19, 'stepsPerEpisode': 181, 'rewardPerEpisode': 151.77392094713923
'totalSteps': 6400, 'rewardStep': 0.5800602565179761, 'errorList': [], 'lossList': [0.0, -1.4119778645038605, 0.0, 34.616821303367615, 0.0, 0.0, 0.0], 'rewardMean': 0.7353712506904155, 'totalEpisodes': 21, 'stepsPerEpisode': 757, 'rewardPerEpisode': 568.7472039931122
'totalSteps': 7680, 'rewardStep': 0.7864021198106798, 'errorList': [], 'lossList': [0.0, -1.3996139997243882, 0.0, 21.220800542235374, 0.0, 0.0, 0.0], 'rewardMean': 0.7438763955437929, 'totalEpisodes': 22, 'stepsPerEpisode': 1274, 'rewardPerEpisode': 1029.7440496533068
'totalSteps': 8960, 'rewardStep': 0.7023090176318859, 'errorList': [], 'lossList': [0.0, -1.3848223185539246, 0.0, 40.378663992881776, 0.0, 0.0, 0.0], 'rewardMean': 0.7379381986992347, 'totalEpisodes': 24, 'stepsPerEpisode': 198, 'rewardPerEpisode': 143.9718084289782
'totalSteps': 10240, 'rewardStep': 0.8710483371707998, 'errorList': [], 'lossList': [0.0, -1.377330384850502, 0.0, 287.90283058166506, 0.0, 0.0, 0.0], 'rewardMean': 0.7545769660081804, 'totalEpisodes': 52, 'stepsPerEpisode': 65, 'rewardPerEpisode': 58.19053033130655
'totalSteps': 11520, 'rewardStep': 0.9110579684330048, 'errorList': [], 'lossList': [0.0, -1.375423172712326, 0.0, 94.77209508895874, 0.0, 0.0, 0.0], 'rewardMean': 0.771963744055383, 'totalEpisodes': 78, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.456246892769455
'totalSteps': 12800, 'rewardStep': 0.7779093404587659, 'errorList': [], 'lossList': [0.0, -1.3756200283765794, 0.0, 57.92893749237061, 0.0, 0.0, 0.0], 'rewardMean': 0.7725583036957213, 'totalEpisodes': 110, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.356393007573972
'totalSteps': 14080, 'rewardStep': 0.5390308947628408, 'errorList': [], 'lossList': [0.0, -1.374272437095642, 0.0, 31.70045831680298, 0.0, 0.0, 0.0], 'rewardMean': 0.7406401823164516, 'totalEpisodes': 126, 'stepsPerEpisode': 114, 'rewardPerEpisode': 85.97101517547232
'totalSteps': 15360, 'rewardStep': 0.7190216096349399, 'errorList': [], 'lossList': [0.0, -1.371902750134468, 0.0, 31.090784645080568, 0.0, 0.0, 0.0], 'rewardMean': 0.7573160279591413, 'totalEpisodes': 137, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.6946785074935153
'totalSteps': 16640, 'rewardStep': 0.835352190726711, 'errorList': [], 'lossList': [0.0, -1.3576706945896149, 0.0, 14.093866324424743, 0.0, 0.0, 0.0], 'rewardMean': 0.747280571606437, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 869.6863084990517
'totalSteps': 17920, 'rewardStep': 0.8909133254273454, 'errorList': [], 'lossList': [0.0, -1.3054809272289276, 0.0, 7.766471343040466, 0.0, 0.0, 0.0], 'rewardMean': 0.7613105060574948, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.5731619353892
'totalSteps': 19200, 'rewardStep': 0.9596738231825765, 'errorList': [0.12349682254832964, 0.07428059447908915, 0.07901553836549387, 0.08022474220673506, 0.11101694920659376, 0.1461563805485732, 0.07419866101818876, 0.07948108920520879, 0.08319001048400207, 0.11859342217310617, 0.07482623057872237, 0.0732866582421121, 0.07632471308561539, 0.07748279588988813, 0.07093283451453987, 0.11311017762402968, 0.09659217263251714, 0.11228218999539834, 0.07411612684655884, 0.128525342166856, 0.07996105218944356, 0.08065163879840273, 0.07343227525671597, 0.15292348555607596, 0.0757557249494943, 0.1723963861374752, 0.07381721611144793, 0.07785072878916904, 0.1263656101745864, 0.09048421588285148, 0.08529575752547615, 0.07809182521553669, 0.07549307352760434, 0.19207268220141555, 0.07361165914329795, 0.0730358738994926, 0.07454790158514482, 0.11122798183621971, 0.106876079734637, 0.20165853399810427, 0.1619552867633545, 0.0754295639063553, 0.09945699802231209, 0.08489386098217463, 0.07499960955745373, 0.07704216257575208, 0.0763940526081606, 0.08020154708848107, 0.07477685275456548, 0.07525724552195383], 'lossList': [0.0, -1.2562444305419922, 0.0, 4.614281233847141, 0.0, 0.0, 0.0], 'rewardMean': 0.7992718627239549, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 982.7208643378657, 'successfulTests': 49
'totalSteps': 20480, 'rewardStep': 0.9424922066022343, 'errorList': [0.2508128005637281, 0.19124378045837562, 0.18931290691541047, 0.24889453061874453, 0.1896465041879342, 0.2011306752930542, 0.4309426628538282, 0.1925027876226286, 0.37163698509169857, 0.19451449175856417, 0.19887149693575645, 0.21065194789654687, 0.19078249185264337, 0.19028862900583374, 0.18707750696657724, 0.1952104491667374, 0.19316196657450369, 0.24841257946340045, 0.18869980435517608, 0.3201722493463142, 0.1921841715015038, 0.19306583265118193, 0.27332482685025383, 0.1915502948226473, 0.18774994792967245, 0.1863593233440231, 0.19271551689588973, 0.2030117310666928, 0.19043292390026298, 0.1901204896658867, 0.2012306837778466, 0.19540775348569298, 0.20228031983439906, 0.1908635497997843, 0.20761527759809856, 0.18235800504108426, 0.33101413733021007, 0.2576546105251502, 0.1826435568019264, 0.3410934977332128, 0.25454930640107803, 0.24382022416627164, 0.19265232965376847, 0.21771650581959492, 0.2140348351445153, 0.1916792932777153, 0.1871793759958765, 0.1849038871206488, 0.1880893071888529, 0.3108474836334731], 'lossList': [0.0, -1.2218321442604065, 0.0, 6.402398214489222, 0.0, 0.0, 0.0], 'rewardMean': 0.8148808714031104, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.2654324421962, 'successfulTests': 29
'totalSteps': 21760, 'rewardStep': 0.9291229852768608, 'errorList': [], 'lossList': [0.0, -1.1811708825826646, 0.0, 3.1096917272359135, 0.0, 0.0, 0.0], 'rewardMean': 0.8375622681676079, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.6904005948377
'totalSteps': 23040, 'rewardStep': 0.7810600790065542, 'errorList': [], 'lossList': [0.0, -1.171046669483185, 0.0, 1.9840373791195451, 0.0, 0.0, 0.0], 'rewardMean': 0.8285634423511834, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1107.0224349028138
'totalSteps': 24320, 'rewardStep': 0.7206247860590623, 'errorList': [], 'lossList': [0.0, -1.1403891277313232, 0.0, 1.4401471967622639, 0.0, 0.0, 0.0], 'rewardMean': 0.8095201241137892, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1087.565297351389
'totalSteps': 25600, 'rewardStep': 0.9575630380408844, 'errorList': [0.21556016933587777, 0.1937940048286971, 0.21620050165973057, 0.20724586259628858, 0.261917058311508, 0.19165921714696202, 0.578352624125985, 0.2046410889706338, 0.15762215532936236, 0.27080498190981395, 0.3511632456454495, 0.22884980940900776, 0.18638989666925057, 0.314029304288026, 0.47365987033266993, 0.22049012630159306, 0.20080624932730987, 0.2269388872594812, 0.3986899790164558, 0.34120382568518626, 0.19394901926716304, 0.4358377347414254, 0.2201385626495411, 0.2867872853593633, 0.12279156777531383, 0.19106581396930197, 0.258255643079003, 0.46332600073079033, 0.16206774159773896, 0.5017163112417546, 0.48200482708661024, 0.2268084089408856, 0.20172684468952753, 0.23785338259061525, 0.18532693613438203, 0.14773734631280755, 0.3173321737226464, 0.2117962054453621, 0.29875601902358684, 0.40700416254087235, 0.18637418766582542, 0.16543375329042395, 0.23806488947288174, 0.39011610576334455, 0.21303945121322432, 0.22550320962851653, 0.45497278213960535, 0.20489245281299867, 0.33196419792587406, 0.4404578528051468], 'lossList': [0.0, -1.1132943600416183, 0.0, 0.8665594779700041, 0.0, 0.0, 0.0], 'rewardMean': 0.827485493872001, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.714940177493, 'successfulTests': 12
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=19200, timeSpent=126.21
