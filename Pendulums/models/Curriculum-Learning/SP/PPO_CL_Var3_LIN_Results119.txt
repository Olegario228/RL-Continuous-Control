#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 119
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8594461213779896, 'errorList': [], 'lossList': [0.0, -1.4279578673839568, 0.0, 32.48005138874054, 0.0, 0.0, 0.0], 'rewardMean': 0.7913985226286453, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 257.6247376472424
'totalSteps': 3840, 'rewardStep': 0.7684971681131898, 'errorList': [], 'lossList': [0.0, -1.4435062462091446, 0.0, 30.70208931207657, 0.0, 0.0, 0.0], 'rewardMean': 0.7837647377901602, 'totalEpisodes': 16, 'stepsPerEpisode': 166, 'rewardPerEpisode': 123.83282027578208
'totalSteps': 5120, 'rewardStep': 0.5507008584365459, 'errorList': [], 'lossList': [0.0, -1.4499580776691436, 0.0, 24.305779658555984, 0.0, 0.0, 0.0], 'rewardMean': 0.7254987679517566, 'totalEpisodes': 17, 'stepsPerEpisode': 281, 'rewardPerEpisode': 207.9008779411302
'totalSteps': 6400, 'rewardStep': 0.9764204646895916, 'errorList': [], 'lossList': [0.0, -1.437523232102394, 0.0, 44.47123051166535, 0.0, 0.0, 0.0], 'rewardMean': 0.7756831072993237, 'totalEpisodes': 21, 'stepsPerEpisode': 34, 'rewardPerEpisode': 29.399335480434864
'totalSteps': 7680, 'rewardStep': 0.8102095723247528, 'errorList': [], 'lossList': [0.0, -1.4306019425392151, 0.0, 61.31034520626068, 0.0, 0.0, 0.0], 'rewardMean': 0.7814375181368951, 'totalEpisodes': 26, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.86490579192619
'totalSteps': 8960, 'rewardStep': 0.6534160337676967, 'errorList': [], 'lossList': [0.0, -1.4084477531909942, 0.0, 128.21482542037964, 0.0, 0.0, 0.0], 'rewardMean': 0.7631487346555811, 'totalEpisodes': 37, 'stepsPerEpisode': 66, 'rewardPerEpisode': 47.3466029030768
'totalSteps': 10240, 'rewardStep': 0.49782741535158265, 'errorList': [], 'lossList': [0.0, -1.3939954847097398, 0.0, 204.10785667419432, 0.0, 0.0, 0.0], 'rewardMean': 0.7299835697425813, 'totalEpisodes': 64, 'stepsPerEpisode': 135, 'rewardPerEpisode': 97.25085128323904
'totalSteps': 11520, 'rewardStep': 0.9388614078586655, 'errorList': [40.45956485408996, 53.604078120835474, 50.44496109859922, 56.986457304622945, 15.7504792763693, 27.078472225415492, 33.37751315106638, 8.126948823521136, 18.49202499667217, 54.24014907464972, 20.16520999366666, 38.71738245900726, 13.687410732795025, 7.590068063640317, 43.17823620790028, 29.951473369264026, 5.889748266259789, 22.423950024855003, 51.42182715450206, 3.476543766662373, 9.782268515453907, 25.616838944702202, 12.736324643429018, 9.38044216618779, 5.444319314273511, 4.60763143560198, 38.81342129860771, 16.531827312596977, 6.815922198816601, 13.240640437555879, 66.79140606999731, 21.137367842423878, 7.1981879757362695, 21.63100678590421, 26.260829514710917, 39.49864097733464, 4.803057715661563, 41.76675544306244, 19.906127185692988, 44.5317449256961, 29.592586471952394, 2.904372898171509, 49.9493840494902, 16.708776879286724, 3.9522068103826125, 5.862900620032726, 38.14445344018843, 55.642836277505076, 5.687432313429879, 61.38730806022184], 'lossList': [0.0, -1.3883169400691986, 0.0, 96.13523138046264, 0.0, 0.0, 0.0], 'rewardMean': 0.7531922184221461, 'totalEpisodes': 84, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.43133523661693, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.6717894973836891, 'errorList': [], 'lossList': [0.0, -1.3787714266777038, 0.0, 58.107198009490965, 0.0, 0.0, 0.0], 'rewardMean': 0.7450519463183005, 'totalEpisodes': 97, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.648217839988758
'totalSteps': 14080, 'rewardStep': 0.5183198174369256, 'errorList': [], 'lossList': [0.0, -1.354312275648117, 0.0, 31.058297090530395, 0.0, 0.0, 0.0], 'rewardMean': 0.7245488356740629, 'totalEpisodes': 105, 'stepsPerEpisode': 175, 'rewardPerEpisode': 132.0337651633386
'totalSteps': 15360, 'rewardStep': 0.9919739507120007, 'errorList': [52.82663558023601, 118.30847474649737, 39.370977328872435, 3.037192189646246, 24.86520526278047, 18.149060080738398, 18.568959406207348, 45.73079903421642, 61.87897384463925, 1.4895884330619218, 66.59743797001654, 8.690266601476141, 34.27783233022742, 98.69022006149261, 80.79123235992296, 83.88893330078035, 90.28930707930132, 78.12797656926004, 73.31299712517898, 9.858925490902916, 2.614243077266398, 52.095956505413994, 32.75941778463615, 67.20973371873856, 35.25783771833895, 22.88200915979733, 56.85374794683014, 38.89398959376875, 36.080552351409246, 86.81116698276301, 31.366110041629405, 112.08605866567855, 3.0335942218429865, 58.83434658935855, 11.95151781846538, 61.468748884948035, 61.34622616711832, 60.84836280687507, 73.38592210487431, 35.20895081642916, 19.76893556893165, 50.67575700558193, 13.581232098884742, 64.3569192709306, 118.329656630537, 65.20291760477866, 99.5993015574149, 21.219497488221226, 58.556291966772534, 79.47083258177932], 'lossList': [0.0, -1.335136650800705, 0.0, 59.838780989646914, 0.0, 0.0, 0.0], 'rewardMean': 0.737801618607464, 'totalEpisodes': 114, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.078884438662627, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.3361980367990589, 'errorList': [], 'lossList': [0.0, -1.321932585835457, 0.0, 21.522408502101896, 0.0, 0.0, 0.0], 'rewardMean': 0.694571705476051, 'totalEpisodes': 118, 'stepsPerEpisode': 164, 'rewardPerEpisode': 123.22732399264848
'totalSteps': 17920, 'rewardStep': 0.6760346208493762, 'errorList': [], 'lossList': [0.0, -1.3021988081932068, 0.0, 15.170937337875365, 0.0, 0.0, 0.0], 'rewardMean': 0.7071050817173339, 'totalEpisodes': 126, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.143227046620801
'totalSteps': 19200, 'rewardStep': 0.9229601659369667, 'errorList': [], 'lossList': [0.0, -1.3196050053834916, 0.0, 5.936842616796493, 0.0, 0.0, 0.0], 'rewardMean': 0.7017590518420715, 'totalEpisodes': 132, 'stepsPerEpisode': 61, 'rewardPerEpisode': 57.49675736891437
'totalSteps': 20480, 'rewardStep': 0.6848068606734221, 'errorList': [], 'lossList': [0.0, -1.331530242562294, 0.0, 4.828992799520493, 0.0, 0.0, 0.0], 'rewardMean': 0.6892187806769384, 'totalEpisodes': 135, 'stepsPerEpisode': 196, 'rewardPerEpisode': 146.45173325796122
'totalSteps': 21760, 'rewardStep': 0.5246557032235923, 'errorList': [], 'lossList': [0.0, -1.3213970816135407, 0.0, 3.468367991447449, 0.0, 0.0, 0.0], 'rewardMean': 0.676342747622528, 'totalEpisodes': 136, 'stepsPerEpisode': 1039, 'rewardPerEpisode': 649.4755486086095
'totalSteps': 23040, 'rewardStep': 0.847766824894618, 'errorList': [], 'lossList': [0.0, -1.3205718916654587, 0.0, 5.265476586222649, 0.0, 0.0, 0.0], 'rewardMean': 0.7113366885768315, 'totalEpisodes': 138, 'stepsPerEpisode': 107, 'rewardPerEpisode': 95.59271533426822
'totalSteps': 24320, 'rewardStep': 0.6632375836511339, 'errorList': [], 'lossList': [0.0, -1.3234201776981354, 0.0, 6.179010591506958, 0.0, 0.0, 0.0], 'rewardMean': 0.6837743061560784, 'totalEpisodes': 140, 'stepsPerEpisode': 207, 'rewardPerEpisode': 172.1077231693611
'totalSteps': 25600, 'rewardStep': 0.9455017328704908, 'errorList': [0.16233946769398327, 0.16282095220761764, 0.16277016614312426, 0.16254125533982564, 0.1627463712200071, 0.16261308125003215, 0.1628337949822979, 0.16280750486625506, 0.16245763828285797, 0.1625903870189906, 0.16259947328792962, 0.16239491328545108, 0.18163717602137883, 0.16283690763897235, 0.16260726844527923, 0.16205903556890555, 0.16261164532267733, 0.2094613347639987, 0.16869473786193337, 0.16228383030237095, 0.1855255876299309, 0.16747404235283442, 0.1622291477328648, 0.16237922730946452, 0.16284591112221608, 0.16265214792750346, 0.16234421197805357, 0.16201202924122757, 0.16271637969201094, 0.16267791577937327, 0.16207669590522983, 0.1624252972837413, 0.1619040752940525, 0.24002203076152911, 0.16214956298277927, 0.18689025532653233, 0.16299316426177163, 0.2099212051193659, 0.16248373026027627, 0.16267384582632394, 0.16213948195029865, 0.16234361060654862, 0.16664066547880388, 0.16261284215883695, 0.18147455177234054, 0.23471358275981022, 0.16259658010633266, 0.16416983351910175, 0.23738951979427012, 0.16182024173882448], 'lossList': [0.0, -1.3049015063047409, 0.0, 2.071692296266556, 0.0, 0.0, 0.0], 'rewardMean': 0.7111455297047585, 'totalEpisodes': 140, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.078726541596, 'successfulTests': 45
#maxSuccessfulTests=45, maxSuccessfulTestsAtStep=25600, timeSpent=123.59
