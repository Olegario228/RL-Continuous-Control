#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 12
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.7001478192627917, 'errorList': [], 'lossList': [0.0, -1.4359890884160995, 0.0, 26.634794855117796, 0.0, 0.0, 0.0], 'rewardMean': 0.5690892057343329, 'totalEpisodes': 16, 'stepsPerEpisode': 134, 'rewardPerEpisode': 87.94991297509601
'totalSteps': 3840, 'rewardStep': 0.711490721871107, 'errorList': [], 'lossList': [0.0, -1.4303549098968507, 0.0, 38.39176087379455, 0.0, 0.0, 0.0], 'rewardMean': 0.6165563777799242, 'totalEpisodes': 28, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.657701794263772
'totalSteps': 5120, 'rewardStep': 0.6155520783968618, 'errorList': [], 'lossList': [0.0, -1.411610522866249, 0.0, 64.37370458602905, 0.0, 0.0, 0.0], 'rewardMean': 0.6163053029341585, 'totalEpisodes': 51, 'stepsPerEpisode': 48, 'rewardPerEpisode': 36.05647132823087
'totalSteps': 6400, 'rewardStep': 0.667234691169978, 'errorList': [], 'lossList': [0.0, -1.4037669676542281, 0.0, 93.09915103912354, 0.0, 0.0, 0.0], 'rewardMean': 0.6264911805813224, 'totalEpisodes': 96, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.304174960314084
'totalSteps': 7680, 'rewardStep': 0.898828687574352, 'errorList': [], 'lossList': [0.0, -1.396419112086296, 0.0, 65.14349533081055, 0.0, 0.0, 0.0], 'rewardMean': 0.6718807650801607, 'totalEpisodes': 124, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.891103299039324
'totalSteps': 8960, 'rewardStep': 0.6813076315407363, 'errorList': [], 'lossList': [0.0, -1.3765703415870667, 0.0, 34.88961744308472, 0.0, 0.0, 0.0], 'rewardMean': 0.6732274602888143, 'totalEpisodes': 134, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.790201855643641
'totalSteps': 10240, 'rewardStep': 0.6630313340694025, 'errorList': [], 'lossList': [0.0, -1.354875280857086, 0.0, 24.678186657428743, 0.0, 0.0, 0.0], 'rewardMean': 0.6719529445113879, 'totalEpisodes': 138, 'stepsPerEpisode': 301, 'rewardPerEpisode': 224.1764806048506
'totalSteps': 11520, 'rewardStep': 0.5752665091813899, 'errorList': [], 'lossList': [0.0, -1.3389526909589768, 0.0, 23.817626440525054, 0.0, 0.0, 0.0], 'rewardMean': 0.6612100072524991, 'totalEpisodes': 143, 'stepsPerEpisode': 78, 'rewardPerEpisode': 49.49786939457541
'totalSteps': 12800, 'rewardStep': 0.516565701891382, 'errorList': [], 'lossList': [0.0, -1.332196488380432, 0.0, 35.094185485839844, 0.0, 0.0, 0.0], 'rewardMean': 0.6467455767163874, 'totalEpisodes': 147, 'stepsPerEpisode': 348, 'rewardPerEpisode': 209.12883117127564
'totalSteps': 14080, 'rewardStep': 0.8132204175003557, 'errorList': [], 'lossList': [0.0, -1.3307750350236893, 0.0, 19.809826679229737, 0.0, 0.0, 0.0], 'rewardMean': 0.6842645592458357, 'totalEpisodes': 151, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.49827751029919
'totalSteps': 15360, 'rewardStep': 0.7154970337381714, 'errorList': [], 'lossList': [0.0, -1.3157284092903136, 0.0, 10.834883762598038, 0.0, 0.0, 0.0], 'rewardMean': 0.6857994806933736, 'totalEpisodes': 153, 'stepsPerEpisode': 307, 'rewardPerEpisode': 217.6453632381716
'totalSteps': 16640, 'rewardStep': 0.8644786983977899, 'errorList': [], 'lossList': [0.0, -1.3097977328300476, 0.0, 9.834669958353043, 0.0, 0.0, 0.0], 'rewardMean': 0.701098278346042, 'totalEpisodes': 154, 'stepsPerEpisode': 77, 'rewardPerEpisode': 65.6968975665201
'totalSteps': 17920, 'rewardStep': 0.6858923171652297, 'errorList': [], 'lossList': [0.0, -1.2991682040691375, 0.0, 2.4222439885139466, 0.0, 0.0, 0.0], 'rewardMean': 0.7081323022228787, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 930.6096208106773
'totalSteps': 19200, 'rewardStep': 0.8251981457118107, 'errorList': [], 'lossList': [0.0, -1.2638151651620866, 0.0, 2.460446819141507, 0.0, 0.0, 0.0], 'rewardMean': 0.7239286476770621, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1018.7787046762642
'totalSteps': 20480, 'rewardStep': 0.9419559686567266, 'errorList': [0.09822994438668706, 0.07119510105973184, 0.15328728246550005, 0.10159952137395521, 0.1367587193546456, 0.11459700522869766, 0.13490860306117256, 0.15864478152578007, 0.06648455744403926, 0.09069490615969783, 0.09616217923898367, 0.08234414693510453, 0.1680451147207825, 0.09287264920805988, 0.17464402110092359, 0.05071860328006279, 0.11322628411194494, 0.052329331704066756, 0.12052002059793122, 0.07023610652814251, 0.07374116440664094, 0.10257562733442849, 0.19641706782176613, 0.07242531782404428, 0.04943732774088773, 0.08594755129704554, 0.11504180033675292, 0.08956291186770517, 0.08432019014366623, 0.1847671029713407, 0.13787607573875235, 0.08037851106159265, 0.08531309300083277, 0.07996519645350475, 0.07123389862855295, 0.10368912402086065, 0.09990636368089015, 0.08841968921844148, 0.13495599981794423, 0.08332799521890566, 0.12864679351107625, 0.14747215322946033, 0.10893765885096111, 0.13687276427165682, 0.056078359214202705, 0.12094842387185614, 0.04823666269759412, 0.11842535304084532, 0.06775395884553458, 0.057128342484754825], 'lossList': [0.0, -1.2257038342952729, 0.0, 3.4607291423529385, 0.0, 0.0, 0.0], 'rewardMean': 0.7282413757852995, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.9320951645661, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=58.25
