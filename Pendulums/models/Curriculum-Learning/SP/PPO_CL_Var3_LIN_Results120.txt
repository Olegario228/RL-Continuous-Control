#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 120
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.8882470851157331, 'errorList': [], 'lossList': [0.0, -1.4354868566989898, 0.0, 30.224735527038575, 0.0, 0.0, 0.0], 'rewardMean': 0.9015338578482806, 'totalEpisodes': 8, 'stepsPerEpisode': 530, 'rewardPerEpisode': 371.9577727912206
'totalSteps': 3840, 'rewardStep': 0.7892598654183623, 'errorList': [], 'lossList': [0.0, -1.4223370355367662, 0.0, 28.968710494041442, 0.0, 0.0, 0.0], 'rewardMean': 0.8641091937049744, 'totalEpisodes': 13, 'stepsPerEpisode': 180, 'rewardPerEpisode': 146.11631004558095
'totalSteps': 5120, 'rewardStep': 0.6708748873224512, 'errorList': [], 'lossList': [0.0, -1.4194040775299073, 0.0, 24.943767343759536, 0.0, 0.0, 0.0], 'rewardMean': 0.8158006171093436, 'totalEpisodes': 14, 'stepsPerEpisode': 473, 'rewardPerEpisode': 353.29579341188196
'totalSteps': 6400, 'rewardStep': 0.8695970768041164, 'errorList': [], 'lossList': [0.0, -1.4007383531332016, 0.0, 20.320167115926743, 0.0, 0.0, 0.0], 'rewardMean': 0.8265599090482981, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.78701517177
'totalSteps': 7680, 'rewardStep': 0.8390679729044885, 'errorList': [], 'lossList': [0.0, -1.3834919017553329, 0.0, 64.37022062778473, 0.0, 0.0, 0.0], 'rewardMean': 0.8286445863576631, 'totalEpisodes': 18, 'stepsPerEpisode': 298, 'rewardPerEpisode': 239.06213828618579
'totalSteps': 8960, 'rewardStep': 0.649595791725168, 'errorList': [], 'lossList': [0.0, -1.372841208577156, 0.0, 59.60184191226959, 0.0, 0.0, 0.0], 'rewardMean': 0.8030661871244495, 'totalEpisodes': 22, 'stepsPerEpisode': 79, 'rewardPerEpisode': 68.14957247225247
'totalSteps': 10240, 'rewardStep': 0.8532993282852412, 'errorList': [], 'lossList': [0.0, -1.3720018440485, 0.0, 405.0806300354004, 0.0, 0.0, 0.0], 'rewardMean': 0.8093453297695486, 'totalEpisodes': 65, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.127339873203189
'totalSteps': 11520, 'rewardStep': 0.8921529307101058, 'errorList': [], 'lossList': [0.0, -1.3692565280199052, 0.0, 194.2385850906372, 0.0, 0.0, 0.0], 'rewardMean': 0.8185461743184994, 'totalEpisodes': 101, 'stepsPerEpisode': 32, 'rewardPerEpisode': 29.98835522288613
'totalSteps': 12800, 'rewardStep': 0.7093939259131756, 'errorList': [], 'lossList': [0.0, -1.3644850516319276, 0.0, 106.10497215270996, 0.0, 0.0, 0.0], 'rewardMean': 0.807630949477967, 'totalEpisodes': 129, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.3940968127537938
'totalSteps': 14080, 'rewardStep': 0.9058110771826479, 'errorList': [], 'lossList': [0.0, -1.3571223521232605, 0.0, 81.6326766204834, 0.0, 0.0, 0.0], 'rewardMean': 0.806729994138149, 'totalEpisodes': 159, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.816453195896607
'totalSteps': 15360, 'rewardStep': 0.8316411899508054, 'errorList': [], 'lossList': [0.0, -1.3411738234758377, 0.0, 34.49794604301453, 0.0, 0.0, 0.0], 'rewardMean': 0.8010694046216562, 'totalEpisodes': 176, 'stepsPerEpisode': 73, 'rewardPerEpisode': 56.09664114169567
'totalSteps': 16640, 'rewardStep': 0.670531050349919, 'errorList': [], 'lossList': [0.0, -1.319915505051613, 0.0, 32.6151527929306, 0.0, 0.0, 0.0], 'rewardMean': 0.789196523114812, 'totalEpisodes': 190, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.0778231661963296
'totalSteps': 17920, 'rewardStep': 0.9678303863057702, 'errorList': [60.69490418277849, 10.916183932869657, 0.9183666345529718, 26.479292219616426, 69.03088194226905, 13.159783563651946, 3.891271228679304, 25.993229798063965, 64.14078236024287, 69.35107222862652, 51.30920725635344, 0.9765272325615669, 5.414267894515961, 74.70575008671346, 93.85621510725409, 64.55465683607967, 106.58229558651924, 76.53416485926543, 55.33305327435846, 2.549723216702498, 63.12313664738739, 59.470687253389386, 99.30642051486315, 97.05962361549128, 0.9107494908195617, 39.05289227803916, 60.93056504994486, 54.3553081356878, 52.693180320438486, 100.86366874509011, 31.406010979693505, 93.51240160093911, 8.465648372668403, 2.3323591870788905, 38.856143131934786, 7.732390854260649, 84.84112644581451, 92.49579895380556, 83.8735037660856, 41.71318809352392, 84.11485750785639, 130.66768837137562, 19.897765130939423, 68.19674918415075, 108.51988420647784, 1.0825661290192539, 13.686198002106359, 65.63378774188682, 11.582896287666138, 26.5783544286297], 'lossList': [0.0, -1.3168601953983308, 0.0, 19.229921135902405, 0.0, 0.0, 0.0], 'rewardMean': 0.8188920730131439, 'totalEpisodes': 198, 'stepsPerEpisode': 33, 'rewardPerEpisode': 29.298603512505924, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.6520145192525618, 'errorList': [], 'lossList': [0.0, -1.3201166027784348, 0.0, 15.01530821800232, 0.0, 0.0, 0.0], 'rewardMean': 0.7971338172579884, 'totalEpisodes': 207, 'stepsPerEpisode': 34, 'rewardPerEpisode': 22.594914211591657
'totalSteps': 20480, 'rewardStep': 0.8198992888171921, 'errorList': [], 'lossList': [0.0, -1.315821578502655, 0.0, 7.790398602485657, 0.0, 0.0, 0.0], 'rewardMean': 0.7952169488492588, 'totalEpisodes': 214, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.730346274265925
'totalSteps': 21760, 'rewardStep': 0.9103157675869785, 'errorList': [], 'lossList': [0.0, -1.3059587854146957, 0.0, 23.554478397369383, 0.0, 0.0, 0.0], 'rewardMean': 0.8212889464354397, 'totalEpisodes': 219, 'stepsPerEpisode': 60, 'rewardPerEpisode': 52.60355337892579
'totalSteps': 23040, 'rewardStep': 0.526051177092504, 'errorList': [], 'lossList': [0.0, -1.2954259717464447, 0.0, 5.9291522425413135, 0.0, 0.0, 0.0], 'rewardMean': 0.7885641313161661, 'totalEpisodes': 221, 'stepsPerEpisode': 602, 'rewardPerEpisode': 493.577405272235
'totalSteps': 24320, 'rewardStep': 0.8365865457940238, 'errorList': [], 'lossList': [0.0, -1.287872946858406, 0.0, 8.755686829090118, 0.0, 0.0, 0.0], 'rewardMean': 0.7830074928245578, 'totalEpisodes': 223, 'stepsPerEpisode': 221, 'rewardPerEpisode': 183.0067503626802
'totalSteps': 25600, 'rewardStep': 0.8232763255242834, 'errorList': [], 'lossList': [0.0, -1.287975127696991, 0.0, 6.235495561957359, 0.0, 0.0, 0.0], 'rewardMean': 0.7943957327856687, 'totalEpisodes': 224, 'stepsPerEpisode': 105, 'rewardPerEpisode': 89.19432987518051
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=82.72
