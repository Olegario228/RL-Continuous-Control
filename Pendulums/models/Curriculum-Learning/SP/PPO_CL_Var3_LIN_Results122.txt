#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 122
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8573024894097909, 'errorList': [], 'lossList': [0.0, -1.445143604874611, 0.0, 31.949788208007813, 0.0, 0.0, 0.0], 'rewardMean': 0.6802016686569013, 'totalEpisodes': 12, 'stepsPerEpisode': 899, 'rewardPerEpisode': 639.7784278208846
'totalSteps': 3840, 'rewardStep': 0.8920502898424226, 'errorList': [], 'lossList': [0.0, -1.4566914343833923, 0.0, 42.98317416191101, 0.0, 0.0, 0.0], 'rewardMean': 0.7508178757187417, 'totalEpisodes': 15, 'stepsPerEpisode': 489, 'rewardPerEpisode': 401.02555624163267
'totalSteps': 5120, 'rewardStep': 0.641339705847414, 'errorList': [], 'lossList': [0.0, -1.451490831375122, 0.0, 17.824568499326706, 0.0, 0.0, 0.0], 'rewardMean': 0.7234483332509098, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 891.424477441456
'totalSteps': 6400, 'rewardStep': 0.38041271349300104, 'errorList': [], 'lossList': [0.0, -1.4276629865169526, 0.0, 42.92016925215721, 0.0, 0.0, 0.0], 'rewardMean': 0.6548412092993281, 'totalEpisodes': 18, 'stepsPerEpisode': 187, 'rewardPerEpisode': 141.67999936732676
'totalSteps': 7680, 'rewardStep': 0.5543464489565706, 'errorList': [], 'lossList': [0.0, -1.4145539087057113, 0.0, 137.33588216781615, 0.0, 0.0, 0.0], 'rewardMean': 0.6380920825755352, 'totalEpisodes': 32, 'stepsPerEpisode': 57, 'rewardPerEpisode': 45.389583217257574
'totalSteps': 8960, 'rewardStep': 0.5358478931458392, 'errorList': [], 'lossList': [0.0, -1.3997396022081374, 0.0, 121.78726644515991, 0.0, 0.0, 0.0], 'rewardMean': 0.6234857697998644, 'totalEpisodes': 46, 'stepsPerEpisode': 33, 'rewardPerEpisode': 20.61393395647553
'totalSteps': 10240, 'rewardStep': 0.7767572128993669, 'errorList': [], 'lossList': [0.0, -1.3910959494113921, 0.0, 182.89838039398194, 0.0, 0.0, 0.0], 'rewardMean': 0.6426447001873021, 'totalEpisodes': 72, 'stepsPerEpisode': 206, 'rewardPerEpisode': 168.4169426037955
'totalSteps': 11520, 'rewardStep': 0.6803461443737512, 'errorList': [], 'lossList': [0.0, -1.3947224128246307, 0.0, 112.5040442276001, 0.0, 0.0, 0.0], 'rewardMean': 0.6468337495413521, 'totalEpisodes': 95, 'stepsPerEpisode': 74, 'rewardPerEpisode': 57.3301140756937
'totalSteps': 12800, 'rewardStep': 0.6747368284751889, 'errorList': [], 'lossList': [0.0, -1.3981679397821427, 0.0, 66.46411967277527, 0.0, 0.0, 0.0], 'rewardMean': 0.6496240574347357, 'totalEpisodes': 110, 'stepsPerEpisode': 88, 'rewardPerEpisode': 68.10689471967416
'totalSteps': 14080, 'rewardStep': 0.7993246681867273, 'errorList': [], 'lossList': [0.0, -1.3938142043352126, 0.0, 21.504503436088562, 0.0, 0.0, 0.0], 'rewardMean': 0.6792464394630073, 'totalEpisodes': 116, 'stepsPerEpisode': 220, 'rewardPerEpisode': 173.66045787660252
'totalSteps': 15360, 'rewardStep': 0.8338026661299304, 'errorList': [], 'lossList': [0.0, -1.3793143647909165, 0.0, 41.85826614379883, 0.0, 0.0, 0.0], 'rewardMean': 0.6768964571350212, 'totalEpisodes': 124, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.328027750734684
'totalSteps': 16640, 'rewardStep': 0.6908307725911739, 'errorList': [], 'lossList': [0.0, -1.3736269408464432, 0.0, 13.871464653015137, 0.0, 0.0, 0.0], 'rewardMean': 0.6567745054098963, 'totalEpisodes': 130, 'stepsPerEpisode': 100, 'rewardPerEpisode': 73.02412182125151
'totalSteps': 17920, 'rewardStep': 0.8029004757983551, 'errorList': [], 'lossList': [0.0, -1.3850170165300368, 0.0, 7.552284424304962, 0.0, 0.0, 0.0], 'rewardMean': 0.6729305824049904, 'totalEpisodes': 136, 'stepsPerEpisode': 60, 'rewardPerEpisode': 54.14958968184056
'totalSteps': 19200, 'rewardStep': 0.9445594129133534, 'errorList': [0.9863181649580396, 0.7795092835547704, 0.691753293228562, 1.1654961422538528, 0.20660360394912147, 0.5870143412155604, 0.5690631856594855, 1.1209482191316302, 1.2732446501295038, 0.7543106453562077, 1.0399659745147234, 1.594643829950872, 1.5341539186599793, 0.176786440274613, 0.8820905078854915, 1.5862084946676351, 1.5614691795124576, 1.1157109824692677, 1.90108734931686, 1.323650088224085, 1.7244289254356726, 1.2285881473772775, 0.5600549147289439, 0.91955493212667, 1.1430279801067509, 0.8402401658487141, 0.9152631162484683, 0.5561465649173787, 0.2033806494748301, 0.15705298470703258, 0.8126737428451911, 0.4043496008416163, 0.3355605154500093, 0.19350215025253056, 0.5018232137464671, 0.9559688072795719, 0.6769115883409881, 0.5024690235921582, 0.2584683933164685, 1.3508089692699137, 0.8296079318416136, 0.1835010545409777, 0.6573071868680904, 1.1473222149231006, 0.8436179746176811, 0.6618554998869285, 0.17121071212139377, 2.1974611651435643, 0.1980155356363932, 1.7861543967560225], 'lossList': [0.0, -1.3832448649406432, 0.0, 7.1228822404146195, 0.0, 0.0, 0.0], 'rewardMean': 0.7293452523470256, 'totalEpisodes': 138, 'stepsPerEpisode': 782, 'rewardPerEpisode': 668.7568193140842, 'successfulTests': 6
'totalSteps': 20480, 'rewardStep': 0.7171322209801303, 'errorList': [], 'lossList': [0.0, -1.3581244665384293, 0.0, 6.342581155896187, 0.0, 0.0, 0.0], 'rewardMean': 0.7456238295493817, 'totalEpisodes': 139, 'stepsPerEpisode': 443, 'rewardPerEpisode': 379.29199681317897
'totalSteps': 21760, 'rewardStep': 0.7652929082171054, 'errorList': [], 'lossList': [0.0, -1.328926591873169, 0.0, 4.980586505532265, 0.0, 0.0, 0.0], 'rewardMean': 0.7685683310565083, 'totalEpisodes': 140, 'stepsPerEpisode': 118, 'rewardPerEpisode': 101.84361350229086
'totalSteps': 23040, 'rewardStep': 0.6801805324568054, 'errorList': [], 'lossList': [0.0, -1.3126225340366364, 0.0, 3.2382142579555513, 0.0, 0.0, 0.0], 'rewardMean': 0.7589106630122522, 'totalEpisodes': 141, 'stepsPerEpisode': 621, 'rewardPerEpisode': 481.51638427918874
'totalSteps': 24320, 'rewardStep': 0.9232746667702606, 'errorList': [], 'lossList': [0.0, -1.2955474519729615, 0.0, 1.2581362317502498, 0.0, 0.0, 0.0], 'rewardMean': 0.783203515251903, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.4673617627014
'totalSteps': 25600, 'rewardStep': 0.8081774063821704, 'errorList': [], 'lossList': [0.0, -1.2576151299476623, 0.0, 1.2965066434442998, 0.0, 0.0, 0.0], 'rewardMean': 0.7965475730426013, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.4999345762928
#maxSuccessfulTests=6, maxSuccessfulTestsAtStep=19200, timeSpent=82.54
