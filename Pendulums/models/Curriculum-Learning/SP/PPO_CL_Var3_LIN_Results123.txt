#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 123
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5366355028965095, 'errorList': [], 'lossList': [0.0, -1.4225577765703201, 0.0, 29.111414771080018, 0.0, 0.0, 0.0], 'rewardMean': 0.717782959361625, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 31.446353622142656
'totalSteps': 3840, 'rewardStep': 0.9387580625935005, 'errorList': [], 'lossList': [0.0, -1.42192385494709, 0.0, 30.18512040257454, 0.0, 0.0, 0.0], 'rewardMean': 0.7914413271055835, 'totalEpisodes': 18, 'stepsPerEpisode': 489, 'rewardPerEpisode': 384.6428544482555
'totalSteps': 5120, 'rewardStep': 0.7888761226813535, 'errorList': [], 'lossList': [0.0, -1.418860519528389, 0.0, 24.177751969993114, 0.0, 0.0, 0.0], 'rewardMean': 0.790800025999526, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1029.2167617099049
'totalSteps': 6400, 'rewardStep': 0.5726057540570405, 'errorList': [], 'lossList': [0.0, -1.3951609408855439, 0.0, 27.566372525691985, 0.0, 0.0, 0.0], 'rewardMean': 0.7471611716110289, 'totalEpisodes': 19, 'stepsPerEpisode': 757, 'rewardPerEpisode': 596.4784187022426
'totalSteps': 7680, 'rewardStep': 0.8365234016437728, 'errorList': [], 'lossList': [0.0, -1.3655627536773682, 0.0, 72.65877431869507, 0.0, 0.0, 0.0], 'rewardMean': 0.7620548766164862, 'totalEpisodes': 24, 'stepsPerEpisode': 108, 'rewardPerEpisode': 82.54942340306985
'totalSteps': 8960, 'rewardStep': 0.8112570200166997, 'errorList': [], 'lossList': [0.0, -1.3584739005565643, 0.0, 19.166425228714942, 0.0, 0.0, 0.0], 'rewardMean': 0.7690837542450881, 'totalEpisodes': 25, 'stepsPerEpisode': 790, 'rewardPerEpisode': 598.4633884512181
'totalSteps': 10240, 'rewardStep': 0.8379457062611002, 'errorList': [], 'lossList': [0.0, -1.3485980540513993, 0.0, 403.0136530303955, 0.0, 0.0, 0.0], 'rewardMean': 0.7776914982470897, 'totalEpisodes': 73, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.322834593664346
'totalSteps': 11520, 'rewardStep': 0.7034695648654088, 'errorList': [], 'lossList': [0.0, -1.344400708079338, 0.0, 135.95080684661866, 0.0, 0.0, 0.0], 'rewardMean': 0.7694446167602362, 'totalEpisodes': 110, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.795178010538166
'totalSteps': 12800, 'rewardStep': 0.8078861310034193, 'errorList': [], 'lossList': [0.0, -1.3370282071828843, 0.0, 113.48702533721924, 0.0, 0.0, 0.0], 'rewardMean': 0.7732887681845546, 'totalEpisodes': 147, 'stepsPerEpisode': 33, 'rewardPerEpisode': 28.156088198405538
'totalSteps': 14080, 'rewardStep': 0.9113041699393049, 'errorList': [], 'lossList': [0.0, -1.3525663274526596, 0.0, 70.15413358688355, 0.0, 0.0, 0.0], 'rewardMean': 0.774526143595811, 'totalEpisodes': 175, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.56689072271649
'totalSteps': 15360, 'rewardStep': 0.7823710009137921, 'errorList': [], 'lossList': [0.0, -1.3628347146511077, 0.0, 73.6032432937622, 0.0, 0.0, 0.0], 'rewardMean': 0.7990996933975392, 'totalEpisodes': 195, 'stepsPerEpisode': 126, 'rewardPerEpisode': 107.96655277901941
'totalSteps': 16640, 'rewardStep': 0.6470786804230021, 'errorList': [], 'lossList': [0.0, -1.3656288409233093, 0.0, 46.34667951583862, 0.0, 0.0, 0.0], 'rewardMean': 0.7699317551804894, 'totalEpisodes': 207, 'stepsPerEpisode': 154, 'rewardPerEpisode': 122.11539834378925
'totalSteps': 17920, 'rewardStep': 0.290717610417486, 'errorList': [], 'lossList': [0.0, -1.3816120088100434, 0.0, 23.87459942102432, 0.0, 0.0, 0.0], 'rewardMean': 0.7201159039541027, 'totalEpisodes': 215, 'stepsPerEpisode': 99, 'rewardPerEpisode': 64.60600756295067
'totalSteps': 19200, 'rewardStep': 0.8128206782043352, 'errorList': [], 'lossList': [0.0, -1.3994635677337646, 0.0, 17.98122024297714, 0.0, 0.0, 0.0], 'rewardMean': 0.744137396368832, 'totalEpisodes': 225, 'stepsPerEpisode': 29, 'rewardPerEpisode': 26.721815762447502
'totalSteps': 20480, 'rewardStep': 0.8339331722835545, 'errorList': [], 'lossList': [0.0, -1.4181813508272172, 0.0, 9.257883727550507, 0.0, 0.0, 0.0], 'rewardMean': 0.7438783734328103, 'totalEpisodes': 232, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.925461710493804
'totalSteps': 21760, 'rewardStep': 0.9416243880673505, 'errorList': [0.7778551149593441, 0.3386023862431563, 0.5760305026524394, 1.1847855811009502, 0.37943560517885405, 0.2907231810701632, 0.4085068387590087, 0.7666194185469613, 0.7551123124505009, 0.534790604541323, 0.23727728617499863, 2.0132857118454512, 0.8917159977872987, 0.4023423749177991, 0.30304112764220487, 0.567952703064831, 0.39977381168820114, 0.4289204283610711, 0.5090884970993748, 0.7325936276442976, 0.837524896646918, 1.5279296668659976, 0.44351844434160925, 0.9081996183504057, 0.29027060326438964, 0.6923534892825777, 1.1808890948240942, 0.5408175531574567, 0.537546630314818, 0.37757232972781335, 0.4582969370825608, 0.22264320358121092, 0.22572016010785595, 0.37306318433597235, 0.30718399269743724, 0.6614367000775285, 0.5669396292804154, 0.3193352242564831, 0.8766856014718212, 0.2495163928737348, 1.0166713319318874, 0.38365861904951154, 0.22226178929973353, 0.7077196452813952, 2.0520622083648794, 0.2621201596666393, 0.7059039947969924, 0.5707792677302891, 0.5202623906529092, 0.32765267477662224], 'lossList': [0.0, -1.426382075548172, 0.0, 8.199797855615616, 0.0, 0.0, 0.0], 'rewardMean': 0.7569151102378753, 'totalEpisodes': 236, 'stepsPerEpisode': 40, 'rewardPerEpisode': 32.67644565464862, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.8246825150207282, 'errorList': [], 'lossList': [0.0, -1.4109179347753524, 0.0, 7.716355100274086, 0.0, 0.0, 0.0], 'rewardMean': 0.7555887911138383, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.5576516308806
'totalSteps': 24320, 'rewardStep': 0.7311017845844994, 'errorList': [], 'lossList': [0.0, -1.366219891309738, 0.0, 5.3831560856103895, 0.0, 0.0, 0.0], 'rewardMean': 0.7583520130857473, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.4409445248948
'totalSteps': 25600, 'rewardStep': 0.9295391329018836, 'errorList': [], 'lossList': [0.0, -1.3431284886598587, 0.0, 4.445886535122991, 0.0, 0.0, 0.0], 'rewardMean': 0.7705173132755936, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.8012262539548
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.72
