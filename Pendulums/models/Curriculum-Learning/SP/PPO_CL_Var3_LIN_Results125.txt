#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 125
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.5478936380258738, 'errorList': [], 'lossList': [0.0, -1.4061779338121414, 0.0, 33.83410252571106, 0.0, 0.0, 0.0], 'rewardMean': 0.49605818950069575, 'totalEpisodes': 61, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.113454335192213
'totalSteps': 3840, 'rewardStep': 0.3652956372686435, 'errorList': [], 'lossList': [0.0, -1.3903940951824187, 0.0, 39.491181735992434, 0.0, 0.0, 0.0], 'rewardMean': 0.4524706720900116, 'totalEpisodes': 79, 'stepsPerEpisode': 91, 'rewardPerEpisode': 68.47484491352952
'totalSteps': 5120, 'rewardStep': 0.8673612663463877, 'errorList': [], 'lossList': [0.0, -1.3699201995134354, 0.0, 41.67161232948303, 0.0, 0.0, 0.0], 'rewardMean': 0.5561933206541056, 'totalEpisodes': 90, 'stepsPerEpisode': 67, 'rewardPerEpisode': 60.9727076217448
'totalSteps': 6400, 'rewardStep': 0.9317881511855742, 'errorList': [], 'lossList': [0.0, -1.3539720106124877, 0.0, 48.06593499183655, 0.0, 0.0, 0.0], 'rewardMean': 0.6313122867603994, 'totalEpisodes': 98, 'stepsPerEpisode': 34, 'rewardPerEpisode': 29.278673639085692
'totalSteps': 7680, 'rewardStep': 0.5524725614185555, 'errorList': [], 'lossList': [0.0, -1.3351410233974457, 0.0, 48.75824019432068, 0.0, 0.0, 0.0], 'rewardMean': 0.6181723325367587, 'totalEpisodes': 102, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.328435703958746
'totalSteps': 8960, 'rewardStep': 0.7580781545907709, 'errorList': [], 'lossList': [0.0, -1.3060764652490615, 0.0, 102.86453680038453, 0.0, 0.0, 0.0], 'rewardMean': 0.6381588785444748, 'totalEpisodes': 113, 'stepsPerEpisode': 249, 'rewardPerEpisode': 203.32082258973827
'totalSteps': 10240, 'rewardStep': 0.5579087448315739, 'errorList': [], 'lossList': [0.0, -1.2976957899332047, 0.0, 124.65814952850342, 0.0, 0.0, 0.0], 'rewardMean': 0.6281276118303621, 'totalEpisodes': 129, 'stepsPerEpisode': 33, 'rewardPerEpisode': 23.79681940108131
'totalSteps': 11520, 'rewardStep': 0.43677505860317833, 'errorList': [], 'lossList': [0.0, -1.2853572714328765, 0.0, 139.45333534240723, 0.0, 0.0, 0.0], 'rewardMean': 0.6068662170273417, 'totalEpisodes': 155, 'stepsPerEpisode': 44, 'rewardPerEpisode': 34.933468933470614
'totalSteps': 12800, 'rewardStep': 0.6961733694984237, 'errorList': [], 'lossList': [0.0, -1.2731927901506424, 0.0, 37.75937033653259, 0.0, 0.0, 0.0], 'rewardMean': 0.6157969322744499, 'totalEpisodes': 170, 'stepsPerEpisode': 26, 'rewardPerEpisode': 15.86076322518613
'totalSteps': 14080, 'rewardStep': 0.2822280645732111, 'errorList': [], 'lossList': [0.0, -1.2769866585731506, 0.0, 23.486285347938537, 0.0, 0.0, 0.0], 'rewardMean': 0.5995974646342191, 'totalEpisodes': 178, 'stepsPerEpisode': 131, 'rewardPerEpisode': 101.73876234635887
'totalSteps': 15360, 'rewardStep': 0.7907368835743581, 'errorList': [], 'lossList': [0.0, -1.2788524752855301, 0.0, 17.1280460357666, 0.0, 0.0, 0.0], 'rewardMean': 0.6238817891890677, 'totalEpisodes': 187, 'stepsPerEpisode': 73, 'rewardPerEpisode': 65.43580999297033
'totalSteps': 16640, 'rewardStep': 0.8612283839826261, 'errorList': [], 'lossList': [0.0, -1.2698830568790436, 0.0, 9.629988158941268, 0.0, 0.0, 0.0], 'rewardMean': 0.673475063860466, 'totalEpisodes': 190, 'stepsPerEpisode': 320, 'rewardPerEpisode': 249.182537884251
'totalSteps': 17920, 'rewardStep': 0.7591009973728828, 'errorList': [], 'lossList': [0.0, -1.251617665886879, 0.0, 10.056752256155015, 0.0, 0.0, 0.0], 'rewardMean': 0.6626490369631155, 'totalEpisodes': 194, 'stepsPerEpisode': 418, 'rewardPerEpisode': 340.5761306219676
'totalSteps': 19200, 'rewardStep': 0.9131551385278661, 'errorList': [], 'lossList': [0.0, -1.2469680732488633, 0.0, 28.248383679389953, 0.0, 0.0, 0.0], 'rewardMean': 0.6607857356973447, 'totalEpisodes': 198, 'stepsPerEpisode': 98, 'rewardPerEpisode': 85.19053037812368
'totalSteps': 20480, 'rewardStep': 0.7601438227387899, 'errorList': [], 'lossList': [0.0, -1.24992311835289, 0.0, 6.25008328974247, 0.0, 0.0, 0.0], 'rewardMean': 0.6815528618293681, 'totalEpisodes': 199, 'stepsPerEpisode': 363, 'rewardPerEpisode': 259.95520242207556
'totalSteps': 21760, 'rewardStep': 0.950113172941288, 'errorList': [0.6745893832709279, 0.7065489332389252, 0.8991466663303146, 0.33530219685483775, 0.4280695517074061, 0.8939710862379727, 0.5699846806178953, 0.7156802137758843, 0.5793749482648601, 0.32692143583678474, 0.7747274076879804, 0.5848615899193814, 0.7978950115193264, 0.9431556278414573, 0.8177817640225877, 0.6451149741651716, 0.6831422077417969, 0.760843612021219, 0.7496265493047026, 0.5884844183233339, 0.5152910824197366, 0.8699913141600842, 0.6452502010368447, 0.6222692952294525, 0.8521360291434261, 0.5708879191293768, 0.7107768853921694, 0.5103838215243288, 0.7992755516931324, 0.9184919306442193, 0.7338320877013113, 0.9485813985987795, 0.52141269015892, 0.6447546652113324, 1.06491083357642, 1.1389340580804337, 0.4757279027766857, 0.7179298597861897, 0.9811434747189628, 0.7862411386573022, 0.49044940348878224, 0.6607744597311754, 0.5982604389017326, 0.8557349077584995, 0.6104974825681979, 0.9519635924361665, 0.8983465107990846, 0.481289816797801, 0.9009531305202103, 0.7715633187152836], 'lossList': [0.0, -1.2500496876239777, 0.0, 12.480800211429596, 0.0, 0.0, 0.0], 'rewardMean': 0.7007563636644198, 'totalEpisodes': 202, 'stepsPerEpisode': 60, 'rewardPerEpisode': 56.212454240156404, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.5681900554407477, 'errorList': [], 'lossList': [0.0, -1.231343807578087, 0.0, 4.322382549643517, 0.0, 0.0, 0.0], 'rewardMean': 0.7017844947253372, 'totalEpisodes': 203, 'stepsPerEpisode': 658, 'rewardPerEpisode': 532.2793086144885
'totalSteps': 24320, 'rewardStep': 0.7283251695205253, 'errorList': [], 'lossList': [0.0, -1.2024331086874007, 0.0, 4.948164084553719, 0.0, 0.0, 0.0], 'rewardMean': 0.7309395058170718, 'totalEpisodes': 204, 'stepsPerEpisode': 336, 'rewardPerEpisode': 287.19674345631
'totalSteps': 25600, 'rewardStep': 0.8001684748728721, 'errorList': [], 'lossList': [0.0, -1.1930778884887696, 0.0, 7.578385113477707, 0.0, 0.0, 0.0], 'rewardMean': 0.7413390163545167, 'totalEpisodes': 206, 'stepsPerEpisode': 178, 'rewardPerEpisode': 159.31067858404424
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.75
