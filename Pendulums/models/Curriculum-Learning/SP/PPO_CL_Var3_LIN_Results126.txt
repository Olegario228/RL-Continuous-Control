#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 126
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.6305389863081532, 'errorList': [], 'lossList': [0.0, -1.4099474436044692, 0.0, 33.08758605003357, 0.0, 0.0, 0.0], 'rewardMean': 0.7227945595747802, 'totalEpisodes': 58, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.96037829183449
'totalSteps': 3840, 'rewardStep': 0.76755555814459, 'errorList': [], 'lossList': [0.0, -1.3839140856266021, 0.0, 40.163423719406126, 0.0, 0.0, 0.0], 'rewardMean': 0.7377148924313834, 'totalEpisodes': 74, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.75376811144096
'totalSteps': 5120, 'rewardStep': 0.3203821945147229, 'errorList': [], 'lossList': [0.0, -1.3649379682540894, 0.0, 39.27579238414764, 0.0, 0.0, 0.0], 'rewardMean': 0.6333817179522183, 'totalEpisodes': 83, 'stepsPerEpisode': 93, 'rewardPerEpisode': 59.01246019136815
'totalSteps': 6400, 'rewardStep': 0.5442629748824936, 'errorList': [], 'lossList': [0.0, -1.3619309961795807, 0.0, 39.15763597488403, 0.0, 0.0, 0.0], 'rewardMean': 0.6155579693382733, 'totalEpisodes': 90, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.13131684638232
'totalSteps': 7680, 'rewardStep': 0.48269335669234525, 'errorList': [], 'lossList': [0.0, -1.3661448001861571, 0.0, 13.62440292239189, 0.0, 0.0, 0.0], 'rewardMean': 0.5934138672306187, 'totalEpisodes': 91, 'stepsPerEpisode': 497, 'rewardPerEpisode': 341.770924431568
'totalSteps': 8960, 'rewardStep': 0.8912350598630931, 'errorList': [], 'lossList': [0.0, -1.3888495969772339, 0.0, 51.303112516403196, 0.0, 0.0, 0.0], 'rewardMean': 0.6359597518924007, 'totalEpisodes': 97, 'stepsPerEpisode': 109, 'rewardPerEpisode': 95.58197254654837
'totalSteps': 10240, 'rewardStep': 0.5462191619442505, 'errorList': [], 'lossList': [0.0, -1.3832496750354766, 0.0, 93.7614462852478, 0.0, 0.0, 0.0], 'rewardMean': 0.624742178148882, 'totalEpisodes': 108, 'stepsPerEpisode': 54, 'rewardPerEpisode': 27.706648047742444
'totalSteps': 11520, 'rewardStep': 0.8666560650002632, 'errorList': [], 'lossList': [0.0, -1.3763370925188065, 0.0, 114.58410718917847, 0.0, 0.0, 0.0], 'rewardMean': 0.6516214989101465, 'totalEpisodes': 125, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.72645275218786
'totalSteps': 12800, 'rewardStep': 0.3945890614435914, 'errorList': [], 'lossList': [0.0, -1.3575545513629914, 0.0, 55.54333227157593, 0.0, 0.0, 0.0], 'rewardMean': 0.625918255163491, 'totalEpisodes': 137, 'stepsPerEpisode': 90, 'rewardPerEpisode': 64.15488471428608
'totalSteps': 14080, 'rewardStep': 0.7819274016466387, 'errorList': [], 'lossList': [0.0, -1.3370708084106446, 0.0, 47.78008271217346, 0.0, 0.0, 0.0], 'rewardMean': 0.6226059820440142, 'totalEpisodes': 146, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.00522770640307
'totalSteps': 15360, 'rewardStep': 0.6050496576159199, 'errorList': [], 'lossList': [0.0, -1.317930561900139, 0.0, 6.6823301351070405, 0.0, 0.0, 0.0], 'rewardMean': 0.6200570491747909, 'totalEpisodes': 153, 'stepsPerEpisode': 77, 'rewardPerEpisode': 56.69644854971647
'totalSteps': 16640, 'rewardStep': 0.5216677760409565, 'errorList': [], 'lossList': [0.0, -1.3115180963277817, 0.0, 6.12188695192337, 0.0, 0.0, 0.0], 'rewardMean': 0.5954682709644276, 'totalEpisodes': 160, 'stepsPerEpisode': 106, 'rewardPerEpisode': 76.4063710571907
'totalSteps': 17920, 'rewardStep': 0.6668445381567958, 'errorList': [], 'lossList': [0.0, -1.3019378364086152, 0.0, 4.01447423517704, 0.0, 0.0, 0.0], 'rewardMean': 0.6301145053286349, 'totalEpisodes': 164, 'stepsPerEpisode': 143, 'rewardPerEpisode': 109.01272788689695
'totalSteps': 19200, 'rewardStep': 0.8765195652713, 'errorList': [], 'lossList': [0.0, -1.2855485314130783, 0.0, 5.162424245476723, 0.0, 0.0, 0.0], 'rewardMean': 0.6633401643675154, 'totalEpisodes': 167, 'stepsPerEpisode': 218, 'rewardPerEpisode': 185.55099605540437
'totalSteps': 20480, 'rewardStep': 0.6852564670897103, 'errorList': [], 'lossList': [0.0, -1.2612263298034667, 0.0, 4.279829424917698, 0.0, 0.0, 0.0], 'rewardMean': 0.6835964754072519, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.5884705135911
'totalSteps': 21760, 'rewardStep': 0.6481059871172964, 'errorList': [], 'lossList': [0.0, -1.2286336243152618, 0.0, 2.7238453567028045, 0.0, 0.0, 0.0], 'rewardMean': 0.6592835681326723, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.4815129645372
'totalSteps': 23040, 'rewardStep': 0.8974621848200321, 'errorList': [], 'lossList': [0.0, -1.2031174385547638, 0.0, 1.29182024307549, 0.0, 0.0, 0.0], 'rewardMean': 0.6944078704202504, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.0946415644912
'totalSteps': 24320, 'rewardStep': 0.8751341634532355, 'errorList': [], 'lossList': [0.0, -1.1623496878147126, 0.0, 1.4336310498043894, 0.0, 0.0, 0.0], 'rewardMean': 0.6952556802655477, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.725034038246
'totalSteps': 25600, 'rewardStep': 0.9946410566692645, 'errorList': [0.08368978792174872, 0.09153742391598098, 0.10537193093067575, 0.082163225689312, 0.09209030194651104, 0.12598239600047365, 0.106692315164292, 0.11079305656763436, 0.12836824975046468, 0.10280657239488603, 0.09114915287609762, 0.10944392995823822, 0.11060716977278014, 0.0988960036188429, 0.09695733929144276, 0.10676767034297066, 0.10277735072934363, 0.09255246559129734, 0.12517171690458917, 0.10428620674661211, 0.11334107190172614, 0.09738398744944335, 0.1012797501192264, 0.13103104153029033, 0.09810544409212947, 0.1042483499243109, 0.13471836931262515, 0.11712650755877267, 0.10613973811028535, 0.09557749565982109, 0.1160635228292881, 0.08097657788771642, 0.08761253006454321, 0.11996605168296942, 0.11752958490638717, 0.10130984478087457, 0.15239302569816518, 0.09112895300046264, 0.10941721837764967, 0.13281072446793846, 0.09855354550017796, 0.09745773921136733, 0.0998270913392931, 0.11823925605875756, 0.105018532416121, 0.10469380629614503, 0.09777176013025905, 0.10735104746782266, 0.11757556240269551, 0.09174188562674702], 'lossList': [0.0, -1.1119595241546631, 0.0, 1.2629591638967395, 0.0, 0.0, 0.0], 'rewardMean': 0.7552608797881151, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1191.9195691473124, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=85.65
