#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 127
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.7575957976530651, 'errorList': [], 'lossList': [0.0, -1.4227072030305863, 0.0, 34.591802759170534, 0.0, 0.0, 0.0], 'rewardMean': 0.7866165697183873, 'totalEpisodes': 71, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.792281214321591
'totalSteps': 3840, 'rewardStep': 0.8885093234330182, 'errorList': [], 'lossList': [0.0, -1.4023581314086915, 0.0, 39.962571153640745, 0.0, 0.0, 0.0], 'rewardMean': 0.8205808209565976, 'totalEpisodes': 95, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.790029774158981
'totalSteps': 5120, 'rewardStep': 0.6568398692029151, 'errorList': [], 'lossList': [0.0, -1.373487057685852, 0.0, 35.669504957199095, 0.0, 0.0, 0.0], 'rewardMean': 0.779645583018177, 'totalEpisodes': 105, 'stepsPerEpisode': 44, 'rewardPerEpisode': 35.20560497356418
'totalSteps': 6400, 'rewardStep': 0.7079805756715059, 'errorList': [], 'lossList': [0.0, -1.3648389369249343, 0.0, 42.547452635765076, 0.0, 0.0, 0.0], 'rewardMean': 0.7653125815488429, 'totalEpisodes': 116, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.890721319254297
'totalSteps': 7680, 'rewardStep': 0.8883695648432959, 'errorList': [], 'lossList': [0.0, -1.3449461859464646, 0.0, 53.3458465719223, 0.0, 0.0, 0.0], 'rewardMean': 0.785822078764585, 'totalEpisodes': 122, 'stepsPerEpisode': 313, 'rewardPerEpisode': 247.41991031720306
'totalSteps': 8960, 'rewardStep': 0.7585214088072062, 'errorList': [], 'lossList': [0.0, -1.3385866534709931, 0.0, 47.78221255779266, 0.0, 0.0, 0.0], 'rewardMean': 0.7819219830563879, 'totalEpisodes': 126, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.9732260116480007
'totalSteps': 10240, 'rewardStep': 0.9423584929345699, 'errorList': [13.370032082593436, 189.06268912047366, 75.18667735910712, 134.7919319674484, 187.86272064456475, 33.09208879096129, 97.15057922397504, 123.80045508003148, 173.62009829133387, 130.65297401428128, 216.33358360193182, 60.23748525192375, 37.32065403034862, 147.46680150141557, 172.1146073120275, 190.67597278403255, 154.1113529006472, 181.96337055256222, 113.33635939097456, 13.401321482962535, 103.32651744675682, 164.35697691160368, 131.63964364264967, 110.72928213100663, 102.38351542138562, 152.78980060879957, 115.87513326281206, 224.12619088590208, 2.9500063025180125, 64.89830206362674, 116.75400420448264, 153.33175402150562, 71.22733875751203, 200.1021966413162, 111.60489139606588, 42.451165784532414, 217.26075019848824, 98.29264227918506, 135.7047989754625, 217.44801089105522, 88.6426159068495, 23.569417658463227, 46.78320682295043, 205.44377742707454, 59.87503449826281, 90.74586548597443, 230.27573163387248, 46.43452421333588, 19.80730324566326, 19.66277514337224], 'lossList': [0.0, -1.3448775070905685, 0.0, 67.32333534240723, 0.0, 0.0, 0.0], 'rewardMean': 0.8019765467911607, 'totalEpisodes': 132, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.40754036703957, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.7156444734286608, 'errorList': [], 'lossList': [0.0, -1.3466269981861114, 0.0, 80.63935934066772, 0.0, 0.0, 0.0], 'rewardMean': 0.7923840941953274, 'totalEpisodes': 145, 'stepsPerEpisode': 18, 'rewardPerEpisode': 11.185941065679966
'totalSteps': 12800, 'rewardStep': 0.4214323688497691, 'errorList': [], 'lossList': [0.0, -1.3627320659160613, 0.0, 23.877655608654024, 0.0, 0.0, 0.0], 'rewardMean': 0.7552889216607716, 'totalEpisodes': 152, 'stepsPerEpisode': 202, 'rewardPerEpisode': 143.58202658528543
'totalSteps': 14080, 'rewardStep': 0.6386288804201592, 'errorList': [], 'lossList': [0.0, -1.3479283213615418, 0.0, 10.227801449298859, 0.0, 0.0, 0.0], 'rewardMean': 0.7375880755244164, 'totalEpisodes': 157, 'stepsPerEpisode': 119, 'rewardPerEpisode': 95.17215758039687
'totalSteps': 15360, 'rewardStep': 0.7940649304291417, 'errorList': [], 'lossList': [0.0, -1.326757827401161, 0.0, 30.85321924686432, 0.0, 0.0, 0.0], 'rewardMean': 0.741234988802024, 'totalEpisodes': 164, 'stepsPerEpisode': 81, 'rewardPerEpisode': 64.51149061500254
'totalSteps': 16640, 'rewardStep': 0.7712230617680906, 'errorList': [], 'lossList': [0.0, -1.3233850276470185, 0.0, 6.983538360595703, 0.0, 0.0, 0.0], 'rewardMean': 0.7295063626355315, 'totalEpisodes': 167, 'stepsPerEpisode': 95, 'rewardPerEpisode': 74.02958360517673
'totalSteps': 17920, 'rewardStep': 0.7782789316097776, 'errorList': [], 'lossList': [0.0, -1.3086345744132997, 0.0, 7.701376171708107, 0.0, 0.0, 0.0], 'rewardMean': 0.7416502688762178, 'totalEpisodes': 168, 'stepsPerEpisode': 650, 'rewardPerEpisode': 533.9256362071416
'totalSteps': 19200, 'rewardStep': 0.9599567960332127, 'errorList': [0.1950316663644835, 0.18031209626303651, 0.2622920524131685, 0.427291538130008, 0.19879585520668658, 0.21734375436817419, 0.31456699689741346, 0.19597696153352698, 0.3171004816853421, 0.2837651932756393, 0.1288344307513714, 0.18166672216169927, 0.2845952146454079, 0.31017456480538264, 0.22078974410757082, 0.20359268364288466, 0.2076938047910017, 0.21661140907290216, 0.3193062375221952, 0.24858534147236655, 0.34006356442895025, 0.17490468589667504, 0.19536295304240384, 0.19102440470514037, 0.25070961844790285, 0.24424310392941873, 0.19962159869910162, 0.17235033942847475, 0.2594884221484053, 0.30552146017054443, 0.43824595052182946, 0.13272606956478664, 0.17831591765841046, 0.1802277507956513, 0.25733192588020926, 0.21988525267098005, 0.28236553151638155, 0.15019877421654812, 0.2992334191941411, 0.19489627031585086, 0.34886069091460165, 0.1690495552569084, 0.30794430077563917, 0.16748543616812736, 0.24635223364817574, 0.24855620556100133, 0.16092790371954205, 0.23919328678112065, 0.14934553659184666, 0.1371400821635526], 'lossList': [0.0, -1.2745813375711441, 0.0, 10.030212690830231, 0.0, 0.0, 0.0], 'rewardMean': 0.7668478909123884, 'totalEpisodes': 170, 'stepsPerEpisode': 755, 'rewardPerEpisode': 584.7070327806715, 'successfulTests': 21
'totalSteps': 20480, 'rewardStep': 0.909847901837241, 'errorList': [], 'lossList': [0.0, -1.235201712846756, 0.0, 2.1025067852437496, 0.0, 0.0, 0.0], 'rewardMean': 0.7689957246117828, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.9856740435664
'totalSteps': 21760, 'rewardStep': 0.8340688878868489, 'errorList': [], 'lossList': [0.0, -1.1976248037815094, 0.0, 2.1340852954983713, 0.0, 0.0, 0.0], 'rewardMean': 0.7765504725197472, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.8405550560665
'totalSteps': 23040, 'rewardStep': 0.9523934731840671, 'errorList': [0.20843468838769277, 0.1628107478740637, 0.16325305242667532, 0.2675504158289979, 0.15212722845001378, 0.1480622683159979, 0.1546267502138358, 0.18267545608622943, 0.13134559342339544, 0.15425565081550774, 0.23884078285457366, 0.1786037024626177, 0.1379117093429147, 0.1502506241013853, 0.2198651410893865, 0.25495895730201473, 0.17336770841389884, 0.13953434460526307, 0.23168644431643615, 0.16379809677582796, 0.21223332915463713, 0.12231432715963339, 0.18758736508580184, 0.1549398240711726, 0.2178273531165507, 0.18231593367327542, 0.13404981668035637, 0.13473710838436623, 0.1583564804037948, 0.1274788846339523, 0.19273377877517162, 0.1454375380361345, 0.10494461966837977, 0.2779974745449691, 0.12814428866768116, 0.12302787280998002, 0.2083597852271524, 0.1409241579607279, 0.10063004046475256, 0.0984641039034974, 0.20085344657726023, 0.09173689606702354, 0.15583621667843972, 0.2417559695295613, 0.16587543672486546, 0.09822466317751784, 0.13753258452925343, 0.09826269462950059, 0.2528918131903729, 0.17840743807654116], 'lossList': [0.0, -1.1457884627580643, 0.0, 1.5453307211771607, 0.0, 0.0, 0.0], 'rewardMean': 0.7775539705446969, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.471633146432, 'successfulTests': 37
'totalSteps': 24320, 'rewardStep': 0.887607546859536, 'errorList': [], 'lossList': [0.0, -1.1112826716899873, 0.0, 1.3036227053403855, 0.0, 0.0, 0.0], 'rewardMean': 0.7947502778877844, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1177.1594546147505
'totalSteps': 25600, 'rewardStep': 0.8763565336426644, 'errorList': [], 'lossList': [0.0, -1.0900524574518204, 0.0, 0.808236044421792, 0.0, 0.0, 0.0], 'rewardMean': 0.840242694367074, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.3381196656421
#maxSuccessfulTests=37, maxSuccessfulTestsAtStep=23040, timeSpent=124.85
