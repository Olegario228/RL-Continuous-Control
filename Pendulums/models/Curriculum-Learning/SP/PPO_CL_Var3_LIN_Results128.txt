#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 128
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.8048054496129363, 'errorList': [], 'lossList': [0.0, -1.4346733021736144, 0.0, 32.68640424728394, 0.0, 0.0, 0.0], 'rewardMean': 0.6548723381766606, 'totalEpisodes': 62, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.260310253010266
'totalSteps': 3840, 'rewardStep': 0.8420848601971853, 'errorList': [], 'lossList': [0.0, -1.435584152340889, 0.0, 42.90108724594116, 0.0, 0.0, 0.0], 'rewardMean': 0.7172765121835022, 'totalEpisodes': 82, 'stepsPerEpisode': 49, 'rewardPerEpisode': 36.90002949101537
'totalSteps': 5120, 'rewardStep': 0.9348303076444235, 'errorList': [], 'lossList': [0.0, -1.437696908712387, 0.0, 42.83885225296021, 0.0, 0.0, 0.0], 'rewardMean': 0.7716649610487325, 'totalEpisodes': 90, 'stepsPerEpisode': 70, 'rewardPerEpisode': 60.70420366595491
'totalSteps': 6400, 'rewardStep': 0.5817603859523575, 'errorList': [], 'lossList': [0.0, -1.4321027708053589, 0.0, 43.83131293773651, 0.0, 0.0, 0.0], 'rewardMean': 0.7336840460294575, 'totalEpisodes': 99, 'stepsPerEpisode': 166, 'rewardPerEpisode': 123.08537580809153
'totalSteps': 7680, 'rewardStep': 0.8597685492113106, 'errorList': [], 'lossList': [0.0, -1.4252825844287873, 0.0, 50.281948113441466, 0.0, 0.0, 0.0], 'rewardMean': 0.7546981298930997, 'totalEpisodes': 106, 'stepsPerEpisode': 99, 'rewardPerEpisode': 74.53154012732688
'totalSteps': 8960, 'rewardStep': 0.7767854096184972, 'errorList': [], 'lossList': [0.0, -1.4197099441289902, 0.0, 55.36200115680695, 0.0, 0.0, 0.0], 'rewardMean': 0.7578534555681564, 'totalEpisodes': 111, 'stepsPerEpisode': 108, 'rewardPerEpisode': 74.6053673624713
'totalSteps': 10240, 'rewardStep': 0.5371146945500263, 'errorList': [], 'lossList': [0.0, -1.422771479487419, 0.0, 45.28218970775604, 0.0, 0.0, 0.0], 'rewardMean': 0.7302611104408903, 'totalEpisodes': 118, 'stepsPerEpisode': 161, 'rewardPerEpisode': 134.03659454500368
'totalSteps': 11520, 'rewardStep': 0.4231745669144418, 'errorList': [], 'lossList': [0.0, -1.436336562037468, 0.0, 22.21366497993469, 0.0, 0.0, 0.0], 'rewardMean': 0.696140383382396, 'totalEpisodes': 125, 'stepsPerEpisode': 73, 'rewardPerEpisode': 52.87829104942898
'totalSteps': 12800, 'rewardStep': 0.5833408592394056, 'errorList': [], 'lossList': [0.0, -1.4492124104499817, 0.0, 11.463291711807251, 0.0, 0.0, 0.0], 'rewardMean': 0.684860430968097, 'totalEpisodes': 129, 'stepsPerEpisode': 99, 'rewardPerEpisode': 72.15548423843894
'totalSteps': 14080, 'rewardStep': 0.8070332569576526, 'errorList': [], 'lossList': [0.0, -1.4455727487802505, 0.0, 16.105985095500944, 0.0, 0.0, 0.0], 'rewardMean': 0.7150698339898236, 'totalEpisodes': 133, 'stepsPerEpisode': 226, 'rewardPerEpisode': 185.82318706662804
'totalSteps': 15360, 'rewardStep': 0.7882207403402842, 'errorList': [], 'lossList': [0.0, -1.4496398627758027, 0.0, 6.170589904785157, 0.0, 0.0, 0.0], 'rewardMean': 0.7134113630625585, 'totalEpisodes': 135, 'stepsPerEpisode': 365, 'rewardPerEpisode': 318.37780492349987
'totalSteps': 16640, 'rewardStep': 0.7753501952118201, 'errorList': [], 'lossList': [0.0, -1.4406733137369157, 0.0, 2.7903778141736986, 0.0, 0.0, 0.0], 'rewardMean': 0.7067378965640219, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.25726094402
'totalSteps': 17920, 'rewardStep': 0.8263494701631432, 'errorList': [], 'lossList': [0.0, -1.413695365190506, 0.0, 1.9611100894212723, 0.0, 0.0, 0.0], 'rewardMean': 0.6958898128158939, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.1556413790026
'totalSteps': 19200, 'rewardStep': 0.9636412868078869, 'errorList': [0.05223799099079629, 0.05369345108979106, 0.05943463234990413, 0.03497389505906442, 0.03565999291992817, 0.0421008515984248, 0.06866778232614229, 0.050589235453022796, 0.043094766868096934, 0.04083616629447799, 0.044850042498470695, 0.045585319008620555, 0.055175951130775315, 0.03512527787601386, 0.051956222474016975, 0.06734918032089225, 0.05681981461225586, 0.04830651090055211, 0.060870022820389495, 0.05937093506852582, 0.06342112870067963, 0.05932727439639877, 0.04391317581474918, 0.04120384783554274, 0.045861611020096366, 0.04128953868741247, 0.05231789768085858, 0.049616046787991835, 0.05767437581781543, 0.04204415687442289, 0.04956741120603818, 0.03760782787739648, 0.049304839002821364, 0.049793498635780094, 0.047025740573879815, 0.035046505688753976, 0.04977944469928316, 0.05399081334838646, 0.059391743904979546, 0.05076640808626715, 0.043746724289773685, 0.04514337968508814, 0.04497697792716608, 0.0432645869376182, 0.05284615917292072, 0.04993376518919377, 0.056912270057112004, 0.03467479687428838, 0.038339736375945274, 0.04602256931801861], 'lossList': [0.0, -1.3730311101675035, 0.0, 1.3621284636855124, 0.0, 0.0, 0.0], 'rewardMean': 0.7340779029014468, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.650823507101, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=68.81
