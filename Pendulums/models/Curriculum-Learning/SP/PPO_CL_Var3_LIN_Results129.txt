#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 129
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.5784352382615021, 'errorList': [], 'lossList': [0.0, -1.4087852555513383, 0.0, 33.89359945297241, 0.0, 0.0, 0.0], 'rewardMean': 0.7497626862064546, 'totalEpisodes': 70, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.479878407048496
'totalSteps': 3840, 'rewardStep': 0.9768762416989831, 'errorList': [], 'lossList': [0.0, -1.4014848297834397, 0.0, 44.25531837463379, 0.0, 0.0, 0.0], 'rewardMean': 0.8254672047039641, 'totalEpisodes': 97, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.761841090855924
'totalSteps': 5120, 'rewardStep': 0.9346486598467276, 'errorList': [], 'lossList': [0.0, -1.3862281960248948, 0.0, 39.10614929199219, 0.0, 0.0, 0.0], 'rewardMean': 0.8527625684896549, 'totalEpisodes': 110, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.481690589929663
'totalSteps': 6400, 'rewardStep': 0.8053861459298731, 'errorList': [], 'lossList': [0.0, -1.3816509371995926, 0.0, 37.66488915681839, 0.0, 0.0, 0.0], 'rewardMean': 0.8432872839776986, 'totalEpisodes': 117, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.766455299176194
'totalSteps': 7680, 'rewardStep': 0.7650843312228186, 'errorList': [], 'lossList': [0.0, -1.3922686082124711, 0.0, 37.57534575939179, 0.0, 0.0, 0.0], 'rewardMean': 0.8302534585185519, 'totalEpisodes': 123, 'stepsPerEpisode': 37, 'rewardPerEpisode': 26.649222496900805
'totalSteps': 8960, 'rewardStep': 0.7728575615528427, 'errorList': [], 'lossList': [0.0, -1.3910050284862518, 0.0, 113.02640766143799, 0.0, 0.0, 0.0], 'rewardMean': 0.8220540446663077, 'totalEpisodes': 135, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.1832764535727764
'totalSteps': 10240, 'rewardStep': 0.9600757883236035, 'errorList': [16.158285542755674, 121.90003493372063, 12.957673348774085, 12.760594081051602, 62.975556638420954, 28.31778496152536, 41.71962751866637, 25.52769822280846, 21.5090960099794, 11.52667708119686, 9.825356788850454, 108.68351524653038, 109.31977484760789, 36.400381878517194, 89.38151618198212, 15.816680628903429, 121.25562697532042, 21.272871666706067, 13.567384591871342, 2.1893783367221484, 52.41636455359198, 27.818840406185593, 47.28533974221891, 21.509563406404343, 19.78306671868954, 17.214948347978638, 62.618293976799684, 65.01649674852085, 10.025413936731116, 24.494951910986195, 21.68632574325815, 48.44785801705495, 13.220771768272632, 6.439477780395515, 92.92313728835065, 15.391959137265326, 35.74994805208769, 64.57111028287255, 43.64934235321565, 20.18712842237576, 32.50546596625001, 14.068402320713671, 14.271165711038869, 25.477355238173768, 5.616371256486573, 12.111980032791315, 19.43607200427198, 16.953137096599292, 18.974093889760095, 99.47556799857215], 'lossList': [0.0, -1.3880518853664399, 0.0, 67.50033321380616, 0.0, 0.0, 0.0], 'rewardMean': 0.8393067626234697, 'totalEpisodes': 144, 'stepsPerEpisode': 129, 'rewardPerEpisode': 111.77494557601234, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.18355364211487457, 'errorList': [], 'lossList': [0.0, -1.379085082411766, 0.0, 49.792083253860476, 0.0, 0.0, 0.0], 'rewardMean': 0.7664453047891814, 'totalEpisodes': 152, 'stepsPerEpisode': 185, 'rewardPerEpisode': 127.52392565766819
'totalSteps': 12800, 'rewardStep': 0.547958339802154, 'errorList': [], 'lossList': [0.0, -1.366569814682007, 0.0, 33.667394795417785, 0.0, 0.0, 0.0], 'rewardMean': 0.7445966082904787, 'totalEpisodes': 160, 'stepsPerEpisode': 63, 'rewardPerEpisode': 46.852446047114306
'totalSteps': 14080, 'rewardStep': 0.8537444604937626, 'errorList': [], 'lossList': [0.0, -1.340858206152916, 0.0, 9.282785258293153, 0.0, 0.0, 0.0], 'rewardMean': 0.7378620409247143, 'totalEpisodes': 167, 'stepsPerEpisode': 63, 'rewardPerEpisode': 52.608229451885755
'totalSteps': 15360, 'rewardStep': 0.8310810374635619, 'errorList': [], 'lossList': [0.0, -1.3132336324453353, 0.0, 48.655711996555326, 0.0, 0.0, 0.0], 'rewardMean': 0.7631266208449203, 'totalEpisodes': 176, 'stepsPerEpisode': 32, 'rewardPerEpisode': 28.347933039990885
'totalSteps': 16640, 'rewardStep': 0.38553242837009216, 'errorList': [], 'lossList': [0.0, -1.286809955239296, 0.0, 7.352779299020767, 0.0, 0.0, 0.0], 'rewardMean': 0.703992239512031, 'totalEpisodes': 181, 'stepsPerEpisode': 229, 'rewardPerEpisode': 168.78408557662834
'totalSteps': 17920, 'rewardStep': 0.28324577817524194, 'errorList': [], 'lossList': [0.0, -1.2685947227478027, 0.0, 10.508238742351532, 0.0, 0.0, 0.0], 'rewardMean': 0.6388519513448825, 'totalEpisodes': 183, 'stepsPerEpisode': 595, 'rewardPerEpisode': 449.0256165522465
'totalSteps': 19200, 'rewardStep': 0.9098373306929166, 'errorList': [], 'lossList': [0.0, -1.2585944765806198, 0.0, 5.436577790975571, 0.0, 0.0, 0.0], 'rewardMean': 0.6492970698211868, 'totalEpisodes': 185, 'stepsPerEpisode': 304, 'rewardPerEpisode': 262.974473599373
'totalSteps': 20480, 'rewardStep': 0.932053070027122, 'errorList': [0.6330193001836105, 0.6748130299463082, 0.5307057569008795, 0.6820010730822026, 0.6428672252803488, 0.6219778879185435, 0.6258189541178668, 0.7215207820532263, 0.63680697690268, 0.6728349004053173, 0.5006734926499293, 0.5420594251078596, 0.6387720275973923, 0.5890130905035181, 0.7250800813367123, 0.6478509878328998, 0.6271737955383642, 0.6079794271812377, 0.5282013357844846, 0.5311146289797686, 0.7544143642898827, 0.6617569523909064, 0.5734909675439777, 0.5101321695101044, 0.6376776678851004, 0.6089057938966721, 0.6659139523037437, 0.595461174909904, 0.5863678088834791, 0.5565260697844255, 0.5343863854370876, 0.5333290575180086, 0.643515422590661, 0.7192029500575108, 0.5456504646002847, 0.6240620058430509, 0.6217411769312845, 0.5584722822954978, 0.6818703759322289, 0.4985354087637299, 0.5504118927198093, 0.7429069651566099, 0.6584449509256186, 0.6324267413049736, 0.6767153316059934, 0.6881113917810552, 0.5689003180454723, 0.6651275452797928, 0.5838455786678701, 0.633694728704803], 'lossList': [0.0, -1.2461123901605606, 0.0, 5.7711925303936, 0.0, 0.0, 0.0], 'rewardMean': 0.6659939437016172, 'totalEpisodes': 186, 'stepsPerEpisode': 778, 'rewardPerEpisode': 610.2342400747797, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.6300546571813943, 'errorList': [], 'lossList': [0.0, -1.2378680050373077, 0.0, 7.2048672318458555, 0.0, 0.0, 0.0], 'rewardMean': 0.6517136532644723, 'totalEpisodes': 188, 'stepsPerEpisode': 255, 'rewardPerEpisode': 214.6936902045047
'totalSteps': 23040, 'rewardStep': 0.6891050850219129, 'errorList': [], 'lossList': [0.0, -1.2146396833658217, 0.0, 2.0527230912446974, 0.0, 0.0, 0.0], 'rewardMean': 0.6246165829343033, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 939.3306735996985
'totalSteps': 24320, 'rewardStep': 0.7931839837055041, 'errorList': [], 'lossList': [0.0, -1.187776391506195, 0.0, 0.8350328065454959, 0.0, 0.0, 0.0], 'rewardMean': 0.6855796170933661, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1056.745900537178
'totalSteps': 25600, 'rewardStep': 0.9687608504614853, 'errorList': [0.11197741002175163, 0.08129449355214483, 0.08349513171327075, 0.11667206453522551, 0.1276175063790183, 0.1459316424147417, 0.10452414600385312, 0.09437223512429009, 0.12398302431979714, 0.16610727021689012, 0.14504398161959478, 0.09537482426499767, 0.1330000011817861, 0.15043690479829444, 0.17381712615136904, 0.11206645672391809, 0.18171859125536916, 0.12105832501673139, 0.1420361402812315, 0.10121819192917791, 0.20654350393841134, 0.11540692379867294, 0.2178175604262088, 0.09644776780462082, 0.10780242552479367, 0.13423624252044475, 0.07995779205825498, 0.11077223314893353, 0.10317479865992106, 0.10975051368274764, 0.0978850386830658, 0.12224871897156113, 0.10623271599529598, 0.13961741683110718, 0.1203717979115363, 0.10303087407670018, 0.10461049726922511, 0.15840272067439748, 0.0944942546543215, 0.1371665569962668, 0.15275009364456982, 0.1722995656128716, 0.13817831866724659, 0.09447758989040701, 0.10038727867522573, 0.17246801072801293, 0.13890195882732553, 0.09101755477798576, 0.10902378501713088, 0.23188018092829335], 'lossList': [0.0, -1.1713506972789764, 0.0, 0.9505089929327368, 0.0, 0.0, 0.0], 'rewardMean': 0.7276598681592994, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.0485570710052, 'successfulTests': 47
#maxSuccessfulTests=47, maxSuccessfulTestsAtStep=25600, timeSpent=121.34
