#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 13
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.8280213114391519, 'errorList': [], 'lossList': [0.0, -1.4203389996290208, 0.0, 22.17950361728668, 0.0, 0.0, 0.0], 'rewardMean': 0.796673572193104, 'totalEpisodes': 17, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.018229061641861
'totalSteps': 3840, 'rewardStep': 0.8288746310143846, 'errorList': [], 'lossList': [0.0, -1.4091526460647583, 0.0, 48.52863198280335, 0.0, 0.0, 0.0], 'rewardMean': 0.8074072584668642, 'totalEpisodes': 30, 'stepsPerEpisode': 49, 'rewardPerEpisode': 40.26384575638281
'totalSteps': 5120, 'rewardStep': 0.8938572685229895, 'errorList': [], 'lossList': [0.0, -1.4048558259010315, 0.0, 72.83265171051025, 0.0, 0.0, 0.0], 'rewardMean': 0.8290197609808956, 'totalEpisodes': 49, 'stepsPerEpisode': 74, 'rewardPerEpisode': 62.98409706492997
'totalSteps': 6400, 'rewardStep': 0.8716001665325603, 'errorList': [], 'lossList': [0.0, -1.411419507265091, 0.0, 100.08279756546021, 0.0, 0.0, 0.0], 'rewardMean': 0.8375358420912284, 'totalEpisodes': 102, 'stepsPerEpisode': 50, 'rewardPerEpisode': 39.08408797672425
'totalSteps': 7680, 'rewardStep': 0.7140063238050679, 'errorList': [], 'lossList': [0.0, -1.412105770111084, 0.0, 56.847549495697024, 0.0, 0.0, 0.0], 'rewardMean': 0.816947589043535, 'totalEpisodes': 130, 'stepsPerEpisode': 79, 'rewardPerEpisode': 54.09893553813586
'totalSteps': 8960, 'rewardStep': 0.7121391546638789, 'errorList': [], 'lossList': [0.0, -1.401071347594261, 0.0, 43.33179804801941, 0.0, 0.0, 0.0], 'rewardMean': 0.801974955560727, 'totalEpisodes': 148, 'stepsPerEpisode': 70, 'rewardPerEpisode': 50.76753012530466
'totalSteps': 10240, 'rewardStep': 0.8784754462789617, 'errorList': [], 'lossList': [0.0, -1.387650769352913, 0.0, 24.89033091545105, 0.0, 0.0, 0.0], 'rewardMean': 0.8115375169005064, 'totalEpisodes': 154, 'stepsPerEpisode': 181, 'rewardPerEpisode': 148.0305662927746
'totalSteps': 11520, 'rewardStep': 0.7311943407504936, 'errorList': [], 'lossList': [0.0, -1.371950662136078, 0.0, 47.66215814590454, 0.0, 0.0, 0.0], 'rewardMean': 0.8026104973282827, 'totalEpisodes': 164, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.770302622260354
'totalSteps': 12800, 'rewardStep': 0.5795782504967215, 'errorList': [], 'lossList': [0.0, -1.35680568754673, 0.0, 11.918700296878814, 0.0, 0.0, 0.0], 'rewardMean': 0.7803072726451267, 'totalEpisodes': 168, 'stepsPerEpisode': 25, 'rewardPerEpisode': 16.697992736089528
'totalSteps': 14080, 'rewardStep': 0.9054176115665677, 'errorList': [], 'lossList': [0.0, -1.318880399465561, 0.0, 9.614424666166306, 0.0, 0.0, 0.0], 'rewardMean': 0.7943164505070778, 'totalEpisodes': 175, 'stepsPerEpisode': 125, 'rewardPerEpisode': 108.15185398642814
'totalSteps': 15360, 'rewardStep': 0.7303325838571173, 'errorList': [], 'lossList': [0.0, -1.2998507314920424, 0.0, 17.757271963357926, 0.0, 0.0, 0.0], 'rewardMean': 0.7845475777488743, 'totalEpisodes': 181, 'stepsPerEpisode': 125, 'rewardPerEpisode': 107.05820398257359
'totalSteps': 16640, 'rewardStep': 0.7708519003499266, 'errorList': [], 'lossList': [0.0, -1.2922568225860596, 0.0, 9.102057784199715, 0.0, 0.0, 0.0], 'rewardMean': 0.7787453046824285, 'totalEpisodes': 183, 'stepsPerEpisode': 439, 'rewardPerEpisode': 369.0426556503346
'totalSteps': 17920, 'rewardStep': 0.8380073425448373, 'errorList': [], 'lossList': [0.0, -1.2791890716552734, 0.0, 6.308845565319062, 0.0, 0.0, 0.0], 'rewardMean': 0.7731603120846132, 'totalEpisodes': 188, 'stepsPerEpisode': 262, 'rewardPerEpisode': 237.32230813762754
'totalSteps': 19200, 'rewardStep': 0.9459128001390076, 'errorList': [7.871356447198091, 0.5102459446670917, 0.5253466757811537, 3.1024094058452647, 3.252290949684933, 18.379512353718866, 22.3344827508574, 2.5758255828365915, 0.30544804005511184, 17.00584049201813, 16.124095806523556, 3.405156719908586, 9.940889980667638, 3.207126777093608, 2.6646530616528668, 4.660942309413448, 13.073400896997647, 23.90184766083301, 0.9587834037498292, 4.081668539132376, 3.1959886306812284, 14.117994655301617, 6.131149841641155, 3.7852878063595536, 4.351158371506333, 1.6027069324979248, 12.907223030138068, 8.087364995887443, 11.722007616665088, 6.660784312881753, 3.3090640852996804, 1.6591384073222433, 14.05840116323795, 5.596215679283064, 11.926411399145756, 9.449514495516892, 3.2682805582300056, 1.0615660934405657, 1.0500621576950082, 3.514788559239864, 4.638568216358236, 12.09192555389456, 1.8365710659095427, 4.363250631940853, 0.4532297367052984, 2.4935474535641515, 4.857396719499515, 20.883961353347217, 1.1295854170868656, 9.38976137606976], 'lossList': [0.0, -1.2838145685195923, 0.0, 7.245729991197586, 0.0, 0.0, 0.0], 'rewardMean': 0.780591575445258, 'totalEpisodes': 193, 'stepsPerEpisode': 127, 'rewardPerEpisode': 105.0199362315494, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.6655394059878932, 'errorList': [], 'lossList': [0.0, -1.2972977429628372, 0.0, 6.351050848960877, 0.0, 0.0, 0.0], 'rewardMean': 0.7757448836635404, 'totalEpisodes': 195, 'stepsPerEpisode': 291, 'rewardPerEpisode': 213.07493576194994
'totalSteps': 21760, 'rewardStep': 0.9315040322617885, 'errorList': [0.16465888192870326, 0.3326718260511933, 0.1044217836010309, 0.17196341534677512, 0.1942414619861571, 0.10035498122370108, 0.16401074717339822, 0.14714213595910064, 0.2321590619380707, 0.2589989395897076, 0.1959495550490041, 0.1481431233565966, 0.20404882197585494, 0.16745626784479678, 0.2972078857011583, 0.17723885326401073, 0.1211937792230836, 0.2587555202401448, 0.17334293776558385, 0.1019323630241079, 0.1201146632792718, 0.16219702410880027, 0.17159842478202114, 0.2505766053427794, 0.24491099703874109, 0.19966399133402854, 0.2586650508315779, 0.13933695651405018, 0.1373206532149614, 0.22599375049983114, 0.18248251242854474, 0.3217155726846985, 0.2226968549808069, 0.13618581153672807, 0.19522956341039827, 0.19062530325831936, 0.23860418880612225, 0.19991856346816508, 0.24988220071780184, 0.2291862364034019, 0.10353549359166107, 0.2876661362867571, 0.21544884862988326, 0.21623449224942087, 0.19774884614298313, 0.12208874931450052, 0.2254102654019808, 0.22119935201135668, 0.334460220268093, 0.16622145357376625], 'lossList': [0.0, -1.297855526804924, 0.0, 6.352147722244263, 0.0, 0.0, 0.0], 'rewardMean': 0.7976813714233314, 'totalEpisodes': 196, 'stepsPerEpisode': 239, 'rewardPerEpisode': 214.5862525860324, 'successfulTests': 29
'totalSteps': 23040, 'rewardStep': 0.41941163496763034, 'errorList': [], 'lossList': [0.0, -1.28360773563385, 0.0, 4.572686973810196, 0.0, 0.0, 0.0], 'rewardMean': 0.7517749902921984, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 802.7902017630206
'totalSteps': 24320, 'rewardStep': 0.09736824202002603, 'errorList': [], 'lossList': [0.0, -1.253086222410202, 0.0, 6.896352593898773, 0.0, 0.0, 0.0], 'rewardMean': 0.6883923804191516, 'totalEpisodes': 197, 'stepsPerEpisode': 1263, 'rewardPerEpisode': 814.3398905879009
'totalSteps': 25600, 'rewardStep': 0.8703288745675758, 'errorList': [], 'lossList': [0.0, -1.235783839225769, 0.0, 2.5682303193211555, 0.0, 0.0, 0.0], 'rewardMean': 0.7174674428262369, 'totalEpisodes': 198, 'stepsPerEpisode': 1275, 'rewardPerEpisode': 1046.4277854654815
#maxSuccessfulTests=29, maxSuccessfulTestsAtStep=21760, timeSpent=88.57
