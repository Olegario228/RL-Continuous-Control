#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 130
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.897473108919548, 'errorList': [], 'lossList': [0.0, -1.3900445222854614, 0.0, 27.245075850486757, 0.0, 0.0, 0.0], 'rewardMean': 0.7540716039306993, 'totalEpisodes': 18, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.122629777126063
'totalSteps': 3840, 'rewardStep': 0.5280628444698968, 'errorList': [], 'lossList': [0.0, -1.3769402420520782, 0.0, 36.74107927799225, 0.0, 0.0, 0.0], 'rewardMean': 0.6787353507770985, 'totalEpisodes': 28, 'stepsPerEpisode': 153, 'rewardPerEpisode': 120.70889334284502
'totalSteps': 5120, 'rewardStep': 0.5638091890111738, 'errorList': [], 'lossList': [0.0, -1.381878712773323, 0.0, 34.07805894851685, 0.0, 0.0, 0.0], 'rewardMean': 0.6500038103356173, 'totalEpisodes': 32, 'stepsPerEpisode': 190, 'rewardPerEpisode': 130.088041897592
'totalSteps': 6400, 'rewardStep': 0.48063567141929336, 'errorList': [], 'lossList': [0.0, -1.3781428277492522, 0.0, 24.87415059566498, 0.0, 0.0, 0.0], 'rewardMean': 0.6161301825523525, 'totalEpisodes': 36, 'stepsPerEpisode': 560, 'rewardPerEpisode': 351.1514179738905
'totalSteps': 7680, 'rewardStep': 0.9570298612528931, 'errorList': [], 'lossList': [0.0, -1.3700966447591783, 0.0, 51.53637854576111, 0.0, 0.0, 0.0], 'rewardMean': 0.6729467956691093, 'totalEpisodes': 41, 'stepsPerEpisode': 290, 'rewardPerEpisode': 207.88411215795344
'totalSteps': 8960, 'rewardStep': 0.6911105844483086, 'errorList': [], 'lossList': [0.0, -1.3532812827825547, 0.0, 39.62071086883545, 0.0, 0.0, 0.0], 'rewardMean': 0.6755416226375663, 'totalEpisodes': 46, 'stepsPerEpisode': 269, 'rewardPerEpisode': 197.2402590018238
'totalSteps': 10240, 'rewardStep': 0.6611059165791382, 'errorList': [], 'lossList': [0.0, -1.359708028435707, 0.0, 192.58068546295166, 0.0, 0.0, 0.0], 'rewardMean': 0.6737371593802629, 'totalEpisodes': 66, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.991758861429843
'totalSteps': 11520, 'rewardStep': 0.6412659904657656, 'errorList': [], 'lossList': [0.0, -1.3506843250989915, 0.0, 224.9912459564209, 0.0, 0.0, 0.0], 'rewardMean': 0.6701292517230966, 'totalEpisodes': 112, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.5173326085670618
'totalSteps': 12800, 'rewardStep': 0.7101585977077054, 'errorList': [], 'lossList': [0.0, -1.3401422756910324, 0.0, 90.3646181678772, 0.0, 0.0, 0.0], 'rewardMean': 0.6741321863215575, 'totalEpisodes': 137, 'stepsPerEpisode': 46, 'rewardPerEpisode': 34.1058093386007
'totalSteps': 14080, 'rewardStep': 0.7129414777913297, 'errorList': [], 'lossList': [0.0, -1.3306812536716461, 0.0, 31.538153252601624, 0.0, 0.0, 0.0], 'rewardMean': 0.6843593242065054, 'totalEpisodes': 150, 'stepsPerEpisode': 63, 'rewardPerEpisode': 47.88209910902076
'totalSteps': 15360, 'rewardStep': 0.6838743598607402, 'errorList': [], 'lossList': [0.0, -1.3356011402606964, 0.0, 27.500129137039185, 0.0, 0.0, 0.0], 'rewardMean': 0.6629994493006246, 'totalEpisodes': 158, 'stepsPerEpisode': 71, 'rewardPerEpisode': 61.087346851015106
'totalSteps': 16640, 'rewardStep': 0.652675588462661, 'errorList': [], 'lossList': [0.0, -1.352927035689354, 0.0, 12.009971537590026, 0.0, 0.0, 0.0], 'rewardMean': 0.675460723699901, 'totalEpisodes': 160, 'stepsPerEpisode': 318, 'rewardPerEpisode': 226.0726263954192
'totalSteps': 17920, 'rewardStep': 0.5496728441943157, 'errorList': [], 'lossList': [0.0, -1.3711153781414032, 0.0, 9.801513520479203, 0.0, 0.0, 0.0], 'rewardMean': 0.6740470892182151, 'totalEpisodes': 165, 'stepsPerEpisode': 159, 'rewardPerEpisode': 90.1725862237372
'totalSteps': 19200, 'rewardStep': 0.7894140195009944, 'errorList': [], 'lossList': [0.0, -1.372675878405571, 0.0, 18.5284085214138, 0.0, 0.0, 0.0], 'rewardMean': 0.7049249240263852, 'totalEpisodes': 167, 'stepsPerEpisode': 819, 'rewardPerEpisode': 619.9794930693964
'totalSteps': 20480, 'rewardStep': 0.7985600157247909, 'errorList': [], 'lossList': [0.0, -1.3708355528116227, 0.0, 3.9786642727255823, 0.0, 0.0, 0.0], 'rewardMean': 0.6890779394735749, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1004.2538913558319
'totalSteps': 21760, 'rewardStep': 0.7730965489731727, 'errorList': [], 'lossList': [0.0, -1.3505026465654373, 0.0, 3.5662495109438894, 0.0, 0.0, 0.0], 'rewardMean': 0.6972765359260614, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 937.5074968880007
'totalSteps': 23040, 'rewardStep': 0.7223370018393338, 'errorList': [], 'lossList': [0.0, -1.328943293094635, 0.0, 1.5514505848288536, 0.0, 0.0, 0.0], 'rewardMean': 0.7033996444520809, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.1361307867639
'totalSteps': 24320, 'rewardStep': 0.6029971617990906, 'errorList': [], 'lossList': [0.0, -1.2944628030061722, 0.0, 0.856902820840478, 0.0, 0.0, 0.0], 'rewardMean': 0.6995727615854135, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1014.3064710370805
'totalSteps': 25600, 'rewardStep': 0.9373597315462137, 'errorList': [0.2539781657845048, 0.19588226429688146, 0.22351949869137147, 0.18939797653202817, 0.22246248638547578, 0.239486942651244, 0.17572921134623914, 0.21220543914809703, 0.20775190957655137, 0.2087707592195576, 0.22692946544565623, 0.20227188431987814, 0.22542813354898822, 0.17345229133687248, 0.16713219111118108, 0.18120407297482516, 0.31257505116290274, 0.18314219065092072, 0.2123655759302353, 0.20462500459025798, 0.1972534313130638, 0.19098980350130024, 0.19466691370466804, 0.1961317947393054, 0.26691579163862184, 0.23450063322337786, 0.29907264018424246, 0.2232999877159025, 0.27179289267769624, 0.1948913791138721, 0.18644611212589046, 0.20283138937348127, 0.19374705254992378, 0.2114154343203661, 0.22679363202753994, 0.16008039417982028, 0.2304606196938626, 0.20610437182792257, 0.2023208078075956, 0.19017310627879117, 0.171342858477789, 0.1790928252260408, 0.20728707838578736, 0.15383921142834137, 0.20288053764329902, 0.20125364294019846, 0.19396147563839047, 0.18090971854081833, 0.20917286043230435, 0.2070141157505766], 'lossList': [0.0, -1.2416143149137497, 0.0, 2.236732806004584, 0.0, 0.0, 0.0], 'rewardMean': 0.7222928749692643, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.9473148788477, 'successfulTests': 21
#maxSuccessfulTests=21, maxSuccessfulTestsAtStep=25600, timeSpent=82.32
