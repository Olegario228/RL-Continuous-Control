#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 131
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.5467444236589825, 'errorList': [], 'lossList': [0.0, -1.4178475469350815, 0.0, 26.071513175964355, 0.0, 0.0, 0.0], 'rewardMean': 0.755016987784846, 'totalEpisodes': 17, 'stepsPerEpisode': 358, 'rewardPerEpisode': 239.30295313058872
'totalSteps': 3840, 'rewardStep': 0.4997730805916251, 'errorList': [], 'lossList': [0.0, -1.4158162701129913, 0.0, 27.857067584991455, 0.0, 0.0, 0.0], 'rewardMean': 0.6699356853871058, 'totalEpisodes': 25, 'stepsPerEpisode': 195, 'rewardPerEpisode': 136.41182398161934
'totalSteps': 5120, 'rewardStep': 0.7378315362709791, 'errorList': [], 'lossList': [0.0, -1.4100317168235779, 0.0, 33.43849148273468, 0.0, 0.0, 0.0], 'rewardMean': 0.6869096481080741, 'totalEpisodes': 34, 'stepsPerEpisode': 52, 'rewardPerEpisode': 39.26159176489319
'totalSteps': 6400, 'rewardStep': 0.743956286326126, 'errorList': [], 'lossList': [0.0, -1.3942165344953537, 0.0, 54.919980926513674, 0.0, 0.0, 0.0], 'rewardMean': 0.6983189757516846, 'totalEpisodes': 47, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.9565539758622
'totalSteps': 7680, 'rewardStep': 0.7250193341017052, 'errorList': [], 'lossList': [0.0, -1.3655004221200944, 0.0, 28.13663419008255, 0.0, 0.0, 0.0], 'rewardMean': 0.7027690354766879, 'totalEpisodes': 55, 'stepsPerEpisode': 50, 'rewardPerEpisode': 35.76802396252349
'totalSteps': 8960, 'rewardStep': 0.7948417016188675, 'errorList': [], 'lossList': [0.0, -1.3286999171972276, 0.0, 77.81638998031616, 0.0, 0.0, 0.0], 'rewardMean': 0.7159222734969993, 'totalEpisodes': 65, 'stepsPerEpisode': 55, 'rewardPerEpisode': 45.023593570884856
'totalSteps': 10240, 'rewardStep': 0.6157105794381756, 'errorList': [], 'lossList': [0.0, -1.3220379906892776, 0.0, 54.28413533687591, 0.0, 0.0, 0.0], 'rewardMean': 0.7033958117396464, 'totalEpisodes': 70, 'stepsPerEpisode': 381, 'rewardPerEpisode': 233.50508872364824
'totalSteps': 11520, 'rewardStep': 0.7181877889667065, 'errorList': [], 'lossList': [0.0, -1.317631910443306, 0.0, 89.72589965820312, 0.0, 0.0, 0.0], 'rewardMean': 0.7050393647648753, 'totalEpisodes': 83, 'stepsPerEpisode': 63, 'rewardPerEpisode': 42.6122162475738
'totalSteps': 12800, 'rewardStep': 0.5948312531986533, 'errorList': [], 'lossList': [0.0, -1.3001998603343963, 0.0, 36.2404919052124, 0.0, 0.0, 0.0], 'rewardMean': 0.694018553608253, 'totalEpisodes': 90, 'stepsPerEpisode': 138, 'rewardPerEpisode': 93.95619215206021
'totalSteps': 14080, 'rewardStep': 0.6965203986780029, 'errorList': [], 'lossList': [0.0, -1.290432276725769, 0.0, 32.74062304496765, 0.0, 0.0, 0.0], 'rewardMean': 0.6673416382849824, 'totalEpisodes': 94, 'stepsPerEpisode': 230, 'rewardPerEpisode': 176.99296611184786
'totalSteps': 15360, 'rewardStep': 0.8544753404088171, 'errorList': [], 'lossList': [0.0, -1.2831210839748382, 0.0, 7.8193584698438645, 0.0, 0.0, 0.0], 'rewardMean': 0.6981147299599659, 'totalEpisodes': 96, 'stepsPerEpisode': 517, 'rewardPerEpisode': 411.8307629778602
'totalSteps': 16640, 'rewardStep': 0.44027106422898443, 'errorList': [], 'lossList': [0.0, -1.2773772168159485, 0.0, 4.892092565894127, 0.0, 0.0, 0.0], 'rewardMean': 0.6921645283237018, 'totalEpisodes': 99, 'stepsPerEpisode': 263, 'rewardPerEpisode': 173.91481198867123
'totalSteps': 17920, 'rewardStep': 0.9056271327293098, 'errorList': [], 'lossList': [0.0, -1.2587938678264619, 0.0, 5.791296959668398, 0.0, 0.0, 0.0], 'rewardMean': 0.7089440879695348, 'totalEpisodes': 100, 'stepsPerEpisode': 1269, 'rewardPerEpisode': 1073.363429591757
'totalSteps': 19200, 'rewardStep': 0.6919255575641717, 'errorList': [], 'lossList': [0.0, -1.2421515810489654, 0.0, 3.4802600434422493, 0.0, 0.0, 0.0], 'rewardMean': 0.7037410150933395, 'totalEpisodes': 100, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.2473411038502
'totalSteps': 20480, 'rewardStep': 0.8296759106362912, 'errorList': [], 'lossList': [0.0, -1.2125691950321198, 0.0, 3.126581676006317, 0.0, 0.0, 0.0], 'rewardMean': 0.714206672746798, 'totalEpisodes': 100, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 976.125685247574
'totalSteps': 21760, 'rewardStep': 0.7746712776777005, 'errorList': [], 'lossList': [0.0, -1.1903341913223267, 0.0, 1.5102690764516593, 0.0, 0.0, 0.0], 'rewardMean': 0.7121896303526813, 'totalEpisodes': 100, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.6393823275414
'totalSteps': 23040, 'rewardStep': 0.9587190010025711, 'errorList': [0.11062272786977256, 0.1074042108379347, 0.08363415224291686, 0.11545728282593189, 0.0903930140432324, 0.08726984726296351, 0.10048714826494888, 0.11123776051294479, 0.14150222157285988, 0.10799227737031229, 0.08379162805604115, 0.12315051312272518, 0.12701372191868307, 0.11952925478603825, 0.10401447394955976, 0.11240346258020825, 0.10603820182791232, 0.14174418808317985, 0.1006114468443577, 0.13309296197307052, 0.09996970691204064, 0.0847878839420245, 0.08978485626897864, 0.12021062179559118, 0.12520426813933938, 0.08103720941067212, 0.11570131271546323, 0.10806172430751419, 0.09381222623830401, 0.10429376696153912, 0.0950175148793229, 0.12241378276135319, 0.11041175855149211, 0.1195908237592915, 0.08600456919695817, 0.09815400892753526, 0.12886009870443565, 0.1293404013448857, 0.12413234949775302, 0.09180132230581632, 0.13364562498498114, 0.11202536648821726, 0.11399984848891627, 0.10429889323935856, 0.10435754131327028, 0.09145606581800285, 0.10430487647758425, 0.1254283697382368, 0.11423542582198566, 0.09851244661698325], 'lossList': [0.0, -1.1752270942926406, 0.0, 1.1268009761348368, 0.0, 0.0, 0.0], 'rewardMean': 0.7464904725091209, 'totalEpisodes': 100, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.5084766324178, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=76.65
