#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 133
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.6741400649811418, 'errorList': [], 'lossList': [0.0, -1.4097743785381318, 0.0, 23.47638499855995, 0.0, 0.0, 0.0], 'rewardMean': 0.5092476282632983, 'totalEpisodes': 15, 'stepsPerEpisode': 25, 'rewardPerEpisode': 17.742924283134137
'totalSteps': 3840, 'rewardStep': 0.642467445984001, 'errorList': [], 'lossList': [0.0, -1.3857791376113893, 0.0, 25.76424515247345, 0.0, 0.0, 0.0], 'rewardMean': 0.5536542341701992, 'totalEpisodes': 22, 'stepsPerEpisode': 182, 'rewardPerEpisode': 129.002784619896
'totalSteps': 5120, 'rewardStep': 0.6650574434974558, 'errorList': [], 'lossList': [0.0, -1.3789672523736953, 0.0, 38.01592472076416, 0.0, 0.0, 0.0], 'rewardMean': 0.5815050365020134, 'totalEpisodes': 27, 'stepsPerEpisode': 76, 'rewardPerEpisode': 60.05274949272314
'totalSteps': 6400, 'rewardStep': 0.779858225043596, 'errorList': [], 'lossList': [0.0, -1.3715628999471665, 0.0, 42.50634642601013, 0.0, 0.0, 0.0], 'rewardMean': 0.6211756742103299, 'totalEpisodes': 33, 'stepsPerEpisode': 27, 'rewardPerEpisode': 19.91904580022502
'totalSteps': 7680, 'rewardStep': 0.9237960266492007, 'errorList': [], 'lossList': [0.0, -1.359391108751297, 0.0, 49.599019446372985, 0.0, 0.0, 0.0], 'rewardMean': 0.6716123996168083, 'totalEpisodes': 41, 'stepsPerEpisode': 53, 'rewardPerEpisode': 44.02188310954101
'totalSteps': 8960, 'rewardStep': 0.933446830820904, 'errorList': [], 'lossList': [0.0, -1.3541034805774688, 0.0, 43.43874458312988, 0.0, 0.0, 0.0], 'rewardMean': 0.7090173183602505, 'totalEpisodes': 47, 'stepsPerEpisode': 40, 'rewardPerEpisode': 34.57321952976808
'totalSteps': 10240, 'rewardStep': 0.7061484960263009, 'errorList': [], 'lossList': [0.0, -1.3445988655090333, 0.0, 101.10003519058228, 0.0, 0.0, 0.0], 'rewardMean': 0.7086587155685069, 'totalEpisodes': 57, 'stepsPerEpisode': 161, 'rewardPerEpisode': 130.78919445937044
'totalSteps': 11520, 'rewardStep': 0.5572268789711555, 'errorList': [], 'lossList': [0.0, -1.349087309241295, 0.0, 109.96434997558593, 0.0, 0.0, 0.0], 'rewardMean': 0.691832955946579, 'totalEpisodes': 75, 'stepsPerEpisode': 62, 'rewardPerEpisode': 50.47735187226923
'totalSteps': 12800, 'rewardStep': 0.5265077400332779, 'errorList': [], 'lossList': [0.0, -1.366430566906929, 0.0, 32.17188051223755, 0.0, 0.0, 0.0], 'rewardMean': 0.6753004343552489, 'totalEpisodes': 86, 'stepsPerEpisode': 60, 'rewardPerEpisode': 47.93141206325327
'totalSteps': 14080, 'rewardStep': 0.7456955297451663, 'errorList': [], 'lossList': [0.0, -1.3680446249246598, 0.0, 22.17911294221878, 0.0, 0.0, 0.0], 'rewardMean': 0.7154344681752199, 'totalEpisodes': 92, 'stepsPerEpisode': 248, 'rewardPerEpisode': 183.64659576487156
'totalSteps': 15360, 'rewardStep': 0.6515129674647836, 'errorList': [], 'lossList': [0.0, -1.3500462007522582, 0.0, 7.4884594893455505, 0.0, 0.0, 0.0], 'rewardMean': 0.7131717584235842, 'totalEpisodes': 94, 'stepsPerEpisode': 362, 'rewardPerEpisode': 297.6548524500593
'totalSteps': 16640, 'rewardStep': 0.8391985404512161, 'errorList': [], 'lossList': [0.0, -1.3225960838794708, 0.0, 4.8711066991090775, 0.0, 0.0, 0.0], 'rewardMean': 0.7328448678703057, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 929.9077885261365
'totalSteps': 17920, 'rewardStep': 0.8993908170484283, 'errorList': [], 'lossList': [0.0, -1.272617164850235, 0.0, 4.459171828776598, 0.0, 0.0, 0.0], 'rewardMean': 0.7562782052254029, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.7262420484508
'totalSteps': 19200, 'rewardStep': 0.9238962362898921, 'errorList': [], 'lossList': [0.0, -1.2315554070472716, 0.0, 3.7575208684802055, 0.0, 0.0, 0.0], 'rewardMean': 0.7706820063500326, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.7206051322155
'totalSteps': 20480, 'rewardStep': 0.9219682880413764, 'errorList': [], 'lossList': [0.0, -1.1973164319992065, 0.0, 3.187050076201558, 0.0, 0.0, 0.0], 'rewardMean': 0.77049923248925, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.0233552953473
'totalSteps': 21760, 'rewardStep': 0.9478121503308604, 'errorList': [0.03080858398289468, 0.04903323016211838, 0.10187138349991792, 0.031650079407820116, 0.05216025823922595, 0.028052447264759472, 0.0451043772703192, 0.06687957047155682, 0.06246975245851654, 0.05239113088027993, 0.03807051669443527, 0.041302094586941084, 0.052368357814081945, 0.05858474189107566, 0.044390028521627996, 0.067262470397738, 0.061920521696050716, 0.047522210522883496, 0.029898772629008887, 0.04687955052371524, 0.07916258568052509, 0.03235287706403113, 0.06821428773807972, 0.0807722866587673, 0.06405404190374688, 0.048305849187957675, 0.037887645633656035, 0.08337452586332947, 0.08124524166076959, 0.08323685735078638, 0.11642632147666615, 0.06278860777883838, 0.054854404209448565, 0.0978849347869553, 0.09138468181677162, 0.06052217109816344, 0.08207168731260765, 0.04438583283900623, 0.04438643233734783, 0.0793686292387357, 0.044716435242925894, 0.08291968475417594, 0.0684945635235597, 0.03600267337912409, 0.06394446363535762, 0.04334640934105874, 0.0758235981925148, 0.03662656580257433, 0.037632568486541874, 0.08968554906858509], 'lossList': [0.0, -1.1634337145090103, 0.0, 2.290889390669763, 0.0, 0.0, 0.0], 'rewardMean': 0.7719357644402457, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.9249321419695, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=73.81
