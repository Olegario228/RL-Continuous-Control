#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 134
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8215192761319708, 'errorList': [], 'lossList': [0.0, -1.4111908209323882, 0.0, 21.22199315786362, 0.0, 0.0, 0.0], 'rewardMean': 0.5927946909537798, 'totalEpisodes': 22, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.773507765501098
'totalSteps': 3840, 'rewardStep': 0.6638661801361244, 'errorList': [], 'lossList': [0.0, -1.414671641588211, 0.0, 33.155648965835574, 0.0, 0.0, 0.0], 'rewardMean': 0.6164851873478946, 'totalEpisodes': 36, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.37602742043462
'totalSteps': 5120, 'rewardStep': 0.32087678491202665, 'errorList': [], 'lossList': [0.0, -1.399910283088684, 0.0, 31.92934642314911, 0.0, 0.0, 0.0], 'rewardMean': 0.5425830867389276, 'totalEpisodes': 46, 'stepsPerEpisode': 237, 'rewardPerEpisode': 155.93452257661716
'totalSteps': 6400, 'rewardStep': 0.681614777812235, 'errorList': [], 'lossList': [0.0, -1.3812391543388367, 0.0, 34.078035969734195, 0.0, 0.0, 0.0], 'rewardMean': 0.570389424953589, 'totalEpisodes': 52, 'stepsPerEpisode': 348, 'rewardPerEpisode': 268.81166187516163
'totalSteps': 7680, 'rewardStep': 0.8226412436805428, 'errorList': [], 'lossList': [0.0, -1.3687312895059585, 0.0, 37.45940368175506, 0.0, 0.0, 0.0], 'rewardMean': 0.6124313947414147, 'totalEpisodes': 57, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.450203221168046
'totalSteps': 8960, 'rewardStep': 0.7429892657533239, 'errorList': [], 'lossList': [0.0, -1.3561833608150482, 0.0, 47.61967529296875, 0.0, 0.0, 0.0], 'rewardMean': 0.6310825191716875, 'totalEpisodes': 63, 'stepsPerEpisode': 344, 'rewardPerEpisode': 262.36306829639693
'totalSteps': 10240, 'rewardStep': 0.8320472706216757, 'errorList': [], 'lossList': [0.0, -1.3368910747766494, 0.0, 151.1371063232422, 0.0, 0.0, 0.0], 'rewardMean': 0.656203113102936, 'totalEpisodes': 79, 'stepsPerEpisode': 109, 'rewardPerEpisode': 79.59788290363417
'totalSteps': 11520, 'rewardStep': 0.7990993504616107, 'errorList': [], 'lossList': [0.0, -1.3270952200889587, 0.0, 66.24908393859863, 0.0, 0.0, 0.0], 'rewardMean': 0.6720804728094554, 'totalEpisodes': 94, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7990993504616107
'totalSteps': 12800, 'rewardStep': 0.7540128423257842, 'errorList': [], 'lossList': [0.0, -1.327758838534355, 0.0, 52.94107621192932, 0.0, 0.0, 0.0], 'rewardMean': 0.6802737097610884, 'totalEpisodes': 107, 'stepsPerEpisode': 126, 'rewardPerEpisode': 101.24121377502027
'totalSteps': 14080, 'rewardStep': 0.8830006767279273, 'errorList': [], 'lossList': [0.0, -1.3261416018009187, 0.0, 23.9522505569458, 0.0, 0.0, 0.0], 'rewardMean': 0.7321667668563222, 'totalEpisodes': 115, 'stepsPerEpisode': 28, 'rewardPerEpisode': 25.43756172488902
'totalSteps': 15360, 'rewardStep': 0.6968770955695706, 'errorList': [], 'lossList': [0.0, -1.313013516664505, 0.0, 19.378097915649413, 0.0, 0.0, 0.0], 'rewardMean': 0.7197025488000821, 'totalEpisodes': 120, 'stepsPerEpisode': 266, 'rewardPerEpisode': 210.33903371388251
'totalSteps': 16640, 'rewardStep': 0.41597683417763653, 'errorList': [], 'lossList': [0.0, -1.310533503293991, 0.0, 36.076509170532226, 0.0, 0.0, 0.0], 'rewardMean': 0.6949136142042334, 'totalEpisodes': 123, 'stepsPerEpisode': 446, 'rewardPerEpisode': 369.3247686425733
'totalSteps': 17920, 'rewardStep': 0.7713606060906921, 'errorList': [], 'lossList': [0.0, -1.302146034836769, 0.0, 13.656359431743622, 0.0, 0.0, 0.0], 'rewardMean': 0.7399619963220998, 'totalEpisodes': 125, 'stepsPerEpisode': 79, 'rewardPerEpisode': 63.96145263508427
'totalSteps': 19200, 'rewardStep': 0.650161897416027, 'errorList': [], 'lossList': [0.0, -1.267677872776985, 0.0, 4.934996784329415, 0.0, 0.0, 0.0], 'rewardMean': 0.736816708282479, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 922.0192687784424
'totalSteps': 20480, 'rewardStep': 0.7210375614490347, 'errorList': [], 'lossList': [0.0, -1.24090651512146, 0.0, 3.196977183818817, 0.0, 0.0, 0.0], 'rewardMean': 0.7266563400593283, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 892.0532727060967
'totalSteps': 21760, 'rewardStep': 0.8143003744713051, 'errorList': [], 'lossList': [0.0, -1.2307620471715928, 0.0, 2.291153377890587, 0.0, 0.0, 0.0], 'rewardMean': 0.7337874509311264, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.132829992891
'totalSteps': 23040, 'rewardStep': 0.8804499625469617, 'errorList': [], 'lossList': [0.0, -1.23198137819767, 0.0, 1.9082053844630718, 0.0, 0.0, 0.0], 'rewardMean': 0.738627720123655, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1084.9113700932367
'totalSteps': 24320, 'rewardStep': 0.8176218130044749, 'errorList': [], 'lossList': [0.0, -1.2183981770277024, 0.0, 1.1817490454390644, 0.0, 0.0, 0.0], 'rewardMean': 0.7404799663779414, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.9535542190456
'totalSteps': 25600, 'rewardStep': 0.9756157845077751, 'errorList': [0.031313706071401104, 0.01225641394060888, 0.03300420109072363, 0.05652173520233067, 0.015461037101481544, 0.048866896258105925, 0.008820270159135777, 0.04170733961695575, 0.02344409216589481, 0.04599517044317097, 0.03999424759498446, 0.06476168542828489, 0.00663499140636116, 0.042915340938763, 0.0322281820981019, 0.04064508184925097, 0.01900889372835469, 0.021954224772993084, 0.021544914606498246, 0.0144917399034985, 0.020980249330301147, 0.019934324335149387, 0.020171734660284137, 0.03263957754936964, 0.033604069412741674, 0.017364802031007542, 0.01931456413951793, 0.01705688493512606, 0.022815097685940866, 0.03197200308144875, 0.019416325095341604, 0.0109434386966299, 0.023474055856249457, 0.011437917831549774, 0.03103002281601571, 0.020329686489194, 0.010165504278949253, 0.03081243702244912, 0.007139416458810137, 0.007741479281976973, 0.017413502924004714, 0.0072192828061723, 0.018775713496113657, 0.027968970018815046, 0.007738883844455572, 0.020282715609476996, 0.03298723867471684, 0.019204934944867122, 0.03455662882851291, 0.015357544640413185], 'lossList': [0.0, -1.1890197402238847, 0.0, 1.1220724039897323, 0.0, 0.0, 0.0], 'rewardMean': 0.7626402605961407, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.1788269023298, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=85.19
