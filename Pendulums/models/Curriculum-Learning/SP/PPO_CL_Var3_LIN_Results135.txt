#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 135
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9067271857893978, 'errorList': [], 'lossList': [0.0, -1.418145790696144, 0.0, 27.45113987326622, 0.0, 0.0, 0.0], 'rewardMean': 0.8532630731441804, 'totalEpisodes': 9, 'stepsPerEpisode': 536, 'rewardPerEpisode': 374.9744062865932
'totalSteps': 3840, 'rewardStep': 0.5106278111293479, 'errorList': [], 'lossList': [0.0, -1.4300823271274568, 0.0, 37.42347328186035, 0.0, 0.0, 0.0], 'rewardMean': 0.7390513191392362, 'totalEpisodes': 13, 'stepsPerEpisode': 153, 'rewardPerEpisode': 116.86152936276437
'totalSteps': 5120, 'rewardStep': 0.74491895351063, 'errorList': [], 'lossList': [0.0, -1.4368975061178206, 0.0, 29.47656133532524, 0.0, 0.0, 0.0], 'rewardMean': 0.7405182277320845, 'totalEpisodes': 15, 'stepsPerEpisode': 469, 'rewardPerEpisode': 337.565249682118
'totalSteps': 6400, 'rewardStep': 0.737923484320315, 'errorList': [], 'lossList': [0.0, -1.4382157814502716, 0.0, 13.367510402202607, 0.0, 0.0, 0.0], 'rewardMean': 0.7399992790497306, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 913.4277749914494
'totalSteps': 7680, 'rewardStep': 0.8115453707844413, 'errorList': [], 'lossList': [0.0, -1.4240072733163833, 0.0, 21.871362532377244, 0.0, 0.0, 0.0], 'rewardMean': 0.7519236276721823, 'totalEpisodes': 16, 'stepsPerEpisode': 419, 'rewardPerEpisode': 315.20335658572725
'totalSteps': 8960, 'rewardStep': 0.725864209072302, 'errorList': [], 'lossList': [0.0, -1.4017711782455444, 0.0, 54.34711508750915, 0.0, 0.0, 0.0], 'rewardMean': 0.7482008535864851, 'totalEpisodes': 19, 'stepsPerEpisode': 267, 'rewardPerEpisode': 193.49088418731117
'totalSteps': 10240, 'rewardStep': 0.8065646055616487, 'errorList': [], 'lossList': [0.0, -1.4016361492872238, 0.0, 155.88598688125612, 0.0, 0.0, 0.0], 'rewardMean': 0.7554963225833806, 'totalEpisodes': 30, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.928737020665297
'totalSteps': 11520, 'rewardStep': 0.6598664581702218, 'errorList': [], 'lossList': [0.0, -1.4013340574502946, 0.0, 198.67651096343994, 0.0, 0.0, 0.0], 'rewardMean': 0.7448707820930296, 'totalEpisodes': 51, 'stepsPerEpisode': 51, 'rewardPerEpisode': 46.003108560106654
'totalSteps': 12800, 'rewardStep': 0.525777310325471, 'errorList': [], 'lossList': [0.0, -1.399881698489189, 0.0, 109.31487020492554, 0.0, 0.0, 0.0], 'rewardMean': 0.7229614349162737, 'totalEpisodes': 76, 'stepsPerEpisode': 48, 'rewardPerEpisode': 28.846567308272206
'totalSteps': 14080, 'rewardStep': 0.5677100946821787, 'errorList': [], 'lossList': [0.0, -1.397118999361992, 0.0, 38.066250762939454, 0.0, 0.0, 0.0], 'rewardMean': 0.6997525483345954, 'totalEpisodes': 85, 'stepsPerEpisode': 59, 'rewardPerEpisode': 36.06128865537171
'totalSteps': 15360, 'rewardStep': 0.715097730114933, 'errorList': [], 'lossList': [0.0, -1.3924347853660584, 0.0, 44.35857900619507, 0.0, 0.0, 0.0], 'rewardMean': 0.680589602767149, 'totalEpisodes': 93, 'stepsPerEpisode': 69, 'rewardPerEpisode': 61.66368342676912
'totalSteps': 16640, 'rewardStep': 0.7286860995987475, 'errorList': [], 'lossList': [0.0, -1.3888024139404296, 0.0, 14.279974546432495, 0.0, 0.0, 0.0], 'rewardMean': 0.7023954316140889, 'totalEpisodes': 97, 'stepsPerEpisode': 132, 'rewardPerEpisode': 101.1206628328099
'totalSteps': 17920, 'rewardStep': 0.8569574368043917, 'errorList': [], 'lossList': [0.0, -1.3950782066583634, 0.0, 7.955081820487976, 0.0, 0.0, 0.0], 'rewardMean': 0.7135992799434652, 'totalEpisodes': 102, 'stepsPerEpisode': 64, 'rewardPerEpisode': 52.381804053976566
'totalSteps': 19200, 'rewardStep': 0.4833608521064169, 'errorList': [], 'lossList': [0.0, -1.3919301015138625, 0.0, 26.992833857536315, 0.0, 0.0, 0.0], 'rewardMean': 0.6881430167220752, 'totalEpisodes': 106, 'stepsPerEpisode': 208, 'rewardPerEpisode': 148.96771394377396
'totalSteps': 20480, 'rewardStep': 0.941872446109157, 'errorList': [0.15860486147484285, 0.33583142920663483, 0.31328529156588797, 0.4788056816515646, 0.4577623836121235, 0.5023544626115877, 0.2856483354915519, 0.30410274813258, 0.31470944713820964, 0.11317202918227949, 0.41731792603271106, 0.24481907395787403, 0.12355464693577706, 0.16395558467735555, 0.2888583951041917, 0.16384018840042203, 0.5114738186402215, 0.13865276723761438, 0.44109055764857424, 0.12184964467292246, 0.08852484287449511, 0.3910974129876528, 0.10200130466631989, 0.2128256598123739, 0.24218558488651948, 0.33489509224562514, 0.4177663241341114, 0.34061389323884156, 0.14583266495229863, 0.1083790197348421, 0.2835658888729363, 0.14483994459271624, 0.4970782117226681, 0.175519093374403, 0.2939757900671579, 0.1177220897560937, 0.13027689337661724, 0.3286896997650497, 0.1158802281406913, 0.25659896146850963, 0.3225632944814542, 0.09903651333229428, 0.19338439479547645, 0.12515032977484142, 0.29746419627414444, 0.38038413171168506, 0.2850532902244688, 0.4248979989217464, 0.20825453510239397, 0.10894257168179504], 'lossList': [0.0, -1.3954206454753875, 0.0, 6.79841958463192, 0.0, 0.0, 0.0], 'rewardMean': 0.7011757242545469, 'totalEpisodes': 109, 'stepsPerEpisode': 334, 'rewardPerEpisode': 276.6278665881259, 'successfulTests': 20
'totalSteps': 21760, 'rewardStep': 0.9074293765887308, 'errorList': [], 'lossList': [0.0, -1.4020853871107102, 0.0, 19.014845950603487, 0.0, 0.0, 0.0], 'rewardMean': 0.7193322410061896, 'totalEpisodes': 110, 'stepsPerEpisode': 60, 'rewardPerEpisode': 54.08036493882163
'totalSteps': 23040, 'rewardStep': 0.3223631541103388, 'errorList': [], 'lossList': [0.0, -1.3857094252109527, 0.0, 8.09211674809456, 0.0, 0.0, 0.0], 'rewardMean': 0.6709120958610588, 'totalEpisodes': 113, 'stepsPerEpisode': 282, 'rewardPerEpisode': 191.46047795566463
'totalSteps': 24320, 'rewardStep': 0.6498850210427114, 'errorList': [], 'lossList': [0.0, -1.3649674844741821, 0.0, 3.9581394577026368, 0.0, 0.0, 0.0], 'rewardMean': 0.6699139521483076, 'totalEpisodes': 116, 'stepsPerEpisode': 322, 'rewardPerEpisode': 252.92772344760252
'totalSteps': 25600, 'rewardStep': 0.8263549557208564, 'errorList': [], 'lossList': [0.0, -1.3624907088279725, 0.0, 4.455240861177445, 0.0, 0.0, 0.0], 'rewardMean': 0.6999717166878463, 'totalEpisodes': 120, 'stepsPerEpisode': 68, 'rewardPerEpisode': 59.64274344969622
#maxSuccessfulTests=20, maxSuccessfulTestsAtStep=20480, timeSpent=80.76
