#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 136
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.8149672772370617, 'errorList': [], 'lossList': [0.0, -1.4476752936840058, 0.0, 31.965758662223816, 0.0, 0.0, 0.0], 'rewardMean': 0.6658805894682573, 'totalEpisodes': 12, 'stepsPerEpisode': 81, 'rewardPerEpisode': 68.02828081586378
'totalSteps': 3840, 'rewardStep': 0.9061955882016965, 'errorList': [], 'lossList': [0.0, -1.4775995296239852, 0.0, 38.28255052804947, 0.0, 0.0, 0.0], 'rewardMean': 0.7459855890460704, 'totalEpisodes': 16, 'stepsPerEpisode': 167, 'rewardPerEpisode': 126.17934055019234
'totalSteps': 5120, 'rewardStep': 0.8597876991440198, 'errorList': [], 'lossList': [0.0, -1.4698404383659363, 0.0, 31.859362349510192, 0.0, 0.0, 0.0], 'rewardMean': 0.7744361165705578, 'totalEpisodes': 19, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.88091090767672
'totalSteps': 6400, 'rewardStep': 0.5742792740980871, 'errorList': [], 'lossList': [0.0, -1.4529459768533706, 0.0, 32.7611331486702, 0.0, 0.0, 0.0], 'rewardMean': 0.7344047480760636, 'totalEpisodes': 21, 'stepsPerEpisode': 82, 'rewardPerEpisode': 66.79651665076162
'totalSteps': 7680, 'rewardStep': 0.7434070636549726, 'errorList': [], 'lossList': [0.0, -1.4140138173103332, 0.0, 11.925920047760009, 0.0, 0.0, 0.0], 'rewardMean': 0.7359051340058818, 'totalEpisodes': 21, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1021.847838726536
'totalSteps': 8960, 'rewardStep': 0.7551549891812115, 'errorList': [], 'lossList': [0.0, -1.3871174943447113, 0.0, 35.710445790290834, 0.0, 0.0, 0.0], 'rewardMean': 0.7386551133166431, 'totalEpisodes': 24, 'stepsPerEpisode': 111, 'rewardPerEpisode': 85.47527246872662
'totalSteps': 10240, 'rewardStep': 0.9312922304139157, 'errorList': [348.79620365214413, 347.1122365038434, 265.2602673974447, 299.70583219147903, 303.1413589197067, 317.33336576452757, 317.64521857662925, 298.87263971759404, 329.6216899120415, 316.6538723707833, 297.05999269689016, 335.1836280397739, 327.9977914659768, 333.29181230872234, 341.26201237495553, 323.1986019462885, 320.23062310224833, 285.79331306887497, 354.52742816872046, 301.6747758439178, 295.5529650564208, 326.9693880794192, 141.07638383728536, 304.0053626579279, 267.56077377235346, 307.88829322278656, 347.3005402727974, 354.961936643694, 329.04312998342687, 320.2729933212411, 311.74896372857194, 315.308698891372, 323.07107595738165, 328.275920238577, 323.22790619259655, 232.08314744154467, 322.4829083478535, 347.66606887806313, 294.46708070882414, 343.61371511767254, 327.33036202443844, 317.3476676943368, 355.92735194080575, 343.8537859241951, 180.43850864169224, 220.98459801046, 330.6093023314206, 252.86527453650748, 300.0175167907856, 312.5866108264836], 'lossList': [0.0, -1.3771938067674636, 0.0, 49.5156181716919, 0.0, 0.0, 0.0], 'rewardMean': 0.7627347529538022, 'totalEpisodes': 29, 'stepsPerEpisode': 96, 'rewardPerEpisode': 78.66451903548949, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8275468541863711, 'errorList': [], 'lossList': [0.0, -1.374397274851799, 0.0, 285.5964523696899, 0.0, 0.0, 0.0], 'rewardMean': 0.7699360975351988, 'totalEpisodes': 61, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.815861902661975
'totalSteps': 12800, 'rewardStep': 0.511803154575079, 'errorList': [], 'lossList': [0.0, -1.373354079723358, 0.0, 78.92933248519897, 0.0, 0.0, 0.0], 'rewardMean': 0.7441228032391868, 'totalEpisodes': 88, 'stepsPerEpisode': 52, 'rewardPerEpisode': 34.35266437481118
'totalSteps': 14080, 'rewardStep': 0.46085593187723545, 'errorList': [], 'lossList': [0.0, -1.3653636628389358, 0.0, 40.93169671058655, 0.0, 0.0, 0.0], 'rewardMean': 0.738529006256965, 'totalEpisodes': 110, 'stepsPerEpisode': 131, 'rewardPerEpisode': 104.36265634741578
'totalSteps': 15360, 'rewardStep': 0.6922924773849257, 'errorList': [], 'lossList': [0.0, -1.3567274928092956, 0.0, 25.0131041097641, 0.0, 0.0, 0.0], 'rewardMean': 0.7262615262717513, 'totalEpisodes': 124, 'stepsPerEpisode': 190, 'rewardPerEpisode': 156.792343370016
'totalSteps': 16640, 'rewardStep': 0.7879141552402242, 'errorList': [], 'lossList': [0.0, -1.3662890315055847, 0.0, 19.694771194458006, 0.0, 0.0, 0.0], 'rewardMean': 0.7144333829756041, 'totalEpisodes': 131, 'stepsPerEpisode': 70, 'rewardPerEpisode': 53.01827930240749
'totalSteps': 17920, 'rewardStep': 0.9091814197001689, 'errorList': [], 'lossList': [0.0, -1.3591153728961944, 0.0, 20.127582597732545, 0.0, 0.0, 0.0], 'rewardMean': 0.7193727550312191, 'totalEpisodes': 138, 'stepsPerEpisode': 36, 'rewardPerEpisode': 30.939608530656468
'totalSteps': 19200, 'rewardStep': 0.5607891376690196, 'errorList': [], 'lossList': [0.0, -1.3445661413669585, 0.0, 17.967640290260317, 0.0, 0.0, 0.0], 'rewardMean': 0.7180237413883124, 'totalEpisodes': 143, 'stepsPerEpisode': 190, 'rewardPerEpisode': 134.3200117963635
'totalSteps': 20480, 'rewardStep': 0.5490055988488554, 'errorList': [], 'lossList': [0.0, -1.3218614643812179, 0.0, 7.352813609838486, 0.0, 0.0, 0.0], 'rewardMean': 0.6985835949077006, 'totalEpisodes': 145, 'stepsPerEpisode': 1090, 'rewardPerEpisode': 812.0611760581842
'totalSteps': 21760, 'rewardStep': 0.7239159363909194, 'errorList': [], 'lossList': [0.0, -1.2894063401222229, 0.0, 6.019405125379563, 0.0, 0.0, 0.0], 'rewardMean': 0.6954596896286714, 'totalEpisodes': 146, 'stepsPerEpisode': 1251, 'rewardPerEpisode': 946.5487906904557
'totalSteps': 23040, 'rewardStep': 0.8339385538559314, 'errorList': [], 'lossList': [0.0, -1.2562519419193268, 0.0, 3.9223078310489656, 0.0, 0.0, 0.0], 'rewardMean': 0.685724321972873, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.7024352745912
'totalSteps': 24320, 'rewardStep': 0.6112473440687063, 'errorList': [], 'lossList': [0.0, -1.219706015586853, 0.0, 2.7916459959745406, 0.0, 0.0, 0.0], 'rewardMean': 0.6640943709611066, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.374708130427
'totalSteps': 25600, 'rewardStep': 0.9702580975987604, 'errorList': [0.02764072363148252, 0.03318256235273563, 0.05341677439544491, 0.027249525683571404, 0.03706151858581501, 0.04410326129706957, 0.05097404103255022, 0.0615355230482892, 0.05526193659879177, 0.04823832928112921, 0.033577607154659225, 0.050966355153852456, 0.05544018575858723, 0.04263066284987786, 0.04374305336391967, 0.05176115824585019, 0.030784431863961702, 0.035214510321775264, 0.08216319611270323, 0.03095531946835554, 0.06603861825948028, 0.043654585170767146, 0.04845060524776581, 0.047439501550876575, 0.044649166968080516, 0.05052748386025712, 0.07709205063541226, 0.07123294245419043, 0.04661247577075418, 0.03806558933048674, 0.04988579156868192, 0.027057846216300745, 0.031918515049614446, 0.04022420187178753, 0.07165087583091408, 0.047400264246123695, 0.06107786288683869, 0.032472112305009716, 0.059823141191071856, 0.04565448959353759, 0.0428128593888033, 0.04012843233345463, 0.047230768481886405, 0.07260204682044236, 0.05192597320170953, 0.03032045818799173, 0.04400487556068818, 0.05716536926081335, 0.0720329432659324, 0.03137761942077124], 'lossList': [0.0, -1.178158221244812, 0.0, 2.3563773107528685, 0.0, 0.0, 0.0], 'rewardMean': 0.7099398652634747, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.9559418188692, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=96.73
