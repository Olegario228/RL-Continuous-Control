#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 137
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.7744106151362641, 'errorList': [], 'lossList': [0.0, -1.4483446007966996, 0.0, 28.243732764720917, 0.0, 0.0, 0.0], 'rewardMean': 0.606220603671069, 'totalEpisodes': 13, 'stepsPerEpisode': 716, 'rewardPerEpisode': 481.8546051615863
'totalSteps': 3840, 'rewardStep': 0.9056059117797923, 'errorList': [], 'lossList': [0.0, -1.4582300704717637, 0.0, 31.23183867931366, 0.0, 0.0, 0.0], 'rewardMean': 0.7060157063739768, 'totalEpisodes': 18, 'stepsPerEpisode': 77, 'rewardPerEpisode': 56.23938436766119
'totalSteps': 5120, 'rewardStep': 0.44291029768434204, 'errorList': [], 'lossList': [0.0, -1.4347502565383912, 0.0, 24.450727734565735, 0.0, 0.0, 0.0], 'rewardMean': 0.6402393542015681, 'totalEpisodes': 23, 'stepsPerEpisode': 67, 'rewardPerEpisode': 38.92525512324072
'totalSteps': 6400, 'rewardStep': 0.9485381789191579, 'errorList': [], 'lossList': [0.0, -1.4263827359676362, 0.0, 39.327600865364076, 0.0, 0.0, 0.0], 'rewardMean': 0.7018991191450861, 'totalEpisodes': 27, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.8694426899436576
'totalSteps': 7680, 'rewardStep': 0.8893651948146563, 'errorList': [], 'lossList': [0.0, -1.4165696305036546, 0.0, 85.39228627204895, 0.0, 0.0, 0.0], 'rewardMean': 0.7331434650900145, 'totalEpisodes': 35, 'stepsPerEpisode': 56, 'rewardPerEpisode': 36.885346157508884
'totalSteps': 8960, 'rewardStep': 0.7423474340648389, 'errorList': [], 'lossList': [0.0, -1.4077806091308593, 0.0, 72.88323429107666, 0.0, 0.0, 0.0], 'rewardMean': 0.7344583178007037, 'totalEpisodes': 41, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.046418793881541
'totalSteps': 10240, 'rewardStep': 0.6716132043655294, 'errorList': [], 'lossList': [0.0, -1.3931112962961196, 0.0, 36.92646779537201, 0.0, 0.0, 0.0], 'rewardMean': 0.7266026786213069, 'totalEpisodes': 43, 'stepsPerEpisode': 303, 'rewardPerEpisode': 227.92965392574754
'totalSteps': 11520, 'rewardStep': 0.5834470019657041, 'errorList': [], 'lossList': [0.0, -1.3890491980314255, 0.0, 166.8031427001953, 0.0, 0.0, 0.0], 'rewardMean': 0.7106964923262399, 'totalEpisodes': 62, 'stepsPerEpisode': 92, 'rewardPerEpisode': 67.52390521993088
'totalSteps': 12800, 'rewardStep': 0.7860923166124114, 'errorList': [], 'lossList': [0.0, -1.390329309105873, 0.0, 63.1072186756134, 0.0, 0.0, 0.0], 'rewardMean': 0.718236074754857, 'totalEpisodes': 74, 'stepsPerEpisode': 145, 'rewardPerEpisode': 111.57356172754322
'totalSteps': 14080, 'rewardStep': 0.7300574987632753, 'errorList': [], 'lossList': [0.0, -1.386280883550644, 0.0, 9.333155957460404, 0.0, 0.0, 0.0], 'rewardMean': 0.7474387654105972, 'totalEpisodes': 78, 'stepsPerEpisode': 383, 'rewardPerEpisode': 231.147569112232
'totalSteps': 15360, 'rewardStep': 0.8543465310919011, 'errorList': [], 'lossList': [0.0, -1.378533238172531, 0.0, 30.722815771102905, 0.0, 0.0, 0.0], 'rewardMean': 0.7554323570061608, 'totalEpisodes': 83, 'stepsPerEpisode': 59, 'rewardPerEpisode': 45.88502425731381
'totalSteps': 16640, 'rewardStep': 0.5027527430057963, 'errorList': [], 'lossList': [0.0, -1.3772109538316726, 0.0, 31.532356617450713, 0.0, 0.0, 0.0], 'rewardMean': 0.7151470401287613, 'totalEpisodes': 86, 'stepsPerEpisode': 206, 'rewardPerEpisode': 129.3022024454494
'totalSteps': 17920, 'rewardStep': 0.7676920013380182, 'errorList': [], 'lossList': [0.0, -1.3582605534791947, 0.0, 4.153433318138123, 0.0, 0.0, 0.0], 'rewardMean': 0.7476252104941289, 'totalEpisodes': 86, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 951.1054233528398
'totalSteps': 19200, 'rewardStep': 0.8968707048996771, 'errorList': [], 'lossList': [0.0, -1.332882126569748, 0.0, 15.8846660810709, 0.0, 0.0, 0.0], 'rewardMean': 0.7424584630921808, 'totalEpisodes': 87, 'stepsPerEpisode': 759, 'rewardPerEpisode': 653.4775793196083
'totalSteps': 20480, 'rewardStep': 0.8692346548554368, 'errorList': [], 'lossList': [0.0, -1.2788505834341048, 0.0, 3.380453934520483, 0.0, 0.0, 0.0], 'rewardMean': 0.7404454090962589, 'totalEpisodes': 87, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.7414856562923
'totalSteps': 21760, 'rewardStep': 0.8789537715394385, 'errorList': [], 'lossList': [0.0, -1.2357058590650558, 0.0, 3.212593555524945, 0.0, 0.0, 0.0], 'rewardMean': 0.7541060428437187, 'totalEpisodes': 87, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1163.3693234144287
'totalSteps': 23040, 'rewardStep': 0.9886657675469399, 'errorList': [0.1220221910195104, 0.1373685551964836, 0.11696229156141615, 0.1164071935609558, 0.11106142214344654, 0.0802023676576826, 0.09375778817700181, 0.11817724135785809, 0.11418479124132377, 0.1400271074896992, 0.10061748013079962, 0.11433560313507204, 0.14063152888132238, 0.14158745184571675, 0.11581016938364536, 0.10995750673514466, 0.13666822296020478, 0.13088621469554432, 0.10239707861575142, 0.10949121640286083, 0.10057734439808892, 0.11292718279893892, 0.16334330780709977, 0.12098993991973121, 0.09612267805480856, 0.14866470892505604, 0.13877600857944283, 0.12516696708787364, 0.08086208047941708, 0.16403837599327822, 0.09917892773799011, 0.13248195690032835, 0.13860787252746437, 0.11593507915097039, 0.09525494521303081, 0.13011629862609522, 0.11366073204145682, 0.09312058752052305, 0.12247676472599131, 0.11854477596553076, 0.10782487643492805, 0.09194337295261401, 0.1113806399370836, 0.13326820364380382, 0.12061281587718702, 0.13079600959745338, 0.1262188175463593, 0.1305988256145616, 0.11811541614166614, 0.1132317775680588], 'lossList': [0.0, -1.2044815838336944, 0.0, 2.463976943269372, 0.0, 0.0, 0.0], 'rewardMean': 0.7858112991618599, 'totalEpisodes': 87, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.4396576157424, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=75.3
