#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 14
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.868980120139048, 'errorList': [], 'lossList': [0.0, -1.4152367252111435, 0.0, 27.839029903411866, 0.0, 0.0, 0.0], 'rewardMean': 0.7704477631411957, 'totalEpisodes': 16, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.807099613512218
'totalSteps': 3840, 'rewardStep': 0.6840977287456277, 'errorList': [], 'lossList': [0.0, -1.413981563448906, 0.0, 47.70031160354614, 0.0, 0.0, 0.0], 'rewardMean': 0.741664418342673, 'totalEpisodes': 32, 'stepsPerEpisode': 102, 'rewardPerEpisode': 73.92904650438926
'totalSteps': 5120, 'rewardStep': 0.7922041734437738, 'errorList': [], 'lossList': [0.0, -1.3993148064613343, 0.0, 65.3040739440918, 0.0, 0.0, 0.0], 'rewardMean': 0.7542993571179483, 'totalEpisodes': 53, 'stepsPerEpisode': 43, 'rewardPerEpisode': 33.27640825961369
'totalSteps': 6400, 'rewardStep': 0.7230312303006594, 'errorList': [], 'lossList': [0.0, -1.396868656873703, 0.0, 102.083899974823, 0.0, 0.0, 0.0], 'rewardMean': 0.7480457317544905, 'totalEpisodes': 104, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.837418331075352
'totalSteps': 7680, 'rewardStep': 0.6923202897189444, 'errorList': [], 'lossList': [0.0, -1.3981324619054794, 0.0, 62.29311756134033, 0.0, 0.0, 0.0], 'rewardMean': 0.7387581580818995, 'totalEpisodes': 142, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.2440986539056706
'totalSteps': 8960, 'rewardStep': 0.5565562995437591, 'errorList': [], 'lossList': [0.0, -1.396285051703453, 0.0, 45.5941987991333, 0.0, 0.0, 0.0], 'rewardMean': 0.7127293211478795, 'totalEpisodes': 162, 'stepsPerEpisode': 30, 'rewardPerEpisode': 21.11630213983221
'totalSteps': 10240, 'rewardStep': 0.8879597298239448, 'errorList': [], 'lossList': [0.0, -1.3838578432798385, 0.0, 33.784787254333494, 0.0, 0.0, 0.0], 'rewardMean': 0.7346331222323875, 'totalEpisodes': 171, 'stepsPerEpisode': 80, 'rewardPerEpisode': 64.01681735321563
'totalSteps': 11520, 'rewardStep': 0.915650888104763, 'errorList': [], 'lossList': [0.0, -1.370480381846428, 0.0, 36.71227700710297, 0.0, 0.0, 0.0], 'rewardMean': 0.7547462073293182, 'totalEpisodes': 174, 'stepsPerEpisode': 286, 'rewardPerEpisode': 228.7363772043456
'totalSteps': 12800, 'rewardStep': 0.8025789918970831, 'errorList': [], 'lossList': [0.0, -1.3671132212877273, 0.0, 27.84377678155899, 0.0, 0.0, 0.0], 'rewardMean': 0.7595294857860947, 'totalEpisodes': 178, 'stepsPerEpisode': 65, 'rewardPerEpisode': 52.193034646827684
'totalSteps': 14080, 'rewardStep': 0.9113614908069025, 'errorList': [], 'lossList': [0.0, -1.3445880216360093, 0.0, 11.290195634961128, 0.0, 0.0, 0.0], 'rewardMean': 0.7834740942524506, 'totalEpisodes': 179, 'stepsPerEpisode': 1111, 'rewardPerEpisode': 880.2209003423624
'totalSteps': 15360, 'rewardStep': 0.9149293675236327, 'errorList': [], 'lossList': [0.0, -1.3139978659152984, 0.0, 9.090998757481575, 0.0, 0.0, 0.0], 'rewardMean': 0.788069018990909, 'totalEpisodes': 180, 'stepsPerEpisode': 590, 'rewardPerEpisode': 502.7244317983294
'totalSteps': 16640, 'rewardStep': 0.5103015486413651, 'errorList': [], 'lossList': [0.0, -1.2997634780406953, 0.0, 6.494938650131226, 0.0, 0.0, 0.0], 'rewardMean': 0.7706894009804828, 'totalEpisodes': 181, 'stepsPerEpisode': 533, 'rewardPerEpisode': 427.53499945698184
'totalSteps': 17920, 'rewardStep': 0.5295066344709812, 'errorList': [], 'lossList': [0.0, -1.2867352664470673, 0.0, 6.070508927702904, 0.0, 0.0, 0.0], 'rewardMean': 0.7444196470832035, 'totalEpisodes': 182, 'stepsPerEpisode': 348, 'rewardPerEpisode': 272.3934077389636
'totalSteps': 19200, 'rewardStep': 0.7050963638961902, 'errorList': [], 'lossList': [0.0, -1.2877670961618424, 0.0, 1.742842753827572, 0.0, 0.0, 0.0], 'rewardMean': 0.7426261604427566, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.1165732289849
'totalSteps': 20480, 'rewardStep': 0.9300783578096373, 'errorList': [0.13132952459819036, 0.05729665737570518, 0.15185707140487054, 0.1615215460795984, 0.2555608243818492, 0.07689408350465718, 0.09957536083598159, 0.07432488009211384, 0.08690932070972947, 0.07628565170502277, 0.09849028206894282, 0.23731918409280606, 0.10066490853981447, 0.06789457601162452, 0.08220647023029909, 0.05100875034972618, 0.07794873977742778, 0.12152753495939045, 0.10997660804102542, 0.08898699950795089, 0.05342519305524295, 0.09393061765562076, 0.06211289293548402, 0.1681878037229146, 0.08849655999598258, 0.06482662738985825, 0.08183892793366726, 0.05849192946951483, 0.10754481033134765, 0.08445341493611772, 0.07554386293397458, 0.11900552949713218, 0.20033126368377158, 0.1567528785267158, 0.17444461947701934, 0.07692350850269637, 0.11742394892740411, 0.22243226704843616, 0.08072535532954192, 0.06983311527841257, 0.11723836858886277, 0.11129616254365528, 0.10367636605975676, 0.056401401677883864, 0.08402576277364172, 0.1009550803675288, 0.08629996638816288, 0.06867991089521505, 0.0845306867928817, 0.07760704753491841], 'lossList': [0.0, -1.2728957045078277, 0.0, 2.0426450406014918, 0.0, 0.0, 0.0], 'rewardMean': 0.7664019672518259, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.2477940590472, 'successfulTests': 46
'totalSteps': 21760, 'rewardStep': 0.8929083184752491, 'errorList': [], 'lossList': [0.0, -1.2494454312324523, 0.0, 1.2776905735209585, 0.0, 0.0, 0.0], 'rewardMean': 0.8000371691449748, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.6090608393135
'totalSteps': 23040, 'rewardStep': 0.8775580026649761, 'errorList': [], 'lossList': [0.0, -1.227132846713066, 0.0, 1.0265847839787603, 0.0, 0.0, 0.0], 'rewardMean': 0.7989969964290781, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.4336443010018
'totalSteps': 24320, 'rewardStep': 0.8921748314358552, 'errorList': [], 'lossList': [0.0, -1.1950674599409103, 0.0, 0.7858927183225751, 0.0, 0.0, 0.0], 'rewardMean': 0.7966493907621872, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.8239465724841
'totalSteps': 25600, 'rewardStep': 0.9543569820844475, 'errorList': [0.05565810452231735, 0.08326167943105436, 0.054731781586242034, 0.053193518719250654, 0.06289419220967322, 0.09444391556902361, 0.10045112119206676, 0.08237501478775647, 0.08797882593414263, 0.052415789713536845, 0.09272346586308404, 0.06303643993488921, 0.10636120189991422, 0.08189567613768836, 0.08406394513445631, 0.0900043500017112, 0.0621479505624798, 0.09652107735976086, 0.10315154931296239, 0.05900393264487664, 0.12434774462015882, 0.05559933657622451, 0.07287689440120604, 0.05051425139088123, 0.053446877416674414, 0.07266683500258779, 0.08670070934537873, 0.09980235361853412, 0.054036505153245235, 0.08661636968742753, 0.08195599978047116, 0.08656241266465772, 0.042477691598878015, 0.04533796096256963, 0.07402538104415882, 0.11643954028202524, 0.03581953008554429, 0.04140577632680802, 0.056218011000773296, 0.051858666093845686, 0.0920513572714278, 0.05215467497701621, 0.06299904466508327, 0.07306579906360472, 0.04731392305116639, 0.10543089348608947, 0.07405780693626408, 0.03768677025922803, 0.05163269060767452, 0.05247182954875639], 'lossList': [0.0, -1.1642617738246919, 0.0, 0.5552062297798693, 0.0, 0.0, 0.0], 'rewardMean': 0.8118271897809237, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.5612754040628, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=89.35
