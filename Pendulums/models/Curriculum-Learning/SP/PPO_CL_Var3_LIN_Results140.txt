#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 140
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9357331305206457, 'errorList': [], 'lossList': [0.0, -1.437845824956894, 0.0, 28.744283574819566, 0.0, 0.0, 0.0], 'rewardMean': 0.9156573608644063, 'totalEpisodes': 8, 'stepsPerEpisode': 531, 'rewardPerEpisode': 372.1430452135244
'totalSteps': 3840, 'rewardStep': 0.710947268168062, 'errorList': [], 'lossList': [0.0, -1.429390748143196, 0.0, 28.25629343509674, 0.0, 0.0, 0.0], 'rewardMean': 0.8474206632989582, 'totalEpisodes': 12, 'stepsPerEpisode': 259, 'rewardPerEpisode': 192.03465497165732
'totalSteps': 5120, 'rewardStep': 0.713494490025977, 'errorList': [], 'lossList': [0.0, -1.4269289726018906, 0.0, 30.439969055652618, 0.0, 0.0, 0.0], 'rewardMean': 0.8139391199807129, 'totalEpisodes': 14, 'stepsPerEpisode': 271, 'rewardPerEpisode': 214.17754612973087
'totalSteps': 6400, 'rewardStep': 0.9259704822522211, 'errorList': [], 'lossList': [0.0, -1.4121805733442307, 0.0, 18.93799739062786, 0.0, 0.0, 0.0], 'rewardMean': 0.8363453924350145, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 994.1453613363132
'totalSteps': 7680, 'rewardStep': 0.8389164150173152, 'errorList': [], 'lossList': [0.0, -1.386383171081543, 0.0, 49.89959632396698, 0.0, 0.0, 0.0], 'rewardMean': 0.8367738961987312, 'totalEpisodes': 17, 'stepsPerEpisode': 298, 'rewardPerEpisode': 231.6184291049563
'totalSteps': 8960, 'rewardStep': 0.6643694094729593, 'errorList': [], 'lossList': [0.0, -1.3624128979444503, 0.0, 54.19360918521881, 0.0, 0.0, 0.0], 'rewardMean': 0.8121446838093352, 'totalEpisodes': 20, 'stepsPerEpisode': 80, 'rewardPerEpisode': 71.02344353148305
'totalSteps': 10240, 'rewardStep': 0.9293296066049102, 'errorList': [], 'lossList': [0.0, -1.3613732182979583, 0.0, 161.17494594573975, 0.0, 0.0, 0.0], 'rewardMean': 0.8267927991587821, 'totalEpisodes': 32, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.310183898870463
'totalSteps': 11520, 'rewardStep': 0.695510784790672, 'errorList': [], 'lossList': [0.0, -1.3609598433971406, 0.0, 288.86958938598633, 0.0, 0.0, 0.0], 'rewardMean': 0.8122059086734366, 'totalEpisodes': 77, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.223872380189368
'totalSteps': 12800, 'rewardStep': 0.6232957292027885, 'errorList': [], 'lossList': [0.0, -1.3584559786319732, 0.0, 97.91882843017578, 0.0, 0.0, 0.0], 'rewardMean': 0.7933148907263717, 'totalEpisodes': 104, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.709085205020347
'totalSteps': 14080, 'rewardStep': 0.7417791026538733, 'errorList': [], 'lossList': [0.0, -1.3515608310699463, 0.0, 31.069867539405823, 0.0, 0.0, 0.0], 'rewardMean': 0.7779346418709425, 'totalEpisodes': 129, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.802889200924093
'totalSteps': 15360, 'rewardStep': 0.35946266512090796, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6851591350262533, 'totalEpisodes': 145, 'stepsPerEpisode': 69, 'rewardPerEpisode': 53.12527139101529
'totalSteps': 16640, 'rewardStep': 0.7400475327175895, 'errorList': [], 'lossList': [0.0, -1.3460902214050292, 0.0, 24.375671882629394, 0.0, 0.0, 0.0], 'rewardMean': 0.6878144392954145, 'totalEpisodes': 160, 'stepsPerEpisode': 29, 'rewardPerEpisode': 20.810485434187385
'totalSteps': 17920, 'rewardStep': 0.8097415058217274, 'errorList': [], 'lossList': [0.0, -1.3477664631605148, 0.0, 18.440361025333406, 0.0, 0.0, 0.0], 'rewardMean': 0.6761915416523652, 'totalEpisodes': 169, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.87299106825751
'totalSteps': 19200, 'rewardStep': 0.5888848815215401, 'errorList': [], 'lossList': [0.0, -1.344378120303154, 0.0, 18.467219994068145, 0.0, 0.0, 0.0], 'rewardMean': 0.6511883883027877, 'totalEpisodes': 177, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.773897336755943
'totalSteps': 20480, 'rewardStep': 0.7610982711694096, 'errorList': [], 'lossList': [0.0, -1.3303095245361327, 0.0, 13.876081290245056, 0.0, 0.0, 0.0], 'rewardMean': 0.6608612744724326, 'totalEpisodes': 182, 'stepsPerEpisode': 355, 'rewardPerEpisode': 259.66289359468254
'totalSteps': 21760, 'rewardStep': 0.9455655079701936, 'errorList': [0.48505437795259976, 0.5028936356289083, 0.4620058620697574, 0.46140307496032396, 0.49825565498042756, 0.4697059600801811, 0.4612505984951714, 0.5071085009941365, 0.45611348463103224, 0.4714109413769203, 0.46667918477041853, 0.4432068715346194, 0.4902902357314978, 0.4509409059039015, 0.47109486765512393, 0.4899469377297436, 0.46959333838843015, 0.456993290762969, 0.4518173963543678, 0.48059977156637984, 0.4651698961167716, 0.4638271609043859, 0.5118135845921946, 0.4587529057494602, 0.4632734217274131, 0.4633884103662988, 0.4654201731877585, 0.4660324737261414, 0.4683423990732157, 0.44814831347188433, 0.4881360718162446, 0.4823673139772374, 0.5307414485948158, 0.49899737242932624, 0.4529588294239649, 0.45388014390421155, 0.4974051666561765, 0.4760433597528147, 0.5017341448648779, 0.48959819267354554, 0.46846142091031745, 0.4726131333855356, 0.4736430073501928, 0.5280107543655372, 0.48332556773253244, 0.45868046696013287, 0.47936838011274513, 0.45472154798463416, 0.4717653036341154, 0.46668026973578436], 'lossList': [0.0, -1.3203826308250428, 0.0, 14.632938175201415, 0.0, 0.0, 0.0], 'rewardMean': 0.6624848646089609, 'totalEpisodes': 184, 'stepsPerEpisode': 59, 'rewardPerEpisode': 54.62090459548359, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.5518931847573547, 'errorList': [], 'lossList': [0.0, -1.2882238334417344, 0.0, 3.7125559461116793, 0.0, 0.0, 0.0], 'rewardMean': 0.6481231046056293, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 918.5263490667515
'totalSteps': 24320, 'rewardStep': 0.8946935676906956, 'errorList': [], 'lossList': [0.0, -1.2466713309288024, 0.0, 5.490784406363964, 0.0, 0.0, 0.0], 'rewardMean': 0.67526288845442, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1038.0513683122097
'totalSteps': 25600, 'rewardStep': 0.8218482128654238, 'errorList': [], 'lossList': [0.0, -1.220393795967102, 0.0, 3.6378576120734216, 0.0, 0.0, 0.0], 'rewardMean': 0.683269799475575, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.61831615123
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=75.77
