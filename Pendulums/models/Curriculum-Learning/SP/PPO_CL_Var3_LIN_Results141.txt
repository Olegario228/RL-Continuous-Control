#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 141
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7111892276157825, 'errorList': [], 'lossList': [0.0, -1.4404981279373168, 0.0, 34.80024647474289, 0.0, 0.0, 0.0], 'rewardMean': 0.6349184730460689, 'totalEpisodes': 12, 'stepsPerEpisode': 76, 'rewardPerEpisode': 63.06396056267789
'totalSteps': 3840, 'rewardStep': 0.8626974247137571, 'errorList': [], 'lossList': [0.0, -1.4471717423200607, 0.0, 28.22084134995937, 0.0, 0.0, 0.0], 'rewardMean': 0.7108447902686317, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 966.6384628083675
'totalSteps': 5120, 'rewardStep': 0.684382954117482, 'errorList': [], 'lossList': [0.0, -1.4315227049589156, 0.0, 24.082465841770173, 0.0, 0.0, 0.0], 'rewardMean': 0.7042293312308443, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.3195834673056
'totalSteps': 6400, 'rewardStep': 0.7648748165432594, 'errorList': [], 'lossList': [0.0, -1.4233648771047591, 0.0, 28.470683666467668, 0.0, 0.0, 0.0], 'rewardMean': 0.7163584282933273, 'totalEpisodes': 13, 'stepsPerEpisode': 303, 'rewardPerEpisode': 229.64296487504265
'totalSteps': 7680, 'rewardStep': 0.8193013232356362, 'errorList': [], 'lossList': [0.0, -1.402900412082672, 0.0, 16.59302206337452, 0.0, 0.0, 0.0], 'rewardMean': 0.7335155774503788, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.3783688166184
'totalSteps': 8960, 'rewardStep': 0.9388812010529248, 'errorList': [], 'lossList': [0.0, -1.3708136069774628, 0.0, 22.61244308769703, 0.0, 0.0, 0.0], 'rewardMean': 0.7628535236793139, 'totalEpisodes': 14, 'stepsPerEpisode': 449, 'rewardPerEpisode': 370.41336195499235
'totalSteps': 10240, 'rewardStep': 0.5725995104277919, 'errorList': [], 'lossList': [0.0, -1.3651420563459395, 0.0, 71.92053071022033, 0.0, 0.0, 0.0], 'rewardMean': 0.7390717720228737, 'totalEpisodes': 18, 'stepsPerEpisode': 56, 'rewardPerEpisode': 39.51175392766363
'totalSteps': 11520, 'rewardStep': 0.8760861212034633, 'errorList': [], 'lossList': [0.0, -1.3668471139669418, 0.0, 501.8841439819336, 0.0, 0.0, 0.0], 'rewardMean': 0.7542955885984948, 'totalEpisodes': 55, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.985787931736136
'totalSteps': 12800, 'rewardStep': 0.6839421648359183, 'errorList': [], 'lossList': [0.0, -1.3632965391874314, 0.0, 106.80437265396118, 0.0, 0.0, 0.0], 'rewardMean': 0.7472602462222371, 'totalEpisodes': 86, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.371101360994597
'totalSteps': 14080, 'rewardStep': 0.6208184230797618, 'errorList': [], 'lossList': [0.0, -1.3559915071725845, 0.0, 38.773712921142575, 0.0, 0.0, 0.0], 'rewardMean': 0.7534773166825778, 'totalEpisodes': 112, 'stepsPerEpisode': 23, 'rewardPerEpisode': 12.988328951625357
'totalSteps': 15360, 'rewardStep': 0.6629124612788586, 'errorList': [], 'lossList': [0.0, -1.349625813961029, 0.0, 25.07579020023346, 0.0, 0.0, 0.0], 'rewardMean': 0.7486496400488853, 'totalEpisodes': 131, 'stepsPerEpisode': 173, 'rewardPerEpisode': 148.45206110587563
'totalSteps': 16640, 'rewardStep': 0.7323031284670313, 'errorList': [], 'lossList': [0.0, -1.3333228665590287, 0.0, 28.155811052322388, 0.0, 0.0, 0.0], 'rewardMean': 0.7356102104242128, 'totalEpisodes': 146, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.671005317304953
'totalSteps': 17920, 'rewardStep': 0.6593433840557543, 'errorList': [], 'lossList': [0.0, -1.319877597093582, 0.0, 18.34699782371521, 0.0, 0.0, 0.0], 'rewardMean': 0.7331062534180399, 'totalEpisodes': 150, 'stepsPerEpisode': 320, 'rewardPerEpisode': 264.8588314969492
'totalSteps': 19200, 'rewardStep': 0.7672104289488166, 'errorList': [], 'lossList': [0.0, -1.3062561684846878, 0.0, 17.789304792881012, 0.0, 0.0, 0.0], 'rewardMean': 0.7333398146585957, 'totalEpisodes': 158, 'stepsPerEpisode': 72, 'rewardPerEpisode': 59.54525421714857
'totalSteps': 20480, 'rewardStep': 0.9236695158421245, 'errorList': [], 'lossList': [0.0, -1.3040717417001724, 0.0, 13.013578925132752, 0.0, 0.0, 0.0], 'rewardMean': 0.7437766339192445, 'totalEpisodes': 161, 'stepsPerEpisode': 189, 'rewardPerEpisode': 150.34456075230543
'totalSteps': 21760, 'rewardStep': 0.7748938254581044, 'errorList': [], 'lossList': [0.0, -1.3048991215229035, 0.0, 5.205341584384441, 0.0, 0.0, 0.0], 'rewardMean': 0.7273778963597625, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.755692850479
'totalSteps': 23040, 'rewardStep': 0.9482046454740252, 'errorList': [0.11373346554476327, 0.1730153900908891, 0.12431936386692306, 0.12704655062504136, 0.12774784642819298, 0.1501058466320939, 0.118878668468617, 0.13885592186300655, 0.12352750211823194, 0.1252707052713789, 0.13158440612514707, 0.12252011133693329, 0.13128038999283922, 0.11843123022811888, 0.101074999480379, 0.11091706377728268, 0.14983284875555372, 0.15067224233522858, 0.167480387507876, 0.11520426562765504, 0.10031504980143473, 0.13916787018858442, 0.13613191358760937, 0.13773291773233415, 0.10623544930326217, 0.16261795256397515, 0.12435183080408474, 0.15150816266604125, 0.10342369595327057, 0.10492785292132338, 0.1523044218575815, 0.14615416607828224, 0.1607810095766854, 0.1174735424022947, 0.15549337847987224, 0.11357432369243388, 0.1283438505022475, 0.09670197239488501, 0.12917711172618865, 0.15269473818815876, 0.1221738794063655, 0.11841217725157922, 0.13656234477668897, 0.10631550541724863, 0.13681278346107995, 0.1649605416055751, 0.16210489803455239, 0.10923674888218743, 0.14416438600151085, 0.14039917279535497], 'lossList': [0.0, -1.2909794557094574, 0.0, 4.099436866939068, 0.0, 0.0, 0.0], 'rewardMean': 0.7649384098643858, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.1724774156835, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=71.71
