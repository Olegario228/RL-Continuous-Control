#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 143
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5822347403413262, 'errorList': [], 'lossList': [0.0, -1.4216382390260696, 0.0, 27.82217511534691, 0.0, 0.0, 0.0], 'rewardMean': 0.7202234244484329, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 30.82382258490013
'totalSteps': 3840, 'rewardStep': 0.9603753532034296, 'errorList': [], 'lossList': [0.0, -1.4201339626312255, 0.0, 28.775810651779175, 0.0, 0.0, 0.0], 'rewardMean': 0.8002740673667651, 'totalEpisodes': 18, 'stepsPerEpisode': 488, 'rewardPerEpisode': 376.41073890279205
'totalSteps': 5120, 'rewardStep': 0.7616384394370392, 'errorList': [], 'lossList': [0.0, -1.4178638190031052, 0.0, 29.642890540361403, 0.0, 0.0, 0.0], 'rewardMean': 0.7906151603843337, 'totalEpisodes': 19, 'stepsPerEpisode': 180, 'rewardPerEpisode': 151.9040835524506
'totalSteps': 6400, 'rewardStep': 0.5928293191686598, 'errorList': [], 'lossList': [0.0, -1.4143984824419022, 0.0, 26.06218644499779, 0.0, 0.0, 0.0], 'rewardMean': 0.7510579921411988, 'totalEpisodes': 20, 'stepsPerEpisode': 756, 'rewardPerEpisode': 575.7193060685544
'totalSteps': 7680, 'rewardStep': 0.8007743460949839, 'errorList': [], 'lossList': [0.0, -1.3899401479959488, 0.0, 14.622989948391915, 0.0, 0.0, 0.0], 'rewardMean': 0.7593440511334965, 'totalEpisodes': 20, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.9770924578334
'totalSteps': 8960, 'rewardStep': 0.8122906543372621, 'errorList': [], 'lossList': [0.0, -1.369031821489334, 0.0, 25.63241441130638, 0.0, 0.0, 0.0], 'rewardMean': 0.7669078515911772, 'totalEpisodes': 21, 'stepsPerEpisode': 788, 'rewardPerEpisode': 602.8326615646206
'totalSteps': 10240, 'rewardStep': 0.7572280610918396, 'errorList': [], 'lossList': [0.0, -1.3566516131162643, 0.0, 44.896185035705564, 0.0, 0.0, 0.0], 'rewardMean': 0.76569787777876, 'totalEpisodes': 23, 'stepsPerEpisode': 410, 'rewardPerEpisode': 319.06545306098656
'totalSteps': 11520, 'rewardStep': 0.6503089611244701, 'errorList': [], 'lossList': [0.0, -1.3550585752725601, 0.0, 383.7422896575928, 0.0, 0.0, 0.0], 'rewardMean': 0.7528768870393944, 'totalEpisodes': 54, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.315973196920046
'totalSteps': 12800, 'rewardStep': 0.5113500157238632, 'errorList': [], 'lossList': [0.0, -1.3509077441692352, 0.0, 184.07767833709715, 0.0, 0.0, 0.0], 'rewardMean': 0.7287241999078413, 'totalEpisodes': 80, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.073637907996456
'totalSteps': 14080, 'rewardStep': 0.8788976672252016, 'errorList': [], 'lossList': [0.0, -1.351964511871338, 0.0, 93.31351552963257, 0.0, 0.0, 0.0], 'rewardMean': 0.7307927557748075, 'totalEpisodes': 101, 'stepsPerEpisode': 46, 'rewardPerEpisode': 38.56667859498539
'totalSteps': 15360, 'rewardStep': 0.7236842845936043, 'errorList': [], 'lossList': [0.0, -1.3643619960546494, 0.0, 71.04596292495728, 0.0, 0.0, 0.0], 'rewardMean': 0.7449377102000353, 'totalEpisodes': 118, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.053472509334448
'totalSteps': 16640, 'rewardStep': 0.5374927288116563, 'errorList': [], 'lossList': [0.0, -1.3629686003923416, 0.0, 17.94528416633606, 0.0, 0.0, 0.0], 'rewardMean': 0.702649447760858, 'totalEpisodes': 125, 'stepsPerEpisode': 176, 'rewardPerEpisode': 122.35896443234802
'totalSteps': 17920, 'rewardStep': 0.9254750027248047, 'errorList': [], 'lossList': [0.0, -1.3501791101694107, 0.0, 25.33841378927231, 0.0, 0.0, 0.0], 'rewardMean': 0.7190331040896345, 'totalEpisodes': 129, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.42498940795429
'totalSteps': 19200, 'rewardStep': 0.8337922690399946, 'errorList': [], 'lossList': [0.0, -1.3462428188323974, 0.0, 10.408322364091873, 0.0, 0.0, 0.0], 'rewardMean': 0.743129399076768, 'totalEpisodes': 131, 'stepsPerEpisode': 226, 'rewardPerEpisode': 167.62672433927494
'totalSteps': 20480, 'rewardStep': 0.6869521329939196, 'errorList': [], 'lossList': [0.0, -1.3221490716934203, 0.0, 7.4386550158262255, 0.0, 0.0, 0.0], 'rewardMean': 0.7317471777666616, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 903.3654124442459
'totalSteps': 21760, 'rewardStep': 0.8258432250929177, 'errorList': [], 'lossList': [0.0, -1.2884461641311646, 0.0, 4.833535781502723, 0.0, 0.0, 0.0], 'rewardMean': 0.7331024348422271, 'totalEpisodes': 135, 'stepsPerEpisode': 103, 'rewardPerEpisode': 91.6937113386623
'totalSteps': 23040, 'rewardStep': 0.610746451859403, 'errorList': [], 'lossList': [0.0, -1.288607752919197, 0.0, 5.655145120620728, 0.0, 0.0, 0.0], 'rewardMean': 0.7184542739189835, 'totalEpisodes': 139, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.386438290503057
'totalSteps': 24320, 'rewardStep': 0.1871017562510086, 'errorList': [], 'lossList': [0.0, -1.291730500459671, 0.0, 2.974088253080845, 0.0, 0.0, 0.0], 'rewardMean': 0.6721335534316374, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 946.5037523173986
'totalSteps': 25600, 'rewardStep': 0.9774645188037822, 'errorList': [0.09163717514348801, 0.11521100391467147, 0.019265034596994076, 0.057142900788618514, 0.018388218384647412, 0.03213473363647234, 0.05020375074629252, 0.015302474984952784, 0.020209217900346788, 0.08968799198282047, 0.0428521367267844, 0.08538708542228209, 0.05782864237556369, 0.0549231509559434, 0.062130431329570766, 0.016399979584177358, 0.02016082870679601, 0.054957551117868764, 0.09140128580722948, 0.023424557584192852, 0.06079076981324153, 0.013652059334912007, 0.014160881189691414, 0.04731660016281909, 0.01621982822971911, 0.012495787925065747, 0.0158391990355224, 0.015185359094502096, 0.08031535521391699, 0.02361936428459553, 0.07181567596729499, 0.06982947406618306, 0.01326789310779968, 0.023442841614159826, 0.019182813714719665, 0.014941399603143148, 0.036048487282085075, 0.02492948158946584, 0.05562264865367103, 0.042017546608934433, 0.07976640897472553, 0.04784277902332776, 0.04155897556931815, 0.033311136087607114, 0.06554543834218486, 0.02133299355862079, 0.015319768592088471, 0.08968961512340076, 0.025693184875355837, 0.015392659148999744], 'lossList': [0.0, -1.285784793496132, 0.0, 1.8603230233490466, 0.0, 0.0, 0.0], 'rewardMean': 0.7187450037396294, 'totalEpisodes': 140, 'stepsPerEpisode': 1275, 'rewardPerEpisode': 1068.868190351507, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=64.82
