#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 146
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.7617994116323085, 'errorList': [], 'lossList': [0.0, -1.445655573606491, 0.0, 37.896582646369936, 0.0, 0.0, 0.0], 'rewardMean': 0.6774886156062339, 'totalEpisodes': 12, 'stepsPerEpisode': 72, 'rewardPerEpisode': 62.04794214253207
'totalSteps': 3840, 'rewardStep': 0.8834641801128564, 'errorList': [], 'lossList': [0.0, -1.458834501504898, 0.0, 32.41871506869793, 0.0, 0.0, 0.0], 'rewardMean': 0.7461471371084413, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.9497193431037
'totalSteps': 5120, 'rewardStep': 0.6492239751716257, 'errorList': [], 'lossList': [0.0, -1.4536351954936981, 0.0, 23.225425819158556, 0.0, 0.0, 0.0], 'rewardMean': 0.7219163466242374, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 986.7443095043502
'totalSteps': 6400, 'rewardStep': 0.8300827026213132, 'errorList': [], 'lossList': [0.0, -1.4405016618967057, 0.0, 19.41743741095066, 0.0, 0.0, 0.0], 'rewardMean': 0.7435496178236526, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1031.3817460703247
'totalSteps': 7680, 'rewardStep': 0.8060311220933873, 'errorList': [], 'lossList': [0.0, -1.4142334455251693, 0.0, 15.364887966662645, 0.0, 0.0, 0.0], 'rewardMean': 0.7539632018686083, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.9996830750636
'totalSteps': 8960, 'rewardStep': 0.9872535983710993, 'errorList': [], 'lossList': [0.0, -1.3883786123991013, 0.0, 46.27417682409286, 0.0, 0.0, 0.0], 'rewardMean': 0.7872904013689642, 'totalEpisodes': 14, 'stepsPerEpisode': 445, 'rewardPerEpisode': 362.6766900452426
'totalSteps': 10240, 'rewardStep': 0.6706150911804444, 'errorList': [], 'lossList': [0.0, -1.3760902220010758, 0.0, 117.95016513824463, 0.0, 0.0, 0.0], 'rewardMean': 0.7727059875953992, 'totalEpisodes': 20, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.32199780286814
'totalSteps': 11520, 'rewardStep': 0.9489710953689421, 'errorList': [205.94709141541705, 243.36938339965116, 254.0300646827526, 212.880905071069, 221.784907908923, 218.22507592270264, 252.1224503926241, 240.25652905767646, 208.41892698227008, 232.2767179798981, 199.04136408890292, 248.04218876232503, 242.5967132550241, 214.46836489249225, 250.77408605601815, 205.38370746996875, 211.33103399047056, 195.9421646789562, 239.7149742532711, 219.4991433015902, 198.71506045168366, 229.0670545611133, 212.25356090116085, 218.15020819545518, 228.63875712497955, 235.33616951830678, 230.6458186349204, 252.26480713739687, 226.57403112556023, 227.89242876283328, 225.56780819125987, 242.90597904672742, 193.6123275614633, 246.20331848330792, 229.45355823706052, 231.31514467443, 249.90833298195943, 238.13069131327262, 225.18506395514902, 256.9781311264241, 244.94586227543397, 245.13031246172105, 241.69417207668985, 233.6060837527227, 253.7679703058227, 254.84236892785262, 242.7297134652815, 243.46935638800764, 229.97693446900533, 241.85028359217304], 'lossList': [0.0, -1.3734271216392517, 0.0, 402.22657012939453, 0.0, 0.0, 0.0], 'rewardMean': 0.7922909995702373, 'totalEpisodes': 57, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.393902139978067, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.9736113363579175, 'errorList': [279.9257748996956, 268.9248913693805, 265.1645141480409, 264.59430914844626, 237.60384706284327, 251.4647959489094, 275.0979688770875, 222.29032011002772, 286.3114605092744, 251.05333693425933, 252.92899493566068, 288.10467440826704, 275.94545579407685, 280.8896647366193, 272.38178383640474, 278.6627931238676, 225.93392822129903, 224.64829795360242, 284.66539150242494, 290.2109803186768, 268.4433868485967, 257.165444650591, 275.11124780491286, 276.16850750106863, 269.8178245518659, 252.46120350903666, 280.4886871221966, 276.9925009646377, 283.0125137098059, 285.7907630018267, 250.16739040661122, 275.0251127555569, 233.9369065382496, 262.7402464375814, 214.91883109306062, 244.87700832789292, 202.00684371760272, 279.6952851458679, 297.9396531562107, 249.8637752085174, 228.32176284354148, 247.6363370863252, 264.9284598879833, 280.0312102942761, 219.01506237120236, 246.69477408775668, 259.14993625630575, 291.3065407883813, 278.63599353827226, 266.08465311003073], 'lossList': [0.0, -1.3737916964292527, 0.0, 184.63748889923096, 0.0, 0.0, 0.0], 'rewardMean': 0.8104230332490054, 'totalEpisodes': 93, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.930403231515726, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.2503337100644468, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7249920521406479, 'totalEpisodes': 118, 'stepsPerEpisode': 101, 'rewardPerEpisode': 76.96934455179988
'totalSteps': 15360, 'rewardStep': 0.6582364080610301, 'errorList': [], 'lossList': [0.0, -1.376478505730629, 0.0, 92.36047489166259, 0.0, 0.0, 0.0], 'rewardMean': 0.7024692749354653, 'totalEpisodes': 143, 'stepsPerEpisode': 78, 'rewardPerEpisode': 59.08674921398691
'totalSteps': 16640, 'rewardStep': 0.39843750923125637, 'errorList': [], 'lossList': [0.0, -1.3839175242185593, 0.0, 81.8519341468811, 0.0, 0.0, 0.0], 'rewardMean': 0.6773906283414284, 'totalEpisodes': 164, 'stepsPerEpisode': 38, 'rewardPerEpisode': 23.027386979531883
'totalSteps': 17920, 'rewardStep': 0.2770841619027142, 'errorList': [], 'lossList': [0.0, -1.3855304062366485, 0.0, 38.950571327209474, 0.0, 0.0, 0.0], 'rewardMean': 0.6220907742695685, 'totalEpisodes': 176, 'stepsPerEpisode': 103, 'rewardPerEpisode': 67.1354647693201
'totalSteps': 19200, 'rewardStep': 0.8139606764325646, 'errorList': [], 'lossList': [0.0, -1.3746800011396407, 0.0, 21.281242861747742, 0.0, 0.0, 0.0], 'rewardMean': 0.6228837297034862, 'totalEpisodes': 182, 'stepsPerEpisode': 135, 'rewardPerEpisode': 113.18498313288538
'totalSteps': 20480, 'rewardStep': 0.802384904563661, 'errorList': [], 'lossList': [0.0, -1.365644130706787, 0.0, 14.752805114984513, 0.0, 0.0, 0.0], 'rewardMean': 0.6043968603227425, 'totalEpisodes': 187, 'stepsPerEpisode': 30, 'rewardPerEpisode': 27.11543545691173
'totalSteps': 21760, 'rewardStep': 0.8291083808190944, 'errorList': [], 'lossList': [0.0, -1.3696006393432618, 0.0, 10.198743625879288, 0.0, 0.0, 0.0], 'rewardMean': 0.6202461892866074, 'totalEpisodes': 192, 'stepsPerEpisode': 28, 'rewardPerEpisode': 20.370176499599367
'totalSteps': 23040, 'rewardStep': 0.802608118786088, 'errorList': [], 'lossList': [0.0, -1.3732623434066773, 0.0, 6.895966861844062, 0.0, 0.0, 0.0], 'rewardMean': 0.6056098916283219, 'totalEpisodes': 194, 'stepsPerEpisode': 182, 'rewardPerEpisode': 152.8501150858776
'totalSteps': 24320, 'rewardStep': 0.4091877173824194, 'errorList': [], 'lossList': [0.0, -1.3678719347715378, 0.0, 5.830389729738235, 0.0, 0.0, 0.0], 'rewardMean': 0.5491675297307721, 'totalEpisodes': 197, 'stepsPerEpisode': 333, 'rewardPerEpisode': 241.84708785684455
'totalSteps': 25600, 'rewardStep': 0.8205728031409609, 'errorList': [], 'lossList': [0.0, -1.3453654503822328, 0.0, 3.427261844277382, 0.0, 0.0, 0.0], 'rewardMean': 0.6061914390384235, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 945.247206334983
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=63.79
