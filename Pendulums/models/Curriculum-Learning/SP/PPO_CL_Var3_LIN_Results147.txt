#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 147
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8585695387971459, 'errorList': [], 'lossList': [0.0, -1.4519423204660415, 0.0, 32.44690713167191, 0.0, 0.0, 0.0], 'rewardMean': 0.6808351933505788, 'totalEpisodes': 12, 'stepsPerEpisode': 904, 'rewardPerEpisode': 653.1662925519266
'totalSteps': 3840, 'rewardStep': 0.8261492159893129, 'errorList': [], 'lossList': [0.0, -1.4746567982435226, 0.0, 43.7924992609024, 0.0, 0.0, 0.0], 'rewardMean': 0.7292732008968236, 'totalEpisodes': 15, 'stepsPerEpisode': 489, 'rewardPerEpisode': 400.9126000772568
'totalSteps': 5120, 'rewardStep': 0.7391374454166131, 'errorList': [], 'lossList': [0.0, -1.4710894721746444, 0.0, 19.837482506036757, 0.0, 0.0, 0.0], 'rewardMean': 0.7317392620267709, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.4085094919673
'totalSteps': 6400, 'rewardStep': 0.47147891251985574, 'errorList': [], 'lossList': [0.0, -1.4452628618478776, 0.0, 34.654396287202836, 0.0, 0.0, 0.0], 'rewardMean': 0.6796871921253878, 'totalEpisodes': 17, 'stepsPerEpisode': 559, 'rewardPerEpisode': 421.4968171295522
'totalSteps': 7680, 'rewardStep': 0.5985501288851207, 'errorList': [], 'lossList': [0.0, -1.4312032556533814, 0.0, 56.45133105278015, 0.0, 0.0, 0.0], 'rewardMean': 0.66616434825201, 'totalEpisodes': 21, 'stepsPerEpisode': 286, 'rewardPerEpisode': 218.97678583692732
'totalSteps': 8960, 'rewardStep': 0.8730555763568575, 'errorList': [], 'lossList': [0.0, -1.4293473732471467, 0.0, 82.00424000740051, 0.0, 0.0, 0.0], 'rewardMean': 0.6957202379812738, 'totalEpisodes': 27, 'stepsPerEpisode': 214, 'rewardPerEpisode': 159.53316170928693
'totalSteps': 10240, 'rewardStep': 0.7382071423245646, 'errorList': [], 'lossList': [0.0, -1.4278324669599534, 0.0, 45.98216989278794, 0.0, 0.0, 0.0], 'rewardMean': 0.7010311010241852, 'totalEpisodes': 31, 'stepsPerEpisode': 272, 'rewardPerEpisode': 213.6188623073287
'totalSteps': 11520, 'rewardStep': 0.8445584443708486, 'errorList': [], 'lossList': [0.0, -1.4341027188301085, 0.0, 347.12848022460935, 0.0, 0.0, 0.0], 'rewardMean': 0.716978583618259, 'totalEpisodes': 75, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.881731043914424
'totalSteps': 12800, 'rewardStep': 0.6848533420708564, 'errorList': [], 'lossList': [0.0, -1.4340917187929154, 0.0, 139.56183853149415, 0.0, 0.0, 0.0], 'rewardMean': 0.7137660594635187, 'totalEpisodes': 117, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.119817732704377
'totalSteps': 14080, 'rewardStep': 0.7293099712572982, 'errorList': [], 'lossList': [0.0, -1.425143923163414, 0.0, 47.83149871826172, 0.0, 0.0, 0.0], 'rewardMean': 0.7363869717988474, 'totalEpisodes': 145, 'stepsPerEpisode': 41, 'rewardPerEpisode': 34.00123184104698
'totalSteps': 15360, 'rewardStep': 0.9013720874714978, 'errorList': [], 'lossList': [0.0, -1.406350405216217, 0.0, 31.993331756591797, 0.0, 0.0, 0.0], 'rewardMean': 0.7406672266662826, 'totalEpisodes': 171, 'stepsPerEpisode': 39, 'rewardPerEpisode': 36.4410021426678
'totalSteps': 16640, 'rewardStep': 0.5831604777885998, 'errorList': [], 'lossList': [0.0, -1.3988551551103592, 0.0, 23.267533988952636, 0.0, 0.0, 0.0], 'rewardMean': 0.7163683528462113, 'totalEpisodes': 186, 'stepsPerEpisode': 45, 'rewardPerEpisode': 29.781136213386688
'totalSteps': 17920, 'rewardStep': 0.6609441502889788, 'errorList': [], 'lossList': [0.0, -1.3932694113254547, 0.0, 17.12986217737198, 0.0, 0.0, 0.0], 'rewardMean': 0.7085490233334479, 'totalEpisodes': 192, 'stepsPerEpisode': 93, 'rewardPerEpisode': 80.03387318476155
'totalSteps': 19200, 'rewardStep': 0.7053134914979564, 'errorList': [], 'lossList': [0.0, -1.3587954604625703, 0.0, 11.543435707092286, 0.0, 0.0, 0.0], 'rewardMean': 0.7319324812312579, 'totalEpisodes': 200, 'stepsPerEpisode': 78, 'rewardPerEpisode': 65.81463854218751
'totalSteps': 20480, 'rewardStep': 0.12785627911384057, 'errorList': [], 'lossList': [0.0, -1.3363660830259323, 0.0, 5.416851989626885, 0.0, 0.0, 0.0], 'rewardMean': 0.6848630962541299, 'totalEpisodes': 204, 'stepsPerEpisode': 155, 'rewardPerEpisode': 95.36108055130109
'totalSteps': 21760, 'rewardStep': 0.9515674746989311, 'errorList': [0.23280536883825384, 0.24288003289869714, 0.25333791697497976, 0.24966145520914543, 0.28334542906186794, 0.23748482808576707, 0.2412446059256555, 0.2360699093230892, 0.2551626886844375, 0.2597033600223965, 0.2384602323611675, 0.2181889798395399, 0.2463100554561846, 0.22108153103724035, 0.34122683841055496, 0.32051932111535436, 0.2640964819722554, 0.24553345675986016, 0.24650326365575195, 0.2518637820804368, 0.24921986801382182, 0.2514453996765742, 0.2903013587255831, 0.24655074097538204, 0.2501414305283478, 0.23538007905797304, 0.24819191994398296, 0.2392657577659718, 0.25972806308630453, 0.2338040107544093, 0.2457692242875501, 0.2522484249565023, 0.23600340152190408, 0.24463625455774538, 0.2554386565401993, 0.2361318079815528, 0.24184280903259175, 0.23847268591530443, 0.39836575182580947, 0.24715461982221618, 0.2803894235155408, 0.22652320712794147, 0.24190167910111351, 0.23223359049314124, 0.23933002814551646, 0.44371676570851015, 0.22249996342409145, 0.24659211299202277, 0.2486586880747168, 0.2387665897151886], 'lossList': [0.0, -1.3333811408281326, 0.0, 8.763072147369385, 0.0, 0.0, 0.0], 'rewardMean': 0.6927142860883373, 'totalEpisodes': 208, 'stepsPerEpisode': 12, 'rewardPerEpisode': 11.021374595760113, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.5924429884264595, 'errorList': [], 'lossList': [0.0, -1.3215524089336395, 0.0, 4.354399601817131, 0.0, 0.0, 0.0], 'rewardMean': 0.6781378706985267, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 829.5695901116104
'totalSteps': 24320, 'rewardStep': 0.8904879052707608, 'errorList': [], 'lossList': [0.0, -1.2868544894456864, 0.0, 2.9998817563056948, 0.0, 0.0, 0.0], 'rewardMean': 0.6827308167885179, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1062.226987921489
'totalSteps': 25600, 'rewardStep': 0.6752902331017318, 'errorList': [], 'lossList': [0.0, -1.2716338062286376, 0.0, 2.3043119587004184, 0.0, 0.0, 0.0], 'rewardMean': 0.6817745058916056, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.543245774616
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=53.95
