#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 148
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5369346486532378, 'errorList': [], 'lossList': [0.0, -1.4224716544151306, 0.0, 29.377661249637605, 0.0, 0.0, 0.0], 'rewardMean': 0.7179325322399891, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 31.52973910489644
'totalSteps': 3840, 'rewardStep': 0.9349534357344562, 'errorList': [], 'lossList': [0.0, -1.4207249063253402, 0.0, 30.62816383957863, 0.0, 0.0, 0.0], 'rewardMean': 0.7902728334048114, 'totalEpisodes': 18, 'stepsPerEpisode': 491, 'rewardPerEpisode': 390.3989927017356
'totalSteps': 5120, 'rewardStep': 0.7995137220682431, 'errorList': [], 'lossList': [0.0, -1.4151351940631867, 0.0, 25.051041237413884, 0.0, 0.0, 0.0], 'rewardMean': 0.7925830555706693, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1037.572402195873
'totalSteps': 6400, 'rewardStep': 0.6496116344238759, 'errorList': [], 'lossList': [0.0, -1.3924613577127456, 0.0, 18.67215315759182, 0.0, 0.0, 0.0], 'rewardMean': 0.7639887713413106, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1031.7710045996048
'totalSteps': 7680, 'rewardStep': 0.8725301346070716, 'errorList': [], 'lossList': [0.0, -1.3657676130533218, 0.0, 27.73975829064846, 0.0, 0.0, 0.0], 'rewardMean': 0.7820789985522708, 'totalEpisodes': 19, 'stepsPerEpisode': 107, 'rewardPerEpisode': 84.88685135335187
'totalSteps': 8960, 'rewardStep': 0.9368114038402339, 'errorList': [], 'lossList': [0.0, -1.3489436882734298, 0.0, 24.397627403140067, 0.0, 0.0, 0.0], 'rewardMean': 0.8041836278791227, 'totalEpisodes': 20, 'stepsPerEpisode': 790, 'rewardPerEpisode': 605.1245390849315
'totalSteps': 10240, 'rewardStep': 0.8355305324055787, 'errorList': [], 'lossList': [0.0, -1.3339944905042649, 0.0, 105.41578001022339, 0.0, 0.0, 0.0], 'rewardMean': 0.8081019909449297, 'totalEpisodes': 26, 'stepsPerEpisode': 140, 'rewardPerEpisode': 121.55820158965808
'totalSteps': 11520, 'rewardStep': 0.6281286835677218, 'errorList': [], 'lossList': [0.0, -1.3325613576173783, 0.0, 397.2243354034424, 0.0, 0.0, 0.0], 'rewardMean': 0.7881049567919066, 'totalEpisodes': 63, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.316211071274553
'totalSteps': 12800, 'rewardStep': 0.9543304108181604, 'errorList': [201.590627024592, 301.098839991885, 304.0571470721221, 260.0602623856329, 279.2452732054314, 276.9324551835234, 241.7781044660495, 302.6762653187543, 207.95266894187995, 267.1696235526268, 308.0271658000294, 277.3849996011001, 207.39260021234978, 263.4884563087772, 217.26251202626423, 268.85084395075444, 166.05224544978245, 301.2228739427226, 295.6623744406655, 312.3648972220659, 254.94045526922318, 242.8974904010182, 303.80563803585693, 265.3523829029512, 295.23939230900555, 268.22758015111145, 245.99880285818878, 269.8460526321407, 290.27867464809754, 276.6535477160055, 276.31055521636, 279.7913589102104, 245.00024877296357, 292.3040557074074, 227.37893702656456, 294.506946537186, 266.7744311161486, 263.1410642341116, 209.8806332939958, 280.85504178064076, 274.42077282144396, 289.52381302612696, 282.4721626493278, 290.92757434244453, 282.58268856367107, 255.15011017354973, 312.7546989523162, 321.7541504012027, 274.60070282708716, 296.6711428907006], 'lossList': [0.0, -1.330568768978119, 0.0, 100.46786785125732, 0.0, 0.0, 0.0], 'rewardMean': 0.8047275021945319, 'totalEpisodes': 97, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.363431203005103, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.5778414702881195, 'errorList': [], 'lossList': [0.0, -1.3270958387851715, 0.0, 57.87697409629822, 0.0, 0.0, 0.0], 'rewardMean': 0.7726186076406699, 'totalEpisodes': 125, 'stepsPerEpisode': 34, 'rewardPerEpisode': 24.122355774148147
'totalSteps': 15360, 'rewardStep': 0.9500828967239189, 'errorList': [114.48347034143244, 163.448990353586, 8.252850604407058, 94.49190800672987, 76.92938821889351, 143.0122407681282, 116.15460660399653, 190.9154627599588, 27.322257508081638, 125.96433957295791, 43.527814585389464, 122.99307493812051, 104.0957249411302, 61.671778008533515, 165.60234254992687, 82.09166595788368, 130.8560159114187, 164.51037744998638, 155.80070768523527, 104.3503396201132, 150.63259548949696, 77.0752772433963, 157.347867465118, 55.25882131180165, 136.2962363025662, 17.733307272282815, 76.28783240209978, 156.65993181976617, 66.6161760061786, 28.831435113154793, 98.30978814716279, 86.75546773062935, 136.2542165096677, 142.8489644189555, 120.4491409946587, 159.3288880927954, 115.53303418551221, 34.3898592555203, 117.04012904017299, 31.54384777256852, 179.77075811550114, 115.35833610863241, 122.73205707283967, 4.345112781113733, 79.63591402616726, 137.16594307608864, 168.52928613880067, 16.987493740009914, 61.06327487801379, 142.2306020154867], 'lossList': [0.0, -1.3174267715215684, 0.0, 42.07581268310547, 0.0, 0.0, 0.0], 'rewardMean': 0.813933432447738, 'totalEpisodes': 155, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.095143050075285, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7382885891447066, 'errorList': [], 'lossList': [0.0, -1.3097077506780623, 0.0, 21.391421122550966, 0.0, 0.0, 0.0], 'rewardMean': 0.7942669477887631, 'totalEpisodes': 168, 'stepsPerEpisode': 38, 'rewardPerEpisode': 29.76254074789276
'totalSteps': 17920, 'rewardStep': 0.7909107641212731, 'errorList': [], 'lossList': [0.0, -1.3034858852624893, 0.0, 18.32100307703018, 0.0, 0.0, 0.0], 'rewardMean': 0.7934066519940661, 'totalEpisodes': 172, 'stepsPerEpisode': 200, 'rewardPerEpisode': 161.83560515164083
'totalSteps': 19200, 'rewardStep': 0.6571897578525445, 'errorList': [], 'lossList': [0.0, -1.292926062941551, 0.0, 21.65668217897415, 0.0, 0.0, 0.0], 'rewardMean': 0.7941644643369328, 'totalEpisodes': 176, 'stepsPerEpisode': 226, 'rewardPerEpisode': 159.8104795933306
'totalSteps': 20480, 'rewardStep': 0.7939867337021684, 'errorList': [], 'lossList': [0.0, -1.2769284355640411, 0.0, 6.53159823179245, 0.0, 0.0, 0.0], 'rewardMean': 0.7863101242464426, 'totalEpisodes': 177, 'stepsPerEpisode': 1242, 'rewardPerEpisode': 933.5301660637585
'totalSteps': 21760, 'rewardStep': 0.9482842539994992, 'errorList': [0.13368890470821504, 0.01946064265625748, 0.030602691144945432, 0.030678544718034537, 0.1311994548436439, 0.0385698833089786, 0.1405983901198243, 0.0450627511008509, 0.088153780643246, 0.11446895905550913, 0.017007770426386695, 0.16474614688598951, 0.12007089909238451, 0.11753578665643821, 0.03832407040081838, 0.1615218522831188, 0.09555132279627915, 0.19663122699218463, 0.2519807193348632, 0.20971706417656952, 0.13888177819902925, 0.06041712424142562, 0.04873502266720982, 0.031424886086530684, 0.04330640588554432, 0.22598976176453017, 0.10430218538995464, 0.02097007323949304, 0.10531533593460468, 0.10647987898077799, 0.17435955249087418, 0.1427881217445679, 0.12610028437610576, 0.07753411881869773, 0.07780574926201964, 0.0282481793697013, 0.12107330948103057, 0.053047144612357544, 0.0629340655463047, 0.07614189420754054, 0.020525175331821827, 0.11484267581045927, 0.23138765336101202, 0.11555662802878003, 0.3395617708557935, 0.24131044374135313, 0.08348739550840095, 0.03802504180812969, 0.28491452685732943, 0.08853015406866067], 'lossList': [0.0, -1.261444182395935, 0.0, 5.980601719319821, 0.0, 0.0, 0.0], 'rewardMean': 0.7874574092623691, 'totalEpisodes': 178, 'stepsPerEpisode': 676, 'rewardPerEpisode': 605.3990918789422, 'successfulTests': 43
'totalSteps': 23040, 'rewardStep': 0.7486568276487702, 'errorList': [], 'lossList': [0.0, -1.2340526711940765, 0.0, 3.4588897129893303, 0.0, 0.0, 0.0], 'rewardMean': 0.7787700387866883, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.6499146675133
'totalSteps': 24320, 'rewardStep': 0.24614213531814438, 'errorList': [], 'lossList': [0.0, -1.2181597870588303, 0.0, 2.833513927459717, 0.0, 0.0, 0.0], 'rewardMean': 0.7405713839617305, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 894.0666493376447
'totalSteps': 25600, 'rewardStep': 0.8097195465686396, 'errorList': [], 'lossList': [0.0, -1.2037726581096648, 0.0, 2.9142154550552366, 0.0, 0.0, 0.0], 'rewardMean': 0.7261102975367784, 'totalEpisodes': 179, 'stepsPerEpisode': 1271, 'rewardPerEpisode': 975.8866990358013
#maxSuccessfulTests=43, maxSuccessfulTestsAtStep=21760, timeSpent=68.29
