#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 15
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9566524921681437, 'errorList': [], 'lossList': [0.0, -1.4359672951698303, 0.0, 26.550688416957854, 0.0, 0.0, 0.0], 'rewardMean': 0.9261170416881555, 'totalEpisodes': 8, 'stepsPerEpisode': 534, 'rewardPerEpisode': 363.33217111924523
'totalSteps': 3840, 'rewardStep': 0.5528717125346739, 'errorList': [], 'lossList': [0.0, -1.426474810242653, 0.0, 40.63596921443939, 0.0, 0.0, 0.0], 'rewardMean': 0.8017019319703284, 'totalEpisodes': 20, 'stepsPerEpisode': 155, 'rewardPerEpisode': 121.44806883695547
'totalSteps': 5120, 'rewardStep': 0.5730532165289919, 'errorList': [], 'lossList': [0.0, -1.4212729120254517, 0.0, 70.21653210639954, 0.0, 0.0, 0.0], 'rewardMean': 0.7445397531099942, 'totalEpisodes': 36, 'stepsPerEpisode': 104, 'rewardPerEpisode': 70.78042082733865
'totalSteps': 6400, 'rewardStep': 0.8746689802960284, 'errorList': [], 'lossList': [0.0, -1.4169272327423095, 0.0, 126.26029384613037, 0.0, 0.0, 0.0], 'rewardMean': 0.770565598547201, 'totalEpisodes': 93, 'stepsPerEpisode': 30, 'rewardPerEpisode': 26.598456003562312
'totalSteps': 7680, 'rewardStep': 0.9447556291608555, 'errorList': [85.69358597977906, 47.62209398278058, 15.31206976214032, 105.1105701206956, 103.25049716041666, 15.482411355516303, 17.795708243850008, 63.47316275306125, 47.90828585009725, 114.17429202181975, 21.917162488834997, 15.465085230679826, 107.86035310836093, 92.88621165262097, 114.61907353929833, 77.97725158281123, 19.55373783428639, 81.27956892105844, 67.00826187927096, 38.406201281141584, 66.83390595457084, 71.79682471664835, 58.74398195620135, 77.23529764954068, 18.958519068612635, 66.10622555104463, 51.34980805711981, 93.28298597520305, 17.789496935556638, 125.55123465840323, 74.89534233430597, 69.05950206878246, 77.90116438461351, 11.812271703831883, 66.55848638440533, 15.563408717195406, 17.80398158142253, 62.556938201093985, 36.38339588105045, 58.528012347613426, 61.448386163179826, 11.584470154727084, 21.390233646586772, 34.97389813356051, 10.159539089776745, 40.75646145976266, 49.62261277253773, 103.0143313228014, 34.79478661647519, 10.137687958361678], 'lossList': [0.0, -1.4069266325235368, 0.0, 63.52424770355225, 0.0, 0.0, 0.0], 'rewardMean': 0.7995972703161435, 'totalEpisodes': 133, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.14185872902875, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.8188514431624294, 'errorList': [], 'lossList': [0.0, -1.3961888748407363, 0.0, 55.10435277938843, 0.0, 0.0, 0.0], 'rewardMean': 0.8023478664370415, 'totalEpisodes': 157, 'stepsPerEpisode': 49, 'rewardPerEpisode': 38.70376857975944
'totalSteps': 10240, 'rewardStep': 0.39119799877674155, 'errorList': [], 'lossList': [0.0, -1.3901941621303557, 0.0, 38.698469457626345, 0.0, 0.0, 0.0], 'rewardMean': 0.750954132979504, 'totalEpisodes': 169, 'stepsPerEpisode': 148, 'rewardPerEpisode': 103.31007797419625
'totalSteps': 11520, 'rewardStep': 0.6669689963737827, 'errorList': [], 'lossList': [0.0, -1.3695406138896942, 0.0, 34.74883306980133, 0.0, 0.0, 0.0], 'rewardMean': 0.7416224511344238, 'totalEpisodes': 179, 'stepsPerEpisode': 69, 'rewardPerEpisode': 61.63537256064782
'totalSteps': 12800, 'rewardStep': 0.753064444412691, 'errorList': [], 'lossList': [0.0, -1.3406836479902267, 0.0, 21.866237456798554, 0.0, 0.0, 0.0], 'rewardMean': 0.7427666504622505, 'totalEpisodes': 184, 'stepsPerEpisode': 95, 'rewardPerEpisode': 79.38602198563137
'totalSteps': 14080, 'rewardStep': 0.6675556144765461, 'errorList': [], 'lossList': [0.0, -1.3259386175870895, 0.0, 13.750240910053254, 0.0, 0.0, 0.0], 'rewardMean': 0.7199640527890885, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 986.8748888939798
'totalSteps': 15360, 'rewardStep': 0.8514666293188888, 'errorList': [], 'lossList': [0.0, -1.308279612660408, 0.0, 13.584652613401413, 0.0, 0.0, 0.0], 'rewardMean': 0.709445466504163, 'totalEpisodes': 190, 'stepsPerEpisode': 43, 'rewardPerEpisode': 38.601970943723174
'totalSteps': 16640, 'rewardStep': 0.7794215583165061, 'errorList': [], 'lossList': [0.0, -1.3082828170061112, 0.0, 6.491520814299584, 0.0, 0.0, 0.0], 'rewardMean': 0.7321004510823461, 'totalEpisodes': 191, 'stepsPerEpisode': 328, 'rewardPerEpisode': 275.55904738597894
'totalSteps': 17920, 'rewardStep': 0.6971343803110983, 'errorList': [], 'lossList': [0.0, -1.3175638765096664, 0.0, 7.080697536468506, 0.0, 0.0, 0.0], 'rewardMean': 0.7445085674605567, 'totalEpisodes': 193, 'stepsPerEpisode': 395, 'rewardPerEpisode': 324.1026587092658
'totalSteps': 19200, 'rewardStep': 0.7327443028231166, 'errorList': [], 'lossList': [0.0, -1.314783427119255, 0.0, 16.0751449239254, 0.0, 0.0, 0.0], 'rewardMean': 0.7303160997132656, 'totalEpisodes': 194, 'stepsPerEpisode': 1221, 'rewardPerEpisode': 957.9631989802278
'totalSteps': 20480, 'rewardStep': 0.9420515215522904, 'errorList': [0.2633396144858788, 0.3408720515781817, 0.06612942026793373, 0.09482356504107352, 0.33151994318850797, 0.09473276369084513, 0.12690834806272377, 0.4130633888431983, 0.16268369290894247, 0.1380323600106339, 0.0676136233986596, 0.3486096415163333, 0.30633177672790046, 0.23935640077433973, 0.10112876859928062, 0.26080690963132724, 0.12537580743744806, 0.16816469375688123, 0.21279550953398416, 0.19328584882099947, 0.14041432231047474, 0.08695924100443332, 0.4322520738179055, 0.1413084847898893, 0.11201505025312385, 0.13000375247115187, 0.10352251638740044, 0.07252590570643777, 0.07353311642468353, 0.2720407555124597, 0.3398316822953273, 0.22282994124577427, 0.5340119158685794, 0.31262087591726123, 0.2114920154900212, 0.2146159618488089, 0.3442190535313207, 0.20565273816481558, 0.35768073232209946, 0.32426347339761347, 0.07427321446143956, 0.10725265762863762, 0.12035909223101311, 0.51889333289335, 0.18840579019589188, 0.17695325194049255, 0.21688463575415187, 0.15548705451089195, 0.14497125209928, 0.10860280602596252], 'lossList': [0.0, -1.3042452663183213, 0.0, 7.2000004732608796, 0.0, 0.0, 0.0], 'rewardMean': 0.7300456889524091, 'totalEpisodes': 195, 'stepsPerEpisode': 29, 'rewardPerEpisode': 26.43998249008635, 'successfulTests': 27
'totalSteps': 21760, 'rewardStep': 0.6683552964945731, 'errorList': [], 'lossList': [0.0, -1.2965080207586288, 0.0, 2.5354485431313516, 0.0, 0.0, 0.0], 'rewardMean': 0.7149960742856234, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 952.0377333103845
'totalSteps': 23040, 'rewardStep': 0.77016050854685, 'errorList': [], 'lossList': [0.0, -1.2941883838176726, 0.0, 1.6177963027358055, 0.0, 0.0, 0.0], 'rewardMean': 0.7528923252626343, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1056.1408837449603
'totalSteps': 24320, 'rewardStep': 0.756561865811768, 'errorList': [], 'lossList': [0.0, -1.2665164548158645, 0.0, 1.323911903053522, 0.0, 0.0, 0.0], 'rewardMean': 0.7618516122064328, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.5377463084453
'totalSteps': 25600, 'rewardStep': 0.9257997369181055, 'errorList': [], 'lossList': [0.0, -1.2123098433017732, 0.0, 0.8010589069873094, 0.0, 0.0, 0.0], 'rewardMean': 0.7791251414569743, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.4378801118162
#maxSuccessfulTests=27, maxSuccessfulTestsAtStep=20480, timeSpent=84.93
