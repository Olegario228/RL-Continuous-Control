#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 17
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.6960283603100778, 'errorList': [], 'lossList': [0.0, -1.4525242465734483, 0.0, 29.316465044021605, 0.0, 0.0, 0.0], 'rewardMean': 0.586907067766438, 'totalEpisodes': 14, 'stepsPerEpisode': 291, 'rewardPerEpisode': 217.48997880819547
'totalSteps': 3840, 'rewardStep': 0.8698960313188704, 'errorList': [], 'lossList': [0.0, -1.458679603934288, 0.0, 36.03763022899628, 0.0, 0.0, 0.0], 'rewardMean': 0.6812367222839155, 'totalEpisodes': 23, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.52167995253075
'totalSteps': 5120, 'rewardStep': 0.5020391323855218, 'errorList': [], 'lossList': [0.0, -1.429691761136055, 0.0, 70.59823369979858, 0.0, 0.0, 0.0], 'rewardMean': 0.6364373248093171, 'totalEpisodes': 44, 'stepsPerEpisode': 53, 'rewardPerEpisode': 36.373313834728535
'totalSteps': 6400, 'rewardStep': 0.5994430042194502, 'errorList': [], 'lossList': [0.0, -1.422318434715271, 0.0, 119.00448402404785, 0.0, 0.0, 0.0], 'rewardMean': 0.6290384606913437, 'totalEpisodes': 98, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.264658245617623
'totalSteps': 7680, 'rewardStep': 0.9160747138055301, 'errorList': [], 'lossList': [0.0, -1.4258356964588166, 0.0, 54.33860788345337, 0.0, 0.0, 0.0], 'rewardMean': 0.6768778362103748, 'totalEpisodes': 138, 'stepsPerEpisode': 40, 'rewardPerEpisode': 30.947451405988595
'totalSteps': 8960, 'rewardStep': 0.8404877421494598, 'errorList': [], 'lossList': [0.0, -1.4191660922765732, 0.0, 36.26398018836975, 0.0, 0.0, 0.0], 'rewardMean': 0.7002506799159584, 'totalEpisodes': 155, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.673120432497468
'totalSteps': 10240, 'rewardStep': 0.5402084030837434, 'errorList': [], 'lossList': [0.0, -1.39475483417511, 0.0, 15.58969769835472, 0.0, 0.0, 0.0], 'rewardMean': 0.6802453953119314, 'totalEpisodes': 161, 'stepsPerEpisode': 63, 'rewardPerEpisode': 50.03097203236495
'totalSteps': 11520, 'rewardStep': 0.5062519047430132, 'errorList': [], 'lossList': [0.0, -1.3792051941156387, 0.0, 28.328018312454223, 0.0, 0.0, 0.0], 'rewardMean': 0.6609127852487183, 'totalEpisodes': 168, 'stepsPerEpisode': 245, 'rewardPerEpisode': 186.63826781374908
'totalSteps': 12800, 'rewardStep': 0.7127834129502137, 'errorList': [], 'lossList': [0.0, -1.377502281665802, 0.0, 30.278273229598998, 0.0, 0.0, 0.0], 'rewardMean': 0.6660998480188678, 'totalEpisodes': 174, 'stepsPerEpisode': 142, 'rewardPerEpisode': 103.34696004417374
'totalSteps': 14080, 'rewardStep': 0.39878566152113837, 'errorList': [], 'lossList': [0.0, -1.3761197918653487, 0.0, 9.268160185217857, 0.0, 0.0, 0.0], 'rewardMean': 0.6581998366487019, 'totalEpisodes': 176, 'stepsPerEpisode': 485, 'rewardPerEpisode': 350.98475196225144
'totalSteps': 15360, 'rewardStep': 0.8906941925086462, 'errorList': [], 'lossList': [0.0, -1.3925642156600953, 0.0, 23.007760572433472, 0.0, 0.0, 0.0], 'rewardMean': 0.6776664198685587, 'totalEpisodes': 178, 'stepsPerEpisode': 1139, 'rewardPerEpisode': 863.3258781793071
'totalSteps': 16640, 'rewardStep': 0.7899902534590049, 'errorList': [], 'lossList': [0.0, -1.3741210210323334, 0.0, 4.883926049768925, 0.0, 0.0, 0.0], 'rewardMean': 0.6696758420825721, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 999.1921091611363
'totalSteps': 17920, 'rewardStep': 0.8695658142430314, 'errorList': [], 'lossList': [0.0, -1.3467766857147216, 0.0, 4.226857351362705, 0.0, 0.0, 0.0], 'rewardMean': 0.7064285102683232, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.6788777324598
'totalSteps': 19200, 'rewardStep': 0.9078752401701435, 'errorList': [], 'lossList': [0.0, -1.3061526370048524, 0.0, 2.993647945523262, 0.0, 0.0, 0.0], 'rewardMean': 0.7372717338633926, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.7753335931322
'totalSteps': 20480, 'rewardStep': 0.9632205781612346, 'errorList': [0.138465664029545, 0.1554925480012124, 0.16007434369471324, 0.17023199633024852, 0.15275291353535567, 0.15731023673651184, 0.1413877730244392, 0.14532889430487572, 0.1299263496506158, 0.1548397464248225, 0.15176396124849875, 0.1424939000496343, 0.16004838386420972, 0.15135680111315705, 0.17366827676439026, 0.1993256868704255, 0.14686606430830432, 0.19200081254270635, 0.14851126608111773, 0.17093080298354965, 0.1505150491311677, 0.1566007666294791, 0.15529278213415532, 0.1556708412427583, 0.15384913022160904, 0.1700756027005107, 0.18046750854782145, 0.15692185140095383, 0.15784658600432525, 0.1450380939049643, 0.14031910849276397, 0.17974362036582658, 0.19335289027863609, 0.1446681205786911, 0.1469742437297628, 0.15385433078223706, 0.14074578051746417, 0.1489133523464143, 0.1439080213087561, 0.20657650534984504, 0.16057934178451128, 0.1486933509254266, 0.1871595821153428, 0.16029959365623728, 0.15157942225293006, 0.15528758333013654, 0.1395748862899501, 0.1428119973520696, 0.159788444655215, 0.17165211787860152], 'lossList': [0.0, -1.2564937072992324, 0.0, 2.7080299771949647, 0.0, 0.0, 0.0], 'rewardMean': 0.7419863202989628, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.88191405844, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.9540073349922268, 'errorList': [0.15187107465682229, 0.1502808725685425, 0.15840100705328217, 0.14188284609440216, 0.1559385541448801, 0.15686466882165512, 0.1529088048726872, 0.15550232397226207, 0.14613635571953398, 0.1552248757334014, 0.15293489694812826, 0.1402139576945983, 0.17086143490788253, 0.14475046680357656, 0.16688750906227648, 0.14874699601341923, 0.1433488447355248, 0.1569521404660437, 0.15098076323003148, 0.16069292528149548, 0.16864642115314288, 0.1629158832967092, 0.14683610206892805, 0.1532792472473927, 0.16922586082462734, 0.15182082642057887, 0.13875779637259322, 0.14163822151023658, 0.17393948626499578, 0.13581512922163783, 0.1489861159050806, 0.1641876866902473, 0.1583102236046718, 0.15290394654697173, 0.15865380058785156, 0.15806545179237774, 0.1467396507533328, 0.14952649871838236, 0.1503262699753585, 0.17573713692344892, 0.14289556398548972, 0.16877930588773446, 0.1560583966478405, 0.16187614041371884, 0.15584748569967433, 0.1632492147049442, 0.1487542898302405, 0.146473840579469, 0.18376265379314558, 0.15136241644539214], 'lossList': [0.0, -1.1965946024656295, 0.0, 1.4265439926274122, 0.0, 0.0, 0.0], 'rewardMean': 0.7533382795832395, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.217890522031, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=78.62
