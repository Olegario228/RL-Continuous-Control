#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 18
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5417348163261636, 'errorList': [], 'lossList': [0.0, -1.4221710526943208, 0.0, 25.39143056511879, 0.0, 0.0, 0.0], 'rewardMean': 0.6999734624408516, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 29.66387626201826
'totalSteps': 3840, 'rewardStep': 0.9183569811906849, 'errorList': [], 'lossList': [0.0, -1.4210409021377564, 0.0, 36.58317741394043, 0.0, 0.0, 0.0], 'rewardMean': 0.772767968690796, 'totalEpisodes': 23, 'stepsPerEpisode': 361, 'rewardPerEpisode': 260.3609976248214
'totalSteps': 5120, 'rewardStep': 0.8509206921455187, 'errorList': [], 'lossList': [0.0, -1.423515080809593, 0.0, 69.18918235778808, 0.0, 0.0, 0.0], 'rewardMean': 0.7923061495544766, 'totalEpisodes': 36, 'stepsPerEpisode': 74, 'rewardPerEpisode': 66.12569954629348
'totalSteps': 6400, 'rewardStep': 0.966553190249365, 'errorList': [241.3656422206834, 266.31566201310864, 260.30994840978127, 242.28696823313567, 221.64992095576608, 198.6669721637175, 285.26391931692666, 314.69199625596093, 292.6698246469805, 203.1307971277991, 276.8245661733058, 165.1237805082797, 256.01810081143634, 284.1444954555488, 164.48499477070953, 142.5059600225547, 259.0038312196655, 307.1935539175442, 276.36554514196433, 259.3807106568335, 309.7069656313521, 284.688756191819, 297.7637876855124, 176.96842052361495, 280.6254399034121, 253.8201622321438, 246.45469412365514, 290.92412094366176, 221.3899238010307, 242.01133824325916, 294.7911329522837, 284.3121676077644, 304.6930035011396, 255.0701528849195, 302.60415231629054, 250.72859051923496, 297.66367479238465, 297.8430927691295, 205.03752314737466, 269.3784763441355, 306.62357093092595, 300.07677072603053, 272.91108868359987, 310.80543451501154, 133.3241746902595, 282.4189232685129, 300.7316767128486, 264.8851152522567, 212.83026076856092, 275.90599002849757], 'lossList': [0.0, -1.4262532109022141, 0.0, 144.62264526367187, 0.0, 0.0, 0.0], 'rewardMean': 0.8271555576934544, 'totalEpisodes': 89, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.09463507087219, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.4980862508836703, 'errorList': [], 'lossList': [0.0, -1.4177975451946259, 0.0, 64.88699718475341, 0.0, 0.0, 0.0], 'rewardMean': 0.772310673225157, 'totalEpisodes': 126, 'stepsPerEpisode': 53, 'rewardPerEpisode': 36.239872925709406
'totalSteps': 8960, 'rewardStep': 0.6751576184761227, 'errorList': [], 'lossList': [0.0, -1.3979261964559555, 0.0, 54.01408922195434, 0.0, 0.0, 0.0], 'rewardMean': 0.7584316654038663, 'totalEpisodes': 161, 'stepsPerEpisode': 69, 'rewardPerEpisode': 51.21177611868535
'totalSteps': 10240, 'rewardStep': 0.6435619921569452, 'errorList': [], 'lossList': [0.0, -1.3693559402227402, 0.0, 34.92876810073852, 0.0, 0.0, 0.0], 'rewardMean': 0.7440729562480013, 'totalEpisodes': 173, 'stepsPerEpisode': 164, 'rewardPerEpisode': 123.51911697338045
'totalSteps': 11520, 'rewardStep': 0.714503889704095, 'errorList': [], 'lossList': [0.0, -1.3532190573215486, 0.0, 23.32336651802063, 0.0, 0.0, 0.0], 'rewardMean': 0.7407875044097895, 'totalEpisodes': 177, 'stepsPerEpisode': 228, 'rewardPerEpisode': 178.82080158345846
'totalSteps': 12800, 'rewardStep': 0.5351441540579847, 'errorList': [], 'lossList': [0.0, -1.3375706845521926, 0.0, 26.984242742061614, 0.0, 0.0, 0.0], 'rewardMean': 0.7202231693746091, 'totalEpisodes': 181, 'stepsPerEpisode': 145, 'rewardPerEpisode': 94.23719066319154
'totalSteps': 14080, 'rewardStep': 0.952913414597566, 'errorList': [0.32966953419242706, 0.2425647197224081, 0.5174887298945171, 0.4878582621511347, 0.3251344268392172, 0.2662936548737258, 0.34692182960740053, 0.5225670096526585, 0.6896788007044306, 0.5488218128102139, 0.7119102697834961, 0.591659854549944, 0.3020389753880032, 0.4515881492700572, 0.38042356403102745, 0.8480279445535294, 0.7902001709660279, 0.35227927490709937, 0.26468959320421115, 0.4955609352253759, 0.39567684813480725, 0.6268771594710969, 0.5865107706586427, 0.39129571302557437, 0.38367982215847896, 0.6762466827379229, 0.8100462572257118, 0.46750406202535943, 0.7817631651590836, 0.2613719113970391, 0.6187095428005849, 0.5836426978872059, 0.30450833832760005, 0.49829303425577787, 0.516705324063948, 0.2748949109151726, 0.7003353982948678, 0.7117769028161345, 0.42580442559348103, 0.4686321397645247, 0.5947487708156067, 0.6773699182988933, 0.3539372863854923, 0.669083523642793, 0.22907188238323725, 0.585101469818685, 0.4172850449887201, 0.47500055174757055, 0.24459922923051858, 0.3406466243357492], 'lossList': [0.0, -1.3021984267234803, 0.0, 9.28234814107418, 0.0, 0.0, 0.0], 'rewardMean': 0.7296932999788115, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 893.7752656449179, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7945395102239102, 'errorList': [], 'lossList': [0.0, -1.2544519293308258, 0.0, 5.272887917160988, 0.0, 0.0, 0.0], 'rewardMean': 0.7549737693685862, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 913.4468708093245
'totalSteps': 16640, 'rewardStep': 0.7703919012062512, 'errorList': [], 'lossList': [0.0, -1.2459643709659576, 0.0, 10.799610636234284, 0.0, 0.0, 0.0], 'rewardMean': 0.7401772613701428, 'totalEpisodes': 183, 'stepsPerEpisode': 281, 'rewardPerEpisode': 236.62592610529788
'totalSteps': 17920, 'rewardStep': 0.7018534685894671, 'errorList': [], 'lossList': [0.0, -1.2517861711978913, 0.0, 2.524537398815155, 0.0, 0.0, 0.0], 'rewardMean': 0.7252705390145378, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 904.6228737484553
'totalSteps': 19200, 'rewardStep': 0.923167924332346, 'errorList': [], 'lossList': [0.0, -1.2290107482671737, 0.0, 2.2521711631864307, 0.0, 0.0, 0.0], 'rewardMean': 0.7209320124228358, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.4461234810609
'totalSteps': 20480, 'rewardStep': 0.9253435844789762, 'errorList': [], 'lossList': [0.0, -1.1882521629333496, 0.0, 2.859439534917474, 0.0, 0.0, 0.0], 'rewardMean': 0.7636577457823664, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.0115214637178
'totalSteps': 21760, 'rewardStep': 0.9187477883862643, 'errorList': [], 'lossList': [0.0, -1.1611926090717315, 0.0, 1.132081108056009, 0.0, 0.0, 0.0], 'rewardMean': 0.7880167627733805, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.4442758289886
'totalSteps': 23040, 'rewardStep': 0.7929110476083636, 'errorList': [], 'lossList': [0.0, -1.1616117346286774, 0.0, 0.9833519174158574, 0.0, 0.0, 0.0], 'rewardMean': 0.8029516683185225, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.3537331644177
'totalSteps': 24320, 'rewardStep': 0.8933570296553124, 'errorList': [], 'lossList': [0.0, -1.1370439231395721, 0.0, 0.8893640859797597, 0.0, 0.0, 0.0], 'rewardMean': 0.8208369823136442, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.7193275924922
'totalSteps': 25600, 'rewardStep': 0.9573436445070298, 'errorList': [0.020701759739041783, 0.08520285387918727, 0.06073405825529876, 0.058057403308173695, 0.02974117021850275, 0.08092654656986728, 0.009893586481996345, 0.06599472784711223, 0.007759158112879011, 0.024465044098760515, 0.09514554574581931, 0.022779650368464282, 0.15911141616921365, 0.05687371826783508, 0.04309069418040421, 0.07141889244776418, 0.05843480386968453, 0.08803998070004675, 0.02284053914098646, 0.10307615188469042, 0.029672605986561233, 0.023865367752432743, 0.12677531417597251, 0.056607393345553196, 0.03324483706718962, 0.040484890073096615, 0.0369423828837795, 0.046932637147033295, 0.11497668191753624, 0.026804470588841826, 0.03609930448048996, 0.08942440918118055, 0.085782538091733, 0.03164971587837997, 0.05389119436165384, 0.0390429253081173, 0.04799574360722287, 0.05835440285591664, 0.06165476210539137, 0.05911042501343753, 0.04416419290418575, 0.06109053635417074, 0.033718253926713286, 0.08016897299271916, 0.06596099899918828, 0.006972638980615306, 0.07670849281732016, 0.07894184200095268, 0.10228452351181588, 0.06780971009956041], 'lossList': [0.0, -1.0998928982019425, 0.0, 0.6946128561161459, 0.0, 0.0, 0.0], 'rewardMean': 0.8630569313585486, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1185.05647524939, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.53
