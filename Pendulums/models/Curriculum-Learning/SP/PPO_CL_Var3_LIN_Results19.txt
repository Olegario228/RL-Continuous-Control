#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 19
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8618896623816446, 'errorList': [], 'lossList': [0.0, -1.425628889799118, 0.0, 29.634528595209122, 0.0, 0.0, 0.0], 'rewardMean': 0.7926202931304727, 'totalEpisodes': 13, 'stepsPerEpisode': 319, 'rewardPerEpisode': 251.79565903211216
'totalSteps': 3840, 'rewardStep': 0.5773296110491879, 'errorList': [], 'lossList': [0.0, -1.436370250582695, 0.0, 37.21792936325073, 0.0, 0.0, 0.0], 'rewardMean': 0.7208567324367111, 'totalEpisodes': 21, 'stepsPerEpisode': 180, 'rewardPerEpisode': 112.44901751832234
'totalSteps': 5120, 'rewardStep': 0.7036556297088705, 'errorList': [], 'lossList': [0.0, -1.438562034368515, 0.0, 73.73330945968628, 0.0, 0.0, 0.0], 'rewardMean': 0.7165564567547509, 'totalEpisodes': 41, 'stepsPerEpisode': 42, 'rewardPerEpisode': 34.080743239669665
'totalSteps': 6400, 'rewardStep': 0.8131416522376362, 'errorList': [], 'lossList': [0.0, -1.4335080409049987, 0.0, 129.27941986083985, 0.0, 0.0, 0.0], 'rewardMean': 0.735873495851328, 'totalEpisodes': 101, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.416974908331682
'totalSteps': 7680, 'rewardStep': 0.5629987026409025, 'errorList': [], 'lossList': [0.0, -1.411407657265663, 0.0, 61.89173885345459, 0.0, 0.0, 0.0], 'rewardMean': 0.707061030316257, 'totalEpisodes': 140, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.665414111830919
'totalSteps': 8960, 'rewardStep': 0.8694083189682883, 'errorList': [], 'lossList': [0.0, -1.3904496681690217, 0.0, 60.33215042114258, 0.0, 0.0, 0.0], 'rewardMean': 0.7302535001236901, 'totalEpisodes': 165, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.553786097867615
'totalSteps': 10240, 'rewardStep': 0.7716101501028052, 'errorList': [], 'lossList': [0.0, -1.3726952934265138, 0.0, 49.75473631858826, 0.0, 0.0, 0.0], 'rewardMean': 0.7354230813710795, 'totalEpisodes': 176, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.067658837015962
'totalSteps': 11520, 'rewardStep': 0.7773746746254621, 'errorList': [], 'lossList': [0.0, -1.3454281723499297, 0.0, 54.51363226890564, 0.0, 0.0, 0.0], 'rewardMean': 0.7400843695104553, 'totalEpisodes': 182, 'stepsPerEpisode': 46, 'rewardPerEpisode': 31.54978605429635
'totalSteps': 12800, 'rewardStep': 0.8321011170274419, 'errorList': [], 'lossList': [0.0, -1.3199708008766173, 0.0, 32.16683979034424, 0.0, 0.0, 0.0], 'rewardMean': 0.749286044262154, 'totalEpisodes': 189, 'stepsPerEpisode': 64, 'rewardPerEpisode': 58.936136791553615
'totalSteps': 14080, 'rewardStep': 0.8057860606530007, 'errorList': [], 'lossList': [0.0, -1.3075462806224822, 0.0, 13.401616996526718, 0.0, 0.0, 0.0], 'rewardMean': 0.757529557939524, 'totalEpisodes': 196, 'stepsPerEpisode': 67, 'rewardPerEpisode': 52.849643053665766
'totalSteps': 15360, 'rewardStep': 0.7882956230170265, 'errorList': [], 'lossList': [0.0, -1.2934580147266388, 0.0, 54.80801518917084, 0.0, 0.0, 0.0], 'rewardMean': 0.7501701540030622, 'totalEpisodes': 204, 'stepsPerEpisode': 202, 'rewardPerEpisode': 181.11062226427413
'totalSteps': 16640, 'rewardStep': 0.6337918932654754, 'errorList': [], 'lossList': [0.0, -1.2843779158592223, 0.0, 24.91397367477417, 0.0, 0.0, 0.0], 'rewardMean': 0.7558163822246909, 'totalEpisodes': 209, 'stepsPerEpisode': 221, 'rewardPerEpisode': 191.59922730785033
'totalSteps': 17920, 'rewardStep': 0.498178091400128, 'errorList': [], 'lossList': [0.0, -1.2707665157318115, 0.0, 10.075563814640045, 0.0, 0.0, 0.0], 'rewardMean': 0.7352686283938167, 'totalEpisodes': 212, 'stepsPerEpisode': 415, 'rewardPerEpisode': 337.22633365189705
'totalSteps': 19200, 'rewardStep': 0.9134119951855043, 'errorList': [], 'lossList': [0.0, -1.2688996988534926, 0.0, 5.079495429992676, 0.0, 0.0, 0.0], 'rewardMean': 0.7452956626886035, 'totalEpisodes': 218, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.912140631770983
'totalSteps': 20480, 'rewardStep': 0.8520525386393255, 'errorList': [], 'lossList': [0.0, -1.257828442454338, 0.0, 6.393308290243149, 0.0, 0.0, 0.0], 'rewardMean': 0.7742010462884458, 'totalEpisodes': 221, 'stepsPerEpisode': 124, 'rewardPerEpisode': 95.72936214567673
'totalSteps': 21760, 'rewardStep': 0.4698302322687901, 'errorList': [], 'lossList': [0.0, -1.2570243453979493, 0.0, 4.79811960875988, 0.0, 0.0, 0.0], 'rewardMean': 0.734243237618496, 'totalEpisodes': 224, 'stepsPerEpisode': 308, 'rewardPerEpisode': 248.6195449438211
'totalSteps': 23040, 'rewardStep': 0.7103591742303381, 'errorList': [], 'lossList': [0.0, -1.2688565182685851, 0.0, 4.176230072975159, 0.0, 0.0, 0.0], 'rewardMean': 0.7281181400312493, 'totalEpisodes': 227, 'stepsPerEpisode': 194, 'rewardPerEpisode': 150.64201773774548
'totalSteps': 24320, 'rewardStep': 0.7786545151594951, 'errorList': [], 'lossList': [0.0, -1.2637261033058167, 0.0, 3.0623932033777237, 0.0, 0.0, 0.0], 'rewardMean': 0.7282461240846525, 'totalEpisodes': 228, 'stepsPerEpisode': 80, 'rewardPerEpisode': 70.05487264846138
'totalSteps': 25600, 'rewardStep': 0.5985074225096731, 'errorList': [], 'lossList': [0.0, -1.2407880610227584, 0.0, 3.4964628928899764, 0.0, 0.0, 0.0], 'rewardMean': 0.7048867546328756, 'totalEpisodes': 230, 'stepsPerEpisode': 366, 'rewardPerEpisode': 273.53364676347957
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=49.74
