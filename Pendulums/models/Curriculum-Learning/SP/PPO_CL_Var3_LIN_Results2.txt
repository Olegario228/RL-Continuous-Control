#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.5144274881492504, 'errorList': [], 'lossList': [0.0, -1.4231219381093978, 0.0, 34.98369889259338, 0.0, 0.0, 0.0], 'rewardMean': 0.6650324149664799, 'totalEpisodes': 70, 'stepsPerEpisode': 11, 'rewardPerEpisode': 5.992836479964731
'totalSteps': 3840, 'rewardStep': 0.9312231163218374, 'errorList': [], 'lossList': [0.0, -1.4050944536924361, 0.0, 46.801900806427, 0.0, 0.0, 0.0], 'rewardMean': 0.7537626487515992, 'totalEpisodes': 97, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.2341349326733395
'totalSteps': 5120, 'rewardStep': 0.8280562755433729, 'errorList': [], 'lossList': [0.0, -1.37304738342762, 0.0, 57.79771591186523, 0.0, 0.0, 0.0], 'rewardMean': 0.7723360554495426, 'totalEpisodes': 123, 'stepsPerEpisode': 27, 'rewardPerEpisode': 24.31011205167326
'totalSteps': 6400, 'rewardStep': 0.7898730040388342, 'errorList': [], 'lossList': [0.0, -1.356512307524681, 0.0, 67.63560297012329, 0.0, 0.0, 0.0], 'rewardMean': 0.775843445167401, 'totalEpisodes': 158, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.841125951767534
'totalSteps': 7680, 'rewardStep': 0.8836046822093977, 'errorList': [], 'lossList': [0.0, -1.35567679643631, 0.0, 62.00221160888672, 0.0, 0.0, 0.0], 'rewardMean': 0.793803651341067, 'totalEpisodes': 181, 'stepsPerEpisode': 57, 'rewardPerEpisode': 44.76742333360682
'totalSteps': 8960, 'rewardStep': 0.7276210319201231, 'errorList': [], 'lossList': [0.0, -1.3623710638284683, 0.0, 59.39629316329956, 0.0, 0.0, 0.0], 'rewardMean': 0.7843489914237894, 'totalEpisodes': 194, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.225039625748128
'totalSteps': 10240, 'rewardStep': 0.7385458945859885, 'errorList': [], 'lossList': [0.0, -1.3729362481832503, 0.0, 43.96155893802643, 0.0, 0.0, 0.0], 'rewardMean': 0.7786236043190642, 'totalEpisodes': 201, 'stepsPerEpisode': 36, 'rewardPerEpisode': 25.24259294519546
'totalSteps': 11520, 'rewardStep': 0.8837273204643218, 'errorList': [], 'lossList': [0.0, -1.3772241634130478, 0.0, 38.14065350532532, 0.0, 0.0, 0.0], 'rewardMean': 0.7903017950018706, 'totalEpisodes': 209, 'stepsPerEpisode': 94, 'rewardPerEpisode': 80.49410678091327
'totalSteps': 12800, 'rewardStep': 0.7082510200106844, 'errorList': [], 'lossList': [0.0, -1.3749904596805573, 0.0, 40.73717140197754, 0.0, 0.0, 0.0], 'rewardMean': 0.782096717502752, 'totalEpisodes': 218, 'stepsPerEpisode': 186, 'rewardPerEpisode': 145.75084129137826
'totalSteps': 14080, 'rewardStep': 0.49377886374795465, 'errorList': [], 'lossList': [0.0, -1.3905549782514572, 0.0, 10.505536256432533, 0.0, 0.0, 0.0], 'rewardMean': 0.7499108696991765, 'totalEpisodes': 221, 'stepsPerEpisode': 366, 'rewardPerEpisode': 238.45931450025378
'totalSteps': 15360, 'rewardStep': 0.8733696306250178, 'errorList': [], 'lossList': [0.0, -1.4056199169158936, 0.0, 17.197799974679945, 0.0, 0.0, 0.0], 'rewardMean': 0.7858050839467532, 'totalEpisodes': 222, 'stepsPerEpisode': 1140, 'rewardPerEpisode': 882.568303035426
'totalSteps': 16640, 'rewardStep': 0.9084687956195334, 'errorList': [], 'lossList': [0.0, -1.398820847272873, 0.0, 6.371913533508778, 0.0, 0.0, 0.0], 'rewardMean': 0.7835296518765229, 'totalEpisodes': 223, 'stepsPerEpisode': 69, 'rewardPerEpisode': 57.96987435035245
'totalSteps': 17920, 'rewardStep': 0.8027296926523239, 'errorList': [], 'lossList': [0.0, -1.373606976866722, 0.0, 2.8186616916954517, 0.0, 0.0, 0.0], 'rewardMean': 0.780996993587418, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1021.0398858276778
'totalSteps': 19200, 'rewardStep': 0.8882397788001962, 'errorList': [], 'lossList': [0.0, -1.3268228882551194, 0.0, 2.7267114613205194, 0.0, 0.0, 0.0], 'rewardMean': 0.7908336710635542, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.035076763468
'totalSteps': 20480, 'rewardStep': 0.9096999027434186, 'errorList': [], 'lossList': [0.0, -1.2791418677568436, 0.0, 3.059658760689199, 0.0, 0.0, 0.0], 'rewardMean': 0.7934431931169562, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.8123748190474
'totalSteps': 21760, 'rewardStep': 0.9578803215079724, 'errorList': [0.1736566787189641, 0.11569572707446492, 0.10544845836507417, 0.08931840390539002, 0.1685477389689263, 0.09511632353085715, 0.14884530705441126, 0.2183107913726112, 0.16047132711174508, 0.06790189815545007, 0.17678764242538508, 0.25016620849594673, 0.09841697006483133, 0.2156206820298386, 0.14619254639583315, 0.15873378651526315, 0.1716433116899972, 0.06814710163305125, 0.17518132699649713, 0.1639095442074969, 0.1400840247539013, 0.16654867831644682, 0.14627888518541277, 0.14557544879974507, 0.07659743507504915, 0.11881855284063526, 0.13309153950894775, 0.15278436031595943, 0.1364506510696928, 0.13585722346642995, 0.12553692513147205, 0.11201593062906787, 0.1328551963291861, 0.15123412252140653, 0.08587569740750856, 0.126746315407278, 0.16863797158849955, 0.18779846820580975, 0.3086334566604477, 0.10817860284014018, 0.10217204294464131, 0.1934737468370063, 0.1737132323175382, 0.17888233710542437, 0.13169790263217254, 0.22987901714971662, 0.2048616835337587, 0.0667469472355739, 0.0966312661048355, 0.1947723573028816], 'lossList': [0.0, -1.2130190771818161, 0.0, 1.9006524899229407, 0.0, 0.0, 0.0], 'rewardMean': 0.8164691220757412, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.6447031657037, 'successfulTests': 44
'totalSteps': 23040, 'rewardStep': 0.9646951849669674, 'errorList': [0.16886030931971444, 0.14744786423483408, 0.08095393529259189, 0.13541435665782803, 0.09446751693158592, 0.07445398670832482, 0.1296938526085947, 0.16400249055782148, 0.1321266887292318, 0.1296205366196785, 0.1331368206991196, 0.24297275674186358, 0.08607396836204938, 0.09252371996254978, 0.1533932082223957, 0.22850949733026155, 0.1417733251063954, 0.32257749522473295, 0.21255195028938983, 0.21405921901814576, 0.2676822395464168, 0.0924088739600756, 0.11977750260196836, 0.2185080684700631, 0.1425915076510752, 0.16126695839052235, 0.09680205807621395, 0.16963251529686424, 0.12641554505055053, 0.14541300517644562, 0.0730450046689246, 0.12075795695839074, 0.11664227617542491, 0.11042244496798205, 0.07648851212816077, 0.13717878217335178, 0.0637103026600623, 0.17653634549364522, 0.1030102472499241, 0.18594115080484133, 0.17146872975486235, 0.08669049812088696, 0.21078035241756374, 0.127053872294711, 0.20262891254603485, 0.09520511031495411, 0.24966284792965052, 0.32087318573649576, 0.09876218523563907, 0.1596918173980357], 'lossList': [0.0, -1.1608182102441789, 0.0, 1.2061058633495123, 0.0, 0.0, 0.0], 'rewardMean': 0.8390840511138391, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.8344423316728, 'successfulTests': 39
'totalSteps': 24320, 'rewardStep': 0.937886180829593, 'errorList': [0.0796999051275338, 0.13274501363636518, 0.09017455524155821, 0.05011018271101976, 0.08241553275242626, 0.04681009810445244, 0.05192618954686885, 0.17101808387617773, 0.10271359419870607, 0.07769211359004145, 0.09118437860866206, 0.046045103504569466, 0.11073113253072064, 0.042731274242580206, 0.04303393469525022, 0.05806634259239884, 0.11203353811244562, 0.04800292309518534, 0.10625165584392453, 0.05531679500494927, 0.1144837200868311, 0.08283183943537716, 0.09267129798029299, 0.10842035390422444, 0.08167594022905147, 0.06607354456401311, 0.07106885549234704, 0.09954337424427295, 0.12410523041114382, 0.05541181882490656, 0.06990049114695716, 0.05789903634085695, 0.06969682066677116, 0.05182802368028151, 0.07570289790865999, 0.05616833464530596, 0.14759856342972563, 0.06036046853065119, 0.0973602439661198, 0.13905680530920272, 0.07897908296005897, 0.10286926614514653, 0.1195150017931665, 0.12202202077930356, 0.07903110682993425, 0.07108942327526431, 0.18065591461103614, 0.07186066105929838, 0.06015800485349286, 0.08731615234851238], 'lossList': [0.0, -1.1274122428894042, 0.0, 0.8248325550556183, 0.0, 0.0, 0.0], 'rewardMean': 0.8444999371503663, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.9791382256694, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=102.41
