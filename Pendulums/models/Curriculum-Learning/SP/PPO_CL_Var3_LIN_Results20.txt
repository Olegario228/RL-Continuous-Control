#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 20
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.8811285410424418, 'errorList': [], 'lossList': [0.0, -1.4396232169866563, 0.0, 28.9915977114439, 0.0, 0.0, 0.0], 'rewardMean': 0.8979745858116349, 'totalEpisodes': 8, 'stepsPerEpisode': 532, 'rewardPerEpisode': 368.645241564048
'totalSteps': 3840, 'rewardStep': 0.6761040072214932, 'errorList': [], 'lossList': [0.0, -1.4277788150310515, 0.0, 31.95155659198761, 0.0, 0.0, 0.0], 'rewardMean': 0.8240177262815876, 'totalEpisodes': 14, 'stepsPerEpisode': 154, 'rewardPerEpisode': 127.236604231561
'totalSteps': 5120, 'rewardStep': 0.7506144231115193, 'errorList': [], 'lossList': [0.0, -1.4268606716394425, 0.0, 91.10432189941406, 0.0, 0.0, 0.0], 'rewardMean': 0.8056669004890705, 'totalEpisodes': 34, 'stepsPerEpisode': 105, 'rewardPerEpisode': 79.30122604798494
'totalSteps': 6400, 'rewardStep': 0.5339638096250764, 'errorList': [], 'lossList': [0.0, -1.4353861528635026, 0.0, 122.68829761505127, 0.0, 0.0, 0.0], 'rewardMean': 0.7513262823162716, 'totalEpisodes': 81, 'stepsPerEpisode': 33, 'rewardPerEpisode': 23.24503417996022
'totalSteps': 7680, 'rewardStep': 0.6795100431621959, 'errorList': [], 'lossList': [0.0, -1.4260624563694, 0.0, 86.01105302810669, 0.0, 0.0, 0.0], 'rewardMean': 0.7393569091239257, 'totalEpisodes': 120, 'stepsPerEpisode': 13, 'rewardPerEpisode': 7.969108504901881
'totalSteps': 8960, 'rewardStep': 0.8730551536729708, 'errorList': [], 'lossList': [0.0, -1.4060645169019699, 0.0, 59.38368348121643, 0.0, 0.0, 0.0], 'rewardMean': 0.7584566583452179, 'totalEpisodes': 139, 'stepsPerEpisode': 101, 'rewardPerEpisode': 86.78081782853532
'totalSteps': 10240, 'rewardStep': 0.5774492894801527, 'errorList': [], 'lossList': [0.0, -1.38120747923851, 0.0, 50.79333576202393, 0.0, 0.0, 0.0], 'rewardMean': 0.7358307372370848, 'totalEpisodes': 152, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.13115304459434
'totalSteps': 11520, 'rewardStep': 0.7457428780317261, 'errorList': [], 'lossList': [0.0, -1.3595892524719237, 0.0, 44.3448299407959, 0.0, 0.0, 0.0], 'rewardMean': 0.7369320862142672, 'totalEpisodes': 164, 'stepsPerEpisode': 44, 'rewardPerEpisode': 36.3750082017128
'totalSteps': 12800, 'rewardStep': 0.7989431971496747, 'errorList': [], 'lossList': [0.0, -1.3533653789758682, 0.0, 18.45084855914116, 0.0, 0.0, 0.0], 'rewardMean': 0.7431331973078079, 'totalEpisodes': 168, 'stepsPerEpisode': 383, 'rewardPerEpisode': 315.2377289472065
'totalSteps': 14080, 'rewardStep': 0.4812939349856657, 'errorList': [], 'lossList': [0.0, -1.360466356277466, 0.0, 20.21466366171837, 0.0, 0.0, 0.0], 'rewardMean': 0.6997805277482916, 'totalEpisodes': 171, 'stepsPerEpisode': 213, 'rewardPerEpisode': 138.56206981561175
'totalSteps': 15360, 'rewardStep': 0.7421255677601137, 'errorList': [], 'lossList': [0.0, -1.3475362795591355, 0.0, 20.289127066135407, 0.0, 0.0, 0.0], 'rewardMean': 0.6858802304200589, 'totalEpisodes': 176, 'stepsPerEpisode': 69, 'rewardPerEpisode': 54.92688923358837
'totalSteps': 16640, 'rewardStep': 0.8275651778280888, 'errorList': [], 'lossList': [0.0, -1.3478765267133712, 0.0, 11.122804378271104, 0.0, 0.0, 0.0], 'rewardMean': 0.7010263474807185, 'totalEpisodes': 179, 'stepsPerEpisode': 77, 'rewardPerEpisode': 60.61494916675835
'totalSteps': 17920, 'rewardStep': 0.86937529369668, 'errorList': [], 'lossList': [0.0, -1.3514486813545228, 0.0, 4.9584650319814685, 0.0, 0.0, 0.0], 'rewardMean': 0.7129024345392345, 'totalEpisodes': 180, 'stepsPerEpisode': 418, 'rewardPerEpisode': 329.83805480888896
'totalSteps': 19200, 'rewardStep': 0.8780965764895597, 'errorList': [], 'lossList': [0.0, -1.343929334282875, 0.0, 28.033891875743866, 0.0, 0.0, 0.0], 'rewardMean': 0.7473157112256827, 'totalEpisodes': 181, 'stepsPerEpisode': 819, 'rewardPerEpisode': 688.6687325413416
'totalSteps': 20480, 'rewardStep': 0.9501116325320288, 'errorList': [0.07736110086463485, 0.08032436202683972, 0.14363205956100913, 0.16058551988621245, 0.09586804996952686, 0.08804566126403943, 0.09593510006593438, 0.11293058004939352, 0.07213236269137166, 0.09849468226697432, 0.17200690873837815, 0.11061421231168629, 0.09328658000246702, 0.2953492044529084, 0.1247004761062808, 0.0974727496737373, 0.0642913469161077, 0.07451029598378567, 0.08648263242326894, 0.17466691291267167, 0.3119591406941161, 0.11466900234927034, 0.08801664103812636, 0.09283969400381119, 0.07187790012636347, 0.18478601521984797, 0.14880015835686902, 0.11325644551800322, 0.07854130361652427, 0.08360113860713245, 0.15033144937115855, 0.10072236587600793, 0.1321116155820349, 0.07845586174117257, 0.084384209494138, 0.08269635802811043, 0.1646140299880514, 0.10813513711896622, 0.14905516892215495, 0.1524990391897314, 0.13328457648693084, 0.07761357024223613, 0.20283857672795294, 0.1229257829726588, 0.2380140379640793, 0.09532061500933686, 0.12528214221962602, 0.14234719389854633, 0.08578785208909213, 0.12026477123413506], 'lossList': [0.0, -1.3500025832653046, 0.0, 3.648087744563818, 0.0, 0.0, 0.0], 'rewardMean': 0.7743758701626662, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1062.2146192478358, 'successfulTests': 46
'totalSteps': 21760, 'rewardStep': 0.9038338361991549, 'errorList': [], 'lossList': [0.0, -1.3570345759391784, 0.0, 2.320685923397541, 0.0, 0.0, 0.0], 'rewardMean': 0.7774537384152846, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.8416236956527
'totalSteps': 23040, 'rewardStep': 0.7729415372025987, 'errorList': [], 'lossList': [0.0, -1.344019911289215, 0.0, 1.8005748942494393, 0.0, 0.0, 0.0], 'rewardMean': 0.7970029631875291, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.844517302076
'totalSteps': 24320, 'rewardStep': 0.8039239486540182, 'errorList': [], 'lossList': [0.0, -1.3078470313549042, 0.0, 1.6284933785349132, 0.0, 0.0, 0.0], 'rewardMean': 0.8028210702497584, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.906215734303
'totalSteps': 25600, 'rewardStep': 0.956621254991607, 'errorList': [0.1130021822365816, 0.04716568543673137, 0.07515480614635629, 0.022221881749048236, 0.054533859867701096, 0.09845719592069019, 0.044016682099010385, 0.0688486347818191, 0.02895308343880244, 0.03448356194903047, 0.07912941854746121, 0.060940447580147834, 0.07857227100661132, 0.02718660044462309, 0.021573884537174414, 0.0285409422037431, 0.10298336301071512, 0.02757546860484027, 0.06669507750128716, 0.050048271743327856, 0.04190971573631643, 0.031855959926682394, 0.03661840785310687, 0.022932389648633155, 0.1292825725350638, 0.06164177106407217, 0.09047180421330471, 0.07551233873667951, 0.06895543172440124, 0.03659497608887895, 0.04265560723686411, 0.04697148875708283, 0.0411815611921093, 0.06219115243972699, 0.05897433032614329, 0.021482318138499528, 0.08817163577444648, 0.04157105934464995, 0.046001888756253606, 0.036339694781524975, 0.028729897538354178, 0.016854240937975804, 0.039852913001842016, 0.024661856941699204, 0.04459574934177927, 0.04452018749985358, 0.03842322612403773, 0.03477158492503747, 0.0400374133329885, 0.05623055148135155], 'lossList': [0.0, -1.2692292445898057, 0.0, 1.2022729555144906, 0.0, 0.0, 0.0], 'rewardMean': 0.8185888760339516, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.889270916565, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=91.18
