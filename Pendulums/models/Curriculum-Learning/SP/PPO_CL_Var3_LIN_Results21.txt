#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 21
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8045002305373588, 'errorList': [], 'lossList': [0.0, -1.451068400144577, 0.0, 35.90864181995392, 0.0, 0.0, 0.0], 'rewardMean': 0.6988390250587591, 'totalEpisodes': 12, 'stepsPerEpisode': 79, 'rewardPerEpisode': 69.95846012917237
'totalSteps': 3840, 'rewardStep': 0.8639347730491397, 'errorList': [], 'lossList': [0.0, -1.4668833833932877, 0.0, 25.94471173644066, 0.0, 0.0, 0.0], 'rewardMean': 0.7538709410555526, 'totalEpisodes': 13, 'stepsPerEpisode': 1262, 'rewardPerEpisode': 908.1494972158861
'totalSteps': 5120, 'rewardStep': 0.8509851348887402, 'errorList': [], 'lossList': [0.0, -1.4455818885564804, 0.0, 67.42268798828125, 0.0, 0.0, 0.0], 'rewardMean': 0.7781494895138495, 'totalEpisodes': 26, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.845331286104265
'totalSteps': 6400, 'rewardStep': 0.6979823627710244, 'errorList': [], 'lossList': [0.0, -1.450538346171379, 0.0, 178.00920211791993, 0.0, 0.0, 0.0], 'rewardMean': 0.7621160641652845, 'totalEpisodes': 88, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.91470820525319
'totalSteps': 7680, 'rewardStep': 0.35447309725554177, 'errorList': [], 'lossList': [0.0, -1.4341351294517517, 0.0, 61.564502258300784, 0.0, 0.0, 0.0], 'rewardMean': 0.6941755696803273, 'totalEpisodes': 123, 'stepsPerEpisode': 53, 'rewardPerEpisode': 31.51748410782169
'totalSteps': 8960, 'rewardStep': 0.4808059325845085, 'errorList': [], 'lossList': [0.0, -1.4150003206729889, 0.0, 47.38118883132935, 0.0, 0.0, 0.0], 'rewardMean': 0.6636941929523532, 'totalEpisodes': 165, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.34132243221375
'totalSteps': 10240, 'rewardStep': 0.8526401301409183, 'errorList': [], 'lossList': [0.0, -1.398695398569107, 0.0, 22.983831191062926, 0.0, 0.0, 0.0], 'rewardMean': 0.6873124351009239, 'totalEpisodes': 178, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.00814791648314
'totalSteps': 11520, 'rewardStep': 0.6694280515821533, 'errorList': [], 'lossList': [0.0, -1.3764969784021377, 0.0, 29.759309997558592, 0.0, 0.0, 0.0], 'rewardMean': 0.6853252813766161, 'totalEpisodes': 186, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.923836306010578
'totalSteps': 12800, 'rewardStep': 0.688178737579378, 'errorList': [], 'lossList': [0.0, -1.3465970277786254, 0.0, 18.14598027944565, 0.0, 0.0, 0.0], 'rewardMean': 0.6856106269968922, 'totalEpisodes': 193, 'stepsPerEpisode': 84, 'rewardPerEpisode': 63.68750130230329
'totalSteps': 14080, 'rewardStep': 0.7649858106630771, 'errorList': [], 'lossList': [0.0, -1.3166822576522828, 0.0, 12.94372897028923, 0.0, 0.0, 0.0], 'rewardMean': 0.7027914261051841, 'totalEpisodes': 196, 'stepsPerEpisode': 157, 'rewardPerEpisode': 137.45702956277708
'totalSteps': 15360, 'rewardStep': 0.6815288766848242, 'errorList': [], 'lossList': [0.0, -1.2879151844978332, 0.0, 7.495628054738045, 0.0, 0.0, 0.0], 'rewardMean': 0.6904942907199306, 'totalEpisodes': 198, 'stepsPerEpisode': 604, 'rewardPerEpisode': 460.1059927499638
'totalSteps': 16640, 'rewardStep': 0.6100580853887114, 'errorList': [], 'lossList': [0.0, -1.2983888119459153, 0.0, 6.670313349366188, 0.0, 0.0, 0.0], 'rewardMean': 0.6651066219538878, 'totalEpisodes': 200, 'stepsPerEpisode': 360, 'rewardPerEpisode': 275.125010011432
'totalSteps': 17920, 'rewardStep': 0.6285132956578994, 'errorList': [], 'lossList': [0.0, -1.2993004930019378, 0.0, 4.45768626332283, 0.0, 0.0, 0.0], 'rewardMean': 0.6428594380308038, 'totalEpisodes': 202, 'stepsPerEpisode': 415, 'rewardPerEpisode': 322.12370544725377
'totalSteps': 19200, 'rewardStep': 0.9683523204977347, 'errorList': [0.21449997925588135, 0.26046888516512423, 0.21657850586575464, 0.2155043409746784, 0.22905912314604973, 0.21601641004991137, 0.21314760208162242, 0.2520470638602631, 0.2232818168513925, 0.2163398910951115, 0.2286720388559715, 0.2110744958880553, 0.21518653407136826, 0.2126459160274213, 0.2116183119855272, 0.2157548963447629, 0.2116996551356048, 0.21233239178938923, 0.24020395217985863, 0.21444423765529624, 0.21588413095953818, 0.22501473832888017, 0.21118945098064873, 0.21311473159254293, 0.21401393574755673, 0.22466710555769226, 0.21288166119088073, 0.21608441779464888, 0.22370493492846685, 0.2150526947230469, 0.21502648825948661, 0.2162367325389368, 0.21508340035301254, 0.2094131623529999, 0.21623557089120207, 0.23365512909016756, 0.22234744943262563, 0.21405590399674604, 0.21272846525778957, 0.21096183503608482, 0.21063709434230043, 0.21234386555202325, 0.21249935246402576, 0.21539853299907127, 0.21395836612052987, 0.21582142903089493, 0.23880801935515197, 0.21230475585707956, 0.21504063941882734, 0.213204670541653], 'lossList': [0.0, -1.2613274937868117, 0.0, 3.6717949840426445, 0.0, 0.0, 0.0], 'rewardMean': 0.6698964338034747, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.6446197158266, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.6678892048778624, 'errorList': [], 'lossList': [0.0, -1.2135513675212861, 0.0, 2.271073795258999, 0.0, 0.0, 0.0], 'rewardMean': 0.7012380445657067, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.4698298717851
'totalSteps': 21760, 'rewardStep': 0.7806233577248183, 'errorList': [], 'lossList': [0.0, -1.1626839888095857, 0.0, 1.7379774213582277, 0.0, 0.0, 0.0], 'rewardMean': 0.7312197870797378, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1095.7284930230128
'totalSteps': 23040, 'rewardStep': 0.9614490045576825, 'errorList': [0.057853926613139185, 0.04526696373705742, 0.025967002327075727, 0.04129350288310506, 0.04597574640243155, 0.03567877282472415, 0.0238794903563839, 0.030298403629541684, 0.042859617122817814, 0.01112122965735697, 0.013043432588796113, 0.025702463881926887, 0.013259233541140576, 0.02082487713800651, 0.022237991918846088, 0.027831684288527662, 0.021011762269949462, 0.011860435161904999, 0.023683613899818347, 0.028259970847739117, 0.024525667456569866, 0.04586266275940433, 0.022860541611934616, 0.022860839654334834, 0.015929646325250693, 0.018730703813185017, 0.01718037123246796, 0.020726258548972035, 0.02888665258776108, 0.02012859248174241, 0.027788354605850024, 0.03988272122688509, 0.01383866743140911, 0.02925455137414192, 0.02211860654221653, 0.033496988819568266, 0.03757611207810916, 0.03150921018722027, 0.017942691686250255, 0.054392713700463155, 0.026814183149642096, 0.014005564597424594, 0.02698943743487299, 0.013996876344586873, 0.01459405517888726, 0.04696544561778397, 0.049464737451610154, 0.02960807886407092, 0.0718466094581863, 0.04357936203494166], 'lossList': [0.0, -1.1306081348657608, 0.0, 1.6985152998194097, 0.0, 0.0, 0.0], 'rewardMean': 0.742100674521414, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.3447695250948, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=86.26
