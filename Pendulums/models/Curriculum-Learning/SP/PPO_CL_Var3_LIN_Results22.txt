#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 22
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.7058913060032233, 'errorList': [], 'lossList': [0.0, -1.4543324369192123, 0.0, 32.33618528366089, 0.0, 0.0, 0.0], 'rewardMean': 0.6044960769536175, 'totalEpisodes': 14, 'stepsPerEpisode': 290, 'rewardPerEpisode': 225.29928752851507
'totalSteps': 3840, 'rewardStep': 0.9563353569184485, 'errorList': [], 'lossList': [0.0, -1.460465134382248, 0.0, 30.015534427165985, 0.0, 0.0, 0.0], 'rewardMean': 0.7217758369418945, 'totalEpisodes': 19, 'stepsPerEpisode': 75, 'rewardPerEpisode': 53.46992793754201
'totalSteps': 5120, 'rewardStep': 0.5963261799632678, 'errorList': [], 'lossList': [0.0, -1.4352599251270295, 0.0, 78.12508947372436, 0.0, 0.0, 0.0], 'rewardMean': 0.6904134226972378, 'totalEpisodes': 38, 'stepsPerEpisode': 53, 'rewardPerEpisode': 34.11762637861062
'totalSteps': 6400, 'rewardStep': 0.6242918209523443, 'errorList': [], 'lossList': [0.0, -1.4408589935302734, 0.0, 128.45669258117675, 0.0, 0.0, 0.0], 'rewardMean': 0.6771891023482591, 'totalEpisodes': 95, 'stepsPerEpisode': 24, 'rewardPerEpisode': 21.77604418375382
'totalSteps': 7680, 'rewardStep': 0.43689754578413065, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6085343719013653, 'totalEpisodes': 136, 'stepsPerEpisode': 22, 'rewardPerEpisode': 13.992641706214338
'totalSteps': 8960, 'rewardStep': 0.8679001461791895, 'errorList': [], 'lossList': [0.0, -1.4261501282453537, 0.0, 73.23583349227906, 0.0, 0.0, 0.0], 'rewardMean': 0.6409550936860933, 'totalEpisodes': 180, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.502408480409969
'totalSteps': 10240, 'rewardStep': 0.6489016838832093, 'errorList': [], 'lossList': [0.0, -1.416460666656494, 0.0, 44.52141547203064, 0.0, 0.0, 0.0], 'rewardMean': 0.6418380481524395, 'totalEpisodes': 197, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.83836109910801
'totalSteps': 11520, 'rewardStep': 0.7732004263438921, 'errorList': [], 'lossList': [0.0, -1.4067424190044404, 0.0, 41.07208996772766, 0.0, 0.0, 0.0], 'rewardMean': 0.6549742859715847, 'totalEpisodes': 214, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.905569459164095
'totalSteps': 12800, 'rewardStep': 0.8194966032334084, 'errorList': [], 'lossList': [0.0, -1.40367924451828, 0.0, 33.67775025844574, 0.0, 0.0, 0.0], 'rewardMean': 0.6866138615045244, 'totalEpisodes': 228, 'stepsPerEpisode': 67, 'rewardPerEpisode': 56.70874835795878
'totalSteps': 14080, 'rewardStep': 0.7478372876778971, 'errorList': [], 'lossList': [0.0, -1.4202810949087143, 0.0, 10.17444379925728, 0.0, 0.0, 0.0], 'rewardMean': 0.6908084596719919, 'totalEpisodes': 235, 'stepsPerEpisode': 46, 'rewardPerEpisode': 36.65189874831577
'totalSteps': 15360, 'rewardStep': 0.8937279758553799, 'errorList': [], 'lossList': [0.0, -1.4145941978693009, 0.0, 21.550904393196106, 0.0, 0.0, 0.0], 'rewardMean': 0.684547721565685, 'totalEpisodes': 242, 'stepsPerEpisode': 39, 'rewardPerEpisode': 34.303338273676864
'totalSteps': 16640, 'rewardStep': 0.4477990859513308, 'errorList': [], 'lossList': [0.0, -1.394678236246109, 0.0, 5.9558039021492, 0.0, 0.0, 0.0], 'rewardMean': 0.6696950121644912, 'totalEpisodes': 247, 'stepsPerEpisode': 252, 'rewardPerEpisode': 174.8088161419433
'totalSteps': 17920, 'rewardStep': 0.41996740913340375, 'errorList': [], 'lossList': [0.0, -1.3878179568052291, 0.0, 6.298084969520569, 0.0, 0.0, 0.0], 'rewardMean': 0.6492625709825972, 'totalEpisodes': 252, 'stepsPerEpisode': 173, 'rewardPerEpisode': 141.06090470172492
'totalSteps': 19200, 'rewardStep': 0.8438537831379495, 'errorList': [], 'lossList': [0.0, -1.3861710274219512, 0.0, 21.1733242225647, 0.0, 0.0, 0.0], 'rewardMean': 0.689958194717979, 'totalEpisodes': 259, 'stepsPerEpisode': 75, 'rewardPerEpisode': 68.17318839939882
'totalSteps': 20480, 'rewardStep': 0.5951220721136705, 'errorList': [], 'lossList': [0.0, -1.3736557847261428, 0.0, 6.198828611969947, 0.0, 0.0, 0.0], 'rewardMean': 0.705780647350933, 'totalEpisodes': 262, 'stepsPerEpisode': 444, 'rewardPerEpisode': 361.67746526357155
'totalSteps': 21760, 'rewardStep': 0.751510703876704, 'errorList': [], 'lossList': [0.0, -1.3590017759799957, 0.0, 9.009486780166625, 0.0, 0.0, 0.0], 'rewardMean': 0.6941417031206846, 'totalEpisodes': 266, 'stepsPerEpisode': 285, 'rewardPerEpisode': 246.80571024687248
'totalSteps': 23040, 'rewardStep': 0.8153297850031639, 'errorList': [], 'lossList': [0.0, -1.350946918129921, 0.0, 2.716265993118286, 0.0, 0.0, 0.0], 'rewardMean': 0.71078451323268, 'totalEpisodes': 269, 'stepsPerEpisode': 216, 'rewardPerEpisode': 189.18030361964023
'totalSteps': 24320, 'rewardStep': 0.952593959322648, 'errorList': [2.3633646739955223, 0.6068642206890494, 3.187499790024548, 2.4351186858294698, 3.2650926779312943, 0.8430283296684382, 0.6358411157742964, 0.6773423490327118, 1.963531510262208, 3.603392785998925, 0.6789077866670896, 7.7803188168391095, 7.428416243107676, 3.547761576124051, 6.919329437691594, 0.7252313115689853, 11.175578569362566, 0.31333775999125735, 3.384922823987619, 5.752958297291889, 4.514279008897646, 1.1347881723667312, 0.2187441894662934, 2.1273183922139673, 7.415721804029998, 0.9917190992585718, 0.9212863107047494, 1.941911469428895, 0.5436263515834521, 0.45764963369414696, 2.707109052441661, 1.3425718423140474, 1.2769563406049784, 1.3237133114992792, 2.5258704567003445, 1.3961919636604454, 1.9876168227202529, 1.615048755522687, 6.020390945022273, 0.8472787704258637, 2.0981541725572606, 0.17339472460414815, 0.535608804769271, 4.337442277160171, 1.3078479366905342, 1.5026355734722625, 1.235106524861807, 0.18911193744041324, 2.6790261652987684, 0.9016643148724456], 'lossList': [0.0, -1.3403955966234207, 0.0, 2.01195366114378, 0.0, 0.0, 0.0], 'rewardMean': 0.7287238665305555, 'totalEpisodes': 271, 'stepsPerEpisode': 480, 'rewardPerEpisode': 429.1459832521616, 'successfulTests': 2
'totalSteps': 25600, 'rewardStep': 0.8712724458860727, 'errorList': [], 'lossList': [0.0, -1.3347070014476776, 0.0, 1.6979190972447396, 0.0, 0.0, 0.0], 'rewardMean': 0.733901450795822, 'totalEpisodes': 273, 'stepsPerEpisode': 248, 'rewardPerEpisode': 220.94314857479324
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=24320, timeSpent=72.1
