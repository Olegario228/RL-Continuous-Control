#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 23
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.4911566296722812, 'errorList': [], 'lossList': [0.0, -1.4240125620365143, 0.0, 27.006003971099855, 0.0, 0.0, 0.0], 'rewardMean': 0.6950435227495108, 'totalEpisodes': 16, 'stepsPerEpisode': 46, 'rewardPerEpisode': 30.081447557783658
'totalSteps': 3840, 'rewardStep': 0.9404153767902769, 'errorList': [], 'lossList': [0.0, -1.4230389720201493, 0.0, 28.44238904118538, 0.0, 0.0, 0.0], 'rewardMean': 0.7768341407630994, 'totalEpisodes': 20, 'stepsPerEpisode': 487, 'rewardPerEpisode': 354.7600812708647
'totalSteps': 5120, 'rewardStep': 0.7199714501155031, 'errorList': [], 'lossList': [0.0, -1.4119698721170426, 0.0, 80.8540849685669, 0.0, 0.0, 0.0], 'rewardMean': 0.7626184681012004, 'totalEpisodes': 36, 'stepsPerEpisode': 76, 'rewardPerEpisode': 65.86882466850255
'totalSteps': 6400, 'rewardStep': 0.6717900517251765, 'errorList': [], 'lossList': [0.0, -1.4065787309408189, 0.0, 153.22517116546632, 0.0, 0.0, 0.0], 'rewardMean': 0.7444527848259955, 'totalEpisodes': 95, 'stepsPerEpisode': 9, 'rewardPerEpisode': 4.784874460612146
'totalSteps': 7680, 'rewardStep': 0.7686664414812622, 'errorList': [], 'lossList': [0.0, -1.396375790834427, 0.0, 83.73818218231202, 0.0, 0.0, 0.0], 'rewardMean': 0.74848839426854, 'totalEpisodes': 142, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.357746856702967
'totalSteps': 8960, 'rewardStep': 0.7356917633134553, 'errorList': [], 'lossList': [0.0, -1.3843863308429718, 0.0, 57.92744834899902, 0.0, 0.0, 0.0], 'rewardMean': 0.7466603041320993, 'totalEpisodes': 185, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.766403346645056
'totalSteps': 10240, 'rewardStep': 0.6408385826169243, 'errorList': [], 'lossList': [0.0, -1.3658699578046798, 0.0, 39.833046960830686, 0.0, 0.0, 0.0], 'rewardMean': 0.7334325889427025, 'totalEpisodes': 200, 'stepsPerEpisode': 43, 'rewardPerEpisode': 28.068377451144936
'totalSteps': 11520, 'rewardStep': 0.8183884837554039, 'errorList': [], 'lossList': [0.0, -1.342881892323494, 0.0, 25.52185736179352, 0.0, 0.0, 0.0], 'rewardMean': 0.7428721328107805, 'totalEpisodes': 210, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.317865703905637
'totalSteps': 12800, 'rewardStep': 0.7472571583344907, 'errorList': [], 'lossList': [0.0, -1.3253408068418502, 0.0, 22.042737209796904, 0.0, 0.0, 0.0], 'rewardMean': 0.7433106353631515, 'totalEpisodes': 215, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.0710610309498
'totalSteps': 14080, 'rewardStep': 0.6395844596434949, 'errorList': [], 'lossList': [0.0, -1.3178070384263991, 0.0, 11.932648907899857, 0.0, 0.0, 0.0], 'rewardMean': 0.7173760397448269, 'totalEpisodes': 218, 'stepsPerEpisode': 349, 'rewardPerEpisode': 267.3136918407471
'totalSteps': 15360, 'rewardStep': 0.3967223590662586, 'errorList': [], 'lossList': [0.0, -1.314014630317688, 0.0, 8.23147581577301, 0.0, 0.0, 0.0], 'rewardMean': 0.7079326126842247, 'totalEpisodes': 221, 'stepsPerEpisode': 404, 'rewardPerEpisode': 311.7602340830559
'totalSteps': 16640, 'rewardStep': 0.6333629993935135, 'errorList': [], 'lossList': [0.0, -1.2989133650064468, 0.0, 6.483373140096664, 0.0, 0.0, 0.0], 'rewardMean': 0.6772273749445483, 'totalEpisodes': 224, 'stepsPerEpisode': 298, 'rewardPerEpisode': 233.9736860809637
'totalSteps': 17920, 'rewardStep': 0.7255554239677089, 'errorList': [], 'lossList': [0.0, -1.2754471427202225, 0.0, 4.025269457399845, 0.0, 0.0, 0.0], 'rewardMean': 0.6777857723297689, 'totalEpisodes': 225, 'stepsPerEpisode': 1040, 'rewardPerEpisode': 836.4534357822814
'totalSteps': 19200, 'rewardStep': 0.8160730608345796, 'errorList': [], 'lossList': [0.0, -1.2521555191278457, 0.0, 2.8141049689054487, 0.0, 0.0, 0.0], 'rewardMean': 0.6922140732407092, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.9787778351357
'totalSteps': 20480, 'rewardStep': 0.9103510694507216, 'errorList': [], 'lossList': [0.0, -1.2149369192123414, 0.0, 3.3547213915735483, 0.0, 0.0, 0.0], 'rewardMean': 0.7063825360376551, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.8037961216482
'totalSteps': 21760, 'rewardStep': 0.9284021696665181, 'errorList': [], 'lossList': [0.0, -1.1762554413080215, 0.0, 2.1312641075626018, 0.0, 0.0, 0.0], 'rewardMean': 0.7256535766729615, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.5560995048402
'totalSteps': 23040, 'rewardStep': 0.8674268229157129, 'errorList': [], 'lossList': [0.0, -1.149567764401436, 0.0, 1.928493918441236, 0.0, 0.0, 0.0], 'rewardMean': 0.7483124007028403, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.0611335419064
'totalSteps': 24320, 'rewardStep': 0.898287235940811, 'errorList': [], 'lossList': [0.0, -1.094604678750038, 0.0, 1.1708593138679861, 0.0, 0.0, 0.0], 'rewardMean': 0.7563022759213809, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.6130805078235
'totalSteps': 25600, 'rewardStep': 0.962001027554234, 'errorList': [0.11015288572974616, 0.060875698152699734, 0.0552310489102206, 0.07957292502912947, 0.1023085448476083, 0.059795545598291165, 0.11387878643097173, 0.054819324354062574, 0.09221676151908308, 0.0902671874252363, 0.11603513532851388, 0.06109021535483147, 0.060301691399506654, 0.1267463977446607, 0.0713357373046131, 0.07322240281801905, 0.10163820316258107, 0.09266128686618921, 0.08358149397923725, 0.11328828091592809, 0.07751311670178065, 0.11953177242143138, 0.06146499888193215, 0.07412926548689536, 0.08016561848944896, 0.05987518870622993, 0.06317689844579588, 0.08316155233482744, 0.06264203534385632, 0.0873022799664989, 0.08527475226092926, 0.06193907549818482, 0.10014942049576166, 0.09350991651315536, 0.0993298079028391, 0.06618164771673793, 0.10542144596242256, 0.08933245277784106, 0.059083294659788656, 0.06599768354170618, 0.07605662954446556, 0.0775147843000933, 0.09297907567445529, 0.11359608146535546, 0.07954992327616316, 0.0812637972318333, 0.08424539436534327, 0.11929341979998466, 0.12450458788256302, 0.10999343528327676], 'lossList': [0.0, -1.0615773862600326, 0.0, 0.8788851454667747, 0.0, 0.0, 0.0], 'rewardMean': 0.7777766628433553, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1212.0347146606698, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=73.93
