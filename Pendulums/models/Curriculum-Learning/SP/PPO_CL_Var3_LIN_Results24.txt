#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 24
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8702040304176358, 'errorList': [], 'lossList': [0.0, -1.4274246460199356, 0.0, 33.23963463008404, 0.0, 0.0, 0.0], 'rewardMean': 0.8097474473584585, 'totalEpisodes': 13, 'stepsPerEpisode': 317, 'rewardPerEpisode': 260.97302319227043
'totalSteps': 3840, 'rewardStep': 0.6655783788867724, 'errorList': [], 'lossList': [0.0, -1.4426380503177643, 0.0, 28.808278338909147, 0.0, 0.0, 0.0], 'rewardMean': 0.7616910912012299, 'totalEpisodes': 16, 'stepsPerEpisode': 174, 'rewardPerEpisode': 123.63959501553408
'totalSteps': 5120, 'rewardStep': 0.7070088390015501, 'errorList': [], 'lossList': [0.0, -1.4492430949211121, 0.0, 83.76535623550416, 0.0, 0.0, 0.0], 'rewardMean': 0.7480205281513099, 'totalEpisodes': 35, 'stepsPerEpisode': 38, 'rewardPerEpisode': 32.13341441451288
'totalSteps': 6400, 'rewardStep': 0.5506821635452251, 'errorList': [], 'lossList': [0.0, -1.4384526598453522, 0.0, 150.71494869232177, 0.0, 0.0, 0.0], 'rewardMean': 0.7085528552300929, 'totalEpisodes': 103, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5506821635452251
'totalSteps': 7680, 'rewardStep': 0.6915187851990413, 'errorList': [], 'lossList': [0.0, -1.43087975025177, 0.0, 63.3329030418396, 0.0, 0.0, 0.0], 'rewardMean': 0.7057138435582511, 'totalEpisodes': 155, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.403189407909766
'totalSteps': 8960, 'rewardStep': 0.9584675161351203, 'errorList': [68.66337749536939, 170.9283593724508, 122.41360440198302, 129.9132846595676, 30.03000379668764, 137.9698582383187, 164.1395994766391, 157.17645825792837, 105.86120317920476, 156.91383308200244, 55.12211362197368, 111.4469531213244, 115.23683832995633, 120.30680192190711, 69.23480400050587, 137.4606343314128, 138.52313508837605, 171.8157901639091, 126.0825719359635, 65.52298365175365, 150.12668683824347, 137.8513871621327, 125.30445893269051, 42.069195273774405, 131.30776265720277, 109.29566309031651, 158.9305966130943, 154.14332997848862, 180.74748299959742, 136.65091151870192, 181.54734351385525, 161.23085040192396, 138.4187676958873, 135.17003103225065, 20.29691151291074, 144.11992620690364, 154.12833463731715, 57.056091128552524, 83.19520999423072, 104.21743564802034, 147.48545547630135, 146.38737032040373, 139.7956072914939, 138.22399265565852, 158.40215427019496, 132.451096242921, 159.17647101968265, 169.75890532822729, 33.29130365442628, 92.76643210564129], 'lossList': [0.0, -1.4260913705825806, 0.0, 51.75621347427368, 0.0, 0.0, 0.0], 'rewardMean': 0.7418215110692323, 'totalEpisodes': 184, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.801757147624565, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.9452223712045205, 'errorList': [19.87710926364112, 52.50943660877778, 75.44762833849148, 33.48200105957497, 83.77883968278198, 76.2183244294735, 8.077288146486346, 68.08005133809664, 18.306936137585012, 66.76608207924187, 53.906413509360675, 105.38993140735501, 46.76347003301803, 108.44634232992752, 111.5003818640088, 95.03514958433526, 49.09466373126344, 23.8703298135124, 103.01398459240467, 12.91124445588729, 69.87695996832865, 131.6904486772942, 76.10317616443524, 81.63707886825568, 97.50534069615603, 99.1300128544821, 25.697652300846627, 27.835326084955973, 55.28687945501628, 69.31961442144424, 77.57542559911498, 106.74124880737143, 112.06678529649534, 82.44211242897251, 155.7752016498987, 0.13780576780395834, 100.90948288290289, 110.83018400737389, 16.85470867845866, 131.66057588190918, 30.058172474582452, 60.03185713209675, 82.02821258651478, 79.88439461207122, 135.16818322765425, 16.898856303140604, 87.44002654950998, 118.62557776952987, 99.07010096165664, 7.348390128092139], 'lossList': [0.0, -1.4006317096948624, 0.0, 47.93650434494018, 0.0, 0.0, 0.0], 'rewardMean': 0.7672466185861434, 'totalEpisodes': 201, 'stepsPerEpisode': 130, 'rewardPerEpisode': 105.01669143994573, 'successfulTests': 1
'totalSteps': 11520, 'rewardStep': 0.9512896872014339, 'errorList': [32.31351735771736, 16.578697811944114, 8.75820113426921, 67.65620453580324, 40.65231398547636, 2.0006330355040234, 53.8289633937064, 0.6927054851838128, 65.7216123281769, 54.00228436423963, 82.98383055030703, 48.02758242357312, 33.9862826521723, 91.70504664897707, 23.125049237830776, 55.007423441747, 101.52239412517066, 44.309447438598994, 90.72783259348104, 29.552159270287028, 31.198860856492175, 40.717020105278465, 28.46814278436384, 16.090575454653457, 6.821353387243697, 25.528571736178947, 19.974856352174804, 0.5543313301072377, 99.47842689367495, 10.726614113118963, 37.72650578030806, 7.292945744483866, 75.52568798628806, 65.99750358411406, 53.88866795417782, 4.451053959129823, 75.02922659808002, 21.252909756889803, 10.806959131235878, 18.034816100899498, 25.912504716090226, 41.9272705874998, 6.954008581624649, 11.567268532939613, 71.24341148497093, 39.2047483087939, 77.80215266744406, 21.140900007385145, 9.485179982098181, 80.31917975328662], 'lossList': [0.0, -1.3814537191390992, 0.0, 36.8154882478714, 0.0, 0.0, 0.0], 'rewardMean': 0.7876958484322868, 'totalEpisodes': 207, 'stepsPerEpisode': 259, 'rewardPerEpisode': 213.0230654517867, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.8762735225820785, 'errorList': [], 'lossList': [0.0, -1.372823656797409, 0.0, 46.43518245220184, 0.0, 0.0, 0.0], 'rewardMean': 0.7965536158472659, 'totalEpisodes': 213, 'stepsPerEpisode': 63, 'rewardPerEpisode': 51.20020763057166
'totalSteps': 14080, 'rewardStep': 0.6458389807541057, 'errorList': [], 'lossList': [0.0, -1.3496527034044266, 0.0, 29.08981163740158, 0.0, 0.0, 0.0], 'rewardMean': 0.7862084274927483, 'totalEpisodes': 217, 'stepsPerEpisode': 309, 'rewardPerEpisode': 253.52655114089544
'totalSteps': 15360, 'rewardStep': 0.465329533853494, 'errorList': [], 'lossList': [0.0, -1.3286695796251298, 0.0, 48.61386246681214, 0.0, 0.0, 0.0], 'rewardMean': 0.7457209778363343, 'totalEpisodes': 224, 'stepsPerEpisode': 188, 'rewardPerEpisode': 147.29963290364117
'totalSteps': 16640, 'rewardStep': 0.5731040671557246, 'errorList': [], 'lossList': [0.0, -1.3288895511627197, 0.0, 34.176848196983336, 0.0, 0.0, 0.0], 'rewardMean': 0.7364735466632294, 'totalEpisodes': 230, 'stepsPerEpisode': 120, 'rewardPerEpisode': 106.58892054948171
'totalSteps': 17920, 'rewardStep': 0.782404973392562, 'errorList': [], 'lossList': [0.0, -1.3335498702526092, 0.0, 31.000389609336853, 0.0, 0.0, 0.0], 'rewardMean': 0.7440131601023305, 'totalEpisodes': 235, 'stepsPerEpisode': 230, 'rewardPerEpisode': 195.20689168928183
'totalSteps': 19200, 'rewardStep': 0.42006510730860247, 'errorList': [], 'lossList': [0.0, -1.3319009137153626, 0.0, 12.784634763002396, 0.0, 0.0, 0.0], 'rewardMean': 0.7309514544786683, 'totalEpisodes': 238, 'stepsPerEpisode': 276, 'rewardPerEpisode': 186.61266280047522
'totalSteps': 20480, 'rewardStep': 0.6597536257994798, 'errorList': [], 'lossList': [0.0, -1.343855984210968, 0.0, 14.675394028425217, 0.0, 0.0, 0.0], 'rewardMean': 0.7277749385387121, 'totalEpisodes': 243, 'stepsPerEpisode': 333, 'rewardPerEpisode': 269.98602132169486
'totalSteps': 21760, 'rewardStep': 0.739539370262067, 'errorList': [], 'lossList': [0.0, -1.3368828749656678, 0.0, 9.233594120740891, 0.0, 0.0, 0.0], 'rewardMean': 0.7058821239514068, 'totalEpisodes': 247, 'stepsPerEpisode': 397, 'rewardPerEpisode': 344.42035869448097
'totalSteps': 23040, 'rewardStep': 0.8884025538842253, 'errorList': [], 'lossList': [0.0, -1.3153650152683258, 0.0, 5.923636778593063, 0.0, 0.0, 0.0], 'rewardMean': 0.7002001422193773, 'totalEpisodes': 249, 'stepsPerEpisode': 190, 'rewardPerEpisode': 160.55161133538448
'totalSteps': 24320, 'rewardStep': 0.8544558583351373, 'errorList': [], 'lossList': [0.0, -1.318008439540863, 0.0, 3.4596380802989004, 0.0, 0.0, 0.0], 'rewardMean': 0.6905167593327476, 'totalEpisodes': 250, 'stepsPerEpisode': 93, 'rewardPerEpisode': 83.1780997711462
'totalSteps': 25600, 'rewardStep': 0.8245086945666029, 'errorList': [], 'lossList': [0.0, -1.3385075050592423, 0.0, 2.089844593703747, 0.0, 0.0, 0.0], 'rewardMean': 0.6853402765312, 'totalEpisodes': 250, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 990.717857943291
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=10240, timeSpent=111.41
