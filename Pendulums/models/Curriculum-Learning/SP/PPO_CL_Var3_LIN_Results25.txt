#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 25
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.9100756893868107, 'errorList': [], 'lossList': [0.0, -1.4002412807941438, 0.0, 34.619477100372315, 0.0, 0.0, 0.0], 'rewardMean': 0.6771492151811642, 'totalEpisodes': 62, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.477918977175317
'totalSteps': 3840, 'rewardStep': 0.8333261082730282, 'errorList': [], 'lossList': [0.0, -1.3756611353158952, 0.0, 47.98494647026062, 0.0, 0.0, 0.0], 'rewardMean': 0.729208179545119, 'totalEpisodes': 87, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.364600150210052
'totalSteps': 5120, 'rewardStep': 0.6605846555695272, 'errorList': [], 'lossList': [0.0, -1.3568189823627472, 0.0, 44.58543758392334, 0.0, 0.0, 0.0], 'rewardMean': 0.712052298551221, 'totalEpisodes': 102, 'stepsPerEpisode': 61, 'rewardPerEpisode': 47.147368066129886
'totalSteps': 6400, 'rewardStep': 0.6876701461656425, 'errorList': [], 'lossList': [0.0, -1.3495406991243362, 0.0, 84.1062075805664, 0.0, 0.0, 0.0], 'rewardMean': 0.7071758680741053, 'totalEpisodes': 129, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.933059078141705
'totalSteps': 7680, 'rewardStep': 0.9500138729980928, 'errorList': [10.343253212208431, 11.866449093648983, 7.01498855508387, 10.568390291533074, 9.47801193499042, 3.309345358497312, 8.56748551967373, 8.535819996346618, 8.795306410928422, 8.62936307358372, 4.133700904066979, 9.007945156330162, 3.5652519062916466, 8.166757674848617, 12.205405433782797, 7.159776226103243, 7.579140288375612, 4.090313848641114, 8.727228941298613, 6.056568804456178, 8.9738420844713, 7.507036040897035, 9.639096011254638, 4.2782346834055955, 9.393312395774823, 2.8736879590193705, 8.453178294862493, 12.508121245194927, 10.806956761915446, 11.717719692535855, 6.723488755204054, 4.561545308760782, 3.1265595275972173, 7.6998026064637735, 8.430358810996264, 3.0580448423306312, 13.004378890275047, 9.329376842134057, 3.9230367177522516, 7.8270625669254565, 5.3661840478766605, 3.4180370452354647, 8.120289704950986, 9.663793327242853, 2.6798754668444023, 4.9352275744065786, 11.901067083318503, 3.662187075640278, 8.515598256527097, 8.486497225957354], 'lossList': [0.0, -1.3549889248609543, 0.0, 59.61563157081604, 0.0, 0.0, 0.0], 'rewardMean': 0.7476488688947699, 'totalEpisodes': 152, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.472782810807432, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.6572976130119854, 'errorList': [], 'lossList': [0.0, -1.3564937180280685, 0.0, 39.03064260482788, 0.0, 0.0, 0.0], 'rewardMean': 0.7347415466258008, 'totalEpisodes': 165, 'stepsPerEpisode': 91, 'rewardPerEpisode': 73.73747052122938
'totalSteps': 10240, 'rewardStep': 0.5095613413257796, 'errorList': [], 'lossList': [0.0, -1.3488709539175034, 0.0, 31.590118741989137, 0.0, 0.0, 0.0], 'rewardMean': 0.706594020963298, 'totalEpisodes': 171, 'stepsPerEpisode': 108, 'rewardPerEpisode': 82.44086128452862
'totalSteps': 11520, 'rewardStep': 0.6161697818847875, 'errorList': [], 'lossList': [0.0, -1.3348097717761993, 0.0, 35.6949014377594, 0.0, 0.0, 0.0], 'rewardMean': 0.6965468832879079, 'totalEpisodes': 178, 'stepsPerEpisode': 68, 'rewardPerEpisode': 53.27354250262697
'totalSteps': 12800, 'rewardStep': 0.6284786095755154, 'errorList': [], 'lossList': [0.0, -1.3268432271480561, 0.0, 11.303850977420806, 0.0, 0.0, 0.0], 'rewardMean': 0.6897400559166688, 'totalEpisodes': 184, 'stepsPerEpisode': 102, 'rewardPerEpisode': 74.7018805313328
'totalSteps': 14080, 'rewardStep': 0.3308604146248747, 'errorList': [], 'lossList': [0.0, -1.329995710849762, 0.0, 8.90222311258316, 0.0, 0.0, 0.0], 'rewardMean': 0.6784038232816043, 'totalEpisodes': 186, 'stepsPerEpisode': 667, 'rewardPerEpisode': 495.0458279941646
'totalSteps': 15360, 'rewardStep': 0.7634590046173478, 'errorList': [], 'lossList': [0.0, -1.32783775806427, 0.0, 6.556039694547653, 0.0, 0.0, 0.0], 'rewardMean': 0.663742154804658, 'totalEpisodes': 189, 'stepsPerEpisode': 777, 'rewardPerEpisode': 574.1749293918296
'totalSteps': 16640, 'rewardStep': 0.8705240854090057, 'errorList': [], 'lossList': [0.0, -1.3115979325771332, 0.0, 5.242007404267788, 0.0, 0.0, 0.0], 'rewardMean': 0.6674619525182559, 'totalEpisodes': 190, 'stepsPerEpisode': 665, 'rewardPerEpisode': 552.3107571693733
'totalSteps': 17920, 'rewardStep': 0.9155239557330279, 'errorList': [], 'lossList': [0.0, -1.2935759842395782, 0.0, 3.3277564811706544, 0.0, 0.0, 0.0], 'rewardMean': 0.692955882534606, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 886.6055718908245
'totalSteps': 19200, 'rewardStep': 0.5535501653223234, 'errorList': [], 'lossList': [0.0, -1.279073161482811, 0.0, 2.676771081984043, 0.0, 0.0, 0.0], 'rewardMean': 0.679543884450274, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 961.343645753922
'totalSteps': 20480, 'rewardStep': 0.9163843440531119, 'errorList': [], 'lossList': [0.0, -1.2575668430328368, 0.0, 2.2085244223475455, 0.0, 0.0, 0.0], 'rewardMean': 0.676180931555776, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 957.9958634502277
'totalSteps': 21760, 'rewardStep': 0.906365633724167, 'errorList': [], 'lossList': [0.0, -1.2428723907470702, 0.0, 1.9567265084385872, 0.0, 0.0, 0.0], 'rewardMean': 0.7010877336269941, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.932921084028
'totalSteps': 23040, 'rewardStep': 0.856818350975111, 'errorList': [], 'lossList': [0.0, -1.2263120186328889, 0.0, 0.9003409839421511, 0.0, 0.0, 0.0], 'rewardMean': 0.7358134345919274, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.3010172100157
'totalSteps': 24320, 'rewardStep': 0.7896245165020298, 'errorList': [], 'lossList': [0.0, -1.1932287377119064, 0.0, 0.9077581134065986, 0.0, 0.0, 0.0], 'rewardMean': 0.7531589080536514, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.8065687480948
'totalSteps': 25600, 'rewardStep': 0.9562096746208414, 'errorList': [0.08488918201962174, 0.03767397581821336, 0.059570502549179824, 0.022560193764083872, 0.051371264517805736, 0.07268290682241643, 0.029672045871386096, 0.051327344779445463, 0.026595572936280773, 0.02283281038521666, 0.06243745049945254, 0.04456802976761332, 0.061642417955682025, 0.022839663593262644, 0.018384389857679068, 0.02455230488253259, 0.06595535254655954, 0.024908032859177982, 0.051069748504199715, 0.04160430971560219, 0.03627074609515609, 0.02916813552314059, 0.033413814190111114, 0.025225196981483015, 0.09671657469553625, 0.03759957004861364, 0.05724334180300509, 0.05918424841252728, 0.06678771628157285, 0.03155512649944783, 0.02411412658731065, 0.03919703342979825, 0.03449809213111, 0.04877316192978262, 0.05429112342578432, 0.018079131009057836, 0.06546125602445045, 0.023252121198411184, 0.03869644890652932, 0.031196510439025053, 0.023001142970615358, 0.019662604466276143, 0.022427356312325123, 0.01933313168528567, 0.04091956094711304, 0.038467878057060936, 0.03311685993125726, 0.02757023943755841, 0.02237233542994885, 0.045289055286004666], 'lossList': [0.0, -1.1429541677236557, 0.0, 0.7372289252281189, 0.0, 0.0, 0.0], 'rewardMean': 0.7859320145581841, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.7762573594669, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=95.21
