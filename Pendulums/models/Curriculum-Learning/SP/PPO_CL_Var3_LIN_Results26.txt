#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 26
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5733816826617898, 'errorList': [], 'lossList': [0.0, -1.4129216182231903, 0.0, 32.450411500930784, 0.0, 0.0, 0.0], 'rewardMean': 0.6942159077515986, 'totalEpisodes': 59, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.68734921718021
'totalSteps': 3840, 'rewardStep': 0.5928017925638662, 'errorList': [], 'lossList': [0.0, -1.3998530054092406, 0.0, 50.6112272644043, 0.0, 0.0, 0.0], 'rewardMean': 0.6604112026890211, 'totalEpisodes': 81, 'stepsPerEpisode': 49, 'rewardPerEpisode': 34.818001474819184
'totalSteps': 5120, 'rewardStep': 0.8078857746153251, 'errorList': [], 'lossList': [0.0, -1.3923743271827698, 0.0, 54.327013607025144, 0.0, 0.0, 0.0], 'rewardMean': 0.6972798456705971, 'totalEpisodes': 97, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.893068347553541
'totalSteps': 6400, 'rewardStep': 0.5946194905333235, 'errorList': [], 'lossList': [0.0, -1.3782143074274062, 0.0, 82.12414930343628, 0.0, 0.0, 0.0], 'rewardMean': 0.6767477746431424, 'totalEpisodes': 117, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5946194905333235
'totalSteps': 7680, 'rewardStep': 0.7529907347567504, 'errorList': [], 'lossList': [0.0, -1.3679783761501312, 0.0, 39.97378695964813, 0.0, 0.0, 0.0], 'rewardMean': 0.6894549346620771, 'totalEpisodes': 129, 'stepsPerEpisode': 37, 'rewardPerEpisode': 31.398755842515584
'totalSteps': 8960, 'rewardStep': 0.7102319350885266, 'errorList': [], 'lossList': [0.0, -1.3515234738588333, 0.0, 55.89868379592895, 0.0, 0.0, 0.0], 'rewardMean': 0.6924230775801413, 'totalEpisodes': 147, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.101621538972912
'totalSteps': 10240, 'rewardStep': 0.7708111667987873, 'errorList': [], 'lossList': [0.0, -1.3392606395483018, 0.0, 22.577577977180482, 0.0, 0.0, 0.0], 'rewardMean': 0.702221588732472, 'totalEpisodes': 155, 'stepsPerEpisode': 54, 'rewardPerEpisode': 46.742328533791564
'totalSteps': 11520, 'rewardStep': 0.2934672562754734, 'errorList': [], 'lossList': [0.0, -1.32897798538208, 0.0, 23.750048334598542, 0.0, 0.0, 0.0], 'rewardMean': 0.6568044406816944, 'totalEpisodes': 157, 'stepsPerEpisode': 588, 'rewardPerEpisode': 490.97244504527475
'totalSteps': 12800, 'rewardStep': 0.7221571858317127, 'errorList': [], 'lossList': [0.0, -1.3115523779392242, 0.0, 7.104620868563652, 0.0, 0.0, 0.0], 'rewardMean': 0.6633397151966962, 'totalEpisodes': 161, 'stepsPerEpisode': 545, 'rewardPerEpisode': 382.72873489183326
'totalSteps': 14080, 'rewardStep': 0.5829786507055733, 'errorList': [], 'lossList': [0.0, -1.2730695176124573, 0.0, 5.331292044520378, 0.0, 0.0, 0.0], 'rewardMean': 0.6401325669831128, 'totalEpisodes': 162, 'stepsPerEpisode': 876, 'rewardPerEpisode': 609.2008158472224
'totalSteps': 15360, 'rewardStep': 0.8745526656088425, 'errorList': [], 'lossList': [0.0, -1.2411593562364578, 0.0, 4.573052671551705, 0.0, 0.0, 0.0], 'rewardMean': 0.670249665277818, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 910.2911070599142
'totalSteps': 16640, 'rewardStep': 0.678250324608342, 'errorList': [], 'lossList': [0.0, -1.2303056061267852, 0.0, 3.2482123118638992, 0.0, 0.0, 0.0], 'rewardMean': 0.6787945184822657, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 892.4759779534027
'totalSteps': 17920, 'rewardStep': 0.5324983354768836, 'errorList': [], 'lossList': [0.0, -1.211739994287491, 0.0, 1.8819183683395386, 0.0, 0.0, 0.0], 'rewardMean': 0.6512557745684215, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 858.0453535530681
'totalSteps': 19200, 'rewardStep': 0.7839466453374098, 'errorList': [], 'lossList': [0.0, -1.1733011478185653, 0.0, 2.3644814257323743, 0.0, 0.0, 0.0], 'rewardMean': 0.6701884900488302, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.5374428933635
'totalSteps': 20480, 'rewardStep': 0.837642385055636, 'errorList': [], 'lossList': [0.0, -1.1215338039398193, 0.0, 2.458413413502276, 0.0, 0.0, 0.0], 'rewardMean': 0.6786536550787188, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.4344114452283
'totalSteps': 21760, 'rewardStep': 0.8509452968811849, 'errorList': [], 'lossList': [0.0, -1.047310671210289, 0.0, 1.5994098282977938, 0.0, 0.0, 0.0], 'rewardMean': 0.6927249912579845, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.0712171112739
'totalSteps': 23040, 'rewardStep': 0.9765855129875183, 'errorList': [0.1384334937277224, 0.1824349742585221, 0.15667111241513815, 0.1345987369114673, 0.12302537758103904, 0.17214750467158565, 0.12858911157712885, 0.14930654123138942, 0.14783478126819685, 0.12822674931850073, 0.15932864648690317, 0.14417187204548426, 0.1701975353547519, 0.12644849832564256, 0.12322986189236648, 0.13334788051206806, 0.1698966166894435, 0.1625669292868043, 0.19172851868926488, 0.1433991532701216, 0.11409531050393176, 0.1445201199201383, 0.1288307840889094, 0.17999975420916026, 0.13118647830831728, 0.17224628580147858, 0.14717825185106928, 0.13928058515565625, 0.131608799653749, 0.13236988407732822, 0.16143435743177084, 0.16100762604702723, 0.2088909432313367, 0.1253614062693275, 0.17216749815251856, 0.10928425493024546, 0.15104741409133882, 0.1297405796359618, 0.16128701677050905, 0.1689016223455135, 0.15611541469933624, 0.14992730468463425, 0.15471445117757848, 0.13274001324370532, 0.15778944289599717, 0.17315686058901958, 0.16985186312672956, 0.13386733566019737, 0.16265098003242776, 0.16241497789041665], 'lossList': [0.0, -0.9985655951499939, 0.0, 0.8991054496355355, 0.0, 0.0, 0.0], 'rewardMean': 0.7133024258768577, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1163.1976242556798, 'successfulTests': 49
'totalSteps': 24320, 'rewardStep': 0.9209317570019105, 'errorList': [], 'lossList': [0.0, -0.9676366487145424, 0.0, 0.6488327961601317, 0.0, 0.0, 0.0], 'rewardMean': 0.7760488759495013, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.102289578115
'totalSteps': 25600, 'rewardStep': 0.989290178075882, 'errorList': [0.018670428176402448, 0.07657990083310347, 0.05703420353727362, 0.09541833264392745, 0.03764312805549116, 0.13939453938149265, 0.026994948116253605, 0.019755384282664618, 0.04330909956534508, 0.02881163783378192, 0.02861830872749626, 0.043680900760586096, 0.1225386925472097, 0.0614991538825043, 0.0466037924619415, 0.015782896928739357, 0.0317102730306132, 0.10113313730099127, 0.1508344023056537, 0.08786475687142248, 0.07175381932212897, 0.08158714458762331, 0.05621469127675775, 0.0692922863643468, 0.1006806664271813, 0.02357852243421917, 0.06796750249699313, 0.05540011845887619, 0.07710115482325469, 0.07414775110718806, 0.022145350439641796, 0.007089788336758559, 0.06090284002683224, 0.02700695420152473, 0.008207790556057939, 0.013725151395322908, 0.07702874318061206, 0.01810451576931077, 0.0423268950707079, 0.0379529334037311, 0.08596604117667812, 0.022961737317760973, 0.06794808549215181, 0.06399796328159178, 0.012628215758601985, 0.0397256901055795, 0.11167841685098936, 0.058111642110121695, 0.06701065893389828, 0.013209186954721258], 'lossList': [0.0, -0.9208215588331222, 0.0, 0.7406515992246568, 0.0, 0.0, 0.0], 'rewardMean': 0.8027621751739182, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1232.0875764897382, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=91.59
