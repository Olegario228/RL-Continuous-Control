#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 27
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.8250431503690503, 'errorList': [], 'lossList': [0.0, -1.4179961723089218, 0.0, 34.487462244033814, 0.0, 0.0, 0.0], 'rewardMean': 0.8203402460763799, 'totalEpisodes': 76, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.43801956108927
'totalSteps': 3840, 'rewardStep': 0.7672990514074549, 'errorList': [], 'lossList': [0.0, -1.396731095314026, 0.0, 44.78680180549622, 0.0, 0.0, 0.0], 'rewardMean': 0.8026598478534049, 'totalEpisodes': 102, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.57007748537421
'totalSteps': 5120, 'rewardStep': 0.8519595098650127, 'errorList': [], 'lossList': [0.0, -1.3696325093507766, 0.0, 50.81802534103394, 0.0, 0.0, 0.0], 'rewardMean': 0.8149847633563069, 'totalEpisodes': 119, 'stepsPerEpisode': 45, 'rewardPerEpisode': 38.420559997650294
'totalSteps': 6400, 'rewardStep': 0.9289102801532454, 'errorList': [], 'lossList': [0.0, -1.3717493414878845, 0.0, 71.45328588485718, 0.0, 0.0, 0.0], 'rewardMean': 0.8377698667156945, 'totalEpisodes': 137, 'stepsPerEpisode': 31, 'rewardPerEpisode': 21.283294063725037
'totalSteps': 7680, 'rewardStep': 0.8830412039765602, 'errorList': [], 'lossList': [0.0, -1.376299279332161, 0.0, 72.33743715286255, 0.0, 0.0, 0.0], 'rewardMean': 0.8453150895925056, 'totalEpisodes': 152, 'stepsPerEpisode': 55, 'rewardPerEpisode': 37.93266567940696
'totalSteps': 8960, 'rewardStep': 0.8013726913818012, 'errorList': [], 'lossList': [0.0, -1.3825109881162643, 0.0, 52.303452272415164, 0.0, 0.0, 0.0], 'rewardMean': 0.8390376041338335, 'totalEpisodes': 159, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.32307506447269
'totalSteps': 10240, 'rewardStep': 0.6299604074658041, 'errorList': [], 'lossList': [0.0, -1.3752372354269027, 0.0, 43.88807260990143, 0.0, 0.0, 0.0], 'rewardMean': 0.8129029545503297, 'totalEpisodes': 163, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.220804750566009
'totalSteps': 11520, 'rewardStep': 0.7885764344926516, 'errorList': [], 'lossList': [0.0, -1.3702041977643966, 0.0, 48.187813119888304, 0.0, 0.0, 0.0], 'rewardMean': 0.8102000078772543, 'totalEpisodes': 168, 'stepsPerEpisode': 150, 'rewardPerEpisode': 119.21043066873858
'totalSteps': 12800, 'rewardStep': 0.8900214711939964, 'errorList': [], 'lossList': [0.0, -1.3686632561683654, 0.0, 46.91218083381653, 0.0, 0.0, 0.0], 'rewardMean': 0.8181821542089285, 'totalEpisodes': 172, 'stepsPerEpisode': 336, 'rewardPerEpisode': 265.97190235341435
'totalSteps': 14080, 'rewardStep': 0.6633694259250694, 'errorList': [], 'lossList': [0.0, -1.3519692784547805, 0.0, 8.054221413731575, 0.0, 0.0, 0.0], 'rewardMean': 0.8029553626230646, 'totalEpisodes': 174, 'stepsPerEpisode': 191, 'rewardPerEpisode': 155.14786184212204
'totalSteps': 15360, 'rewardStep': 0.7908111600530345, 'errorList': [], 'lossList': [0.0, -1.3492212200164795, 0.0, 12.92675200343132, 0.0, 0.0, 0.0], 'rewardMean': 0.7995321635914631, 'totalEpisodes': 179, 'stepsPerEpisode': 358, 'rewardPerEpisode': 293.8812250229515
'totalSteps': 16640, 'rewardStep': 0.7971976414703807, 'errorList': [], 'lossList': [0.0, -1.3643918645381927, 0.0, 8.468949791193008, 0.0, 0.0, 0.0], 'rewardMean': 0.8025220225977556, 'totalEpisodes': 184, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.367435818210023
'totalSteps': 17920, 'rewardStep': 0.7718917773476536, 'errorList': [], 'lossList': [0.0, -1.3517262095212936, 0.0, 5.991462831497192, 0.0, 0.0, 0.0], 'rewardMean': 0.7945152493460197, 'totalEpisodes': 188, 'stepsPerEpisode': 93, 'rewardPerEpisode': 78.1656262531713
'totalSteps': 19200, 'rewardStep': 0.3833502686625992, 'errorList': [], 'lossList': [0.0, -1.3253582334518432, 0.0, 26.148785123825075, 0.0, 0.0, 0.0], 'rewardMean': 0.7399592481969551, 'totalEpisodes': 194, 'stepsPerEpisode': 127, 'rewardPerEpisode': 81.59821588350582
'totalSteps': 20480, 'rewardStep': 0.8851175115979408, 'errorList': [], 'lossList': [0.0, -1.3106040281057358, 0.0, 3.711029837727547, 0.0, 0.0, 0.0], 'rewardMean': 0.7401668789590932, 'totalEpisodes': 200, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.91486416669716
'totalSteps': 21760, 'rewardStep': 0.7921436917891129, 'errorList': [], 'lossList': [0.0, -1.2848899322748184, 0.0, 4.34984275996685, 0.0, 0.0, 0.0], 'rewardMean': 0.7392439789998244, 'totalEpisodes': 201, 'stepsPerEpisode': 219, 'rewardPerEpisode': 189.21197377044595
'totalSteps': 23040, 'rewardStep': 0.9610621662028823, 'errorList': [0.30713339813966706, 0.3215170077771195, 0.3358948389537686, 0.32354896469437133, 0.27518722881310914, 0.24505178671180414, 0.39893367938792945, 0.2998303240262551, 0.33768922587299005, 0.3382730955783408, 0.2767515813330994, 0.33442753559161903, 0.32804499146033334, 0.3821240248195131, 0.2949761257826208, 0.3468817279692088, 0.40822188872785287, 0.21983837268453074, 0.30724816202640703, 0.28053985230344286, 0.30062387759090514, 0.3578056946404392, 0.3322651152932841, 0.2601357930342323, 0.31232823774223983, 0.3075462703737078, 0.3707008326876016, 0.32342632778494446, 0.29064610233631905, 0.26165901955658805, 0.23822679116555637, 0.37999454983211167, 0.2702063990436916, 0.2718592236532844, 0.3007884658388818, 0.23108323043099505, 0.3427179479089766, 0.3611521752038725, 0.2678259021377529, 0.3706165496035894, 0.2717060365269916, 0.30917606238185474, 0.3995981570145612, 0.25264554005504625, 0.24797137748127734, 0.3794257109263344, 0.35603824413612156, 0.38998442415974677, 0.3162883238673571, 0.36778929852539766], 'lossList': [0.0, -1.2517688482999803, 0.0, 2.204198150113225, 0.0, 0.0, 0.0], 'rewardMean': 0.7723541548735321, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.2715702370813, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.809858498669191, 'errorList': [], 'lossList': [0.0, -1.2260453099012374, 0.0, 1.2225600892305375, 0.0, 0.0, 0.0], 'rewardMean': 0.7744823612911862, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.1812313012797
'totalSteps': 25600, 'rewardStep': 0.7898789072700857, 'errorList': [], 'lossList': [0.0, -1.2146853429079056, 0.0, 1.0364284849911929, 0.0, 0.0, 0.0], 'rewardMean': 0.7644681048987951, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.4326756901837
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=73.31
