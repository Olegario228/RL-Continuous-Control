#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 28
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.514362062689648, 'errorList': [], 'lossList': [0.0, -1.434564672112465, 0.0, 33.010489501953124, 0.0, 0.0, 0.0], 'rewardMean': 0.5096506447150164, 'totalEpisodes': 64, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.643352463411723
'totalSteps': 3840, 'rewardStep': 0.6975586058925689, 'errorList': [], 'lossList': [0.0, -1.4279529345035553, 0.0, 41.72685560226441, 0.0, 0.0, 0.0], 'rewardMean': 0.5722866317742006, 'totalEpisodes': 90, 'stepsPerEpisode': 50, 'rewardPerEpisode': 36.93533769375553
'totalSteps': 5120, 'rewardStep': 0.8062404994918231, 'errorList': [], 'lossList': [0.0, -1.415274658203125, 0.0, 52.153582639694214, 0.0, 0.0, 0.0], 'rewardMean': 0.6307750987036063, 'totalEpisodes': 106, 'stepsPerEpisode': 73, 'rewardPerEpisode': 56.80404070189988
'totalSteps': 6400, 'rewardStep': 0.7646017732579956, 'errorList': [], 'lossList': [0.0, -1.3862156784534454, 0.0, 85.03708904266358, 0.0, 0.0, 0.0], 'rewardMean': 0.6575404336144841, 'totalEpisodes': 130, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.70529730120585
'totalSteps': 7680, 'rewardStep': 0.7887875232584627, 'errorList': [], 'lossList': [0.0, -1.3739775979518891, 0.0, 88.848385887146, 0.0, 0.0, 0.0], 'rewardMean': 0.6794149485551473, 'totalEpisodes': 155, 'stepsPerEpisode': 53, 'rewardPerEpisode': 44.20932536914468
'totalSteps': 8960, 'rewardStep': 0.8195833011044977, 'errorList': [], 'lossList': [0.0, -1.3640566301345824, 0.0, 65.42663793563842, 0.0, 0.0, 0.0], 'rewardMean': 0.6994389989193401, 'totalEpisodes': 175, 'stepsPerEpisode': 71, 'rewardPerEpisode': 58.77624772688571
'totalSteps': 10240, 'rewardStep': 0.686714650976283, 'errorList': [], 'lossList': [0.0, -1.3517436975240706, 0.0, 32.662630870342255, 0.0, 0.0, 0.0], 'rewardMean': 0.697848455426458, 'totalEpisodes': 181, 'stepsPerEpisode': 163, 'rewardPerEpisode': 124.73477668505386
'totalSteps': 11520, 'rewardStep': 0.8729780854752366, 'errorList': [], 'lossList': [0.0, -1.338242574930191, 0.0, 15.750162096619606, 0.0, 0.0, 0.0], 'rewardMean': 0.7173073032096556, 'totalEpisodes': 182, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.439567086419506
'totalSteps': 12800, 'rewardStep': 0.7084030952146189, 'errorList': [], 'lossList': [0.0, -1.3258360147476196, 0.0, 10.241116195321084, 0.0, 0.0, 0.0], 'rewardMean': 0.716416882410152, 'totalEpisodes': 187, 'stepsPerEpisode': 133, 'rewardPerEpisode': 118.56221817107583
'totalSteps': 14080, 'rewardStep': 0.9122466653711506, 'errorList': [], 'lossList': [0.0, -1.3016241353750229, 0.0, 5.090263789594173, 0.0, 0.0, 0.0], 'rewardMean': 0.7571476262732285, 'totalEpisodes': 189, 'stepsPerEpisode': 155, 'rewardPerEpisode': 128.65840311540092
'totalSteps': 15360, 'rewardStep': 0.5791823555420624, 'errorList': [], 'lossList': [0.0, -1.2698209035396575, 0.0, 7.549417466521263, 0.0, 0.0, 0.0], 'rewardMean': 0.7636296555584701, 'totalEpisodes': 190, 'stepsPerEpisode': 512, 'rewardPerEpisode': 397.4424756352315
'totalSteps': 16640, 'rewardStep': 0.8925333980441449, 'errorList': [], 'lossList': [0.0, -1.2531543916463852, 0.0, 4.216895915865898, 0.0, 0.0, 0.0], 'rewardMean': 0.7831271347736275, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1037.4623942156325
'totalSteps': 17920, 'rewardStep': 0.9043162126795967, 'errorList': [], 'lossList': [0.0, -1.245740087032318, 0.0, 3.3065623854845763, 0.0, 0.0, 0.0], 'rewardMean': 0.792934706092405, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.7522782838616
'totalSteps': 19200, 'rewardStep': 0.921578099518626, 'errorList': [], 'lossList': [0.0, -1.2388832485675811, 0.0, 2.414150526858866, 0.0, 0.0, 0.0], 'rewardMean': 0.8086323387184681, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.6593287733835
'totalSteps': 20480, 'rewardStep': 0.9708658109875397, 'errorList': [0.05595070655783714, 0.04887330367778892, 0.06377351488022823, 0.08740969173142236, 0.056455587081911854, 0.05490157295637405, 0.10860289561479414, 0.06784749132187785, 0.07202036595851928, 0.0960627637466085, 0.06117403725636778, 0.06193492040436964, 0.05871587454559303, 0.08113607817452397, 0.06237769571545661, 0.048108949917909746, 0.08643736841701312, 0.11145891242696758, 0.08893594919735069, 0.108602375955926, 0.050229677522146014, 0.0651168781983741, 0.11386918457873561, 0.08766743506379475, 0.05065934332119066, 0.04690926019000025, 0.0891577926942761, 0.09957746827501153, 0.10361568379332717, 0.10793864030178746, 0.04373664199237869, 0.10803748385400469, 0.06185186648346815, 0.08385495550359828, 0.13825064160733802, 0.08782027575223399, 0.10670966203968303, 0.08607846135132448, 0.049830134201006154, 0.04754763513601557, 0.060522718180690535, 0.11937613561011014, 0.07299730323893244, 0.08535322758833147, 0.09604188377238569, 0.05964550197016446, 0.04476285847012769, 0.061554969434655465, 0.09616799437285986, 0.09456456539191839], 'lossList': [0.0, -1.1995352387428284, 0.0, 2.136132531836629, 0.0, 0.0, 0.0], 'rewardMean': 0.8268401674913758, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.7697805082407, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=61.96
