#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 29
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7522632398719453, 'errorList': [], 'lossList': [0.0, -1.4114288121461869, 0.0, 34.43540017127991, 0.0, 0.0, 0.0], 'rewardMean': 0.8366766870116762, 'totalEpisodes': 72, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.984478170500754
'totalSteps': 3840, 'rewardStep': 0.9150157785922992, 'errorList': [], 'lossList': [0.0, -1.4111976265907287, 0.0, 44.446381378173825, 0.0, 0.0, 0.0], 'rewardMean': 0.8627897175385506, 'totalEpisodes': 99, 'stepsPerEpisode': 57, 'rewardPerEpisode': 43.86815222607861
'totalSteps': 5120, 'rewardStep': 0.8162804335013417, 'errorList': [], 'lossList': [0.0, -1.3949245399236678, 0.0, 58.102193641662595, 0.0, 0.0, 0.0], 'rewardMean': 0.8511623965292484, 'totalEpisodes': 121, 'stepsPerEpisode': 42, 'rewardPerEpisode': 26.830506072326827
'totalSteps': 6400, 'rewardStep': 0.8113066253219097, 'errorList': [], 'lossList': [0.0, -1.386381698846817, 0.0, 68.12529123306274, 0.0, 0.0, 0.0], 'rewardMean': 0.8431912422877806, 'totalEpisodes': 139, 'stepsPerEpisode': 37, 'rewardPerEpisode': 31.53985810386098
'totalSteps': 7680, 'rewardStep': 0.8497221725206227, 'errorList': [], 'lossList': [0.0, -1.3836621475219726, 0.0, 50.131396236419675, 0.0, 0.0, 0.0], 'rewardMean': 0.8442797306599209, 'totalEpisodes': 152, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.119948091784035
'totalSteps': 8960, 'rewardStep': 0.32275264909411155, 'errorList': [], 'lossList': [0.0, -1.3733770579099656, 0.0, 38.58615165233612, 0.0, 0.0, 0.0], 'rewardMean': 0.7697758618648053, 'totalEpisodes': 162, 'stepsPerEpisode': 228, 'rewardPerEpisode': 145.06926602589007
'totalSteps': 10240, 'rewardStep': 0.5624430086793883, 'errorList': [], 'lossList': [0.0, -1.3615337866544723, 0.0, 17.602216910123825, 0.0, 0.0, 0.0], 'rewardMean': 0.7438592552166282, 'totalEpisodes': 168, 'stepsPerEpisode': 130, 'rewardPerEpisode': 95.43491451742545
'totalSteps': 11520, 'rewardStep': 0.4261887368494978, 'errorList': [], 'lossList': [0.0, -1.3455992019176484, 0.0, 8.286382972002029, 0.0, 0.0, 0.0], 'rewardMean': 0.7085625309536137, 'totalEpisodes': 174, 'stepsPerEpisode': 112, 'rewardPerEpisode': 82.39881055906406
'totalSteps': 12800, 'rewardStep': 0.6733230851233996, 'errorList': [], 'lossList': [0.0, -1.3360666596889497, 0.0, 26.30485765695572, 0.0, 0.0, 0.0], 'rewardMean': 0.7050385863705924, 'totalEpisodes': 183, 'stepsPerEpisode': 63, 'rewardPerEpisode': 46.87388208493539
'totalSteps': 14080, 'rewardStep': 0.9524080771690311, 'errorList': [0.31966723391928004, 21.85403758920239, 1.868357752935074, 0.5047819124285216, 13.106849952586341, 17.639355356770576, 1.0450909429106694, 12.036518577019066, 3.5130698901606863, 9.834885720120766, 9.503293019461037, 27.323132868481164, 15.761601701810246, 19.37907817937396, 1.4408020524291107, 0.6957341131858552, 7.003463757110798, 1.091816917417598, 2.993732187315718, 14.96865941961006, 2.224246616884798, 26.27213947060639, 5.234353002412547, 8.376080502763813, 8.25789101008868, 2.406436084370427, 7.221095917395671, 4.194650607947783, 1.8545589720901572, 0.9818882257584484, 2.438673593588372, 26.665749942873617, 19.237806752712675, 2.248660567398251, 0.46912548783557195, 6.815031219469068, 2.5749574045582992, 4.792273654299138, 5.819370425017993, 1.285280717132809, 6.54169587990094, 6.255932454419046, 4.00701050294669, 3.0761377396010525, 6.346182647155422, 17.260917436388052, 20.98873158968082, 1.543089761518352, 11.107899123461795, 8.565150300606122], 'lossList': [0.0, -1.3207163965702058, 0.0, 7.492221790552139, 0.0, 0.0, 0.0], 'rewardMean': 0.7081703806723547, 'totalEpisodes': 189, 'stepsPerEpisode': 74, 'rewardPerEpisode': 63.76132682962723, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.9649578967393659, 'errorList': [4.440523355265789, 5.581750318272777, 5.777546079414854, 5.189254645173266, 2.4789101542554097, 2.7518587356747144, 1.9901406742935719, 3.939766430737633, 5.7868999983934835, 4.517664592390174, 4.5149480529664325, 1.8364357122274406, 5.247435595522114, 2.618100679813854, 8.08686358325992, 2.607641484237478, 2.8888983616365826, 2.1693465335477655, 2.8668313616824914, 6.113205249652898, 1.8269381979669883, 5.271351599391851, 3.55821334465497, 2.5407341760653828, 2.6548793741646928, 3.9095607123806957, 5.79555137993088, 2.4969478087744426, 5.5999205955426765, 5.729391558508829, 2.016327829344198, 7.174447877989936, 1.343088187461418, 5.158677975151994, 2.396554486494781, 3.9209734032097754, 2.409969850601417, 7.49432112125551, 2.9890580339152137, 4.253364869321015, 1.6393381235705036, 3.0009558907572407, 4.7705733703655415, 9.350894413510506, 7.438558165944714, 7.528411603167004, 5.566796725006231, 3.6857002720152625, 3.4575245204486094, 2.408966336133312], 'lossList': [0.0, -1.3113953703641892, 0.0, 8.611643693447114, 0.0, 0.0, 0.0], 'rewardMean': 0.7294398463590968, 'totalEpisodes': 193, 'stepsPerEpisode': 68, 'rewardPerEpisode': 61.61531993929634, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5215895954257218, 'errorList': [], 'lossList': [0.0, -1.3128170758485793, 0.0, 8.081721967458725, 0.0, 0.0, 0.0], 'rewardMean': 0.6900972280424389, 'totalEpisodes': 197, 'stepsPerEpisode': 236, 'rewardPerEpisode': 193.4927157389772
'totalSteps': 17920, 'rewardStep': 0.7477081496307877, 'errorList': [], 'lossList': [0.0, -1.3230222082138061, 0.0, 5.179356218576431, 0.0, 0.0, 0.0], 'rewardMean': 0.6832399996553836, 'totalEpisodes': 201, 'stepsPerEpisode': 33, 'rewardPerEpisode': 28.089160140916945
'totalSteps': 19200, 'rewardStep': 0.9035056646434572, 'errorList': [], 'lossList': [0.0, -1.3342083394527435, 0.0, 3.642736102938652, 0.0, 0.0, 0.0], 'rewardMean': 0.6924599035875383, 'totalEpisodes': 202, 'stepsPerEpisode': 395, 'rewardPerEpisode': 335.70298392014195
'totalSteps': 20480, 'rewardStep': 0.8627658874915316, 'errorList': [], 'lossList': [0.0, -1.3076064521074295, 0.0, 1.579533004760742, 0.0, 0.0, 0.0], 'rewardMean': 0.6937642750846293, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.6353802878152
'totalSteps': 21760, 'rewardStep': 0.8144998397478717, 'errorList': [], 'lossList': [0.0, -1.3137621998786926, 0.0, 0.8166944986581802, 0.0, 0.0, 0.0], 'rewardMean': 0.7429389941500052, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.930672335234
'totalSteps': 23040, 'rewardStep': 0.8664547543368013, 'errorList': [], 'lossList': [0.0, -1.313915805220604, 0.0, 0.7494610074162483, 0.0, 0.0, 0.0], 'rewardMean': 0.7733401687157466, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.4076959851066
'totalSteps': 24320, 'rewardStep': 0.9164820663195982, 'errorList': [], 'lossList': [0.0, -1.2993874311447144, 0.0, 0.6234538828022778, 0.0, 0.0, 0.0], 'rewardMean': 0.8223695016627566, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.885478118717
'totalSteps': 25600, 'rewardStep': 0.9913536653888426, 'errorList': [0.06599323956962529, 0.08678960087807261, 0.08139179389936964, 0.037156432609968953, 0.012458126561800788, 0.024594652794203736, 0.05348231152806455, 0.040705358608945516, 0.08231228607037198, 0.1479328475451268, 0.06084636340909397, 0.03239417408819941, 0.011099307916021536, 0.09342506363722519, 0.01950865459303807, 0.11109750367840397, 0.12271238757683924, 0.07268789372772981, 0.11311942764672107, 0.10379202708823118, 0.07808225009220113, 0.06612874550881233, 0.07391574808832145, 0.07964122527644071, 0.029343524497776663, 0.01661070490546338, 0.1423523305487415, 0.04433074909440844, 0.03173135140445171, 0.10959976533162229, 0.01714029838859088, 0.07413348590900841, 0.05377661231269878, 0.10250846408671582, 0.11024526130398343, 0.014144612027560866, 0.03755618235592358, 0.05052695613707718, 0.01560931764534043, 0.033265853576410785, 0.05424344329056568, 0.04424217992789622, 0.0769617788337028, 0.08619183560625264, 0.027743398230219218, 0.01501415781812625, 0.11391370825510962, 0.018692995178054346, 0.10087810240071676, 0.05122289102537686], 'lossList': [0.0, -1.282417739033699, 0.0, 0.6250548899546265, 0.0, 0.0, 0.0], 'rewardMean': 0.854172559689301, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1203.094064853309, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=112.09
