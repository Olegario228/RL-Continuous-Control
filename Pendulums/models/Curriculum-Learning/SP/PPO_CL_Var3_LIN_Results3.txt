#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.9091581066866185, 'errorList': [], 'lossList': [0.0, -1.432120172381401, 0.0, 32.73473254203796, 0.0, 0.0, 0.0], 'rewardMean': 0.7070486667135016, 'totalEpisodes': 68, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9091581066866185
'totalSteps': 3840, 'rewardStep': 0.7704485482011743, 'errorList': [], 'lossList': [0.0, -1.4205732756853104, 0.0, 44.19885259628296, 0.0, 0.0, 0.0], 'rewardMean': 0.7281819605427259, 'totalEpisodes': 94, 'stepsPerEpisode': 15, 'rewardPerEpisode': 9.954997379412426
'totalSteps': 5120, 'rewardStep': 0.6194016524397317, 'errorList': [], 'lossList': [0.0, -1.4075763183832168, 0.0, 59.55775047302246, 0.0, 0.0, 0.0], 'rewardMean': 0.7009868835169772, 'totalEpisodes': 117, 'stepsPerEpisode': 74, 'rewardPerEpisode': 56.73294690790319
'totalSteps': 6400, 'rewardStep': 0.800106428074584, 'errorList': [], 'lossList': [0.0, -1.4034751868247985, 0.0, 68.2456243133545, 0.0, 0.0, 0.0], 'rewardMean': 0.7208107924284987, 'totalEpisodes': 147, 'stepsPerEpisode': 15, 'rewardPerEpisode': 14.038212264435098
'totalSteps': 7680, 'rewardStep': 0.7667223605336232, 'errorList': [], 'lossList': [0.0, -1.3997522300481797, 0.0, 63.039403457641605, 0.0, 0.0, 0.0], 'rewardMean': 0.7284627204460193, 'totalEpisodes': 168, 'stepsPerEpisode': 55, 'rewardPerEpisode': 39.53598587444618
'totalSteps': 8960, 'rewardStep': 0.7899738565037376, 'errorList': [], 'lossList': [0.0, -1.3837514889240266, 0.0, 42.30192539215088, 0.0, 0.0, 0.0], 'rewardMean': 0.737250025597122, 'totalEpisodes': 177, 'stepsPerEpisode': 69, 'rewardPerEpisode': 52.17214437913162
'totalSteps': 10240, 'rewardStep': 0.6542776057796147, 'errorList': [], 'lossList': [0.0, -1.3815992259979248, 0.0, 34.505422110557554, 0.0, 0.0, 0.0], 'rewardMean': 0.7268784731199336, 'totalEpisodes': 183, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.563708891876317
'totalSteps': 11520, 'rewardStep': 0.7298097102957803, 'errorList': [], 'lossList': [0.0, -1.3898794752359391, 0.0, 31.30509605884552, 0.0, 0.0, 0.0], 'rewardMean': 0.7272041661394721, 'totalEpisodes': 189, 'stepsPerEpisode': 19, 'rewardPerEpisode': 12.149496868363592
'totalSteps': 12800, 'rewardStep': 0.9570374937438537, 'errorList': [56.59207140565258, 73.78751160860301, 47.20792957901202, 70.83443620433364, 37.053953214035715, 54.736594734484115, 7.982350639631731, 110.82040999390826, 121.05602753863897, 0.961227901260445, 77.655014257068, 63.744187681729784, 99.82624534629909, 105.86691453682195, 41.58004200448711, 80.71366280395424, 3.5566098380689213, 97.62231826028295, 120.5889446038652, 88.26577279447366, 59.21336529622671, 145.21489761948988, 49.201769250471834, 119.20988931866138, 33.48820233587779, 89.6759633306646, 3.4809716986327617, 107.09711712149011, 78.08957614774762, 63.201145626496945, 87.41102822562205, 89.05807941818182, 66.1571573255754, 124.6928313901053, 76.73860889639688, 70.20235924719624, 81.92007941573547, 33.66629058788493, 102.24889520385832, 140.11803977921775, 38.17526098492799, 0.921795203721373, 108.11636814325647, 53.746014774662584, 17.132638178130016, 3.2327184812601306, 103.96369887846359, 86.44652171303368, 129.12916440933776, 23.098548195263405], 'lossList': [0.0, -1.3716871273517608, 0.0, 45.042062549591066, 0.0, 0.0, 0.0], 'rewardMean': 0.7501874988999103, 'totalEpisodes': 199, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.520940153140597, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.4952463605183537, 'errorList': [], 'lossList': [0.0, -1.370167214870453, 0.0, 20.10447501540184, 0.0, 0.0, 0.0], 'rewardMean': 0.7492182122777071, 'totalEpisodes': 203, 'stepsPerEpisode': 536, 'rewardPerEpisode': 448.7562980242594
'totalSteps': 15360, 'rewardStep': 0.7712513926223625, 'errorList': [], 'lossList': [0.0, -1.3754710918664932, 0.0, 14.523347390890121, 0.0, 0.0, 0.0], 'rewardMean': 0.7354275408712816, 'totalEpisodes': 207, 'stepsPerEpisode': 262, 'rewardPerEpisode': 218.43075917911918
'totalSteps': 16640, 'rewardStep': 0.49375532842743064, 'errorList': [], 'lossList': [0.0, -1.3769688618183136, 0.0, 15.43291465640068, 0.0, 0.0, 0.0], 'rewardMean': 0.7077582188939072, 'totalEpisodes': 210, 'stepsPerEpisode': 309, 'rewardPerEpisode': 226.10452753572966
'totalSteps': 17920, 'rewardStep': 0.882290849677821, 'errorList': [], 'lossList': [0.0, -1.37494755089283, 0.0, 5.139353416264057, 0.0, 0.0, 0.0], 'rewardMean': 0.7340471386177161, 'totalEpisodes': 214, 'stepsPerEpisode': 303, 'rewardPerEpisode': 275.6973980226412
'totalSteps': 19200, 'rewardStep': 0.7532082471038337, 'errorList': [], 'lossList': [0.0, -1.360109407901764, 0.0, 4.938743885159493, 0.0, 0.0, 0.0], 'rewardMean': 0.7293573205206411, 'totalEpisodes': 217, 'stepsPerEpisode': 260, 'rewardPerEpisode': 223.37804907933264
'totalSteps': 20480, 'rewardStep': 0.9003881552632664, 'errorList': [], 'lossList': [0.0, -1.3411115944385528, 0.0, 3.8703719183802603, 0.0, 0.0, 0.0], 'rewardMean': 0.7427238999936054, 'totalEpisodes': 218, 'stepsPerEpisode': 796, 'rewardPerEpisode': 703.4873239975874
'totalSteps': 21760, 'rewardStep': 0.8026842148929837, 'errorList': [], 'lossList': [0.0, -1.3223700720071792, 0.0, 5.193521485328675, 0.0, 0.0, 0.0], 'rewardMean': 0.7439949358325301, 'totalEpisodes': 219, 'stepsPerEpisode': 396, 'rewardPerEpisode': 309.72409083823516
'totalSteps': 23040, 'rewardStep': 0.8428895538041821, 'errorList': [], 'lossList': [0.0, -1.2704653769731522, 0.0, 1.4612598142400384, 0.0, 0.0, 0.0], 'rewardMean': 0.7628561306349868, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.825238241153
'totalSteps': 24320, 'rewardStep': 0.8550412932232525, 'errorList': [], 'lossList': [0.0, -1.2074282336235047, 0.0, 1.0770048261806369, 0.0, 0.0, 0.0], 'rewardMean': 0.7753792889277341, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.9304610024208
'totalSteps': 25600, 'rewardStep': 0.9613868054569253, 'errorList': [0.035721428595929305, 0.0161538518349998, 0.01662777462049671, 0.0221310861396652, 0.030088639087381228, 0.016440475096171542, 0.018150688867108244, 0.015779303769146748, 0.02730519240030085, 0.02144830174987179, 0.03372806575903947, 0.015709997887451956, 0.015860099333525158, 0.04252326754968303, 0.018995895264480055, 0.019424155954258674, 0.03172011424135022, 0.028497624551654072, 0.01751477146772612, 0.033479755554217526, 0.020996458118982243, 0.03376733113777583, 0.015410767997757143, 0.016159668494271383, 0.02064726779181098, 0.016715347876785077, 0.01668577464704851, 0.018308639744694263, 0.016976658292996886, 0.018390519060631858, 0.01851208874368358, 0.015792393508792173, 0.030492936892460364, 0.028572761269183403, 0.03127267822601093, 0.01765959194293146, 0.029698063755114527, 0.026917235312540994, 0.016364587827904738, 0.018823458361306486, 0.020967601502501532, 0.021303208587482254, 0.02517722213020885, 0.030665208689513196, 0.0219857514863309, 0.02319470482811182, 0.017841101926068338, 0.04026483232890185, 0.04070771136964379, 0.024803638759047942], 'lossList': [0.0, -1.182724027633667, 0.0, 0.8075917035341262, 0.0, 0.0, 0.0], 'rewardMean': 0.7758142200990412, 'totalEpisodes': 219, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1202.365715148515, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=88.01
