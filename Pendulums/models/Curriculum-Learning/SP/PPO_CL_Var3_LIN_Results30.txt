#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 30
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.7296067906308707, 'errorList': [], 'lossList': [0.0, -1.401526057124138, 0.0, 24.568436832427977, 0.0, 0.0, 0.0], 'rewardMean': 0.6701384447863606, 'totalEpisodes': 21, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.352780160511761
'totalSteps': 3840, 'rewardStep': 0.5128398116720464, 'errorList': [], 'lossList': [0.0, -1.3939784038066865, 0.0, 37.76754286289215, 0.0, 0.0, 0.0], 'rewardMean': 0.6177055670815892, 'totalEpisodes': 35, 'stepsPerEpisode': 169, 'rewardPerEpisode': 116.07198793267222
'totalSteps': 5120, 'rewardStep': 0.7022770953943746, 'errorList': [], 'lossList': [0.0, -1.384092794060707, 0.0, 33.98120769023895, 0.0, 0.0, 0.0], 'rewardMean': 0.6388484491597856, 'totalEpisodes': 45, 'stepsPerEpisode': 67, 'rewardPerEpisode': 52.346982668944996
'totalSteps': 6400, 'rewardStep': 0.8922272117829303, 'errorList': [], 'lossList': [0.0, -1.3797546494007111, 0.0, 74.43724443435669, 0.0, 0.0, 0.0], 'rewardMean': 0.6895242016844145, 'totalEpisodes': 60, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.24466643770386
'totalSteps': 7680, 'rewardStep': 0.7455564628051177, 'errorList': [], 'lossList': [0.0, -1.3744364535808564, 0.0, 125.79612327575684, 0.0, 0.0, 0.0], 'rewardMean': 0.6988629118711983, 'totalEpisodes': 90, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.437993676289608
'totalSteps': 8960, 'rewardStep': 0.6330014782863508, 'errorList': [], 'lossList': [0.0, -1.3715582901239396, 0.0, 88.45337078094482, 0.0, 0.0, 0.0], 'rewardMean': 0.6894541356447916, 'totalEpisodes': 126, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.5942395764651853
'totalSteps': 10240, 'rewardStep': 0.6970645095906296, 'errorList': [], 'lossList': [0.0, -1.3649854701757431, 0.0, 35.64471811771393, 0.0, 0.0, 0.0], 'rewardMean': 0.6904054323880213, 'totalEpisodes': 139, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.260142072910104
'totalSteps': 11520, 'rewardStep': 0.6552889280100114, 'errorList': [], 'lossList': [0.0, -1.3338340026140214, 0.0, 28.517993562221527, 0.0, 0.0, 0.0], 'rewardMean': 0.6865035985682425, 'totalEpisodes': 145, 'stepsPerEpisode': 66, 'rewardPerEpisode': 58.65581431356078
'totalSteps': 12800, 'rewardStep': 0.7774641720788562, 'errorList': [], 'lossList': [0.0, -1.297588080763817, 0.0, 23.611406440734864, 0.0, 0.0, 0.0], 'rewardMean': 0.6955996559193038, 'totalEpisodes': 151, 'stepsPerEpisode': 122, 'rewardPerEpisode': 99.69677593235245
'totalSteps': 14080, 'rewardStep': 0.5392542344197926, 'errorList': [], 'lossList': [0.0, -1.286441822052002, 0.0, 7.596409751176834, 0.0, 0.0, 0.0], 'rewardMean': 0.688458069467098, 'totalEpisodes': 152, 'stepsPerEpisode': 404, 'rewardPerEpisode': 315.733772454831
'totalSteps': 15360, 'rewardStep': 0.5407685146912538, 'errorList': [], 'lossList': [0.0, -1.2791461211442947, 0.0, 7.801097432374954, 0.0, 0.0, 0.0], 'rewardMean': 0.6695742418731363, 'totalEpisodes': 153, 'stepsPerEpisode': 246, 'rewardPerEpisode': 196.13475861701806
'totalSteps': 16640, 'rewardStep': 0.9403329961936313, 'errorList': [0.02579946674369656, 0.049718682610554316, 0.0617748111285643, 0.05823493132622164, 0.030705711433528432, 0.06999649980338943, 0.025758748117109886, 0.02994067169647514, 0.038652653490476044, 0.037722284483323223, 0.04410095013630534, 0.053683021666960616, 0.031972156717178075, 0.050074568377781466, 0.03775158759769758, 0.035693079867053924, 0.03833361630479004, 0.04736431493556039, 0.05405141480028847, 0.040135788194003874, 0.035658860047980946, 0.04967424243632578, 0.031569931249612285, 0.026533538253881963, 0.0309685130401382, 0.03411814262883003, 0.042821520512105075, 0.04711783698147644, 0.025341644368372926, 0.03975410793712854, 0.05283255360124747, 0.05718550531859337, 0.04454279190369955, 0.03405144727426028, 0.028814971337192494, 0.05855917034471356, 0.028315744684315, 0.025709488310342025, 0.042267626329666, 0.05086895280130009, 0.0623974658497886, 0.042370144547726495, 0.03692923748426306, 0.03725969264203137, 0.03920403068641923, 0.07539042041205206, 0.0630302667593677, 0.03744274815737691, 0.03510366001741168, 0.03936361328546976], 'lossList': [0.0, -1.2572342729568482, 0.0, 5.94599425971508, 0.0, 0.0, 0.0], 'rewardMean': 0.7123235603252949, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1028.33548325988, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=16640, timeSpent=54.5
