#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 31
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.7036547309869478, 'errorList': [], 'lossList': [0.0, -1.4091686540842057, 0.0, 27.907749724388122, 0.0, 0.0, 0.0], 'rewardMean': 0.8334721414488288, 'totalEpisodes': 22, 'stepsPerEpisode': 39, 'rewardPerEpisode': 34.79824615200026
'totalSteps': 3840, 'rewardStep': 0.7261168014263513, 'errorList': [], 'lossList': [0.0, -1.38986854493618, 0.0, 40.973144941329956, 0.0, 0.0, 0.0], 'rewardMean': 0.7976870281080028, 'totalEpisodes': 41, 'stepsPerEpisode': 37, 'rewardPerEpisode': 24.73905668925076
'totalSteps': 5120, 'rewardStep': 0.5404721652544691, 'errorList': [], 'lossList': [0.0, -1.3773542076349259, 0.0, 43.376123685836795, 0.0, 0.0, 0.0], 'rewardMean': 0.7333833123946194, 'totalEpisodes': 55, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.428621619950567
'totalSteps': 6400, 'rewardStep': 0.9712759074296434, 'errorList': [278.93068750347226, 320.44003470616246, 335.94399060295865, 320.6182896316085, 301.7385338056643, 335.85792903857975, 354.1366462156057, 338.58318998281754, 376.6088574064652, 381.14407394695775, 287.24691342864566, 228.11020056551808, 386.77527422260977, 311.16602341131176, 316.8896216753945, 373.741974632857, 370.1138491778949, 367.60731567598924, 276.0541666422765, 295.81387690310567, 381.56462633608334, 307.48678717950924, 377.38433607853045, 391.55900297712657, 320.921043664956, 345.0528843481205, 321.5467337393775, 362.88554934147265, 292.30453069465904, 405.9927489919316, 244.00877590601615, 351.1903081107466, 225.09996034393964, 269.4043216902775, 333.05460660837616, 343.58090923544444, 341.5195136362771, 384.6894803427858, 240.36410382593448, 346.9147207174117, 189.98521815544132, 307.1564535688011, 359.9668785480045, 349.8290561931019, 287.760380281024, 311.3575307952882, 345.703941580421, 247.15547720930053, 367.5944098704517, 355.0993148562592], 'lossList': [0.0, -1.3725233644247055, 0.0, 105.29424308776855, 0.0, 0.0, 0.0], 'rewardMean': 0.7809618314016242, 'totalEpisodes': 78, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9712759074296434, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.6662085963876474, 'errorList': [], 'lossList': [0.0, -1.3775000590085984, 0.0, 87.68906084060669, 0.0, 0.0, 0.0], 'rewardMean': 0.761836292232628, 'totalEpisodes': 107, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.946925419481971
'totalSteps': 8960, 'rewardStep': 0.29419005918159985, 'errorList': [], 'lossList': [0.0, -1.3778126209974288, 0.0, 56.43143804550171, 0.0, 0.0, 0.0], 'rewardMean': 0.6950296875110525, 'totalEpisodes': 122, 'stepsPerEpisode': 110, 'rewardPerEpisode': 71.00709120774742
'totalSteps': 10240, 'rewardStep': 0.5018981508762719, 'errorList': [], 'lossList': [0.0, -1.3742172938585282, 0.0, 23.715083327293396, 0.0, 0.0, 0.0], 'rewardMean': 0.670888245431705, 'totalEpisodes': 127, 'stepsPerEpisode': 173, 'rewardPerEpisode': 124.12904464259262
'totalSteps': 11520, 'rewardStep': 0.4587096334355823, 'errorList': [], 'lossList': [0.0, -1.3699057084321975, 0.0, 41.17274712085724, 0.0, 0.0, 0.0], 'rewardMean': 0.6473128440988024, 'totalEpisodes': 133, 'stepsPerEpisode': 13, 'rewardPerEpisode': 6.254745850001077
'totalSteps': 12800, 'rewardStep': 0.5926507273475399, 'errorList': [], 'lossList': [0.0, -1.350424857735634, 0.0, 20.89478224515915, 0.0, 0.0, 0.0], 'rewardMean': 0.6418466324236762, 'totalEpisodes': 137, 'stepsPerEpisode': 500, 'rewardPerEpisode': 361.43139259803473
'totalSteps': 14080, 'rewardStep': 0.7332877485701188, 'errorList': [], 'lossList': [0.0, -1.3303371566534041, 0.0, 8.819831356108189, 0.0, 0.0, 0.0], 'rewardMean': 0.6188464520896171, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 961.6483206749656
'totalSteps': 15360, 'rewardStep': 0.705022595993763, 'errorList': [], 'lossList': [0.0, -1.3051730954647065, 0.0, 9.115668255984783, 0.0, 0.0, 0.0], 'rewardMean': 0.6189832385902987, 'totalEpisodes': 138, 'stepsPerEpisode': 497, 'rewardPerEpisode': 399.6982974529142
'totalSteps': 16640, 'rewardStep': 0.6654941642803424, 'errorList': [], 'lossList': [0.0, -1.2992587792873382, 0.0, 3.7196151861548423, 0.0, 0.0, 0.0], 'rewardMean': 0.6129209748756977, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 927.2630949334941
'totalSteps': 17920, 'rewardStep': 0.6616438457023267, 'errorList': [], 'lossList': [0.0, -1.2843452447652817, 0.0, 1.9956496956944465, 0.0, 0.0, 0.0], 'rewardMean': 0.6250381429204835, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 953.5601666846045
'totalSteps': 19200, 'rewardStep': 0.7657249542789537, 'errorList': [], 'lossList': [0.0, -1.245052398443222, 0.0, 0.7299995413422584, 0.0, 0.0, 0.0], 'rewardMean': 0.6044830476054146, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 892.7027938219817
'totalSteps': 20480, 'rewardStep': 0.7616285849852901, 'errorList': [], 'lossList': [0.0, -1.2117615973949432, 0.0, 1.3089508783817292, 0.0, 0.0, 0.0], 'rewardMean': 0.6140250464651789, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1028.899472390206
'totalSteps': 21760, 'rewardStep': 0.8799027038104557, 'errorList': [], 'lossList': [0.0, -1.1627178555727005, 0.0, 1.2525333609431981, 0.0, 0.0, 0.0], 'rewardMean': 0.6725963109280644, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.950523738698
'totalSteps': 23040, 'rewardStep': 0.943667237756977, 'errorList': [0.1370257794137265, 0.1537921565108824, 0.11353336703381398, 0.12576986932485482, 0.1800291437165283, 0.11881639130371531, 0.14263762161090213, 0.1412027116939649, 0.07918058855291049, 0.15456893665519933, 0.10748263338717202, 0.0969914670646068, 0.12933270320228857, 0.13892828245713174, 0.20857081801651955, 0.11741089717700524, 0.1420535997244384, 0.09959010121692544, 0.12015761694425502, 0.11565572241628336, 0.1542302210206529, 0.15960994973334403, 0.13069227763977803, 0.15537129436058442, 0.1374149848336453, 0.10786465161784599, 0.14401420734771173, 0.1415731411195416, 0.10034440799999537, 0.11699367472540331, 0.10574574493887476, 0.14496687582594978, 0.16511101964990593, 0.14590513540614047, 0.13561999511936118, 0.11311135017407445, 0.169081205389399, 0.16489326538027768, 0.14333820536595004, 0.13556004742706737, 0.11654194941161752, 0.15825137953306861, 0.1404840374724374, 0.20024204764675024, 0.17474551269115254, 0.12336832553614709, 0.16528271228875835, 0.18923049067289488, 0.15049826645383133, 0.08620411768635676], 'lossList': [0.0, -1.1283328235149384, 0.0, 0.8140650434792042, 0.0, 0.0, 0.0], 'rewardMean': 0.7167732196161349, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.7286020325014, 'successfulTests': 48
'totalSteps': 24320, 'rewardStep': 0.8802118513208527, 'errorList': [], 'lossList': [0.0, -1.1016281485557555, 0.0, 0.6430307637900115, 0.0, 0.0, 0.0], 'rewardMean': 0.7589234414046621, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.5317273651017
'totalSteps': 25600, 'rewardStep': 0.9813933474170067, 'errorList': [0.10437269211486348, 0.12446786346888845, 0.16617875734352944, 0.10097436716883086, 0.12888340964090697, 0.184984714914061, 0.1510752652118071, 0.1780062255517064, 0.19121158999442536, 0.15508223890842546, 0.12541781237725402, 0.15730575944509523, 0.1578468681574858, 0.14387894386278405, 0.1459130850489887, 0.14950209273839782, 0.1360720743470541, 0.12753945756644497, 0.22567460464871944, 0.13577910693097806, 0.18348329182572837, 0.1417496734231961, 0.1575403229369382, 0.2009446967339689, 0.14397517938720603, 0.15832493554194757, 0.22744034662125365, 0.19658447606710763, 0.14444450027313355, 0.1363265472877954, 0.1722350972993306, 0.09856169697565201, 0.11650287517011508, 0.16928308832727249, 0.20240852464869283, 0.15180958341035572, 0.24226080714122586, 0.11094627076109992, 0.17466970864078132, 0.2017396747183226, 0.14264211353965628, 0.14396446585421632, 0.1525576293676498, 0.20203878949025955, 0.16081260536559291, 0.13966866829807287, 0.1439000059712275, 0.16745009822433918, 0.2008593913343466, 0.1273889517458315], 'lossList': [0.0, -1.058477474451065, 0.0, 0.5777109712362289, 0.0, 0.0, 0.0], 'rewardMean': 0.7977977034116088, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1207.9835236202598, 'successfulTests': 42
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=23040, timeSpent=114.58
