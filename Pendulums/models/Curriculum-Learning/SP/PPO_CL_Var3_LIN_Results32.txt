#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 32
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.3412649119412833, 'errorList': [], 'lossList': [0.0, -1.423034406900406, 0.0, 27.72029501438141, 0.0, 0.0, 0.0], 'rewardMean': 0.5675174565607173, 'totalEpisodes': 26, 'stepsPerEpisode': 133, 'rewardPerEpisode': 74.52691889785277
'totalSteps': 3840, 'rewardStep': 0.6738168367325699, 'errorList': [], 'lossList': [0.0, -1.4179648792743682, 0.0, 44.84231925010681, 0.0, 0.0, 0.0], 'rewardMean': 0.6029505832846681, 'totalEpisodes': 42, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.438265329347495
'totalSteps': 5120, 'rewardStep': 0.1804179360336679, 'errorList': [], 'lossList': [0.0, -1.399947018623352, 0.0, 61.96162925720215, 0.0, 0.0, 0.0], 'rewardMean': 0.4973174214719181, 'totalEpisodes': 57, 'stepsPerEpisode': 87, 'rewardPerEpisode': 51.20641933970019
'totalSteps': 6400, 'rewardStep': 0.9389438963674905, 'errorList': [311.50327635258276, 258.16691174801576, 216.9230927874777, 266.31774269633775, 348.2392808918337, 319.9872023938855, 150.49489006418182, 198.56770389092662, 268.91618011573644, 63.624530835879426, 254.56653880266126, 286.4584840726858, 2.4730823835425544, 187.14852823382043, 176.27706920574096, 294.81736768728143, 337.87662077360017, 262.3311977286285, 286.5345858744162, 315.9493781520038, 303.6442446680445, 229.4513154555801, 329.8650923220129, 233.84548490208053, 236.18458625536172, 240.38568434877905, 301.4292183562065, 134.59181516149602, 295.61784328779316, 157.88230188603288, 236.48251744342528, 309.0698530287183, 343.1247163513673, 270.1281005803506, 301.1216165617123, 224.13759130059597, 202.63181931536522, 329.5625121358202, 321.59163738896234, 333.24754605601856, 264.5332812191328, 155.20139575820977, 75.59350312502025, 322.46020261223674, 259.9793775588683, 305.15855005902483, 312.1003592699249, 103.28819111326789, 302.49915433050467, 14.53968882416149], 'lossList': [0.0, -1.3937006777524947, 0.0, 88.88091257095337, 0.0, 0.0, 0.0], 'rewardMean': 0.5856427164510325, 'totalEpisodes': 81, 'stepsPerEpisode': 42, 'rewardPerEpisode': 27.777848783331628, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.8443298416976135, 'errorList': [], 'lossList': [0.0, -1.3806536996364593, 0.0, 100.57390434265136, 0.0, 0.0, 0.0], 'rewardMean': 0.6287572373254627, 'totalEpisodes': 119, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.812749920878474
'totalSteps': 8960, 'rewardStep': 0.6319023944960099, 'errorList': [], 'lossList': [0.0, -1.3763559651374817, 0.0, 61.32280843734741, 0.0, 0.0, 0.0], 'rewardMean': 0.6292065454926837, 'totalEpisodes': 137, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.002338700075239
'totalSteps': 10240, 'rewardStep': 0.6435746208129035, 'errorList': [], 'lossList': [0.0, -1.3751185911893844, 0.0, 29.68818839073181, 0.0, 0.0, 0.0], 'rewardMean': 0.6310025549077112, 'totalEpisodes': 142, 'stepsPerEpisode': 172, 'rewardPerEpisode': 133.30394211085206
'totalSteps': 11520, 'rewardStep': 0.37237289192080825, 'errorList': [], 'lossList': [0.0, -1.365584312081337, 0.0, 22.44372477889061, 0.0, 0.0, 0.0], 'rewardMean': 0.6022659256869441, 'totalEpisodes': 145, 'stepsPerEpisode': 518, 'rewardPerEpisode': 335.2579542312867
'totalSteps': 12800, 'rewardStep': 0.739300166129419, 'errorList': [], 'lossList': [0.0, -1.368029652237892, 0.0, 30.023542416095733, 0.0, 0.0, 0.0], 'rewardMean': 0.6159693497311916, 'totalEpisodes': 151, 'stepsPerEpisode': 142, 'rewardPerEpisode': 103.92146343615737
'totalSteps': 14080, 'rewardStep': 0.6183274242459011, 'errorList': [], 'lossList': [0.0, -1.3389131379127504, 0.0, 6.400311838686466, 0.0, 0.0, 0.0], 'rewardMean': 0.5984250920377666, 'totalEpisodes': 153, 'stepsPerEpisode': 357, 'rewardPerEpisode': 267.05222728462604
'totalSteps': 15360, 'rewardStep': 0.7201783262474173, 'errorList': [], 'lossList': [0.0, -1.3066236877441406, 0.0, 7.735234487950802, 0.0, 0.0, 0.0], 'rewardMean': 0.6363164334683802, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 917.2017063809712
'totalSteps': 16640, 'rewardStep': 0.9514545968804454, 'errorList': [0.18205321798218246, 0.09896292197440985, 0.21838894538555592, 0.13022756910338962, 0.10564960299973053, 0.15237051814280322, 0.18201600968037385, 0.12431372475401019, 0.31305166394312073, 0.17251894555364755, 0.39577238359420247, 0.2906480653117886, 0.20533525996229499, 0.19147241332176576, 0.23963093847713254, 0.18581593524329734, 0.24060923054430242, 0.18864858927764722, 0.13554405044734, 0.1657095799817389, 0.2236591515003268, 0.16479676170427643, 0.18455740044755614, 0.2268686545566807, 0.19425515268401689, 0.21189857387081482, 0.12323951162128885, 0.14055629073911183, 0.13649279373825265, 0.31619057954797986, 0.18781165372876726, 0.2713594411478791, 0.15910447747759976, 0.19880239536543315, 0.16321035166725417, 0.3482929584287787, 0.2147939176051122, 0.09634679035474722, 0.19454508536728496, 0.1293912413038542, 0.21477167835011776, 0.18048644945872175, 0.16472126565755765, 0.1505920583084685, 0.2288202332870271, 0.22180626409850293, 0.18074965684043384, 0.3588625011965709, 0.1503478740203986, 0.19072091973341843], 'lossList': [0.0, -1.3117339491844178, 0.0, 4.787448030114174, 0.0, 0.0, 0.0], 'rewardMean': 0.6640802094831676, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 952.5154702843671, 'successfulTests': 32
'totalSteps': 17920, 'rewardStep': 0.8521784379624502, 'errorList': [], 'lossList': [0.0, -1.304112077355385, 0.0, 4.942514223158359, 0.0, 0.0, 0.0], 'rewardMean': 0.7312562596760459, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.4201130007673
'totalSteps': 19200, 'rewardStep': 0.8378185557808484, 'errorList': [], 'lossList': [0.0, -1.2764771223068236, 0.0, 2.976968217641115, 0.0, 0.0, 0.0], 'rewardMean': 0.7211437256173817, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.1331698292586
'totalSteps': 20480, 'rewardStep': 0.9734249810946562, 'errorList': [0.15449075465801318, 0.1213089488578565, 0.12133010008801738, 0.20248158131666344, 0.2972574412893991, 0.17085900135403842, 0.21551259815280238, 0.1527491237984479, 0.15819153017836848, 0.16498754661394127, 0.21907533772131133, 0.16162586911203333, 0.11020737074511228, 0.1498085684735049, 0.1590597068560379, 0.11538701339514096, 0.22330227873319353, 0.1492514692848898, 0.1708564644847426, 0.2031653561613942, 0.2162782611712838, 0.15631204354323958, 0.1542027577958991, 0.2193992674068001, 0.1379397561276873, 0.20752076502831052, 0.13469122716096854, 0.15446220074328548, 0.1865728453816191, 0.16216460097880986, 0.11207402743826034, 0.1821772123089564, 0.12077867395734217, 0.13063638731068, 0.16106510459366896, 0.2595541378640895, 0.11696660149972887, 0.1468907484578576, 0.20726858742966453, 0.19930920208283698, 0.18370247598548006, 0.17501970560895191, 0.24019267105692718, 0.15878897213074242, 0.17727878055602353, 0.3187980168713362, 0.1679325294042522, 0.12716528243681927, 0.172646996515553, 0.11760379450408708], 'lossList': [0.0, -1.2467560243606568, 0.0, 3.068991252183914, 0.0, 0.0, 0.0], 'rewardMean': 0.7340532395570859, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.346914436734, 'successfulTests': 37
'totalSteps': 21760, 'rewardStep': 0.9802011260381316, 'errorList': [0.030096474999925408, 0.016925084322453467, 0.03248385726737532, 0.0639051222624354, 0.015997695171531053, 0.013101952082063389, 0.12316866548159869, 0.017608108173913558, 0.020802896196115396, 0.013463360031901898, 0.01833105921199326, 0.018306278363096996, 0.08371647403013842, 0.13330684546875468, 0.013178767546216545, 0.00947052272055614, 0.09928135790650526, 0.015313105414207735, 0.08705867857402447, 0.014013124258257289, 0.02675395167347942, 0.046491770620129706, 0.09632720932915542, 0.00836252194755225, 0.00533044499352951, 0.026857384222853116, 0.06232171132374271, 0.022324768171401235, 0.09656175483415087, 0.05160663234805495, 0.05447308064757832, 0.04085789349097735, 0.08056295035453775, 0.05015966886773533, 0.019093355896378, 0.03550728383914981, 0.01577835581496319, 0.02048035963170442, 0.01083616467013916, 0.03003375843505456, 0.03848194276248885, 0.04696268966964701, 0.02142161169081105, 0.048723501410041295, 0.02201181491685051, 0.10931774334516058, 0.04811749670812926, 0.013762256346632917, 0.021037084008125616, 0.05571575292475223], 'lossList': [0.0, -1.1986164742708205, 0.0, 1.9915325659140946, 0.0, 0.0, 0.0], 'rewardMean': 0.7688831127112982, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.237288744812, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=128.45
