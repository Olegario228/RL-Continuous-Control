#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 34
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8205739103498719, 'errorList': [], 'lossList': [0.0, -1.409426463842392, 0.0, 30.616635112762452, 0.0, 0.0, 0.0], 'rewardMean': 0.5923220080627303, 'totalEpisodes': 27, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.387768993428907
'totalSteps': 3840, 'rewardStep': 0.8608343777988061, 'errorList': [], 'lossList': [0.0, -1.4116768395900727, 0.0, 31.64863498210907, 0.0, 0.0, 0.0], 'rewardMean': 0.6818261313080889, 'totalEpisodes': 44, 'stepsPerEpisode': 56, 'rewardPerEpisode': 46.83469146892595
'totalSteps': 5120, 'rewardStep': 0.864169170683289, 'errorList': [], 'lossList': [0.0, -1.4026395046710969, 0.0, 47.69699798583984, 0.0, 0.0, 0.0], 'rewardMean': 0.727411891151889, 'totalEpisodes': 55, 'stepsPerEpisode': 42, 'rewardPerEpisode': 37.523834734928954
'totalSteps': 6400, 'rewardStep': 0.5901106700821317, 'errorList': [], 'lossList': [0.0, -1.3830147594213487, 0.0, 76.4382343864441, 0.0, 0.0, 0.0], 'rewardMean': 0.6999516469379375, 'totalEpisodes': 67, 'stepsPerEpisode': 36, 'rewardPerEpisode': 21.667076247070536
'totalSteps': 7680, 'rewardStep': 0.4636659663209709, 'errorList': [], 'lossList': [0.0, -1.374567009806633, 0.0, 119.920202293396, 0.0, 0.0, 0.0], 'rewardMean': 0.6605707001684431, 'totalEpisodes': 101, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.473892860875296
'totalSteps': 8960, 'rewardStep': 0.74116325839854, 'errorList': [], 'lossList': [0.0, -1.3737293082475661, 0.0, 49.57464332580567, 0.0, 0.0, 0.0], 'rewardMean': 0.6720839227727426, 'totalEpisodes': 124, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.256006675296569
'totalSteps': 10240, 'rewardStep': 0.9426613839604159, 'errorList': [1.3937256990106337, 2.142374228633052, 13.479234179878285, 3.365097685717277, 0.4915504264205683, 0.618719139444273, 12.61738956296031, 6.265535717379411, 11.652948225620824, 8.091295977876607, 5.309731987232812, 2.5542828747960526, 5.807023415301927, 6.76706137594851, 0.4594126016357782, 7.236121615235319, 1.2208690955108397, 5.136420145968114, 7.438945278891762, 10.462520551876322, 12.070451509472337, 3.0634385921191156, 0.2760459020724197, 2.9083102924243245, 6.656083637629518, 10.448409082428448, 2.2636517311230513, 14.213508909868485, 13.367702437465166, 12.874290151545324, 3.4716323615420794, 5.792390062645899, 3.180215926772577, 6.976583550620352, 4.4227062353663875, 2.514436875746698, 4.805550485250627, 4.752370677375984, 8.068884137600158, 10.759779355770174, 9.082580029478233, 3.539918051704765, 5.471474699807212, 12.664397751164078, 10.590654789067251, 9.860174497252608, 4.236891834140906, 0.5553800943324345, 9.567448268288532, 7.648711452857178], 'lossList': [0.0, -1.3560704654455185, 0.0, 25.350755996704102, 0.0, 0.0, 0.0], 'rewardMean': 0.7059061054212018, 'totalEpisodes': 130, 'stepsPerEpisode': 83, 'rewardPerEpisode': 71.7756224995648, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6258236010887691, 'errorList': [], 'lossList': [0.0, -1.325994566679001, 0.0, 16.435810124874116, 0.0, 0.0, 0.0], 'rewardMean': 0.6970080493842649, 'totalEpisodes': 131, 'stepsPerEpisode': 854, 'rewardPerEpisode': 654.4391445860522
'totalSteps': 12800, 'rewardStep': 0.9188654395302681, 'errorList': [], 'lossList': [0.0, -1.3036852943897248, 0.0, 17.35378234386444, 0.0, 0.0, 0.0], 'rewardMean': 0.7191937883988653, 'totalEpisodes': 137, 'stepsPerEpisode': 146, 'rewardPerEpisode': 116.68443412541733
'totalSteps': 14080, 'rewardStep': 0.9374448132166295, 'errorList': [1.7324476442470484, 2.4149749179172044, 1.8263170428965263, 1.7446173720677414, 2.2335597641562126, 2.3850162669396293, 1.789573934136334, 2.209925094828613, 2.046718996488966, 2.301951471700323, 2.190394180848417, 2.503730552981587, 2.399806290071961, 2.4368087461047834, 1.6335529600764593, 1.86573526744986, 1.45109605212507, 1.838483747782316, 1.9751099386340676, 2.4205840920843125, 1.6911328026216266, 2.511054109891508, 2.143323360231012, 2.0793551953480605, 2.1816544433230693, 1.8576117453030105, 2.160604605008034, 1.9924435406631698, 1.9435494542527931, 1.763634990935884, 1.867610435211437, 2.4982213449371335, 2.491660738136567, 1.7840247739728174, 1.6915011758260559, 2.156408637815381, 1.5801277030640777, 2.0424342303118426, 2.0475060934802585, 1.5946812762253277, 2.0254549799675847, 2.157166217808816, 1.9156474573943443, 1.5041692733424383, 2.016095748099542, 2.358285534981749, 2.459990645114908, 1.838343340978176, 2.3167666750377194, 2.180978721807507], 'lossList': [0.0, -1.300940889120102, 0.0, 7.44994383096695, 0.0, 0.0, 0.0], 'rewardMean': 0.7765312591429693, 'totalEpisodes': 141, 'stepsPerEpisode': 123, 'rewardPerEpisode': 107.84410382097012, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7714196602479063, 'errorList': [], 'lossList': [0.0, -1.2973953533172606, 0.0, 21.95912041425705, 0.0, 0.0, 0.0], 'rewardMean': 0.7716158341327726, 'totalEpisodes': 145, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.208728243711298
'totalSteps': 16640, 'rewardStep': 0.39848450885112574, 'errorList': [], 'lossList': [0.0, -1.2768300914764403, 0.0, 4.4603718489408495, 0.0, 0.0, 0.0], 'rewardMean': 0.7253808472380047, 'totalEpisodes': 147, 'stepsPerEpisode': 281, 'rewardPerEpisode': 207.68837345002882
'totalSteps': 17920, 'rewardStep': 0.8707255521831485, 'errorList': [], 'lossList': [0.0, -1.261678860783577, 0.0, 9.962259510755539, 0.0, 0.0, 0.0], 'rewardMean': 0.7260364853879906, 'totalEpisodes': 153, 'stepsPerEpisode': 32, 'rewardPerEpisode': 30.17723550489573
'totalSteps': 19200, 'rewardStep': 0.7389772601385365, 'errorList': [], 'lossList': [0.0, -1.264379010796547, 0.0, 3.5145677819848062, 0.0, 0.0, 0.0], 'rewardMean': 0.740923144393631, 'totalEpisodes': 155, 'stepsPerEpisode': 566, 'rewardPerEpisode': 475.878318536466
'totalSteps': 20480, 'rewardStep': 0.919064491148315, 'errorList': [], 'lossList': [0.0, -1.2655246770381927, 0.0, 2.720840657353401, 0.0, 0.0, 0.0], 'rewardMean': 0.7864629968763654, 'totalEpisodes': 157, 'stepsPerEpisode': 347, 'rewardPerEpisode': 257.8418282617818
'totalSteps': 21760, 'rewardStep': 0.7078066241607859, 'errorList': [], 'lossList': [0.0, -1.2604625368118285, 0.0, 2.553910672366619, 0.0, 0.0, 0.0], 'rewardMean': 0.7831273334525901, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 939.7442784260638
'totalSteps': 23040, 'rewardStep': 0.8926702979451666, 'errorList': [], 'lossList': [0.0, -1.2497271466255189, 0.0, 2.7385377043485644, 0.0, 0.0, 0.0], 'rewardMean': 0.7781282248510651, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.2930313115166
'totalSteps': 24320, 'rewardStep': 0.8729784392258444, 'errorList': [], 'lossList': [0.0, -1.2190667307376861, 0.0, 0.9629789549857378, 0.0, 0.0, 0.0], 'rewardMean': 0.8028437086647727, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.909301023262
'totalSteps': 25600, 'rewardStep': 0.9643185190572627, 'errorList': [0.009963497138365114, 0.00987767045833787, 0.007509849977507606, 0.010404578699234945, 0.01902300948936564, 0.028109456850268538, 0.009166795589315657, 0.007869942571190504, 0.017196307396333585, 0.027530036519173618, 0.02352744997367473, 0.014500224761228017, 0.023597985192890688, 0.036343972665669105, 0.03301336354897748, 0.019944227596171715, 0.028918296802541547, 0.01126847519671815, 0.026946996415352722, 0.008833916141301006, 0.0406682922219942, 0.012679214013898666, 0.04330400767440189, 0.011729548470156056, 0.008762341488150915, 0.02085241274154699, 0.014171210612538744, 0.00979279677903101, 0.008277561376321818, 0.013870291510316057, 0.012721255954370746, 0.018210180455548168, 0.009341863220905964, 0.02627475253410058, 0.020958994051813177, 0.00858812030814593, 0.008684871350090492, 0.027559197809479803, 0.007672873015796026, 0.027352446462197037, 0.02636875052224771, 0.035122040985153015, 0.022905653603824818, 0.0077542775462068, 0.012935696444581016, 0.030308157494121513, 0.031018463473785843, 0.007806069800218915, 0.009487398448240512, 0.04752649957191137], 'lossList': [0.0, -1.175712142586708, 0.0, 0.7226593645662069, 0.0, 0.0, 0.0], 'rewardMean': 0.8073890166174722, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.8155418842455, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=114.28
