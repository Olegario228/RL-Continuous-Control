#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 35
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.8944858781539997, 'errorList': [], 'lossList': [0.0, -1.4202724832296372, 0.0, 24.836352142095567, 0.0, 0.0, 0.0], 'rewardMean': 0.8471424193264814, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 366.70690495882866
'totalSteps': 3840, 'rewardStep': 0.45026799060528633, 'errorList': [], 'lossList': [0.0, -1.4244656437635421, 0.0, 45.26062394142151, 0.0, 0.0, 0.0], 'rewardMean': 0.7148509430860831, 'totalEpisodes': 16, 'stepsPerEpisode': 153, 'rewardPerEpisode': 115.702341850083
'totalSteps': 5120, 'rewardStep': 0.6969526099298531, 'errorList': [], 'lossList': [0.0, -1.4229525166749954, 0.0, 34.69930917263031, 0.0, 0.0, 0.0], 'rewardMean': 0.7103763597970256, 'totalEpisodes': 21, 'stepsPerEpisode': 192, 'rewardPerEpisode': 142.88585375709877
'totalSteps': 6400, 'rewardStep': 0.8644962012767333, 'errorList': [], 'lossList': [0.0, -1.4192304748296738, 0.0, 87.28133205413819, 0.0, 0.0, 0.0], 'rewardMean': 0.7412003280929672, 'totalEpisodes': 34, 'stepsPerEpisode': 33, 'rewardPerEpisode': 22.174252187193314
'totalSteps': 7680, 'rewardStep': 0.5944083309480432, 'errorList': [], 'lossList': [0.0, -1.416834009885788, 0.0, 160.19284339904786, 0.0, 0.0, 0.0], 'rewardMean': 0.7167349952354799, 'totalEpisodes': 67, 'stepsPerEpisode': 43, 'rewardPerEpisode': 30.051566793595775
'totalSteps': 8960, 'rewardStep': 0.7887820413861769, 'errorList': [], 'lossList': [0.0, -1.4142925477027892, 0.0, 96.08257802963257, 0.0, 0.0, 0.0], 'rewardMean': 0.7270274303998651, 'totalEpisodes': 97, 'stepsPerEpisode': 24, 'rewardPerEpisode': 21.095628404530014
'totalSteps': 10240, 'rewardStep': 0.40985210194064403, 'errorList': [], 'lossList': [0.0, -1.4019790744781495, 0.0, 52.36350791931152, 0.0, 0.0, 0.0], 'rewardMean': 0.6873805143424625, 'totalEpisodes': 110, 'stepsPerEpisode': 101, 'rewardPerEpisode': 77.01636058877061
'totalSteps': 11520, 'rewardStep': 0.8142577422185189, 'errorList': [], 'lossList': [0.0, -1.3837885624170303, 0.0, 41.796128091812136, 0.0, 0.0, 0.0], 'rewardMean': 0.7014779841064688, 'totalEpisodes': 118, 'stepsPerEpisode': 72, 'rewardPerEpisode': 57.9503172845943
'totalSteps': 12800, 'rewardStep': 0.7848339004853979, 'errorList': [], 'lossList': [0.0, -1.3675197553634644, 0.0, 18.761175948381425, 0.0, 0.0, 0.0], 'rewardMean': 0.7098135757443618, 'totalEpisodes': 121, 'stepsPerEpisode': 480, 'rewardPerEpisode': 346.57342475810555
'totalSteps': 14080, 'rewardStep': 0.6539714226569819, 'errorList': [], 'lossList': [0.0, -1.3569311445951462, 0.0, 15.452751930952072, 0.0, 0.0, 0.0], 'rewardMean': 0.6952308219601636, 'totalEpisodes': 122, 'stepsPerEpisode': 556, 'rewardPerEpisode': 413.46784969946697
'totalSteps': 15360, 'rewardStep': 0.836855108596535, 'errorList': [], 'lossList': [0.0, -1.3589346295595168, 0.0, 49.91946422100067, 0.0, 0.0, 0.0], 'rewardMean': 0.6894677450044171, 'totalEpisodes': 131, 'stepsPerEpisode': 67, 'rewardPerEpisode': 56.71321999270589
'totalSteps': 16640, 'rewardStep': 0.8580820607443443, 'errorList': [], 'lossList': [0.0, -1.3346539622545242, 0.0, 7.672473335266114, 0.0, 0.0, 0.0], 'rewardMean': 0.7302491520183229, 'totalEpisodes': 134, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.16658966130525
'totalSteps': 17920, 'rewardStep': 0.8621873597176966, 'errorList': [], 'lossList': [0.0, -1.325778116583824, 0.0, 9.950073437690735, 0.0, 0.0, 0.0], 'rewardMean': 0.7467726269971072, 'totalEpisodes': 141, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.120749641910281
'totalSteps': 19200, 'rewardStep': 0.9529909265957068, 'errorList': [6.927792897949595, 5.214917928927837, 3.041618267555181, 2.3131738125590786, 2.7700906573493245, 8.136654789494928, 8.339194987156318, 8.739769727048932, 2.0867588099735483, 4.919679408915906, 3.3064558635494703, 4.790152245674785, 6.284401420532237, 1.7223260683132708, 1.795648676328595, 3.3005555956954193, 5.766220487400066, 5.305347809898874, 5.492407064139938, 7.8116227120368515, 4.024689223407435, 6.933870663424115, 8.407745746657797, 2.4595950696987487, 7.802952697205161, 7.3448301506377405, 5.4203292053196535, 5.371554007395493, 9.268955458446898, 1.6142872229071927, 5.53261109468437, 3.810404777391373, 8.345665527409887, 5.288914430879318, 5.851114057767176, 6.1717942286814935, 7.787688093116281, 7.565417116744859, 4.865788078632312, 2.9978583633454745, 3.0417165884068504, 7.840548031584147, 7.816459230610327, 3.011475437477327, 8.573145121137687, 5.415129726740203, 5.329374317881518, 8.397190440390844, 5.521992490197135, 2.557128180671064], 'lossList': [0.0, -1.3344991332292557, 0.0, 25.211880495548247, 0.0, 0.0, 0.0], 'rewardMean': 0.7556220995290046, 'totalEpisodes': 147, 'stepsPerEpisode': 30, 'rewardPerEpisode': 28.5566349222589, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8870939880890518, 'errorList': [], 'lossList': [0.0, -1.3251268458366394, 0.0, 5.369389035701752, 0.0, 0.0, 0.0], 'rewardMean': 0.7848906652431054, 'totalEpisodes': 151, 'stepsPerEpisode': 123, 'rewardPerEpisode': 104.87483991262525
'totalSteps': 21760, 'rewardStep': 0.944947284730776, 'errorList': [0.7153360324459972, 0.4150132494658541, 1.4329595839486062, 1.2924959049444715, 1.3724529050903849, 0.12472740363851127, 1.9948068082222195, 1.0705632847322446, 0.6060010429970056, 1.2442582150322667, 4.426930313561907, 1.0873888728298495, 1.5654648825281618, 0.1042272186991933, 2.541248443861782, 0.24347359184459974, 1.8929694737628573, 1.8591662912133167, 1.7209653511561556, 1.5310263314916848, 0.8919977781659841, 1.7125796135941276, 0.24921110357412188, 0.5511422161737715, 0.4284088969364229, 4.751724893874556, 1.528700722362398, 0.3280111956540533, 2.410905864780814, 0.4060510187859236, 3.0517544122004536, 2.7422495336093147, 1.1219029406461762, 1.5227402054171797, 0.8233596890140182, 0.7360593634605336, 1.147559259328645, 0.8374046135498724, 1.0416060335708055, 0.5660394369982722, 0.6769004011471823, 3.1727212951542754, 1.9206911024704085, 0.10190340798314358, 1.2607964199687167, 0.6162001655745003, 2.4588196928258896, 2.8843785043796233, 1.4034801120421236, 1.0660553644765334], 'lossList': [0.0, -1.328035348057747, 0.0, 3.718451436161995, 0.0, 0.0, 0.0], 'rewardMean': 0.8005071895775654, 'totalEpisodes': 154, 'stepsPerEpisode': 284, 'rewardPerEpisode': 247.1027712597294, 'successfulTests': 3
'totalSteps': 23040, 'rewardStep': 0.7646945311988783, 'errorList': [], 'lossList': [0.0, -1.3346864742040634, 0.0, 4.2028137737512585, 0.0, 0.0, 0.0], 'rewardMean': 0.8359914325033888, 'totalEpisodes': 157, 'stepsPerEpisode': 58, 'rewardPerEpisode': 48.58682445448464
'totalSteps': 24320, 'rewardStep': 0.7519383873013552, 'errorList': [], 'lossList': [0.0, -1.3260159939527512, 0.0, 3.739363370835781, 0.0, 0.0, 0.0], 'rewardMean': 0.8297594970116723, 'totalEpisodes': 159, 'stepsPerEpisode': 186, 'rewardPerEpisode': 155.8777299412766
'totalSteps': 25600, 'rewardStep': 0.8167593570181415, 'errorList': [], 'lossList': [0.0, -1.32190793633461, 0.0, 4.339217489361763, 0.0, 0.0, 0.0], 'rewardMean': 0.8329520426649468, 'totalEpisodes': 161, 'stepsPerEpisode': 261, 'rewardPerEpisode': 238.80063049187967
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=21760, timeSpent=94.96
