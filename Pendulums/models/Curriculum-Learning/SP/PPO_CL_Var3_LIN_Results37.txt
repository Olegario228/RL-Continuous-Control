#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 37
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.7343485431697397, 'errorList': [], 'lossList': [0.0, -1.4378436183929444, 0.0, 28.214116325378416, 0.0, 0.0, 0.0], 'rewardMean': 0.5861895676878068, 'totalEpisodes': 15, 'stepsPerEpisode': 586, 'rewardPerEpisode': 394.4889796608627
'totalSteps': 3840, 'rewardStep': 0.8601636280367179, 'errorList': [], 'lossList': [0.0, -1.4385881406068801, 0.0, 37.318773531913756, 0.0, 0.0, 0.0], 'rewardMean': 0.6775142544707772, 'totalEpisodes': 23, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.8265158904408025
'totalSteps': 5120, 'rewardStep': 0.49597865801972196, 'errorList': [], 'lossList': [0.0, -1.4278724265098572, 0.0, 54.23773561477661, 0.0, 0.0, 0.0], 'rewardMean': 0.6321303553580133, 'totalEpisodes': 37, 'stepsPerEpisode': 83, 'rewardPerEpisode': 56.42301230397594
'totalSteps': 6400, 'rewardStep': 0.5163612020573362, 'errorList': [], 'lossList': [0.0, -1.4148929578065872, 0.0, 91.08551605224609, 0.0, 0.0, 0.0], 'rewardMean': 0.6089765246978779, 'totalEpisodes': 57, 'stepsPerEpisode': 32, 'rewardPerEpisode': 25.669508278683367
'totalSteps': 7680, 'rewardStep': 0.8353507804829762, 'errorList': [], 'lossList': [0.0, -1.3980289554595948, 0.0, 106.72370185852051, 0.0, 0.0, 0.0], 'rewardMean': 0.6467055673287275, 'totalEpisodes': 91, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.881126042501315
'totalSteps': 8960, 'rewardStep': 0.5817632055125249, 'errorList': [], 'lossList': [0.0, -1.3840916049480438, 0.0, 69.39910617828369, 0.0, 0.0, 0.0], 'rewardMean': 0.63742808706927, 'totalEpisodes': 108, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.171370590759863
'totalSteps': 10240, 'rewardStep': 0.6250590629614831, 'errorList': [], 'lossList': [0.0, -1.378515409231186, 0.0, 39.040400309562685, 0.0, 0.0, 0.0], 'rewardMean': 0.6358819590557967, 'totalEpisodes': 116, 'stepsPerEpisode': 298, 'rewardPerEpisode': 212.58911123964845
'totalSteps': 11520, 'rewardStep': 0.9589623122576726, 'errorList': [76.3043522497816, 82.98147225446289, 52.371429613519986, 26.614238051106376, 80.71370574427995, 22.42246583004964, 68.1841850265372, 56.59816512654483, 82.21230828509333, 27.10493165132898, 62.01418048303842, 76.39821030584659, 49.137259258163134, 88.00407676557698, 32.27696376511835, 71.86192268486386, 64.8387828918156, 28.783641836722875, 12.346562392056793, 35.085370140417794, 33.89727820711617, 64.16191823200745, 2.83928898897454, 82.34826674020418, 43.98110603934507, 77.6372578899073, 77.05617158517755, 48.24022245138818, 76.07931814591153, 51.07754294410405, 28.361177762854894, 43.83373254745497, 40.298054947186856, 22.456242880486542, 75.51723662988074, 70.1761439801019, 23.289942934042777, 12.259238344807244, 61.38901404292226, 76.63869756297052, 35.44389092963766, 77.72864160176482, 82.75334933087477, 34.39022732195635, 22.2912702940975, 74.79876062768157, 72.42278303805688, 79.27289871844368, 73.59963999324535, 67.13001861658806], 'lossList': [0.0, -1.3717132478952407, 0.0, 20.54255535364151, 0.0, 0.0, 0.0], 'rewardMean': 0.6717797760782274, 'totalEpisodes': 123, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.814690734413444, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.7987763801143581, 'errorList': [], 'lossList': [0.0, -1.3763408559560775, 0.0, 29.17651198387146, 0.0, 0.0, 0.0], 'rewardMean': 0.6844794364818404, 'totalEpisodes': 130, 'stepsPerEpisode': 24, 'rewardPerEpisode': 17.001499815039125
'totalSteps': 14080, 'rewardStep': 0.7771724349864979, 'errorList': [], 'lossList': [0.0, -1.3658761471509933, 0.0, 10.051180016994476, 0.0, 0.0, 0.0], 'rewardMean': 0.7183936207599028, 'totalEpisodes': 135, 'stepsPerEpisode': 54, 'rewardPerEpisode': 45.19506660359377
'totalSteps': 15360, 'rewardStep': 0.7668808800555845, 'errorList': [], 'lossList': [0.0, -1.3603227096796036, 0.0, 8.635361858606338, 0.0, 0.0, 0.0], 'rewardMean': 0.7216468544484873, 'totalEpisodes': 141, 'stepsPerEpisode': 170, 'rewardPerEpisode': 125.8103873495911
'totalSteps': 16640, 'rewardStep': 0.7936930698074565, 'errorList': [], 'lossList': [0.0, -1.3565328979492188, 0.0, 9.375427660942078, 0.0, 0.0, 0.0], 'rewardMean': 0.7149997986255612, 'totalEpisodes': 145, 'stepsPerEpisode': 199, 'rewardPerEpisode': 170.46948274529998
'totalSteps': 17920, 'rewardStep': 0.6547153907957146, 'errorList': [], 'lossList': [0.0, -1.370894455909729, 0.0, 6.577028683423996, 0.0, 0.0, 0.0], 'rewardMean': 0.7308734719031604, 'totalEpisodes': 150, 'stepsPerEpisode': 102, 'rewardPerEpisode': 78.74204714395184
'totalSteps': 19200, 'rewardStep': 0.8346701086572419, 'errorList': [], 'lossList': [0.0, -1.3702078384160996, 0.0, 25.455124539136886, 0.0, 0.0, 0.0], 'rewardMean': 0.762704362563151, 'totalEpisodes': 152, 'stepsPerEpisode': 337, 'rewardPerEpisode': 240.16309175807473
'totalSteps': 20480, 'rewardStep': 0.8149979274136122, 'errorList': [], 'lossList': [0.0, -1.349789746403694, 0.0, 8.315626373291016, 0.0, 0.0, 0.0], 'rewardMean': 0.7606690772562146, 'totalEpisodes': 154, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.749498800591681
'totalSteps': 21760, 'rewardStep': 0.602752436568837, 'errorList': [], 'lossList': [0.0, -1.336664496064186, 0.0, 2.0994815218448637, 0.0, 0.0, 0.0], 'rewardMean': 0.7627680003618458, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.793699062978
'totalSteps': 23040, 'rewardStep': 0.8535127530203748, 'errorList': [], 'lossList': [0.0, -1.3187303757667541, 0.0, 1.0263010181486607, 0.0, 0.0, 0.0], 'rewardMean': 0.785613369367735, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.642316226742
'totalSteps': 24320, 'rewardStep': 0.9471767489673258, 'errorList': [0.10944281507227968, 0.14510071489188076, 0.12855952050575042, 0.07526041158171876, 0.09803281107815204, 0.10771579175925017, 0.09744280295638073, 0.0877420359019212, 0.2160862703135481, 0.09972307027611355, 0.1046768377754135, 0.22617428973642348, 0.1096918428754002, 0.11480840789131104, 0.10622885795802008, 0.10814809501108591, 0.1662106881352735, 0.1240508763065905, 0.13827380880087886, 0.08212335602796444, 0.1670497462604032, 0.08152024419362676, 0.16543166220022884, 0.11108280276070635, 0.08745926135033093, 0.07355398398587971, 0.12096006375209581, 0.08437034959387105, 0.17796076385626938, 0.12021480935266533, 0.19444283547544416, 0.26598603996012077, 0.32427476451022813, 0.08019570006796543, 0.07555612943111295, 0.1138263420398915, 0.1870959217886634, 0.12544317228869115, 0.1403651328208196, 0.15072650953800626, 0.21434254387610394, 0.15725950830689564, 0.15402898695337763, 0.12108713819428452, 0.10300579082411802, 0.14028312984186028, 0.09285506193863063, 0.11212428295435559, 0.07887195475400158, 0.2959228552743781], 'lossList': [0.0, -1.2960514056682586, 0.0, 1.5357280222326517, 0.0, 0.0, 0.0], 'rewardMean': 0.7844348130387004, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.3818878890627, 'successfulTests': 44
'totalSteps': 25600, 'rewardStep': 0.5976005964461343, 'errorList': [], 'lossList': [0.0, -1.2765616530179977, 0.0, 1.2226272993907332, 0.0, 0.0, 0.0], 'rewardMean': 0.764317234671878, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1119.7706258931664
#maxSuccessfulTests=44, maxSuccessfulTestsAtStep=24320, timeSpent=93.58
