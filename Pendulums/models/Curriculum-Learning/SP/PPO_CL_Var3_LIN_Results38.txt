#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 38
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.7702748197187346, 'errorList': [], 'lossList': [0.0, -1.4181769758462905, 0.0, 23.522050161361694, 0.0, 0.0, 0.0], 'rewardMean': 0.7678003263328953, 'totalEpisodes': 17, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.14424151042915
'totalSteps': 3840, 'rewardStep': 0.8906477213394205, 'errorList': [], 'lossList': [0.0, -1.3997847652435302, 0.0, 33.34167497396469, 0.0, 0.0, 0.0], 'rewardMean': 0.808749458001737, 'totalEpisodes': 22, 'stepsPerEpisode': 378, 'rewardPerEpisode': 253.01222207754284
'totalSteps': 5120, 'rewardStep': 0.8656543281545803, 'errorList': [], 'lossList': [0.0, -1.3917106688022614, 0.0, 49.71305774688721, 0.0, 0.0, 0.0], 'rewardMean': 0.8229756755399478, 'totalEpisodes': 30, 'stepsPerEpisode': 76, 'rewardPerEpisode': 70.7268509255456
'totalSteps': 6400, 'rewardStep': 0.9731964745174329, 'errorList': [226.01863841849226, 262.40849947196955, 268.60692760313856, 248.25936916631898, 177.04246868126333, 255.0825140906522, 262.1446621717555, 258.9299509389947, 272.4916730763807, 233.813882984385, 254.65883722804543, 197.7987341723992, 256.85948040715664, 257.52826659518644, 275.1212838560934, 222.47341487634483, 244.6294294504115, 210.7368427236911, 257.0966363841426, 268.2928649114572, 252.0189781072608, 282.2779929906484, 153.17534301093292, 253.56775558598204, 255.89523351943637, 257.9845560143502, 256.8019183997359, 235.98953471480263, 174.55736220876108, 253.62471457527397, 207.60070136928852, 209.85557476036698, 214.82826578981692, 267.0382122510641, 248.99807641870672, 248.2400616325926, 231.41901846309642, 240.9949994273508, 195.25741928423594, 283.07241811489064, 231.5699066711698, 257.60360993094156, 171.82201310698483, 198.8472335101736, 252.11318494913786, 262.4162662887283, 272.65409348940096, 254.78359412403822, 267.51841253094017, 192.1507193314247], 'lossList': [0.0, -1.387215303182602, 0.0, 94.58535402297974, 0.0, 0.0, 0.0], 'rewardMean': 0.8530198353354448, 'totalEpisodes': 47, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.019667046922894, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.7424780711540564, 'errorList': [], 'lossList': [0.0, -1.3852937752008438, 0.0, 157.5496520614624, 0.0, 0.0, 0.0], 'rewardMean': 0.83459620797188, 'totalEpisodes': 90, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.332984790732787
'totalSteps': 8960, 'rewardStep': 0.7254877377562965, 'errorList': [], 'lossList': [0.0, -1.382839651107788, 0.0, 60.836847076416014, 0.0, 0.0, 0.0], 'rewardMean': 0.8190092836553681, 'totalEpisodes': 112, 'stepsPerEpisode': 73, 'rewardPerEpisode': 54.44353306242301
'totalSteps': 10240, 'rewardStep': 0.9440842972943808, 'errorList': [0.40565800671970303, 6.045587319698303, 0.6425518002685215, 1.9320892599225379, 6.466309855955617, 0.17564878446829307, 3.7787267665251534, 3.67701796627277, 2.133712334064226, 4.092805505421627, 5.20327946147446, 3.709705025674734, 1.5417467549117039, 4.976775713070969, 0.16261654880916687, 1.0149047227045844, 6.1790218689161, 0.5029895808811337, 2.962418758374682, 7.058977641510652, 3.6248593391552086, 3.8874637974535045, 0.49487675239688833, 4.45600671518301, 3.1947606459229183, 0.20344630103857606, 0.9146514977240958, 2.621733503183934, 5.554645550511954, 1.5433999553373454, 3.4524261542918, 3.635739357277981, 7.29107137425193, 3.719736605035892, 4.552831269478367, 4.5253322641680915, 0.1755622667992988, 1.628763310552762, 6.566832203808384, 0.17047611527661444, 2.7861587794950524, 7.145170237684331, 1.0821800644699138, 1.2679251730417422, 2.7563125009836775, 1.319644076365715, 1.2634876213685695, 2.6845214669367676, 1.2634068128038145, 1.4180504363233992], 'lossList': [0.0, -1.379755454659462, 0.0, 68.34825956344605, 0.0, 0.0, 0.0], 'rewardMean': 0.8346436603602447, 'totalEpisodes': 132, 'stepsPerEpisode': 38, 'rewardPerEpisode': 33.95727102384704, 'successfulTests': 4
'totalSteps': 11520, 'rewardStep': 0.5842012949832076, 'errorList': [], 'lossList': [0.0, -1.3675217127799988, 0.0, 28.141130294799805, 0.0, 0.0, 0.0], 'rewardMean': 0.8068167308739073, 'totalEpisodes': 141, 'stepsPerEpisode': 169, 'rewardPerEpisode': 120.48621146385338
'totalSteps': 12800, 'rewardStep': 0.6108270670432153, 'errorList': [], 'lossList': [0.0, -1.348999048471451, 0.0, 19.4152991938591, 0.0, 0.0, 0.0], 'rewardMean': 0.787217764490838, 'totalEpisodes': 144, 'stepsPerEpisode': 463, 'rewardPerEpisode': 352.90790385149836
'totalSteps': 14080, 'rewardStep': 0.558895985126759, 'errorList': [], 'lossList': [0.0, -1.3241831374168396, 0.0, 9.040886501073837, 0.0, 0.0, 0.0], 'rewardMean': 0.7665747797088084, 'totalEpisodes': 147, 'stepsPerEpisode': 264, 'rewardPerEpisode': 172.92415785946383
'totalSteps': 15360, 'rewardStep': 0.7707530928839829, 'errorList': [], 'lossList': [0.0, -1.3080775457620621, 0.0, 8.156353148818017, 0.0, 0.0, 0.0], 'rewardMean': 0.7666226070253332, 'totalEpisodes': 150, 'stepsPerEpisode': 184, 'rewardPerEpisode': 161.5601742309471
'totalSteps': 16640, 'rewardStep': 0.7306893165027144, 'errorList': [], 'lossList': [0.0, -1.3195131891965866, 0.0, 5.722851278781891, 0.0, 0.0, 0.0], 'rewardMean': 0.7506267665416626, 'totalEpisodes': 152, 'stepsPerEpisode': 274, 'rewardPerEpisode': 224.47497533581992
'totalSteps': 17920, 'rewardStep': 0.4983627664646422, 'errorList': [], 'lossList': [0.0, -1.3277656900882722, 0.0, 3.0602577233314516, 0.0, 0.0, 0.0], 'rewardMean': 0.7138976103726689, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 819.8725377504641
'totalSteps': 19200, 'rewardStep': 0.8716055358992142, 'errorList': [], 'lossList': [0.0, -1.3153448760509492, 0.0, 5.574535146951676, 0.0, 0.0, 0.0], 'rewardMean': 0.703738516510847, 'totalEpisodes': 154, 'stepsPerEpisode': 214, 'rewardPerEpisode': 177.21619117647703
'totalSteps': 20480, 'rewardStep': 0.8766222268355882, 'errorList': [], 'lossList': [0.0, -1.3189018708467484, 0.0, 6.20774896144867, 0.0, 0.0, 0.0], 'rewardMean': 0.7171529320790001, 'totalEpisodes': 156, 'stepsPerEpisode': 62, 'rewardPerEpisode': 57.60683323956054
'totalSteps': 21760, 'rewardStep': 0.7297229884152779, 'errorList': [], 'lossList': [0.0, -1.3095725804567337, 0.0, 3.951341748833656, 0.0, 0.0, 0.0], 'rewardMean': 0.7175764571448983, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 859.6340897489823
'totalSteps': 23040, 'rewardStep': 0.8594183750925255, 'errorList': [], 'lossList': [0.0, -1.264591824412346, 0.0, 1.664218824505806, 0.0, 0.0, 0.0], 'rewardMean': 0.7091098649247127, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.5903507615249
'totalSteps': 24320, 'rewardStep': 0.6960309852820274, 'errorList': [], 'lossList': [0.0, -1.2162421667575836, 0.0, 1.1379863038659095, 0.0, 0.0, 0.0], 'rewardMean': 0.7202928339545946, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.1492828689743
'totalSteps': 25600, 'rewardStep': 0.9944100816759848, 'errorList': [0.07560148892300941, 0.05260955489468405, 0.04987574131159918, 0.060851484427978594, 0.07011842510890143, 0.050649518277584495, 0.07058373495082522, 0.050573630024478525, 0.06683060809062416, 0.059981715652714894, 0.07280380272701145, 0.05005983319934798, 0.0545045525379096, 0.07907787525209939, 0.054461593194985974, 0.05897596926149211, 0.07212505555725365, 0.0674489305919584, 0.057481768903468636, 0.07342280781402571, 0.0593265089148479, 0.07427870125579135, 0.05118114850289326, 0.05581403294315499, 0.056990189123966244, 0.050071367105682406, 0.0500497158113328, 0.05748625237167772, 0.05328324159104133, 0.06133505368590154, 0.0580053369765057, 0.0495371521054753, 0.07039868776275426, 0.06709063464722426, 0.07079519638931304, 0.05441218934513187, 0.06948544960905761, 0.06614797522398766, 0.0556230721770608, 0.05232859776314347, 0.061132202484982044, 0.06088629880155802, 0.06446729524674993, 0.07652883766414244, 0.06043399041183082, 0.06228964027961071, 0.06046623636585554, 0.07947116096283162, 0.08086516330112645, 0.0750312645009491], 'lossList': [0.0, -1.1879234379529953, 0.0, 1.0316667561978101, 0.0, 0.0, 0.0], 'rewardMean': 0.7586511354178715, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.4593420098513, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=114.29
