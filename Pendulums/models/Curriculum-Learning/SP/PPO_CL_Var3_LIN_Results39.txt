#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 39
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.955012079019379, 'errorList': [], 'lossList': [0.0, -1.4159818840026857, 0.0, 29.49910296678543, 0.0, 0.0, 0.0], 'rewardMean': 0.8134637425813611, 'totalEpisodes': 16, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.32873154621657
'totalSteps': 3840, 'rewardStep': 0.7076542252333056, 'errorList': [], 'lossList': [0.0, -1.4197494381666182, 0.0, 21.63988068342209, 0.0, 0.0, 0.0], 'rewardMean': 0.7781939034653426, 'totalEpisodes': 20, 'stepsPerEpisode': 198, 'rewardPerEpisode': 124.82076906674075
'totalSteps': 5120, 'rewardStep': 0.6795167868991601, 'errorList': [], 'lossList': [0.0, -1.4116535657644271, 0.0, 51.655330991745, 0.0, 0.0, 0.0], 'rewardMean': 0.753524624323797, 'totalEpisodes': 29, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.648879240776616
'totalSteps': 6400, 'rewardStep': 0.7506623711195439, 'errorList': [], 'lossList': [0.0, -1.3998311460018158, 0.0, 79.76827045440673, 0.0, 0.0, 0.0], 'rewardMean': 0.7529521736829464, 'totalEpisodes': 42, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.257274326807508
'totalSteps': 7680, 'rewardStep': 0.8024985964510793, 'errorList': [], 'lossList': [0.0, -1.3956838166713714, 0.0, 136.22238441467286, 0.0, 0.0, 0.0], 'rewardMean': 0.7612099108109686, 'totalEpisodes': 76, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.010419624981878
'totalSteps': 8960, 'rewardStep': 0.682135653930979, 'errorList': [], 'lossList': [0.0, -1.3938333415985107, 0.0, 80.03346267700195, 0.0, 0.0, 0.0], 'rewardMean': 0.7499135883995416, 'totalEpisodes': 107, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.6445991784395266
'totalSteps': 10240, 'rewardStep': 0.8212821797921814, 'errorList': [], 'lossList': [0.0, -1.371099072098732, 0.0, 52.31946849822998, 0.0, 0.0, 0.0], 'rewardMean': 0.7588346623236215, 'totalEpisodes': 127, 'stepsPerEpisode': 40, 'rewardPerEpisode': 32.98663943050173
'totalSteps': 11520, 'rewardStep': 0.82004816691349, 'errorList': [], 'lossList': [0.0, -1.3411105042695999, 0.0, 22.557483735084535, 0.0, 0.0, 0.0], 'rewardMean': 0.7656361628336069, 'totalEpisodes': 134, 'stepsPerEpisode': 78, 'rewardPerEpisode': 65.43503416976895
'totalSteps': 12800, 'rewardStep': 0.774866602089542, 'errorList': [], 'lossList': [0.0, -1.3223176389932632, 0.0, 27.237772512435914, 0.0, 0.0, 0.0], 'rewardMean': 0.7665592067592004, 'totalEpisodes': 146, 'stepsPerEpisode': 63, 'rewardPerEpisode': 49.69436316024965
'totalSteps': 14080, 'rewardStep': 0.9153013042289004, 'errorList': [], 'lossList': [0.0, -1.3097924208641052, 0.0, 20.582406828403474, 0.0, 0.0, 0.0], 'rewardMean': 0.7908977965677562, 'totalEpisodes': 153, 'stepsPerEpisode': 64, 'rewardPerEpisode': 54.466283648699246
'totalSteps': 15360, 'rewardStep': 0.8232527233525284, 'errorList': [], 'lossList': [0.0, -1.3103080874681472, 0.0, 9.137164452075957, 0.0, 0.0, 0.0], 'rewardMean': 0.777721861001071, 'totalEpisodes': 161, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.586854225447237
'totalSteps': 16640, 'rewardStep': 0.6644806817343895, 'errorList': [], 'lossList': [0.0, -1.311447287797928, 0.0, 8.733658796548843, 0.0, 0.0, 0.0], 'rewardMean': 0.7734045066511793, 'totalEpisodes': 164, 'stepsPerEpisode': 142, 'rewardPerEpisode': 125.67435594695529
'totalSteps': 17920, 'rewardStep': 0.7953235490773553, 'errorList': [], 'lossList': [0.0, -1.2938295167684555, 0.0, 5.97813090801239, 0.0, 0.0, 0.0], 'rewardMean': 0.7849851828689989, 'totalEpisodes': 168, 'stepsPerEpisode': 185, 'rewardPerEpisode': 159.30980640213977
'totalSteps': 19200, 'rewardStep': 0.8374920198154436, 'errorList': [], 'lossList': [0.0, -1.295257225036621, 0.0, 5.633523016571998, 0.0, 0.0, 0.0], 'rewardMean': 0.793668147738589, 'totalEpisodes': 171, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.62550994738898
'totalSteps': 20480, 'rewardStep': 0.7190085766873787, 'errorList': [], 'lossList': [0.0, -1.2988359028100966, 0.0, 4.9158233284950255, 0.0, 0.0, 0.0], 'rewardMean': 0.7853191457622188, 'totalEpisodes': 172, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.418598326519466
'totalSteps': 21760, 'rewardStep': 0.7255716951969001, 'errorList': [], 'lossList': [0.0, -1.2769856554269792, 0.0, 3.0220716220140456, 0.0, 0.0, 0.0], 'rewardMean': 0.7896627498888109, 'totalEpisodes': 174, 'stepsPerEpisode': 237, 'rewardPerEpisode': 205.44609169505605
'totalSteps': 23040, 'rewardStep': 0.8739985312843225, 'errorList': [], 'lossList': [0.0, -1.2622735911607743, 0.0, 2.2459822496771813, 0.0, 0.0, 0.0], 'rewardMean': 0.794934385038025, 'totalEpisodes': 175, 'stepsPerEpisode': 929, 'rewardPerEpisode': 796.4197404038634
'totalSteps': 24320, 'rewardStep': 0.8406776285092492, 'errorList': [], 'lossList': [0.0, -1.2528918778896332, 0.0, 1.4467852076888084, 0.0, 0.0, 0.0], 'rewardMean': 0.796997331197601, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1140.6103937180458
'totalSteps': 25600, 'rewardStep': 0.9919232018957596, 'errorList': [0.04515261808533301, 0.03483520709077963, 0.025086164516971744, 0.06743029876534473, 0.04441832569812958, 0.026298691537537496, 0.026380921318394996, 0.025663358683954224, 0.03577849957400915, 0.05422003295756664, 0.03563755241784056, 0.027685770202683806, 0.057513760038072466, 0.024512750649129618, 0.026077602831390375, 0.08110778350557848, 0.026264073760798045, 0.0840256397263432, 0.0313145004294676, 0.09476633788641803, 0.04637213020384179, 0.023270106168968147, 0.0886623863274785, 0.026208394904927646, 0.02315027521252472, 0.03975563598225072, 0.10965741107102485, 0.08272650439393694, 0.06759053131542432, 0.0731505216693825, 0.052970363819872024, 0.09180847501656375, 0.11193818868866895, 0.13222745012307274, 0.14217122161543053, 0.04456204754543413, 0.023856782121088098, 0.08499744498314053, 0.027729696061774174, 0.023566992390256905, 0.08695913978910495, 0.09646431090559067, 0.043079292822342206, 0.06983249194096239, 0.034623202426102236, 0.13645994452270174, 0.0238118486613608, 0.042332358332754796, 0.1307395411664626, 0.1347218687411432], 'lossList': [0.0, -1.227095876932144, 0.0, 1.3532633221521975, 0.0, 0.0, 0.0], 'rewardMean': 0.8187029911782228, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.6555228016136, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=79.57
