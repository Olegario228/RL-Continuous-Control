#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.6574967750330113, 'errorList': [], 'lossList': [0.0, -1.4142423635721206, 0.0, 30.890269145965576, 0.0, 0.0, 0.0], 'rewardMean': 0.7892934545922092, 'totalEpisodes': 71, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.799232177315293
'totalSteps': 3840, 'rewardStep': 0.6868434714703704, 'errorList': [], 'lossList': [0.0, -1.4172551381587981, 0.0, 45.92749038696289, 0.0, 0.0, 0.0], 'rewardMean': 0.755143460218263, 'totalEpisodes': 104, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.515799080615796
'totalSteps': 5120, 'rewardStep': 0.941361429406707, 'errorList': [87.62743496012455, 205.27004650388983, 138.0045048178735, 99.58910355404711, 89.21286034351225, 141.38816540048222, 117.01782031672474, 67.51980909174826, 184.37670072547027, 104.68912432905155, 131.88339469466246, 111.22948695775479, 128.81038428743057, 184.8229001564812, 72.7217767767706, 159.1948475811673, 92.31613384446509, 134.0481376796582, 122.0366578198827, 141.3031606912394, 160.45311455940848, 156.63613336452866, 111.25258345123586, 132.5344229682367, 109.22952172367552, 125.67511175585305, 92.07108887186119, 238.0884642034546, 192.57970490074214, 110.3901662179398, 122.39766579313414, 137.87775500265798, 137.27565424998275, 86.265516165484, 126.92931168887628, 125.5404561944814, 64.56010063407807, 125.58539625975428, 185.56651691856368, 122.49332916461704, 100.85954934190353, 159.34542465027724, 70.71752041646685, 146.90742566768122, 31.257518945790217, 140.5469795988803, 96.21875982814036, 114.54086298951982, 118.90266472987138, 108.18289410716694], 'lossList': [0.0, -1.3975168144702912, 0.0, 66.59655975341796, 0.0, 0.0, 0.0], 'rewardMean': 0.801697952515374, 'totalEpisodes': 139, 'stepsPerEpisode': 41, 'rewardPerEpisode': 35.72983131964804, 'successfulTests': 0
'totalSteps': 6400, 'rewardStep': 0.746332429334849, 'errorList': [], 'lossList': [0.0, -1.3854846149682998, 0.0, 59.58252391815186, 0.0, 0.0, 0.0], 'rewardMean': 0.790624847879269, 'totalEpisodes': 159, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.666824110474876
'totalSteps': 7680, 'rewardStep': 0.7423937957583373, 'errorList': [], 'lossList': [0.0, -1.386194674372673, 0.0, 51.328187084198, 0.0, 0.0, 0.0], 'rewardMean': 0.7825863391924471, 'totalEpisodes': 170, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.481487217814745
'totalSteps': 8960, 'rewardStep': 0.7719380244754938, 'errorList': [], 'lossList': [0.0, -1.3765033346414566, 0.0, 37.29718557357788, 0.0, 0.0, 0.0], 'rewardMean': 0.7810651513757395, 'totalEpisodes': 178, 'stepsPerEpisode': 213, 'rewardPerEpisode': 163.4293477173775
'totalSteps': 10240, 'rewardStep': 0.8848115968271465, 'errorList': [], 'lossList': [0.0, -1.3630087691545487, 0.0, 13.478561209440231, 0.0, 0.0, 0.0], 'rewardMean': 0.7940334570571653, 'totalEpisodes': 183, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.29913389958763
'totalSteps': 11520, 'rewardStep': 0.5760542428118294, 'errorList': [], 'lossList': [0.0, -1.3645876914262771, 0.0, 5.49061870276928, 0.0, 0.0, 0.0], 'rewardMean': 0.7698135443632391, 'totalEpisodes': 187, 'stepsPerEpisode': 135, 'rewardPerEpisode': 101.16410528428652
'totalSteps': 12800, 'rewardStep': 0.8416654449253307, 'errorList': [], 'lossList': [0.0, -1.3722731429338455, 0.0, 26.212859292030334, 0.0, 0.0, 0.0], 'rewardMean': 0.7769987344194483, 'totalEpisodes': 191, 'stepsPerEpisode': 64, 'rewardPerEpisode': 55.97370356787715
'totalSteps': 14080, 'rewardStep': 0.8518126795748903, 'errorList': [], 'lossList': [0.0, -1.3573418533802033, 0.0, 7.954363622963428, 0.0, 0.0, 0.0], 'rewardMean': 0.7700709889617966, 'totalEpisodes': 192, 'stepsPerEpisode': 1126, 'rewardPerEpisode': 925.9196938023949
'totalSteps': 15360, 'rewardStep': 0.7358975224132362, 'errorList': [], 'lossList': [0.0, -1.348857234120369, 0.0, 36.1975422668457, 0.0, 0.0, 0.0], 'rewardMean': 0.777911063699819, 'totalEpisodes': 194, 'stepsPerEpisode': 899, 'rewardPerEpisode': 646.1247413895701
'totalSteps': 16640, 'rewardStep': 0.6091572760730279, 'errorList': [], 'lossList': [0.0, -1.3319289898872375, 0.0, 10.999162276387215, 0.0, 0.0, 0.0], 'rewardMean': 0.7701424441600848, 'totalEpisodes': 195, 'stepsPerEpisode': 447, 'rewardPerEpisode': 341.7705258843072
'totalSteps': 17920, 'rewardStep': 0.8381376180231243, 'errorList': [], 'lossList': [0.0, -1.3054288619756698, 0.0, 3.807500773370266, 0.0, 0.0, 0.0], 'rewardMean': 0.7598200630217266, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.3740532848474
'totalSteps': 19200, 'rewardStep': 0.868084625662159, 'errorList': [], 'lossList': [0.0, -1.2581983679533004, 0.0, 1.747547570168972, 0.0, 0.0, 0.0], 'rewardMean': 0.7719952826544575, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.93974836577
'totalSteps': 20480, 'rewardStep': 0.9677325216007615, 'errorList': [0.13493955413131092, 0.10636054646869941, 0.14280708494482913, 0.16258656948900288, 0.2231475628100176, 0.12234294692092258, 0.12663095851998735, 0.1177113972073259, 0.12040678387442667, 0.11940365545286484, 0.13085170484485667, 0.21473859060775174, 0.1245200777049248, 0.11383673646879583, 0.12344703373888576, 0.1026013554191715, 0.12161200413743993, 0.1384191226936823, 0.13266436807595886, 0.12215896205035282, 0.10676567081566951, 0.12180200961985854, 0.10666451437420199, 0.14733748744469666, 0.12914302315648446, 0.10277805209355052, 0.12476338475228786, 0.10370912464374245, 0.1282870951300111, 0.12193238879310492, 0.10215798656186406, 0.1320027523639511, 0.1942257999289429, 0.1661186375637695, 0.16963036953536187, 0.11157624467226046, 0.14264759191257342, 0.2052355945242745, 0.1238788288369305, 0.10697957885249623, 0.13897478066863037, 0.13004912767886925, 0.13418903485739783, 0.10178297966339679, 0.12580144408032287, 0.12681317759197747, 0.12695983577340614, 0.11608080916536656, 0.1200836022889531, 0.11545297620935781], 'lossList': [0.0, -1.1927588081359863, 0.0, 1.6477953666076064, 0.0, 0.0, 0.0], 'rewardMean': 0.7945291552386999, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.811774786174, 'successfulTests': 47
'totalSteps': 21760, 'rewardStep': 0.9182642496345362, 'errorList': [], 'lossList': [0.0, -1.1776918518543242, 0.0, 0.9053897260129452, 0.0, 0.0, 0.0], 'rewardMean': 0.8091617777546043, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.0605836884686
'totalSteps': 23040, 'rewardStep': 0.8974781040234421, 'errorList': [], 'lossList': [0.0, -1.1603460466861726, 0.0, 0.5255360526032746, 0.0, 0.0, 0.0], 'rewardMean': 0.8104284284742338, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.8570867594933
'totalSteps': 24320, 'rewardStep': 0.9373416647104511, 'errorList': [0.02169765587699668, 0.026912550656749017, 0.033944056402504925, 0.026653571477265162, 0.04342604978531606, 0.038705582937412525, 0.044867883490998205, 0.06356361023550179, 0.026316372633391767, 0.05186875709060478, 0.05528802417114661, 0.042694977898923826, 0.060135132093491, 0.034245005252499953, 0.02138285878291189, 0.02590007951848381, 0.07851288589236341, 0.11170746414512998, 0.039732039211315066, 0.02129750172780336, 0.047898894883561714, 0.03232273478478687, 0.03451831309679884, 0.03103379539900397, 0.08171165510453836, 0.033684027997418355, 0.039497038723780735, 0.05363558669994537, 0.11642041919173647, 0.021562671377248155, 0.029907986322047747, 0.023490241470640667, 0.1295617254247698, 0.09203909056693155, 0.043098620831517304, 0.029183922299451204, 0.058388370533887436, 0.0797132874167318, 0.030843276255225625, 0.027374495034604342, 0.039115361196515874, 0.05140672382290637, 0.05565731365160861, 0.02254620846751314, 0.04581683141965779, 0.04404021185435901, 0.040718449015980034, 0.04091653814454041, 0.06537858352347266, 0.07209801652260557], 'lossList': [0.0, -1.1245537388324738, 0.0, 0.4958053315244615, 0.0, 0.0, 0.0], 'rewardMean': 0.846557170664096, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.1379061923694, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=101.44
