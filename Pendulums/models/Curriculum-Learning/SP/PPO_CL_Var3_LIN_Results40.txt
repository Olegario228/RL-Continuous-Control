#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 40
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.945611612079625, 'errorList': [], 'lossList': [0.0, -1.4367957425117492, 0.0, 27.47330219268799, 0.0, 0.0, 0.0], 'rewardMean': 0.920596601643896, 'totalEpisodes': 8, 'stepsPerEpisode': 533, 'rewardPerEpisode': 366.79070831287703
'totalSteps': 3840, 'rewardStep': 0.6404778081765409, 'errorList': [], 'lossList': [0.0, -1.4275330966711044, 0.0, 33.311814951896665, 0.0, 0.0, 0.0], 'rewardMean': 0.827223670488111, 'totalEpisodes': 15, 'stepsPerEpisode': 154, 'rewardPerEpisode': 128.03601234169935
'totalSteps': 5120, 'rewardStep': 0.7480655066232544, 'errorList': [], 'lossList': [0.0, -1.428822500705719, 0.0, 54.04591473579407, 0.0, 0.0, 0.0], 'rewardMean': 0.8074341295218969, 'totalEpisodes': 23, 'stepsPerEpisode': 194, 'rewardPerEpisode': 148.34136309874535
'totalSteps': 6400, 'rewardStep': 0.6585881304212111, 'errorList': [], 'lossList': [0.0, -1.4303725242614747, 0.0, 95.62075550079345, 0.0, 0.0, 0.0], 'rewardMean': 0.7776649297017597, 'totalEpisodes': 40, 'stepsPerEpisode': 104, 'rewardPerEpisode': 78.2900571050969
'totalSteps': 7680, 'rewardStep': 0.7676591842972122, 'errorList': [], 'lossList': [0.0, -1.4194338947534562, 0.0, 129.41993011474608, 0.0, 0.0, 0.0], 'rewardMean': 0.7759973054676684, 'totalEpisodes': 74, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.13534255840573
'totalSteps': 8960, 'rewardStep': 0.7246896462518844, 'errorList': [], 'lossList': [0.0, -1.4041460168361664, 0.0, 84.89431255340577, 0.0, 0.0, 0.0], 'rewardMean': 0.7686676398654136, 'totalEpisodes': 110, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.385290642410523
'totalSteps': 10240, 'rewardStep': 0.7725535947656985, 'errorList': [], 'lossList': [0.0, -1.3882506453990937, 0.0, 51.79268947601318, 0.0, 0.0, 0.0], 'rewardMean': 0.7691533842279492, 'totalEpisodes': 125, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.449945897953765
'totalSteps': 11520, 'rewardStep': 0.8028740483837407, 'errorList': [], 'lossList': [0.0, -1.3618469113111495, 0.0, 34.67969099521637, 0.0, 0.0, 0.0], 'rewardMean': 0.7729001246897038, 'totalEpisodes': 133, 'stepsPerEpisode': 76, 'rewardPerEpisode': 56.195357476505315
'totalSteps': 12800, 'rewardStep': 0.9546420659286754, 'errorList': [0.12512003878636624, 0.15563517451960077, 0.21418113122492619, 0.07169467466474387, 0.0950432423938386, 0.07228164678617024, 0.06939752889108755, 0.2735040216287817, 0.07985802880860225, 0.1105646796209771, 0.12150530257443873, 0.13159984096594524, 0.07918807407925561, 0.12229983918506006, 0.28505747037870205, 0.22610943856685264, 0.27908130463680575, 0.09603249991284105, 0.2284242272719281, 0.20549165593907254, 0.09435516132921581, 0.15213483763296604, 0.06317805155877478, 0.22615602549461047, 0.11060983522561645, 0.058242355223132064, 0.09569087551960051, 0.2523139738015101, 0.09794054391989004, 0.07006546380510954, 0.0652704986127433, 0.1437737780953002, 0.25546703240627483, 0.08332365102686048, 0.0675132510365316, 0.1949902396344741, 0.06702455498609099, 0.14707690548095903, 0.08115196670646284, 0.059565509970087166, 0.08677647318899004, 0.07166265942420265, 0.1647803551430284, 0.14887287002705396, 0.17552435497139926, 0.06141095655982521, 0.0626758001540026, 0.16009777482084417, 0.05806337699090644, 0.222103886957691], 'lossList': [0.0, -1.3387196213006973, 0.0, 19.975122539997102, 0.0, 0.0, 0.0], 'rewardMean': 0.791074318813601, 'totalEpisodes': 135, 'stepsPerEpisode': 121, 'rewardPerEpisode': 105.41110597144679, 'successfulTests': 39
'totalSteps': 14080, 'rewardStep': 0.7514120682492466, 'errorList': [], 'lossList': [0.0, -1.3229135745763778, 0.0, 11.07049064129591, 0.0, 0.0, 0.0], 'rewardMean': 0.776657366517709, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.9342278499571
'totalSteps': 15360, 'rewardStep': 0.5097950795621085, 'errorList': [], 'lossList': [0.0, -1.2953232306241989, 0.0, 6.186659083366394, 0.0, 0.0, 0.0], 'rewardMean': 0.7330757132659572, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 886.5361874290063
'totalSteps': 16640, 'rewardStep': 0.8856511318336875, 'errorList': [], 'lossList': [0.0, -1.2899476325511932, 0.0, 8.491359873116016, 0.0, 0.0, 0.0], 'rewardMean': 0.757593045631672, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.8996410687723
'totalSteps': 17920, 'rewardStep': 0.8279172411721811, 'errorList': [], 'lossList': [0.0, -1.2688965249061583, 0.0, 4.9825921996682885, 0.0, 0.0, 0.0], 'rewardMean': 0.7655782190865645, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.0783466396767
'totalSteps': 19200, 'rewardStep': 0.9165268705946066, 'errorList': [], 'lossList': [0.0, -1.2317070704698563, 0.0, 4.2410012395679955, 0.0, 0.0, 0.0], 'rewardMean': 0.7913720931039041, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.3659733994996
'totalSteps': 20480, 'rewardStep': 0.8924310898115313, 'errorList': [], 'lossList': [0.0, -1.209477854371071, 0.0, 2.8882207952812315, 0.0, 0.0, 0.0], 'rewardMean': 0.8038492836553359, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.2148941621067
'totalSteps': 21760, 'rewardStep': 0.9530439320295485, 'errorList': [0.1775751357610967, 0.17200737778358552, 0.17585475130439165, 0.1542680818690375, 0.16691690591170122, 0.17342038612129704, 0.1631811859439808, 0.17892825827515402, 0.18163871826337472, 0.16855387199191554, 0.16786849773206583, 0.18243524159299437, 0.21993872642658413, 0.17502952404678787, 0.18035640964152705, 0.16429801421935472, 0.18142038710073966, 0.30372336995299176, 0.17256620794947952, 0.16939888484119311, 0.2734917397041755, 0.17654572791024337, 0.16312991287407874, 0.17976817445777268, 0.16551713127243092, 0.17568187105295135, 0.17341958124626028, 0.17396690219502628, 0.15718463676366215, 0.17669294269860192, 0.17325243627933137, 0.1819417304753705, 0.16738595300468617, 0.21922219269229348, 0.17137410121731927, 0.16747371718526505, 0.17103137117596112, 0.17183441268197747, 0.1727310251732428, 0.16356276310056556, 0.1994129680174902, 0.17809822054818866, 0.17246986963537436, 0.15449776283454553, 0.16879386767515572, 0.18720395894472924, 0.16731701022138915, 0.20903237704793254, 0.15988397135767046, 0.16719440156841764], 'lossList': [0.0, -1.1892064267396927, 0.0, 1.6766717945411802, 0.0, 0.0, 0.0], 'rewardMean': 0.8266847122331024, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.9952209039157, 'successfulTests': 45
'totalSteps': 23040, 'rewardStep': 0.8440941958658339, 'errorList': [], 'lossList': [0.0, -1.1706406950950623, 0.0, 0.7286407354101538, 0.0, 0.0, 0.0], 'rewardMean': 0.8338387723431161, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1136.5012713689011
'totalSteps': 24320, 'rewardStep': 0.7081969797055232, 'errorList': [], 'lossList': [0.0, -1.1377283692359925, 0.0, 0.2918755265232176, 0.0, 0.0, 0.0], 'rewardMean': 0.8243710654752942, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.2015068977341
'totalSteps': 25600, 'rewardStep': 0.9681098961840664, 'errorList': [0.11725836600108115, 0.07945264891831601, 0.18786325109897586, 0.172178147425099, 0.07829869012654535, 0.08608726408901142, 0.15447036778398823, 0.08711699919863632, 0.08337054913252785, 0.08737191078307503, 0.0743344624287819, 0.07964077207169397, 0.08578936109747039, 0.0770355172288001, 0.08045384136642349, 0.08130712790333758, 0.17811311209762232, 0.08262644453954104, 0.12140799965345746, 0.13696784535756162, 0.08548928727171297, 0.08599576890870621, 0.09200990701992807, 0.10945729094278754, 0.09032120454809739, 0.31415035930602847, 0.14844287554852229, 0.11568827336120802, 0.2421791257563348, 0.07790404025631746, 0.07897899196721209, 0.12773246611784006, 0.08116071952300272, 0.08104103111281227, 0.16361822434781761, 0.09158926500706185, 0.07963227546788547, 0.2746724206812587, 0.0748823124930611, 0.08214091994189202, 0.07738855738550973, 0.08597831016060972, 0.08126789444754706, 0.07877942750629997, 0.11669957487155104, 0.0762165096496664, 0.08187304322890185, 0.08162589382913929, 0.2405566626977452, 0.07323285989029808], 'lossList': [0.0, -1.1046759498119354, 0.0, 0.9150620198622346, 0.0, 0.0, 0.0], 'rewardMean': 0.8257178485008332, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1178.8585893392076, 'successfulTests': 46
#maxSuccessfulTests=46, maxSuccessfulTestsAtStep=25600, timeSpent=117.98
