#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 41
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8266675784039538, 'errorList': [], 'lossList': [0.0, -1.4498333436250688, 0.0, 33.646943078041076, 0.0, 0.0, 0.0], 'rewardMean': 0.6926576484401544, 'totalEpisodes': 12, 'stepsPerEpisode': 80, 'rewardPerEpisode': 70.73685203524754
'totalSteps': 3840, 'rewardStep': 0.8730151281365663, 'errorList': [], 'lossList': [0.0, -1.4724855029582977, 0.0, 31.682995685338973, 0.0, 0.0, 0.0], 'rewardMean': 0.7527768083389583, 'totalEpisodes': 15, 'stepsPerEpisode': 167, 'rewardPerEpisode': 121.86731458876703
'totalSteps': 5120, 'rewardStep': 0.8702119791060828, 'errorList': [], 'lossList': [0.0, -1.4657773387432098, 0.0, 47.03616209030152, 0.0, 0.0, 0.0], 'rewardMean': 0.7821356010307394, 'totalEpisodes': 23, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.787713309084993
'totalSteps': 6400, 'rewardStep': 0.496099875615256, 'errorList': [], 'lossList': [0.0, -1.4587411147356033, 0.0, 91.44797542572022, 0.0, 0.0, 0.0], 'rewardMean': 0.7249284559476428, 'totalEpisodes': 39, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.901006488031749
'totalSteps': 7680, 'rewardStep': 0.279687071132324, 'errorList': [], 'lossList': [0.0, -1.4447751384973526, 0.0, 113.58957160949707, 0.0, 0.0, 0.0], 'rewardMean': 0.650721558478423, 'totalEpisodes': 75, 'stepsPerEpisode': 82, 'rewardPerEpisode': 52.33459591492239
'totalSteps': 8960, 'rewardStep': 0.6635538106330613, 'errorList': [], 'lossList': [0.0, -1.4351746678352355, 0.0, 67.61321298599243, 0.0, 0.0, 0.0], 'rewardMean': 0.652554737357657, 'totalEpisodes': 108, 'stepsPerEpisode': 31, 'rewardPerEpisode': 27.1689881197056
'totalSteps': 10240, 'rewardStep': 0.7289971764081409, 'errorList': [], 'lossList': [0.0, -1.417824668288231, 0.0, 45.551011648178104, 0.0, 0.0, 0.0], 'rewardMean': 0.6621100422389674, 'totalEpisodes': 126, 'stepsPerEpisode': 100, 'rewardPerEpisode': 76.35478660086609
'totalSteps': 11520, 'rewardStep': 0.7485209364346848, 'errorList': [], 'lossList': [0.0, -1.393181226849556, 0.0, 37.566882166862484, 0.0, 0.0, 0.0], 'rewardMean': 0.6717112527051582, 'totalEpisodes': 137, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.122036516626164
'totalSteps': 12800, 'rewardStep': 0.5648628681343812, 'errorList': [], 'lossList': [0.0, -1.3532703137397766, 0.0, 37.01645833492279, 0.0, 0.0, 0.0], 'rewardMean': 0.6610264142480805, 'totalEpisodes': 144, 'stepsPerEpisode': 127, 'rewardPerEpisode': 82.99946451304979
'totalSteps': 14080, 'rewardStep': 0.770622853265485, 'errorList': [], 'lossList': [0.0, -1.3089017534255982, 0.0, 22.37466523528099, 0.0, 0.0, 0.0], 'rewardMean': 0.6822239277269937, 'totalEpisodes': 146, 'stepsPerEpisode': 1075, 'rewardPerEpisode': 848.5686034244453
'totalSteps': 15360, 'rewardStep': 0.7769619512317592, 'errorList': [], 'lossList': [0.0, -1.2785288411378861, 0.0, 9.009717848300934, 0.0, 0.0, 0.0], 'rewardMean': 0.6772533650097742, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 992.5115843073163
'totalSteps': 16640, 'rewardStep': 0.821517996538628, 'errorList': [], 'lossList': [0.0, -1.2513041633367539, 0.0, 15.743389890193939, 0.0, 0.0, 0.0], 'rewardMean': 0.6721036518499803, 'totalEpisodes': 149, 'stepsPerEpisode': 104, 'rewardPerEpisode': 87.2519843812265
'totalSteps': 17920, 'rewardStep': 0.7857398157354476, 'errorList': [], 'lossList': [0.0, -1.2462375754117965, 0.0, 5.393104433715344, 0.0, 0.0, 0.0], 'rewardMean': 0.6636564355129168, 'totalEpisodes': 150, 'stepsPerEpisode': 751, 'rewardPerEpisode': 576.6438941468663
'totalSteps': 19200, 'rewardStep': 0.804832879648195, 'errorList': [], 'lossList': [0.0, -1.215527440905571, 0.0, 2.1091361317038535, 0.0, 0.0, 0.0], 'rewardMean': 0.6945297359162107, 'totalEpisodes': 150, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 937.8124412120151
'totalSteps': 20480, 'rewardStep': 0.6437274273251613, 'errorList': [], 'lossList': [0.0, -1.1740964484214782, 0.0, 2.118773737102747, 0.0, 0.0, 0.0], 'rewardMean': 0.7309337715354944, 'totalEpisodes': 150, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.1276118816851
'totalSteps': 21760, 'rewardStep': 0.8409320174112237, 'errorList': [], 'lossList': [0.0, -1.1295664840936661, 0.0, 1.512924153059721, 0.0, 0.0, 0.0], 'rewardMean': 0.7486715922133107, 'totalEpisodes': 150, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.3678465243836
'totalSteps': 23040, 'rewardStep': 0.9583725156334754, 'errorList': [0.08617064284273805, 0.0970091565187846, 0.0661803022270318, 0.0813566445518833, 0.1255725756326775, 0.06701147228258718, 0.0886939090370622, 0.08247517441831956, 0.05287162919207993, 0.10438087587952245, 0.05797844917468713, 0.045441061043263294, 0.07605742744174412, 0.07488732471507485, 0.12512876369017345, 0.06609066600804983, 0.08915875531526286, 0.048048819174252785, 0.05845124553811001, 0.06487416995229224, 0.09344385951993019, 0.08429879891469207, 0.07650970268398233, 0.08982529091851757, 0.08426438770033028, 0.04977998834579947, 0.0927229700956636, 0.07799007529051377, 0.04080630173007474, 0.05784639293896435, 0.058714618752602046, 0.06872405596251845, 0.11559880688639111, 0.09363537335728794, 0.06339019731397032, 0.05235741900995795, 0.094795415300825, 0.07238716340860277, 0.08948346013412323, 0.08061370657454359, 0.05727007537872167, 0.10335677759200049, 0.08236383649419628, 0.1463204292646807, 0.09967796783922508, 0.06969873263346313, 0.08482699925374139, 0.08598594276080267, 0.07798187946408129, 0.05840865677667221], 'lossList': [0.0, -1.1067438793182374, 0.0, 1.3241061349213123, 0.0, 0.0, 0.0], 'rewardMean': 0.7716091261358441, 'totalEpisodes': 150, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.802478489886, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=72.08
