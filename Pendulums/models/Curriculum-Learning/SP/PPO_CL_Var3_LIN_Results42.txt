#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 42
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.709270645526797, 'errorList': [], 'lossList': [0.0, -1.4562231647968291, 0.0, 31.308547098636627, 0.0, 0.0, 0.0], 'rewardMean': 0.5935282103747976, 'totalEpisodes': 14, 'stepsPerEpisode': 285, 'rewardPerEpisode': 213.67577433709386
'totalSteps': 3840, 'rewardStep': 0.7678212081154949, 'errorList': [], 'lossList': [0.0, -1.4612478518486023, 0.0, 31.06364069223404, 0.0, 0.0, 0.0], 'rewardMean': 0.6516258762883634, 'totalEpisodes': 20, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.754120084878801
'totalSteps': 5120, 'rewardStep': 0.7189437847537108, 'errorList': [], 'lossList': [0.0, -1.4239537489414216, 0.0, 50.63384852409363, 0.0, 0.0, 0.0], 'rewardMean': 0.6684553534047002, 'totalEpisodes': 31, 'stepsPerEpisode': 53, 'rewardPerEpisode': 37.795149378846794
'totalSteps': 6400, 'rewardStep': 0.6094088459603085, 'errorList': [], 'lossList': [0.0, -1.4058351510763167, 0.0, 113.99642766952515, 0.0, 0.0, 0.0], 'rewardMean': 0.6566460519158219, 'totalEpisodes': 53, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.752512914287653
'totalSteps': 7680, 'rewardStep': 0.8014427473800857, 'errorList': [], 'lossList': [0.0, -1.3976887542009353, 0.0, 117.5387952041626, 0.0, 0.0, 0.0], 'rewardMean': 0.6807788344931992, 'totalEpisodes': 99, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.370971184869338
'totalSteps': 8960, 'rewardStep': 0.9099239295267428, 'errorList': [], 'lossList': [0.0, -1.3800742405653, 0.0, 54.12796080589295, 0.0, 0.0, 0.0], 'rewardMean': 0.7135138480694198, 'totalEpisodes': 130, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.081511920236233
'totalSteps': 10240, 'rewardStep': 0.5123402165612705, 'errorList': [], 'lossList': [0.0, -1.3628395074605941, 0.0, 33.70476689815521, 0.0, 0.0, 0.0], 'rewardMean': 0.688367144130901, 'totalEpisodes': 144, 'stepsPerEpisode': 121, 'rewardPerEpisode': 84.8418678092329
'totalSteps': 11520, 'rewardStep': 0.8780242952591881, 'errorList': [], 'lossList': [0.0, -1.3613840079307555, 0.0, 22.25800581932068, 0.0, 0.0, 0.0], 'rewardMean': 0.709440160922933, 'totalEpisodes': 152, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.407409733842787
'totalSteps': 12800, 'rewardStep': 0.8395419356418462, 'errorList': [], 'lossList': [0.0, -1.351819447875023, 0.0, 26.34184139728546, 0.0, 0.0, 0.0], 'rewardMean': 0.7224503383948243, 'totalEpisodes': 159, 'stepsPerEpisode': 143, 'rewardPerEpisode': 117.39245040849077
'totalSteps': 14080, 'rewardStep': 0.8737214792763576, 'errorList': [], 'lossList': [0.0, -1.341772004365921, 0.0, 7.960269194245338, 0.0, 0.0, 0.0], 'rewardMean': 0.7620439088001802, 'totalEpisodes': 163, 'stepsPerEpisode': 128, 'rewardPerEpisode': 102.19871760222372
'totalSteps': 15360, 'rewardStep': 0.6263346248158301, 'errorList': [], 'lossList': [0.0, -1.3462522184848786, 0.0, 13.966583632230758, 0.0, 0.0, 0.0], 'rewardMean': 0.7537503067290836, 'totalEpisodes': 166, 'stepsPerEpisode': 344, 'rewardPerEpisode': 255.47392082253745
'totalSteps': 16640, 'rewardStep': 0.837751107432886, 'errorList': [], 'lossList': [0.0, -1.344202972650528, 0.0, 5.254165314435959, 0.0, 0.0, 0.0], 'rewardMean': 0.7607432966608225, 'totalEpisodes': 168, 'stepsPerEpisode': 392, 'rewardPerEpisode': 302.71693322996845
'totalSteps': 17920, 'rewardStep': 0.6612701857958837, 'errorList': [], 'lossList': [0.0, -1.332260810136795, 0.0, 9.152456354498863, 0.0, 0.0, 0.0], 'rewardMean': 0.7549759367650399, 'totalEpisodes': 169, 'stepsPerEpisode': 492, 'rewardPerEpisode': 400.6323758993779
'totalSteps': 19200, 'rewardStep': 0.6928977328353491, 'errorList': [], 'lossList': [0.0, -1.3012987303733825, 0.0, 23.32157954931259, 0.0, 0.0, 0.0], 'rewardMean': 0.763324825452544, 'totalEpisodes': 170, 'stepsPerEpisode': 759, 'rewardPerEpisode': 562.9351620481997
'totalSteps': 20480, 'rewardStep': 0.7144146476543047, 'errorList': [], 'lossList': [0.0, -1.2704932636022568, 0.0, 1.3698197916150092, 0.0, 0.0, 0.0], 'rewardMean': 0.7546220154799659, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 888.0842287569453
'totalSteps': 21760, 'rewardStep': 0.8072561918690221, 'errorList': [], 'lossList': [0.0, -1.233191437125206, 0.0, 2.0414938723295926, 0.0, 0.0, 0.0], 'rewardMean': 0.7443552417141939, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1107.2611651112888
'totalSteps': 23040, 'rewardStep': 0.9363095852539585, 'errorList': [0.12345671968510274, 0.13168674203106986, 0.1417968729011288, 0.19885918817224954, 0.13632483196128892, 0.1237493361912779, 0.1824744354024152, 0.20020437041264383, 0.17031812044774533, 0.14284975451103193, 0.13034720624866103, 0.1852881687307514, 0.16213398117392885, 0.1710069684658111, 0.12467426458021032, 0.1871817940572681, 0.1875240490917291, 0.0893785797594732, 0.12509066935999952, 0.10949920289329518, 0.1290508039321283, 0.15840370203875956, 0.21804602746850701, 0.1413832723606396, 0.12834920598777774, 0.127437128278694, 0.20552020859506312, 0.15571797575998006, 0.12041611401192483, 0.10005430885198793, 0.09289998557850293, 0.17199830998693938, 0.13963210900896156, 0.11413053300843434, 0.12238584009986461, 0.09311804865278461, 0.1455273327671517, 0.15403983687566195, 0.10801332530853716, 0.16402324064047172, 0.1037490093506308, 0.12998740941181508, 0.18198841231958857, 0.10809546263473502, 0.09466077532679336, 0.1721719256406458, 0.22894116600618805, 0.2696627000690336, 0.12884518939944753, 0.1587249604046818], 'lossList': [0.0, -1.1907131457328797, 0.0, 1.6352494833245874, 0.0, 0.0, 0.0], 'rewardMean': 0.7867521785834626, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.005578709318, 'successfulTests': 45
'totalSteps': 24320, 'rewardStep': 0.897040584128022, 'errorList': [], 'lossList': [0.0, -1.1633256566524506, 0.0, 1.5631844310835004, 0.0, 0.0, 0.0], 'rewardMean': 0.788653807470346, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.1278961774715
'totalSteps': 25600, 'rewardStep': 0.863608986747194, 'errorList': [], 'lossList': [0.0, -1.1545844465494155, 0.0, 1.1818136044964194, 0.0, 0.0, 0.0], 'rewardMean': 0.7910605125808807, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.738767628542
#maxSuccessfulTests=45, maxSuccessfulTestsAtStep=23040, timeSpent=77.76
