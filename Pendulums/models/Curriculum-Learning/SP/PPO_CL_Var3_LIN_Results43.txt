#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 43
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5476352894638511, 'errorList': [], 'lossList': [0.0, -1.4221004742383956, 0.0, 26.19516420841217, 0.0, 0.0, 0.0], 'rewardMean': 0.7029236990096954, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 30.059735551125083
'totalSteps': 3840, 'rewardStep': 0.9470897572562742, 'errorList': [], 'lossList': [0.0, -1.4195315110683442, 0.0, 29.651113181114198, 0.0, 0.0, 0.0], 'rewardMean': 0.7843123850918884, 'totalEpisodes': 21, 'stepsPerEpisode': 378, 'rewardPerEpisode': 276.05092708869176
'totalSteps': 5120, 'rewardStep': 0.7407985184308393, 'errorList': [], 'lossList': [0.0, -1.4178706270456314, 0.0, 54.77670885562897, 0.0, 0.0, 0.0], 'rewardMean': 0.7734339184266261, 'totalEpisodes': 28, 'stepsPerEpisode': 79, 'rewardPerEpisode': 70.86457674742707
'totalSteps': 6400, 'rewardStep': 0.8360269916718849, 'errorList': [], 'lossList': [0.0, -1.4140170139074326, 0.0, 116.07427551269531, 0.0, 0.0, 0.0], 'rewardMean': 0.7859525330756779, 'totalEpisodes': 51, 'stepsPerEpisode': 28, 'rewardPerEpisode': 17.39493677795445
'totalSteps': 7680, 'rewardStep': 0.546792240147921, 'errorList': [], 'lossList': [0.0, -1.4061312609910965, 0.0, 134.73013023376464, 0.0, 0.0, 0.0], 'rewardMean': 0.746092484254385, 'totalEpisodes': 94, 'stepsPerEpisode': 47, 'rewardPerEpisode': 33.752630105317934
'totalSteps': 8960, 'rewardStep': 0.6845241799956577, 'errorList': [], 'lossList': [0.0, -1.3957412737607955, 0.0, 79.36619722366333, 0.0, 0.0, 0.0], 'rewardMean': 0.737297012217424, 'totalEpisodes': 128, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.01520521506317
'totalSteps': 10240, 'rewardStep': 0.4343086874052084, 'errorList': [], 'lossList': [0.0, -1.3941433835029602, 0.0, 37.46267696380615, 0.0, 0.0, 0.0], 'rewardMean': 0.6994234716158971, 'totalEpisodes': 143, 'stepsPerEpisode': 165, 'rewardPerEpisode': 110.00049307804608
'totalSteps': 11520, 'rewardStep': 0.534653002202323, 'errorList': [], 'lossList': [0.0, -1.3981212186813354, 0.0, 18.119348690509796, 0.0, 0.0, 0.0], 'rewardMean': 0.6811156416810555, 'totalEpisodes': 151, 'stepsPerEpisode': 140, 'rewardPerEpisode': 113.44760980509729
'totalSteps': 12800, 'rewardStep': 0.6896918131355388, 'errorList': [], 'lossList': [0.0, -1.3942790657281876, 0.0, 17.319213573932647, 0.0, 0.0, 0.0], 'rewardMean': 0.681973258826504, 'totalEpisodes': 155, 'stepsPerEpisode': 328, 'rewardPerEpisode': 250.75439392007536
'totalSteps': 14080, 'rewardStep': 0.7829720251959096, 'errorList': [], 'lossList': [0.0, -1.3892092090845107, 0.0, 14.189768830537796, 0.0, 0.0, 0.0], 'rewardMean': 0.6744492504905408, 'totalEpisodes': 160, 'stepsPerEpisode': 69, 'rewardPerEpisode': 58.348879337634344
'totalSteps': 15360, 'rewardStep': 0.3714941402430885, 'errorList': [], 'lossList': [0.0, -1.3990604901313781, 0.0, 6.665953494310379, 0.0, 0.0, 0.0], 'rewardMean': 0.6568351355684645, 'totalEpisodes': 163, 'stepsPerEpisode': 287, 'rewardPerEpisode': 203.23633395238406
'totalSteps': 16640, 'rewardStep': 0.47991470571235856, 'errorList': [], 'lossList': [0.0, -1.3844404810667037, 0.0, 7.834265244007111, 0.0, 0.0, 0.0], 'rewardMean': 0.610117630414073, 'totalEpisodes': 165, 'stepsPerEpisode': 534, 'rewardPerEpisode': 324.8575696313065
'totalSteps': 17920, 'rewardStep': 0.6429228430358432, 'errorList': [], 'lossList': [0.0, -1.3515127623081207, 0.0, 3.3641296285390854, 0.0, 0.0, 0.0], 'rewardMean': 0.6003300628745735, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 928.4038747265705
'totalSteps': 19200, 'rewardStep': 0.970578145028598, 'errorList': [0.033061051954548115, 0.023381148808807598, 0.025032467249787364, 0.03090366966065921, 0.03398869705185469, 0.033779657182901116, 0.02931179720434935, 0.024076891299503866, 0.05564504917875671, 0.030701529642071915, 0.03734544386184119, 0.038806420996126204, 0.04231491219157172, 0.022742214384718306, 0.03089220689644114, 0.03400321618309315, 0.04772148440028916, 0.034672447549586996, 0.026150904004727266, 0.0244036518816212, 0.035028334926442715, 0.031029565455407686, 0.03419837140970343, 0.04245567149890045, 0.048414745575325935, 0.031821584095172266, 0.028168510272988223, 0.03676549348236286, 0.029374980282955522, 0.024326450212533133, 0.028452761126868793, 0.023667890766648852, 0.05234750523786854, 0.032819092215320714, 0.03654443355141102, 0.023134239055070803, 0.03660787217830117, 0.0227253444021775, 0.05629728980641415, 0.03125021907177309, 0.030712669852264143, 0.029554434655476615, 0.029377572674254292, 0.02915560893639926, 0.024581645028062423, 0.045515479162678035, 0.022859667861916927, 0.03634829109344973, 0.02308162868793171, 0.04944258372908411], 'lossList': [0.0, -1.3205147278308869, 0.0, 4.529474573731423, 0.0, 0.0, 0.0], 'rewardMean': 0.6137851782102447, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 958.9126182242361, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=63.53
