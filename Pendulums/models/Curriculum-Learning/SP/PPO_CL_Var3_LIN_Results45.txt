#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 45
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.8828832870101601, 'errorList': [], 'lossList': [0.0, -1.4388261610269546, 0.0, 29.415266981720926, 0.0, 0.0, 0.0], 'rewardMean': 0.8988519587954941, 'totalEpisodes': 8, 'stepsPerEpisode': 531, 'rewardPerEpisode': 370.2848689407091
'totalSteps': 3840, 'rewardStep': 0.7416304278364738, 'errorList': [], 'lossList': [0.0, -1.430791288614273, 0.0, 29.42438432455063, 0.0, 0.0, 0.0], 'rewardMean': 0.846444781809154, 'totalEpisodes': 13, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.28431143762003
'totalSteps': 5120, 'rewardStep': 0.6708354787872656, 'errorList': [], 'lossList': [0.0, -1.430565001964569, 0.0, 47.75996758460999, 0.0, 0.0, 0.0], 'rewardMean': 0.8025424560536819, 'totalEpisodes': 19, 'stepsPerEpisode': 173, 'rewardPerEpisode': 133.27543768007502
'totalSteps': 6400, 'rewardStep': 0.9177823734539097, 'errorList': [], 'lossList': [0.0, -1.4237470477819443, 0.0, 99.31335800170899, 0.0, 0.0, 0.0], 'rewardMean': 0.8255904395337275, 'totalEpisodes': 35, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.182406418129702
'totalSteps': 7680, 'rewardStep': 0.6904294368792152, 'errorList': [], 'lossList': [0.0, -1.4179705077409743, 0.0, 147.0571771621704, 0.0, 0.0, 0.0], 'rewardMean': 0.8030636057579755, 'totalEpisodes': 78, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.452833906808754
'totalSteps': 8960, 'rewardStep': 0.6964345128746197, 'errorList': [], 'lossList': [0.0, -1.4059509843587876, 0.0, 87.01350790023804, 0.0, 0.0, 0.0], 'rewardMean': 0.7878308782032103, 'totalEpisodes': 115, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6964345128746197
'totalSteps': 10240, 'rewardStep': 0.5125814700769556, 'errorList': [], 'lossList': [0.0, -1.3961487799882888, 0.0, 72.7509252166748, 0.0, 0.0, 0.0], 'rewardMean': 0.7534247021874284, 'totalEpisodes': 143, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.5347011644974748
'totalSteps': 11520, 'rewardStep': 0.8963965044167729, 'errorList': [], 'lossList': [0.0, -1.3866621977090836, 0.0, 55.16405518531799, 0.0, 0.0, 0.0], 'rewardMean': 0.7693104579906889, 'totalEpisodes': 160, 'stepsPerEpisode': 68, 'rewardPerEpisode': 59.58199085722726
'totalSteps': 12800, 'rewardStep': 0.9090121492380269, 'errorList': [], 'lossList': [0.0, -1.3689990800619125, 0.0, 40.13973970413208, 0.0, 0.0, 0.0], 'rewardMean': 0.7832806271154227, 'totalEpisodes': 167, 'stepsPerEpisode': 138, 'rewardPerEpisode': 107.18105942368449
'totalSteps': 14080, 'rewardStep': 0.5948893429303235, 'errorList': [], 'lossList': [0.0, -1.3556297516822815, 0.0, 12.194692777395248, 0.0, 0.0, 0.0], 'rewardMean': 0.7512874983503722, 'totalEpisodes': 168, 'stepsPerEpisode': 129, 'rewardPerEpisode': 100.22004360590115
'totalSteps': 15360, 'rewardStep': 0.8737013749504169, 'errorList': [], 'lossList': [0.0, -1.3283520919084548, 0.0, 18.284665937423707, 0.0, 0.0, 0.0], 'rewardMean': 0.750369307144398, 'totalEpisodes': 173, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.200631475897776
'totalSteps': 16640, 'rewardStep': 0.9050499497547639, 'errorList': [], 'lossList': [0.0, -1.312601021528244, 0.0, 8.43908802241087, 0.0, 0.0, 0.0], 'rewardMean': 0.766711259336227, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.0166479246668
'totalSteps': 17920, 'rewardStep': 0.8106216554977133, 'errorList': [], 'lossList': [0.0, -1.3104340279102324, 0.0, 4.412402004748583, 0.0, 0.0, 0.0], 'rewardMean': 0.7806898770072717, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1023.6115755765536
'totalSteps': 19200, 'rewardStep': 0.8459880626488091, 'errorList': [], 'lossList': [0.0, -1.263212285041809, 0.0, 4.552070220708847, 0.0, 0.0, 0.0], 'rewardMean': 0.7735104459267617, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.3675692174124
'totalSteps': 20480, 'rewardStep': 0.859713432008811, 'errorList': [], 'lossList': [0.0, -1.2325420820713042, 0.0, 3.034235278144479, 0.0, 0.0, 0.0], 'rewardMean': 0.7904388454397212, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.3547257966284
'totalSteps': 21760, 'rewardStep': 0.978545930395727, 'errorList': [0.08186211595416185, 0.07605255070493198, 0.07767471667197975, 0.07912635070589394, 0.07730280813762429, 0.07621376787324298, 0.07762615226150921, 0.08344186804124046, 0.10895611672380147, 0.07699099356705773, 0.07743426544086528, 0.10679424061098529, 0.1420019363032089, 0.07650901470472045, 0.09602599435466241, 0.07730384023232179, 0.09453986334111339, 0.19954322301502206, 0.07592331340711708, 0.07579753644792847, 0.1768131151862849, 0.07652896932568388, 0.07700711522745261, 0.09530382826584687, 0.07701962333259527, 0.0760495348488082, 0.07570400151851787, 0.0772224312472901, 0.07903042179715473, 0.07646606625470968, 0.07779491189003566, 0.1046442740448635, 0.07746649655982756, 0.14608839413784136, 0.07715851981903632, 0.07767066315017394, 0.07728797618083656, 0.07666068765302482, 0.07598236845745951, 0.07707811986350135, 0.1246937228388121, 0.08105046707504879, 0.07589376606489225, 0.0791944819582634, 0.07685829746306078, 0.11549915304081136, 0.0772248832141761, 0.13030414827807915, 0.07798722857450915, 0.07750494680097006], 'lossList': [0.0, -1.204640217423439, 0.0, 2.304134864360094, 0.0, 0.0, 0.0], 'rewardMean': 0.818649987191832, 'totalEpisodes': 173, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.7013200632211, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=71.26
