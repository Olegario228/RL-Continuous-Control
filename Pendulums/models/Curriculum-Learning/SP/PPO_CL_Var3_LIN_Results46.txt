#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 46
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.7904191962234508, 'errorList': [], 'lossList': [0.0, -1.4515067946910858, 0.0, 36.37450621366501, 0.0, 0.0, 0.0], 'rewardMean': 0.6917985079018051, 'totalEpisodes': 12, 'stepsPerEpisode': 78, 'rewardPerEpisode': 68.7536103995439
'totalSteps': 3840, 'rewardStep': 0.8601831269841882, 'errorList': [], 'lossList': [0.0, -1.46943665266037, 0.0, 26.434318221211434, 0.0, 0.0, 0.0], 'rewardMean': 0.7479267142625995, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 958.9390658485596
'totalSteps': 5120, 'rewardStep': 0.8637461202921901, 'errorList': [], 'lossList': [0.0, -1.4480917310714723, 0.0, 39.14565692424774, 0.0, 0.0, 0.0], 'rewardMean': 0.7768815657699972, 'totalEpisodes': 15, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.887630742055723
'totalSteps': 6400, 'rewardStep': 0.8945589464535509, 'errorList': [], 'lossList': [0.0, -1.4353752851486206, 0.0, 83.58063041687012, 0.0, 0.0, 0.0], 'rewardMean': 0.800417041906708, 'totalEpisodes': 25, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.706473219868125
'totalSteps': 7680, 'rewardStep': 0.8805798914447203, 'errorList': [], 'lossList': [0.0, -1.424822023510933, 0.0, 153.73028888702393, 0.0, 0.0, 0.0], 'rewardMean': 0.81377751682971, 'totalEpisodes': 58, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.641081281318161
'totalSteps': 8960, 'rewardStep': 0.9679915172388316, 'errorList': [223.53292962739118, 280.8380557821808, 243.95584590109033, 260.429139434365, 261.2020004375937, 288.1380329293434, 275.45142651078197, 279.5084725923736, 279.81957878909964, 289.61594205017116, 237.42756198395776, 190.90462988786308, 287.3817772207916, 249.1910282784415, 261.96917243157293, 284.15551582741193, 273.4705861371064, 250.15870663407378, 252.95930433208173, 274.8841850907144, 281.17359823249444, 284.9743189648884, 287.50637626369496, 288.335818990417, 270.23024402539636, 280.13144218312107, 301.1289021939189, 284.22773010354035, 244.89155436428496, 249.79490157173612, 275.77333634686306, 257.94073584862315, 262.3293186261914, 276.41476039280775, 276.4017711924485, 218.15488781843135, 265.2288845181903, 291.9284813787377, 203.01968228109155, 288.96216915184857, 255.6710553113582, 295.0459202824841, 289.45255889385237, 265.5897418663177, 278.3316003830964, 286.1860734047381, 288.58708938541923, 297.38554053698203, 251.8376974374168, 265.8851860705766], 'lossList': [0.0, -1.4168137151002884, 0.0, 88.45767360687256, 0.0, 0.0, 0.0], 'rewardMean': 0.8358080883167274, 'totalEpisodes': 104, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.854479996548147, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.9079144054148982, 'errorList': [], 'lossList': [0.0, -1.4067672908306121, 0.0, 57.88347438812256, 0.0, 0.0, 0.0], 'rewardMean': 0.8448213779539987, 'totalEpisodes': 124, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.381171602107656
'totalSteps': 11520, 'rewardStep': 0.5885442820233294, 'errorList': [], 'lossList': [0.0, -1.3941524302959443, 0.0, 42.662906560897824, 0.0, 0.0, 0.0], 'rewardMean': 0.8163461450728132, 'totalEpisodes': 134, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.414563439210026
'totalSteps': 12800, 'rewardStep': 0.927778948391627, 'errorList': [], 'lossList': [0.0, -1.3649719673395158, 0.0, 37.97599378585815, 0.0, 0.0, 0.0], 'rewardMean': 0.8274894254046947, 'totalEpisodes': 141, 'stepsPerEpisode': 87, 'rewardPerEpisode': 64.45710733371872
'totalSteps': 14080, 'rewardStep': 0.7473376615844832, 'errorList': [], 'lossList': [0.0, -1.3407621014118194, 0.0, 17.078610247373582, 0.0, 0.0, 0.0], 'rewardMean': 0.8429054096051269, 'totalEpisodes': 143, 'stepsPerEpisode': 1036, 'rewardPerEpisode': 801.1273552330701
'totalSteps': 15360, 'rewardStep': 0.8204806142760551, 'errorList': [], 'lossList': [0.0, -1.3297394609451294, 0.0, 11.631577200889588, 0.0, 0.0, 0.0], 'rewardMean': 0.8459115514103874, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.8725155639105
'totalSteps': 16640, 'rewardStep': 0.7160835069857059, 'errorList': [], 'lossList': [0.0, -1.3013027316331864, 0.0, 5.075049473941326, 0.0, 0.0, 0.0], 'rewardMean': 0.8315015894105391, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.7377844515893
'totalSteps': 17920, 'rewardStep': 0.8552125184377881, 'errorList': [], 'lossList': [0.0, -1.2738060194253922, 0.0, 5.820200100168586, 0.0, 0.0, 0.0], 'rewardMean': 0.8306482292250991, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.740809922621
'totalSteps': 19200, 'rewardStep': 0.9420902378307366, 'errorList': [0.07115004541240262, 0.07065279928964234, 0.07003137639972279, 0.100067222279547, 0.07008584000934337, 0.1063080752234114, 0.08134973013506794, 0.09049768082464664, 0.062376372599258025, 0.0650079697350727, 0.08947291546183483, 0.0682388484186162, 0.06360691407816654, 0.08501769713639774, 0.08930139436522591, 0.09811089618622415, 0.06718766495455056, 0.09709665356708606, 0.11230891589201553, 0.11950262438776985, 0.06371309386515657, 0.08891916027114442, 0.08754233537536486, 0.09951410799429246, 0.07044453685467238, 0.07409257427006681, 0.07402904160095836, 0.13871035303998872, 0.11450156066579127, 0.059764279279297366, 0.10293280833528515, 0.06553293579097848, 0.1078976715378937, 0.07985952212266226, 0.07744045039036271, 0.08712052683861282, 0.0673520793222486, 0.07564241190917574, 0.06796045467383272, 0.06415696002500455, 0.09110530116245341, 0.09506530265535952, 0.06901543294727348, 0.05963598179268372, 0.06284262814596554, 0.08385730360319021, 0.05850908942901943, 0.09981253044959205, 0.06942770566515115, 0.09122948678865957], 'lossList': [0.0, -1.2541361618041993, 0.0, 3.819794801622629, 0.0, 0.0, 0.0], 'rewardMean': 0.8354013583628175, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.2568386854732, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=85.29
