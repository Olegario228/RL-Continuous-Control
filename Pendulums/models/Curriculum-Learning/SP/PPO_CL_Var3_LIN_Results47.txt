#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 47
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.7302053974658015, 'errorList': [], 'lossList': [0.0, -1.4568211591243745, 0.0, 32.576294667720795, 0.0, 0.0, 0.0], 'rewardMean': 0.6166531226849066, 'totalEpisodes': 14, 'stepsPerEpisode': 291, 'rewardPerEpisode': 228.17167635030634
'totalSteps': 3840, 'rewardStep': 0.8623348175130714, 'errorList': [], 'lossList': [0.0, -1.469697824716568, 0.0, 32.246434798240664, 0.0, 0.0, 0.0], 'rewardMean': 0.6985470209609614, 'totalEpisodes': 18, 'stepsPerEpisode': 485, 'rewardPerEpisode': 378.333066496561
'totalSteps': 5120, 'rewardStep': 0.48630664463626727, 'errorList': [], 'lossList': [0.0, -1.4328784656524658, 0.0, 41.14206067085266, 0.0, 0.0, 0.0], 'rewardMean': 0.6454869268797879, 'totalEpisodes': 26, 'stepsPerEpisode': 69, 'rewardPerEpisode': 38.842038946141116
'totalSteps': 6400, 'rewardStep': 0.7308688369789836, 'errorList': [], 'lossList': [0.0, -1.4269934540987015, 0.0, 106.10561321258545, 0.0, 0.0, 0.0], 'rewardMean': 0.662563308899627, 'totalEpisodes': 46, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.821152404687645
'totalSteps': 7680, 'rewardStep': 0.4910348303825446, 'errorList': [], 'lossList': [0.0, -1.420056574344635, 0.0, 156.52121772766114, 0.0, 0.0, 0.0], 'rewardMean': 0.63397522914678, 'totalEpisodes': 100, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.281223415402657
'totalSteps': 8960, 'rewardStep': 0.8404754656526614, 'errorList': [], 'lossList': [0.0, -1.415085586309433, 0.0, 62.08437072753906, 0.0, 0.0, 0.0], 'rewardMean': 0.6634752629333345, 'totalEpisodes': 138, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.494901739667755
'totalSteps': 10240, 'rewardStep': 0.5093052421076849, 'errorList': [], 'lossList': [0.0, -1.413004637360573, 0.0, 27.238730278015137, 0.0, 0.0, 0.0], 'rewardMean': 0.6442040103301283, 'totalEpisodes': 147, 'stepsPerEpisode': 109, 'rewardPerEpisode': 89.57693053236201
'totalSteps': 11520, 'rewardStep': 0.6843264486555368, 'errorList': [], 'lossList': [0.0, -1.4025680387020112, 0.0, 30.518633909225464, 0.0, 0.0, 0.0], 'rewardMean': 0.6486620590329515, 'totalEpisodes': 155, 'stepsPerEpisode': 64, 'rewardPerEpisode': 49.690719985433404
'totalSteps': 12800, 'rewardStep': 0.90616424128066, 'errorList': [], 'lossList': [0.0, -1.3925038760900497, 0.0, 18.65484783887863, 0.0, 0.0, 0.0], 'rewardMean': 0.6744122772577223, 'totalEpisodes': 159, 'stepsPerEpisode': 144, 'rewardPerEpisode': 116.17335755972317
'totalSteps': 14080, 'rewardStep': 0.7729950034018305, 'errorList': [], 'lossList': [0.0, -1.3829100900888442, 0.0, 13.184390742182732, 0.0, 0.0, 0.0], 'rewardMean': 0.7014016928075042, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 981.9230943087839
'totalSteps': 15360, 'rewardStep': 0.6214428262920769, 'errorList': [], 'lossList': [0.0, -1.405505793094635, 0.0, 11.688907066583633, 0.0, 0.0, 0.0], 'rewardMean': 0.6905254356901318, 'totalEpisodes': 161, 'stepsPerEpisode': 248, 'rewardPerEpisode': 178.97548503356424
'totalSteps': 16640, 'rewardStep': 0.9656164104356026, 'errorList': [0.11704764454581869, 0.16289701060131784, 0.11549402578686879, 0.12807859543404904, 0.2295434669458236, 0.1422922688745868, 0.05764599617291009, 0.12033060931127468, 0.16216632256719898, 0.0701042465872517, 0.0799840376639295, 0.11686370241279896, 0.1582079937538587, 0.26049133962851667, 0.057466170294073214, 0.12544712701893593, 0.151218805247341, 0.23752832876759336, 0.12769884144732363, 0.07746007780668906, 0.10671109976515415, 0.05942364805069465, 0.14838971546499677, 0.1648269623322766, 0.06463487954808034, 0.1370544663171662, 0.2072291730706254, 0.1021948560137742, 0.14358231057487159, 0.1303683331892789, 0.1358494066874809, 0.18673099494672854, 0.17395456082073268, 0.0963077874654067, 0.06860788669835871, 0.16539790725131281, 0.12896536536184164, 0.1695545536562978, 0.14087794446427104, 0.1380564127984545, 0.09824653877967804, 0.21775614598767998, 0.11515304513795577, 0.14455595422738074, 0.14647786482244674, 0.16553312399286066, 0.1635648974954994, 0.07017484303624764, 0.1206964236428585, 0.10411073372507856], 'lossList': [0.0, -1.4160818320512771, 0.0, 7.948968285918236, 0.0, 0.0, 0.0], 'rewardMean': 0.7008535949823849, 'totalEpisodes': 162, 'stepsPerEpisode': 1226, 'rewardPerEpisode': 857.8182171645204, 'successfulTests': 45
'totalSteps': 17920, 'rewardStep': 0.8207861616401835, 'errorList': [], 'lossList': [0.0, -1.389575670361519, 0.0, 4.405190332531929, 0.0, 0.0, 0.0], 'rewardMean': 0.7343015466827765, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.2681427753055
'totalSteps': 19200, 'rewardStep': 0.9552227266883145, 'errorList': [0.030069646114539782, 0.02345714925329891, 0.06539365338522897, 0.11510233760777812, 0.02197253942173325, 0.04986954992190928, 0.07719198753254816, 0.03208249735186319, 0.09775885475201934, 0.08904695551620787, 0.02620620901913548, 0.03621564368526296, 0.0474175635745554, 0.047274929740862254, 0.05124084943860562, 0.03637034684635745, 0.03811725940136606, 0.043155575731280066, 0.086267801222436, 0.058616495736788427, 0.1051246308905296, 0.027637290335919645, 0.05986424589058898, 0.029143346846684053, 0.06146435332086606, 0.05371282396808085, 0.035064907535427674, 0.03962568746518331, 0.05648135153141276, 0.09770346240696752, 0.09480838223268895, 0.021299290445211103, 0.020393258542602807, 0.02406724325157021, 0.0381500212122823, 0.04985728808453363, 0.071735932293955, 0.0282472579338404, 0.039834585745822194, 0.02130422046175594, 0.05553780610373456, 0.02097608654935779, 0.09099501541324599, 0.02048055577391383, 0.055872855832212624, 0.026974362154860394, 0.012408108521685082, 0.022821592957323875, 0.01671544806788159, 0.026210728798708248], 'lossList': [0.0, -1.3578366672992705, 0.0, 3.717219618111849, 0.0, 0.0, 0.0], 'rewardMean': 0.7567369356537097, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.119239072261, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=83.3
