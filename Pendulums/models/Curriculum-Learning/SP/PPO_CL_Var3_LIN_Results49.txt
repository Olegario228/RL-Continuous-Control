#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 49
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8821045295576111, 'errorList': [], 'lossList': [0.0, -1.4280032587051392, 0.0, 34.29245977401733, 0.0, 0.0, 0.0], 'rewardMean': 0.8156976969284461, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 262.02664377229206
'totalSteps': 3840, 'rewardStep': 0.7627993763782349, 'errorList': [], 'lossList': [0.0, -1.4431326901912689, 0.0, 30.858963141441347, 0.0, 0.0, 0.0], 'rewardMean': 0.798064923411709, 'totalEpisodes': 16, 'stepsPerEpisode': 169, 'rewardPerEpisode': 127.47643813989704
'totalSteps': 5120, 'rewardStep': 0.4770551096961097, 'errorList': [], 'lossList': [0.0, -1.4515051716566085, 0.0, 43.8475749540329, 0.0, 0.0, 0.0], 'rewardMean': 0.7178124699828091, 'totalEpisodes': 22, 'stepsPerEpisode': 236, 'rewardPerEpisode': 181.27075879796027
'totalSteps': 6400, 'rewardStep': 0.7296149043789782, 'errorList': [], 'lossList': [0.0, -1.4469409638643265, 0.0, 122.79527256011963, 0.0, 0.0, 0.0], 'rewardMean': 0.720172956862043, 'totalEpisodes': 43, 'stepsPerEpisode': 34, 'rewardPerEpisode': 24.058606279091034
'totalSteps': 7680, 'rewardStep': 0.6277071641867155, 'errorList': [], 'lossList': [0.0, -1.4330994719266892, 0.0, 144.9935443878174, 0.0, 0.0, 0.0], 'rewardMean': 0.704761991416155, 'totalEpisodes': 93, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.579322055853611
'totalSteps': 8960, 'rewardStep': 0.7830095176220573, 'errorList': [], 'lossList': [0.0, -1.4208391231298447, 0.0, 80.23993728637696, 0.0, 0.0, 0.0], 'rewardMean': 0.7159402094455697, 'totalEpisodes': 141, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.61589015885069
'totalSteps': 10240, 'rewardStep': 0.9304856314401906, 'errorList': [4.972322060465851, 32.83475158435077, 24.39222897516462, 14.895278531318946, 40.823968306910416, 40.85269379220422, 33.58940952369545, 7.487060275928177, 27.866706181652557, 10.627626840652077, 10.709391871912164, 6.632693344601602, 39.55709501214707, 10.083017430249786, 15.879940121396293, 24.334960840810886, 45.65434984818672, 48.39415984457853, 28.57924723735545, 53.765591655871134, 7.188662006698602, 11.703951750293573, 4.254214034612852, 23.85954472076108, 20.08232464841543, 9.80509001627293, 18.72352517993417, 35.87175031769993, 33.53936452716047, 2.1101634972732524, 4.298446669684252, 56.85829348730819, 17.48120816385159, 8.914491258202208, 10.350496644613733, 15.416522410186456, 4.288853809728858, 30.54553304634667, 17.972581113334233, 34.89668260299459, 2.3732224819274457, 21.625271017970245, 17.668059617179903, 4.913235783465251, 23.785090484843284, 19.59374298393083, 0.6921119578943772, 9.595052507352971, 3.7226156666129184, 42.80264968755474], 'lossList': [0.0, -1.4088365542888641, 0.0, 77.43607894897461, 0.0, 0.0, 0.0], 'rewardMean': 0.7427583871948973, 'totalEpisodes': 167, 'stepsPerEpisode': 80, 'rewardPerEpisode': 65.7551792986732, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.2606314758908733, 'errorList': [], 'lossList': [0.0, -1.3931892645359039, 0.0, 38.69527626037598, 0.0, 0.0, 0.0], 'rewardMean': 0.689188730383339, 'totalEpisodes': 174, 'stepsPerEpisode': 257, 'rewardPerEpisode': 162.62005891342446
'totalSteps': 12800, 'rewardStep': 0.7189693306485816, 'errorList': [], 'lossList': [0.0, -1.3723727309703826, 0.0, 45.74816299915314, 0.0, 0.0, 0.0], 'rewardMean': 0.6921667904098634, 'totalEpisodes': 183, 'stepsPerEpisode': 63, 'rewardPerEpisode': 49.40995700219532
'totalSteps': 14080, 'rewardStep': 0.615758690439086, 'errorList': [], 'lossList': [0.0, -1.3630676996707916, 0.0, 15.966372272372245, 0.0, 0.0, 0.0], 'rewardMean': 0.6788135730238438, 'totalEpisodes': 187, 'stepsPerEpisode': 483, 'rewardPerEpisode': 370.36244203084766
'totalSteps': 15360, 'rewardStep': 0.5012121268170718, 'errorList': [], 'lossList': [0.0, -1.3517343324422837, 0.0, 54.52625688552857, 0.0, 0.0, 0.0], 'rewardMean': 0.6407243327497898, 'totalEpisodes': 199, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.311381001876852
'totalSteps': 16640, 'rewardStep': 0.6298501736283707, 'errorList': [], 'lossList': [0.0, -1.3379585647583008, 0.0, 13.201961059570312, 0.0, 0.0, 0.0], 'rewardMean': 0.6274294124748035, 'totalEpisodes': 204, 'stepsPerEpisode': 121, 'rewardPerEpisode': 107.09669226629981
'totalSteps': 17920, 'rewardStep': 0.40445990834029627, 'errorList': [], 'lossList': [0.0, -1.3325296467542649, 0.0, 11.026099231243133, 0.0, 0.0, 0.0], 'rewardMean': 0.620169892339222, 'totalEpisodes': 210, 'stepsPerEpisode': 126, 'rewardPerEpisode': 95.07102649571891
'totalSteps': 19200, 'rewardStep': 0.8965299839945913, 'errorList': [], 'lossList': [0.0, -1.3325697529315947, 0.0, 8.64635660648346, 0.0, 0.0, 0.0], 'rewardMean': 0.6368614003007835, 'totalEpisodes': 215, 'stepsPerEpisode': 246, 'rewardPerEpisode': 219.07158735831635
'totalSteps': 20480, 'rewardStep': 0.8618557135350305, 'errorList': [], 'lossList': [0.0, -1.319646443128586, 0.0, 7.168874590396881, 0.0, 0.0, 0.0], 'rewardMean': 0.6602762552356151, 'totalEpisodes': 221, 'stepsPerEpisode': 55, 'rewardPerEpisode': 43.60231806025854
'totalSteps': 21760, 'rewardStep': 0.770638361531634, 'errorList': [], 'lossList': [0.0, -1.3114642816781998, 0.0, 6.669483867883682, 0.0, 0.0, 0.0], 'rewardMean': 0.6590391396265726, 'totalEpisodes': 226, 'stepsPerEpisode': 74, 'rewardPerEpisode': 64.77268921431275
'totalSteps': 23040, 'rewardStep': 0.603151180546633, 'errorList': [], 'lossList': [0.0, -1.3007626819610596, 0.0, 4.692398222684861, 0.0, 0.0, 0.0], 'rewardMean': 0.6263056945372169, 'totalEpisodes': 229, 'stepsPerEpisode': 432, 'rewardPerEpisode': 351.9274651843024
'totalSteps': 24320, 'rewardStep': 0.7652596808220538, 'errorList': [], 'lossList': [0.0, -1.2845278412103653, 0.0, 3.114239426255226, 0.0, 0.0, 0.0], 'rewardMean': 0.6767685150303351, 'totalEpisodes': 235, 'stepsPerEpisode': 61, 'rewardPerEpisode': 51.17168323925347
'totalSteps': 25600, 'rewardStep': 0.8504004977358993, 'errorList': [], 'lossList': [0.0, -1.2724371325969697, 0.0, 2.514289216399193, 0.0, 0.0, 0.0], 'rewardMean': 0.6899116317390666, 'totalEpisodes': 239, 'stepsPerEpisode': 88, 'rewardPerEpisode': 72.02245968821927
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=79.44
