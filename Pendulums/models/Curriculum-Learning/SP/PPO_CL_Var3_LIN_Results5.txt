#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 5
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.678633249474327, 'errorList': [], 'lossList': [0.0, -1.3899819022417068, 0.0, 25.93935613155365, 0.0, 0.0, 0.0], 'rewardMean': 0.6446516742080888, 'totalEpisodes': 23, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.87095203297098
'totalSteps': 3840, 'rewardStep': 0.2886683848984915, 'errorList': [], 'lossList': [0.0, -1.3740999126434326, 0.0, 40.684054670333865, 0.0, 0.0, 0.0], 'rewardMean': 0.5259905777715563, 'totalEpisodes': 42, 'stepsPerEpisode': 91, 'rewardPerEpisode': 65.74643566947697
'totalSteps': 5120, 'rewardStep': 0.6223375171273913, 'errorList': [], 'lossList': [0.0, -1.3549785089492798, 0.0, 57.865010318756106, 0.0, 0.0, 0.0], 'rewardMean': 0.5500773126105151, 'totalEpisodes': 62, 'stepsPerEpisode': 93, 'rewardPerEpisode': 64.00915899309291
'totalSteps': 6400, 'rewardStep': 0.8880719509451275, 'errorList': [], 'lossList': [0.0, -1.3390474838018418, 0.0, 88.91392488479615, 0.0, 0.0, 0.0], 'rewardMean': 0.6176762402774376, 'totalEpisodes': 101, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8880719509451275
'totalSteps': 7680, 'rewardStep': 0.8548094558989874, 'errorList': [], 'lossList': [0.0, -1.335799009203911, 0.0, 54.6617991733551, 0.0, 0.0, 0.0], 'rewardMean': 0.6571984428810292, 'totalEpisodes': 133, 'stepsPerEpisode': 45, 'rewardPerEpisode': 37.1200597124681
'totalSteps': 8960, 'rewardStep': 0.5784492360016011, 'errorList': [], 'lossList': [0.0, -1.3374199259281159, 0.0, 35.27528774261474, 0.0, 0.0, 0.0], 'rewardMean': 0.6459485561839681, 'totalEpisodes': 146, 'stepsPerEpisode': 81, 'rewardPerEpisode': 51.05084951723424
'totalSteps': 10240, 'rewardStep': 0.2633631294619797, 'errorList': [], 'lossList': [0.0, -1.3309888929128646, 0.0, 32.57608075618744, 0.0, 0.0, 0.0], 'rewardMean': 0.5981253778437194, 'totalEpisodes': 154, 'stepsPerEpisode': 164, 'rewardPerEpisode': 94.2244737888536
'totalSteps': 11520, 'rewardStep': 0.5661488809929278, 'errorList': [], 'lossList': [0.0, -1.3224690914154054, 0.0, 21.312499321699143, 0.0, 0.0, 0.0], 'rewardMean': 0.594572433749187, 'totalEpisodes': 160, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.28624298877718
'totalSteps': 12800, 'rewardStep': 0.6482405226142846, 'errorList': [], 'lossList': [0.0, -1.2946856302022933, 0.0, 15.548689450025558, 0.0, 0.0, 0.0], 'rewardMean': 0.5999392426356968, 'totalEpisodes': 163, 'stepsPerEpisode': 633, 'rewardPerEpisode': 501.8015550815789
'totalSteps': 14080, 'rewardStep': 0.7794189695905005, 'errorList': [], 'lossList': [0.0, -1.2842174994945526, 0.0, 11.749215476512909, 0.0, 0.0, 0.0], 'rewardMean': 0.6168141297005618, 'totalEpisodes': 166, 'stepsPerEpisode': 212, 'rewardPerEpisode': 161.581819819697
'totalSteps': 15360, 'rewardStep': 0.661009240412284, 'errorList': [], 'lossList': [0.0, -1.2761154025793076, 0.0, 9.67458541035652, 0.0, 0.0, 0.0], 'rewardMean': 0.6150517287943575, 'totalEpisodes': 168, 'stepsPerEpisode': 285, 'rewardPerEpisode': 245.3597242390567
'totalSteps': 16640, 'rewardStep': 0.9440036143713443, 'errorList': [0.7855581369408358, 0.8419575221782707, 0.7347124084827479, 0.85506840755927, 0.8083448412254104, 0.7338045160537484, 0.7875498949418315, 0.7896551481793067, 0.7784494363039056, 0.8083752258139816, 0.8186165848593614, 0.8582239516965166, 0.814262876556335, 0.8372420891560745, 0.8119705791088575, 0.8242047417362541, 0.7841583444967075, 0.8303878681588933, 0.7550886324658602, 0.8155046552815058, 0.7734894863896505, 0.8346271165779418, 0.7742240161278966, 0.7970716644367914, 0.779717393276789, 0.7801157093172487, 0.8113939226042074, 0.8317787191686663, 0.7950967117163279, 0.7959010308599809, 0.7673315023661359, 0.753561659699015, 0.8247629008149707, 0.8052444513888727, 0.7885366107480094, 0.7454920875967431, 0.8110160915614295, 0.7891589225905897, 0.8114466119203866, 0.8394415205183392, 0.7495821835375664, 0.7647353093552017, 0.7833905346794154, 0.775442729597925, 0.7666002824051639, 0.7297940232972849, 0.7453353328821007, 0.8068178645285803, 0.8275396199536545, 0.8068974694060982], 'lossList': [0.0, -1.2608713376522065, 0.0, 6.2924659073352815, 0.0, 0.0, 0.0], 'rewardMean': 0.6805852517416429, 'totalEpisodes': 168, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 996.4811568011162, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7381265877757799, 'errorList': [], 'lossList': [0.0, -1.25910036444664, 0.0, 4.916790515780449, 0.0, 0.0, 0.0], 'rewardMean': 0.6921641588064817, 'totalEpisodes': 170, 'stepsPerEpisode': 409, 'rewardPerEpisode': 339.1892913588301
'totalSteps': 19200, 'rewardStep': 0.6631662951931825, 'errorList': [], 'lossList': [0.0, -1.2508389925956727, 0.0, 4.716307631731033, 0.0, 0.0, 0.0], 'rewardMean': 0.6696735932312871, 'totalEpisodes': 171, 'stepsPerEpisode': 243, 'rewardPerEpisode': 196.75406063057923
'totalSteps': 20480, 'rewardStep': 0.9578205506677976, 'errorList': [0.0243525513830286, 0.05257645603869583, 0.07364236670083882, 0.026545095371060434, 0.09883495568692785, 0.03763372378382011, 0.06411621041539413, 0.05379629864762262, 0.06145878861631146, 0.007650663399831735, 0.02258415566978702, 0.12368733043904816, 0.04127841044869578, 0.07841470898922233, 0.04578450764140506, 0.0224995213142488, 0.016504246885755535, 0.06433255107804896, 0.05990340307102622, 0.017841044630165833, 0.019092935370971224, 0.01812252080992933, 0.07672700596811537, 0.03356237308998022, 0.030492674272102668, 0.06890373372769626, 0.05787792628179625, 0.02511183463040441, 0.04524771271693537, 0.06910207390810474, 0.04454988551186315, 0.014140744555257337, 0.09772578301201292, 0.025566715393433265, 0.04808007174291451, 0.05729714728090495, 0.05599772627567714, 0.05126497856792801, 0.1033732826534367, 0.05173724247579238, 0.029493070939597667, 0.020594684566958947, 0.022495430585252746, 0.12548539419313418, 0.032010473751345574, 0.059863187454337906, 0.015859260430987808, 0.07563444149555064, 0.010603755214428918, 0.02072205620459463], 'lossList': [0.0, -1.253824138045311, 0.0, 2.21798158839345, 0.0, 0.0, 0.0], 'rewardMean': 0.6799747027081681, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1018.6309664131167, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=74.68
