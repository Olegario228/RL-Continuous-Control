#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 52
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9145072257809304, 'errorList': [], 'lossList': [0.0, -1.4225682431459428, 0.0, 32.692888259887695, 0.0, 0.0, 0.0], 'rewardMean': 0.8650722837823199, 'totalEpisodes': 71, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.5529882732226
'totalSteps': 3840, 'rewardStep': 0.8063539353281346, 'errorList': [], 'lossList': [0.0, -1.4004344141483307, 0.0, 43.418388519287106, 0.0, 0.0, 0.0], 'rewardMean': 0.8454995009642582, 'totalEpisodes': 97, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.750132450541821
'totalSteps': 5120, 'rewardStep': 0.7705886781174643, 'errorList': [], 'lossList': [0.0, -1.377228420972824, 0.0, 49.546300630569455, 0.0, 0.0, 0.0], 'rewardMean': 0.8267717952525597, 'totalEpisodes': 114, 'stepsPerEpisode': 47, 'rewardPerEpisode': 38.43743493925669
'totalSteps': 6400, 'rewardStep': 0.4784854623421275, 'errorList': [], 'lossList': [0.0, -1.3624406415224075, 0.0, 59.87804023742676, 0.0, 0.0, 0.0], 'rewardMean': 0.7571145286704732, 'totalEpisodes': 127, 'stepsPerEpisode': 21, 'rewardPerEpisode': 14.523200099484445
'totalSteps': 7680, 'rewardStep': 0.6499889691965115, 'errorList': [], 'lossList': [0.0, -1.347873148918152, 0.0, 97.870536403656, 0.0, 0.0, 0.0], 'rewardMean': 0.7392602687581462, 'totalEpisodes': 147, 'stepsPerEpisode': 54, 'rewardPerEpisode': 37.58197600254954
'totalSteps': 8960, 'rewardStep': 0.579288721906979, 'errorList': [], 'lossList': [0.0, -1.353868852853775, 0.0, 36.56688858509064, 0.0, 0.0, 0.0], 'rewardMean': 0.7164071906365509, 'totalEpisodes': 163, 'stepsPerEpisode': 79, 'rewardPerEpisode': 64.68870529731502
'totalSteps': 10240, 'rewardStep': 0.504951087986799, 'errorList': [], 'lossList': [0.0, -1.3547086840867997, 0.0, 43.902490587234496, 0.0, 0.0, 0.0], 'rewardMean': 0.689975177805332, 'totalEpisodes': 170, 'stepsPerEpisode': 111, 'rewardPerEpisode': 74.65450793145044
'totalSteps': 11520, 'rewardStep': 0.8394607880004058, 'errorList': [], 'lossList': [0.0, -1.3658605974912643, 0.0, 30.423219904899597, 0.0, 0.0, 0.0], 'rewardMean': 0.7065846900492292, 'totalEpisodes': 180, 'stepsPerEpisode': 108, 'rewardPerEpisode': 93.27573517782946
'totalSteps': 12800, 'rewardStep': 0.9043522787328659, 'errorList': [], 'lossList': [0.0, -1.3750750297307968, 0.0, 27.2585803937912, 0.0, 0.0, 0.0], 'rewardMean': 0.7263614489175929, 'totalEpisodes': 187, 'stepsPerEpisode': 416, 'rewardPerEpisode': 354.55098138024783
'totalSteps': 14080, 'rewardStep': 0.693199832680407, 'errorList': [], 'lossList': [0.0, -1.3619830459356308, 0.0, 10.595690788030625, 0.0, 0.0, 0.0], 'rewardMean': 0.7141176980072624, 'totalEpisodes': 189, 'stepsPerEpisode': 323, 'rewardPerEpisode': 277.3924686099864
'totalSteps': 15360, 'rewardStep': 0.6125771848482898, 'errorList': [], 'lossList': [0.0, -1.3444674766063691, 0.0, 6.950500717163086, 0.0, 0.0, 0.0], 'rewardMean': 0.6839246939139985, 'totalEpisodes': 190, 'stepsPerEpisode': 1105, 'rewardPerEpisode': 792.956010402422
'totalSteps': 16640, 'rewardStep': 0.8256355986998709, 'errorList': [], 'lossList': [0.0, -1.3603602945804596, 0.0, 4.436603336334229, 0.0, 0.0, 0.0], 'rewardMean': 0.6858528602511721, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 950.768683338996
'totalSteps': 17920, 'rewardStep': 0.9012575823279342, 'errorList': [], 'lossList': [0.0, -1.3458691757917405, 0.0, 4.157932773530483, 0.0, 0.0, 0.0], 'rewardMean': 0.6989197506722191, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.8444764006301
'totalSteps': 19200, 'rewardStep': 0.8819849568110729, 'errorList': [], 'lossList': [0.0, -1.3054784917831421, 0.0, 2.5418298901617526, 0.0, 0.0, 0.0], 'rewardMean': 0.7392697001191136, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.1988328343318
'totalSteps': 20480, 'rewardStep': 0.9418138922224978, 'errorList': [0.01966462573336607, 0.026776767784679866, 0.15076576160975538, 0.031325079340836136, 0.10589931458318735, 0.06146586547044873, 0.02518953455521493, 0.04848650912270108, 0.018353030557681525, 0.01976847827391268, 0.039826627264292695, 0.06037511476820652, 0.08021505407812532, 0.028692840437438586, 0.057064503650223655, 0.021719355283867002, 0.06740802383396789, 0.028851729017902072, 0.053681410531618116, 0.028250074016903427, 0.025344957490465676, 0.08172830694465141, 0.09023677399320003, 0.037152786512984336, 0.026684848148021613, 0.0545523592481187, 0.07339089457411538, 0.06370019101869719, 0.03144123660718003, 0.08047073131570995, 0.06473411998423409, 0.06491884781530163, 0.05163069733502799, 0.050135146541318426, 0.025006939908217053, 0.0209193421406336, 0.030250564054602764, 0.05190912732277707, 0.04316914067007114, 0.03661817567475411, 0.05148652974423427, 0.04932844861730662, 0.08105442590002615, 0.11958522449653579, 0.0216876847630428, 0.10539026147825799, 0.022186067759421543, 0.08494237519166695, 0.03024706879542264, 0.030522285620046], 'lossList': [0.0, -1.2586990356445313, 0.0, 2.620180404484272, 0.0, 0.0, 0.0], 'rewardMean': 0.7684521924217123, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.3993010824313, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=68.1
