#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 54
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7218894211499995, 'errorList': [], 'lossList': [0.0, -1.407439987063408, 0.0, 33.485224895477295, 0.0, 0.0, 0.0], 'rewardMean': 0.8214897776507033, 'totalEpisodes': 74, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.906163914114494
'totalSteps': 3840, 'rewardStep': 0.860034591286956, 'errorList': [], 'lossList': [0.0, -1.4082632261514663, 0.0, 42.04536117553711, 0.0, 0.0, 0.0], 'rewardMean': 0.8343380488627875, 'totalEpisodes': 99, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.438701370124765
'totalSteps': 5120, 'rewardStep': 0.9062041202918791, 'errorList': [], 'lossList': [0.0, -1.390533336997032, 0.0, 54.73082570075989, 0.0, 0.0, 0.0], 'rewardMean': 0.8523045667200604, 'totalEpisodes': 116, 'stepsPerEpisode': 39, 'rewardPerEpisode': 29.7768180949388
'totalSteps': 6400, 'rewardStep': 0.6173962974739636, 'errorList': [], 'lossList': [0.0, -1.3771902292966842, 0.0, 50.349394159317015, 0.0, 0.0, 0.0], 'rewardMean': 0.8053229128708411, 'totalEpisodes': 124, 'stepsPerEpisode': 34, 'rewardPerEpisode': 19.95452837885052
'totalSteps': 7680, 'rewardStep': 0.7838475656333707, 'errorList': [], 'lossList': [0.0, -1.3735713315010072, 0.0, 85.53939172744751, 0.0, 0.0, 0.0], 'rewardMean': 0.8017436883312627, 'totalEpisodes': 139, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.56339135044107
'totalSteps': 8960, 'rewardStep': 0.7117748739383687, 'errorList': [], 'lossList': [0.0, -1.3687734031677246, 0.0, 83.61430997848511, 0.0, 0.0, 0.0], 'rewardMean': 0.7888910005608494, 'totalEpisodes': 163, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.8615465145540266
'totalSteps': 10240, 'rewardStep': 0.8107186489958885, 'errorList': [], 'lossList': [0.0, -1.3647590112686157, 0.0, 35.783201179504395, 0.0, 0.0, 0.0], 'rewardMean': 0.7916194566152291, 'totalEpisodes': 173, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.362121947450158
'totalSteps': 11520, 'rewardStep': 0.9389473526704366, 'errorList': [0.26884394822286434, 0.2813021329256448, 0.22255834561143711, 0.8497305023340367, 0.10815013867426028, 0.5883132997496453, 0.2818946186797819, 0.4357730363448184, 0.3296181948375976, 0.3743419101658753, 0.25959173830836557, 0.11956100340896075, 0.11069615552270179, 0.5122551858001589, 0.0972421915979232, 0.2452230342598853, 0.7496916408577181, 0.14532803732423463, 0.5601423347575725, 0.21105915171036577, 0.6793580879188079, 0.26536488041650524, 0.3560040425298568, 0.18520323714372255, 0.4189070539992381, 0.23763365821064064, 0.2697797276963248, 0.18366006676913546, 0.15434319996749596, 0.1739137899018529, 0.5762597157511551, 0.210294391334751, 0.7529266573917858, 0.08778618054437715, 0.39352385919521354, 0.2899601725064995, 0.35159442347859904, 0.7600958550129502, 0.34263232366372254, 0.36252563294495904, 0.07864542282623165, 0.15176149626411856, 0.21165449647619966, 0.5691890949617349, 0.18813560808163515, 0.1610333949208327, 0.3365290247102025, 0.31720669838973076, 0.6466764428480822, 0.47859118294332914], 'lossList': [0.0, -1.3589639592170715, 0.0, 12.917236077785493, 0.0, 0.0, 0.0], 'rewardMean': 0.8079892228435855, 'totalEpisodes': 178, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.237415625795418, 'successfulTests': 14
'totalSteps': 12800, 'rewardStep': 0.8196177525829862, 'errorList': [], 'lossList': [0.0, -1.3492533874511718, 0.0, 57.09493371963501, 0.0, 0.0, 0.0], 'rewardMean': 0.8091520758175257, 'totalEpisodes': 184, 'stepsPerEpisode': 64, 'rewardPerEpisode': 56.37414368893395
'totalSteps': 14080, 'rewardStep': 0.7533587680463176, 'errorList': [], 'lossList': [0.0, -1.3171377027034759, 0.0, 5.728565730154514, 0.0, 0.0, 0.0], 'rewardMean': 0.7923789392070167, 'totalEpisodes': 185, 'stepsPerEpisode': 1105, 'rewardPerEpisode': 692.234293535399
'totalSteps': 15360, 'rewardStep': 0.6465346298527306, 'errorList': [], 'lossList': [0.0, -1.285838640332222, 0.0, 25.291591337919236, 0.0, 0.0, 0.0], 'rewardMean': 0.7848434600772898, 'totalEpisodes': 188, 'stepsPerEpisode': 746, 'rewardPerEpisode': 571.6633989877091
'totalSteps': 16640, 'rewardStep': 0.5838979660933076, 'errorList': [], 'lossList': [0.0, -1.2606671476364135, 0.0, 6.933009795546532, 0.0, 0.0, 0.0], 'rewardMean': 0.757229797557925, 'totalEpisodes': 190, 'stepsPerEpisode': 105, 'rewardPerEpisode': 86.54055495157509
'totalSteps': 17920, 'rewardStep': 0.8046789084392617, 'errorList': [], 'lossList': [0.0, -1.2261027586460114, 0.0, 3.871226572394371, 0.0, 0.0, 0.0], 'rewardMean': 0.7470772763726632, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 943.0370749332187
'totalSteps': 19200, 'rewardStep': 0.7533161806087563, 'errorList': [], 'lossList': [0.0, -1.1976178634166716, 0.0, 2.2704319223761558, 0.0, 0.0, 0.0], 'rewardMean': 0.7606692646861425, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1026.733036533481
'totalSteps': 20480, 'rewardStep': 0.8862917448292846, 'errorList': [], 'lossList': [0.0, -1.164522376060486, 0.0, 2.2147044629603623, 0.0, 0.0, 0.0], 'rewardMean': 0.7709136826057338, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.6457069837552
'totalSteps': 21760, 'rewardStep': 0.8762952384519, 'errorList': [], 'lossList': [0.0, -1.1376483982801437, 0.0, 1.7590522699803115, 0.0, 0.0, 0.0], 'rewardMean': 0.787365719057087, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1095.0970211175622
'totalSteps': 23040, 'rewardStep': 0.8858053509000647, 'errorList': [], 'lossList': [0.0, -1.132482603788376, 0.0, 1.5071716478466988, 0.0, 0.0, 0.0], 'rewardMean': 0.7948743892475046, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.6306834317986
'totalSteps': 24320, 'rewardStep': 0.9379759965394389, 'errorList': [0.04552550525780558, 0.04220615509088663, 0.052336855048233266, 0.13195070256345373, 0.14053477313087812, 0.1631489604899273, 0.08949486128163936, 0.04608386194540822, 0.04228997613343661, 0.1330820239776718, 0.047138067537881814, 0.06168106741110355, 0.04785280264838284, 0.04334351849121221, 0.046565746604642196, 0.04906680707411937, 0.17932625180724707, 0.09205980606945624, 0.08755369642147669, 0.040232893017033554, 0.1344631829913909, 0.04306671247743452, 0.06819290717495141, 0.1415449330310025, 0.084158411974207, 0.04505179036465757, 0.08172273979856061, 0.197216189494783, 0.04615598970228753, 0.0476596239308296, 0.148341833989775, 0.06360674610189357, 0.11645996572486776, 0.09013963602176274, 0.06311045030943442, 0.04146324754820149, 0.048745367975169554, 0.0729917024854439, 0.06635333284483459, 0.07858750384036157, 0.10448808261162304, 0.09207274160522448, 0.06471361812963994, 0.044328849112707666, 0.04827481184790401, 0.04678401663384885, 0.043444319749634065, 0.041403476051938365, 0.044047957740533245, 0.056357072024548045], 'lossList': [0.0, -1.1298671472072601, 0.0, 1.0597822512872517, 0.0, 0.0, 0.0], 'rewardMean': 0.7947772536344049, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.8047941019072, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=98.13
