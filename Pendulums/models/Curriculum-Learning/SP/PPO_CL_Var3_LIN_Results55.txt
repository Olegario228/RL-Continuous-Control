#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 55
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8242263325162886, 'errorList': [], 'lossList': [0.0, -1.3991444545984268, 0.0, 25.465400218963623, 0.0, 0.0, 0.0], 'rewardMean': 0.7174482157290696, 'totalEpisodes': 19, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.1966276683692
'totalSteps': 3840, 'rewardStep': 0.5762530351958357, 'errorList': [], 'lossList': [0.0, -1.3890397441387177, 0.0, 40.654256706237796, 0.0, 0.0, 0.0], 'rewardMean': 0.6703831555513249, 'totalEpisodes': 33, 'stepsPerEpisode': 168, 'rewardPerEpisode': 116.62373979096202
'totalSteps': 5120, 'rewardStep': 0.646492444140965, 'errorList': [], 'lossList': [0.0, -1.3799488407373428, 0.0, 28.270583221912386, 0.0, 0.0, 0.0], 'rewardMean': 0.664410477698735, 'totalEpisodes': 38, 'stepsPerEpisode': 192, 'rewardPerEpisode': 140.27806537325873
'totalSteps': 6400, 'rewardStep': 0.8307555196213459, 'errorList': [], 'lossList': [0.0, -1.370416100025177, 0.0, 54.004088916778564, 0.0, 0.0, 0.0], 'rewardMean': 0.6976794860832571, 'totalEpisodes': 48, 'stepsPerEpisode': 208, 'rewardPerEpisode': 153.59434466426512
'totalSteps': 7680, 'rewardStep': 0.8592173205184526, 'errorList': [], 'lossList': [0.0, -1.3629507499933242, 0.0, 96.60667053222656, 0.0, 0.0, 0.0], 'rewardMean': 0.7246024584891231, 'totalEpisodes': 64, 'stepsPerEpisode': 70, 'rewardPerEpisode': 51.59926480332388
'totalSteps': 8960, 'rewardStep': 0.49963560722071804, 'errorList': [], 'lossList': [0.0, -1.3515596199035644, 0.0, 89.1958652305603, 0.0, 0.0, 0.0], 'rewardMean': 0.6924643368793509, 'totalEpisodes': 82, 'stepsPerEpisode': 97, 'rewardPerEpisode': 66.6862735733717
'totalSteps': 10240, 'rewardStep': 0.6137392188944838, 'errorList': [], 'lossList': [0.0, -1.340619398355484, 0.0, 62.10842337608337, 0.0, 0.0, 0.0], 'rewardMean': 0.6826236971312425, 'totalEpisodes': 95, 'stepsPerEpisode': 110, 'rewardPerEpisode': 87.47495766903866
'totalSteps': 11520, 'rewardStep': 0.8439237598657179, 'errorList': [], 'lossList': [0.0, -1.3341093438863754, 0.0, 33.9213671875, 0.0, 0.0, 0.0], 'rewardMean': 0.700545926323962, 'totalEpisodes': 101, 'stepsPerEpisode': 68, 'rewardPerEpisode': 60.67061417749956
'totalSteps': 12800, 'rewardStep': 0.7885271061076954, 'errorList': [], 'lossList': [0.0, -1.3200204664468764, 0.0, 39.683167390823364, 0.0, 0.0, 0.0], 'rewardMean': 0.7093440443023353, 'totalEpisodes': 107, 'stepsPerEpisode': 141, 'rewardPerEpisode': 116.37339956972278
'totalSteps': 14080, 'rewardStep': 0.603717303551053, 'errorList': [], 'lossList': [0.0, -1.3077236449718475, 0.0, 16.904385532140733, 0.0, 0.0, 0.0], 'rewardMean': 0.7086487647632557, 'totalEpisodes': 112, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.347882652825653
'totalSteps': 15360, 'rewardStep': 0.4271446114717047, 'errorList': [], 'lossList': [0.0, -1.3030905663967132, 0.0, 10.081753447055817, 0.0, 0.0, 0.0], 'rewardMean': 0.6689405926587971, 'totalEpisodes': 114, 'stepsPerEpisode': 375, 'rewardPerEpisode': 235.61596478473314
'totalSteps': 16640, 'rewardStep': 0.6070572807296948, 'errorList': [], 'lossList': [0.0, -1.2801430940628051, 0.0, 5.530709274411201, 0.0, 0.0, 0.0], 'rewardMean': 0.6720210172121831, 'totalEpisodes': 116, 'stepsPerEpisode': 735, 'rewardPerEpisode': 547.0909671623416
'totalSteps': 17920, 'rewardStep': 0.6028694987494694, 'errorList': [], 'lossList': [0.0, -1.2468837583065033, 0.0, 4.543061454296112, 0.0, 0.0, 0.0], 'rewardMean': 0.6676587226730335, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 778.2470561110905
'totalSteps': 19200, 'rewardStep': 0.6925998025616069, 'errorList': [], 'lossList': [0.0, -1.226873555779457, 0.0, 26.832099665403366, 0.0, 0.0, 0.0], 'rewardMean': 0.6538431509670597, 'totalEpisodes': 117, 'stepsPerEpisode': 819, 'rewardPerEpisode': 647.3822082701789
'totalSteps': 20480, 'rewardStep': 0.9549137766591401, 'errorList': [0.01055796288537187, 0.025806729618371965, 0.033647351184609896, 0.04116368436584718, 0.039505885917349805, 0.01624915452076363, 0.06302671617949104, 0.03330567733833783, 0.027213324611117418, 0.05387698323835504, 0.04810150289438161, 0.02704254098347271, 0.050576693099517234, 0.08412668670725973, 0.04344705686330242, 0.04542073385324968, 0.029226275955265325, 0.01270119721909455, 0.050112433318253186, 0.06022041925795074, 0.09524643628001969, 0.06998435320623426, 0.06715027513545714, 0.08419624604135874, 0.024768787793503808, 0.04760347424070573, 0.04117290920978107, 0.022778189722544712, 0.011022182151134383, 0.025469026361728125, 0.09072730206487704, 0.05186745040618733, 0.06778735010825122, 0.011342888513516098, 0.0715664779816016, 0.013548744927094665, 0.038881765754341865, 0.07873916864163419, 0.05870830777885173, 0.05306212028001401, 0.027625021455421776, 0.04328409128404739, 0.058876515797135, 0.06082038057154762, 0.06419223952872453, 0.027839290755770164, 0.045027098485260396, 0.048875970920720775, 0.015847864268341665, 0.0392470189926517], 'lossList': [0.0, -1.2285754227638244, 0.0, 3.041495860517025, 0.0, 0.0, 0.0], 'rewardMean': 0.6634127965811284, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.5610914042581, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=66.77
