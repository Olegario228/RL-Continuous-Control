#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 58
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.596191662621566, 'errorList': [], 'lossList': [0.0, -1.4149378085136413, 0.0, 29.54194637298584, 0.0, 0.0, 0.0], 'rewardMean': 0.4702734270835104, 'totalEpisodes': 18, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.23960706616413
'totalSteps': 3840, 'rewardStep': 0.5820944967165953, 'errorList': [], 'lossList': [0.0, -1.3993846333026887, 0.0, 39.91473955631256, 0.0, 0.0, 0.0], 'rewardMean': 0.5075471169612054, 'totalEpisodes': 27, 'stepsPerEpisode': 279, 'rewardPerEpisode': 197.9422019821315
'totalSteps': 5120, 'rewardStep': 0.7757720245594746, 'errorList': [], 'lossList': [0.0, -1.407080233693123, 0.0, 51.10148663520813, 0.0, 0.0, 0.0], 'rewardMean': 0.5746033438607727, 'totalEpisodes': 35, 'stepsPerEpisode': 73, 'rewardPerEpisode': 61.05928789179372
'totalSteps': 6400, 'rewardStep': 0.8906286471626732, 'errorList': [], 'lossList': [0.0, -1.4097927689552308, 0.0, 49.822685632705685, 0.0, 0.0, 0.0], 'rewardMean': 0.6378084045211528, 'totalEpisodes': 47, 'stepsPerEpisode': 24, 'rewardPerEpisode': 16.828468645081728
'totalSteps': 7680, 'rewardStep': 0.8372386334268471, 'errorList': [], 'lossList': [0.0, -1.4006614005565643, 0.0, 115.76627513885498, 0.0, 0.0, 0.0], 'rewardMean': 0.6710467760054352, 'totalEpisodes': 67, 'stepsPerEpisode': 58, 'rewardPerEpisode': 47.746209098268736
'totalSteps': 8960, 'rewardStep': 0.8477594392508336, 'errorList': [], 'lossList': [0.0, -1.3926164883375167, 0.0, 113.00860334396363, 0.0, 0.0, 0.0], 'rewardMean': 0.6962914421833493, 'totalEpisodes': 90, 'stepsPerEpisode': 73, 'rewardPerEpisode': 56.5149143443769
'totalSteps': 10240, 'rewardStep': 0.695537299917725, 'errorList': [], 'lossList': [0.0, -1.3777692979574203, 0.0, 44.40796877384186, 0.0, 0.0, 0.0], 'rewardMean': 0.6961971744001463, 'totalEpisodes': 98, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.3640696056698765
'totalSteps': 11520, 'rewardStep': 0.48892134044301283, 'errorList': [], 'lossList': [0.0, -1.3466865170001983, 0.0, 30.932473092079164, 0.0, 0.0, 0.0], 'rewardMean': 0.673166526182687, 'totalEpisodes': 103, 'stepsPerEpisode': 376, 'rewardPerEpisode': 301.8743127263843
'totalSteps': 12800, 'rewardStep': 0.7803239108238023, 'errorList': [], 'lossList': [0.0, -1.3323074078559876, 0.0, 24.841955099105835, 0.0, 0.0, 0.0], 'rewardMean': 0.6838822646467986, 'totalEpisodes': 109, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7803239108238023
'totalSteps': 14080, 'rewardStep': 0.8346370513014808, 'errorList': [], 'lossList': [0.0, -1.333194974064827, 0.0, 12.948411614894868, 0.0, 0.0, 0.0], 'rewardMean': 0.732910450622401, 'totalEpisodes': 114, 'stepsPerEpisode': 63, 'rewardPerEpisode': 45.87340741714627
'totalSteps': 15360, 'rewardStep': 0.669437078094089, 'errorList': [], 'lossList': [0.0, -1.3433648574352264, 0.0, 8.159717713594437, 0.0, 0.0, 0.0], 'rewardMean': 0.7402349921696534, 'totalEpisodes': 117, 'stepsPerEpisode': 146, 'rewardPerEpisode': 117.29578129106964
'totalSteps': 16640, 'rewardStep': 0.7964617985626604, 'errorList': [], 'lossList': [0.0, -1.3347555375099183, 0.0, 4.894149574041367, 0.0, 0.0, 0.0], 'rewardMean': 0.7616717223542598, 'totalEpisodes': 119, 'stepsPerEpisode': 230, 'rewardPerEpisode': 191.36661484737135
'totalSteps': 17920, 'rewardStep': 0.8422600932583941, 'errorList': [], 'lossList': [0.0, -1.3264516252279281, 0.0, 4.5388616740703585, 0.0, 0.0, 0.0], 'rewardMean': 0.768320529224152, 'totalEpisodes': 120, 'stepsPerEpisode': 417, 'rewardPerEpisode': 352.9256951595086
'totalSteps': 19200, 'rewardStep': 0.820014288056332, 'errorList': [], 'lossList': [0.0, -1.3015493232011794, 0.0, 5.554233326911926, 0.0, 0.0, 0.0], 'rewardMean': 0.7612590933135178, 'totalEpisodes': 121, 'stepsPerEpisode': 745, 'rewardPerEpisode': 552.2832000375483
'totalSteps': 20480, 'rewardStep': 0.8430598729964814, 'errorList': [], 'lossList': [0.0, -1.2933614683151244, 0.0, 1.8076098519563675, 0.0, 0.0, 0.0], 'rewardMean': 0.7618412172704812, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1026.2634488348297
'totalSteps': 21760, 'rewardStep': 0.8482371884554104, 'errorList': [], 'lossList': [0.0, -1.2828499954938888, 0.0, 0.8982267106324434, 0.0, 0.0, 0.0], 'rewardMean': 0.7618889921909389, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.8269025930695
'totalSteps': 23040, 'rewardStep': 0.7591853751549694, 'errorList': [], 'lossList': [0.0, -1.2602410972118379, 0.0, 0.9540080785751343, 0.0, 0.0, 0.0], 'rewardMean': 0.7682537997146633, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.7492477871813
'totalSteps': 24320, 'rewardStep': 0.8364459794356379, 'errorList': [], 'lossList': [0.0, -1.2154883986711502, 0.0, 1.1078112871013581, 0.0, 0.0, 0.0], 'rewardMean': 0.8030062636139258, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.177461694858
'totalSteps': 25600, 'rewardStep': 0.9831395661346405, 'errorList': [0.053615583438668686, 0.09793424305614719, 0.051758101462374966, 0.031199480585628908, 0.027788878199907923, 0.05834928999888932, 0.08423972970585242, 0.02699420006938641, 0.11777157060634212, 0.020127110176757584, 0.12796903651048916, 0.021789419542699837, 0.051675475480304066, 0.10122620343221078, 0.04447260583883767, 0.1092907271466619, 0.10546483029690902, 0.11245897687408869, 0.06548885111057648, 0.08064468640908803, 0.026320443584587988, 0.07291317014505379, 0.06128483023164451, 0.1183638534393103, 0.019092193388906994, 0.04607587179862315, 0.06568893863825652, 0.0588568359118378, 0.03843771025634845, 0.034024084148154214, 0.022241396047269272, 0.0777858322958823, 0.07435389983727053, 0.03437698624395245, 0.06838828405403795, 0.009249006455440344, 0.026700611133307458, 0.040048400594747666, 0.06999435882737562, 0.10000473608348869, 0.03177549732386649, 0.036439820293074646, 0.03927116767181568, 0.045676597441902465, 0.07154804890859123, 0.09055249407640524, 0.1105065809788463, 0.05324526483753827, 0.044086688164777885, 0.02501363047066978], 'lossList': [0.0, -1.1811030143499375, 0.0, 0.743679756782949, 0.0, 0.0, 0.0], 'rewardMean': 0.8232878291450095, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1178.1153561789106, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.0
