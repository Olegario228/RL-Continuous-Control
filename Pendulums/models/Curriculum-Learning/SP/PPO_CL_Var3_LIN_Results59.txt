#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 59
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8628216698809725, 'errorList': [], 'lossList': [0.0, -1.4152914518117905, 0.0, 25.748786602020264, 0.0, 0.0, 0.0], 'rewardMean': 0.6134458878282806, 'totalEpisodes': 22, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.700128485809703
'totalSteps': 3840, 'rewardStep': 0.7721518065035557, 'errorList': [], 'lossList': [0.0, -1.4225139379501344, 0.0, 35.3571387386322, 0.0, 0.0, 0.0], 'rewardMean': 0.6663478607200389, 'totalEpisodes': 36, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.075625429251104
'totalSteps': 5120, 'rewardStep': 0.6984891046073736, 'errorList': [], 'lossList': [0.0, -1.4141523891687393, 0.0, 31.69369204044342, 0.0, 0.0, 0.0], 'rewardMean': 0.6743831716918727, 'totalEpisodes': 45, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.728525044845604
'totalSteps': 6400, 'rewardStep': 0.6422985453547302, 'errorList': [], 'lossList': [0.0, -1.4031337875127792, 0.0, 39.08803608417511, 0.0, 0.0, 0.0], 'rewardMean': 0.6679662464244441, 'totalEpisodes': 50, 'stepsPerEpisode': 272, 'rewardPerEpisode': 229.77168761016986
'totalSteps': 7680, 'rewardStep': 0.8868533670820055, 'errorList': [], 'lossList': [0.0, -1.3905707478523255, 0.0, 116.11907382965087, 0.0, 0.0, 0.0], 'rewardMean': 0.7044474332007044, 'totalEpisodes': 68, 'stepsPerEpisode': 34, 'rewardPerEpisode': 24.983805172125876
'totalSteps': 8960, 'rewardStep': 0.6075199137230511, 'errorList': [], 'lossList': [0.0, -1.3957262867689133, 0.0, 73.32266389846802, 0.0, 0.0, 0.0], 'rewardMean': 0.6906006447038967, 'totalEpisodes': 87, 'stepsPerEpisode': 64, 'rewardPerEpisode': 48.024337326525234
'totalSteps': 10240, 'rewardStep': 0.773469409759404, 'errorList': [], 'lossList': [0.0, -1.3839784395694732, 0.0, 26.39553982257843, 0.0, 0.0, 0.0], 'rewardMean': 0.7009592403358351, 'totalEpisodes': 95, 'stepsPerEpisode': 130, 'rewardPerEpisode': 97.53821411211891
'totalSteps': 11520, 'rewardStep': 0.9237839178369921, 'errorList': [], 'lossList': [0.0, -1.3519777470827103, 0.0, 8.828735337257385, 0.0, 0.0, 0.0], 'rewardMean': 0.7257175378359637, 'totalEpisodes': 100, 'stepsPerEpisode': 35, 'rewardPerEpisode': 27.470579712898168
'totalSteps': 12800, 'rewardStep': 0.5212810412142417, 'errorList': [], 'lossList': [0.0, -1.3233295267820357, 0.0, 9.998930422067643, 0.0, 0.0, 0.0], 'rewardMean': 0.7052738881737916, 'totalEpisodes': 105, 'stepsPerEpisode': 230, 'rewardPerEpisode': 156.2571873781052
'totalSteps': 14080, 'rewardStep': 0.9533559051191913, 'errorList': [1.2876768436966144, 1.2090576952494454, 1.245164350371387, 1.1725775420056, 1.284617315679122, 1.381492247779858, 1.1807334499580984, 1.2248723853467056, 1.1951703330992196, 1.2637304459009475, 1.2513075193348209, 1.2661578195517516, 1.2427880441663621, 1.2370291502387247, 1.2229919848597814, 1.2879131206150132, 1.2535733860905707, 1.2022173485389565, 1.213928547153363, 1.2482159049533517, 1.369807686191767, 1.2670309480234843, 1.2280596634357919, 1.2203527584140228, 1.2595804316783277, 1.2765767947113011, 1.1681300328238244, 1.2434966219557004, 1.278026942105352, 1.2586218592526528, 1.244375499347897, 1.174067967927935, 1.2189955110424595, 1.2444081782090286, 1.19776364551269, 1.2903536909664135, 1.2350760991081642, 1.2575279528444918, 1.323803066665548, 1.3500495946049513, 1.2287129832016992, 1.242217861769022, 1.249237635429923, 1.2686745501899142, 1.2334089588680475, 1.2605245898857815, 1.1956608714358679, 1.246933778763842, 1.2706663768632878, 1.3107343335156931], 'lossList': [0.0, -1.3178436583280564, 0.0, 5.1608706721663475, 0.0, 0.0, 0.0], 'rewardMean': 0.7642024681081517, 'totalEpisodes': 110, 'stepsPerEpisode': 166, 'rewardPerEpisode': 144.02245735388988, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.6792568813327695, 'errorList': [], 'lossList': [0.0, -1.3327736020088197, 0.0, 14.690764011144639, 0.0, 0.0, 0.0], 'rewardMean': 0.7458459892533315, 'totalEpisodes': 113, 'stepsPerEpisode': 741, 'rewardPerEpisode': 548.9549931483169
'totalSteps': 16640, 'rewardStep': 0.6034327080621812, 'errorList': [], 'lossList': [0.0, -1.3200066030025481, 0.0, 11.341564425230025, 0.0, 0.0, 0.0], 'rewardMean': 0.7289740794091941, 'totalEpisodes': 114, 'stepsPerEpisode': 448, 'rewardPerEpisode': 343.3577565836668
'totalSteps': 17920, 'rewardStep': 0.778808966249138, 'errorList': [], 'lossList': [0.0, -1.2928296625614166, 0.0, 5.448515730500222, 0.0, 0.0, 0.0], 'rewardMean': 0.7370060655733706, 'totalEpisodes': 114, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1042.210979718607
'totalSteps': 19200, 'rewardStep': 0.9306529891719589, 'errorList': [0.06420149233418333, 0.05184620110022705, 0.0629821291567272, 0.05356249991235259, 0.053642803706238085, 0.05395973250934262, 0.054037645847257974, 0.059405658645144535, 0.05196044342391053, 0.057760240140949416, 0.05110287820146121, 0.05394398978534261, 0.05422329030397128, 0.051366383177583665, 0.05275446812552259, 0.05264850882154322, 0.05402061982118417, 0.05191914227467635, 0.05091318776721633, 0.05343702502256974, 0.059238490687611486, 0.06748134791304806, 0.06790200984142317, 0.05106573734832757, 0.054478055426910525, 0.052085318048949135, 0.05261774620956004, 0.052847450217496685, 0.059542696530166514, 0.053446655846954676, 0.0695344975830935, 0.049738496307953343, 0.053775215898053015, 0.05200451330415368, 0.0529901994158927, 0.05319233752492394, 0.053631722755655226, 0.052163555035291694, 0.055313262225192054, 0.05197822922262725, 0.05281678990104252, 0.05129606266912958, 0.05412334637114236, 0.051775583352165006, 0.05127588308339335, 0.05135428875585438, 0.06857118745131358, 0.05312147986944839, 0.06340296561555478, 0.06058483092948812], 'lossList': [0.0, -1.2422952193021775, 0.0, 3.4325846928358077, 0.0, 0.0, 0.0], 'rewardMean': 0.7658415099550934, 'totalEpisodes': 114, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.6858744107835, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=83.47
