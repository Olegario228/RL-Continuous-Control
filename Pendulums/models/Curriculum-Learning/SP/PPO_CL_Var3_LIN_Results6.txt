#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 6
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.692089982909304, 'errorList': [], 'lossList': [0.0, -1.415241801738739, 0.0, 30.136311235427858, 0.0, 0.0, 0.0], 'rewardMean': 0.8276897674100069, 'totalEpisodes': 25, 'stepsPerEpisode': 39, 'rewardPerEpisode': 35.883515643636265
'totalSteps': 3840, 'rewardStep': 0.7516665015940175, 'errorList': [], 'lossList': [0.0, -1.4093963253498076, 0.0, 44.37467406272888, 0.0, 0.0, 0.0], 'rewardMean': 0.8023486788046771, 'totalEpisodes': 45, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.823034016665327
'totalSteps': 5120, 'rewardStep': 0.6815571954080865, 'errorList': [], 'lossList': [0.0, -1.3973071783781053, 0.0, 57.27991667747497, 0.0, 0.0, 0.0], 'rewardMean': 0.7721508079555294, 'totalEpisodes': 64, 'stepsPerEpisode': 23, 'rewardPerEpisode': 15.277643408016132
'totalSteps': 6400, 'rewardStep': 0.6297762562293224, 'errorList': [], 'lossList': [0.0, -1.3896510690450667, 0.0, 82.32135400772094, 0.0, 0.0, 0.0], 'rewardMean': 0.743675897610288, 'totalEpisodes': 93, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.299876513192185
'totalSteps': 7680, 'rewardStep': 0.776582555987228, 'errorList': [], 'lossList': [0.0, -1.3750093543529511, 0.0, 19.938929991722105, 0.0, 0.0, 0.0], 'rewardMean': 0.7491603406731113, 'totalEpisodes': 102, 'stepsPerEpisode': 25, 'rewardPerEpisode': 17.801141190351835
'totalSteps': 8960, 'rewardStep': 0.5207850956192256, 'errorList': [], 'lossList': [0.0, -1.3536372965574264, 0.0, 30.843105516433717, 0.0, 0.0, 0.0], 'rewardMean': 0.7165353056654133, 'totalEpisodes': 113, 'stepsPerEpisode': 110, 'rewardPerEpisode': 81.06904594905393
'totalSteps': 10240, 'rewardStep': 0.6892554880022509, 'errorList': [], 'lossList': [0.0, -1.3418304640054703, 0.0, 17.858988879919053, 0.0, 0.0, 0.0], 'rewardMean': 0.7131253284575181, 'totalEpisodes': 120, 'stepsPerEpisode': 57, 'rewardPerEpisode': 43.83314216797495
'totalSteps': 11520, 'rewardStep': 0.7698371056575091, 'errorList': [], 'lossList': [0.0, -1.3449096441268922, 0.0, 8.784480258226395, 0.0, 0.0, 0.0], 'rewardMean': 0.7194266370352949, 'totalEpisodes': 124, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.47812308292973
'totalSteps': 12800, 'rewardStep': 0.7921918854638892, 'errorList': [], 'lossList': [0.0, -1.3529589772224426, 0.0, 16.538726532459258, 0.0, 0.0, 0.0], 'rewardMean': 0.7267031618781543, 'totalEpisodes': 130, 'stepsPerEpisode': 153, 'rewardPerEpisode': 130.4341006140149
'totalSteps': 14080, 'rewardStep': 0.6213302467051884, 'errorList': [], 'lossList': [0.0, -1.3415595191717147, 0.0, 17.31560644388199, 0.0, 0.0, 0.0], 'rewardMean': 0.6925072313576022, 'totalEpisodes': 134, 'stepsPerEpisode': 231, 'rewardPerEpisode': 175.04794570193553
'totalSteps': 15360, 'rewardStep': 0.7820716302301163, 'errorList': [], 'lossList': [0.0, -1.3192767769098281, 0.0, 5.736954196691513, 0.0, 0.0, 0.0], 'rewardMean': 0.7015053960896833, 'totalEpisodes': 139, 'stepsPerEpisode': 123, 'rewardPerEpisode': 104.5918127014628
'totalSteps': 16640, 'rewardStep': 0.8875982246230665, 'errorList': [], 'lossList': [0.0, -1.286960871219635, 0.0, 4.38455223441124, 0.0, 0.0, 0.0], 'rewardMean': 0.7150985683925882, 'totalEpisodes': 143, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.689284347299257
'totalSteps': 17920, 'rewardStep': 0.8463776235216305, 'errorList': [], 'lossList': [0.0, -1.2741286909580232, 0.0, 3.4996188509464266, 0.0, 0.0, 0.0], 'rewardMean': 0.7315806112039427, 'totalEpisodes': 145, 'stepsPerEpisode': 217, 'rewardPerEpisode': 191.45478464981085
'totalSteps': 19200, 'rewardStep': 0.7222335399870141, 'errorList': [], 'lossList': [0.0, -1.2648452180624008, 0.0, 3.2575665426254274, 0.0, 0.0, 0.0], 'rewardMean': 0.7408263395797119, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 920.1593525789273
'totalSteps': 20480, 'rewardStep': 0.5154461922766367, 'errorList': [], 'lossList': [0.0, -1.2384710997343062, 0.0, 2.5262762370705603, 0.0, 0.0, 0.0], 'rewardMean': 0.7147127032086527, 'totalEpisodes': 146, 'stepsPerEpisode': 378, 'rewardPerEpisode': 320.8719144852091
'totalSteps': 21760, 'rewardStep': 0.7517687789831043, 'errorList': [], 'lossList': [0.0, -1.226817376613617, 0.0, 2.8326721477508543, 0.0, 0.0, 0.0], 'rewardMean': 0.7378110715450406, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 971.240998184198
'totalSteps': 23040, 'rewardStep': 0.8767137945252764, 'errorList': [], 'lossList': [0.0, -1.1964954310655593, 0.0, 0.94483544126153, 0.0, 0.0, 0.0], 'rewardMean': 0.7565569021973431, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.4637365145434
'totalSteps': 24320, 'rewardStep': 0.8139951429634406, 'errorList': [], 'lossList': [0.0, -1.1472638380527496, 0.0, 0.6696844157576561, 0.0, 0.0, 0.0], 'rewardMean': 0.7609727059279363, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.4223685662616
'totalSteps': 25600, 'rewardStep': 0.9872970625025324, 'errorList': [0.0537498839056367, 0.0613664374886619, 0.07306353894168788, 0.053794440360425966, 0.060753420996106776, 0.06807096302357683, 0.068949564864916, 0.07881063143856785, 0.0732612949032911, 0.07218815822836674, 0.06071403768124849, 0.07091317289238513, 0.07089391104787612, 0.06862320706186581, 0.06463671892672698, 0.06813810157377286, 0.05895611876245532, 0.0627398619376148, 0.08998982054004814, 0.055384216370009905, 0.0819621946285825, 0.06570013874757914, 0.06861659618903113, 0.07316564070433948, 0.06621502449629373, 0.07427850697225918, 0.0939948767321609, 0.0843891399425305, 0.0654664723320244, 0.06516485878270495, 0.07307404704248381, 0.053645926775457556, 0.05719513012670146, 0.06322034798143741, 0.08319435659106253, 0.06992684456151818, 0.08089169290875466, 0.05613105480578104, 0.07727443014803909, 0.07158581656850449, 0.06803460791473163, 0.06694410492477305, 0.06708232440926998, 0.08456676131444948, 0.07436528493055683, 0.058279161122559174, 0.06591191397558367, 0.0755794288569348, 0.08379109927161374, 0.06208707101834704], 'lossList': [0.0, -1.0780537045001983, 0.0, 0.6148095934838057, 0.0, 0.0, 0.0], 'rewardMean': 0.7804832236318007, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1198.428737939482, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=68.16
