#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 60
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.8982062736906329, 'errorList': [], 'lossList': [0.0, -1.4142334181070328, 0.0, 23.9292578291893, 0.0, 0.0, 0.0], 'rewardMean': 0.849002617094798, 'totalEpisodes': 8, 'stepsPerEpisode': 537, 'rewardPerEpisode': 369.2641013949357
'totalSteps': 3840, 'rewardStep': 0.45674650766516756, 'errorList': [], 'lossList': [0.0, -1.4216842007637025, 0.0, 44.0606484413147, 0.0, 0.0, 0.0], 'rewardMean': 0.7182505806182545, 'totalEpisodes': 17, 'stepsPerEpisode': 154, 'rewardPerEpisode': 114.66577090455792
'totalSteps': 5120, 'rewardStep': 0.7749802074350883, 'errorList': [], 'lossList': [0.0, -1.4195291876792908, 0.0, 32.666750581264495, 0.0, 0.0, 0.0], 'rewardMean': 0.732432987322463, 'totalEpisodes': 21, 'stepsPerEpisode': 192, 'rewardPerEpisode': 147.94035867810834
'totalSteps': 6400, 'rewardStep': 0.6645048190691228, 'errorList': [], 'lossList': [0.0, -1.4232883530855178, 0.0, 50.805412325859066, 0.0, 0.0, 0.0], 'rewardMean': 0.718847353671795, 'totalEpisodes': 27, 'stepsPerEpisode': 103, 'rewardPerEpisode': 83.74792235786012
'totalSteps': 7680, 'rewardStep': 0.4793525530674406, 'errorList': [], 'lossList': [0.0, -1.418601513504982, 0.0, 117.75512645721436, 0.0, 0.0, 0.0], 'rewardMean': 0.6789315535710693, 'totalEpisodes': 43, 'stepsPerEpisode': 10, 'rewardPerEpisode': 5.932900733197943
'totalSteps': 8960, 'rewardStep': 0.6249290320665948, 'errorList': [], 'lossList': [0.0, -1.407692579627037, 0.0, 135.52445930480957, 0.0, 0.0, 0.0], 'rewardMean': 0.6712169076418586, 'totalEpisodes': 82, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.883659636235684
'totalSteps': 10240, 'rewardStep': 0.8708362188032646, 'errorList': [], 'lossList': [0.0, -1.402494017481804, 0.0, 87.83508359909058, 0.0, 0.0, 0.0], 'rewardMean': 0.6961693215370344, 'totalEpisodes': 108, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.197976955183607
'totalSteps': 11520, 'rewardStep': 0.7736875709865048, 'errorList': [], 'lossList': [0.0, -1.3930973744392394, 0.0, 30.67118149280548, 0.0, 0.0, 0.0], 'rewardMean': 0.7047824603647533, 'totalEpisodes': 117, 'stepsPerEpisode': 68, 'rewardPerEpisode': 59.83917430827601
'totalSteps': 12800, 'rewardStep': 0.6209176209099769, 'errorList': [], 'lossList': [0.0, -1.373722692131996, 0.0, 25.358185200691224, 0.0, 0.0, 0.0], 'rewardMean': 0.6963959764192758, 'totalEpisodes': 122, 'stepsPerEpisode': 122, 'rewardPerEpisode': 79.12197330527476
'totalSteps': 14080, 'rewardStep': 0.6216341143406229, 'errorList': [], 'lossList': [0.0, -1.3679273909330367, 0.0, 16.144599273204804, 0.0, 0.0, 0.0], 'rewardMean': 0.6785794918034417, 'totalEpisodes': 124, 'stepsPerEpisode': 607, 'rewardPerEpisode': 485.3093421455587
'totalSteps': 15360, 'rewardStep': 0.41423761484308497, 'errorList': [], 'lossList': [0.0, -1.3569921058416368, 0.0, 21.79498008966446, 0.0, 0.0, 0.0], 'rewardMean': 0.6301826259186868, 'totalEpisodes': 127, 'stepsPerEpisode': 687, 'rewardPerEpisode': 490.63204354968394
'totalSteps': 16640, 'rewardStep': 0.6594955552809918, 'errorList': [], 'lossList': [0.0, -1.3421144038438797, 0.0, 4.819969632327557, 0.0, 0.0, 0.0], 'rewardMean': 0.6504575306802691, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 935.8064974750963
'totalSteps': 17920, 'rewardStep': 0.8843358019473552, 'errorList': [], 'lossList': [0.0, -1.3189753097295762, 0.0, 4.291732488572597, 0.0, 0.0, 0.0], 'rewardMean': 0.6613930901314958, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.5226667629893
'totalSteps': 19200, 'rewardStep': 0.863810540781043, 'errorList': [], 'lossList': [0.0, -1.2747692227363587, 0.0, 2.915475871041417, 0.0, 0.0, 0.0], 'rewardMean': 0.6813236623026879, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.4850534862214
'totalSteps': 20480, 'rewardStep': 0.7977888866294334, 'errorList': [], 'lossList': [0.0, -1.2494836020469666, 0.0, 1.3027157183736562, 0.0, 0.0, 0.0], 'rewardMean': 0.7131672956588873, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.022290865012
'totalSteps': 21760, 'rewardStep': 0.8513988598837282, 'errorList': [], 'lossList': [0.0, -1.23198655128479, 0.0, 0.8818159720301628, 0.0, 0.0, 0.0], 'rewardMean': 0.7358142784406005, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.6729506946704
'totalSteps': 23040, 'rewardStep': 0.8092373310139344, 'errorList': [], 'lossList': [0.0, -1.1964290392398835, 0.0, 0.8918607478961349, 0.0, 0.0, 0.0], 'rewardMean': 0.7296543896616675, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.5809923920638
'totalSteps': 24320, 'rewardStep': 0.7695790110764196, 'errorList': [], 'lossList': [0.0, -1.1548564946651458, 0.0, 0.808385563865304, 0.0, 0.0, 0.0], 'rewardMean': 0.7292435336706591, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.3712612350344
'totalSteps': 25600, 'rewardStep': 0.9315689428264273, 'errorList': [0.1464284919285412, 0.21207002565182764, 0.17142528931531612, 0.2107316905136504, 0.16784922491680757, 0.13783871380459284, 0.16115458647785474, 0.18133875257880847, 0.10405035330732354, 0.18389867329702167, 0.103310816193844, 0.13884127448970962, 0.1789969675155236, 0.15137349333675887, 0.12086010073892384, 0.14173605000297018, 0.16036820301686816, 0.16958050856579215, 0.2075164972343001, 0.11746763522872561, 0.21153748279930137, 0.1957018981967762, 0.21819246203908693, 0.16394604073272737, 0.10654536575765029, 0.1877906297020268, 0.11908703238118101, 0.2093065444293396, 0.1846995181637418, 0.13255320860160077, 0.1474191864288273, 0.17336449239280805, 0.21011117553480793, 0.1927416738342175, 0.12231124862637902, 0.19741589135911017, 0.12796173804047314, 0.158489369446313, 0.1962530465293564, 0.1935073667104018, 0.10298011410760591, 0.15733844100767233, 0.16070878411355266, 0.13334698006662982, 0.18852894203672393, 0.16577350593578263, 0.19369993777088554, 0.13704352038153433, 0.17223923181647793, 0.17825416136197297], 'lossList': [0.0, -1.1141542816162109, 0.0, 1.0316163112409413, 0.0, 0.0, 0.0], 'rewardMean': 0.760308665862304, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.634722411689, 'successfulTests': 43
#maxSuccessfulTests=43, maxSuccessfulTestsAtStep=25600, timeSpent=80.68
