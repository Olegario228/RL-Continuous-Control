#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 61
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7265897572310046, 'errorList': [], 'lossList': [0.0, -1.4357337480783463, 0.0, 32.5626208114624, 0.0, 0.0, 0.0], 'rewardMean': 0.6216918294652287, 'totalEpisodes': 14, 'stepsPerEpisode': 90, 'rewardPerEpisode': 73.7756365301455
'totalSteps': 3840, 'rewardStep': 0.9192032547338116, 'errorList': [], 'lossList': [0.0, -1.4469725835323333, 0.0, 33.33750340700149, 0.0, 0.0, 0.0], 'rewardMean': 0.7208623045547563, 'totalEpisodes': 19, 'stepsPerEpisode': 380, 'rewardPerEpisode': 275.9504637632822
'totalSteps': 5120, 'rewardStep': 0.7975599460496599, 'errorList': [], 'lossList': [0.0, -1.4392276948690415, 0.0, 37.272658972740174, 0.0, 0.0, 0.0], 'rewardMean': 0.7400367149284821, 'totalEpisodes': 26, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.014247182809486
'totalSteps': 6400, 'rewardStep': 0.9144102050104816, 'errorList': [], 'lossList': [0.0, -1.423369659781456, 0.0, 63.500356130599975, 0.0, 0.0, 0.0], 'rewardMean': 0.774911412944882, 'totalEpisodes': 36, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.073431075509523
'totalSteps': 7680, 'rewardStep': 0.9335417531630333, 'errorList': [331.74099330939856, 278.8013134612439, 332.55910826150387, 339.1277166888897, 305.1510979475315, 322.15200527403516, 329.6426488527848, 320.79168661365316, 176.92559362953756, 254.6613685724055, 332.70276122137125, 322.8823259979066, 261.6782133576083, 363.16525551995215, 335.7965031346192, 358.23393112789233, 291.15966654553625, 348.07367284313204, 302.22967306120944, 347.83986970943926, 328.379344129894, 347.1708113235051, 360.98647884854563, 255.35866571759834, 318.6987325336821, 320.7744703320687, 340.8033954366199, 304.6750819939328, 327.91961845071546, 318.04056518613197, 297.97319752202554, 334.5400480890539, 331.5713502850504, 309.57978720162095, 314.30293835813023, 359.99112411208966, 361.60811599291634, 273.3470195377318, 344.7496478721685, 328.5776918875641, 365.77275167212616, 296.6467428550224, 342.55389883910146, 331.29045841630847, 319.43670980954124, 303.22371797685696, 349.6012088632626, 329.6699382474011, 323.2531275342111, 271.73211032786685], 'lossList': [0.0, -1.389498165845871, 0.0, 103.42644580841065, 0.0, 0.0, 0.0], 'rewardMean': 0.8013498029812406, 'totalEpisodes': 46, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.8514154179674356, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.8439203814037215, 'errorList': [], 'lossList': [0.0, -1.3690601271390914, 0.0, 161.2476982498169, 0.0, 0.0, 0.0], 'rewardMean': 0.8074313141844521, 'totalEpisodes': 79, 'stepsPerEpisode': 60, 'rewardPerEpisode': 48.27049788275568
'totalSteps': 10240, 'rewardStep': 0.7222502001425097, 'errorList': [], 'lossList': [0.0, -1.3564958238601685, 0.0, 53.36766501426697, 0.0, 0.0, 0.0], 'rewardMean': 0.7967836749292094, 'totalEpisodes': 94, 'stepsPerEpisode': 56, 'rewardPerEpisode': 46.35005581002053
'totalSteps': 11520, 'rewardStep': 0.565144476579289, 'errorList': [], 'lossList': [0.0, -1.3502144491672516, 0.0, 31.066222076416015, 0.0, 0.0, 0.0], 'rewardMean': 0.7710459862236627, 'totalEpisodes': 104, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.986032464570325
'totalSteps': 12800, 'rewardStep': 0.609272772247929, 'errorList': [], 'lossList': [0.0, -1.3321931946277619, 0.0, 33.82993832111359, 0.0, 0.0, 0.0], 'rewardMean': 0.7548686648260894, 'totalEpisodes': 112, 'stepsPerEpisode': 180, 'rewardPerEpisode': 135.6679796843918
'totalSteps': 14080, 'rewardStep': 0.5881137740963732, 'errorList': [], 'lossList': [0.0, -1.3108779805898667, 0.0, 10.321490572690964, 0.0, 0.0, 0.0], 'rewardMean': 0.7620006520657813, 'totalEpisodes': 116, 'stepsPerEpisode': 428, 'rewardPerEpisode': 319.1482874272841
'totalSteps': 15360, 'rewardStep': 0.525265589015661, 'errorList': [], 'lossList': [0.0, -1.3135939675569535, 0.0, 7.349661127328873, 0.0, 0.0, 0.0], 'rewardMean': 0.741868235244247, 'totalEpisodes': 118, 'stepsPerEpisode': 525, 'rewardPerEpisode': 363.1302858115022
'totalSteps': 16640, 'rewardStep': 0.8603872914198489, 'errorList': [], 'lossList': [0.0, -1.2949509400129318, 0.0, 6.56055990934372, 0.0, 0.0, 0.0], 'rewardMean': 0.7359866389128508, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 854.246517368847
'totalSteps': 17920, 'rewardStep': 0.821093663512155, 'errorList': [], 'lossList': [0.0, -1.235781661272049, 0.0, 4.51171074911952, 0.0, 0.0, 0.0], 'rewardMean': 0.7383400106591003, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.1799003919434
'totalSteps': 19200, 'rewardStep': 0.9518233853312978, 'errorList': [0.01758568586055176, 0.04076103497840457, 0.052342500388439725, 0.008099025825807812, 0.019969962407857617, 0.016072129705831845, 0.02109874237549949, 0.061702345803221176, 0.03359342325872535, 0.05585643734823154, 0.03630413395804307, 0.020214357571610816, 0.017954224719343614, 0.044207069264820875, 0.022133838507378614, 0.02672710169966848, 0.022928748182320234, 0.011306704003641753, 0.044617264875573, 0.02241691509723018, 0.029871861487409196, 0.0668858216203057, 0.023128577025907295, 0.009138375186997873, 0.011102326747108995, 0.039607854640942394, 0.04066983299001416, 0.021432785459347896, 0.03620237731896022, 0.021880974292319774, 0.01286621655577083, 0.016859963714449016, 0.01645552686349333, 0.03772996428647767, 0.029875539411980213, 0.044971367739620204, 0.016772785931415967, 0.004831785629347387, 0.019408845208713808, 0.022591489786126007, 0.025763156889866717, 0.02884732681438267, 0.01391330537579894, 0.009588938951176807, 0.007603746305528485, 0.051368815021904955, 0.03489902365831494, 0.020394090477830768, 0.01730172764782662, 0.014513258914734121], 'lossList': [0.0, -1.1877282512187959, 0.0, 3.3872958562523126, 0.0, 0.0, 0.0], 'rewardMean': 0.7420813286911818, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.3336733640724, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=85.39
