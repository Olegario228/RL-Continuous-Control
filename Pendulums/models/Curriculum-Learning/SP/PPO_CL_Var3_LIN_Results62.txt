#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 62
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.7472919170206407, 'errorList': [], 'lossList': [0.0, -1.4391621106863022, 0.0, 27.066994273662566, 0.0, 0.0, 0.0], 'rewardMean': 0.5926612546132574, 'totalEpisodes': 13, 'stepsPerEpisode': 716, 'rewardPerEpisode': 472.4465487198959
'totalSteps': 3840, 'rewardStep': 0.8879390886160274, 'errorList': [], 'lossList': [0.0, -1.4396784174442292, 0.0, 28.87793251276016, 0.0, 0.0, 0.0], 'rewardMean': 0.6910871992808474, 'totalEpisodes': 19, 'stepsPerEpisode': 78, 'rewardPerEpisode': 57.970024498292084
'totalSteps': 5120, 'rewardStep': 0.3112551305326742, 'errorList': [], 'lossList': [0.0, -1.415574254989624, 0.0, 46.975132894515994, 0.0, 0.0, 0.0], 'rewardMean': 0.596129182093804, 'totalEpisodes': 30, 'stepsPerEpisode': 68, 'rewardPerEpisode': 33.70583041189159
'totalSteps': 6400, 'rewardStep': 0.7763048373894064, 'errorList': [], 'lossList': [0.0, -1.3942707473039626, 0.0, 80.94526870727539, 0.0, 0.0, 0.0], 'rewardMean': 0.6321643131529245, 'totalEpisodes': 43, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.9855583718926963
'totalSteps': 7680, 'rewardStep': 0.733215136504546, 'errorList': [], 'lossList': [0.0, -1.3739155608415603, 0.0, 122.95217067718505, 0.0, 0.0, 0.0], 'rewardMean': 0.6490061170448614, 'totalEpisodes': 65, 'stepsPerEpisode': 56, 'rewardPerEpisode': 40.47762103048284
'totalSteps': 8960, 'rewardStep': 0.6524150948637001, 'errorList': [], 'lossList': [0.0, -1.3574357169866562, 0.0, 121.1781143951416, 0.0, 0.0, 0.0], 'rewardMean': 0.6494931138761241, 'totalEpisodes': 94, 'stepsPerEpisode': 102, 'rewardPerEpisode': 73.86922321889192
'totalSteps': 10240, 'rewardStep': 0.8708771014981127, 'errorList': [], 'lossList': [0.0, -1.3546194714307784, 0.0, 47.18574186325073, 0.0, 0.0, 0.0], 'rewardMean': 0.6771661123288727, 'totalEpisodes': 109, 'stepsPerEpisode': 55, 'rewardPerEpisode': 42.93367906556662
'totalSteps': 11520, 'rewardStep': 0.41809425879805395, 'errorList': [], 'lossList': [0.0, -1.3615513283014298, 0.0, 41.314119024276735, 0.0, 0.0, 0.0], 'rewardMean': 0.6483803508254484, 'totalEpisodes': 122, 'stepsPerEpisode': 92, 'rewardPerEpisode': 58.2675339243139
'totalSteps': 12800, 'rewardStep': 0.7039816957197531, 'errorList': [], 'lossList': [0.0, -1.3454075580835343, 0.0, 24.624970855712892, 0.0, 0.0, 0.0], 'rewardMean': 0.6539404853148788, 'totalEpisodes': 129, 'stepsPerEpisode': 150, 'rewardPerEpisode': 110.42400052605191
'totalSteps': 14080, 'rewardStep': 0.6750194023741872, 'errorList': [], 'lossList': [0.0, -1.3193521177768708, 0.0, 19.21577177286148, 0.0, 0.0, 0.0], 'rewardMean': 0.6776393663317101, 'totalEpisodes': 135, 'stepsPerEpisode': 41, 'rewardPerEpisode': 27.251438505896683
'totalSteps': 15360, 'rewardStep': 0.8626163143470444, 'errorList': [], 'lossList': [0.0, -1.3120080345869065, 0.0, 20.84121250629425, 0.0, 0.0, 0.0], 'rewardMean': 0.6891718060643506, 'totalEpisodes': 138, 'stepsPerEpisode': 90, 'rewardPerEpisode': 73.18166736190385
'totalSteps': 16640, 'rewardStep': 0.6727318183753617, 'errorList': [], 'lossList': [0.0, -1.2911209213733672, 0.0, 8.228279743790626, 0.0, 0.0, 0.0], 'rewardMean': 0.667651079040284, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 935.9169254479638
'totalSteps': 17920, 'rewardStep': 0.8481599516271346, 'errorList': [], 'lossList': [0.0, -1.2427158731222152, 0.0, 5.281597377955913, 0.0, 0.0, 0.0], 'rewardMean': 0.72134156114973, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.9993333442003
'totalSteps': 19200, 'rewardStep': 0.9096573257252162, 'errorList': [], 'lossList': [0.0, -1.199431893825531, 0.0, 4.882396459132433, 0.0, 0.0, 0.0], 'rewardMean': 0.7346768099833109, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.9459085022384
'totalSteps': 20480, 'rewardStep': 0.9764101292872491, 'errorList': [0.041616738935904014, 0.04169159587136482, 0.047319890491685224, 0.016087576519361803, 0.040192709706445506, 0.025902902979358853, 0.024044956844215513, 0.020595852243278485, 0.026795737859347447, 0.025031704357413406, 0.02990329619807637, 0.016323995301071034, 0.03583096636511919, 0.029845274639730895, 0.032093105716343315, 0.020983501890730075, 0.026629383626435715, 0.017871844675843962, 0.025795896417888566, 0.0364926711789887, 0.016430949837156653, 0.025017545422037265, 0.05260203908605453, 0.025485339997062654, 0.015915074841056132, 0.05705588690418393, 0.029857170975969367, 0.02081270859606118, 0.016196830343891803, 0.01741118604327979, 0.03645702973544332, 0.031645444207287335, 0.026192668480564344, 0.03525639648460381, 0.01733792752979607, 0.01632275084986408, 0.02682744297436255, 0.027453979681825346, 0.02669491086103226, 0.019724466931799387, 0.027186842768984632, 0.019317439611868357, 0.04467793514085703, 0.01640305022490027, 0.015812524280840647, 0.01600940415052712, 0.025813488649000807, 0.030096832482529326, 0.021161676162917288, 0.028960206538576755], 'lossList': [0.0, -1.1423154073953627, 0.0, 4.2904636088758705, 0.0, 0.0, 0.0], 'rewardMean': 0.7589963092615812, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.6416278314705, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=66.81
