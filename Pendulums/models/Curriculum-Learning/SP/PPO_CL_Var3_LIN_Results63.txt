#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 63
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.5495366801434844, 'errorList': [], 'lossList': [0.0, -1.4193675100803376, 0.0, 23.143793524503707, 0.0, 0.0, 0.0], 'rewardMean': 0.6574312565452702, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 29.20948691007733
'totalSteps': 3840, 'rewardStep': 0.8827336765908789, 'errorList': [], 'lossList': [0.0, -1.4034188669919967, 0.0, 32.34538039922714, 0.0, 0.0, 0.0], 'rewardMean': 0.7325320632271398, 'totalEpisodes': 20, 'stepsPerEpisode': 378, 'rewardPerEpisode': 266.7463759913915
'totalSteps': 5120, 'rewardStep': 0.8274479768728289, 'errorList': [], 'lossList': [0.0, -1.390928900241852, 0.0, 43.84643132686615, 0.0, 0.0, 0.0], 'rewardMean': 0.756261041638562, 'totalEpisodes': 25, 'stepsPerEpisode': 77, 'rewardPerEpisode': 69.2252669359756
'totalSteps': 6400, 'rewardStep': 0.7063347764751449, 'errorList': [], 'lossList': [0.0, -1.39201105594635, 0.0, 57.70133472919464, 0.0, 0.0, 0.0], 'rewardMean': 0.7462757886058785, 'totalEpisodes': 35, 'stepsPerEpisode': 18, 'rewardPerEpisode': 11.760434955831734
'totalSteps': 7680, 'rewardStep': 0.903792076386571, 'errorList': [], 'lossList': [0.0, -1.3844482672214509, 0.0, 132.30572425842286, 0.0, 0.0, 0.0], 'rewardMean': 0.772528503235994, 'totalEpisodes': 55, 'stepsPerEpisode': 52, 'rewardPerEpisode': 44.89241337540858
'totalSteps': 8960, 'rewardStep': 0.9680865124732009, 'errorList': [335.59840874268207, 178.42100012111044, 232.96641351254314, 341.0253020029296, 200.92990478803245, 221.46214979255268, 253.9329834370137, 258.45092330365657, 293.35761071807696, 189.87376201945716, 229.78561868813892, 277.77302816480886, 226.89000082921214, 264.4094544790701, 312.8121991746932, 196.45968036539318, 173.74859243340165, 273.80828138443894, 255.5809988433019, 240.39962712019258, 241.28576214568352, 301.4007799154467, 303.86614248837594, 266.444016727574, 263.5115044990591, 257.224296037662, 269.04174754148664, 187.0031817217097, 215.47976626824476, 299.4987389357787, 42.90077340740252, 245.2282091093211, 239.71065977620418, 297.35588302715814, 263.4875686953384, 268.7655809118762, 301.63781721926114, 274.40977700818536, 233.36343378974038, 294.7163030088439, 253.6277307265252, 171.89124643337257, 245.6673615866668, 309.9848074168507, 274.8690452063751, 306.32677664172115, 243.50976126202696, 300.2806590198439, 344.35675055872383, 280.16234062758843], 'lossList': [0.0, -1.378744506239891, 0.0, 140.14790245056153, 0.0, 0.0, 0.0], 'rewardMean': 0.8004653616984522, 'totalEpisodes': 86, 'stepsPerEpisode': 48, 'rewardPerEpisode': 42.75302900258069, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.4887053937672992, 'errorList': [], 'lossList': [0.0, -1.3711951875686645, 0.0, 59.02337100028992, 0.0, 0.0, 0.0], 'rewardMean': 0.761495365707058, 'totalEpisodes': 109, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.6193350071886408
'totalSteps': 11520, 'rewardStep': 0.6195191707832537, 'errorList': [], 'lossList': [0.0, -1.3647999584674835, 0.0, 32.86213797092438, 0.0, 0.0, 0.0], 'rewardMean': 0.7457202329377464, 'totalEpisodes': 118, 'stepsPerEpisode': 90, 'rewardPerEpisode': 74.41015872205134
'totalSteps': 12800, 'rewardStep': 0.8008665343104169, 'errorList': [], 'lossList': [0.0, -1.35404940366745, 0.0, 13.863590364456178, 0.0, 0.0, 0.0], 'rewardMean': 0.7512348630750134, 'totalEpisodes': 125, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.03711082753627
'totalSteps': 14080, 'rewardStep': 0.5872970541038449, 'errorList': [], 'lossList': [0.0, -1.3299695503711702, 0.0, 14.054699378013611, 0.0, 0.0, 0.0], 'rewardMean': 0.7334319851906923, 'totalEpisodes': 127, 'stepsPerEpisode': 220, 'rewardPerEpisode': 130.91437509827233
'totalSteps': 15360, 'rewardStep': 0.5610813110521928, 'errorList': [], 'lossList': [0.0, -1.3110646164417268, 0.0, 6.363488301038742, 0.0, 0.0, 0.0], 'rewardMean': 0.734586448281563, 'totalEpisodes': 128, 'stepsPerEpisode': 539, 'rewardPerEpisode': 362.57926119294444
'totalSteps': 16640, 'rewardStep': 0.8005300013667597, 'errorList': [], 'lossList': [0.0, -1.2944339883327485, 0.0, 5.459100580364466, 0.0, 0.0, 0.0], 'rewardMean': 0.7263660807591512, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.9289033350506
'totalSteps': 17920, 'rewardStep': 0.8559477953922248, 'errorList': [], 'lossList': [0.0, -1.2738715648651122, 0.0, 6.096884363293648, 0.0, 0.0, 0.0], 'rewardMean': 0.7292160626110908, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.3852527723604
'totalSteps': 19200, 'rewardStep': 0.8425737111868922, 'errorList': [], 'lossList': [0.0, -1.2484470754861832, 0.0, 4.615406331345439, 0.0, 0.0, 0.0], 'rewardMean': 0.7428399560822656, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.3991359839488
'totalSteps': 20480, 'rewardStep': 0.9574886606523071, 'errorList': [0.16855953928235967, 0.09173938397926966, 0.1685785006678848, 0.12947853979436177, 0.1754796233821163, 0.10856660261121254, 0.1434984139244887, 0.11667345141825722, 0.1162113141664939, 0.13630965071231202, 0.13963206399959924, 0.11874234717432046, 0.11040206355140295, 0.12375908396779813, 0.11262232519185324, 0.10486701564920532, 0.1275905097672311, 0.1459899556706242, 0.1301237754912865, 0.1670395863144955, 0.1321941239829985, 0.12679653612555747, 0.2581918101914037, 0.12878761454812582, 0.10560198800431152, 0.10320081186930251, 0.1299186717149034, 0.1377429452421416, 0.1396838177156671, 0.14147646933553715, 0.1032306144816755, 0.14208654358015704, 0.1598438118881449, 0.12519132695743976, 0.16346292143415944, 0.1299029371326808, 0.1418396768018789, 0.16698804741045048, 0.10950632394247387, 0.1171760144314209, 0.11052884879790639, 0.18509555480173415, 0.12150078275734129, 0.12559735032214647, 0.13631255361037475, 0.19525530796358145, 0.09606657143712298, 0.1124138431213322, 0.149483643852974, 0.13198490809782804], 'lossList': [0.0, -1.2156786477565766, 0.0, 3.710065807700157, 0.0, 0.0, 0.0], 'rewardMean': 0.7482096145088392, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.4117794144813, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.923241103466569, 'errorList': [], 'lossList': [0.0, -1.1846894419193268, 0.0, 2.285207353234291, 0.0, 0.0, 0.0], 'rewardMean': 0.743725073608176, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.9582326146717
'totalSteps': 23040, 'rewardStep': 0.8708374304923336, 'errorList': [], 'lossList': [0.0, -1.173693066239357, 0.0, 2.07877908796072, 0.0, 0.0, 0.0], 'rewardMean': 0.7819382772806793, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1195.6944406960547
'totalSteps': 24320, 'rewardStep': 0.8884228112536674, 'errorList': [], 'lossList': [0.0, -1.137325531244278, 0.0, 1.072761277295649, 0.0, 0.0, 0.0], 'rewardMean': 0.8088286413277208, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.1725459961358
'totalSteps': 25600, 'rewardStep': 0.8590027041339698, 'errorList': [], 'lossList': [0.0, -1.1132155179977417, 0.0, 0.588796295709908, 0.0, 0.0, 0.0], 'rewardMean': 0.814642258310076, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.3037726359212
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=20480, timeSpent=95.76
