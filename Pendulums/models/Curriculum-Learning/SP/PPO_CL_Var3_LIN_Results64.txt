#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 64
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.9277072359794358, 'errorList': [], 'lossList': [0.0, -1.4168395739793778, 0.0, 31.289864168167114, 0.0, 0.0, 0.0], 'rewardMean': 0.7998113210613895, 'totalEpisodes': 15, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.390213901415144
'totalSteps': 3840, 'rewardStep': 0.5876131014564355, 'errorList': [], 'lossList': [0.0, -1.422229253053665, 0.0, 23.053666911125184, 0.0, 0.0, 0.0], 'rewardMean': 0.7290785811930715, 'totalEpisodes': 18, 'stepsPerEpisode': 185, 'rewardPerEpisode': 132.35490894417146
'totalSteps': 5120, 'rewardStep': 0.5455389286637197, 'errorList': [], 'lossList': [0.0, -1.421178035736084, 0.0, 34.97685368061066, 0.0, 0.0, 0.0], 'rewardMean': 0.6831936680607336, 'totalEpisodes': 22, 'stepsPerEpisode': 236, 'rewardPerEpisode': 187.70796728607553
'totalSteps': 6400, 'rewardStep': 0.5595834581730315, 'errorList': [], 'lossList': [0.0, -1.4056514817476273, 0.0, 64.24430232524873, 0.0, 0.0, 0.0], 'rewardMean': 0.6584716260831931, 'totalEpisodes': 29, 'stepsPerEpisode': 350, 'rewardPerEpisode': 249.01879959686917
'totalSteps': 7680, 'rewardStep': 0.9448395854142141, 'errorList': [225.0894598657202, 246.07654649814089, 246.69212976193262, 275.9587100595497, 276.644861525822, 302.09833834285223, 282.53141923172194, 246.74817598929766, 290.2324390224552, 219.93262358516915, 242.97665402797765, 297.58606326692245, 305.3229129007092, 294.4333922933894, 300.5537953843668, 309.11912617765023, 304.2204905615666, 213.9767052590785, 260.5059167801211, 281.50055414024337, 266.9961684326487, 290.78766311008764, 285.74188004706133, 281.6125572499052, 281.1528394119851, 269.629599196293, 294.1859894918479, 261.96774347892665, 267.1709747445531, 213.30492749407554, 266.89987945529464, 269.25967376499113, 282.21116567281655, 216.00260004178782, 289.9759187601778, 226.6493145615983, 227.08535552987894, 158.52054719990878, 286.80405231063713, 297.06303815905153, 296.1548327538969, 307.56100133745554, 293.3626961031103, 281.75118529165496, 220.0098994596266, 298.0582126634063, 279.9706749046484, 290.33011964794946, 283.84511320962497, 274.85256747251276], 'lossList': [0.0, -1.3900408220291138, 0.0, 120.68054677963256, 0.0, 0.0, 0.0], 'rewardMean': 0.7061996193050301, 'totalEpisodes': 45, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.811148966575755, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.11275326886195175, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5578380316942604, 'totalEpisodes': 77, 'stepsPerEpisode': 106, 'rewardPerEpisode': 67.35612571084752
'totalSteps': 10240, 'rewardStep': 0.7493649349792627, 'errorList': [], 'lossList': [0.0, -1.3933688616752624, 0.0, 145.40659233093263, 0.0, 0.0, 0.0], 'rewardMean': 0.5791187987259273, 'totalEpisodes': 112, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.075105651192807
'totalSteps': 11520, 'rewardStep': 0.7528041074754463, 'errorList': [], 'lossList': [0.0, -1.3955549496412276, 0.0, 59.63812589645386, 0.0, 0.0, 0.0], 'rewardMean': 0.5964873296008792, 'totalEpisodes': 138, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.3674400068286863
'totalSteps': 12800, 'rewardStep': 0.6600667369228428, 'errorList': [], 'lossList': [0.0, -1.396160099506378, 0.0, 40.14065464019775, 0.0, 0.0, 0.0], 'rewardMean': 0.5953024626788291, 'totalEpisodes': 154, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.711618154156322
'totalSteps': 14080, 'rewardStep': 0.2937170833748116, 'errorList': [], 'lossList': [0.0, -1.3851381522417068, 0.0, 16.47871262073517, 0.0, 0.0, 0.0], 'rewardMean': 0.5319034474183668, 'totalEpisodes': 159, 'stepsPerEpisode': 102, 'rewardPerEpisode': 60.92753922208001
'totalSteps': 15360, 'rewardStep': 0.8164870677543293, 'errorList': [], 'lossList': [0.0, -1.3620323663949967, 0.0, 29.25374690771103, 0.0, 0.0, 0.0], 'rewardMean': 0.5547908440481563, 'totalEpisodes': 166, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.372395591356472
'totalSteps': 16640, 'rewardStep': 0.08517802587381462, 'errorList': [], 'lossList': [0.0, -1.3625695192813874, 0.0, 10.117821694612504, 0.0, 0.0, 0.0], 'rewardMean': 0.5087547537691657, 'totalEpisodes': 170, 'stepsPerEpisode': 350, 'rewardPerEpisode': 256.3590531916467
'totalSteps': 17920, 'rewardStep': 0.47505721863825356, 'errorList': [], 'lossList': [0.0, -1.376188763976097, 0.0, 10.992896833419799, 0.0, 0.0, 0.0], 'rewardMean': 0.5003021298156879, 'totalEpisodes': 174, 'stepsPerEpisode': 203, 'rewardPerEpisode': 156.19841331368397
'totalSteps': 19200, 'rewardStep': 0.7543483766812225, 'errorList': [], 'lossList': [0.0, -1.373685104250908, 0.0, 6.633508905172348, 0.0, 0.0, 0.0], 'rewardMean': 0.48125300894238865, 'totalEpisodes': 177, 'stepsPerEpisode': 309, 'rewardPerEpisode': 238.0902328672752
'totalSteps': 20480, 'rewardStep': 0.7132437109094729, 'errorList': [], 'lossList': [0.0, -1.338870449066162, 0.0, 7.206097500324249, 0.0, 0.0, 0.0], 'rewardMean': 0.5413020531471407, 'totalEpisodes': 179, 'stepsPerEpisode': 431, 'rewardPerEpisode': 346.1687480262162
'totalSteps': 21760, 'rewardStep': 0.5611939048123905, 'errorList': [], 'lossList': [0.0, -1.319085047841072, 0.0, 5.537019064426422, 0.0, 0.0, 0.0], 'rewardMean': 0.5861461167421848, 'totalEpisodes': 180, 'stepsPerEpisode': 339, 'rewardPerEpisode': 252.38235394279963
'totalSteps': 23040, 'rewardStep': 0.4842441078502349, 'errorList': [], 'lossList': [0.0, -1.2939965671300888, 0.0, 2.575151116847992, 0.0, 0.0, 0.0], 'rewardMean': 0.5596340340292819, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 848.6494683683912
'totalSteps': 24320, 'rewardStep': 0.842582542938593, 'errorList': [], 'lossList': [0.0, -1.2516649436950684, 0.0, 2.3383683654665948, 0.0, 0.0, 0.0], 'rewardMean': 0.5686118775755966, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.8964951309326
'totalSteps': 25600, 'rewardStep': 0.9902471483339852, 'errorList': [0.08230967927237805, 0.09270503055968435, 0.09948256205307915, 0.09421958829936063, 0.16091710619689026, 0.09680871591132688, 0.10160879762439842, 0.07078525600912064, 0.10403866935513116, 0.10640572944443898, 0.09524958082000895, 0.095921199171143, 0.10078068966731418, 0.15529031959282574, 0.09690488084417902, 0.09075113660822985, 0.10628873974126446, 0.10929538843335306, 0.07872966824461619, 0.09934478813600289, 0.08232545033379733, 0.1630915167883567, 0.13489342812815844, 0.09524594512466182, 0.09337371069243575, 0.12403827727526977, 0.11499415356919658, 0.09531136174249698, 0.1954046734898565, 0.12609830798635405, 0.0782737611673624, 0.16623027288525877, 0.11037847268338205, 0.11417741916982453, 0.07667295115468821, 0.06942581776527165, 0.07925126967906901, 0.16032792169770171, 0.08484838345377664, 0.11557222872228132, 0.10127646800056152, 0.09093106297092807, 0.09697804253995031, 0.10087992178582209, 0.10525003905089388, 0.1419931810100631, 0.19225771212279072, 0.09691578873868635, 0.0818061670938773, 0.08487816102833477], 'lossList': [0.0, -1.226361123919487, 0.0, 1.7717459251731633, 0.0, 0.0, 0.0], 'rewardMean': 0.6016299187167108, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.554586808145, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=98.69
