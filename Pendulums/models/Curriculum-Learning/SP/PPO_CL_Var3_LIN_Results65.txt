#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 65
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9404718286725651, 'errorList': [], 'lossList': [0.0, -1.4391050517559052, 0.0, 27.966964702010156, 0.0, 0.0, 0.0], 'rewardMean': 0.9180267099403661, 'totalEpisodes': 8, 'stepsPerEpisode': 532, 'rewardPerEpisode': 368.853686049928
'totalSteps': 3840, 'rewardStep': 0.6743927516805848, 'errorList': [], 'lossList': [0.0, -1.4340528255701066, 0.0, 29.82778364419937, 0.0, 0.0, 0.0], 'rewardMean': 0.8368153905204391, 'totalEpisodes': 13, 'stepsPerEpisode': 154, 'rewardPerEpisode': 128.7030664830073
'totalSteps': 5120, 'rewardStep': 0.7064170810777604, 'errorList': [], 'lossList': [0.0, -1.434004259109497, 0.0, 40.027988982200625, 0.0, 0.0, 0.0], 'rewardMean': 0.8042158131597694, 'totalEpisodes': 17, 'stepsPerEpisode': 271, 'rewardPerEpisode': 201.70407866272632
'totalSteps': 6400, 'rewardStep': 0.7769002323311393, 'errorList': [], 'lossList': [0.0, -1.4345453101396561, 0.0, 39.8534906411171, 0.0, 0.0, 0.0], 'rewardMean': 0.7987526969940434, 'totalEpisodes': 20, 'stepsPerEpisode': 207, 'rewardPerEpisode': 163.3221077756555
'totalSteps': 7680, 'rewardStep': 0.9494733576011196, 'errorList': [255.2874156521839, 303.29533263411645, 300.5003047009216, 310.08580981466673, 310.36192952474937, 303.48541893704675, 332.77749994685206, 297.2914186136733, 322.5249529340822, 322.02670944280527, 321.8673588427508, 325.8447373995219, 331.0397900965989, 242.59270634919582, 253.7198101628159, 323.20512891927467, 320.5267494352655, 323.98195660463864, 327.8270717579726, 340.90537498432093, 301.8933850626408, 297.2310376393981, 288.4960386677962, 333.36378093152547, 196.39138222949848, 310.6429504826571, 271.83501274473025, 328.28170225384713, 218.2371818612928, 294.57303206173646, 273.0390497350044, 320.1730676543837, 299.98022432380577, 319.76763116293085, 256.825661558952, 313.1477570522482, 317.97221964426177, 325.15346719660687, 327.38069347960806, 300.0065763749581, 313.8744559473259, 280.6376720944915, 226.8050138906917, 313.08421725924467, 285.271182115678, 315.04227896800217, 328.3134939184732, 289.3217338812227, 309.3437959529812, 289.8019467386508], 'lossList': [0.0, -1.4225769942998887, 0.0, 138.94050861358642, 0.0, 0.0, 0.0], 'rewardMean': 0.8238728070952227, 'totalEpisodes': 35, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.615545739093305, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.5057251452703184, 'errorList': [], 'lossList': [0.0, -1.4121436178684235, 0.0, 167.57777114868165, 0.0, 0.0, 0.0], 'rewardMean': 0.7784231411202364, 'totalEpisodes': 65, 'stepsPerEpisode': 38, 'rewardPerEpisode': 24.821505089428637
'totalSteps': 10240, 'rewardStep': 0.9810905016057245, 'errorList': [12.853832384438025, 5.45081414563643, 12.800136911962637, 34.69303324061742, 26.62876685045899, 10.415245668112929, 17.00049479548284, 42.05790661992371, 46.65746718775749, 17.64625137206225, 21.794659266947956, 37.66131604705683, 15.315862236280159, 16.073787251761395, 14.328559804588794, 1.4013150264484937, 12.917137788260847, 28.934382164450533, 39.97967600387121, 10.60570642532546, 22.772399137337736, 28.411005439514565, 12.501144795108551, 33.58338858629149, 18.728780432776876, 11.668627323439631, 33.23501939976991, 4.768818718512274, 42.79857349857286, 14.980046541596108, 11.169005995331228, 17.126294546779853, 12.034637362194982, 17.846062067604727, 20.522516316330506, 11.3229118451977, 29.571352798825668, 9.285970399731507, 9.856128619906185, 28.072706188089708, 13.23337612501027, 17.23521243541192, 40.83332311451959, 13.772676586408274, 21.709552745087485, 12.383786467579764, 17.911530101903804, 18.783204790492086, 27.549420853497615, 49.10420520136118], 'lossList': [0.0, -1.4078661972284316, 0.0, 115.99172687530518, 0.0, 0.0, 0.0], 'rewardMean': 0.8037565611809223, 'totalEpisodes': 97, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.978123903319513, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8205736224812464, 'errorList': [], 'lossList': [0.0, -1.3995018601417542, 0.0, 54.94968320846558, 0.0, 0.0, 0.0], 'rewardMean': 0.805625123547625, 'totalEpisodes': 110, 'stepsPerEpisode': 69, 'rewardPerEpisode': 62.38538738795597
'totalSteps': 12800, 'rewardStep': 0.36458893706969825, 'errorList': [], 'lossList': [0.0, -1.3760936903953551, 0.0, 45.54179908752442, 0.0, 0.0, 0.0], 'rewardMean': 0.7615215048998324, 'totalEpisodes': 118, 'stepsPerEpisode': 122, 'rewardPerEpisode': 75.95112530254778
'totalSteps': 14080, 'rewardStep': 0.4173657403986426, 'errorList': [], 'lossList': [0.0, -1.3642394101619721, 0.0, 15.662938873767853, 0.0, 0.0, 0.0], 'rewardMean': 0.71369991981888, 'totalEpisodes': 124, 'stepsPerEpisode': 230, 'rewardPerEpisode': 182.9158815224363
'totalSteps': 15360, 'rewardStep': 0.6116486351420891, 'errorList': [], 'lossList': [0.0, -1.372368273139, 0.0, 26.294802567958833, 0.0, 0.0, 0.0], 'rewardMean': 0.6808176004658323, 'totalEpisodes': 130, 'stepsPerEpisode': 68, 'rewardPerEpisode': 53.7455122875018
'totalSteps': 16640, 'rewardStep': 0.7921365290050771, 'errorList': [], 'lossList': [0.0, -1.3567527031898499, 0.0, 9.492520642280578, 0.0, 0.0, 0.0], 'rewardMean': 0.6925919781982816, 'totalEpisodes': 135, 'stepsPerEpisode': 116, 'rewardPerEpisode': 95.75966716848372
'totalSteps': 17920, 'rewardStep': 0.8457351616144381, 'errorList': [], 'lossList': [0.0, -1.3387871128320694, 0.0, 10.65134074807167, 0.0, 0.0, 0.0], 'rewardMean': 0.7065237862519493, 'totalEpisodes': 139, 'stepsPerEpisode': 139, 'rewardPerEpisode': 117.26470014870866
'totalSteps': 19200, 'rewardStep': 0.5948034766825183, 'errorList': [], 'lossList': [0.0, -1.3222349721193314, 0.0, 5.376313167214394, 0.0, 0.0, 0.0], 'rewardMean': 0.6883141106870873, 'totalEpisodes': 142, 'stepsPerEpisode': 431, 'rewardPerEpisode': 347.1694765587378
'totalSteps': 20480, 'rewardStep': 0.7609157268158314, 'errorList': [], 'lossList': [0.0, -1.3095760971307755, 0.0, 5.091111608743668, 0.0, 0.0, 0.0], 'rewardMean': 0.6694583476085584, 'totalEpisodes': 146, 'stepsPerEpisode': 243, 'rewardPerEpisode': 188.56487482465263
'totalSteps': 21760, 'rewardStep': 0.8875114667550987, 'errorList': [], 'lossList': [0.0, -1.3195037180185318, 0.0, 3.1677694648504255, 0.0, 0.0, 0.0], 'rewardMean': 0.7076369797570364, 'totalEpisodes': 148, 'stepsPerEpisode': 136, 'rewardPerEpisode': 112.32189942624888
'totalSteps': 23040, 'rewardStep': 0.77297655323805, 'errorList': [], 'lossList': [0.0, -1.318696082830429, 0.0, 2.4763747316598894, 0.0, 0.0, 0.0], 'rewardMean': 0.686825584920269, 'totalEpisodes': 151, 'stepsPerEpisode': 67, 'rewardPerEpisode': 59.74084999833814
'totalSteps': 24320, 'rewardStep': 0.6678771514734777, 'errorList': [], 'lossList': [0.0, -1.2937940561771393, 0.0, 1.352574782818556, 0.0, 0.0, 0.0], 'rewardMean': 0.6715559378194922, 'totalEpisodes': 151, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.2854859989939
'totalSteps': 25600, 'rewardStep': 0.42070867003746115, 'errorList': [], 'lossList': [0.0, -1.2726643270254134, 0.0, 2.247292746901512, 0.0, 0.0, 0.0], 'rewardMean': 0.6771679111162685, 'totalEpisodes': 152, 'stepsPerEpisode': 970, 'rewardPerEpisode': 801.5030487309019
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=99.02
