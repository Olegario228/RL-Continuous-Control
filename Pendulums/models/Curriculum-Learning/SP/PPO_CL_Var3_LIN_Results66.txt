#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 66
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7947902518580165, 'errorList': [], 'lossList': [0.0, -1.4494698286056518, 0.0, 34.36730170249939, 0.0, 0.0, 0.0], 'rewardMean': 0.6767189851671858, 'totalEpisodes': 12, 'stepsPerEpisode': 79, 'rewardPerEpisode': 69.20803319275251
'totalSteps': 3840, 'rewardStep': 0.8807246940912297, 'errorList': [], 'lossList': [0.0, -1.4669822359085083, 0.0, 24.038377723097803, 0.0, 0.0, 0.0], 'rewardMean': 0.7447208881418671, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 940.2354297676701
'totalSteps': 5120, 'rewardStep': 0.8473891906807482, 'errorList': [], 'lossList': [0.0, -1.438940327167511, 0.0, 37.76863458871841, 0.0, 0.0, 0.0], 'rewardMean': 0.7703879637765874, 'totalEpisodes': 15, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.85980598300861
'totalSteps': 6400, 'rewardStep': 0.8986591598179556, 'errorList': [], 'lossList': [0.0, -1.4280080288648604, 0.0, 72.03036458015441, 0.0, 0.0, 0.0], 'rewardMean': 0.7960422029848611, 'totalEpisodes': 22, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.797655103302295
'totalSteps': 7680, 'rewardStep': 0.7986721976808226, 'errorList': [], 'lossList': [0.0, -1.4127275329828262, 0.0, 63.58140436172485, 0.0, 0.0, 0.0], 'rewardMean': 0.796480535434188, 'totalEpisodes': 27, 'stepsPerEpisode': 564, 'rewardPerEpisode': 446.5472069730753
'totalSteps': 8960, 'rewardStep': 0.5103985301204531, 'errorList': [], 'lossList': [0.0, -1.404212321639061, 0.0, 179.04923328399659, 0.0, 0.0, 0.0], 'rewardMean': 0.7556116775322258, 'totalEpisodes': 61, 'stepsPerEpisode': 68, 'rewardPerEpisode': 52.2019844082673
'totalSteps': 10240, 'rewardStep': 0.9332871276866689, 'errorList': [90.72795814178704, 38.607931036594586, 244.07783642095296, 140.62339693987906, 260.4045269643172, 24.60084551222127, 53.80213448904379, 238.41866366708084, 151.28976289206702, 22.63221311956905, 58.162890116662766, 175.64943457930497, 231.72408768603546, 230.46512289874988, 213.66279168064102, 18.765056391437554, 77.67732880395326, 29.396170276788517, 193.14983286290862, 40.32157701874568, 231.68394281038465, 115.91625200418822, 20.980786909489005, 71.44107930814798, 193.79251687834042, 175.9322721035269, 226.179221108137, 219.26390960386183, 248.27301949151914, 20.70282749302095, 74.30733506250236, 47.02196914865412, 46.583605061316554, 9.472103215553286, 50.75894042680056, 120.64639417068727, 212.81397386915182, 10.036065685141375, 48.09186158995587, 55.27785474525199, 14.13810764043992, 258.444275646406, 254.60244167329995, 45.250627253519184, 212.45878044551728, 51.52139080198898, 32.06435047245302, 239.8886590159079, 44.277405475276865, 131.80556227013554], 'lossList': [0.0, -1.4041674482822417, 0.0, 84.45543014526368, 0.0, 0.0, 0.0], 'rewardMean': 0.7778211088015312, 'totalEpisodes': 95, 'stepsPerEpisode': 28, 'rewardPerEpisode': 24.022940020735565, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9324384066828469, 'errorList': [4.934845004499917, 4.597489793033052, 4.638493165554972, 5.054817221364802, 2.377366145751014, 4.17980953557587, 3.4764427622028258, 4.795299631746181, 5.2229234854743805, 2.4977385074584264, 4.571158498946244, 3.465777759817043, 4.45680476039995, 5.258720014973622, 2.840917227630325, 5.303095783098877, 4.761724494292418, 1.8206537755631607, 3.733140108317447, 4.725899718315995, 4.572104584938351, 3.537710177956823, 5.11991293774649, 3.35192051341946, 4.58521877912098, 4.296045957751386, 1.5268043161639804, 4.817383781744869, 4.064896822002275, 4.580649392898199, 4.551889536391816, 4.746926515050786, 4.208840380110135, 2.8866211567804902, 4.943111746789108, 4.872396023280097, 2.6699900834454087, 3.186844248595652, 0.7782404376649615, 3.727554158110396, 3.8507616133543343, 2.4339963990775044, 2.728789033516345, 4.915723170585242, 4.571087367297145, 3.7499250360751635, 3.968196063622, 3.76213189879281, 4.50718967644658, 5.007620021132915], 'lossList': [0.0, -1.3975427788496018, 0.0, 38.495550365448, 0.0, 0.0, 0.0], 'rewardMean': 0.7950008085661219, 'totalEpisodes': 111, 'stepsPerEpisode': 18, 'rewardPerEpisode': 12.446086475632981, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.6268390696225172, 'errorList': [], 'lossList': [0.0, -1.3796028363704682, 0.0, 33.019495754241944, 0.0, 0.0, 0.0], 'rewardMean': 0.7781846346717614, 'totalEpisodes': 121, 'stepsPerEpisode': 130, 'rewardPerEpisode': 99.35504193489459
'totalSteps': 14080, 'rewardStep': 0.8614396196379717, 'errorList': [], 'lossList': [0.0, -1.3610117053985595, 0.0, 19.637962892055512, 0.0, 0.0, 0.0], 'rewardMean': 0.808463824787923, 'totalEpisodes': 126, 'stepsPerEpisode': 251, 'rewardPerEpisode': 195.64636966178935
'totalSteps': 15360, 'rewardStep': 0.8346829284820776, 'errorList': [], 'lossList': [0.0, -1.3426701760292052, 0.0, 11.514775924682617, 0.0, 0.0, 0.0], 'rewardMean': 0.8124530924503292, 'totalEpisodes': 130, 'stepsPerEpisode': 73, 'rewardPerEpisode': 59.81588434556683
'totalSteps': 16640, 'rewardStep': 0.7985877242427291, 'errorList': [], 'lossList': [0.0, -1.3370702803134917, 0.0, 24.754588322639464, 0.0, 0.0, 0.0], 'rewardMean': 0.8042393954654792, 'totalEpisodes': 135, 'stepsPerEpisode': 248, 'rewardPerEpisode': 193.11592998079436
'totalSteps': 17920, 'rewardStep': 0.5440122789778746, 'errorList': [], 'lossList': [0.0, -1.3183656060695648, 0.0, 34.96937774658203, 0.0, 0.0, 0.0], 'rewardMean': 0.7739017042951918, 'totalEpisodes': 137, 'stepsPerEpisode': 403, 'rewardPerEpisode': 263.2770308636891
'totalSteps': 19200, 'rewardStep': 0.76123262233326, 'errorList': [], 'lossList': [0.0, -1.3030292773246765, 0.0, 6.2774893665313725, 0.0, 0.0, 0.0], 'rewardMean': 0.7601590505467223, 'totalEpisodes': 139, 'stepsPerEpisode': 210, 'rewardPerEpisode': 155.06582889328618
'totalSteps': 20480, 'rewardStep': 0.6054912893697917, 'errorList': [], 'lossList': [0.0, -1.2555230832099915, 0.0, 3.3705161201953886, 0.0, 0.0, 0.0], 'rewardMean': 0.7408409597156191, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1042.7443624477319
'totalSteps': 21760, 'rewardStep': 0.6844632436455199, 'errorList': [], 'lossList': [0.0, -1.1859994208812714, 0.0, 1.2119542212784291, 0.0, 0.0, 0.0], 'rewardMean': 0.7582474310681258, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 990.2925863530003
'totalSteps': 23040, 'rewardStep': 0.9128531137726655, 'errorList': [], 'lossList': [0.0, -1.1509585464000702, 0.0, 1.9221425920724868, 0.0, 0.0, 0.0], 'rewardMean': 0.7562040296767254, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.8841342642277
'totalSteps': 24320, 'rewardStep': 0.8893390407837354, 'errorList': [], 'lossList': [0.0, -1.1121469974517821, 0.0, 1.5700033910572528, 0.0, 0.0, 0.0], 'rewardMean': 0.7518940930868142, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.1584400944043
'totalSteps': 25600, 'rewardStep': 0.9472125510536795, 'errorList': [0.09112878884042777, 0.046449711156077836, 0.04135102210305559, 0.054727576989628685, 0.038823328514330714, 0.0451798632632722, 0.02785592723615822, 0.04150299887440529, 0.02659384874724222, 0.029145349525223317, 0.07006782023150057, 0.044197956225026126, 0.02700934250504498, 0.049492850695761, 0.041781333990039624, 0.028034171604121166, 0.06556386260030127, 0.02772738832041777, 0.06006978657198835, 0.042984617212998025, 0.058065658800877724, 0.032546502369001536, 0.03611901542658913, 0.02738610356964693, 0.030926797342340308, 0.08575878633766684, 0.09220011982364672, 0.05331008522081633, 0.03851189489417553, 0.026992340134594114, 0.049346760615646336, 0.04905296011507466, 0.03373039070107013, 0.026779188644210705, 0.07025091118915029, 0.034172900758970066, 0.07055469316226058, 0.04076257611853701, 0.03198058870190139, 0.05982341047315251, 0.06610122961811643, 0.039594663488285664, 0.027783880401983197, 0.07427665062812637, 0.09230576764205824, 0.0697841233391312, 0.06532333790012164, 0.048526439627706926, 0.03230869599176619, 0.0601162193253179], 'lossList': [0.0, -1.0530760490894318, 0.0, 0.9763169389218092, 0.0, 0.0, 0.0], 'rewardMean': 0.7839314412299305, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.412203504616, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=121.51
