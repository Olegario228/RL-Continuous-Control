#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 67
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.7031882365499933, 'errorList': [], 'lossList': [0.0, -1.4564979231357575, 0.0, 31.163916478157045, 0.0, 0.0, 0.0], 'rewardMean': 0.5904870058863958, 'totalEpisodes': 14, 'stepsPerEpisode': 290, 'rewardPerEpisode': 221.7498495361293
'totalSteps': 3840, 'rewardStep': 0.8446858503425053, 'errorList': [], 'lossList': [0.0, -1.4678905951976775, 0.0, 30.04692358493805, 0.0, 0.0, 0.0], 'rewardMean': 0.6752199540384324, 'totalEpisodes': 18, 'stepsPerEpisode': 485, 'rewardPerEpisode': 369.9989940588367
'totalSteps': 5120, 'rewardStep': 0.5338136493062411, 'errorList': [], 'lossList': [0.0, -1.4286870062351227, 0.0, 35.145032806396486, 0.0, 0.0, 0.0], 'rewardMean': 0.6398683778553845, 'totalEpisodes': 24, 'stepsPerEpisode': 69, 'rewardPerEpisode': 40.20834939524653
'totalSteps': 6400, 'rewardStep': 0.7320361769663758, 'errorList': [], 'lossList': [0.0, -1.4110269719362258, 0.0, 68.62602380752564, 0.0, 0.0, 0.0], 'rewardMean': 0.6583019376775827, 'totalEpisodes': 33, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.887034444189018
'totalSteps': 7680, 'rewardStep': 0.9473535277392529, 'errorList': [229.5910184880399, 217.46226521491593, 194.5626327978816, 228.58917988421615, 240.55390041033957, 216.15594218766012, 185.3189019910954, 227.24055596810845, 229.42218158474793, 232.83389631075653, 190.07808169545376, 238.72828190449377, 227.01511637616633, 241.08618999194917, 228.19764754006752, 243.0871018625961, 226.38661561563308, 233.18738557794842, 231.39728273380337, 228.83655681904926, 200.18273013176545, 201.53579189511274, 243.2019279421057, 243.44385508639644, 190.44075078510872, 193.44242256979956, 221.90022559221953, 242.96886256054592, 206.6628340409699, 233.40061162183642, 235.3012897635054, 234.67091605655582, 185.5984084969836, 225.78283689055039, 223.79654830268345, 221.8323782751047, 227.98980512930478, 238.26194507414962, 218.2960494416151, 246.85241549040413, 186.3199796020774, 235.08357801564324, 204.25653592281273, 237.62192821051943, 221.0068333961961, 214.51708544422524, 226.8099609612597, 198.43845246546238, 219.2257930068986, 212.64296154040542], 'lossList': [0.0, -1.401377569437027, 0.0, 137.50885829925537, 0.0, 0.0, 0.0], 'rewardMean': 0.7064772026878611, 'totalEpisodes': 53, 'stepsPerEpisode': 51, 'rewardPerEpisode': 38.858594824874686, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.6832391743603916, 'errorList': [], 'lossList': [0.0, -1.4036358731985092, 0.0, 150.28609188079835, 0.0, 0.0, 0.0], 'rewardMean': 0.7031574843553654, 'totalEpisodes': 95, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.61869004383316
'totalSteps': 10240, 'rewardStep': 0.673890142848473, 'errorList': [], 'lossList': [0.0, -1.4039247196912765, 0.0, 64.33831100463867, 0.0, 0.0, 0.0], 'rewardMean': 0.6994990666670039, 'totalEpisodes': 124, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.41777371476094
'totalSteps': 11520, 'rewardStep': 0.6661949228144998, 'errorList': [], 'lossList': [0.0, -1.4063126039505005, 0.0, 31.22946235179901, 0.0, 0.0, 0.0], 'rewardMean': 0.6957986062389478, 'totalEpisodes': 141, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.169739043287652
'totalSteps': 12800, 'rewardStep': 0.8645601151567275, 'errorList': [], 'lossList': [0.0, -1.4172004598379135, 0.0, 34.070014929771425, 0.0, 0.0, 0.0], 'rewardMean': 0.7126747571307258, 'totalEpisodes': 150, 'stepsPerEpisode': 145, 'rewardPerEpisode': 123.1626297476657
'totalSteps': 14080, 'rewardStep': 0.8378527457607061, 'errorList': [], 'lossList': [0.0, -1.4125342786312103, 0.0, 11.332357114553451, 0.0, 0.0, 0.0], 'rewardMean': 0.7486814541845167, 'totalEpisodes': 156, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.320789089121742
'totalSteps': 15360, 'rewardStep': 0.9675339884680059, 'errorList': [11.994084016335428, 38.25691174019328, 6.340103924073183, 18.82094671614034, 30.51378110050331, 7.286490802333506, 3.3662816647121816, 3.200163659095819, 17.56653008395449, 0.6324373789481982, 0.572227052850155, 10.087902449750223, 16.881366936314347, 6.625762712867491, 49.26801738621619, 0.9871756604528074, 27.398941031995562, 1.7183759290904246, 7.653891159930428, 13.807069843007545, 37.16764029170283, 35.57444771324174, 17.915812622751332, 3.357362404712454, 27.620918024894795, 16.265738476779912, 3.4117752543848137, 34.04771314463045, 2.3291862400078567, 8.076743967699953, 2.942473724244692, 11.078495590530975, 30.577214779858924, 5.700775362095134, 6.557100982673119, 26.239269081345043, 8.580199544890798, 3.0242369735150665, 20.29165778061047, 0.7458783006392397, 5.503175855896335, 36.38190470708597, 6.293297336618199, 4.383468802523048, 33.95623024470133, 3.7615622376550215, 11.217273538103745, 7.52923537311014, 8.751681911814906, 48.93685298113582], 'lossList': [0.0, -1.4056445896625518, 0.0, 9.718329648971558, 0.0, 0.0, 0.0], 'rewardMean': 0.7751160293763178, 'totalEpisodes': 161, 'stepsPerEpisode': 72, 'rewardPerEpisode': 60.74420838369256, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.9140851692632316, 'errorList': [], 'lossList': [0.0, -1.388587566614151, 0.0, 7.0858503329753875, 0.0, 0.0, 0.0], 'rewardMean': 0.7820559612683905, 'totalEpisodes': 165, 'stepsPerEpisode': 81, 'rewardPerEpisode': 68.23764419393096
'totalSteps': 17920, 'rewardStep': 0.7996529902478217, 'errorList': [], 'lossList': [0.0, -1.3708079916238785, 0.0, 6.022107365131379, 0.0, 0.0, 0.0], 'rewardMean': 0.8086398953625487, 'totalEpisodes': 168, 'stepsPerEpisode': 276, 'rewardPerEpisode': 244.6828069584567
'totalSteps': 19200, 'rewardStep': 0.8622284137337474, 'errorList': [], 'lossList': [0.0, -1.3482063561677933, 0.0, 5.992124274969101, 0.0, 0.0, 0.0], 'rewardMean': 0.8216591190392858, 'totalEpisodes': 172, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.14172299487759
'totalSteps': 20480, 'rewardStep': 0.1692534294682847, 'errorList': [], 'lossList': [0.0, -1.3337740921974182, 0.0, 3.6676638668775556, 0.0, 0.0, 0.0], 'rewardMean': 0.7438491092121889, 'totalEpisodes': 175, 'stepsPerEpisode': 303, 'rewardPerEpisode': 223.7203046901657
'totalSteps': 21760, 'rewardStep': 0.9028966129127487, 'errorList': [], 'lossList': [0.0, -1.3268692529201507, 0.0, 2.8479492792487147, 0.0, 0.0, 0.0], 'rewardMean': 0.7658148530674247, 'totalEpisodes': 178, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.551931845091676
'totalSteps': 23040, 'rewardStep': 0.9343307690000958, 'errorList': [0.06685394011359547, 0.03877631373519539, 0.045855533513403594, 0.04359807339801479, 0.08598900135346602, 0.09747634542830544, 0.03168363477104514, 0.04758436890192242, 0.031550875728355396, 0.032504601567481785, 0.04392637084130879, 0.04992179161753283, 0.030924738039195387, 0.030963808187061626, 0.032231703067790815, 0.05724600182111622, 0.1021185635650061, 0.0400280240102396, 0.030772600742241356, 0.03184731834350282, 0.07205154565742113, 0.030645641800585214, 0.03173317137366092, 0.058561486937415246, 0.03153094644722622, 0.03160597526880423, 0.0323428380055153, 0.03155150946345258, 0.03150309155373287, 0.05549952412703034, 0.06995311839537482, 0.08876016297033273, 0.06432145123247957, 0.048053414583331046, 0.05676576285442106, 0.031004380408802098, 0.04329257147461861, 0.045901574268411405, 0.03257293352369774, 0.04311849339563096, 0.06811033599672772, 0.08001068718829893, 0.03176953525426494, 0.03279008966061824, 0.031016178609211142, 0.040405414282412774, 0.09583335253391548, 0.05539510301074702, 0.03527304316893027, 0.031400689560132124], 'lossList': [0.0, -1.3160190790891648, 0.0, 2.0307209116220473, 0.0, 0.0, 0.0], 'rewardMean': 0.7918589156825869, 'totalEpisodes': 179, 'stepsPerEpisode': 77, 'rewardPerEpisode': 68.72678386423588, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=116.91
