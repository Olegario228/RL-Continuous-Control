#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 68
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5502280371270855, 'errorList': [], 'lossList': [0.0, -1.4216834193468093, 0.0, 26.771386903524398, 0.0, 0.0, 0.0], 'rewardMean': 0.7042200728413126, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 30.262794729768682
'totalSteps': 3840, 'rewardStep': 0.9419112127675138, 'errorList': [], 'lossList': [0.0, -1.4186916601657868, 0.0, 25.03918586730957, 0.0, 0.0, 0.0], 'rewardMean': 0.7834504528167129, 'totalEpisodes': 18, 'stepsPerEpisode': 492, 'rewardPerEpisode': 363.85461837999037
'totalSteps': 5120, 'rewardStep': 0.7234081888416142, 'errorList': [], 'lossList': [0.0, -1.4121344238519669, 0.0, 48.464568634033206, 0.0, 0.0, 0.0], 'rewardMean': 0.7684398868229383, 'totalEpisodes': 23, 'stepsPerEpisode': 78, 'rewardPerEpisode': 68.78340703298076
'totalSteps': 6400, 'rewardStep': 0.5120185384288567, 'errorList': [], 'lossList': [0.0, -1.3984526079893111, 0.0, 94.8989677810669, 0.0, 0.0, 0.0], 'rewardMean': 0.7171556171441219, 'totalEpisodes': 36, 'stepsPerEpisode': 28, 'rewardPerEpisode': 18.491479109566328
'totalSteps': 7680, 'rewardStep': 0.8432463635893275, 'errorList': [], 'lossList': [0.0, -1.389695279598236, 0.0, 153.783497505188, 0.0, 0.0, 0.0], 'rewardMean': 0.7381707415516562, 'totalEpisodes': 62, 'stepsPerEpisode': 55, 'rewardPerEpisode': 43.42678399077061
'totalSteps': 8960, 'rewardStep': 0.6022716674914372, 'errorList': [], 'lossList': [0.0, -1.3842847263813018, 0.0, 100.9010046005249, 0.0, 0.0, 0.0], 'rewardMean': 0.7187565881144821, 'totalEpisodes': 94, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.684742022262139
'totalSteps': 10240, 'rewardStep': 0.5869687810892162, 'errorList': [], 'lossList': [0.0, -1.382537031173706, 0.0, 64.4283772277832, 0.0, 0.0, 0.0], 'rewardMean': 0.7022831122363238, 'totalEpisodes': 119, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.219145520992473
'totalSteps': 11520, 'rewardStep': 0.1269684614175835, 'errorList': [], 'lossList': [0.0, -1.3651441836357117, 0.0, 36.84303807258606, 0.0, 0.0, 0.0], 'rewardMean': 0.6383592621453527, 'totalEpisodes': 129, 'stepsPerEpisode': 143, 'rewardPerEpisode': 96.79275087652863
'totalSteps': 12800, 'rewardStep': 0.333221358482283, 'errorList': [], 'lossList': [0.0, -1.3366950970888138, 0.0, 35.354236330986026, 0.0, 0.0, 0.0], 'rewardMean': 0.6078454717790457, 'totalEpisodes': 135, 'stepsPerEpisode': 139, 'rewardPerEpisode': 79.75651207284379
'totalSteps': 14080, 'rewardStep': 0.9526217821315942, 'errorList': [0.03319237959760834, 0.050482750482528614, 0.03846732292935485, 0.03436995974677751, 0.03566749886653931, 0.034513460646709425, 0.11394768254005966, 0.03380223067128885, 0.07619335520801834, 0.04620518923506442, 0.03569804010586968, 0.035289101049579184, 0.04590482200339007, 0.034590966242562, 0.03697055578998738, 0.06618929184064812, 0.07712936574465115, 0.035277473257400056, 0.0512376305684314, 0.038614842639496154, 0.12110856828278649, 0.03385793045785301, 0.15671574605879546, 0.05048749550291584, 0.05638659612690265, 0.03654285068982575, 0.0338649997500664, 0.03432729897638881, 0.04822473370001334, 0.08339652888489413, 0.03384398549943305, 0.03602209342008031, 0.04044329059852201, 0.049073371739863016, 0.037097789820013105, 0.03396085453593429, 0.06060427987310381, 0.04723843562841196, 0.032658236056654445, 0.03501393457078235, 0.07175940562713386, 0.03610161186873229, 0.033061732425007116, 0.09785730657043153, 0.05905483430254447, 0.0337172036347294, 0.03719740755713536, 0.035226044118078476, 0.0342110249683022, 0.09771818579012517], 'lossList': [0.0, -1.3175425398349763, 0.0, 25.263577950000762, 0.0, 0.0, 0.0], 'rewardMean': 0.6172864391366512, 'totalEpisodes': 137, 'stepsPerEpisode': 178, 'rewardPerEpisode': 138.7744187281901, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=14080, timeSpent=57.42
