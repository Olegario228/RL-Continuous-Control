#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 69
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8497572530389365, 'errorList': [], 'lossList': [0.0, -1.4271836519241332, 0.0, 31.638405756950377, 0.0, 0.0, 0.0], 'rewardMean': 0.7865540884591187, 'totalEpisodes': 13, 'stepsPerEpisode': 317, 'rewardPerEpisode': 256.4445894369631
'totalSteps': 3840, 'rewardStep': 0.6824951569600125, 'errorList': [], 'lossList': [0.0, -1.4410503172874451, 0.0, 28.819555780887605, 0.0, 0.0, 0.0], 'rewardMean': 0.7518677779594166, 'totalEpisodes': 16, 'stepsPerEpisode': 171, 'rewardPerEpisode': 124.44455427799168
'totalSteps': 5120, 'rewardStep': 0.5058993523078081, 'errorList': [], 'lossList': [0.0, -1.4459448230266572, 0.0, 32.274264523983, 0.0, 0.0, 0.0], 'rewardMean': 0.6903756715465145, 'totalEpisodes': 19, 'stepsPerEpisode': 282, 'rewardPerEpisode': 204.82755624322735
'totalSteps': 6400, 'rewardStep': 0.9627161197015984, 'errorList': [], 'lossList': [0.0, -1.449645534157753, 0.0, 74.67579500198364, 0.0, 0.0, 0.0], 'rewardMean': 0.7448437611775313, 'totalEpisodes': 27, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.159600818292798
'totalSteps': 7680, 'rewardStep': 0.7925888426750309, 'errorList': [], 'lossList': [0.0, -1.4476562058925628, 0.0, 151.38770198822021, 0.0, 0.0, 0.0], 'rewardMean': 0.7528012747604479, 'totalEpisodes': 53, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.910209493828136
'totalSteps': 8960, 'rewardStep': 0.5415560140660522, 'errorList': [], 'lossList': [0.0, -1.4438814532756805, 0.0, 119.51307987213134, 0.0, 0.0, 0.0], 'rewardMean': 0.7226233803755342, 'totalEpisodes': 94, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.260196437439124
'totalSteps': 10240, 'rewardStep': 0.9206345261693405, 'errorList': [], 'lossList': [0.0, -1.434457587003708, 0.0, 83.47500198364258, 0.0, 0.0, 0.0], 'rewardMean': 0.74737477359976, 'totalEpisodes': 121, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.813018868941829
'totalSteps': 11520, 'rewardStep': 0.6068587456312989, 'errorList': [], 'lossList': [0.0, -1.4286536037921906, 0.0, 64.10970415115357, 0.0, 0.0, 0.0], 'rewardMean': 0.7317618816032643, 'totalEpisodes': 137, 'stepsPerEpisode': 73, 'rewardPerEpisode': 52.07480448750487
'totalSteps': 12800, 'rewardStep': 0.7750360781040337, 'errorList': [], 'lossList': [0.0, -1.4186133402585983, 0.0, 49.427243337631225, 0.0, 0.0, 0.0], 'rewardMean': 0.7360893012533413, 'totalEpisodes': 148, 'stepsPerEpisode': 61, 'rewardPerEpisode': 51.805573949577905
'totalSteps': 14080, 'rewardStep': 0.8625464947896344, 'errorList': [], 'lossList': [0.0, -1.390776823759079, 0.0, 23.019570887088776, 0.0, 0.0, 0.0], 'rewardMean': 0.7500088583443746, 'totalEpisodes': 155, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.24235214821751
'totalSteps': 15360, 'rewardStep': 0.4944835557075199, 'errorList': [], 'lossList': [0.0, -1.354848437309265, 0.0, 41.50444487571716, 0.0, 0.0, 0.0], 'rewardMean': 0.714481488611233, 'totalEpisodes': 164, 'stepsPerEpisode': 109, 'rewardPerEpisode': 81.77910179668828
'totalSteps': 16640, 'rewardStep': 0.2960277594345174, 'errorList': [], 'lossList': [0.0, -1.3439051163196565, 0.0, 12.559230850934982, 0.0, 0.0, 0.0], 'rewardMean': 0.6758347488586834, 'totalEpisodes': 172, 'stepsPerEpisode': 109, 'rewardPerEpisode': 81.12050224538304
'totalSteps': 17920, 'rewardStep': 0.23426059577038116, 'errorList': [], 'lossList': [0.0, -1.3485751450061798, 0.0, 7.544440438747406, 0.0, 0.0, 0.0], 'rewardMean': 0.6486708732049408, 'totalEpisodes': 178, 'stepsPerEpisode': 172, 'rewardPerEpisode': 123.4654418226022
'totalSteps': 19200, 'rewardStep': 0.981156523133717, 'errorList': [34.80312585438693, 22.80517722659467, 11.781350116171234, 36.04047044416535, 39.673582309413696, 6.354568875988278, 10.954614679327403, 47.29449653177962, 43.41999179042556, 4.304431927734588, 5.687110164304945, 30.898723217232966, 12.020634474484869, 18.976022550531756, 49.22559999776066, 2.1651095929727466, 4.865939259838826, 11.171528850986745, 2.1915461207946496, 0.9915631067464924, 9.509719886713963, 29.18983745323819, 16.738779081977587, 0.07031477468868082, 56.657614606725645, 25.29506229818604, 15.323492970870793, 35.56714523259629, 14.350297293215236, 34.40584527859757, 6.8842706457031415, 32.79845507129155, 8.392072179353676, 25.894511438162745, 24.222605484229668, 33.23265248440323, 2.9260464348881925, 44.69376993133972, 41.42116893471405, 76.25420119439684, 1.9158706417014084, 23.252094718952783, 28.80482014619144, 70.12054884617781, 62.29632130902907, 7.263744732812478, 5.384714726640715, 65.26425704266364, 23.096578657809477, 45.41575334986485], 'lossList': [0.0, -1.347071340084076, 0.0, 6.997661224603653, 0.0, 0.0, 0.0], 'rewardMean': 0.6505149135481527, 'totalEpisodes': 184, 'stepsPerEpisode': 50, 'rewardPerEpisode': 45.80439043127806, 'successfulTests': 1
'totalSteps': 20480, 'rewardStep': 0.9048619094652233, 'errorList': [], 'lossList': [0.0, -1.3370555877685546, 0.0, 5.2060112953186035, 0.0, 0.0, 0.0], 'rewardMean': 0.6617422202271719, 'totalEpisodes': 189, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.166087903776056
'totalSteps': 21760, 'rewardStep': 0.5564039844899684, 'errorList': [], 'lossList': [0.0, -1.3362564891576767, 0.0, 5.985697830319404, 0.0, 0.0, 0.0], 'rewardMean': 0.6632270172695635, 'totalEpisodes': 191, 'stepsPerEpisode': 283, 'rewardPerEpisode': 226.2096167377802
'totalSteps': 23040, 'rewardStep': 0.569108620442438, 'errorList': [], 'lossList': [0.0, -1.330361061692238, 0.0, 4.470801357030869, 0.0, 0.0, 0.0], 'rewardMean': 0.6280744266968732, 'totalEpisodes': 193, 'stepsPerEpisode': 551, 'rewardPerEpisode': 421.5771686501889
'totalSteps': 24320, 'rewardStep': 0.7139376635449074, 'errorList': [], 'lossList': [0.0, -1.331131786108017, 0.0, 3.8198198437690736, 0.0, 0.0, 0.0], 'rewardMean': 0.6387823184882341, 'totalEpisodes': 195, 'stepsPerEpisode': 192, 'rewardPerEpisode': 162.30407493662736
'totalSteps': 25600, 'rewardStep': 0.7172676752448199, 'errorList': [], 'lossList': [0.0, -1.330912910103798, 0.0, 4.137905606031418, 0.0, 0.0, 0.0], 'rewardMean': 0.6330054782023127, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.4178977889601
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=19200, timeSpent=79.85
