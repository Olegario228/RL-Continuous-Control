#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 71
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.7226639047268479, 'errorList': [], 'lossList': [0.0, -1.4430132669210434, 0.0, 36.90351122379303, 0.0, 0.0, 0.0], 'rewardMean': 0.6579208621535036, 'totalEpisodes': 12, 'stepsPerEpisode': 76, 'rewardPerEpisode': 63.69676891766748
'totalSteps': 3840, 'rewardStep': 0.85195027556436, 'errorList': [], 'lossList': [0.0, -1.447424835562706, 0.0, 28.725851695537568, 0.0, 0.0, 0.0], 'rewardMean': 0.7225973332904557, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 980.9508511844587
'totalSteps': 5120, 'rewardStep': 0.6367133565180623, 'errorList': [], 'lossList': [0.0, -1.4254430174827575, 0.0, 22.067241792082786, 0.0, 0.0, 0.0], 'rewardMean': 0.7011263390973574, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.4321471892086
'totalSteps': 6400, 'rewardStep': 0.617692348913793, 'errorList': [], 'lossList': [0.0, -1.410282797217369, 0.0, 37.311822497844695, 0.0, 0.0, 0.0], 'rewardMean': 0.6844395410606445, 'totalEpisodes': 14, 'stepsPerEpisode': 83, 'rewardPerEpisode': 69.32449291036416
'totalSteps': 7680, 'rewardStep': 0.7896222493350545, 'errorList': [], 'lossList': [0.0, -1.3764312505722045, 0.0, 60.30017563819885, 0.0, 0.0, 0.0], 'rewardMean': 0.7019699924397128, 'totalEpisodes': 18, 'stepsPerEpisode': 721, 'rewardPerEpisode': 549.2869327701048
'totalSteps': 8960, 'rewardStep': 0.8880283187872166, 'errorList': [], 'lossList': [0.0, -1.3799480938911437, 0.0, 329.58884971618653, 0.0, 0.0, 0.0], 'rewardMean': 0.7285497533464991, 'totalEpisodes': 60, 'stepsPerEpisode': 30, 'rewardPerEpisode': 26.32928723091041
'totalSteps': 10240, 'rewardStep': 0.8122991675180113, 'errorList': [], 'lossList': [0.0, -1.381059901714325, 0.0, 149.55747783660888, 0.0, 0.0, 0.0], 'rewardMean': 0.7390184301179381, 'totalEpisodes': 102, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.2326625021542483
'totalSteps': 11520, 'rewardStep': 0.7256445291792654, 'errorList': [], 'lossList': [0.0, -1.3786530101299286, 0.0, 95.19105438232423, 0.0, 0.0, 0.0], 'rewardMean': 0.7375324411247522, 'totalEpisodes': 130, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.98620796623454
'totalSteps': 12800, 'rewardStep': 0.9700793691118376, 'errorList': [231.55893372707186, 72.30660071666324, 113.14508252881552, 212.89072170226027, 191.6347960847445, 126.82198720989808, 223.6758660633957, 182.42338437782684, 70.07381872371433, 141.6461030244683, 203.08811174371638, 86.7816736366713, 84.04877942443409, 31.290961458602904, 219.99115496528026, 14.100531773326685, 159.04630224947863, 191.4749787923061, 235.660037034495, 88.2004989212361, 211.19815941675247, 202.36534021172383, 56.57665517446175, 225.59315170355194, 210.33483886191146, 195.42018377436915, 45.83045196800949, 62.34830893575224, 10.575225972924942, 241.17602969255842, 195.93739839818443, 217.03458319443664, 198.4872545373906, 94.38497685976121, 181.55721758863268, 186.83710210106958, 178.14173875758277, 4.2782002204833605, 145.17021722380494, 204.5720622174096, 181.49416067181136, 144.78435415126347, 206.6455347783793, 28.821088278830064, 175.2440262867052, 152.62050065211625, 197.29147741368328, 103.6996400129628, 52.84636057771246, 217.74969007604793], 'lossList': [0.0, -1.3695961737632751, 0.0, 84.53499773025513, 0.0, 0.0, 0.0], 'rewardMean': 0.7607871339234608, 'totalEpisodes': 153, 'stepsPerEpisode': 42, 'rewardPerEpisode': 36.43092029609795, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.9567459784743106, 'errorList': [148.3023683218844, 78.05026667746135, 28.51314662281192, 1.1071576004418529, 193.56874505542234, 124.34082991843579, 28.326614128964916, 112.29244033092338, 83.08064246999912, 90.75613143636983, 158.3548342684552, 134.552466486013, 190.8267771097056, 145.1359346358698, 175.09110720930713, 150.37546136678597, 175.8389419030801, 16.453384199662246, 46.3817634838778, 130.67339758362903, 141.38295301030251, 175.31714090627543, 37.67653953550483, 2.5892199019690687, 149.79630797179038, 86.24330226931335, 132.28859160949122, 89.46997603227392, 65.14462085198986, 150.0363109861462, 95.70111534761229, 4.632550398530414, 175.82744327705055, 128.42338325517437, 116.74514710423587, 178.9936297983529, 157.51278063851052, 76.5814515913677, 6.424850722064099, 83.8900304410528, 94.63018286053777, 10.035272829364656, 171.7827910189982, 166.162324933307, 76.6776742650097, 185.34846125358953, 89.17949744445538, 165.14052739049382, 78.72860849441086, 110.53614773920576], 'lossList': [0.0, -1.359679074883461, 0.0, 43.09610083580017, 0.0, 0.0, 0.0], 'rewardMean': 0.797143949812876, 'totalEpisodes': 164, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.201801815840376, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.5771229694964539, 'errorList': [], 'lossList': [0.0, -1.3526076024770737, 0.0, 16.08412010073662, 0.0, 0.0, 0.0], 'rewardMean': 0.7825898562898365, 'totalEpisodes': 172, 'stepsPerEpisode': 125, 'rewardPerEpisode': 94.52321432023207
'totalSteps': 16640, 'rewardStep': 0.7783912950235735, 'errorList': [], 'lossList': [0.0, -1.3488919895887375, 0.0, 24.890161361694336, 0.0, 0.0, 0.0], 'rewardMean': 0.7752339582357579, 'totalEpisodes': 180, 'stepsPerEpisode': 194, 'rewardPerEpisode': 157.2872076056782
'totalSteps': 17920, 'rewardStep': 0.8888249224941921, 'errorList': [], 'lossList': [0.0, -1.3661498951911926, 0.0, 7.8038863956928255, 0.0, 0.0, 0.0], 'rewardMean': 0.8004451148333708, 'totalEpisodes': 185, 'stepsPerEpisode': 79, 'rewardPerEpisode': 67.5975615040469
'totalSteps': 19200, 'rewardStep': 0.41169595757666977, 'errorList': [], 'lossList': [0.0, -1.3792146795988083, 0.0, 6.722082735300064, 0.0, 0.0, 0.0], 'rewardMean': 0.7798454756996585, 'totalEpisodes': 187, 'stepsPerEpisode': 207, 'rewardPerEpisode': 134.15332774791787
'totalSteps': 20480, 'rewardStep': 0.49724727370595134, 'errorList': [], 'lossList': [0.0, -1.352781263589859, 0.0, 8.357595936059951, 0.0, 0.0, 0.0], 'rewardMean': 0.7506079781367482, 'totalEpisodes': 189, 'stepsPerEpisode': 583, 'rewardPerEpisode': 396.6589768850539
'totalSteps': 21760, 'rewardStep': 0.5777142964654273, 'errorList': [], 'lossList': [0.0, -1.3295921516418456, 0.0, 19.363530178666114, 0.0, 0.0, 0.0], 'rewardMean': 0.7195765759045694, 'totalEpisodes': 190, 'stepsPerEpisode': 823, 'rewardPerEpisode': 697.1247736351811
'totalSteps': 23040, 'rewardStep': 0.833792311203858, 'errorList': [], 'lossList': [0.0, -1.3075654923915863, 0.0, 2.720589602738619, 0.0, 0.0, 0.0], 'rewardMean': 0.7217258902731539, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1065.7387942553328
'totalSteps': 24320, 'rewardStep': 0.7647950976149456, 'errorList': [], 'lossList': [0.0, -1.275176756978035, 0.0, 2.116186939254403, 0.0, 0.0, 0.0], 'rewardMean': 0.725640947116722, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.3496617091891
'totalSteps': 25600, 'rewardStep': 0.9572632890524604, 'errorList': [0.062130869299277885, 0.0401605460669821, 0.060559311604536606, 0.04647866168468748, 0.06343538655280445, 0.0415361511177546, 0.049756646167527896, 0.04581714657921978, 0.05237930017908488, 0.05720208034489517, 0.07177817185623789, 0.03979656591600986, 0.05378810795243808, 0.08394454827074706, 0.06049651060114284, 0.05693364936262337, 0.05683341306808656, 0.05273312441355432, 0.048890961200087894, 0.03957185082579957, 0.044926787564333305, 0.04365174220542244, 0.0608356432515456, 0.05677440801681107, 0.06307607481303186, 0.07430255765337578, 0.06673995835818851, 0.04203364091608149, 0.06658517466729492, 0.05487105632017776, 0.04929507802956129, 0.06226036770795036, 0.061690516196238374, 0.05341213304773405, 0.06289258240116471, 0.04948225248350224, 0.06903936353846421, 0.055912679029990774, 0.04740710190624535, 0.05241374803049537, 0.06504862997173487, 0.07479609431790384, 0.04881365373161555, 0.06290810382909011, 0.08232923778487526, 0.07000602064112715, 0.04845590891100501, 0.04895490200214918, 0.05777988289982758, 0.04532274594101801], 'lossList': [0.0, -1.221552801132202, 0.0, 1.9145670038461686, 0.0, 0.0, 0.0], 'rewardMean': 0.7243593391107843, 'totalEpisodes': 190, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.0432401085104, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=121.45
