#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 72
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8483817590307892, 'errorList': [], 'lossList': [0.0, -1.4531765806674957, 0.0, 31.759528498649598, 0.0, 0.0, 0.0], 'rewardMean': 0.6757413034674005, 'totalEpisodes': 12, 'stepsPerEpisode': 906, 'rewardPerEpisode': 646.4117830062656
'totalSteps': 3840, 'rewardStep': 0.8652607947489028, 'errorList': [], 'lossList': [0.0, -1.4764565551280975, 0.0, 40.96969671487808, 0.0, 0.0, 0.0], 'rewardMean': 0.7389144672279012, 'totalEpisodes': 15, 'stepsPerEpisode': 487, 'rewardPerEpisode': 391.6808889913718
'totalSteps': 5120, 'rewardStep': 0.6974168416651085, 'errorList': [], 'lossList': [0.0, -1.4657105165719986, 0.0, 36.554793286323545, 0.0, 0.0, 0.0], 'rewardMean': 0.728540060837203, 'totalEpisodes': 19, 'stepsPerEpisode': 67, 'rewardPerEpisode': 43.45745369553245
'totalSteps': 6400, 'rewardStep': 0.8955107875603863, 'errorList': [], 'lossList': [0.0, -1.444583057165146, 0.0, 83.57219974517822, 0.0, 0.0, 0.0], 'rewardMean': 0.7619342061818397, 'totalEpisodes': 31, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.6235543135737336
'totalSteps': 7680, 'rewardStep': 0.5602531288858201, 'errorList': [], 'lossList': [0.0, -1.4324890780448913, 0.0, 166.73516147613526, 0.0, 0.0, 0.0], 'rewardMean': 0.7283206932991698, 'totalEpisodes': 64, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.651995353926347
'totalSteps': 8960, 'rewardStep': 0.6084825461512463, 'errorList': [], 'lossList': [0.0, -1.4185702395439148, 0.0, 131.30303104400636, 0.0, 0.0, 0.0], 'rewardMean': 0.7112009579923236, 'totalEpisodes': 116, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.747181815439163
'totalSteps': 10240, 'rewardStep': 0.763347468777655, 'errorList': [], 'lossList': [0.0, -1.4180580312013626, 0.0, 74.77303121566773, 0.0, 0.0, 0.0], 'rewardMean': 0.71771927184049, 'totalEpisodes': 146, 'stepsPerEpisode': 48, 'rewardPerEpisode': 32.830255368725496
'totalSteps': 11520, 'rewardStep': 0.6179753044108153, 'errorList': [], 'lossList': [0.0, -1.420209919810295, 0.0, 48.533095359802246, 0.0, 0.0, 0.0], 'rewardMean': 0.7066366087927484, 'totalEpisodes': 167, 'stepsPerEpisode': 41, 'rewardPerEpisode': 29.522861721589436
'totalSteps': 12800, 'rewardStep': 0.4253472277155885, 'errorList': [], 'lossList': [0.0, -1.411567456126213, 0.0, 47.251089582443235, 0.0, 0.0, 0.0], 'rewardMean': 0.6785076706850324, 'totalEpisodes': 183, 'stepsPerEpisode': 86, 'rewardPerEpisode': 63.29447900669744
'totalSteps': 14080, 'rewardStep': 0.7800622497002323, 'errorList': [], 'lossList': [0.0, -1.4154552352428436, 0.0, 19.51219755411148, 0.0, 0.0, 0.0], 'rewardMean': 0.7062038108646544, 'totalEpisodes': 193, 'stepsPerEpisode': 133, 'rewardPerEpisode': 103.91776823855848
'totalSteps': 15360, 'rewardStep': 0.8162515595594431, 'errorList': [], 'lossList': [0.0, -1.4157923674583435, 0.0, 18.732485678195953, 0.0, 0.0, 0.0], 'rewardMean': 0.7029907909175197, 'totalEpisodes': 203, 'stepsPerEpisode': 91, 'rewardPerEpisode': 73.99953172744584
'totalSteps': 16640, 'rewardStep': 0.785451856561829, 'errorList': [], 'lossList': [0.0, -1.412687726020813, 0.0, 8.251241986751557, 0.0, 0.0, 0.0], 'rewardMean': 0.6950098970988126, 'totalEpisodes': 210, 'stepsPerEpisode': 126, 'rewardPerEpisode': 109.07772163983866
'totalSteps': 17920, 'rewardStep': 0.6002326064700165, 'errorList': [], 'lossList': [0.0, -1.4058575409650802, 0.0, 8.529098155498504, 0.0, 0.0, 0.0], 'rewardMean': 0.6852914735793032, 'totalEpisodes': 215, 'stepsPerEpisode': 214, 'rewardPerEpisode': 160.5564767577852
'totalSteps': 19200, 'rewardStep': 0.6454970884785347, 'errorList': [], 'lossList': [0.0, -1.3817092216014861, 0.0, 13.864254003763198, 0.0, 0.0, 0.0], 'rewardMean': 0.6602901036711181, 'totalEpisodes': 216, 'stepsPerEpisode': 759, 'rewardPerEpisode': 552.2812184742386
'totalSteps': 20480, 'rewardStep': 0.8980313606875976, 'errorList': [], 'lossList': [0.0, -1.3394828087091446, 0.0, 7.806881780922413, 0.0, 0.0, 0.0], 'rewardMean': 0.6940679268512958, 'totalEpisodes': 216, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.9002296014514
'totalSteps': 21760, 'rewardStep': 0.819930843716397, 'errorList': [], 'lossList': [0.0, -1.2897936540842057, 0.0, 6.3578700172156095, 0.0, 0.0, 0.0], 'rewardMean': 0.7152127566078109, 'totalEpisodes': 216, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.8996085040214
'totalSteps': 23040, 'rewardStep': 0.955108782827276, 'errorList': [0.03277867766649801, 0.0268103411273825, 0.050639860609963734, 0.044884813819349875, 0.05238287537166481, 0.056627031595586234, 0.044445358287612834, 0.04457010765841131, 0.05088186526580179, 0.03283893486590598, 0.07724836800507935, 0.037147784511259566, 0.017475007780991595, 0.06487071271537084, 0.020927701326611377, 0.036730711503805205, 0.03661154457513057, 0.06019335818179248, 0.046307357121079994, 0.029911194257998524, 0.07490498912422569, 0.0487913388188574, 0.026868723894839235, 0.023389422478128315, 0.006719689852682954, 0.02346813139251547, 0.023239528704518033, 0.040769069228698655, 0.043783171885264, 0.036172580452681465, 0.03359467812913357, 0.0394952079371036, 0.02541000892078043, 0.037259181469227826, 0.07375338936218313, 0.03061000882149032, 0.076125405136311, 0.02008502626201817, 0.0038581009087855565, 0.04602007753216215, 0.041085768975786016, 0.0429141356844989, 0.043953688279319066, 0.08264798246995315, 0.08063655472154704, 0.04622092346490302, 0.01879231913667659, 0.056031030612156495, 0.03707491364955567, 0.05753119113466927], 'lossList': [0.0, -1.254203315973282, 0.0, 4.4545163752511145, 0.0, 0.0, 0.0], 'rewardMean': 0.734388888012773, 'totalEpisodes': 216, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1157.9771812424653, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=73.03
