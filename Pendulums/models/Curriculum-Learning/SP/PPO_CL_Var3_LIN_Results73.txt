#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 73
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.49127290600844203, 'errorList': [], 'lossList': [0.0, -1.4226862114667893, 0.0, 28.317540403604507, 0.0, 0.0, 0.0], 'rewardMean': 0.6951016609175912, 'totalEpisodes': 16, 'stepsPerEpisode': 46, 'rewardPerEpisode': 30.643637452362015
'totalSteps': 3840, 'rewardStep': 0.9321319519293474, 'errorList': [], 'lossList': [0.0, -1.4215143710374831, 0.0, 33.20662166953087, 0.0, 0.0, 0.0], 'rewardMean': 0.77411175792151, 'totalEpisodes': 20, 'stepsPerEpisode': 489, 'rewardPerEpisode': 380.3135487824672
'totalSteps': 5120, 'rewardStep': 0.7516040755887863, 'errorList': [], 'lossList': [0.0, -1.414032456278801, 0.0, 27.138025817871092, 0.0, 0.0, 0.0], 'rewardMean': 0.7684848373383291, 'totalEpisodes': 21, 'stepsPerEpisode': 181, 'rewardPerEpisode': 151.86227867701675
'totalSteps': 6400, 'rewardStep': 0.7068785132365422, 'errorList': [], 'lossList': [0.0, -1.4090321797132492, 0.0, 82.36213178634644, 0.0, 0.0, 0.0], 'rewardMean': 0.7561635725179717, 'totalEpisodes': 30, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.476964250188685
'totalSteps': 7680, 'rewardStep': 0.7413870157058585, 'errorList': [], 'lossList': [0.0, -1.3930530607700349, 0.0, 163.96658210754396, 0.0, 0.0, 0.0], 'rewardMean': 0.7537008130492863, 'totalEpisodes': 54, 'stepsPerEpisode': 47, 'rewardPerEpisode': 35.38517293574793
'totalSteps': 8960, 'rewardStep': 0.7556202434890072, 'errorList': [], 'lossList': [0.0, -1.3864903873205185, 0.0, 154.3615339279175, 0.0, 0.0, 0.0], 'rewardMean': 0.7539750173978178, 'totalEpisodes': 102, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.7689370978683
'totalSteps': 10240, 'rewardStep': 0.6980665908567306, 'errorList': [], 'lossList': [0.0, -1.3824811899662017, 0.0, 67.51034143447876, 0.0, 0.0, 0.0], 'rewardMean': 0.7469864640801819, 'totalEpisodes': 135, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.104059039602841
'totalSteps': 11520, 'rewardStep': 0.8682664535661658, 'errorList': [], 'lossList': [0.0, -1.3753829795122146, 0.0, 38.36737232208252, 0.0, 0.0, 0.0], 'rewardMean': 0.7604620184675135, 'totalEpisodes': 163, 'stepsPerEpisode': 36, 'rewardPerEpisode': 24.90880537576378
'totalSteps': 12800, 'rewardStep': 0.6435549918041641, 'errorList': [], 'lossList': [0.0, -1.3650236666202544, 0.0, 29.299062914848328, 0.0, 0.0, 0.0], 'rewardMean': 0.7487713158011785, 'totalEpisodes': 177, 'stepsPerEpisode': 54, 'rewardPerEpisode': 44.51052967261358
'totalSteps': 14080, 'rewardStep': 0.8214264951501846, 'errorList': [], 'lossList': [0.0, -1.347602447271347, 0.0, 15.70468386888504, 0.0, 0.0, 0.0], 'rewardMean': 0.7410209237335229, 'totalEpisodes': 184, 'stepsPerEpisode': 82, 'rewardPerEpisode': 59.68588072608081
'totalSteps': 15360, 'rewardStep': 0.72301313650191, 'errorList': [], 'lossList': [0.0, -1.3399603921175003, 0.0, 14.21033821105957, 0.0, 0.0, 0.0], 'rewardMean': 0.7641949467828697, 'totalEpisodes': 189, 'stepsPerEpisode': 51, 'rewardPerEpisode': 42.14244825708715
'totalSteps': 16640, 'rewardStep': 0.8719337912348166, 'errorList': [], 'lossList': [0.0, -1.3429654741287231, 0.0, 10.091998034715653, 0.0, 0.0, 0.0], 'rewardMean': 0.7581751307134167, 'totalEpisodes': 194, 'stepsPerEpisode': 38, 'rewardPerEpisode': 27.788793339936603
'totalSteps': 17920, 'rewardStep': 0.8583057238943309, 'errorList': [], 'lossList': [0.0, -1.3293423080444335, 0.0, 7.882467503547669, 0.0, 0.0, 0.0], 'rewardMean': 0.7688452955439711, 'totalEpisodes': 196, 'stepsPerEpisode': 431, 'rewardPerEpisode': 387.70765633380137
'totalSteps': 19200, 'rewardStep': 0.8041388899537246, 'errorList': [], 'lossList': [0.0, -1.3248414075374604, 0.0, 7.979916689395904, 0.0, 0.0, 0.0], 'rewardMean': 0.7785713332156894, 'totalEpisodes': 198, 'stepsPerEpisode': 577, 'rewardPerEpisode': 440.0852235198822
'totalSteps': 20480, 'rewardStep': 0.8036113410464936, 'errorList': [], 'lossList': [0.0, -1.3100587195158004, 0.0, 3.97377816259861, 0.0, 0.0, 0.0], 'rewardMean': 0.7847937657497528, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.5526820356284
'totalSteps': 21760, 'rewardStep': 0.9112249974638628, 'errorList': [], 'lossList': [0.0, -1.271421480178833, 0.0, 2.3363155322521925, 0.0, 0.0, 0.0], 'rewardMean': 0.8003542411472384, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.8394849449799
'totalSteps': 23040, 'rewardStep': 0.8089115281530479, 'errorList': [], 'lossList': [0.0, -1.2257861971855164, 0.0, 2.566331464201212, 0.0, 0.0, 0.0], 'rewardMean': 0.81143873487687, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.793834289055
'totalSteps': 24320, 'rewardStep': 0.8717635192124954, 'errorList': [], 'lossList': [0.0, -1.1828175538778305, 0.0, 1.952669587060809, 0.0, 0.0, 0.0], 'rewardMean': 0.8117884414415031, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1178.784206059911
'totalSteps': 25600, 'rewardStep': 0.9568104396598058, 'errorList': [0.06215122812335603, 0.07231638715623621, 0.04339219239712636, 0.0661638201757157, 0.05521787879248578, 0.04857015440003762, 0.04024759811726795, 0.060819525331947004, 0.04542884909843208, 0.07576027167044218, 0.04223025388031215, 0.06020038634882158, 0.05433738484039173, 0.05963924537774688, 0.06196410463096486, 0.038781009130913535, 0.0656916541453882, 0.059796928046802575, 0.06859553784190746, 0.04424559658914314, 0.05814527999540669, 0.04841808137025228, 0.04203372552546138, 0.051263480668061925, 0.05687188492702138, 0.06656895094712399, 0.041110203770595016, 0.045593569432473065, 0.06571041403715235, 0.042958555393163934, 0.04282404288340201, 0.06314356480526567, 0.05420030519489435, 0.04002812688756383, 0.0573340010083359, 0.03873016711199773, 0.06330452464592706, 0.05583117601356891, 0.04230851522981105, 0.039900884228928776, 0.07083495411785001, 0.04123924524418661, 0.06126514589981792, 0.04087581863628791, 0.06678286954978674, 0.044113398953237244, 0.038492120544014026, 0.045142066095313536, 0.040559150136948774, 0.039670414213942407], 'lossList': [0.0, -1.1497455430030823, 0.0, 1.402744260504842, 0.0, 0.0, 0.0], 'rewardMean': 0.8431139862270672, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1204.2107334747284, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.31
