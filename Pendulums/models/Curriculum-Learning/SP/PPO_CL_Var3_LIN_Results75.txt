#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 75
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.6793924193387717, 'errorList': [], 'lossList': [0.0, -1.4045557922124863, 0.0, 33.020765695571896, 0.0, 0.0, 0.0], 'rewardMean': 0.5618075801571447, 'totalEpisodes': 60, 'stepsPerEpisode': 21, 'rewardPerEpisode': 12.011571726618387
'totalSteps': 3840, 'rewardStep': 0.24419872003422516, 'errorList': [], 'lossList': [0.0, -1.381933354139328, 0.0, 41.76608085632324, 0.0, 0.0, 0.0], 'rewardMean': 0.4559379601161715, 'totalEpisodes': 80, 'stepsPerEpisode': 91, 'rewardPerEpisode': 57.60663766186872
'totalSteps': 5120, 'rewardStep': 0.8082183890503488, 'errorList': [], 'lossList': [0.0, -1.3572419917583465, 0.0, 49.7876531791687, 0.0, 0.0, 0.0], 'rewardMean': 0.5440080673497159, 'totalEpisodes': 95, 'stepsPerEpisode': 66, 'rewardPerEpisode': 54.284745577591146
'totalSteps': 6400, 'rewardStep': 0.8210596493539141, 'errorList': [], 'lossList': [0.0, -1.3387699395418167, 0.0, 64.63700466156006, 0.0, 0.0, 0.0], 'rewardMean': 0.5994183837505556, 'totalEpisodes': 110, 'stepsPerEpisode': 30, 'rewardPerEpisode': 20.319827560244338
'totalSteps': 7680, 'rewardStep': 0.9706057134287491, 'errorList': [], 'lossList': [0.0, -1.3236871254444122, 0.0, 103.3408396911621, 0.0, 0.0, 0.0], 'rewardMean': 0.6612829386969211, 'totalEpisodes': 131, 'stepsPerEpisode': 50, 'rewardPerEpisode': 43.989486916561255
'totalSteps': 8960, 'rewardStep': 0.8023965104364079, 'errorList': [], 'lossList': [0.0, -1.320227355360985, 0.0, 71.3877463722229, 0.0, 0.0, 0.0], 'rewardMean': 0.6814420203739907, 'totalEpisodes': 144, 'stepsPerEpisode': 97, 'rewardPerEpisode': 82.27429806031384
'totalSteps': 10240, 'rewardStep': 0.4751708663315828, 'errorList': [], 'lossList': [0.0, -1.3253664237260818, 0.0, 46.366633129119876, 0.0, 0.0, 0.0], 'rewardMean': 0.6556581261186897, 'totalEpisodes': 157, 'stepsPerEpisode': 107, 'rewardPerEpisode': 83.10988910277855
'totalSteps': 11520, 'rewardStep': 0.7603102455650474, 'errorList': [], 'lossList': [0.0, -1.3203038275241852, 0.0, 18.90531556844711, 0.0, 0.0, 0.0], 'rewardMean': 0.6672861393905072, 'totalEpisodes': 164, 'stepsPerEpisode': 68, 'rewardPerEpisode': 59.51939017156089
'totalSteps': 12800, 'rewardStep': 0.7601387783853906, 'errorList': [], 'lossList': [0.0, -1.298543219566345, 0.0, 13.779987118244172, 0.0, 0.0, 0.0], 'rewardMean': 0.6765714032899954, 'totalEpisodes': 168, 'stepsPerEpisode': 89, 'rewardPerEpisode': 75.59383981623638
'totalSteps': 14080, 'rewardStep': 0.5209290087604642, 'errorList': [], 'lossList': [0.0, -1.3003305041790008, 0.0, 9.083322637081146, 0.0, 0.0, 0.0], 'rewardMean': 0.6842420300684902, 'totalEpisodes': 173, 'stepsPerEpisode': 134, 'rewardPerEpisode': 110.10449261492617
'totalSteps': 15360, 'rewardStep': 0.5907440805358556, 'errorList': [], 'lossList': [0.0, -1.29147123336792, 0.0, 6.2272019320726395, 0.0, 0.0, 0.0], 'rewardMean': 0.6753771961881985, 'totalEpisodes': 177, 'stepsPerEpisode': 598, 'rewardPerEpisode': 484.9154612377028
'totalSteps': 16640, 'rewardStep': 0.9615367652241809, 'errorList': [3.0258934048449726, 0.5511521748143231, 5.261081789965529, 1.3516995102354699, 1.9045100875071808, 4.951262153036478, 2.8311059816974216, 2.351487909328885, 3.0165482632664586, 1.2874751107128701, 0.4053279997687846, 1.1367423451844347, 1.3120265008521224, 0.5985343832554371, 1.4128506713859372, 0.5526329327657468, 2.51442953830688, 0.36562402521107945, 4.063471189276214, 0.7815977621685285, 3.540932156403233, 0.45713496948353827, 3.701233044194634, 1.8402315693815896, 3.189278641541655, 2.9836056365122525, 0.6481668398905576, 0.2919006601182118, 2.1947880770558084, 1.4364835156566456, 3.1540043249453538, 4.057150847830593, 0.2691343540512414, 1.3492378598285557, 2.675520437372495, 4.625433615700361, 1.4596254424261816, 2.643008938987997, 0.6022430276318191, 0.5112129774617034, 4.138733962705831, 3.9402905118467673, 2.616792249869258, 3.2824726556326755, 3.932049150283031, 4.990635056195517, 4.4107344962138955, 0.9126223065029243, 0.9777762348710861, 1.4625856927733938], 'lossList': [0.0, -1.2897760993242264, 0.0, 5.232568174600601, 0.0, 0.0, 0.0], 'rewardMean': 0.747111000707194, 'totalEpisodes': 182, 'stepsPerEpisode': 193, 'rewardPerEpisode': 180.26075346086398, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7616038666472266, 'errorList': [], 'lossList': [0.0, -1.3044547426700592, 0.0, 3.7854175716638565, 0.0, 0.0, 0.0], 'rewardMean': 0.7424495484668819, 'totalEpisodes': 183, 'stepsPerEpisode': 452, 'rewardPerEpisode': 348.98743056125954
'totalSteps': 19200, 'rewardStep': 0.8722643261228377, 'errorList': [], 'lossList': [0.0, -1.2990215194225312, 0.0, 5.870572333931923, 0.0, 0.0, 0.0], 'rewardMean': 0.7475700161437743, 'totalEpisodes': 185, 'stepsPerEpisode': 535, 'rewardPerEpisode': 477.9138028267953
'totalSteps': 20480, 'rewardStep': 0.9496439239477626, 'errorList': [0.09144195963725663, 0.09804896813074188, 0.10485167645175307, 0.07421801009736112, 0.09255895073937216, 0.08996228407031027, 0.09058510821168031, 0.10810237070097947, 0.08721508055158773, 0.08238046973819876, 0.07612292116259761, 0.09462882338226535, 0.09471980209797239, 0.08651701074246586, 0.09395707391190702, 0.09087547579001165, 0.07270371044376041, 0.08718313848679853, 0.08145760457589447, 0.0920912296027051, 0.0935183096401668, 0.08362372648628753, 0.10703274788018574, 0.07397638114712407, 0.07101064364511867, 0.09037157768816749, 0.08822214241905717, 0.07038548731444003, 0.09060144728196391, 0.07879937779211257, 0.09957378134196446, 0.08735912468566999, 0.11754520709101965, 0.10340647018256124, 0.07520336785981642, 0.0783341905956968, 0.09760629208852306, 0.07952169562037475, 0.09529522001174927, 0.09575867214006292, 0.08310803617600615, 0.07040305288444959, 0.08453277375876904, 0.11223238164212886, 0.079152514267209, 0.08203141605734234, 0.08646382077623238, 0.09656093577995307, 0.0850956754501412, 0.08700135071426703], 'lossList': [0.0, -1.3135119205713273, 0.0, 1.6503895944356919, 0.0, 0.0, 0.0], 'rewardMean': 0.7454738371956757, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.6555085776847, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=91.3
