#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 77
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.7523777640448377, 'errorList': [], 'lossList': [0.0, -1.4258032315969467, 0.0, 33.47848267555237, 0.0, 0.0, 0.0], 'rewardMean': 0.7840075529142736, 'totalEpisodes': 71, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.77357337025995
'totalSteps': 3840, 'rewardStep': 0.9387296010154247, 'errorList': [], 'lossList': [0.0, -1.4100033849477769, 0.0, 43.870957460403446, 0.0, 0.0, 0.0], 'rewardMean': 0.8355815689479907, 'totalEpisodes': 95, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.061092173783685
'totalSteps': 5120, 'rewardStep': 0.4485035255548169, 'errorList': [], 'lossList': [0.0, -1.3860734742879868, 0.0, 40.620096440315244, 0.0, 0.0, 0.0], 'rewardMean': 0.7388120580996973, 'totalEpisodes': 107, 'stepsPerEpisode': 85, 'rewardPerEpisode': 60.728202041611816
'totalSteps': 6400, 'rewardStep': 0.6537538663545819, 'errorList': [], 'lossList': [0.0, -1.3732463765144347, 0.0, 44.70427833557129, 0.0, 0.0, 0.0], 'rewardMean': 0.7218004197506742, 'totalEpisodes': 118, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.640778610637327
'totalSteps': 7680, 'rewardStep': 0.9453163981540283, 'errorList': [], 'lossList': [0.0, -1.3500756323337555, 0.0, 73.53859309196473, 0.0, 0.0, 0.0], 'rewardMean': 0.7590530828178998, 'totalEpisodes': 128, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.30502380484403
'totalSteps': 8960, 'rewardStep': 0.6281007542010524, 'errorList': [], 'lossList': [0.0, -1.3426190090179444, 0.0, 87.16439401626587, 0.0, 0.0, 0.0], 'rewardMean': 0.7403456073012074, 'totalEpisodes': 143, 'stepsPerEpisode': 135, 'rewardPerEpisode': 101.98041804892188
'totalSteps': 10240, 'rewardStep': 0.5082292990364833, 'errorList': [], 'lossList': [0.0, -1.3475907766819, 0.0, 71.98194862365723, 0.0, 0.0, 0.0], 'rewardMean': 0.7113310687681169, 'totalEpisodes': 155, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.2249205193805395
'totalSteps': 11520, 'rewardStep': 0.890269527722982, 'errorList': [], 'lossList': [0.0, -1.3537021201848984, 0.0, 65.53233117103576, 0.0, 0.0, 0.0], 'rewardMean': 0.7312131197631019, 'totalEpisodes': 164, 'stepsPerEpisode': 28, 'rewardPerEpisode': 24.11331590286293
'totalSteps': 12800, 'rewardStep': 0.9705638910192274, 'errorList': [23.252748411980637, 48.411397712147206, 1.3964598606435563, 0.6479384195805082, 37.105841452703125, 44.278299853548354, 2.8506204556998718, 56.351077758411925, 57.25070470952705, 1.1065508365026837, 0.5062583203087285, 23.710469142123937, 32.5348985397321, 12.750357669420156, 6.958998876585865, 16.142084300600917, 2.185406707161153, 3.449927503666055, 12.324520400815922, 8.68967372366961, 4.078182354088826, 0.561625289034291, 16.64738135427273, 19.57647462006616, 49.603894789726056, 0.8971475445662067, 1.852981942752502, 17.39978604085964, 1.105322508492651, 18.258736750595094, 5.08914733346019, 21.13373195929521, 18.33080713494414, 6.7956030279674104, 25.0517676684106, 3.1163054396667262, 33.13442835426475, 5.815463225548509, 2.4602457504632183, 8.959779166305376, 23.438722381277824, 4.851436965357803, 24.983709277110343, 8.726605276006074, 12.531521885475366, 28.65472509543019, 23.417533150171707, 2.225588777591972, 3.2606760099940044, 19.449050645963393], 'lossList': [0.0, -1.3555461770296098, 0.0, 51.043811779022214, 0.0, 0.0, 0.0], 'rewardMean': 0.7551481968887145, 'totalEpisodes': 172, 'stepsPerEpisode': 41, 'rewardPerEpisode': 38.87528469536463, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.8189587284109908, 'errorList': [], 'lossList': [0.0, -1.3656024396419526, 0.0, 19.049878993034362, 0.0, 0.0, 0.0], 'rewardMean': 0.7554803355514426, 'totalEpisodes': 177, 'stepsPerEpisode': 115, 'rewardPerEpisode': 98.78650501698397
'totalSteps': 15360, 'rewardStep': 0.7112293972626615, 'errorList': [], 'lossList': [0.0, -1.384762777686119, 0.0, 8.931136545538902, 0.0, 0.0, 0.0], 'rewardMean': 0.7513654988732249, 'totalEpisodes': 180, 'stepsPerEpisode': 503, 'rewardPerEpisode': 394.4106901602148
'totalSteps': 16640, 'rewardStep': 0.6104370105242838, 'errorList': [], 'lossList': [0.0, -1.3857689327001572, 0.0, 4.441534117460251, 0.0, 0.0, 0.0], 'rewardMean': 0.7185362398241109, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 967.6132687238554
'totalSteps': 17920, 'rewardStep': 0.8440223807992008, 'errorList': [], 'lossList': [0.0, -1.3622712129354477, 0.0, 4.088002573400736, 0.0, 0.0, 0.0], 'rewardMean': 0.7580881253485493, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1047.4564852240273
'totalSteps': 19200, 'rewardStep': 0.9512255699125586, 'errorList': [0.07001449500882573, 0.05996469830985858, 0.06856525732439239, 0.0664120613086524, 0.056107659479626294, 0.04997001327113906, 0.054370987694805285, 0.044121483448820296, 0.07780154078419754, 0.04799781550201493, 0.05890764089578876, 0.045675073105405396, 0.04605949366890974, 0.04738666038195124, 0.04902932637999561, 0.04637530782323901, 0.06770167146194385, 0.08040986363495438, 0.06228092730979956, 0.03641949613041001, 0.045342178665783925, 0.06441922488446761, 0.06861034541455573, 0.061511810990289004, 0.05808886626381644, 0.04268685478770579, 0.03625960187071636, 0.05089102246897907, 0.04528339503688315, 0.06361450503994584, 0.03574981645518289, 0.052662227935803, 0.040315519776607134, 0.05414634592888011, 0.07633258291035877, 0.05283321133706959, 0.03732493012117614, 0.06995409482340421, 0.03694127094279581, 0.059693437342708905, 0.0586600498595466, 0.0671019536545312, 0.0689104361814045, 0.06041177174271871, 0.04107516484395774, 0.05471864021841753, 0.052803941117457454, 0.04433424640200712, 0.042843772395647024, 0.06792732558586013], 'lossList': [0.0, -1.3202585232257844, 0.0, 3.347389323040843, 0.0, 0.0, 0.0], 'rewardMean': 0.7878352957043468, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.7659354686116, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=87.05
