#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 78
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.39371497778487063, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4307897274367087, 'totalEpisodes': 64, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.822672783068338
'totalSteps': 3840, 'rewardStep': 0.8921399505388067, 'errorList': [], 'lossList': [0.0, -1.426083955168724, 0.0, 34.789516334533694, 0.0, 0.0, 0.0], 'rewardMean': 0.5461272832122331, 'totalEpisodes': 100, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.22659953105913
'totalSteps': 5120, 'rewardStep': 0.8987691629052774, 'errorList': [], 'lossList': [0.0, -1.4125089049339294, 0.0, 40.615207271575926, 0.0, 0.0, 0.0], 'rewardMean': 0.616655659150842, 'totalEpisodes': 119, 'stepsPerEpisode': 80, 'rewardPerEpisode': 68.59595022605251
'totalSteps': 6400, 'rewardStep': 0.5965319560787831, 'errorList': [], 'lossList': [0.0, -1.4020157498121262, 0.0, 60.46128410339355, 0.0, 0.0, 0.0], 'rewardMean': 0.6133017086388322, 'totalEpisodes': 139, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.116447058301583
'totalSteps': 7680, 'rewardStep': 0.7216658016378834, 'errorList': [], 'lossList': [0.0, -1.3869399666786193, 0.0, 77.4789206123352, 0.0, 0.0, 0.0], 'rewardMean': 0.6287822933529823, 'totalEpisodes': 156, 'stepsPerEpisode': 84, 'rewardPerEpisode': 55.07837987356307
'totalSteps': 8960, 'rewardStep': 0.6229736922385833, 'errorList': [], 'lossList': [0.0, -1.388227252960205, 0.0, 85.45645908355714, 0.0, 0.0, 0.0], 'rewardMean': 0.6280562182136825, 'totalEpisodes': 179, 'stepsPerEpisode': 73, 'rewardPerEpisode': 57.62181992865409
'totalSteps': 10240, 'rewardStep': 0.4005196145210731, 'errorList': [], 'lossList': [0.0, -1.3853046309947967, 0.0, 35.470697746276855, 0.0, 0.0, 0.0], 'rewardMean': 0.6027743733589481, 'totalEpisodes': 187, 'stepsPerEpisode': 137, 'rewardPerEpisode': 106.3341033695762
'totalSteps': 11520, 'rewardStep': 0.6509655058577437, 'errorList': [], 'lossList': [0.0, -1.3726510363817215, 0.0, 26.493127267360688, 0.0, 0.0, 0.0], 'rewardMean': 0.6075934866088277, 'totalEpisodes': 196, 'stepsPerEpisode': 52, 'rewardPerEpisode': 42.17446414789271
'totalSteps': 12800, 'rewardStep': 0.1536478146054271, 'errorList': [], 'lossList': [0.0, -1.3762857842445373, 0.0, 11.84310553073883, 0.0, 0.0, 0.0], 'rewardMean': 0.5724643453953319, 'totalEpisodes': 201, 'stepsPerEpisode': 175, 'rewardPerEpisode': 128.67330512553838
'totalSteps': 14080, 'rewardStep': 0.7887054983222926, 'errorList': [], 'lossList': [0.0, -1.3811158549785614, 0.0, 7.15499917268753, 0.0, 0.0, 0.0], 'rewardMean': 0.6119633974490741, 'totalEpisodes': 204, 'stepsPerEpisode': 673, 'rewardPerEpisode': 503.85594101405394
'totalSteps': 15360, 'rewardStep': 0.49899029522886595, 'errorList': [], 'lossList': [0.0, -1.3930608022212982, 0.0, 6.071743940114975, 0.0, 0.0, 0.0], 'rewardMean': 0.6224909291934737, 'totalEpisodes': 207, 'stepsPerEpisode': 351, 'rewardPerEpisode': 212.2040112866949
'totalSteps': 16640, 'rewardStep': 0.8563345673666527, 'errorList': [], 'lossList': [0.0, -1.373530221581459, 0.0, 5.422801812291145, 0.0, 0.0, 0.0], 'rewardMean': 0.6189103908762583, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 951.1953712422891
'totalSteps': 17920, 'rewardStep': 0.8597405436899318, 'errorList': [], 'lossList': [0.0, -1.3338973760604858, 0.0, 3.5591929003596308, 0.0, 0.0, 0.0], 'rewardMean': 0.6150075289547237, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 993.104206269737
'totalSteps': 19200, 'rewardStep': 0.9195984122993947, 'errorList': [], 'lossList': [0.0, -1.290928111076355, 0.0, 2.52349592551589, 0.0, 0.0, 0.0], 'rewardMean': 0.6473141745767848, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.5058975583
'totalSteps': 20480, 'rewardStep': 0.9601744922788159, 'errorList': [0.06727837135445397, 0.0672490339981749, 0.09065353489060114, 0.05515474321537836, 0.03883797766315487, 0.044203784500758216, 0.07891206615018045, 0.08107617007325847, 0.05858155003416779, 0.04833184002650539, 0.0644911879760832, 0.03979629655318785, 0.08080310533740359, 0.06382304155884552, 0.03890305830305517, 0.04097767296626567, 0.10706968143404341, 0.08883925171634263, 0.055212694861074656, 0.07713714552019796, 0.03930343082547417, 0.07965380596100756, 0.04036826613053571, 0.06549394244590016, 0.07858741356591802, 0.06770329681534958, 0.05226469506860342, 0.06263518658204165, 0.03675290919514398, 0.07815349293271072, 0.05192335834879912, 0.07364679877667656, 0.06030599763877306, 0.03992355871486337, 0.0626239390653977, 0.06620721225128358, 0.0667838247197971, 0.06846465750203104, 0.08218008989356503, 0.046135594900895374, 0.05876427217831418, 0.05344777352221243, 0.04529895026047744, 0.058103003227458996, 0.08843848817653224, 0.04123851623236594, 0.06265391069299206, 0.04017623081817542, 0.06873640627373893, 0.04364079970725387], 'lossList': [0.0, -1.2600105965137482, 0.0, 2.7829129549115894, 0.0, 0.0, 0.0], 'rewardMean': 0.671165043640878, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.7855888300348, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=69.6
