#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 8
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.637594130413628, 'errorList': [], 'lossList': [0.0, -1.4100495928525925, 0.0, 30.55223626613617, 0.0, 0.0, 0.0], 'rewardMean': 0.49097466097954146, 'totalEpisodes': 22, 'stepsPerEpisode': 42, 'rewardPerEpisode': 33.914617556302986
'totalSteps': 3840, 'rewardStep': 0.8799942761259258, 'errorList': [], 'lossList': [0.0, -1.38650317966938, 0.0, 45.55612933158874, 0.0, 0.0, 0.0], 'rewardMean': 0.6206478660283362, 'totalEpisodes': 37, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.23799514284418
'totalSteps': 5120, 'rewardStep': 0.6960168660494526, 'errorList': [], 'lossList': [0.0, -1.3664315140247345, 0.0, 69.7157993888855, 0.0, 0.0, 0.0], 'rewardMean': 0.6394901160336153, 'totalEpisodes': 54, 'stepsPerEpisode': 181, 'rewardPerEpisode': 137.2270565856251
'totalSteps': 6400, 'rewardStep': 0.8818974010868893, 'errorList': [], 'lossList': [0.0, -1.3502302664518355, 0.0, 87.77612277984619, 0.0, 0.0, 0.0], 'rewardMean': 0.6879715730442701, 'totalEpisodes': 79, 'stepsPerEpisode': 66, 'rewardPerEpisode': 47.8952381773218
'totalSteps': 7680, 'rewardStep': 0.662266014585774, 'errorList': [], 'lossList': [0.0, -1.3438559168577193, 0.0, 78.12785341262817, 0.0, 0.0, 0.0], 'rewardMean': 0.6836873133011875, 'totalEpisodes': 98, 'stepsPerEpisode': 85, 'rewardPerEpisode': 62.61444098040829
'totalSteps': 8960, 'rewardStep': 0.9651580963260022, 'errorList': [3.261745232605039, 2.174019889752853, 1.553954886564103, 3.7168820940221505, 2.482565181517093, 1.2883543045025951, 1.2880085654474749, 4.050085287452345, 4.361533452831641, 2.8572237135759924, 4.693574169535906, 1.8621567640910444, 5.579558844389978, 3.514345741502484, 2.576773027096675, 3.70077479751024, 4.135073878735986, 5.62900182752746, 2.072240560370811, 1.1680080471064782, 3.858106393293993, 1.4944879970921165, 1.9666368517224415, 2.65462935150919, 3.581653342035899, 4.8696127574782135, 5.563295886883011, 1.1437633061364705, 4.361400809713104, 4.017455322320332, 1.8714041247035005, 5.103286619810553, 3.431920457064882, 1.9961080083804494, 1.7240513702403848, 2.7191732198959397, 6.4024807530924575, 2.489268460751202, 5.506066216857161, 3.3496743355792047, 3.5371496303168453, 4.895827215463974, 1.2875246075391862, 5.579645795076974, 1.6694823644212722, 2.559933349566918, 3.5653703182255145, 4.259455608002288, 3.9868327794511225, 1.986996154290448], 'lossList': [0.0, -1.339891031384468, 0.0, 48.38072051048279, 0.0, 0.0, 0.0], 'rewardMean': 0.7238974251618753, 'totalEpisodes': 105, 'stepsPerEpisode': 81, 'rewardPerEpisode': 66.5505266604724, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.6040884910032627, 'errorList': [], 'lossList': [0.0, -1.3425767797231674, 0.0, 17.9809914124012, 0.0, 0.0, 0.0], 'rewardMean': 0.7089213083920487, 'totalEpisodes': 107, 'stepsPerEpisode': 336, 'rewardPerEpisode': 212.70360938532275
'totalSteps': 11520, 'rewardStep': 0.6261080885919191, 'errorList': [], 'lossList': [0.0, -1.3350390541553496, 0.0, 15.00407002210617, 0.0, 0.0, 0.0], 'rewardMean': 0.6997198395253676, 'totalEpisodes': 109, 'stepsPerEpisode': 331, 'rewardPerEpisode': 230.19841395758405
'totalSteps': 12800, 'rewardStep': 0.8277303981800929, 'errorList': [], 'lossList': [0.0, -1.32881376683712, 0.0, 25.42569352865219, 0.0, 0.0, 0.0], 'rewardMean': 0.7125208953908402, 'totalEpisodes': 113, 'stepsPerEpisode': 207, 'rewardPerEpisode': 173.08779269331774
'totalSteps': 14080, 'rewardStep': 0.813201761873724, 'errorList': [], 'lossList': [0.0, -1.3097372204065323, 0.0, 7.776907578706742, 0.0, 0.0, 0.0], 'rewardMean': 0.7594055524236671, 'totalEpisodes': 113, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 887.6543640578494
'totalSteps': 15360, 'rewardStep': 0.6447725094130405, 'errorList': [], 'lossList': [0.0, -1.2799176746606826, 0.0, 7.378151971101761, 0.0, 0.0, 0.0], 'rewardMean': 0.7601233903236083, 'totalEpisodes': 114, 'stepsPerEpisode': 525, 'rewardPerEpisode': 388.90102245356337
'totalSteps': 16640, 'rewardStep': 0.8867754049045116, 'errorList': [], 'lossList': [0.0, -1.2666984379291535, 0.0, 4.741598016470671, 0.0, 0.0, 0.0], 'rewardMean': 0.7608015032014669, 'totalEpisodes': 114, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.7974129936385
'totalSteps': 17920, 'rewardStep': 0.9839812399353777, 'errorList': [22.101964683327573, 1.266183438033305, 7.985670385707051, 0.1454859776294153, 7.9186127966421544, 12.550430555435845, 3.555849387312718, 3.8395009338415655, 3.9884441053892177, 8.589329379028056, 7.9362769261214865, 4.166537849020145, 1.80165654185805, 8.007934642490527, 1.4088032246562059, 2.913431402971752, 3.4053566717093124, 13.004472069454666, 10.602489276816137, 5.158781517329478, 9.947271612473392, 12.947475801399916, 5.573892799014027, 1.381155532657018, 10.041068387926808, 4.960336719397713, 11.92743757441304, 1.7428311636005005, 0.040934418756236074, 5.754205214327781, 5.411242428605105, 7.4481106077750585, 6.716043533773456, 5.144809053545639, 3.0582557464361693, 5.8005593479058675, 15.580791561383622, 0.35392196712880114, 9.584493578102796, 17.44326417617801, 9.725533112051007, 5.763132578677431, 7.322391591157951, 3.7904836304000344, 15.903023184920142, 3.364773040772519, 18.30885337625093, 6.078323531584582, 9.037463109630425, 3.7944983005259907], 'lossList': [0.0, -1.258367258310318, 0.0, 14.40238885641098, 0.0, 0.0, 0.0], 'rewardMean': 0.7895979405900595, 'totalEpisodes': 119, 'stepsPerEpisode': 57, 'rewardPerEpisode': 52.31083009195388, 'successfulTests': 2
'totalSteps': 19200, 'rewardStep': 0.7747802725472072, 'errorList': [], 'lossList': [0.0, -1.2519463980197907, 0.0, 14.209487974643707, 0.0, 0.0, 0.0], 'rewardMean': 0.7788862277360911, 'totalEpisodes': 123, 'stepsPerEpisode': 169, 'rewardPerEpisode': 133.93196297897478
'totalSteps': 20480, 'rewardStep': 0.8676154106170175, 'errorList': [], 'lossList': [0.0, -1.248021530508995, 0.0, 7.855771368741989, 0.0, 0.0, 0.0], 'rewardMean': 0.7994211673392155, 'totalEpisodes': 127, 'stepsPerEpisode': 227, 'rewardPerEpisode': 201.81075810906708
'totalSteps': 21760, 'rewardStep': 0.8204018474268628, 'errorList': [], 'lossList': [0.0, -1.2461773294210434, 0.0, 7.8310187458992, 0.0, 0.0, 0.0], 'rewardMean': 0.7849455424493016, 'totalEpisodes': 131, 'stepsPerEpisode': 220, 'rewardPerEpisode': 197.79235301897546
'totalSteps': 23040, 'rewardStep': 0.8498348974196802, 'errorList': [], 'lossList': [0.0, -1.2387384366989136, 0.0, 5.407663909196853, 0.0, 0.0, 0.0], 'rewardMean': 0.8095201830909433, 'totalEpisodes': 134, 'stepsPerEpisode': 258, 'rewardPerEpisode': 222.8627507209983
'totalSteps': 24320, 'rewardStep': 0.6674465587254483, 'errorList': [], 'lossList': [0.0, -1.2304786282777787, 0.0, 4.035533373355865, 0.0, 0.0, 0.0], 'rewardMean': 0.8136540301042963, 'totalEpisodes': 136, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.00970471054593
'totalSteps': 25600, 'rewardStep': 0.8383767853252405, 'errorList': [], 'lossList': [0.0, -1.2147355824708939, 0.0, 2.347705283164978, 0.0, 0.0, 0.0], 'rewardMean': 0.814718668818811, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.2633222537474
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=17920, timeSpent=87.71
