#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 80
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.892779100980045, 'errorList': [], 'lossList': [0.0, -1.3898104602098464, 0.0, 26.574597396850585, 0.0, 0.0, 0.0], 'rewardMean': 0.7517245999609478, 'totalEpisodes': 18, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.016260432224401
'totalSteps': 3840, 'rewardStep': 0.42790038705254785, 'errorList': [], 'lossList': [0.0, -1.3718113094568252, 0.0, 38.06045132637024, 0.0, 0.0, 0.0], 'rewardMean': 0.6437831956581478, 'totalEpisodes': 31, 'stepsPerEpisode': 155, 'rewardPerEpisode': 119.3030164815078
'totalSteps': 5120, 'rewardStep': 0.5321225459605066, 'errorList': [], 'lossList': [0.0, -1.375742520093918, 0.0, 34.37991442203522, 0.0, 0.0, 0.0], 'rewardMean': 0.6158680332337374, 'totalEpisodes': 37, 'stepsPerEpisode': 192, 'rewardPerEpisode': 128.24410559295097
'totalSteps': 6400, 'rewardStep': 0.6455912042419947, 'errorList': [], 'lossList': [0.0, -1.38735205411911, 0.0, 42.46678095340729, 0.0, 0.0, 0.0], 'rewardMean': 0.6218126674353888, 'totalEpisodes': 43, 'stepsPerEpisode': 206, 'rewardPerEpisode': 159.91235664688043
'totalSteps': 7680, 'rewardStep': 0.9116487522073722, 'errorList': [], 'lossList': [0.0, -1.3832926595211028, 0.0, 62.11125056266785, 0.0, 0.0, 0.0], 'rewardMean': 0.6701186815640527, 'totalEpisodes': 50, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.458033885670081
'totalSteps': 8960, 'rewardStep': 0.6045652871806402, 'errorList': [], 'lossList': [0.0, -1.3691076403856277, 0.0, 186.35786949157716, 0.0, 0.0, 0.0], 'rewardMean': 0.660753910937851, 'totalEpisodes': 81, 'stepsPerEpisode': 56, 'rewardPerEpisode': 46.9367587504667
'totalSteps': 10240, 'rewardStep': 0.48752548014688946, 'errorList': [], 'lossList': [0.0, -1.3640245741605759, 0.0, 94.94683528900147, 0.0, 0.0, 0.0], 'rewardMean': 0.6391003570889808, 'totalEpisodes': 99, 'stepsPerEpisode': 38, 'rewardPerEpisode': 29.954049107474603
'totalSteps': 11520, 'rewardStep': 0.6978347406021195, 'errorList': [], 'lossList': [0.0, -1.368446370959282, 0.0, 51.61974112510681, 0.0, 0.0, 0.0], 'rewardMean': 0.6456263997015518, 'totalEpisodes': 113, 'stepsPerEpisode': 68, 'rewardPerEpisode': 42.862940257597955
'totalSteps': 12800, 'rewardStep': 0.7322933273146779, 'errorList': [], 'lossList': [0.0, -1.3518177092075347, 0.0, 45.06503810882568, 0.0, 0.0, 0.0], 'rewardMean': 0.6542930924628644, 'totalEpisodes': 123, 'stepsPerEpisode': 121, 'rewardPerEpisode': 90.90334460540474
'totalSteps': 14080, 'rewardStep': 0.5582250528025913, 'errorList': [], 'lossList': [0.0, -1.3280715662240983, 0.0, 19.588837025165557, 0.0, 0.0, 0.0], 'rewardMean': 0.6490485878489384, 'totalEpisodes': 126, 'stepsPerEpisode': 61, 'rewardPerEpisode': 49.81055859464268
'totalSteps': 15360, 'rewardStep': 0.7572733547816103, 'errorList': [], 'lossList': [0.0, -1.3232653999328614, 0.0, 8.59300489783287, 0.0, 0.0, 0.0], 'rewardMean': 0.6354980132290949, 'totalEpisodes': 131, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.894644422115256
'totalSteps': 16640, 'rewardStep': 0.7713569590294418, 'errorList': [], 'lossList': [0.0, -1.329205520749092, 0.0, 6.711946477890015, 0.0, 0.0, 0.0], 'rewardMean': 0.6698436704267843, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 948.4457223858027
'totalSteps': 17920, 'rewardStep': 0.7865815576925443, 'errorList': [], 'lossList': [0.0, -1.3241611528396606, 0.0, 4.632775443196297, 0.0, 0.0, 0.0], 'rewardMean': 0.6952895715999882, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 978.7391384916376
'totalSteps': 19200, 'rewardStep': 0.8506639770231035, 'errorList': [], 'lossList': [0.0, -1.299726384282112, 0.0, 3.755345217883587, 0.0, 0.0, 0.0], 'rewardMean': 0.715796848878099, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1045.9481402582444
'totalSteps': 20480, 'rewardStep': 0.8421139975192626, 'errorList': [], 'lossList': [0.0, -1.2830989134311677, 0.0, 1.8187717401981354, 0.0, 0.0, 0.0], 'rewardMean': 0.708843373409288, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.4740069775664
'totalSteps': 21760, 'rewardStep': 0.7943796044841397, 'errorList': [], 'lossList': [0.0, -1.2704338747262955, 0.0, 1.2925073119252921, 0.0, 0.0, 0.0], 'rewardMean': 0.7278248051396381, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.7244010011095
'totalSteps': 23040, 'rewardStep': 0.8398543518155981, 'errorList': [], 'lossList': [0.0, -1.2517774468660354, 0.0, 1.2772018356993795, 0.0, 0.0, 0.0], 'rewardMean': 0.7630576923065089, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.4919006298169
'totalSteps': 24320, 'rewardStep': 0.7126872375226762, 'errorList': [], 'lossList': [0.0, -1.2181305712461472, 0.0, 0.806501146145165, 0.0, 0.0, 0.0], 'rewardMean': 0.7645429419985645, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.6835328230172
'totalSteps': 25600, 'rewardStep': 0.9337072111295296, 'errorList': [0.07333648334702203, 0.17425105464236718, 0.11294801789450293, 0.1331149346077815, 0.10895652999266839, 0.0636167925569041, 0.10817717062759198, 0.09996097044746304, 0.054576556210842446, 0.0959970132647478, 0.052556213923616105, 0.0805140056704944, 0.10967539972067426, 0.10468787417535037, 0.05982828289871215, 0.10033749962104317, 0.10824907730331482, 0.0850874055841779, 0.20137500410970155, 0.039443925025944286, 0.20358248348064478, 0.1232609419792718, 0.17354512719874698, 0.0970709900980569, 0.060826790165428406, 0.13749119654768585, 0.055397579234779226, 0.13877800625473374, 0.19675179159539552, 0.06617388796600464, 0.06567352852909843, 0.09192182114757355, 0.15454411891210004, 0.16316465131639685, 0.051268834587010266, 0.22355927916752152, 0.03903182634576879, 0.1159906580595688, 0.11952672509124314, 0.1143629497858726, 0.04685911440429524, 0.06803390509986405, 0.07104815631060787, 0.10104473442715328, 0.10212033584556288, 0.12617961637655445, 0.1485953527209001, 0.07857648164434408, 0.14017462400665356, 0.08954851573546978], 'lossList': [0.0, -1.1864110827445984, 0.0, 1.1113565826416016, 0.0, 0.0, 0.0], 'rewardMean': 0.7846843303800497, 'totalEpisodes': 131, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.3265774941476, 'successfulTests': 47
#maxSuccessfulTests=47, maxSuccessfulTestsAtStep=25600, timeSpent=78.64
