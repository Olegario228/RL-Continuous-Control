#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 82
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.40440566554821006, 'errorList': [], 'lossList': [0.0, -1.4247208267450333, 0.0, 28.214349656105043, 0.0, 0.0, 0.0], 'rewardMean': 0.5990878333641806, 'totalEpisodes': 24, 'stepsPerEpisode': 133, 'rewardPerEpisode': 72.062601269896
'totalSteps': 3840, 'rewardStep': 0.6743450920729368, 'errorList': [], 'lossList': [0.0, -1.427193997502327, 0.0, 43.459392623901365, 0.0, 0.0, 0.0], 'rewardMean': 0.6241735862670993, 'totalEpisodes': 36, 'stepsPerEpisode': 31, 'rewardPerEpisode': 22.896583730291802
'totalSteps': 5120, 'rewardStep': 0.11261140086587829, 'errorList': [], 'lossList': [0.0, -1.4168859648704528, 0.0, 39.76725923538208, 0.0, 0.0, 0.0], 'rewardMean': 0.49628303991679407, 'totalEpisodes': 48, 'stepsPerEpisode': 86, 'rewardPerEpisode': 50.488065675655335
'totalSteps': 6400, 'rewardStep': 0.7745104074832979, 'errorList': [], 'lossList': [0.0, -1.4167788195610047, 0.0, 49.64524154663086, 0.0, 0.0, 0.0], 'rewardMean': 0.5519285134300949, 'totalEpisodes': 60, 'stepsPerEpisode': 24, 'rewardPerEpisode': 18.547692678919447
'totalSteps': 7680, 'rewardStep': 0.9423458205840303, 'errorList': [], 'lossList': [0.0, -1.4217022997140885, 0.0, 76.72862873077392, 0.0, 0.0, 0.0], 'rewardMean': 0.6169980646224174, 'totalEpisodes': 70, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.918483155356654
'totalSteps': 8960, 'rewardStep': 0.682173383711929, 'errorList': [], 'lossList': [0.0, -1.4252311319112778, 0.0, 115.83433864593506, 0.0, 0.0, 0.0], 'rewardMean': 0.6263088244923475, 'totalEpisodes': 88, 'stepsPerEpisode': 195, 'rewardPerEpisode': 145.20099191315995
'totalSteps': 10240, 'rewardStep': 0.6049567921024457, 'errorList': [], 'lossList': [0.0, -1.4304305398464203, 0.0, 52.15228325843811, 0.0, 0.0, 0.0], 'rewardMean': 0.6236398204436099, 'totalEpisodes': 103, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.1948313071394145
'totalSteps': 11520, 'rewardStep': 0.8879601645450586, 'errorList': [], 'lossList': [0.0, -1.436256963610649, 0.0, 44.26126153945923, 0.0, 0.0, 0.0], 'rewardMean': 0.6530087475659931, 'totalEpisodes': 116, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.838779857337778
'totalSteps': 12800, 'rewardStep': 0.6943929222432523, 'errorList': [], 'lossList': [0.0, -1.423019963502884, 0.0, 24.9067799282074, 0.0, 0.0, 0.0], 'rewardMean': 0.6571471650337191, 'totalEpisodes': 123, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.90441731771272
'totalSteps': 14080, 'rewardStep': 0.6458221108454919, 'errorList': [], 'lossList': [0.0, -1.4119043856859208, 0.0, 9.93708638548851, 0.0, 0.0, 0.0], 'rewardMean': 0.6423523760002531, 'totalEpisodes': 126, 'stepsPerEpisode': 302, 'rewardPerEpisode': 235.2335678266203
'totalSteps': 15360, 'rewardStep': 0.7901602870787078, 'errorList': [], 'lossList': [0.0, -1.4084981203079223, 0.0, 37.58836180925369, 0.0, 0.0, 0.0], 'rewardMean': 0.6809278381533028, 'totalEpisodes': 130, 'stepsPerEpisode': 315, 'rewardPerEpisode': 255.7185437933591
'totalSteps': 16640, 'rewardStep': 0.8733999098942415, 'errorList': [], 'lossList': [0.0, -1.3906939899921418, 0.0, 6.396179091334343, 0.0, 0.0, 0.0], 'rewardMean': 0.7008333199354333, 'totalEpisodes': 133, 'stepsPerEpisode': 218, 'rewardPerEpisode': 167.80006849654046
'totalSteps': 17920, 'rewardStep': 0.8618965297817455, 'errorList': [], 'lossList': [0.0, -1.361076802611351, 0.0, 5.221137876212597, 0.0, 0.0, 0.0], 'rewardMean': 0.77576183282702, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.1053605380994
'totalSteps': 19200, 'rewardStep': 0.9112033346622562, 'errorList': [], 'lossList': [0.0, -1.322372270822525, 0.0, 4.159511432796717, 0.0, 0.0, 0.0], 'rewardMean': 0.789431125544916, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.6627447739033
'totalSteps': 20480, 'rewardStep': 0.9646372205465074, 'errorList': [0.0332151137221995, 0.023806633815862167, 0.016364270351783273, 0.037946676526318654, 0.023041098299194533, 0.01410651024411304, 0.0191799567175821, 0.026014342571501665, 0.01442177611178965, 0.008705719632485476, 0.02655270218704962, 0.023352227731968586, 0.03303032712702616, 0.013854272094812565, 0.02331791559009128, 0.025711852157480782, 0.023349889381380145, 0.01212249907132583, 0.024013192532653282, 0.0046224653494867434, 0.022181709816929913, 0.02653757866449793, 0.026253314322297282, 0.01661322786748465, 0.04571040419096561, 0.02436172638301554, 0.009406719540562603, 0.04882666622976495, 0.03489852993923331, 0.01677958033619756, 0.008116592378718548, 0.0030991417027757184, 0.04976154159916241, 0.03768802268403766, 0.023973873644993527, 0.022018006054202628, 0.02337160160767637, 0.020584855989253872, 0.024933363575600605, 0.02801915879438926, 0.026703630302539288, 0.007648018585775444, 0.026967209524862076, 0.010376088513297044, 0.02756839401317867, 0.023706531339723343, 0.0061211352882837786, 0.006857905989985097, 0.02131325645581975, 0.014716139503418787], 'lossList': [0.0, -1.2646393698453904, 0.0, 3.8077300780639054, 0.0, 0.0, 0.0], 'rewardMean': 0.7916602655411636, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.6171735422795, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=68.55
