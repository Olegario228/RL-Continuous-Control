#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 84
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8650779905923909, 'errorList': [], 'lossList': [0.0, -1.415137374997139, 0.0, 26.14956648826599, 0.0, 0.0, 0.0], 'rewardMean': 0.6145740481839898, 'totalEpisodes': 22, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.814517373555972
'totalSteps': 3840, 'rewardStep': 0.6140257931329653, 'errorList': [], 'lossList': [0.0, -1.4251534163951873, 0.0, 29.458484873771667, 0.0, 0.0, 0.0], 'rewardMean': 0.614391296500315, 'totalEpisodes': 35, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.36370281516811
'totalSteps': 5120, 'rewardStep': 0.7117088459011681, 'errorList': [], 'lossList': [0.0, -1.4167477017641068, 0.0, 40.766426992416385, 0.0, 0.0, 0.0], 'rewardMean': 0.6387206838505283, 'totalEpisodes': 43, 'stepsPerEpisode': 42, 'rewardPerEpisode': 36.04753179537375
'totalSteps': 6400, 'rewardStep': 0.44904192017173616, 'errorList': [], 'lossList': [0.0, -1.405839318037033, 0.0, 26.235940148830412, 0.0, 0.0, 0.0], 'rewardMean': 0.6007849311147698, 'totalEpisodes': 48, 'stepsPerEpisode': 349, 'rewardPerEpisode': 272.77088517607393
'totalSteps': 7680, 'rewardStep': 0.7260910685573403, 'errorList': [], 'lossList': [0.0, -1.4030312091112136, 0.0, 61.92248093605041, 0.0, 0.0, 0.0], 'rewardMean': 0.6216692873551982, 'totalEpisodes': 57, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.189765442942626
'totalSteps': 8960, 'rewardStep': 0.7460450535205598, 'errorList': [], 'lossList': [0.0, -1.4019126033782958, 0.0, 205.2811986541748, 0.0, 0.0, 0.0], 'rewardMean': 0.63943725395025, 'totalEpisodes': 92, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.522762074474473
'totalSteps': 10240, 'rewardStep': 0.9702152909702942, 'errorList': [276.82231478367794, 293.7408979789376, 269.73183098872914, 263.8092306561606, 249.64809901203873, 260.3524508301596, 332.58623995869834, 258.72546460525575, 243.30807223548464, 237.7881371355732, 238.55444406239064, 284.04738213283895, 186.73117770657674, 306.8241840195808, 153.99390904748438, 296.3893622796089, 292.9478262701093, 255.45584698493028, 222.625166464979, 228.13029209552076, 275.5897214291405, 266.9880409340268, 258.5885466078472, 286.68469593158403, 299.36372243784496, 308.9725091003011, 308.67409428879694, 93.01097619027189, 277.94255247312714, 253.2696294005756, 213.64362189500403, 254.30101984675008, 306.1759191631127, 271.46476000537166, 242.75844226942837, 292.63275254698624, 316.8321421187708, 321.4172652525106, 257.6461254612763, 186.32722330758713, 311.0144341505141, 294.773972852437, 317.9324427754911, 287.21791768590015, 279.3851530615575, 290.9023030369241, 314.065744801049, 243.77303359635457, 284.0163619658327, 310.3145083617265], 'lossList': [0.0, -1.3933720231056212, 0.0, 103.99723615646363, 0.0, 0.0, 0.0], 'rewardMean': 0.6807845085777554, 'totalEpisodes': 121, 'stepsPerEpisode': 69, 'rewardPerEpisode': 61.33448524552899, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9114688508685507, 'errorList': [], 'lossList': [0.0, -1.3893367409706117, 0.0, 41.45666890144348, 0.0, 0.0, 0.0], 'rewardMean': 0.7064161021656216, 'totalEpisodes': 143, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.652356186282108
'totalSteps': 12800, 'rewardStep': 0.480336031991748, 'errorList': [], 'lossList': [0.0, -1.3731850880384444, 0.0, 27.539797768592834, 0.0, 0.0, 0.0], 'rewardMean': 0.6838080951482343, 'totalEpisodes': 155, 'stepsPerEpisode': 65, 'rewardPerEpisode': 49.135857474224146
'totalSteps': 14080, 'rewardStep': 0.7428266866166606, 'errorList': [], 'lossList': [0.0, -1.3541969466209411, 0.0, 14.083968555927276, 0.0, 0.0, 0.0], 'rewardMean': 0.7216837532323415, 'totalEpisodes': 162, 'stepsPerEpisode': 52, 'rewardPerEpisode': 37.34992410637202
'totalSteps': 15360, 'rewardStep': 0.8480624318964861, 'errorList': [], 'lossList': [0.0, -1.3364622527360916, 0.0, 34.05284433364868, 0.0, 0.0, 0.0], 'rewardMean': 0.7199821973627509, 'totalEpisodes': 170, 'stepsPerEpisode': 68, 'rewardPerEpisode': 54.60014219315813
'totalSteps': 16640, 'rewardStep': 0.6878453833777884, 'errorList': [], 'lossList': [0.0, -1.320409858226776, 0.0, 7.905525976419449, 0.0, 0.0, 0.0], 'rewardMean': 0.7273641563872333, 'totalEpisodes': 175, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.395316342459337
'totalSteps': 17920, 'rewardStep': 0.7327092910443591, 'errorList': [], 'lossList': [0.0, -1.3046736007928847, 0.0, 6.933008729219437, 0.0, 0.0, 0.0], 'rewardMean': 0.7294642009015524, 'totalEpisodes': 178, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.52409301086512
'totalSteps': 19200, 'rewardStep': 0.6243946826875495, 'errorList': [], 'lossList': [0.0, -1.2770615082979202, 0.0, 4.286851266026497, 0.0, 0.0, 0.0], 'rewardMean': 0.7469994771531338, 'totalEpisodes': 180, 'stepsPerEpisode': 497, 'rewardPerEpisode': 345.86790919290087
'totalSteps': 20480, 'rewardStep': 0.9545555348655002, 'errorList': [1.094776332515574, 0.2348404064290937, 1.146447346001302, 0.18587216477344848, 0.5887408790385704, 0.9283282572245742, 0.2896357724732708, 0.1316857755466802, 0.5908254286777075, 0.8740017415622992, 0.6427050682036701, 0.488156487254843, 0.38516538726193583, 0.38593600607247974, 0.16856255413489776, 0.9562212775462684, 0.2701353683784043, 0.4322034873704745, 0.9712533574257749, 0.718421461394349, 0.18604513949495324, 0.21592622820401353, 0.75059607518389, 0.6807479446139645, 0.6204542189414144, 0.31229974976155145, 0.22630136140360776, 0.5363791304977347, 0.576182686222275, 0.8405154359001222, 0.434952328751539, 0.5049294721948497, 0.5412894063639684, 0.14258428989824365, 0.49248110228702807, 0.3050365803334189, 0.29145258393413936, 0.8897658052693129, 0.18577267646518497, 0.8473669610905235, 0.665160183807651, 0.19565232277778252, 0.214269836934371, 0.4297258528640576, 1.7840825782629006, 0.17099243987826765, 0.32802757187668796, 0.18534674250169034, 0.5432938677940307, 0.7072528822727011], 'lossList': [0.0, -1.2619980049133301, 0.0, 4.374394588470459, 0.0, 0.0, 0.0], 'rewardMean': 0.7698459237839497, 'totalEpisodes': 184, 'stepsPerEpisode': 124, 'rewardPerEpisode': 111.0039421625109, 'successfulTests': 9
'totalSteps': 21760, 'rewardStep': 0.8167111596990958, 'errorList': [], 'lossList': [0.0, -1.270089647769928, 0.0, 3.4879882597923277, 0.0, 0.0, 0.0], 'rewardMean': 0.7769125344018033, 'totalEpisodes': 185, 'stepsPerEpisode': 263, 'rewardPerEpisode': 232.94106666945595
'totalSteps': 23040, 'rewardStep': 0.837466995511292, 'errorList': [], 'lossList': [0.0, -1.253834301829338, 0.0, 2.989451888501644, 0.0, 0.0, 0.0], 'rewardMean': 0.763637704855903, 'totalEpisodes': 186, 'stepsPerEpisode': 142, 'rewardPerEpisode': 115.0587916502694
'totalSteps': 24320, 'rewardStep': 0.5897023928278553, 'errorList': [], 'lossList': [0.0, -1.2307054036855698, 0.0, 2.1867608585953713, 0.0, 0.0, 0.0], 'rewardMean': 0.7314610590518336, 'totalEpisodes': 187, 'stepsPerEpisode': 645, 'rewardPerEpisode': 503.84631112831937
'totalSteps': 25600, 'rewardStep': 0.8457560515260975, 'errorList': [], 'lossList': [0.0, -1.2174480867385864, 0.0, 1.921748065650463, 0.0, 0.0, 0.0], 'rewardMean': 0.7680030610052685, 'totalEpisodes': 188, 'stepsPerEpisode': 969, 'rewardPerEpisode': 786.5333165127414
#maxSuccessfulTests=9, maxSuccessfulTestsAtStep=20480, timeSpent=101.83
