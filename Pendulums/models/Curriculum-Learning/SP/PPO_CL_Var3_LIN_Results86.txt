#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 86
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.8202592307194753, 'errorList': [], 'lossList': [0.0, -1.4368663680553437, 0.0, 33.2147686624527, 0.0, 0.0, 0.0], 'rewardMean': 0.668526566209464, 'totalEpisodes': 13, 'stepsPerEpisode': 82, 'rewardPerEpisode': 70.79383485615665
'totalSteps': 3840, 'rewardStep': 0.911647902341948, 'errorList': [], 'lossList': [0.0, -1.4474934393167496, 0.0, 25.89280375301838, 0.0, 0.0, 0.0], 'rewardMean': 0.7495670115869587, 'totalEpisodes': 16, 'stepsPerEpisode': 545, 'rewardPerEpisode': 392.6939283723548
'totalSteps': 5120, 'rewardStep': 0.8667136761307004, 'errorList': [], 'lossList': [0.0, -1.438965847492218, 0.0, 30.000740866661072, 0.0, 0.0, 0.0], 'rewardMean': 0.778853677722894, 'totalEpisodes': 22, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.140406514846903
'totalSteps': 6400, 'rewardStep': 0.9281030509471067, 'errorList': [], 'lossList': [0.0, -1.4231798857450486, 0.0, 57.191010837554934, 0.0, 0.0, 0.0], 'rewardMean': 0.8087035523677365, 'totalEpisodes': 28, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.254850277145483
'totalSteps': 7680, 'rewardStep': 0.8035401425187028, 'errorList': [], 'lossList': [0.0, -1.4082603824138642, 0.0, 23.44621151804924, 0.0, 0.0, 0.0], 'rewardMean': 0.8078429840595641, 'totalEpisodes': 29, 'stepsPerEpisode': 1061, 'rewardPerEpisode': 821.2462680842846
'totalSteps': 8960, 'rewardStep': 0.7944805681274578, 'errorList': [], 'lossList': [0.0, -1.4072444903850556, 0.0, 204.86415409088136, 0.0, 0.0, 0.0], 'rewardMean': 0.8059340674978347, 'totalEpisodes': 53, 'stepsPerEpisode': 29, 'rewardPerEpisode': 26.583017315543245
'totalSteps': 10240, 'rewardStep': 0.8714189970465398, 'errorList': [], 'lossList': [0.0, -1.4050825655460357, 0.0, 125.76775623321534, 0.0, 0.0, 0.0], 'rewardMean': 0.8141196836914228, 'totalEpisodes': 81, 'stepsPerEpisode': 55, 'rewardPerEpisode': 49.609803516540055
'totalSteps': 11520, 'rewardStep': 0.6162854530627324, 'errorList': [], 'lossList': [0.0, -1.3985963243246078, 0.0, 55.322526874542234, 0.0, 0.0, 0.0], 'rewardMean': 0.7921381025104572, 'totalEpisodes': 95, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.43053139453471
'totalSteps': 12800, 'rewardStep': 0.6013679807130556, 'errorList': [], 'lossList': [0.0, -1.3773985695838928, 0.0, 45.896926460266116, 0.0, 0.0, 0.0], 'rewardMean': 0.7730610903307171, 'totalEpisodes': 106, 'stepsPerEpisode': 274, 'rewardPerEpisode': 205.996007192492
'totalSteps': 14080, 'rewardStep': 0.8143094331467786, 'errorList': [], 'lossList': [0.0, -1.348826266527176, 0.0, 12.4175226187706, 0.0, 0.0, 0.0], 'rewardMean': 0.8028126434754498, 'totalEpisodes': 112, 'stepsPerEpisode': 109, 'rewardPerEpisode': 81.07393630399511
'totalSteps': 15360, 'rewardStep': 0.8753636670604892, 'errorList': [], 'lossList': [0.0, -1.3294060605764388, 0.0, 9.875455800294876, 0.0, 0.0, 0.0], 'rewardMean': 0.808323087109551, 'totalEpisodes': 115, 'stepsPerEpisode': 99, 'rewardPerEpisode': 78.18082575174962
'totalSteps': 16640, 'rewardStep': 0.8050651814603469, 'errorList': [], 'lossList': [0.0, -1.3141536444425583, 0.0, 26.54222156047821, 0.0, 0.0, 0.0], 'rewardMean': 0.7976648150213911, 'totalEpisodes': 118, 'stepsPerEpisode': 176, 'rewardPerEpisode': 151.7968039619792
'totalSteps': 17920, 'rewardStep': 0.8588397871197024, 'errorList': [], 'lossList': [0.0, -1.2891145211458206, 0.0, 19.30356769680977, 0.0, 0.0, 0.0], 'rewardMean': 0.7968774261202912, 'totalEpisodes': 119, 'stepsPerEpisode': 927, 'rewardPerEpisode': 750.489835959169
'totalSteps': 19200, 'rewardStep': 0.8073265575176821, 'errorList': [], 'lossList': [0.0, -1.2572256690263748, 0.0, 4.052595155835152, 0.0, 0.0, 0.0], 'rewardMean': 0.7847997767773488, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 963.8649169948192
'totalSteps': 20480, 'rewardStep': 0.74942338663748, 'errorList': [], 'lossList': [0.0, -1.1994973808526992, 0.0, 3.211620869338512, 0.0, 0.0, 0.0], 'rewardMean': 0.7793881011892265, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.6344184175277
'totalSteps': 21760, 'rewardStep': 0.8316867143893443, 'errorList': [], 'lossList': [0.0, -1.13912695646286, 0.0, 1.7256019896268844, 0.0, 0.0, 0.0], 'rewardMean': 0.7831087158154152, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1084.2921829504232
'totalSteps': 23040, 'rewardStep': 0.9523529810761817, 'errorList': [0.18901461006545905, 0.34343351626162527, 0.21343135091626755, 0.19671925306020457, 0.16320111900826909, 0.2643048791681301, 0.1715474748528814, 0.2374602511108229, 0.21639317403582306, 0.17318934555811097, 0.24576186689619536, 0.20514782685158842, 0.24912518406491838, 0.17963648376527894, 0.15167648619667753, 0.17898588778972918, 0.2894135938251484, 0.27905401517356376, 0.29461152361199333, 0.18069103884602944, 0.1303006052786062, 0.21162858723728478, 0.18762720919497974, 0.25765944532425217, 0.17181978815982982, 0.31418080779750696, 0.2145058148730523, 0.21553128391687904, 0.18250585297839253, 0.17133911522785486, 0.2536338710530966, 0.2407341044579728, 0.3195039412200727, 0.1749785151609985, 0.2560795440763634, 0.1288898129125433, 0.2267586125947257, 0.1785315575283801, 0.23470061318782168, 0.29858196285739114, 0.2227744654893146, 0.215670031615429, 0.23706139664283823, 0.16931294505500602, 0.2510637260065696, 0.30706709669579046, 0.30231880175070575, 0.17536375641560617, 0.24892002123417645, 0.2638299167906974], 'lossList': [0.0, -1.110415723323822, 0.0, 1.8108975109085441, 0.0, 0.0, 0.0], 'rewardMean': 0.7912021142183794, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.5371860015564, 'successfulTests': 19
'totalSteps': 24320, 'rewardStep': 0.8411419255780914, 'errorList': [], 'lossList': [0.0, -1.082930829524994, 0.0, 1.138014328740537, 0.0, 0.0, 0.0], 'rewardMean': 0.8136877614699152, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.9244946137146
'totalSteps': 25600, 'rewardStep': 0.9877657932864239, 'errorList': [0.06577060140722735, 0.17698117613439593, 0.15343047482353295, 0.22588296273258124, 0.17665464085827828, 0.33781203466812043, 0.0661337383920461, 0.07640083955781218, 0.19293515547251053, 0.08118850189501703, 0.04665344232817985, 0.1505615295068957, 0.38777379374907234, 0.21428316715373347, 0.08964979488935117, 0.12983160595501334, 0.11425869707686913, 0.33423914629875057, 0.4634635093969717, 0.2040769341834591, 0.18319041168936895, 0.188405394172363, 0.11574314093906224, 0.19695931342820766, 0.23505479656685496, 0.10114130864700983, 0.17840530957517722, 0.13183098154808093, 0.2682567415785115, 0.1576376700784819, 0.1364974123451581, 0.09962758430072442, 0.13029659372384497, 0.11801247136477024, 0.060745323127612334, 0.05223359521057293, 0.24623191673706324, 0.12865600591387324, 0.15386697284334874, 0.09694158833053615, 0.19521621488678031, 0.09994541315059183, 0.23968801364991704, 0.1558257627805817, 0.11766307974476259, 0.10256277810559632, 0.2470871778447747, 0.1928983645604437, 0.17899787817784057, 0.09238019067540927], 'lossList': [0.0, -1.0524078011512756, 0.0, 1.3421900748461484, 0.0, 0.0, 0.0], 'rewardMean': 0.8523275427272521, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1213.5755025376477, 'successfulTests': 38
#maxSuccessfulTests=38, maxSuccessfulTestsAtStep=25600, timeSpent=105.05
