#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 87
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.7593786337006914, 'errorList': [], 'lossList': [0.0, -1.440846610069275, 0.0, 27.371457712650297, 0.0, 0.0, 0.0], 'rewardMean': 0.5987046129532827, 'totalEpisodes': 13, 'stepsPerEpisode': 716, 'rewardPerEpisode': 476.49668380946673
'totalSteps': 3840, 'rewardStep': 0.9141873239104769, 'errorList': [], 'lossList': [0.0, -1.4357879281044006, 0.0, 32.58772308588028, 0.0, 0.0, 0.0], 'rewardMean': 0.7038655166056808, 'totalEpisodes': 19, 'stepsPerEpisode': 77, 'rewardPerEpisode': 58.62297160027575
'totalSteps': 5120, 'rewardStep': 0.34532743065241706, 'errorList': [], 'lossList': [0.0, -1.40763065636158, 0.0, 37.7701744222641, 0.0, 0.0, 0.0], 'rewardMean': 0.6142309951173648, 'totalEpisodes': 28, 'stepsPerEpisode': 67, 'rewardPerEpisode': 39.14797759223066
'totalSteps': 6400, 'rewardStep': 0.9797349529209195, 'errorList': [], 'lossList': [0.0, -1.3917046761512757, 0.0, 60.09928277015686, 0.0, 0.0, 0.0], 'rewardMean': 0.6873317866780757, 'totalEpisodes': 36, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.8507329628673364
'totalSteps': 7680, 'rewardStep': 0.9305241689085414, 'errorList': [], 'lossList': [0.0, -1.3726183742284774, 0.0, 87.53097415924073, 0.0, 0.0, 0.0], 'rewardMean': 0.7278638503831534, 'totalEpisodes': 49, 'stepsPerEpisode': 54, 'rewardPerEpisode': 40.52376776001999
'totalSteps': 8960, 'rewardStep': 0.44384264490421077, 'errorList': [], 'lossList': [0.0, -1.3663491547107696, 0.0, 120.71641044616699, 0.0, 0.0, 0.0], 'rewardMean': 0.6872893924575901, 'totalEpisodes': 68, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.14025726195272
'totalSteps': 10240, 'rewardStep': 0.789478390491962, 'errorList': [], 'lossList': [0.0, -1.3647661292552948, 0.0, 71.49270099639892, 0.0, 0.0, 0.0], 'rewardMean': 0.7000630172118866, 'totalEpisodes': 93, 'stepsPerEpisode': 86, 'rewardPerEpisode': 73.11288016137054
'totalSteps': 11520, 'rewardStep': 0.7389901622401671, 'errorList': [], 'lossList': [0.0, -1.3660993087291717, 0.0, 40.48806082725525, 0.0, 0.0, 0.0], 'rewardMean': 0.7043882555483623, 'totalEpisodes': 107, 'stepsPerEpisode': 67, 'rewardPerEpisode': 55.43161658946397
'totalSteps': 12800, 'rewardStep': 0.9636965708625068, 'errorList': [79.76537015566797, 48.564830187405526, 39.64629704128002, 2.337257474357714, 13.335128466685676, 19.72954034705736, 40.027986777157196, 76.80191793204285, 75.1170139701633, 40.77736908883889, 48.53485745058398, 27.340570139375515, 28.393904795221122, 21.475309742972268, 19.385737472801097, 35.29359047731869, 18.068033198925818, 31.919337196654684, 98.8862789820966, 25.547631018487664, 47.45326072238601, 4.815425661589867, 1.6628263014330227, 29.86311192461846, 86.30088847940021, 4.644516279119934, 82.92841614881134, 4.31660193224032, 21.38691386963138, 37.717551829150054, 45.721907212873454, 31.311628537153087, 40.36093096934627, 57.798137231481995, 24.087432163110243, 29.79887910862479, 7.88470148567882, 31.810335103164523, 31.001611745268704, 71.8837836900505, 22.86575969986194, 6.446552656472528, 78.78916372203345, 3.1740894227748173, 37.36856567048339, 47.3792739962999, 47.79910402318783, 38.01141913569749, 31.444882601365105, 10.24115635606855], 'lossList': [0.0, -1.3580335652828217, 0.0, 39.71927987575531, 0.0, 0.0, 0.0], 'rewardMean': 0.7303190870797767, 'totalEpisodes': 117, 'stepsPerEpisode': 35, 'rewardPerEpisode': 33.00152704031869, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.6835176472616773, 'errorList': [], 'lossList': [0.0, -1.332087128162384, 0.0, 23.67359620332718, 0.0, 0.0, 0.0], 'rewardMean': 0.7548677925853571, 'totalEpisodes': 123, 'stepsPerEpisode': 37, 'rewardPerEpisode': 26.608841481633753
'totalSteps': 15360, 'rewardStep': 0.8806633260208463, 'errorList': [], 'lossList': [0.0, -1.3137356841564178, 0.0, 11.02907555103302, 0.0, 0.0, 0.0], 'rewardMean': 0.7669962618173726, 'totalEpisodes': 128, 'stepsPerEpisode': 37, 'rewardPerEpisode': 33.304568059588746
'totalSteps': 16640, 'rewardStep': 0.9508378412936366, 'errorList': [1.4051726846105563, 5.609482243741856, 0.09920555479920452, 1.5980267731123703, 1.4261871124898577, 2.0248624463703098, 0.3110299887659845, 1.3141046930615616, 0.8807169855594328, 1.7906119150392665, 0.8391687452222485, 0.4811923173669041, 0.17662023143753885, 7.923016214731576, 0.7831721094545976, 3.9220260984317785, 4.840234121288683, 1.871309174198607, 1.7066746906419548, 1.537192204260902, 1.1947479540228696, 0.3989703460744969, 0.5876207854500204, 4.255421016800492, 0.43636336351467137, 1.4958240894081158, 6.252322782842317, 2.6667917115289526, 1.1707731730597566, 0.8181109751694291, 3.748868755670879, 4.542380421729234, 0.1557251249554918, 1.095503007976393, 0.48042763763645235, 0.609453049781846, 0.717779377817177, 3.464533312060063, 3.2238392688693462, 0.9916642463057382, 0.444880111534253, 5.369779321520954, 2.5060334655832195, 0.4187255944737674, 3.8838343943172062, 0.3736246126724732, 1.8761483151932346, 0.7598033027876754, 2.9782337646309966, 1.2935971779394195], 'lossList': [0.0, -1.3048481011390687, 0.0, 5.838056215643883, 0.0, 0.0, 0.0], 'rewardMean': 0.7706613135556885, 'totalEpisodes': 131, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.543523047422835, 'successfulTests': 3
'totalSteps': 17920, 'rewardStep': 0.7430543479054907, 'errorList': [], 'lossList': [0.0, -1.2597173130512238, 0.0, 5.341735632419586, 0.0, 0.0, 0.0], 'rewardMean': 0.8104340052809957, 'totalEpisodes': 133, 'stepsPerEpisode': 567, 'rewardPerEpisode': 491.31813320187797
'totalSteps': 19200, 'rewardStep': 0.6433528587564261, 'errorList': [], 'lossList': [0.0, -1.2096299594640731, 0.0, 15.42333640217781, 0.0, 0.0, 0.0], 'rewardMean': 0.7767957958645465, 'totalEpisodes': 138, 'stepsPerEpisode': 294, 'rewardPerEpisode': 237.31962972479346
'totalSteps': 20480, 'rewardStep': 0.7638713084831502, 'errorList': [], 'lossList': [0.0, -1.1913688629865646, 0.0, 4.024492837786674, 0.0, 0.0, 0.0], 'rewardMean': 0.7601305098220074, 'totalEpisodes': 142, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.935784438193704
'totalSteps': 21760, 'rewardStep': 0.6929009850458847, 'errorList': [], 'lossList': [0.0, -1.1921953415870667, 0.0, 3.9072229152917863, 0.0, 0.0, 0.0], 'rewardMean': 0.7850363438361747, 'totalEpisodes': 145, 'stepsPerEpisode': 343, 'rewardPerEpisode': 287.5189053020735
'totalSteps': 23040, 'rewardStep': 0.9208594336859603, 'errorList': [], 'lossList': [0.0, -1.1802074575424195, 0.0, 2.2233981138467787, 0.0, 0.0, 0.0], 'rewardMean': 0.7981744481555746, 'totalEpisodes': 149, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.688011765909147
'totalSteps': 24320, 'rewardStep': 0.9149340663087315, 'errorList': [], 'lossList': [0.0, -1.1616177976131439, 0.0, 1.992824884057045, 0.0, 0.0, 0.0], 'rewardMean': 0.8157688385624311, 'totalEpisodes': 151, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.529525926012195
'totalSteps': 25600, 'rewardStep': 0.7858597218171148, 'errorList': [], 'lossList': [0.0, -1.160856350660324, 0.0, 3.1664761412143707, 0.0, 0.0, 0.0], 'rewardMean': 0.7979851536578918, 'totalEpisodes': 152, 'stepsPerEpisode': 289, 'rewardPerEpisode': 256.0181255195539
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=16640, timeSpent=104.43
