#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 88
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.5579582166007546, 'errorList': [], 'lossList': [0.0, -1.4192784088850021, 0.0, 23.66794992208481, 0.0, 0.0, 0.0], 'rewardMean': 0.6616420247739053, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 29.376893844053093
'totalSteps': 3840, 'rewardStep': 0.8956337245185434, 'errorList': [], 'lossList': [0.0, -1.4027382671833037, 0.0, 33.72859588384628, 0.0, 0.0, 0.0], 'rewardMean': 0.7396392580221179, 'totalEpisodes': 20, 'stepsPerEpisode': 377, 'rewardPerEpisode': 270.02530277548055
'totalSteps': 5120, 'rewardStep': 0.8305139201783023, 'errorList': [], 'lossList': [0.0, -1.388297793865204, 0.0, 46.62664010047913, 0.0, 0.0, 0.0], 'rewardMean': 0.762357923561164, 'totalEpisodes': 25, 'stepsPerEpisode': 77, 'rewardPerEpisode': 69.47500580856209
'totalSteps': 6400, 'rewardStep': 0.5605062788345881, 'errorList': [], 'lossList': [0.0, -1.3845483654737472, 0.0, 34.91766657590866, 0.0, 0.0, 0.0], 'rewardMean': 0.7219875946158488, 'totalEpisodes': 29, 'stepsPerEpisode': 26, 'rewardPerEpisode': 17.536384598714534
'totalSteps': 7680, 'rewardStep': 0.7751182250357541, 'errorList': [], 'lossList': [0.0, -1.3672526395320892, 0.0, 95.77568492889404, 0.0, 0.0, 0.0], 'rewardMean': 0.7308426996858332, 'totalEpisodes': 42, 'stepsPerEpisode': 49, 'rewardPerEpisode': 38.147571813911945
'totalSteps': 8960, 'rewardStep': 0.6744628683273696, 'errorList': [], 'lossList': [0.0, -1.349398120045662, 0.0, 111.30993906021118, 0.0, 0.0, 0.0], 'rewardMean': 0.7227884380631954, 'totalEpisodes': 59, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.551393524735492
'totalSteps': 10240, 'rewardStep': 0.6064057398733294, 'errorList': [], 'lossList': [0.0, -1.3433246672153474, 0.0, 107.08101697921752, 0.0, 0.0, 0.0], 'rewardMean': 0.7082406007894623, 'totalEpisodes': 79, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.8668787940952303
'totalSteps': 11520, 'rewardStep': 0.8228131068217766, 'errorList': [], 'lossList': [0.0, -1.3449182641506194, 0.0, 29.717307305336, 0.0, 0.0, 0.0], 'rewardMean': 0.7209708792374971, 'totalEpisodes': 88, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.60997384268333
'totalSteps': 12800, 'rewardStep': 0.3281919475307993, 'errorList': [], 'lossList': [0.0, -1.339090645313263, 0.0, 11.699505617618561, 0.0, 0.0, 0.0], 'rewardMean': 0.6816929860668274, 'totalEpisodes': 94, 'stepsPerEpisode': 143, 'rewardPerEpisode': 104.21687072127929
'totalSteps': 14080, 'rewardStep': 0.7854557195923997, 'errorList': [], 'lossList': [0.0, -1.3297282838821411, 0.0, 11.957668423652649, 0.0, 0.0, 0.0], 'rewardMean': 0.6837059747313616, 'totalEpisodes': 101, 'stepsPerEpisode': 49, 'rewardPerEpisode': 36.31755970023351
'totalSteps': 15360, 'rewardStep': 0.6258048051971576, 'errorList': [], 'lossList': [0.0, -1.3223461228609086, 0.0, 22.875911252498625, 0.0, 0.0, 0.0], 'rewardMean': 0.690490633591002, 'totalEpisodes': 104, 'stepsPerEpisode': 320, 'rewardPerEpisode': 258.3306505054099
'totalSteps': 16640, 'rewardStep': 0.6519582124629395, 'errorList': [], 'lossList': [0.0, -1.310278099179268, 0.0, 4.93985369861126, 0.0, 0.0, 0.0], 'rewardMean': 0.6661230823854416, 'totalEpisodes': 106, 'stepsPerEpisode': 367, 'rewardPerEpisode': 270.35845415457834
'totalSteps': 17920, 'rewardStep': 0.7764393504190257, 'errorList': [], 'lossList': [0.0, -1.2966725426912307, 0.0, 7.55587999522686, 0.0, 0.0, 0.0], 'rewardMean': 0.6607156254095139, 'totalEpisodes': 108, 'stepsPerEpisode': 247, 'rewardPerEpisode': 207.41719568671445
'totalSteps': 19200, 'rewardStep': 0.7600561985564718, 'errorList': [], 'lossList': [0.0, -1.3101595795154573, 0.0, 5.337142471075058, 0.0, 0.0, 0.0], 'rewardMean': 0.6806706173817023, 'totalEpisodes': 110, 'stepsPerEpisode': 570, 'rewardPerEpisode': 441.3098225772874
'totalSteps': 20480, 'rewardStep': 0.9324848258740147, 'errorList': [0.02825571774476947, 0.0379999145973279, 0.04445308007508659, 0.07146740368294931, 0.03705154976519553, 0.03554846330996118, 0.05190031439441902, 0.025029124357166956, 0.03019168069865452, 0.0384283014112979, 0.034388071197644005, 0.0480723094563011, 0.03782180172739151, 0.03806259065138428, 0.03390000579735358, 0.043709494094917686, 0.03538210413711114, 0.0447719115091845, 0.03382803667566766, 0.05620810687441758, 0.02468282725104563, 0.037098750099094524, 0.04107284783499766, 0.043578235612353, 0.059550291421919276, 0.05461108528929064, 0.046725734953705966, 0.05354194731649964, 0.046317207954737705, 0.0404077052842771, 0.04846285654463616, 0.027356843916279534, 0.03937984288252314, 0.024796122552954193, 0.03698085591710846, 0.05637679799796852, 0.0376528337519353, 0.03604625144932183, 0.03374190106414606, 0.02776218773457958, 0.02502775129594584, 0.0300517012756837, 0.05337466893227855, 0.02611829315083292, 0.06359063179521633, 0.04421032280350916, 0.038125564229336883, 0.03602741910741054, 0.028831789233727048, 0.051080926378349906], 'lossList': [0.0, -1.2938469523191451, 0.0, 2.3991800120472906, 0.0, 0.0, 0.0], 'rewardMean': 0.6964072774655283, 'totalEpisodes': 110, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.9050869681134, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=69.89
