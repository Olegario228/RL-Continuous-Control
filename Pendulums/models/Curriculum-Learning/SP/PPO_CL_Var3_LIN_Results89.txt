#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 89
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.9272927709297856, 'errorList': [], 'lossList': [0.0, -1.4181872618198395, 0.0, 29.16476485610008, 0.0, 0.0, 0.0], 'rewardMean': 0.7996040885365645, 'totalEpisodes': 14, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.18729448414041
'totalSteps': 3840, 'rewardStep': 0.6118503607455161, 'errorList': [], 'lossList': [0.0, -1.4206957644224167, 0.0, 25.840228893756866, 0.0, 0.0, 0.0], 'rewardMean': 0.737019512606215, 'totalEpisodes': 17, 'stepsPerEpisode': 182, 'rewardPerEpisode': 129.07934793105403
'totalSteps': 5120, 'rewardStep': 0.5499741426400955, 'errorList': [], 'lossList': [0.0, -1.4177311909198762, 0.0, 25.58519330739975, 0.0, 0.0, 0.0], 'rewardMean': 0.6902581701146852, 'totalEpisodes': 19, 'stepsPerEpisode': 281, 'rewardPerEpisode': 200.88160222483145
'totalSteps': 6400, 'rewardStep': 0.669874788470646, 'errorList': [], 'lossList': [0.0, -1.4018223124742508, 0.0, 67.63624906539917, 0.0, 0.0, 0.0], 'rewardMean': 0.6861814937858772, 'totalEpisodes': 26, 'stepsPerEpisode': 349, 'rewardPerEpisode': 245.45613733716783
'totalSteps': 7680, 'rewardStep': 0.6281974077801393, 'errorList': [], 'lossList': [0.0, -1.3983427065610885, 0.0, 93.32838605880737, 0.0, 0.0, 0.0], 'rewardMean': 0.6765174794515877, 'totalEpisodes': 37, 'stepsPerEpisode': 35, 'rewardPerEpisode': 24.719664715435314
'totalSteps': 8960, 'rewardStep': 0.7499292802004395, 'errorList': [], 'lossList': [0.0, -1.3942438763380052, 0.0, 185.1823823928833, 0.0, 0.0, 0.0], 'rewardMean': 0.6870048795585665, 'totalEpisodes': 73, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.4910409667570925
'totalSteps': 10240, 'rewardStep': 0.6239464911730819, 'errorList': [], 'lossList': [0.0, -1.3870620185136795, 0.0, 98.90159208297729, 0.0, 0.0, 0.0], 'rewardMean': 0.6791225810103809, 'totalEpisodes': 102, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.965675536217873
'totalSteps': 11520, 'rewardStep': 0.7605555363644935, 'errorList': [], 'lossList': [0.0, -1.381883150935173, 0.0, 42.04455150604248, 0.0, 0.0, 0.0], 'rewardMean': 0.6881706871608378, 'totalEpisodes': 113, 'stepsPerEpisode': 81, 'rewardPerEpisode': 69.32757861514528
'totalSteps': 12800, 'rewardStep': 0.5521583507817172, 'errorList': [], 'lossList': [0.0, -1.3661129403114318, 0.0, 48.39876922607422, 0.0, 0.0, 0.0], 'rewardMean': 0.6745694535229257, 'totalEpisodes': 125, 'stepsPerEpisode': 66, 'rewardPerEpisode': 50.06762534180751
'totalSteps': 14080, 'rewardStep': 0.8799277513222814, 'errorList': [], 'lossList': [0.0, -1.3433395063877105, 0.0, 18.363298501968384, 0.0, 0.0, 0.0], 'rewardMean': 0.6953706880408195, 'totalEpisodes': 131, 'stepsPerEpisode': 93, 'rewardPerEpisode': 79.74877302193208
'totalSteps': 15360, 'rewardStep': 0.4755088952378733, 'errorList': [], 'lossList': [0.0, -1.31243219435215, 0.0, 41.1229114151001, 0.0, 0.0, 0.0], 'rewardMean': 0.6501923004716283, 'totalEpisodes': 140, 'stepsPerEpisode': 129, 'rewardPerEpisode': 97.14983412836105
'totalSteps': 16640, 'rewardStep': 0.5041542620288504, 'errorList': [], 'lossList': [0.0, -1.301926446557045, 0.0, 31.931476905345917, 0.0, 0.0, 0.0], 'rewardMean': 0.6394226905999618, 'totalEpisodes': 149, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.790151499672079
'totalSteps': 17920, 'rewardStep': 0.3508361166764675, 'errorList': [], 'lossList': [0.0, -1.3046930181980132, 0.0, 22.090129797458648, 0.0, 0.0, 0.0], 'rewardMean': 0.619508888003599, 'totalEpisodes': 156, 'stepsPerEpisode': 161, 'rewardPerEpisode': 123.99437191659506
'totalSteps': 19200, 'rewardStep': 0.8086366218925949, 'errorList': [], 'lossList': [0.0, -1.298845803141594, 0.0, 11.447447727918625, 0.0, 0.0, 0.0], 'rewardMean': 0.6333850713457939, 'totalEpisodes': 162, 'stepsPerEpisode': 108, 'rewardPerEpisode': 89.99052179743569
'totalSteps': 20480, 'rewardStep': 0.8803083307317223, 'errorList': [], 'lossList': [0.0, -1.2708579820394517, 0.0, 5.295636734962463, 0.0, 0.0, 0.0], 'rewardMean': 0.6585961636409523, 'totalEpisodes': 168, 'stepsPerEpisode': 244, 'rewardPerEpisode': 197.6015300950183
'totalSteps': 21760, 'rewardStep': 0.8048029298067682, 'errorList': [], 'lossList': [0.0, -1.2704980677366258, 0.0, 4.188457560241222, 0.0, 0.0, 0.0], 'rewardMean': 0.664083528601585, 'totalEpisodes': 169, 'stepsPerEpisode': 337, 'rewardPerEpisode': 297.50279260395
'totalSteps': 23040, 'rewardStep': 0.8247350444399368, 'errorList': [], 'lossList': [0.0, -1.2786950933933259, 0.0, 17.6995161485672, 0.0, 0.0, 0.0], 'rewardMean': 0.6841623839282704, 'totalEpisodes': 171, 'stepsPerEpisode': 175, 'rewardPerEpisode': 146.73466287929136
'totalSteps': 24320, 'rewardStep': 0.5242917682368602, 'errorList': [], 'lossList': [0.0, -1.2663398736715317, 0.0, 1.9020711116492748, 0.0, 0.0, 0.0], 'rewardMean': 0.6605360071155072, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.2028069283652
'totalSteps': 25600, 'rewardStep': 0.9446233636735175, 'errorList': [0.025145244064085137, 0.013789915338431925, 0.012044786875412756, 0.029478234064436802, 0.01832720124220086, 0.014826400002324044, 0.011624225149513683, 0.012100856033798197, 0.020620338209350744, 0.02452098727595824, 0.01855654200538102, 0.018509574402220078, 0.01438928563123194, 0.011813382336358057, 0.013801551815237395, 0.041412666741999345, 0.013619815884903545, 0.022653457512518106, 0.013017636823583973, 0.04239577184535163, 0.020080846861473635, 0.012621015622435948, 0.04114418866135835, 0.015270489873892061, 0.01232432772436818, 0.018610463195986103, 0.04565671717296195, 0.04243834484515106, 0.025938317624599968, 0.028810412775357758, 0.03186597207447303, 0.0390541655898613, 0.05342137547346472, 0.03902070927838841, 0.05847053383288547, 0.015929100003744767, 0.01236458234526677, 0.026609402770102274, 0.01656177740557682, 0.012445513994123452, 0.0418582117535878, 0.041633489833078005, 0.01342745493792863, 0.028997420580658483, 0.01575293357321555, 0.04271774055729447, 0.013436319636954113, 0.01382341666669915, 0.04113728023172363, 0.061461648690006594], 'lossList': [0.0, -1.2396913540363312, 0.0, 1.807195013165474, 0.0, 0.0, 0.0], 'rewardMean': 0.6997825084046874, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1109.5208357766105, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.81
