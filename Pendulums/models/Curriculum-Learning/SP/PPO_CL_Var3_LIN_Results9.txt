#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 9
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8160317088184096, 'errorList': [], 'lossList': [0.0, -1.4058965462446213, 0.0, 29.001500010490417, 0.0, 0.0, 0.0], 'rewardMean': 0.590050907296999, 'totalEpisodes': 27, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.10133017825686
'totalSteps': 3840, 'rewardStep': 0.8915584321328326, 'errorList': [], 'lossList': [0.0, -1.3936732137203216, 0.0, 39.67452642440796, 0.0, 0.0, 0.0], 'rewardMean': 0.6905534155756102, 'totalEpisodes': 47, 'stepsPerEpisode': 57, 'rewardPerEpisode': 47.33320774730044
'totalSteps': 5120, 'rewardStep': 0.930212844925161, 'errorList': [391.1987891389701, 408.0081577954419, 314.48047466764865, 327.54408571749957, 383.0751182024831, 378.1317181203962, 423.93417017885264, 372.62875004604984, 294.01640129260204, 432.49586140292837, 365.981601051031, 387.11006127276124, 307.725093546678, 379.87112033678716, 374.98461719447806, 395.99047710292774, 432.37237281854027, 418.03832667829283, 388.2880633225417, 251.03077438770976, 417.0223400606089, 424.2370490610051, 401.1709070278781, 403.88024201142144, 397.1590525829743, 369.8022244080387, 412.7290952161777, 343.5007304287683, 368.98085374673514, 382.0996058536419, 326.3845420959953, 374.6559962249961, 247.84846894885467, 313.5088810475254, 425.3972233623449, 356.0623435384003, 381.6855041061131, 401.1725047435143, 428.40562011036883, 205.82268229961534, 414.879995167011, 342.70807660994683, 377.92348562155945, 319.31591000961754, 406.95770108001096, 310.4085274129234, 420.20900015061005, 384.1651009624361, 373.93997331243384, 423.9470960771497], 'lossList': [0.0, -1.3814964801073075, 0.0, 67.58029008865357, 0.0, 0.0, 0.0], 'rewardMean': 0.7504682729129979, 'totalEpisodes': 67, 'stepsPerEpisode': 43, 'rewardPerEpisode': 37.16548145662307, 'successfulTests': 0
'totalSteps': 6400, 'rewardStep': 0.4723489200725169, 'errorList': [], 'lossList': [0.0, -1.3776869076490401, 0.0, 91.78433177947998, 0.0, 0.0, 0.0], 'rewardMean': 0.6948444023449017, 'totalEpisodes': 107, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.052775458703242
'totalSteps': 7680, 'rewardStep': 0.7638669280874364, 'errorList': [], 'lossList': [0.0, -1.3733911007642745, 0.0, 48.519948358535764, 0.0, 0.0, 0.0], 'rewardMean': 0.7063481566353241, 'totalEpisodes': 128, 'stepsPerEpisode': 137, 'rewardPerEpisode': 107.84030472520887
'totalSteps': 8960, 'rewardStep': 0.5609978235638877, 'errorList': [], 'lossList': [0.0, -1.3697698074579239, 0.0, 35.78218257904053, 0.0, 0.0, 0.0], 'rewardMean': 0.6855838233394047, 'totalEpisodes': 142, 'stepsPerEpisode': 26, 'rewardPerEpisode': 16.47660708235098
'totalSteps': 10240, 'rewardStep': 0.7514717684529111, 'errorList': [], 'lossList': [0.0, -1.3546794575452805, 0.0, 17.53276705622673, 0.0, 0.0, 0.0], 'rewardMean': 0.6938198164785929, 'totalEpisodes': 147, 'stepsPerEpisode': 152, 'rewardPerEpisode': 117.66679675760074
'totalSteps': 11520, 'rewardStep': 0.7747221881086118, 'errorList': [], 'lossList': [0.0, -1.3259522444009781, 0.0, 8.229255290031434, 0.0, 0.0, 0.0], 'rewardMean': 0.7028089688819283, 'totalEpisodes': 150, 'stepsPerEpisode': 153, 'rewardPerEpisode': 118.6536095050309
'totalSteps': 12800, 'rewardStep': 0.8256868777517139, 'errorList': [], 'lossList': [0.0, -1.3096911579370498, 0.0, 33.80289668560028, 0.0, 0.0, 0.0], 'rewardMean': 0.715096759768907, 'totalEpisodes': 156, 'stepsPerEpisode': 62, 'rewardPerEpisode': 54.63238849449563
'totalSteps': 14080, 'rewardStep': 0.6187614967633166, 'errorList': [], 'lossList': [0.0, -1.2872361147403717, 0.0, 6.247156990468502, 0.0, 0.0, 0.0], 'rewardMean': 0.7405658988676798, 'totalEpisodes': 157, 'stepsPerEpisode': 815, 'rewardPerEpisode': 589.1108516069911
'totalSteps': 15360, 'rewardStep': 0.91810937681607, 'errorList': [], 'lossList': [0.0, -1.273489112854004, 0.0, 7.646738588809967, 0.0, 0.0, 0.0], 'rewardMean': 0.7507736656674459, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1000.7235814847155
'totalSteps': 16640, 'rewardStep': 0.5335571471250581, 'errorList': [], 'lossList': [0.0, -1.263365489244461, 0.0, 4.5302399760484695, 0.0, 0.0, 0.0], 'rewardMean': 0.7149735371666683, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.699654836764
'totalSteps': 17920, 'rewardStep': 0.7850582755889379, 'errorList': [], 'lossList': [0.0, -1.2368714439868926, 0.0, 2.1371978432685137, 0.0, 0.0, 0.0], 'rewardMean': 0.700458080233046, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.6592228654542
'totalSteps': 19200, 'rewardStep': 0.8142637829654116, 'errorList': [], 'lossList': [0.0, -1.2176178205013275, 0.0, 2.3686376332491634, 0.0, 0.0, 0.0], 'rewardMean': 0.7346495665223355, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.8533076078422
'totalSteps': 20480, 'rewardStep': 0.9865172691144127, 'errorList': [0.23534773763065509, 0.030163615272220553, 0.2884759111318381, 0.17657575673233059, 0.2614627755773997, 0.03307516108198961, 0.20063323048751874, 0.0910559640459897, 0.13472364647787413, 0.07210232730546962, 0.18705685469914107, 0.29278517882830224, 0.06817463819043615, 0.0585381018650448, 0.09284040403514106, 0.04131267652910772, 0.050172149700796025, 0.15580745330337398, 0.05541096770773441, 0.16326789459624347, 0.023069688362438393, 0.09192725750972847, 0.02320950307784021, 0.30736660422622253, 0.05304617419768112, 0.05796958198453625, 0.10215234944899049, 0.021366631640740517, 0.08037158889475594, 0.12494634391502701, 0.09005424112892516, 0.09340931541349064, 0.1630213963608495, 0.15472323971155363, 0.22755407671433026, 0.06144534576696836, 0.2203973966887729, 0.29847937876190633, 0.05757729830683158, 0.04782674580576682, 0.08201886035295099, 0.17912849768392025, 0.19594046971249993, 0.024438357402188846, 0.07725702608685694, 0.07632099466147771, 0.09048172813876126, 0.02956534636357166, 0.032175462243370825, 0.11119215213247315], 'lossList': [0.0, -1.1966309142112732, 0.0, 2.904021519422531, 0.0, 0.0, 0.0], 'rewardMean': 0.7569146006250331, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1177.3563126222639, 'successfulTests': 41
'totalSteps': 21760, 'rewardStep': 0.8779020578935202, 'errorList': [], 'lossList': [0.0, -1.1808435201644898, 0.0, 1.3076994074881076, 0.0, 0.0, 0.0], 'rewardMean': 0.7886050240579964, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.1701744605537
'totalSteps': 23040, 'rewardStep': 0.9263977409245601, 'errorList': [], 'lossList': [0.0, -1.1719938534498215, 0.0, 0.9010932071506977, 0.0, 0.0, 0.0], 'rewardMean': 0.8060976213051612, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.330257290101
'totalSteps': 24320, 'rewardStep': 0.9379464764371517, 'errorList': [0.035091587072517014, 0.013239027342021087, 0.023517433291240907, 0.01849232103952864, 0.014249290568559175, 0.038682073807601934, 0.053466411905625275, 0.11669615635057146, 0.04649302367612088, 0.021161671727615877, 0.13066778278391533, 0.036065575318389734, 0.05394602128313778, 0.08970637470800165, 0.03372731694324578, 0.06218121461796191, 0.10270203896709064, 0.14727829146267007, 0.06856328524792392, 0.03648818394914776, 0.05749840936809609, 0.011059903224719753, 0.01464993576962028, 0.018071523830725623, 0.08786957027887494, 0.06651745519503895, 0.06642484087356476, 0.0474916763797603, 0.15201674267852575, 0.02997303948299039, 0.08655324394872893, 0.03482117389482927, 0.17225336972584748, 0.1686895176560545, 0.0750105345727272, 0.02679501390059826, 0.034796880737453566, 0.09796887632435534, 0.013913393625580202, 0.05875067644639415, 0.050665694439136655, 0.040878215305176796, 0.06181645199265894, 0.02358765124969354, 0.09986148562523987, 0.08019501801133506, 0.04222248255216816, 0.030662026535057244, 0.1325833254994279, 0.07350535854331641], 'lossList': [0.0, -1.1575799298286438, 0.0, 0.7591763950511813, 0.0, 0.0, 0.0], 'rewardMean': 0.8224200501380153, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1206.685501775489, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=102.61
