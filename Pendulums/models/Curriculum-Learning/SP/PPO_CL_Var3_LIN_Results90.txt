#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 90
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9373168733040402, 'errorList': [], 'lossList': [0.0, -1.439223422408104, 0.0, 28.382462654709816, 0.0, 0.0, 0.0], 'rewardMean': 0.9164492322561036, 'totalEpisodes': 8, 'stepsPerEpisode': 532, 'rewardPerEpisode': 370.91449748648665
'totalSteps': 3840, 'rewardStep': 0.6957205350275607, 'errorList': [], 'lossList': [0.0, -1.434981558918953, 0.0, 30.6204384970665, 0.0, 0.0, 0.0], 'rewardMean': 0.8428729998465894, 'totalEpisodes': 13, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.49143370104645
'totalSteps': 5120, 'rewardStep': 0.6961351700021364, 'errorList': [], 'lossList': [0.0, -1.4330272620916367, 0.0, 37.17690249443054, 0.0, 0.0, 0.0], 'rewardMean': 0.8061885423854761, 'totalEpisodes': 16, 'stepsPerEpisode': 271, 'rewardPerEpisode': 209.7032417381639
'totalSteps': 6400, 'rewardStep': 0.8797061889247948, 'errorList': [], 'lossList': [0.0, -1.4257226103544236, 0.0, 33.92806453466415, 0.0, 0.0, 0.0], 'rewardMean': 0.8208920716933399, 'totalEpisodes': 18, 'stepsPerEpisode': 205, 'rewardPerEpisode': 163.70797026365472
'totalSteps': 7680, 'rewardStep': 0.9367640491084861, 'errorList': [], 'lossList': [0.0, -1.4101518911123276, 0.0, 93.56282463073731, 0.0, 0.0, 0.0], 'rewardMean': 0.8402040679291977, 'totalEpisodes': 26, 'stepsPerEpisode': 192, 'rewardPerEpisode': 138.88312082102885
'totalSteps': 8960, 'rewardStep': 0.5690649682033311, 'errorList': [], 'lossList': [0.0, -1.3944019663333893, 0.0, 185.28980075836182, 0.0, 0.0, 0.0], 'rewardMean': 0.8014699108255023, 'totalEpisodes': 48, 'stepsPerEpisode': 38, 'rewardPerEpisode': 25.07679928958773
'totalSteps': 10240, 'rewardStep': 0.9309219773980899, 'errorList': [7.368388623076641, 43.77283046237011, 50.3184465022055, 88.68750139802998, 8.166664289982005, 46.26146800963837, 96.59913098698466, 104.71613407493263, 7.3617265125024804, 67.13770036568685, 24.951868749781333, 46.57960254101493, 69.64725680911216, 103.1594690068262, 70.99064038475274, 56.00653283991447, 83.68468024657726, 87.968100161313, 72.89627728552982, 29.846799371575663, 87.70021506689979, 41.57871999571335, 59.71652911354945, 114.77853873527535, 26.57110943627554, 66.1952543698911, 78.15401987539487, 12.22705687623606, 132.8846894284158, 20.40288657743015, 1.3149089426305376, 37.1939575900962, 108.80979089350825, 63.43840609798664, 14.806277945449759, 43.3769531301241, 92.31162559976757, 26.755921861146664, 116.03955640895947, 68.42916852290777, 79.23148787641094, 72.44331508213824, 57.807469813969135, 75.60714782643508, 7.39240689316044, 16.56423034342959, 17.30812585124738, 79.19356233055882, 23.54600675629628, 104.26542957713208], 'lossList': [0.0, -1.383104060292244, 0.0, 166.7210228729248, 0.0, 0.0, 0.0], 'rewardMean': 0.8176514191470758, 'totalEpisodes': 78, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.41189129517207, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6877656316830095, 'errorList': [], 'lossList': [0.0, -1.3795020914077758, 0.0, 83.45926959991455, 0.0, 0.0, 0.0], 'rewardMean': 0.8032196649844018, 'totalEpisodes': 94, 'stepsPerEpisode': 71, 'rewardPerEpisode': 51.491556865383295
'totalSteps': 12800, 'rewardStep': 0.7834276811056242, 'errorList': [], 'lossList': [0.0, -1.364789027571678, 0.0, 54.51323575019836, 0.0, 0.0, 0.0], 'rewardMean': 0.801240466596524, 'totalEpisodes': 105, 'stepsPerEpisode': 141, 'rewardPerEpisode': 107.35202882607814
'totalSteps': 14080, 'rewardStep': 0.7495511064351911, 'errorList': [], 'lossList': [0.0, -1.3561556667089463, 0.0, 21.185426745414734, 0.0, 0.0, 0.0], 'rewardMean': 0.7866374181192264, 'totalEpisodes': 111, 'stepsPerEpisode': 217, 'rewardPerEpisode': 173.1169309071464
'totalSteps': 15360, 'rewardStep': 0.6712073867839505, 'errorList': [], 'lossList': [0.0, -1.3638711470365523, 0.0, 23.407438356876373, 0.0, 0.0, 0.0], 'rewardMean': 0.7600264694672174, 'totalEpisodes': 115, 'stepsPerEpisode': 67, 'rewardPerEpisode': 54.24721069496744
'totalSteps': 16640, 'rewardStep': 0.5197144324080869, 'errorList': [], 'lossList': [0.0, -1.3722868275642395, 0.0, 8.306151993274689, 0.0, 0.0, 0.0], 'rewardMean': 0.74242585920527, 'totalEpisodes': 117, 'stepsPerEpisode': 564, 'rewardPerEpisode': 333.70600825542823
'totalSteps': 17920, 'rewardStep': 0.729243307071062, 'errorList': [], 'lossList': [0.0, -1.3661810386180877, 0.0, 5.013666235208511, 0.0, 0.0, 0.0], 'rewardMean': 0.7457366729121626, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 810.7468751277396
'totalSteps': 19200, 'rewardStep': 0.7785285424763524, 'errorList': [], 'lossList': [0.0, -1.3545711648464203, 0.0, 17.171546149849892, 0.0, 0.0, 0.0], 'rewardMean': 0.7356189082673183, 'totalEpisodes': 118, 'stepsPerEpisode': 821, 'rewardPerEpisode': 655.097702930342
'totalSteps': 20480, 'rewardStep': 0.8913248570310743, 'errorList': [], 'lossList': [0.0, -1.3583571130037309, 0.0, 4.48863150537014, 0.0, 0.0, 0.0], 'rewardMean': 0.7310749890595771, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1016.3110840100452
'totalSteps': 21760, 'rewardStep': 0.829678442244211, 'errorList': [], 'lossList': [0.0, -1.364831874370575, 0.0, 3.609119544327259, 0.0, 0.0, 0.0], 'rewardMean': 0.7571363364636652, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.5873673764465
'totalSteps': 23040, 'rewardStep': 0.8132608536314426, 'errorList': [], 'lossList': [0.0, -1.3457314270734786, 0.0, 3.1205178071558475, 0.0, 0.0, 0.0], 'rewardMean': 0.7453702240870005, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.2991347324062
'totalSteps': 24320, 'rewardStep': 0.6641828415367612, 'errorList': [], 'lossList': [0.0, -1.3090544217824935, 0.0, 1.9781258323788642, 0.0, 0.0, 0.0], 'rewardMean': 0.7430119450723757, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.253939532485
'totalSteps': 25600, 'rewardStep': 0.9434555871364481, 'errorList': [0.08245905502259714, 0.052943407523261155, 0.04657253488044735, 0.031322890559636, 0.042782794816499166, 0.08984737564410186, 0.06059545718349385, 0.06993286858066348, 0.042466657027589526, 0.044831149584158625, 0.05101476804197011, 0.07126533579084059, 0.04567822143573652, 0.042783604304413914, 0.031132041072249768, 0.03172204944854968, 0.10654032867046451, 0.02885687943397123, 0.06745535666970857, 0.023077029804465153, 0.015913015215257027, 0.017901308222251375, 0.016007643521729266, 0.03411521264613439, 0.09542515346760876, 0.07153838417037482, 0.09647278728072806, 0.04609795825114549, 0.06523924341501515, 0.010321642712044455, 0.053927505723175166, 0.019514903567656155, 0.039655749964423795, 0.033369149291185486, 0.045140723290302026, 0.03345667027396792, 0.08619769277473244, 0.053475635331411295, 0.019092515688158007, 0.036276702219864905, 0.04492100929618453, 0.01606149375884345, 0.04947508014267939, 0.03737150059748779, 0.02647399054882653, 0.023278582650505586, 0.024320756339091157, 0.04508531942093099, 0.04890763101181621, 0.04140020767188025], 'lossList': [0.0, -1.2734310460090636, 0.0, 2.0837045492976904, 0.0, 0.0, 0.0], 'rewardMean': 0.7590147356754581, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.81457256824, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=103.41
