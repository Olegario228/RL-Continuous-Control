#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 91
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7783980485351634, 'errorList': [], 'lossList': [0.0, -1.4497920042276382, 0.0, 34.69412711143494, 0.0, 0.0, 0.0], 'rewardMean': 0.6685228835057593, 'totalEpisodes': 12, 'stepsPerEpisode': 78, 'rewardPerEpisode': 67.96301976569416
'totalSteps': 3840, 'rewardStep': 0.8718853306574059, 'errorList': [], 'lossList': [0.0, -1.4684944260120392, 0.0, 25.355727227330206, 0.0, 0.0, 0.0], 'rewardMean': 0.7363103658896414, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 952.6939269590532
'totalSteps': 5120, 'rewardStep': 0.9010059744608659, 'errorList': [], 'lossList': [0.0, -1.4367064827680587, 0.0, 34.67731132626533, 0.0, 0.0, 0.0], 'rewardMean': 0.7774842680324475, 'totalEpisodes': 14, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.067044508323458
'totalSteps': 6400, 'rewardStep': 0.6636226574082232, 'errorList': [], 'lossList': [0.0, -1.4207793545722962, 0.0, 38.024244780540464, 0.0, 0.0, 0.0], 'rewardMean': 0.7547119459076026, 'totalEpisodes': 16, 'stepsPerEpisode': 82, 'rewardPerEpisode': 68.84176143874639
'totalSteps': 7680, 'rewardStep': 0.7747702356131009, 'errorList': [], 'lossList': [0.0, -1.3831776016950608, 0.0, 11.980853143632412, 0.0, 0.0, 0.0], 'rewardMean': 0.7580549941918524, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1022.168476765294
'totalSteps': 8960, 'rewardStep': 0.4329286241659598, 'errorList': [], 'lossList': [0.0, -1.3481031280755997, 0.0, 235.73209915161132, 0.0, 0.0, 0.0], 'rewardMean': 0.7116083699024391, 'totalEpisodes': 37, 'stepsPerEpisode': 17, 'rewardPerEpisode': 10.556442454093297
'totalSteps': 10240, 'rewardStep': 0.8182611902799458, 'errorList': [], 'lossList': [0.0, -1.3457968533039093, 0.0, 169.86796878814698, 0.0, 0.0, 0.0], 'rewardMean': 0.7249399724496275, 'totalEpisodes': 69, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.640019307710567
'totalSteps': 11520, 'rewardStep': 0.825116195573261, 'errorList': [], 'lossList': [0.0, -1.3448296892642975, 0.0, 101.58184377670288, 0.0, 0.0, 0.0], 'rewardMean': 0.736070663907809, 'totalEpisodes': 95, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.40448718463777
'totalSteps': 12800, 'rewardStep': 0.6081511865764619, 'errorList': [], 'lossList': [0.0, -1.3417568135261535, 0.0, 75.46744094848633, 0.0, 0.0, 0.0], 'rewardMean': 0.7232787161746743, 'totalEpisodes': 126, 'stepsPerEpisode': 69, 'rewardPerEpisode': 50.24373064937015
'totalSteps': 14080, 'rewardStep': 0.7850648001700029, 'errorList': [], 'lossList': [0.0, -1.3330916368961334, 0.0, 60.474461193084714, 0.0, 0.0, 0.0], 'rewardMean': 0.7459204243440392, 'totalEpisodes': 145, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.364269756883553
'totalSteps': 15360, 'rewardStep': 0.4035766856097407, 'errorList': [], 'lossList': [0.0, -1.325423607826233, 0.0, 29.95172556400299, 0.0, 0.0, 0.0], 'rewardMean': 0.7084382880514969, 'totalEpisodes': 149, 'stepsPerEpisode': 185, 'rewardPerEpisode': 119.97990351227442
'totalSteps': 16640, 'rewardStep': 0.6152683320440857, 'errorList': [], 'lossList': [0.0, -1.312747437953949, 0.0, 23.354162657260893, 0.0, 0.0, 0.0], 'rewardMean': 0.6827765881901648, 'totalEpisodes': 151, 'stepsPerEpisode': 179, 'rewardPerEpisode': 146.09662861325177
'totalSteps': 17920, 'rewardStep': 0.8268033267400663, 'errorList': [], 'lossList': [0.0, -1.2892839801311493, 0.0, 28.366928923130036, 0.0, 0.0, 0.0], 'rewardMean': 0.6753563234180848, 'totalEpisodes': 153, 'stepsPerEpisode': 653, 'rewardPerEpisode': 491.026239691757
'totalSteps': 19200, 'rewardStep': 0.8220713479561689, 'errorList': [], 'lossList': [0.0, -1.26588694691658, 0.0, 7.834946409463883, 0.0, 0.0, 0.0], 'rewardMean': 0.6912011924728794, 'totalEpisodes': 154, 'stepsPerEpisode': 240, 'rewardPerEpisode': 181.54101056927334
'totalSteps': 20480, 'rewardStep': 0.649456728351806, 'errorList': [], 'lossList': [0.0, -1.2277718055248261, 0.0, 4.386676518321037, 0.0, 0.0, 0.0], 'rewardMean': 0.6786698417467499, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1014.3125737766063
'totalSteps': 21760, 'rewardStep': 0.7419642533767404, 'errorList': [], 'lossList': [0.0, -1.1560037219524384, 0.0, 3.4378343145549297, 0.0, 0.0, 0.0], 'rewardMean': 0.709573404667828, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1079.2319389193797
'totalSteps': 23040, 'rewardStep': 0.9518608363898526, 'errorList': [0.09069163388767557, 0.11842280647003406, 0.0729327431142368, 0.08587833769644453, 0.1222117789729006, 0.07391903851731053, 0.10846029264876701, 0.08397766383471678, 0.062305696734177726, 0.1031516767852963, 0.06436060655159614, 0.05359970577075216, 0.07776750553221384, 0.08559649281101094, 0.16474148043878187, 0.07286205982982134, 0.09208315084193819, 0.05712674263235079, 0.06355190687300682, 0.07355726780744323, 0.09203540990548634, 0.09333978169273917, 0.10326899608483527, 0.088763922779206, 0.08409175651492078, 0.0735056673009516, 0.09580280158814375, 0.11285869720002009, 0.05197905969402193, 0.062376150877638696, 0.06515271626249029, 0.10771320920800861, 0.11450955515090798, 0.09558344944149152, 0.11320299271591043, 0.09198807317876906, 0.12243649917552574, 0.14130897935861714, 0.09100804082062344, 0.10978600361678607, 0.06407392585565254, 0.10231441337276517, 0.08404915758122004, 0.1394724400866558, 0.13553898155284905, 0.07708432589065035, 0.13886495071215005, 0.16116762688709485, 0.12175585122310754, 0.060250703067619864], 'lossList': [0.0, -1.1261333638429643, 0.0, 2.262403923906386, 0.0, 0.0, 0.0], 'rewardMean': 0.7229333692788187, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.2892824789446, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=78.25
