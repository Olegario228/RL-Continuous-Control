#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 92
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.7170620901486295, 'errorList': [], 'lossList': [0.0, -1.4561407351493836, 0.0, 32.060683858394626, 0.0, 0.0, 0.0], 'rewardMean': 0.5974239326857138, 'totalEpisodes': 14, 'stepsPerEpisode': 288, 'rewardPerEpisode': 220.95643159564128
'totalSteps': 3840, 'rewardStep': 0.827317748903778, 'errorList': [], 'lossList': [0.0, -1.4737644946575166, 0.0, 23.85213539123535, 0.0, 0.0, 0.0], 'rewardMean': 0.6740552047584019, 'totalEpisodes': 16, 'stepsPerEpisode': 711, 'rewardPerEpisode': 479.7039432307733
'totalSteps': 5120, 'rewardStep': 0.5155249898929559, 'errorList': [], 'lossList': [0.0, -1.4489086121320724, 0.0, 26.533323040008543, 0.0, 0.0, 0.0], 'rewardMean': 0.6344226510420403, 'totalEpisodes': 19, 'stepsPerEpisode': 68, 'rewardPerEpisode': 39.070104432097175
'totalSteps': 6400, 'rewardStep': 0.9803757466133826, 'errorList': [], 'lossList': [0.0, -1.4387885266542435, 0.0, 66.15060499191284, 0.0, 0.0, 0.0], 'rewardMean': 0.7036132701563088, 'totalEpisodes': 28, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.9241444533934087
'totalSteps': 7680, 'rewardStep': 0.9084140645766962, 'errorList': [], 'lossList': [0.0, -1.4335864329338073, 0.0, 105.59782527923583, 0.0, 0.0, 0.0], 'rewardMean': 0.73774673589304, 'totalEpisodes': 43, 'stepsPerEpisode': 57, 'rewardPerEpisode': 43.01898769370192
'totalSteps': 8960, 'rewardStep': 0.9522177040892429, 'errorList': [308.2867457606349, 294.0225002107423, 225.7694644756239, 297.66340637602264, 296.4497163647969, 334.2143051571322, 316.353346109196, 232.9074301381213, 326.3073842446, 162.2830128057717, 262.0702500800086, 343.61189715760247, 299.20362838960705, 257.2424865195596, 304.89404346025856, 328.5014162949343, 312.62741902631996, 269.3681741388334, 214.70662143535972, 245.41714390631859, 280.154572092041, 322.1873219085222, 301.5918547079285, 306.96178884822575, 294.05665172936716, 280.1192037564549, 329.6582838328132, 336.09310677734726, 268.10695605321075, 334.2040343286225, 293.1058276948516, 295.93960693369394, 278.26709378366485, 255.25666111861273, 265.9886352989325, 309.0604971054707, 299.8559669494726, 304.3745936394936, 310.1127456194642, 321.2844938021707, 297.1365057898664, 230.62703160247503, 223.79278142617724, 279.5729973654356, 281.64871408356385, 329.25653445269046, 332.9164843161745, 316.647973372544, 266.3804208693647, 308.7367028338669], 'lossList': [0.0, -1.4271000760793686, 0.0, 155.20407360076905, 0.0, 0.0, 0.0], 'rewardMean': 0.7683854456353547, 'totalEpisodes': 70, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.5688520638801595, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.5488534687875162, 'errorList': [], 'lossList': [0.0, -1.4170163673162461, 0.0, 85.02925939559937, 0.0, 0.0, 0.0], 'rewardMean': 0.740943948529375, 'totalEpisodes': 98, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.1004119659687635
'totalSteps': 11520, 'rewardStep': 0.5853354529053016, 'errorList': [], 'lossList': [0.0, -1.412582426071167, 0.0, 60.98258155822754, 0.0, 0.0, 0.0], 'rewardMean': 0.7236541156822557, 'totalEpisodes': 115, 'stepsPerEpisode': 186, 'rewardPerEpisode': 139.58006557431256
'totalSteps': 12800, 'rewardStep': 0.9079243985993172, 'errorList': [], 'lossList': [0.0, -1.4080718106031418, 0.0, 58.02141987800598, 0.0, 0.0, 0.0], 'rewardMean': 0.7420811439739619, 'totalEpisodes': 123, 'stepsPerEpisode': 146, 'rewardPerEpisode': 113.27320597399584
'totalSteps': 14080, 'rewardStep': 0.79323581830215, 'errorList': [], 'lossList': [0.0, -1.4044522750377655, 0.0, 11.028158649206162, 0.0, 0.0, 0.0], 'rewardMean': 0.7736261482818969, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 981.8924914889649
'totalSteps': 15360, 'rewardStep': 0.7235723058226262, 'errorList': [], 'lossList': [0.0, -1.3872527539730073, 0.0, 48.28940732479096, 0.0, 0.0, 0.0], 'rewardMean': 0.7742771698492966, 'totalEpisodes': 128, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.510352771874214
'totalSteps': 16640, 'rewardStep': 0.9080093268095147, 'errorList': [], 'lossList': [0.0, -1.3563920813798904, 0.0, 6.615449025034905, 0.0, 0.0, 0.0], 'rewardMean': 0.7823463276398703, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 912.1737143108503
'totalSteps': 17920, 'rewardStep': 0.7803797459321796, 'errorList': [], 'lossList': [0.0, -1.3147249341011047, 0.0, 4.59147570848465, 0.0, 0.0, 0.0], 'rewardMean': 0.8088318032437927, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1016.1354435701422
'totalSteps': 19200, 'rewardStep': 0.7684122353440898, 'errorList': [], 'lossList': [0.0, -1.3020291596651077, 0.0, 3.0355160763859748, 0.0, 0.0, 0.0], 'rewardMean': 0.7876354521168635, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.0522615443394
'totalSteps': 20480, 'rewardStep': 0.8979281774123011, 'errorList': [], 'lossList': [0.0, -1.282095018029213, 0.0, 4.466702560186386, 0.0, 0.0, 0.0], 'rewardMean': 0.786586863400424, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.5576489597427
'totalSteps': 21760, 'rewardStep': 0.8756670470258081, 'errorList': [], 'lossList': [0.0, -1.2373130190372468, 0.0, 2.7714368049800395, 0.0, 0.0, 0.0], 'rewardMean': 0.7789317976940805, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.1172547284582
'totalSteps': 23040, 'rewardStep': 0.9223570631179656, 'errorList': [], 'lossList': [0.0, -1.2021023732423783, 0.0, 1.255751435495913, 0.0, 0.0, 0.0], 'rewardMean': 0.8162821571271254, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.4618762146274
'totalSteps': 24320, 'rewardStep': 0.9618285430424466, 'errorList': [0.0932212902137784, 0.09232073190884073, 0.07746401194319623, 0.07905518217961156, 0.08522143067243948, 0.09086820002251814, 0.12032547740359437, 0.1921095108901164, 0.07807791067534199, 0.08523170304142993, 0.10136291367358478, 0.10643306757934186, 0.05605174453602515, 0.166395031035241, 0.06115873041933988, 0.09485147001197346, 0.1226067732733816, 0.12078708008517379, 0.07380192136085208, 0.25805655517089654, 0.17197822760030065, 0.15290109833944415, 0.15921713042795768, 0.29262429809509977, 0.1452098205319136, 0.04352211102509063, 0.1094316151617824, 0.07750975943156468, 0.06235307428983571, 0.16084454412962795, 0.13314615644572822, 0.0923193209380995, 0.06261363587027716, 0.05529616783411631, 0.19642231864976864, 0.17083420983477726, 0.10609688844413225, 0.18580391945676364, 0.05566727144876882, 0.05224967842613118, 0.16437279092846344, 0.20484439157431403, 0.16353325635039584, 0.26567470792316733, 0.04632553694901506, 0.09537652540059653, 0.047054055155948724, 0.07196327623411607, 0.1952954771275821, 0.1402641804158285], 'lossList': [0.0, -1.1689203244447708, 0.0, 1.4217757249064744, 0.0, 0.0, 0.0], 'rewardMean': 0.8539314661408399, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.166011931173, 'successfulTests': 46
'totalSteps': 25600, 'rewardStep': 0.7988595467816892, 'errorList': [], 'lossList': [0.0, -1.132926778793335, 0.0, 0.8691723807901144, 0.0, 0.0, 0.0], 'rewardMean': 0.8430249809590771, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.855807320346
#maxSuccessfulTests=46, maxSuccessfulTestsAtStep=24320, timeSpent=103.61
