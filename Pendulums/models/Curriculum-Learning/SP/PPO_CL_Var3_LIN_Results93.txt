#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 93
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5515334237429164, 'errorList': [], 'lossList': [0.0, -1.421652584671974, 0.0, 27.2060043489933, 0.0, 0.0, 0.0], 'rewardMean': 0.704872766149228, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 30.401362406347808
'totalSteps': 3840, 'rewardStep': 0.9328154211280709, 'errorList': [], 'lossList': [0.0, -1.4188456791639328, 0.0, 25.369922044277192, 0.0, 0.0, 0.0], 'rewardMean': 0.7808536511421756, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 366.14125894079115
'totalSteps': 5120, 'rewardStep': 0.7297656592409887, 'errorList': [], 'lossList': [0.0, -1.4175975906848908, 0.0, 26.666953001618385, 0.0, 0.0, 0.0], 'rewardMean': 0.7680816531668788, 'totalEpisodes': 19, 'stepsPerEpisode': 182, 'rewardPerEpisode': 148.1964421476384
'totalSteps': 6400, 'rewardStep': 0.816697824040293, 'errorList': [], 'lossList': [0.0, -1.4188905262947082, 0.0, 70.39104681968689, 0.0, 0.0, 0.0], 'rewardMean': 0.7778048873415616, 'totalEpisodes': 26, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.087050701573094
'totalSteps': 7680, 'rewardStep': 0.8583479733990211, 'errorList': [], 'lossList': [0.0, -1.4090601789951325, 0.0, 101.40406103134156, 0.0, 0.0, 0.0], 'rewardMean': 0.7912287350178048, 'totalEpisodes': 36, 'stepsPerEpisode': 53, 'rewardPerEpisode': 45.03728735271273
'totalSteps': 8960, 'rewardStep': 0.7874045499684257, 'errorList': [], 'lossList': [0.0, -1.403900116086006, 0.0, 190.31255950927735, 0.0, 0.0, 0.0], 'rewardMean': 0.7906824228678936, 'totalEpisodes': 67, 'stepsPerEpisode': 36, 'rewardPerEpisode': 29.636066257403847
'totalSteps': 10240, 'rewardStep': 0.8603692142239722, 'errorList': [], 'lossList': [0.0, -1.4017109537124635, 0.0, 103.63049486160278, 0.0, 0.0, 0.0], 'rewardMean': 0.7993932717874035, 'totalEpisodes': 109, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.858989804617842
'totalSteps': 11520, 'rewardStep': 0.4291456689678323, 'errorList': [], 'lossList': [0.0, -1.3945330607891082, 0.0, 36.706805849075316, 0.0, 0.0, 0.0], 'rewardMean': 0.7582546492518956, 'totalEpisodes': 136, 'stepsPerEpisode': 45, 'rewardPerEpisode': 28.721223717212794
'totalSteps': 12800, 'rewardStep': 0.7707283271308573, 'errorList': [], 'lossList': [0.0, -1.3849314212799073, 0.0, 28.22249291419983, 0.0, 0.0, 0.0], 'rewardMean': 0.7595020170397918, 'totalEpisodes': 153, 'stepsPerEpisode': 70, 'rewardPerEpisode': 50.01006533896927
'totalSteps': 14080, 'rewardStep': 0.9012424646502417, 'errorList': [], 'lossList': [0.0, -1.3778742921352387, 0.0, 20.833268685340883, 0.0, 0.0, 0.0], 'rewardMean': 0.763805052649262, 'totalEpisodes': 162, 'stepsPerEpisode': 40, 'rewardPerEpisode': 31.989622284162312
'totalSteps': 15360, 'rewardStep': 0.5990867076342608, 'errorList': [], 'lossList': [0.0, -1.3579949671030045, 0.0, 15.489880373477936, 0.0, 0.0, 0.0], 'rewardMean': 0.7685603810383963, 'totalEpisodes': 168, 'stepsPerEpisode': 18, 'rewardPerEpisode': 10.470493532787883
'totalSteps': 16640, 'rewardStep': 0.49292688568678966, 'errorList': [], 'lossList': [0.0, -1.3146648132801055, 0.0, 7.598000317811966, 0.0, 0.0, 0.0], 'rewardMean': 0.7245715274942681, 'totalEpisodes': 172, 'stepsPerEpisode': 219, 'rewardPerEpisode': 151.91092574028514
'totalSteps': 17920, 'rewardStep': 0.7713136487499062, 'errorList': [], 'lossList': [0.0, -1.3059643840789794, 0.0, 7.853239223957062, 0.0, 0.0, 0.0], 'rewardMean': 0.72872632644516, 'totalEpisodes': 176, 'stepsPerEpisode': 105, 'rewardPerEpisode': 87.28019358752374
'totalSteps': 19200, 'rewardStep': 0.6526628932774614, 'errorList': [], 'lossList': [0.0, -1.315149287581444, 0.0, 9.61807746887207, 0.0, 0.0, 0.0], 'rewardMean': 0.7123228333688767, 'totalEpisodes': 178, 'stepsPerEpisode': 773, 'rewardPerEpisode': 514.7865071778823
'totalSteps': 20480, 'rewardStep': 0.8932685588508772, 'errorList': [], 'lossList': [0.0, -1.3057906913757324, 0.0, 5.591014422178269, 0.0, 0.0, 0.0], 'rewardMean': 0.7158148919140624, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 944.2662802691246
'totalSteps': 21760, 'rewardStep': 0.9145343850499856, 'errorList': [], 'lossList': [0.0, -1.2954619282484054, 0.0, 3.8199745284020903, 0.0, 0.0, 0.0], 'rewardMean': 0.7285278754222184, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.9130717474393
'totalSteps': 23040, 'rewardStep': 0.8192182846173824, 'errorList': [], 'lossList': [0.0, -1.2651296252012252, 0.0, 3.376063823029399, 0.0, 0.0, 0.0], 'rewardMean': 0.7244127824615594, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.7788637998779
'totalSteps': 24320, 'rewardStep': 0.85497897395917, 'errorList': [], 'lossList': [0.0, -1.2053624182939529, 0.0, 2.20549269720912, 0.0, 0.0, 0.0], 'rewardMean': 0.7669961129606933, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.999728665547
'totalSteps': 25600, 'rewardStep': 0.9556974437754047, 'errorList': [0.10743833613307624, 0.13521025924190183, 0.08741290398933774, 0.09220386294964487, 0.10308541665700921, 0.1374199280775262, 0.1060988035900785, 0.08563205792733102, 0.08062525978040909, 0.09952303587492944, 0.11129678334539947, 0.1168133596415955, 0.13395081784330742, 0.14717759730516045, 0.11404974372610435, 0.10544072947086813, 0.08642937006574421, 0.10906081848810532, 0.1442384850048547, 0.12281685969228492, 0.13141596841556707, 0.11195352139104094, 0.10413496611441418, 0.08653148131462068, 0.14119316970492718, 0.12063179219017045, 0.11821552797338072, 0.12897438793022342, 0.08404721512473974, 0.09600138567609284, 0.0945000694122018, 0.12983236366767664, 0.10601539757620912, 0.09214616172830675, 0.0925322969025336, 0.10457894101608828, 0.14486991086628845, 0.07890558533071752, 0.11472373961857382, 0.08893015731327787, 0.11697895789543267, 0.10360138984606235, 0.10703351363456168, 0.13957612245012674, 0.13450239882729362, 0.11083551836444447, 0.10554551559398698, 0.07961591838444776, 0.11445780433769878, 0.1309629280657088], 'lossList': [0.0, -1.1710634529590607, 0.0, 1.5415492371842265, 0.0, 0.0, 0.0], 'rewardMean': 0.785493024625148, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1191.480670244116, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=84.26
