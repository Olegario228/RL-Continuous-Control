#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 94
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8570027882616486, 'errorList': [], 'lossList': [0.0, -1.4276801526546479, 0.0, 32.35108895361424, 0.0, 0.0, 0.0], 'rewardMean': 0.7901768560704747, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 256.700829698755
'totalSteps': 3840, 'rewardStep': 0.7374701401151194, 'errorList': [], 'lossList': [0.0, -1.4426867008209228, 0.0, 29.243821291923524, 0.0, 0.0, 0.0], 'rewardMean': 0.772607950752023, 'totalEpisodes': 16, 'stepsPerEpisode': 167, 'rewardPerEpisode': 122.5844562415104
'totalSteps': 5120, 'rewardStep': 0.5144315481137873, 'errorList': [], 'lossList': [0.0, -1.4510124868154526, 0.0, 27.99847917199135, 0.0, 0.0, 0.0], 'rewardMean': 0.7080638500924641, 'totalEpisodes': 18, 'stepsPerEpisode': 282, 'rewardPerEpisode': 202.41667101491916
'totalSteps': 6400, 'rewardStep': 0.9574786489992985, 'errorList': [], 'lossList': [0.0, -1.4551706171035768, 0.0, 44.04804771900177, 0.0, 0.0, 0.0], 'rewardMean': 0.7579468098738309, 'totalEpisodes': 22, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.951580808067224
'totalSteps': 7680, 'rewardStep': 0.6670149774832098, 'errorList': [], 'lossList': [0.0, -1.4542632102966309, 0.0, 98.03927639007568, 0.0, 0.0, 0.0], 'rewardMean': 0.742791504475394, 'totalEpisodes': 32, 'stepsPerEpisode': 35, 'rewardPerEpisode': 25.37779496591948
'totalSteps': 8960, 'rewardStep': 0.8640118655579415, 'errorList': [], 'lossList': [0.0, -1.45142234146595, 0.0, 216.7835041809082, 0.0, 0.0, 0.0], 'rewardMean': 0.7601086989157579, 'totalEpisodes': 68, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.806337792455603
'totalSteps': 10240, 'rewardStep': 0.9615228077645179, 'errorList': [214.7548548834055, 193.11889434595855, 253.38731448916283, 278.19064622964004, 211.059176155813, 217.59579639267088, 265.2265077442545, 158.2732565470245, 220.66318661719282, 228.56376016834648, 222.02885775386846, 235.0679222525568, 203.06456129219237, 277.37062339014614, 259.54698690557865, 248.12244248534472, 276.22571476925975, 246.26156457640528, 229.61653464190516, 267.2462023255651, 186.94046882758013, 226.4353001066801, 266.27052210837235, 264.5601127749524, 279.2556846681571, 246.06583164059376, 256.8848913793968, 187.21744897062746, 276.1453868758298, 254.28029011608064, 241.8909200858644, 246.25576609444843, 258.725745725712, 180.3152031961768, 282.5974891675642, 225.9440037461464, 276.51783638703006, 226.8131635894022, 266.25179369055115, 281.3579045812748, 229.06958709887553, 262.07981894086475, 266.1897028167095, 284.4200525129653, 260.911271185154, 226.07723467681217, 225.59404265374383, 271.90527158005034, 187.44938323624913, 184.66185152949612], 'lossList': [0.0, -1.439496472477913, 0.0, 135.3100729751587, 0.0, 0.0, 0.0], 'rewardMean': 0.785285462521853, 'totalEpisodes': 103, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.618524166405649, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8330134696831559, 'errorList': [], 'lossList': [0.0, -1.4308208775520326, 0.0, 91.41668544769287, 0.0, 0.0, 0.0], 'rewardMean': 0.7905885744286645, 'totalEpisodes': 126, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.842135962723734
'totalSteps': 12800, 'rewardStep': 0.8578046776116846, 'errorList': [], 'lossList': [0.0, -1.4246969133615495, 0.0, 61.56839233398438, 0.0, 0.0, 0.0], 'rewardMean': 0.7973101847469665, 'totalEpisodes': 140, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.165227915525744
'totalSteps': 14080, 'rewardStep': 0.7416542781388877, 'errorList': [], 'lossList': [0.0, -1.4083222818374634, 0.0, 23.563512523174285, 0.0, 0.0, 0.0], 'rewardMean': 0.799140520172925, 'totalEpisodes': 146, 'stepsPerEpisode': 94, 'rewardPerEpisode': 65.41276933782845
'totalSteps': 15360, 'rewardStep': 0.6013476432149609, 'errorList': [], 'lossList': [0.0, -1.3876786202192306, 0.0, 43.49781356334686, 0.0, 0.0, 0.0], 'rewardMean': 0.7735750056682563, 'totalEpisodes': 150, 'stepsPerEpisode': 859, 'rewardPerEpisode': 624.4269350556737
'totalSteps': 16640, 'rewardStep': 0.530658179181096, 'errorList': [], 'lossList': [0.0, -1.3727252101898193, 0.0, 15.13007724761963, 0.0, 0.0, 0.0], 'rewardMean': 0.752893809574854, 'totalEpisodes': 151, 'stepsPerEpisode': 446, 'rewardPerEpisode': 359.5065853532393
'totalSteps': 17920, 'rewardStep': 0.7129020353063074, 'errorList': [], 'lossList': [0.0, -1.3660326206684112, 0.0, 11.329490992426873, 0.0, 0.0, 0.0], 'rewardMean': 0.772740858294106, 'totalEpisodes': 152, 'stepsPerEpisode': 252, 'rewardPerEpisode': 217.8956844913614
'totalSteps': 19200, 'rewardStep': 0.6307785949711318, 'errorList': [], 'lossList': [0.0, -1.3609196770191192, 0.0, 3.586144812703133, 0.0, 0.0, 0.0], 'rewardMean': 0.7400708528912892, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 932.9117667891936
'totalSteps': 20480, 'rewardStep': 0.8747217551281732, 'errorList': [], 'lossList': [0.0, -1.3649972265958785, 0.0, 4.665104527175426, 0.0, 0.0, 0.0], 'rewardMean': 0.7608415306557856, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.287206217823
'totalSteps': 21760, 'rewardStep': 0.8204029000907004, 'errorList': [], 'lossList': [0.0, -1.3509790343046189, 0.0, 5.810142202377319, 0.0, 0.0, 0.0], 'rewardMean': 0.7564806341090615, 'totalEpisodes': 153, 'stepsPerEpisode': 850, 'rewardPerEpisode': 660.5998535748734
'totalSteps': 23040, 'rewardStep': 0.7707153184706429, 'errorList': [], 'lossList': [0.0, -1.3381456178426743, 0.0, 5.361600231230259, 0.0, 0.0, 0.0], 'rewardMean': 0.737399885179674, 'totalEpisodes': 154, 'stepsPerEpisode': 702, 'rewardPerEpisode': 505.15710935182057
'totalSteps': 24320, 'rewardStep': 0.6675920488343966, 'errorList': [], 'lossList': [0.0, -1.3307052928209304, 0.0, 1.1149680383503437, 0.0, 0.0, 0.0], 'rewardMean': 0.7208577430947981, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 968.8902225805737
'totalSteps': 25600, 'rewardStep': 0.9109539784845464, 'errorList': [], 'lossList': [0.0, -1.322742886543274, 0.0, 1.5504306289553642, 0.0, 0.0, 0.0], 'rewardMean': 0.7261726731820843, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.5163500085134
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.38
