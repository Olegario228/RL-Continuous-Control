#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 95
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.8866809014599144, 'errorList': [], 'lossList': [0.0, -1.4358206522464751, 0.0, 29.97704879462719, 0.0, 0.0, 0.0], 'rewardMean': 0.9007507660203713, 'totalEpisodes': 8, 'stepsPerEpisode': 530, 'rewardPerEpisode': 370.6527119598667
'totalSteps': 3840, 'rewardStep': 0.7394697272329338, 'errorList': [], 'lossList': [0.0, -1.4229345703125, 0.0, 27.673352816104888, 0.0, 0.0, 0.0], 'rewardMean': 0.846990419757892, 'totalEpisodes': 12, 'stepsPerEpisode': 261, 'rewardPerEpisode': 192.95985787212683
'totalSteps': 5120, 'rewardStep': 0.6963012738847687, 'errorList': [], 'lossList': [0.0, -1.4207010811567307, 0.0, 25.08979174375534, 0.0, 0.0, 0.0], 'rewardMean': 0.8093181332896112, 'totalEpisodes': 13, 'stepsPerEpisode': 473, 'rewardPerEpisode': 351.82148850836165
'totalSteps': 6400, 'rewardStep': 0.9265862434459766, 'errorList': [], 'lossList': [0.0, -1.407531344294548, 0.0, 35.062218635082246, 0.0, 0.0, 0.0], 'rewardMean': 0.8327717553208842, 'totalEpisodes': 15, 'stepsPerEpisode': 207, 'rewardPerEpisode': 170.95297071504595
'totalSteps': 7680, 'rewardStep': 0.7913887565331732, 'errorList': [], 'lossList': [0.0, -1.3891277712583543, 0.0, 102.39420707702637, 0.0, 0.0, 0.0], 'rewardMean': 0.8258745888562657, 'totalEpisodes': 24, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.985952421717087
'totalSteps': 8960, 'rewardStep': 0.5345401665428324, 'errorList': [], 'lossList': [0.0, -1.3803957635164261, 0.0, 191.10371559143067, 0.0, 0.0, 0.0], 'rewardMean': 0.7842553856686324, 'totalEpisodes': 44, 'stepsPerEpisode': 113, 'rewardPerEpisode': 83.95771180587252
'totalSteps': 10240, 'rewardStep': 0.6766587030949112, 'errorList': [], 'lossList': [0.0, -1.3787043380737305, 0.0, 197.28081604003907, 0.0, 0.0, 0.0], 'rewardMean': 0.7708058003469174, 'totalEpisodes': 81, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.304927255856107
'totalSteps': 11520, 'rewardStep': 0.9412514859978702, 'errorList': [270.28704842095954, 239.90372260035798, 259.4159253223846, 298.6210513873441, 243.69265561468143, 238.90231646864305, 144.33145370828106, 233.5923321360701, 172.51883742379258, 227.63681568892622, 212.9854212337548, 243.15114406130928, 80.09021073864143, 250.7183573732052, 275.9239127691774, 163.87974163217473, 237.98082222566404, 204.84931304613406, 126.23811172670521, 177.98406506401489, 284.5858612343752, 225.57327581699957, 252.93168586569874, 260.6161804131057, 242.25802089416533, 285.5719243845404, 287.0711981801029, 294.5739556797238, 245.85048534602825, 278.1459009845742, 228.21451405505832, 280.0221376166164, 240.36643545676176, 258.27102201990243, 257.2115514235271, 224.96758025839526, 290.320399985394, 273.928617028051, 165.6500798644632, 276.4417610261621, 180.76681008599027, 88.07888667754747, 280.14478199531953, 69.78942029623985, 272.5757267771436, 253.0492313067674, 295.46603574629023, 248.50282260373115, 223.19391769983525, 236.2941004400015], 'lossList': [0.0, -1.3756628847122192, 0.0, 93.70690780639649, 0.0, 0.0, 0.0], 'rewardMean': 0.78974420986369, 'totalEpisodes': 104, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.4316323082251206, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.6536987051395159, 'errorList': [], 'lossList': [0.0, -1.360930431485176, 0.0, 72.41450435638427, 0.0, 0.0, 0.0], 'rewardMean': 0.7761396593912726, 'totalEpisodes': 117, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.386006475335476
'totalSteps': 14080, 'rewardStep': 0.6365366379663753, 'errorList': [], 'lossList': [0.0, -1.3468300634622574, 0.0, 40.43147874832153, 0.0, 0.0, 0.0], 'rewardMean': 0.7483112601298272, 'totalEpisodes': 125, 'stepsPerEpisode': 65, 'rewardPerEpisode': 56.61939400412446
'totalSteps': 15360, 'rewardStep': 0.5484005678368392, 'errorList': [], 'lossList': [0.0, -1.3496817737817763, 0.0, 59.879727611541746, 0.0, 0.0, 0.0], 'rewardMean': 0.7144832267675196, 'totalEpisodes': 133, 'stepsPerEpisode': 118, 'rewardPerEpisode': 89.29737242245444
'totalSteps': 16640, 'rewardStep': 0.8543614632912444, 'errorList': [], 'lossList': [0.0, -1.3449498343467712, 0.0, 25.076836347579956, 0.0, 0.0, 0.0], 'rewardMean': 0.7259724003733508, 'totalEpisodes': 140, 'stepsPerEpisode': 101, 'rewardPerEpisode': 82.85666939443267
'totalSteps': 17920, 'rewardStep': 0.9016237359588483, 'errorList': [], 'lossList': [0.0, -1.3299999606609345, 0.0, 15.006153091192246, 0.0, 0.0, 0.0], 'rewardMean': 0.7465046465807588, 'totalEpisodes': 145, 'stepsPerEpisode': 52, 'rewardPerEpisode': 43.745996513682464
'totalSteps': 19200, 'rewardStep': 0.7394083451054281, 'errorList': [], 'lossList': [0.0, -1.3131488782167435, 0.0, 13.824200413227082, 0.0, 0.0, 0.0], 'rewardMean': 0.7277868567467038, 'totalEpisodes': 150, 'stepsPerEpisode': 39, 'rewardPerEpisode': 28.80822726396321
'totalSteps': 20480, 'rewardStep': 0.9796981666535539, 'errorList': [7.3276743871383765, 10.025453672989158, 3.307783728796868, 1.3729211764095666, 6.8443919863392715, 3.828594858179398, 1.9515114135218277, 12.501552408061082, 1.1295711085480695, 3.850545990471346, 2.1454064715691046, 0.22745636530375538, 8.310483014207183, 0.5254400609583902, 4.462133538159103, 7.864358852121224, 2.712447253076917, 1.1918915773735639, 0.5234228810336765, 6.849232798840022, 3.6921687321399808, 2.4097043006195356, 12.157634936258493, 0.9776505139135179, 1.3407555179231396, 2.1896058124065547, 2.4847987585317126, 1.7503075474211642, 3.478596678997108, 0.20381629571246695, 8.928218533217631, 6.363536669750009, 15.235318417304638, 11.455130581537778, 0.4412181283007952, 0.5688283955687703, 9.352015082966794, 3.630643547706341, 7.662645526166534, 8.232395980147453, 2.95355431065686, 2.678752918917067, 4.258445032428664, 13.023994287874793, 4.672875752587393, 1.1230726854352238, 5.779053987760037, 1.3755035873709995, 4.2015486713144705, 3.2183879398065565], 'lossList': [0.0, -1.3028194737434386, 0.0, 4.9319458800554274, 0.0, 0.0, 0.0], 'rewardMean': 0.7466177977587419, 'totalEpisodes': 153, 'stepsPerEpisode': 465, 'rewardPerEpisode': 371.4047238726331, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.9801811540998988, 'errorList': [0.24056255714372113, 0.9128224894795169, 2.8798800526200443, 0.6263512752243888, 3.037991048454778, 0.5035376009289259, 0.5370791491229456, 1.1903773297770757, 0.9454795513048871, 3.7183496771174136, 1.2812075362281297, 1.1087651779111245, 1.3097851014427055, 1.1785134209849206, 1.365504548223705, 3.349404655912927, 0.4286653867658367, 0.8109693415025775, 2.048332950684108, 0.9739790203228335, 3.280085947806455, 0.6319827361550063, 0.4942230062715299, 0.9220783739297749, 1.0801908649080176, 0.8235200852318857, 0.5171112959079889, 0.5714157326095566, 1.3049781565375709, 1.9497950706658054, 0.45761825637037923, 1.975947220675589, 0.6371895086221843, 1.1910159016921378, 0.7300566616711562, 1.963366764280459, 0.4721806816211182, 1.4110436719976547, 1.7557956929771998, 1.2597340086443907, 0.9017960685671029, 3.7386865322428777, 1.0381683642118498, 0.9138397464556857, 1.530547374378737, 0.8451648574074697, 1.3960154945583994, 0.5472733870917278, 0.7504515019133872, 0.44082542044739653], 'lossList': [0.0, -1.286374762058258, 0.0, 27.107475332021714, 0.0, 0.0, 0.0], 'rewardMean': 0.7911818965144486, 'totalEpisodes': 157, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.70036783805949, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.7777640338624665, 'errorList': [], 'lossList': [0.0, -1.263093320131302, 0.0, 2.4959668558835983, 0.0, 0.0, 0.0], 'rewardMean': 0.8012924295912042, 'totalEpisodes': 160, 'stepsPerEpisode': 40, 'rewardPerEpisode': 32.226561069335276
'totalSteps': 24320, 'rewardStep': 0.883130898313696, 'errorList': [], 'lossList': [0.0, -1.2300213062763214, 0.0, 4.5225734382867815, 0.0, 0.0, 0.0], 'rewardMean': 0.7954803708227867, 'totalEpisodes': 162, 'stepsPerEpisode': 458, 'rewardPerEpisode': 393.7835856859103
'totalSteps': 25600, 'rewardStep': 0.8222085046286994, 'errorList': [], 'lossList': [0.0, -1.1995968502759933, 0.0, 2.1763926277309658, 0.0, 0.0, 0.0], 'rewardMean': 0.8123313507717048, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.0805273020296
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=121.47
