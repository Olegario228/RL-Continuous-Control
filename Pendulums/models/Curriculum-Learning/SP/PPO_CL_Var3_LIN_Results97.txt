#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 97
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8535575427506107, 'errorList': [], 'lossList': [0.0, -1.4462355518341063, 0.0, 31.85060744524002, 0.0, 0.0, 0.0], 'rewardMean': 0.6783291953273112, 'totalEpisodes': 12, 'stepsPerEpisode': 902, 'rewardPerEpisode': 642.232650775474
'totalSteps': 3840, 'rewardStep': 0.8965705741897014, 'errorList': [], 'lossList': [0.0, -1.455697934627533, 0.0, 40.603606884479525, 0.0, 0.0, 0.0], 'rewardMean': 0.7510763216147746, 'totalEpisodes': 15, 'stepsPerEpisode': 489, 'rewardPerEpisode': 397.24768313985265
'totalSteps': 5120, 'rewardStep': 0.6158492898312606, 'errorList': [], 'lossList': [0.0, -1.4482728165388108, 0.0, 16.584087060689924, 0.0, 0.0, 0.0], 'rewardMean': 0.717269563668896, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 867.3237309691835
'totalSteps': 6400, 'rewardStep': 0.28199900368059627, 'errorList': [], 'lossList': [0.0, -1.4249104642868042, 0.0, 48.476646728515625, 0.0, 0.0, 0.0], 'rewardMean': 0.630215451671236, 'totalEpisodes': 19, 'stepsPerEpisode': 187, 'rewardPerEpisode': 136.68642665974616
'totalSteps': 7680, 'rewardStep': 0.5830903043132711, 'errorList': [], 'lossList': [0.0, -1.4209900844097136, 0.0, 137.82211143493652, 0.0, 0.0, 0.0], 'rewardMean': 0.6223612604449086, 'totalEpisodes': 38, 'stepsPerEpisode': 58, 'rewardPerEpisode': 44.38541010642566
'totalSteps': 8960, 'rewardStep': 0.8036913539982599, 'errorList': [], 'lossList': [0.0, -1.4161557734012604, 0.0, 169.17452350616455, 0.0, 0.0, 0.0], 'rewardMean': 0.6482655595239588, 'totalEpisodes': 68, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.272744971880533
'totalSteps': 10240, 'rewardStep': 0.8712232517780083, 'errorList': [], 'lossList': [0.0, -1.4087963217496873, 0.0, 126.3170851135254, 0.0, 0.0, 0.0], 'rewardMean': 0.676135271055715, 'totalEpisodes': 107, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.46471579584357
'totalSteps': 11520, 'rewardStep': 0.6843219255259645, 'errorList': [], 'lossList': [0.0, -1.3985239171981811, 0.0, 68.62763158798218, 0.0, 0.0, 0.0], 'rewardMean': 0.6770448993301872, 'totalEpisodes': 125, 'stepsPerEpisode': 20, 'rewardPerEpisode': 13.529551317194887
'totalSteps': 12800, 'rewardStep': 0.17253684768957128, 'errorList': [], 'lossList': [0.0, -1.3986557245254516, 0.0, 86.18986206054687, 0.0, 0.0, 0.0], 'rewardMean': 0.6265940941661257, 'totalEpisodes': 145, 'stepsPerEpisode': 107, 'rewardPerEpisode': 64.86560653787107
'totalSteps': 14080, 'rewardStep': 0.7057912194838435, 'errorList': [], 'lossList': [0.0, -1.4179977256059646, 0.0, 23.48852065086365, 0.0, 0.0, 0.0], 'rewardMean': 0.6468631313241088, 'totalEpisodes': 155, 'stepsPerEpisode': 46, 'rewardPerEpisode': 39.16627963804559
'totalSteps': 15360, 'rewardStep': 0.9627129166785503, 'errorList': [12.799801583871988, 16.678473790347145, 2.5037415546860817, 49.553557378609845, 42.93366382285577, 27.53830942606265, 37.08729222702206, 17.53716507567801, 40.896721476090406, 27.843026169673585, 9.007430173035601, 21.348987236401673, 6.620586565245813, 10.909671843732479, 9.532656054688383, 13.338980341994738, 18.09220675372246, 24.918044278442753, 0.41030530353962147, 29.418031456297893, 12.927472273047579, 24.621278791086493, 2.2799075564275775, 17.590545427022466, 13.11542376810231, 19.626502581441564, 16.373589919890936, 21.311835489905764, 40.71915574726103, 56.008403060979674, 7.393163859007637, 16.04743429901943, 21.318995900422284, 14.582800125767825, 29.00085600742499, 9.55177027455604, 47.425301182282595, 10.032839126377764, 5.775850519690438, 47.403235006426605, 20.826849374078805, 39.49364044079555, 17.763868660010193, 36.1431321973207, 15.016058965017262, 38.98718498526367, 33.594208085804404, 7.870578274302809, 1.128490732809938, 21.40670424553084], 'lossList': [0.0, -1.4375037741661072, 0.0, 35.50919818878174, 0.0, 0.0, 0.0], 'rewardMean': 0.6577786687169027, 'totalEpisodes': 164, 'stepsPerEpisode': 92, 'rewardPerEpisode': 72.15312542609031, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7074929138495728, 'errorList': [], 'lossList': [0.0, -1.4424758279323577, 0.0, 10.848505535125732, 0.0, 0.0, 0.0], 'rewardMean': 0.6388709026828899, 'totalEpisodes': 171, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.0766348430003094
'totalSteps': 17920, 'rewardStep': 0.7599793756633141, 'errorList': [], 'lossList': [0.0, -1.4404570710659028, 0.0, 11.6034668469429, 0.0, 0.0, 0.0], 'rewardMean': 0.6532839112660953, 'totalEpisodes': 174, 'stepsPerEpisode': 289, 'rewardPerEpisode': 259.5031013811856
'totalSteps': 19200, 'rewardStep': 0.7526172420984336, 'errorList': [], 'lossList': [0.0, -1.416283056139946, 0.0, 20.46466099023819, 0.0, 0.0, 0.0], 'rewardMean': 0.700345735107879, 'totalEpisodes': 178, 'stepsPerEpisode': 352, 'rewardPerEpisode': 282.4429873878094
'totalSteps': 20480, 'rewardStep': 0.388542322801879, 'errorList': [], 'lossList': [0.0, -1.397309914827347, 0.0, 6.001866655051709, 0.0, 0.0, 0.0], 'rewardMean': 0.6808909369567396, 'totalEpisodes': 180, 'stepsPerEpisode': 220, 'rewardPerEpisode': 147.41807831188612
'totalSteps': 21760, 'rewardStep': 0.49532200745144245, 'errorList': [], 'lossList': [0.0, -1.3676468712091445, 0.0, 5.177483578324318, 0.0, 0.0, 0.0], 'rewardMean': 0.650054002302058, 'totalEpisodes': 182, 'stepsPerEpisode': 428, 'rewardPerEpisode': 321.8501855881272
'totalSteps': 23040, 'rewardStep': 0.9521647789578833, 'errorList': [2.2640214918407726, 2.727805078782008, 1.1635412692532432, 2.3874151163834294, 1.761262453977602, 1.6057921149198546, 1.6115452065872968, 1.20958048328469, 1.9602006072678966, 1.5531333009573765, 1.0294560878499464, 1.063424567056877, 2.4273123307144018, 2.394514388945312, 1.8315240413571192, 1.5526160490165986, 1.1064039707651239, 0.922590990441373, 1.1122224113139334, 0.750905299031548, 0.628925341827402, 0.8500241467702198, 1.8180491806415087, 1.3427672079795465, 2.336186876257594, 1.1902064898876685, 2.1892618579924212, 0.9058618180053717, 1.2030191796566578, 1.6764319849814546, 1.8743884013682632, 1.438321703894189, 1.1617779336624974, 1.6242083403104326, 0.8539108519299374, 1.0606047777880423, 0.8161378085386006, 1.5566271561843728, 1.2363419336714763, 0.9950650148887036, 1.7594119385540794, 1.7394403861375232, 2.3574359364417767, 0.790263385110982, 1.075690257447693, 2.111379105124038, 1.6679415359732344, 1.2243877032705113, 1.707875541808577, 2.0724313655102167], 'lossList': [0.0, -1.3431100845336914, 0.0, 5.846404563188553, 0.0, 0.0, 0.0], 'rewardMean': 0.6581481550200456, 'totalEpisodes': 184, 'stepsPerEpisode': 65, 'rewardPerEpisode': 56.23162425113774, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.7552608875323917, 'errorList': [], 'lossList': [0.0, -1.3293974298238753, 0.0, 3.8552879059314726, 0.0, 0.0, 0.0], 'rewardMean': 0.6652420512206881, 'totalEpisodes': 186, 'stepsPerEpisode': 145, 'rewardPerEpisode': 122.2729860965908
'totalSteps': 25600, 'rewardStep': 0.8249832667613649, 'errorList': [], 'lossList': [0.0, -1.3061588150262833, 0.0, 2.8380383610725404, 0.0, 0.0, 0.0], 'rewardMean': 0.7304866931278675, 'totalEpisodes': 186, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 885.5962532453815
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=105.09
