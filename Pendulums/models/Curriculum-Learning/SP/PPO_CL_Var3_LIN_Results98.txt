#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 98
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_LIN_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'lin', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5363488825990133, 'errorList': [], 'lossList': [0.0, -1.4227584993839264, 0.0, 28.782502332925798, 0.0, 0.0, 0.0], 'rewardMean': 0.7176396492128768, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 31.34128983397042
'totalSteps': 3840, 'rewardStep': 0.9387086631660899, 'errorList': [], 'lossList': [0.0, -1.4199591046571731, 0.0, 30.574979095458986, 0.0, 0.0, 0.0], 'rewardMean': 0.7913293205306146, 'totalEpisodes': 18, 'stepsPerEpisode': 484, 'rewardPerEpisode': 381.64105012469486
'totalSteps': 5120, 'rewardStep': 0.7684262410619435, 'errorList': [], 'lossList': [0.0, -1.4148932242393493, 0.0, 22.586327713131904, 0.0, 0.0, 0.0], 'rewardMean': 0.7856035506634468, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1012.597594551232
'totalSteps': 6400, 'rewardStep': 0.6710580929743492, 'errorList': [], 'lossList': [0.0, -1.4005281513929366, 0.0, 61.47639405250549, 0.0, 0.0, 0.0], 'rewardMean': 0.7626944591256273, 'totalEpisodes': 24, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.042318250290817
'totalSteps': 7680, 'rewardStep': 0.7360468546221488, 'errorList': [], 'lossList': [0.0, -1.3927732610702515, 0.0, 79.55453187942504, 0.0, 0.0, 0.0], 'rewardMean': 0.7582531917083809, 'totalEpisodes': 31, 'stepsPerEpisode': 67, 'rewardPerEpisode': 49.721522776603024
'totalSteps': 8960, 'rewardStep': 0.48598437643310605, 'errorList': [], 'lossList': [0.0, -1.3872233593463899, 0.0, 214.0259328842163, 0.0, 0.0, 0.0], 'rewardMean': 0.7193576466690559, 'totalEpisodes': 63, 'stepsPerEpisode': 73, 'rewardPerEpisode': 53.04321419104806
'totalSteps': 10240, 'rewardStep': 0.9227183601393453, 'errorList': [], 'lossList': [0.0, -1.386860556602478, 0.0, 187.04450790405272, 0.0, 0.0, 0.0], 'rewardMean': 0.744777735852842, 'totalEpisodes': 105, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.255547482727849
'totalSteps': 11520, 'rewardStep': 0.5851976270911577, 'errorList': [], 'lossList': [0.0, -1.399980239868164, 0.0, 75.84751817703247, 0.0, 0.0, 0.0], 'rewardMean': 0.7270466126570994, 'totalEpisodes': 133, 'stepsPerEpisode': 71, 'rewardPerEpisode': 57.932728102009214
'totalSteps': 12800, 'rewardStep': 0.738839326249093, 'errorList': [], 'lossList': [0.0, -1.4117526757717132, 0.0, 50.38450999259949, 0.0, 0.0, 0.0], 'rewardMean': 0.7282258840162987, 'totalEpisodes': 160, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.476920937111899
'totalSteps': 14080, 'rewardStep': 0.8588351413098703, 'errorList': [], 'lossList': [0.0, -1.4090846478939056, 0.0, 35.17827428817749, 0.0, 0.0, 0.0], 'rewardMean': 0.7242163565646117, 'totalEpisodes': 178, 'stepsPerEpisode': 44, 'rewardPerEpisode': 37.743763042626675
'totalSteps': 15360, 'rewardStep': 0.7643292226744893, 'errorList': [], 'lossList': [0.0, -1.4042656826972961, 0.0, 17.0026992559433, 0.0, 0.0, 0.0], 'rewardMean': 0.7470143905721593, 'totalEpisodes': 186, 'stepsPerEpisode': 58, 'rewardPerEpisode': 50.242557773305016
'totalSteps': 16640, 'rewardStep': 0.919129413628794, 'errorList': [], 'lossList': [0.0, -1.4096127724647523, 0.0, 11.821269221305847, 0.0, 0.0, 0.0], 'rewardMean': 0.7450564656184298, 'totalEpisodes': 196, 'stepsPerEpisode': 79, 'rewardPerEpisode': 68.66332720878901
'totalSteps': 17920, 'rewardStep': 0.8597666794109357, 'errorList': [], 'lossList': [0.0, -1.4013304120302201, 0.0, 8.253271905183793, 0.0, 0.0, 0.0], 'rewardMean': 0.754190509453329, 'totalEpisodes': 202, 'stepsPerEpisode': 93, 'rewardPerEpisode': 79.90270429680466
'totalSteps': 19200, 'rewardStep': 0.19559280370137744, 'errorList': [], 'lossList': [0.0, -1.3763944011926652, 0.0, 10.036505564451218, 0.0, 0.0, 0.0], 'rewardMean': 0.7066439805260318, 'totalEpisodes': 208, 'stepsPerEpisode': 170, 'rewardPerEpisode': 127.26775474172747
'totalSteps': 20480, 'rewardStep': 0.9795993206511513, 'errorList': [41.32150480618847, 122.5286903739878, 21.242784307637056, 118.36103550988237, 82.01986805019853, 20.992567037944774, 164.92374831395287, 105.91450325837494, 102.15496749728949, 0.5333523134308602, 118.76861315936488, 81.857965091973, 7.018627936309459, 54.91818490740507, 1.5054326193065553, 39.72088407812449, 0.5153081874755078, 129.7645637825795, 3.3689282230066255, 2.2250177624288807, 25.043480140837694, 69.01341416285544, 43.879069556850645, 114.78460939056689, 5.207441392180555, 70.31696430583719, 85.77126442669929, 69.75172963475288, 17.53680659520787, 12.271870305521105, 74.60914533047053, 70.84040090373063, 71.5777375329055, 3.840343708573146, 0.633697011894183, 1.2502066748832703, 137.3338408401032, 76.2039074782279, 30.71259927807162, 14.188095445163887, 0.5414412813838965, 116.7989213817148, 2.349847367346715, 62.856647343045154, 2.2605101491201816, 0.034127798073530086, 31.58364839032272, 0.4948596734721154, 0.7849403993868737, 1.0829603997419956], 'lossList': [0.0, -1.3466395449638366, 0.0, 6.621677596569061, 0.0, 0.0, 0.0], 'rewardMean': 0.7309992271289321, 'totalEpisodes': 215, 'stepsPerEpisode': 61, 'rewardPerEpisode': 54.28199984867718, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.9319167003636365, 'errorList': [130.02936037648783, 70.59073070222401, 114.12246524380437, 155.00994993059305, 188.82063136894726, 176.38783495161198, 151.23381012516518, 150.2790938133437, 171.3641255450344, 70.87241340361616, 153.70293097307828, 70.50007815539614, 117.46805456951449, 155.6852129065828, 3.3231609942618636, 102.10554474260468, 124.90077907695019, 181.59831002539886, 68.72295747885826, 134.80164891480027, 181.92211644417256, 7.1173318696980035, 70.42771365787323, 178.94142207048515, 122.64597069045638, 1.8758753808488666, 22.066514003701382, 172.97654736352928, 142.10877076831076, 0.7461141610665586, 21.525471532043838, 201.97626117141408, 163.3809496363918, 82.01380035753594, 112.94598925571871, 106.20218744483512, 109.580378228182, 126.6594238123513, 163.0354955200683, 9.492750110523815, 139.1438927487362, 88.34329060071663, 37.8317288514274, 206.8147391471914, 29.891592243440883, 46.48059282516843, 84.73934333897152, 181.8870781547681, 143.9407880832965, 92.35515712917167], 'lossList': [0.0, -1.3193150210380553, 0.0, 5.293666399717331, 0.0, 0.0, 0.0], 'rewardMean': 0.775592459521985, 'totalEpisodes': 219, 'stepsPerEpisode': 60, 'rewardPerEpisode': 56.0622813333721, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.8985781376054833, 'errorList': [], 'lossList': [0.0, -1.3097538667917252, 0.0, 8.057167823314666, 0.0, 0.0, 0.0], 'rewardMean': 0.7731784372685988, 'totalEpisodes': 225, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.699306408641487
'totalSteps': 24320, 'rewardStep': 0.753384326911077, 'errorList': [], 'lossList': [0.0, -1.318293063044548, 0.0, 5.741745463609695, 0.0, 0.0, 0.0], 'rewardMean': 0.7899971072505908, 'totalEpisodes': 229, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.489909243505538
'totalSteps': 25600, 'rewardStep': 0.6516194197807418, 'errorList': [], 'lossList': [0.0, -1.3131907165050507, 0.0, 4.530917050838471, 0.0, 0.0, 0.0], 'rewardMean': 0.7812751166037557, 'totalEpisodes': 230, 'stepsPerEpisode': 517, 'rewardPerEpisode': 413.7205646746704
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=20480, timeSpent=103.72
