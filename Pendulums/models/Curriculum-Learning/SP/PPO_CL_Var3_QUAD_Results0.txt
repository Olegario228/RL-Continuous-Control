#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.5689578198767129, 'errorList': [], 'lossList': [0.0, -1.408999491930008, 0.0, 33.064925060272216, 0.0, 0.0, 0.0], 'rewardMean': 0.5065902804261153, 'totalEpisodes': 57, 'stepsPerEpisode': 23, 'rewardPerEpisode': 16.721857359528027
'totalSteps': 3840, 'rewardStep': 0.8508002277858938, 'errorList': [], 'lossList': [0.0, -1.3856450563669205, 0.0, 39.99263157844543, 0.0, 0.0, 0.0], 'rewardMean': 0.6213269295460414, 'totalEpisodes': 81, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.544798002036159
'totalSteps': 5120, 'rewardStep': 0.720787517250176, 'errorList': [], 'lossList': [0.0, -1.3623356002569198, 0.0, 52.14297180175781, 0.0, 0.0, 0.0], 'rewardMean': 0.6461920764720751, 'totalEpisodes': 100, 'stepsPerEpisode': 48, 'rewardPerEpisode': 41.592929010084205
'totalSteps': 6400, 'rewardStep': 0.9176381229573658, 'errorList': [], 'lossList': [0.0, -1.3458233439922334, 0.0, 85.08489358901977, 0.0, 0.0, 0.0], 'rewardMean': 0.7004812857691333, 'totalEpisodes': 147, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.992090552772854
'totalSteps': 7680, 'rewardStep': 0.7233615972591058, 'errorList': [], 'lossList': [0.0, -1.3401135420799255, 0.0, 57.346863422393795, 0.0, 0.0, 0.0], 'rewardMean': 0.704294671017462, 'totalEpisodes': 175, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.352175652658567
'totalSteps': 8960, 'rewardStep': 0.8443892832272296, 'errorList': [], 'lossList': [0.0, -1.3353406858444214, 0.0, 43.58156922340393, 0.0, 0.0, 0.0], 'rewardMean': 0.7243081870474288, 'totalEpisodes': 189, 'stepsPerEpisode': 99, 'rewardPerEpisode': 85.2285253023983
'totalSteps': 10240, 'rewardStep': 0.5774261622757648, 'errorList': [], 'lossList': [0.0, -1.3401083594560623, 0.0, 25.31633186340332, 0.0, 0.0, 0.0], 'rewardMean': 0.7059479339509709, 'totalEpisodes': 196, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.95355998878566
'totalSteps': 11520, 'rewardStep': 0.61458525884971, 'errorList': [], 'lossList': [0.0, -1.3343730998039245, 0.0, 39.03673652648926, 0.0, 0.0, 0.0], 'rewardMean': 0.6957965256063863, 'totalEpisodes': 203, 'stepsPerEpisode': 68, 'rewardPerEpisode': 51.67381557139029
'totalSteps': 12800, 'rewardStep': 0.3447022022162346, 'errorList': [], 'lossList': [0.0, -1.3040999066829682, 0.0, 11.741648773550986, 0.0, 0.0, 0.0], 'rewardMean': 0.6606870932673712, 'totalEpisodes': 205, 'stepsPerEpisode': 818, 'rewardPerEpisode': 564.9633974118649
'totalSteps': 14080, 'rewardStep': 0.5876680562782942, 'errorList': [], 'lossList': [0.0, -1.2712806248664856, 0.0, 8.237634418606758, 0.0, 0.0, 0.0], 'rewardMean': 0.6750316247976488, 'totalEpisodes': 206, 'stepsPerEpisode': 952, 'rewardPerEpisode': 740.077364031103
'totalSteps': 15360, 'rewardStep': 0.510493213104571, 'errorList': [], 'lossList': [0.0, -1.2546295028924943, 0.0, 7.648652314543724, 0.0, 0.0, 0.0], 'rewardMean': 0.6691851641204345, 'totalEpisodes': 207, 'stepsPerEpisode': 266, 'rewardPerEpisode': 209.4727088534978
'totalSteps': 16640, 'rewardStep': 0.9642851685945862, 'errorList': [0.007522473696664604, 0.020611853079419103, 0.046981259551807514, 0.031796287604557466, 0.03921242186464221, 0.014730634877168238, 0.05006068165348107, 0.07375343671765566, 0.05115056741330784, 0.020435556396509715, 0.0475058247027398, 0.08104390396284428, 0.0151532694946559, 0.04698110300985591, 0.06920116899648021, 0.01815398139939108, 0.03845719772564397, 0.11829824744786303, 0.020635117394052955, 0.012990506577736573, 0.011052000484547147, 0.047173877679120874, 0.014644728747178636, 0.02054790184028585, 0.024594482203919882, 0.030720651472333233, 0.04048254353987588, 0.02600324315455843, 0.0157098495213114, 0.02379619941912338, 0.02185148972588021, 0.02009772185196737, 0.012929848067218882, 0.009064648453762161, 0.051350930097300136, 0.013021820564484558, 0.041078656708289266, 0.0448283592592427, 0.02095718475757022, 0.019251472137506602, 0.026450438452093713, 0.029524847985789672, 0.05350969856996243, 0.04095997851690058, 0.031818707823302095, 0.03737233448911407, 0.027555179100457686, 0.01116742863192896, 0.008253090183732027, 0.03993332472868311], 'lossList': [0.0, -1.2496229457855224, 0.0, 5.448763303905725, 0.0, 0.0, 0.0], 'rewardMean': 0.6805336582013038, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.6693117129446, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=16640, timeSpent=52.5
