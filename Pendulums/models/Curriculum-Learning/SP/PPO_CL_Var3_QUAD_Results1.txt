#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5956455450469885, 'errorList': [], 'lossList': [0.0, -1.415670256614685, 0.0, 31.04442521095276, 0.0, 0.0, 0.0], 'rewardMean': 0.705347838944198, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.204449827012006
'totalSteps': 3840, 'rewardStep': 0.621684557950523, 'errorList': [], 'lossList': [0.0, -1.403967941403389, 0.0, 43.675010194778444, 0.0, 0.0, 0.0], 'rewardMean': 0.677460078612973, 'totalEpisodes': 73, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.14888337991963
'totalSteps': 5120, 'rewardStep': 0.5419816463605718, 'errorList': [], 'lossList': [0.0, -1.4027080357074737, 0.0, 62.075177631378175, 0.0, 0.0, 0.0], 'rewardMean': 0.6435904705498727, 'totalEpisodes': 92, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.076781174497486
'totalSteps': 6400, 'rewardStep': 0.8561125268525529, 'errorList': [], 'lossList': [0.0, -1.3990442925691604, 0.0, 90.09710689544677, 0.0, 0.0, 0.0], 'rewardMean': 0.6860948818104088, 'totalEpisodes': 126, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.489835690347608
'totalSteps': 7680, 'rewardStep': 0.46104218700704397, 'errorList': [], 'lossList': [0.0, -1.4040417766571045, 0.0, 45.92063876628876, 0.0, 0.0, 0.0], 'rewardMean': 0.6485860993431813, 'totalEpisodes': 137, 'stepsPerEpisode': 153, 'rewardPerEpisode': 115.81323174243472
'totalSteps': 8960, 'rewardStep': 0.42730072552636317, 'errorList': [], 'lossList': [0.0, -1.4077656590938568, 0.0, 32.85589882135391, 0.0, 0.0, 0.0], 'rewardMean': 0.6169739030836359, 'totalEpisodes': 147, 'stepsPerEpisode': 205, 'rewardPerEpisode': 146.31570987253303
'totalSteps': 10240, 'rewardStep': 0.8177237077357741, 'errorList': [], 'lossList': [0.0, -1.3968549907207488, 0.0, 21.312055060863496, 0.0, 0.0, 0.0], 'rewardMean': 0.6420676286651531, 'totalEpisodes': 152, 'stepsPerEpisode': 72, 'rewardPerEpisode': 60.881963385284436
'totalSteps': 11520, 'rewardStep': 0.5966640125203191, 'errorList': [], 'lossList': [0.0, -1.3921767359972, 0.0, 11.028551551103591, 0.0, 0.0, 0.0], 'rewardMean': 0.6370227824268382, 'totalEpisodes': 156, 'stepsPerEpisode': 53, 'rewardPerEpisode': 42.86040364015134
'totalSteps': 12800, 'rewardStep': 0.529716160173467, 'errorList': [], 'lossList': [0.0, -1.3773480534553528, 0.0, 9.943823043107987, 0.0, 0.0, 0.0], 'rewardMean': 0.626292120201501, 'totalEpisodes': 161, 'stepsPerEpisode': 133, 'rewardPerEpisode': 96.76806061551213
'totalSteps': 14080, 'rewardStep': 0.8038764993065334, 'errorList': [], 'lossList': [0.0, -1.342335770726204, 0.0, 7.304084017276764, 0.0, 0.0, 0.0], 'rewardMean': 0.6251747568480137, 'totalEpisodes': 164, 'stepsPerEpisode': 230, 'rewardPerEpisode': 200.70780596347456
'totalSteps': 15360, 'rewardStep': 0.7955771896806083, 'errorList': [], 'lossList': [0.0, -1.3353064167499542, 0.0, 5.740169194340706, 0.0, 0.0, 0.0], 'rewardMean': 0.6451679213113757, 'totalEpisodes': 166, 'stepsPerEpisode': 275, 'rewardPerEpisode': 223.68021730812214
'totalSteps': 16640, 'rewardStep': 0.42504828950805323, 'errorList': [], 'lossList': [0.0, -1.3143533504009246, 0.0, 4.3605657267570495, 0.0, 0.0, 0.0], 'rewardMean': 0.6255042944671286, 'totalEpisodes': 167, 'stepsPerEpisode': 570, 'rewardPerEpisode': 400.8234804492613
'totalSteps': 17920, 'rewardStep': 0.6425902465947637, 'errorList': [], 'lossList': [0.0, -1.2975755774974822, 0.0, 4.571146368980408, 0.0, 0.0, 0.0], 'rewardMean': 0.6355651544905478, 'totalEpisodes': 169, 'stepsPerEpisode': 233, 'rewardPerEpisode': 183.43512291965592
'totalSteps': 19200, 'rewardStep': 0.8256662718865952, 'errorList': [], 'lossList': [0.0, -1.2494133639335632, 0.0, 2.3326090914011, 0.0, 0.0, 0.0], 'rewardMean': 0.6325205289939521, 'totalEpisodes': 169, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1006.4539381058348
'totalSteps': 20480, 'rewardStep': 0.8098605034470071, 'errorList': [], 'lossList': [0.0, -1.160425402522087, 0.0, 1.4962770919501782, 0.0, 0.0, 0.0], 'rewardMean': 0.6674023606379483, 'totalEpisodes': 169, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.8525166485524
'totalSteps': 21760, 'rewardStep': 0.798954079514704, 'errorList': [], 'lossList': [0.0, -1.0864885526895522, 0.0, 1.318494782820344, 0.0, 0.0, 0.0], 'rewardMean': 0.7045676960367826, 'totalEpisodes': 169, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.6863756581058
'totalSteps': 23040, 'rewardStep': 0.9655701365759144, 'errorList': [0.07088535191555248, 0.18514757918673175, 0.07069049814528923, 0.0748062061938073, 0.07216872467082021, 0.08279348608474604, 0.05441450715725136, 0.10656436590064952, 0.09516799002378455, 0.05019784078168806, 0.12061103140420373, 0.08506391128349046, 0.12802428008420194, 0.06694578805145998, 0.041915182387713995, 0.06231672824103614, 0.15045281084315454, 0.1380705683002764, 0.08943412441059417, 0.06679003162709334, 0.05104846253167116, 0.05993169190665732, 0.05092982995328156, 0.0801741917489048, 0.05886088073005812, 0.16355612366024422, 0.09314042997675662, 0.0637360243264541, 0.07816689109044969, 0.05563763039255588, 0.09948809570455813, 0.06838297736467687, 0.08953823662016078, 0.06282661800505657, 0.08476902405106983, 0.062199457752950514, 0.10312564101654997, 0.0825528470357215, 0.1156142685492654, 0.15331605948029067, 0.10306826810248651, 0.09814316907706211, 0.11170659996786486, 0.04950365593647018, 0.12114056379830025, 0.1604818400623686, 0.15538179880983818, 0.061759951468254096, 0.08893143878195922, 0.13150079312763785], 'lossList': [0.0, -1.0542802673578262, 0.0, 1.104130339603871, 0.0, 0.0, 0.0], 'rewardMean': 0.7193523389207966, 'totalEpisodes': 169, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1193.7560320050443, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=69.78
