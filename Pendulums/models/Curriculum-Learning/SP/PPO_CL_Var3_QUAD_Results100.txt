#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 100
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7341361937430552, 'errorList': [], 'lossList': [0.0, -1.4131451916694642, 0.0, 32.48867074966431, 0.0, 0.0, 0.0], 'rewardMean': 0.5891794673592865, 'totalEpisodes': 56, 'stepsPerEpisode': 22, 'rewardPerEpisode': 13.391045814994602
'totalSteps': 3840, 'rewardStep': 0.1568820679853009, 'errorList': [], 'lossList': [0.0, -1.402818723320961, 0.0, 39.068427352905275, 0.0, 0.0, 0.0], 'rewardMean': 0.4450803342346246, 'totalEpisodes': 73, 'stepsPerEpisode': 92, 'rewardPerEpisode': 64.54799508351196
'totalSteps': 5120, 'rewardStep': 0.69687405451022, 'errorList': [], 'lossList': [0.0, -1.3830112195014954, 0.0, 40.85049667835236, 0.0, 0.0, 0.0], 'rewardMean': 0.5080287643035235, 'totalEpisodes': 84, 'stepsPerEpisode': 189, 'rewardPerEpisode': 145.90929683573214
'totalSteps': 6400, 'rewardStep': 0.5453802788523282, 'errorList': [], 'lossList': [0.0, -1.3635744285583495, 0.0, 29.399362576007842, 0.0, 0.0, 0.0], 'rewardMean': 0.5154990672132844, 'totalEpisodes': 90, 'stepsPerEpisode': 342, 'rewardPerEpisode': 248.36107619312722
'totalSteps': 7680, 'rewardStep': 0.7676156307322761, 'errorList': [], 'lossList': [0.0, -1.3468537306785584, 0.0, 33.49448817253113, 0.0, 0.0, 0.0], 'rewardMean': 0.5575184944664496, 'totalEpisodes': 94, 'stepsPerEpisode': 311, 'rewardPerEpisode': 217.00146157873868
'totalSteps': 8960, 'rewardStep': 0.9599874825410155, 'errorList': [], 'lossList': [0.0, -1.3311127376556398, 0.0, 82.89425354003906, 0.0, 0.0, 0.0], 'rewardMean': 0.6150140641913876, 'totalEpisodes': 102, 'stepsPerEpisode': 82, 'rewardPerEpisode': 71.79066445778675
'totalSteps': 10240, 'rewardStep': 0.5229541103268422, 'errorList': [], 'lossList': [0.0, -1.3224997472763063, 0.0, 189.83694854736328, 0.0, 0.0, 0.0], 'rewardMean': 0.6035065699583195, 'totalEpisodes': 128, 'stepsPerEpisode': 112, 'rewardPerEpisode': 86.02301306495538
'totalSteps': 11520, 'rewardStep': 0.9400754701601015, 'errorList': [224.07474037465758, 59.18334972851953, 38.213796212773275, 215.54035958050977, 204.35944759742733, 177.08328002137654, 135.6935766936915, 152.96018133703137, 205.19352298404783, 177.71500629924964, 105.28986973063422, 137.443592354386, 136.25750017817015, 83.58674338602941, 201.55940577382955, 208.30822404904256, 124.98366738153183, 106.10961846036743, 165.08711305455134, 176.91512694143213, 217.51263109010287, 166.41233812357535, 177.0015727777516, 192.87030037744245, 174.84921809258796, 151.96733025797576, 175.3994564030843, 148.76711856989886, 134.88755053778888, 153.49745178135987, 132.55630379490802, 200.30834284409516, 206.54142525905456, 191.08014578949206, 205.26403273458973, 27.82349950778848, 156.05779723195124, 219.93096370046885, 181.7473715886486, 208.03669228456818, 145.3683212158707, 194.6602800048274, 32.69035631443662, 102.47846073927137, 54.94963395462402, 214.7350724197817, 217.63199882674522, 16.526894302806415, 39.47692779562358, 184.81736498992555], 'lossList': [0.0, -1.3045063650608062, 0.0, 86.43245719909667, 0.0, 0.0, 0.0], 'rewardMean': 0.6409031144251842, 'totalEpisodes': 152, 'stepsPerEpisode': 43, 'rewardPerEpisode': 38.70019062520571, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.9681464333263358, 'errorList': [7.007739329179318, 39.76337119194797, 9.82701639185937, 22.24476209351022, 49.03545299409188, 37.67719457767972, 26.004029018457548, 12.031826358531232, 31.09301400369771, 30.385897444385417, 13.98406382041311, 18.03450533297474, 33.85390566093892, 21.869293778689116, 29.4579692221999, 13.90444157696676, 2.873176692238498, 2.8031770141957777, 6.041615793417955, 33.86948461002197, 37.16120130484722, 12.704561334375624, 25.517591305642643, 7.71807194244592, 15.199770281247035, 12.964654890373849, 18.77778342942991, 11.155349367574013, 9.404210230654376, 30.877477047944097, 6.4232693768541225, 26.047806275342147, 13.243070129237053, 24.34313115720171, 35.554572393695025, 18.473895217280297, 43.11637832993802, 32.58003902500585, 49.6183901538173, 34.16237761631374, 12.296716847304337, 10.353084506398007, 30.429503964043356, 10.293773731412232, 9.574883924223764, 8.105995264073176, 20.13778708249121, 42.241838244875886, 14.218770259915011, 39.97018564459005], 'lossList': [0.0, -1.2852800285816193, 0.0, 50.77844350814819, 0.0, 0.0, 0.0], 'rewardMean': 0.6736274463152994, 'totalEpisodes': 167, 'stepsPerEpisode': 143, 'rewardPerEpisode': 126.51846510604963, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.7138540366411479, 'errorList': [], 'lossList': [0.0, -1.2565294110774994, 0.0, 25.592136466503142, 0.0, 0.0, 0.0], 'rewardMean': 0.7005905758818625, 'totalEpisodes': 176, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.955073173285207
'totalSteps': 15360, 'rewardStep': 0.7053337058250536, 'errorList': [], 'lossList': [0.0, -1.2382745462656022, 0.0, 20.286068925857546, 0.0, 0.0, 0.0], 'rewardMean': 0.6977103270900623, 'totalEpisodes': 185, 'stepsPerEpisode': 68, 'rewardPerEpisode': 56.16845909824684
'totalSteps': 16640, 'rewardStep': 0.5926932101087088, 'errorList': [], 'lossList': [0.0, -1.224178009033203, 0.0, 7.741318116188049, 0.0, 0.0, 0.0], 'rewardMean': 0.741291441302403, 'totalEpisodes': 189, 'stepsPerEpisode': 214, 'rewardPerEpisode': 173.84134616824113
'totalSteps': 17920, 'rewardStep': 0.7720077436028074, 'errorList': [], 'lossList': [0.0, -1.2079362386465073, 0.0, 7.183935546278954, 0.0, 0.0, 0.0], 'rewardMean': 0.7488048102116617, 'totalEpisodes': 194, 'stepsPerEpisode': 297, 'rewardPerEpisode': 248.68950633313497
'totalSteps': 19200, 'rewardStep': 0.6283825242944787, 'errorList': [], 'lossList': [0.0, -1.1829508972167968, 0.0, 10.498200488090514, 0.0, 0.0, 0.0], 'rewardMean': 0.7571050347558768, 'totalEpisodes': 198, 'stepsPerEpisode': 196, 'rewardPerEpisode': 157.22963271330457
'totalSteps': 20480, 'rewardStep': 0.871346892756837, 'errorList': [], 'lossList': [0.0, -1.1584579932689667, 0.0, 9.913925009965897, 0.0, 0.0, 0.0], 'rewardMean': 0.7674781609583328, 'totalEpisodes': 201, 'stepsPerEpisode': 83, 'rewardPerEpisode': 69.18422736713616
'totalSteps': 21760, 'rewardStep': 0.6627297389085208, 'errorList': [], 'lossList': [0.0, -1.1448323208093643, 0.0, 4.350982592701912, 0.0, 0.0, 0.0], 'rewardMean': 0.7377523865950834, 'totalEpisodes': 204, 'stepsPerEpisode': 348, 'rewardPerEpisode': 278.5017817197757
'totalSteps': 23040, 'rewardStep': 0.5723762056760295, 'errorList': [], 'lossList': [0.0, -1.1234051954746247, 0.0, 2.8406417694687844, 0.0, 0.0, 0.0], 'rewardMean': 0.7426945961300021, 'totalEpisodes': 205, 'stepsPerEpisode': 1219, 'rewardPerEpisode': 898.8102088356203
'totalSteps': 24320, 'rewardStep': 0.6787998368802036, 'errorList': [], 'lossList': [0.0, -1.080253345966339, 0.0, 2.1430821929872037, 0.0, 0.0, 0.0], 'rewardMean': 0.7165670328020124, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.4766238718262
'totalSteps': 25600, 'rewardStep': 0.8502661387642619, 'errorList': [], 'lossList': [0.0, -1.0445544445514678, 0.0, 1.3178016871213913, 0.0, 0.0, 0.0], 'rewardMean': 0.704779003345805, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.1265560932518
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=104.69
