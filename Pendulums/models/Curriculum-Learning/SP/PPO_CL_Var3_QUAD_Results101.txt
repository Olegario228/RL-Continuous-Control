#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 101
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5970743884757028, 'errorList': [], 'lossList': [0.0, -1.4200319582223893, 0.0, 31.15204571723938, 0.0, 0.0, 0.0], 'rewardMean': 0.7060622606585552, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.302781378979596
'totalSteps': 3840, 'rewardStep': 0.8299561895351347, 'errorList': [], 'lossList': [0.0, -1.401192564368248, 0.0, 40.74918669700622, 0.0, 0.0, 0.0], 'rewardMean': 0.7473602369507484, 'totalEpisodes': 71, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.053767291474443
'totalSteps': 5120, 'rewardStep': 0.948961899698033, 'errorList': [], 'lossList': [0.0, -1.3892078149318694, 0.0, 28.043263921737672, 0.0, 0.0, 0.0], 'rewardMean': 0.7977606526375696, 'totalEpisodes': 80, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.038612045310588
'totalSteps': 6400, 'rewardStep': 0.9196944756548994, 'errorList': [], 'lossList': [0.0, -1.3880936485528945, 0.0, 46.47861210823059, 0.0, 0.0, 0.0], 'rewardMean': 0.8221474172410355, 'totalEpisodes': 88, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.4646740933011944
'totalSteps': 7680, 'rewardStep': 0.8478115644708055, 'errorList': [], 'lossList': [0.0, -1.3749258106946944, 0.0, 17.05364475786686, 0.0, 0.0, 0.0], 'rewardMean': 0.8264247751126638, 'totalEpisodes': 90, 'stepsPerEpisode': 188, 'rewardPerEpisode': 160.52579322734277
'totalSteps': 8960, 'rewardStep': 0.5036707515176566, 'errorList': [], 'lossList': [0.0, -1.3688377338647841, 0.0, 41.17609411239624, 0.0, 0.0, 0.0], 'rewardMean': 0.7803170574562343, 'totalEpisodes': 95, 'stepsPerEpisode': 444, 'rewardPerEpisode': 314.07349777718446
'totalSteps': 10240, 'rewardStep': 0.6305403518897679, 'errorList': [], 'lossList': [0.0, -1.3572573560476302, 0.0, 114.9898719215393, 0.0, 0.0, 0.0], 'rewardMean': 0.761594969260426, 'totalEpisodes': 114, 'stepsPerEpisode': 29, 'rewardPerEpisode': 21.882457049442824
'totalSteps': 11520, 'rewardStep': 0.7697374842014763, 'errorList': [], 'lossList': [0.0, -1.353743671774864, 0.0, 87.45700799942017, 0.0, 0.0, 0.0], 'rewardMean': 0.762499693142765, 'totalEpisodes': 130, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.568927016158796
'totalSteps': 12800, 'rewardStep': 0.7280211212275538, 'errorList': [], 'lossList': [0.0, -1.3487753194570542, 0.0, 23.356664271354674, 0.0, 0.0, 0.0], 'rewardMean': 0.7590518359512439, 'totalEpisodes': 142, 'stepsPerEpisode': 86, 'rewardPerEpisode': 63.460575613621586
'totalSteps': 14080, 'rewardStep': 0.9457933539384944, 'errorList': [0.7829066829247039, 1.9961848994732365, 1.0066248493871364, 2.1447604168871557, 2.4238210144271357, 2.5642743115116424, 1.883754026373842, 1.8438168798806918, 0.9277716109110875, 1.1084904665044635, 2.09598206719145, 2.3835268885817613, 1.2723355494646096, 0.42395130547414994, 0.47714099555196143, 0.45401718703486327, 1.1857038321190412, 2.143840305019096, 1.3148565456381873, 0.5094795704992379, 0.3008501625565598, 1.1771488980550684, 0.46871334398499737, 1.3818392529955408, 1.2832344918534666, 0.12683078036834833, 0.6636328590057002, 1.2874825172296236, 0.23712250068407048, 2.0649762341403934, 1.829967157799735, 0.2804818542094857, 0.5642884103544236, 0.42166885135434895, 1.3339614233268813, 1.2107668609744706, 0.5940617731097358, 0.40877591835639415, 0.4647857161419769, 2.103752583831505, 0.8644366016806293, 0.30960355571646386, 1.2022640145989398, 0.7667478135951619, 2.1355515800802047, 0.9626125653793383, 0.3677204464684759, 1.278873087931387, 1.4135112110543828, 1.3583702761934973], 'lossList': [0.0, -1.329428904056549, 0.0, 11.264403084516525, 0.0, 0.0, 0.0], 'rewardMean': 0.7721261580609525, 'totalEpisodes': 147, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.33708980654712, 'successfulTests': 1
'totalSteps': 15360, 'rewardStep': 0.8778065015311866, 'errorList': [], 'lossList': [0.0, -1.3141414946317673, 0.0, 8.890708618164062, 0.0, 0.0, 0.0], 'rewardMean': 0.8001993693665008, 'totalEpisodes': 150, 'stepsPerEpisode': 329, 'rewardPerEpisode': 272.410452059872
'totalSteps': 16640, 'rewardStep': 0.820018568174808, 'errorList': [], 'lossList': [0.0, -1.3076572000980378, 0.0, 12.93702751159668, 0.0, 0.0, 0.0], 'rewardMean': 0.7992056072304681, 'totalEpisodes': 151, 'stepsPerEpisode': 69, 'rewardPerEpisode': 57.57581236141111
'totalSteps': 17920, 'rewardStep': 0.6861925472400295, 'errorList': [], 'lossList': [0.0, -1.2952800798416138, 0.0, 7.61171236038208, 0.0, 0.0, 0.0], 'rewardMean': 0.7729286719846679, 'totalEpisodes': 155, 'stepsPerEpisode': 140, 'rewardPerEpisode': 113.70651126586004
'totalSteps': 19200, 'rewardStep': 0.5495897564488008, 'errorList': [], 'lossList': [0.0, -1.2806081420183182, 0.0, 6.826664683818817, 0.0, 0.0, 0.0], 'rewardMean': 0.7359182000640578, 'totalEpisodes': 158, 'stepsPerEpisode': 434, 'rewardPerEpisode': 343.00269277872604
'totalSteps': 20480, 'rewardStep': 0.6785048021336145, 'errorList': [], 'lossList': [0.0, -1.2558896803855897, 0.0, 3.783593794852495, 0.0, 0.0, 0.0], 'rewardMean': 0.7189875238303389, 'totalEpisodes': 159, 'stepsPerEpisode': 1233, 'rewardPerEpisode': 1029.5076524782173
'totalSteps': 21760, 'rewardStep': 0.8208609565459534, 'errorList': [], 'lossList': [0.0, -1.2217164027690888, 0.0, 1.5573224993050099, 0.0, 0.0, 0.0], 'rewardMean': 0.7507065443331685, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.1583054716543
'totalSteps': 23040, 'rewardStep': 0.927404976888604, 'errorList': [], 'lossList': [0.0, -1.2036653554439545, 0.0, 1.7086952154710888, 0.0, 0.0, 0.0], 'rewardMean': 0.7803930068330522, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.3658593262808
'totalSteps': 24320, 'rewardStep': 0.8234810655998797, 'errorList': [], 'lossList': [0.0, -1.1782682013511658, 0.0, 1.2744604717567563, 0.0, 0.0, 0.0], 'rewardMean': 0.7857673649728925, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.083060983319
'totalSteps': 25600, 'rewardStep': 0.9850860213104556, 'errorList': [0.31878572431051, 0.1805306738949134, 0.27191055798532365, 0.22269992812089567, 0.27164833620221873, 0.18892364561199457, 0.2296613216024078, 0.21551219536145036, 0.2388510225386449, 0.254289308628177, 0.3792370290584094, 0.16855322769772518, 0.24411300137668565, 0.37776295021715867, 0.2713508946718218, 0.2543403655864098, 0.2759815486819296, 0.2404560935578114, 0.23743741613814037, 0.17264221633981902, 0.22761331001932208, 0.20883317763445447, 0.27060458108642094, 0.25420074237987356, 0.27231695711602555, 0.3582890649482452, 0.3369165822017639, 0.20474693024828122, 0.28582512095684665, 0.24730290583070283, 0.22946497939706645, 0.2871079708408564, 0.2649366174775114, 0.24243678336315924, 0.30107599553565384, 0.2292071175131189, 0.36466363868156554, 0.254203288761039, 0.22149241154760366, 0.26731919505574797, 0.3384593176893078, 0.3193341819490937, 0.22646033661514553, 0.3047241555925533, 0.39107655378959344, 0.36853003247422317, 0.24083842930709906, 0.22517101984434515, 0.25911073287216563, 0.220043470933592], 'lossList': [0.0, -1.1398584997653962, 0.0, 1.2967430904693902, 0.0, 0.0, 0.0], 'rewardMean': 0.8114738549811827, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1216.270124605795, 'successfulTests': 4
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=25600, timeSpent=104.67
