#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 103
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.5431679457291241, 'errorList': [], 'lossList': [0.0, -1.4378193020820618, 0.0, 32.21238406658173, 0.0, 0.0, 0.0], 'rewardMean': 0.5240535862347544, 'totalEpisodes': 57, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.94225556429931
'totalSteps': 3840, 'rewardStep': 0.9239705829153086, 'errorList': [], 'lossList': [0.0, -1.434510245323181, 0.0, 38.510329456329345, 0.0, 0.0, 0.0], 'rewardMean': 0.6573592517949391, 'totalEpisodes': 71, 'stepsPerEpisode': 48, 'rewardPerEpisode': 37.893859580333114
'totalSteps': 5120, 'rewardStep': 0.8995982984940171, 'errorList': [], 'lossList': [0.0, -1.4312567430734635, 0.0, 30.67166790485382, 0.0, 0.0, 0.0], 'rewardMean': 0.7179190134697087, 'totalEpisodes': 81, 'stepsPerEpisode': 71, 'rewardPerEpisode': 61.13541435331152
'totalSteps': 6400, 'rewardStep': 0.7709373951123939, 'errorList': [], 'lossList': [0.0, -1.4327211141586305, 0.0, 42.79745579719543, 0.0, 0.0, 0.0], 'rewardMean': 0.7285226897982457, 'totalEpisodes': 92, 'stepsPerEpisode': 164, 'rewardPerEpisode': 132.65548523486953
'totalSteps': 7680, 'rewardStep': 0.828509302975854, 'errorList': [], 'lossList': [0.0, -1.423473945260048, 0.0, 48.292466707229615, 0.0, 0.0, 0.0], 'rewardMean': 0.7451871253278471, 'totalEpisodes': 102, 'stepsPerEpisode': 85, 'rewardPerEpisode': 70.94944772331452
'totalSteps': 8960, 'rewardStep': 0.9721338359012796, 'errorList': [], 'lossList': [0.0, -1.4013710474967958, 0.0, 74.75003894805909, 0.0, 0.0, 0.0], 'rewardMean': 0.7776080839811945, 'totalEpisodes': 111, 'stepsPerEpisode': 70, 'rewardPerEpisode': 57.933684925796626
'totalSteps': 10240, 'rewardStep': 0.8000852662610903, 'errorList': [], 'lossList': [0.0, -1.3993683850765228, 0.0, 118.69323081970215, 0.0, 0.0, 0.0], 'rewardMean': 0.7804177317661816, 'totalEpisodes': 129, 'stepsPerEpisode': 41, 'rewardPerEpisode': 30.216361557000198
'totalSteps': 11520, 'rewardStep': 0.8556657179645475, 'errorList': [], 'lossList': [0.0, -1.408268967270851, 0.0, 42.113462438583376, 0.0, 0.0, 0.0], 'rewardMean': 0.7887786191215556, 'totalEpisodes': 140, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.01967848107897
'totalSteps': 12800, 'rewardStep': 0.4393445387541418, 'errorList': [], 'lossList': [0.0, -1.4092111146450044, 0.0, 23.55514499425888, 0.0, 0.0, 0.0], 'rewardMean': 0.7538352110848142, 'totalEpisodes': 148, 'stepsPerEpisode': 137, 'rewardPerEpisode': 106.52420193949169
'totalSteps': 14080, 'rewardStep': 0.5739955366756591, 'errorList': [], 'lossList': [0.0, -1.3990461945533752, 0.0, 18.572896807193757, 0.0, 0.0, 0.0], 'rewardMean': 0.7607408420783417, 'totalEpisodes': 151, 'stepsPerEpisode': 220, 'rewardPerEpisode': 126.84692946052935
'totalSteps': 15360, 'rewardStep': 0.6636011378933018, 'errorList': [], 'lossList': [0.0, -1.3744808226823806, 0.0, 8.058525609970093, 0.0, 0.0, 0.0], 'rewardMean': 0.7727841612947595, 'totalEpisodes': 151, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 931.6899749013884
'totalSteps': 16640, 'rewardStep': 0.7794029815845696, 'errorList': [], 'lossList': [0.0, -1.3492813885211945, 0.0, 5.344970247149467, 0.0, 0.0, 0.0], 'rewardMean': 0.7583274011616854, 'totalEpisodes': 151, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1071.6049767467516
'totalSteps': 17920, 'rewardStep': 0.8361189639636829, 'errorList': [], 'lossList': [0.0, -1.331437869668007, 0.0, 3.9919490736722945, 0.0, 0.0, 0.0], 'rewardMean': 0.751979467708652, 'totalEpisodes': 151, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.194606112253
'totalSteps': 19200, 'rewardStep': 0.9130734312303533, 'errorList': [], 'lossList': [0.0, -1.3030609822273254, 0.0, 2.978213303387165, 0.0, 0.0, 0.0], 'rewardMean': 0.766193071320448, 'totalEpisodes': 151, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.898491905445
'totalSteps': 20480, 'rewardStep': 0.9423126329802077, 'errorList': [0.027307185100395336, 0.06200257294348492, 0.027885978841692426, 0.033319286176619285, 0.03198740833263122, 0.04242590687625954, 0.026516025609862662, 0.030865655756055224, 0.031268404401093845, 0.06913086678215155, 0.055977916954555736, 0.03411393696647374, 0.06252563813386128, 0.05186066327366682, 0.035961267630784104, 0.06268719570589243, 0.033605332585266476, 0.041282706110068554, 0.03996594881200842, 0.031736105163456624, 0.03913694889807204, 0.06687672695127356, 0.032613555547622415, 0.06505116166899677, 0.030048323186055905, 0.09011118579593753, 0.03589253324841485, 0.03772978673103399, 0.027942740234622974, 0.03404022356402237, 0.025995986428268485, 0.03691774497688809, 0.06432696440437054, 0.053214499345362286, 0.07185536476105103, 0.031084895413172216, 0.025146348229445843, 0.03630523943569916, 0.07579996669780592, 0.03444842741936293, 0.03271777916604054, 0.03363762592822751, 0.03453707339127455, 0.033412345998759196, 0.03398668299998179, 0.028396738625707164, 0.04998274838510698, 0.06841275242009695, 0.0556439764054143, 0.04028547353455716], 'lossList': [0.0, -1.2581252139806747, 0.0, 3.0101202607899906, 0.0, 0.0, 0.0], 'rewardMean': 0.7775734043208834, 'totalEpisodes': 151, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.6585202941865, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=71.35
