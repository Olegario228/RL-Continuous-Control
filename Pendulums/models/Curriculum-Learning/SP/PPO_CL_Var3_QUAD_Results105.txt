#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 105
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8718991920033401, 'errorList': [], 'lossList': [0.0, -1.3990988302230836, 0.0, 28.487236969470977, 0.0, 0.0, 0.0], 'rewardMean': 0.7412846454725953, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.923915711974905
'totalSteps': 3840, 'rewardStep': 0.5632760380541288, 'errorList': [], 'lossList': [0.0, -1.3886934208869934, 0.0, 26.750232865810393, 0.0, 0.0, 0.0], 'rewardMean': 0.6819484429997731, 'totalEpisodes': 25, 'stepsPerEpisode': 154, 'rewardPerEpisode': 122.1441518965934
'totalSteps': 5120, 'rewardStep': 0.887114702741671, 'errorList': [], 'lossList': [0.0, -1.3931902849674225, 0.0, 34.13075775384903, 0.0, 0.0, 0.0], 'rewardMean': 0.7332400079352476, 'totalEpisodes': 30, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.54271701171918
'totalSteps': 6400, 'rewardStep': 0.6816448034300111, 'errorList': [], 'lossList': [0.0, -1.3834579026699065, 0.0, 16.184855862855912, 0.0, 0.0, 0.0], 'rewardMean': 0.7229209670342003, 'totalEpisodes': 30, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 914.2550425536022
'totalSteps': 7680, 'rewardStep': 0.7161589231189043, 'errorList': [], 'lossList': [0.0, -1.3793996107578277, 0.0, 83.10969647407532, 0.0, 0.0, 0.0], 'rewardMean': 0.7217939597149843, 'totalEpisodes': 36, 'stepsPerEpisode': 178, 'rewardPerEpisode': 127.12626736758136
'totalSteps': 8960, 'rewardStep': 0.878466435604603, 'errorList': [], 'lossList': [0.0, -1.3627424341440202, 0.0, 64.25911133289337, 0.0, 0.0, 0.0], 'rewardMean': 0.7441757419849299, 'totalEpisodes': 42, 'stepsPerEpisode': 85, 'rewardPerEpisode': 66.79610486892737
'totalSteps': 10240, 'rewardStep': 0.9828807273229175, 'errorList': [330.6976127086029, 326.7065365241042, 336.6745649882421, 321.15815739912057, 245.65802596067468, 332.7187046845495, 291.18048517228334, 334.9176367215144, 331.505916484633, 324.1580761658403, 332.25422928922177, 253.61403940217497, 313.6456841530557, 258.2773525052746, 332.68730237330453, 349.0290777524877, 329.0210561587348, 346.74988195982667, 254.10708732056554, 340.3861094504096, 354.1130219877448, 338.4072230330712, 340.8629760368401, 278.54032205408396, 346.1785940911575, 313.8169950434413, 329.1820032257236, 303.99248420336386, 344.38020275336163, 295.28378213080316, 274.51088423279106, 333.3014733881739, 264.0732790740124, 333.4392716887272, 240.39344346834793, 327.0155985003623, 265.6525850160657, 337.4540124246968, 281.5568047021994, 349.512798904478, 326.57345263005135, 288.73469836718704, 355.4662736481741, 335.73123882579836, 310.7383331832219, 247.3691026221799, 321.1047336421816, 321.0986341886009, 331.944868800611, 240.7315462139553], 'lossList': [0.0, -1.3654495257139205, 0.0, 229.1201901245117, 0.0, 0.0, 0.0], 'rewardMean': 0.7740138651521784, 'totalEpisodes': 69, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.906287381977675, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.7395804498370161, 'errorList': [], 'lossList': [0.0, -1.3601154589653015, 0.0, 156.2191417312622, 0.0, 0.0, 0.0], 'rewardMean': 0.7701879301171604, 'totalEpisodes': 95, 'stepsPerEpisode': 68, 'rewardPerEpisode': 55.243886591218086
'totalSteps': 12800, 'rewardStep': 0.15699982062257312, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6635020913357739, 'totalEpisodes': 116, 'stepsPerEpisode': 146, 'rewardPerEpisode': 105.10501774044192
'totalSteps': 14080, 'rewardStep': 0.7818340622787749, 'errorList': [], 'lossList': [0.0, -1.35706081032753, 0.0, 105.79202077865601, 0.0, 0.0, 0.0], 'rewardMean': 0.6544955783633173, 'totalEpisodes': 138, 'stepsPerEpisode': 60, 'rewardPerEpisode': 47.30849169132223
'totalSteps': 15360, 'rewardStep': 0.5536300615921242, 'errorList': [], 'lossList': [0.0, -1.3575722020864487, 0.0, 76.18163661956787, 0.0, 0.0, 0.0], 'rewardMean': 0.6535309807171169, 'totalEpisodes': 153, 'stepsPerEpisode': 73, 'rewardPerEpisode': 60.4459213465014
'totalSteps': 16640, 'rewardStep': 0.9410842782493449, 'errorList': [11.340149491016867, 8.703659324124052, 9.469374349700788, 11.395874196845197, 5.634128426980991, 1.990868308413897, 1.3832797021962535, 9.830300218147203, 5.999873546092748, 5.444975058102596, 1.6877566440439273, 3.8119875400836505, 8.380732794068395, 6.308580261104977, 1.5742510475265343, 11.206746754813121, 10.084870913152027, 1.6047146905462657, 3.6374061577609824, 3.3588458469630065, 1.1380201352466044, 8.77854048667769, 7.230738086553765, 1.8173193865006068, 10.040174338139156, 6.00152025409176, 6.597877021127381, 10.828237376724516, 0.5446607754812025, 11.563364941058788, 11.00065971554549, 3.8689574716969615, 7.832857890613697, 8.147831430220945, 6.1301819967842714, 6.479206452953933, 9.607891124146782, 11.78852545241164, 8.59645914165738, 3.5026179767270658, 7.765120933806376, 8.218617492035982, 3.112137507278054, 8.556036503334049, 6.791830280400474, 10.338242803980231, 10.48990549983556, 9.602423921382695, 1.54028189247956, 0.8892925737944075], 'lossList': [0.0, -1.3555417788028716, 0.0, 59.17695846557617, 0.0, 0.0, 0.0], 'rewardMean': 0.6589279382678843, 'totalEpisodes': 161, 'stepsPerEpisode': 119, 'rewardPerEpisode': 108.12253866176276, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.45671436833755324, 'errorList': [], 'lossList': [0.0, -1.354543195962906, 0.0, 42.62603175163269, 0.0, 0.0, 0.0], 'rewardMean': 0.6364348947586385, 'totalEpisodes': 165, 'stepsPerEpisode': 165, 'rewardPerEpisode': 96.29494805608077
'totalSteps': 19200, 'rewardStep': 0.7125069360218149, 'errorList': [], 'lossList': [0.0, -1.3666139835119246, 0.0, 61.838555560112, 0.0, 0.0, 0.0], 'rewardMean': 0.6360696960489295, 'totalEpisodes': 172, 'stepsPerEpisode': 20, 'rewardPerEpisode': 12.496698911659838
'totalSteps': 20480, 'rewardStep': 0.8083972445404107, 'errorList': [], 'lossList': [0.0, -1.3786819398403167, 0.0, 14.033929858505726, 0.0, 0.0, 0.0], 'rewardMean': 0.6290627769425103, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.0518994381218
'totalSteps': 21760, 'rewardStep': 0.9291480832031256, 'errorList': [], 'lossList': [0.0, -1.3769537943601609, 0.0, 7.367096810340882, 0.0, 0.0, 0.0], 'rewardMean': 0.6236895125305312, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 906.9131659911476
'totalSteps': 23040, 'rewardStep': 0.7171676544497942, 'errorList': [], 'lossList': [0.0, -1.3802988159656524, 0.0, 6.93485991075635, 0.0, 0.0, 0.0], 'rewardMean': 0.621448232991809, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1047.3849376689366
'totalSteps': 24320, 'rewardStep': 0.8702236648325067, 'errorList': [], 'lossList': [0.0, -1.3919380789995193, 0.0, 100.95071056842804, 0.0, 0.0, 0.0], 'rewardMean': 0.6927706174128023, 'totalEpisodes': 176, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.548220995688188
'totalSteps': 25600, 'rewardStep': 0.809518664896999, 'errorList': [], 'lossList': [0.0, -1.3791096383333206, 0.0, 5.54075914978981, 0.0, 0.0, 0.0], 'rewardMean': 0.7580225018402448, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 952.823439396962
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=102.28
