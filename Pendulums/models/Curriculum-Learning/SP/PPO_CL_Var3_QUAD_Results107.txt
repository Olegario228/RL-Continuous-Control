#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 107
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.5600902805421399, 'errorList': [], 'lossList': [0.0, -1.4204067307710648, 0.0, 25.2733735370636, 0.0, 0.0, 0.0], 'rewardMean': 0.6769301408611456, 'totalEpisodes': 21, 'stepsPerEpisode': 134, 'rewardPerEpisode': 80.88075584724255
'totalSteps': 3840, 'rewardStep': 0.837096053946114, 'errorList': [], 'lossList': [0.0, -1.4120613950490952, 0.0, 27.930059449672697, 0.0, 0.0, 0.0], 'rewardMean': 0.730318778556135, 'totalEpisodes': 26, 'stepsPerEpisode': 77, 'rewardPerEpisode': 59.557129669891694
'totalSteps': 5120, 'rewardStep': 0.8762699313932921, 'errorList': [], 'lossList': [0.0, -1.3887593978643418, 0.0, 16.798460755348206, 0.0, 0.0, 0.0], 'rewardMean': 0.7668065667654242, 'totalEpisodes': 32, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.583632301254998
'totalSteps': 6400, 'rewardStep': 0.8370435110987733, 'errorList': [], 'lossList': [0.0, -1.3853953516483306, 0.0, 49.160984282493594, 0.0, 0.0, 0.0], 'rewardMean': 0.7808539556320941, 'totalEpisodes': 38, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.6364426456645513
'totalSteps': 7680, 'rewardStep': 0.9639998165452193, 'errorList': [], 'lossList': [0.0, -1.3760866010189057, 0.0, 62.77653133392334, 0.0, 0.0, 0.0], 'rewardMean': 0.8113782657842816, 'totalEpisodes': 44, 'stepsPerEpisode': 55, 'rewardPerEpisode': 37.40510609055522
'totalSteps': 8960, 'rewardStep': 0.8163527713151904, 'errorList': [], 'lossList': [0.0, -1.3616877436637878, 0.0, 42.35461847305298, 0.0, 0.0, 0.0], 'rewardMean': 0.8120889094315543, 'totalEpisodes': 47, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.728661138531049
'totalSteps': 10240, 'rewardStep': 0.4804376758179306, 'errorList': [], 'lossList': [0.0, -1.3478401631116868, 0.0, 157.9342333984375, 0.0, 0.0, 0.0], 'rewardMean': 0.7706325052298513, 'totalEpisodes': 71, 'stepsPerEpisode': 51, 'rewardPerEpisode': 26.355414787374965
'totalSteps': 11520, 'rewardStep': 0.7158661609737362, 'errorList': [], 'lossList': [0.0, -1.3437762999534606, 0.0, 100.28233758926392, 0.0, 0.0, 0.0], 'rewardMean': 0.7645473558680607, 'totalEpisodes': 93, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.240237490316654
'totalSteps': 12800, 'rewardStep': 0.5239375204154766, 'errorList': [], 'lossList': [0.0, -1.3398821121454239, 0.0, 41.08011864185333, 0.0, 0.0, 0.0], 'rewardMean': 0.7404863723228022, 'totalEpisodes': 105, 'stepsPerEpisode': 312, 'rewardPerEpisode': 248.24194181263857
'totalSteps': 14080, 'rewardStep': 0.8875660150298382, 'errorList': [], 'lossList': [0.0, -1.3397669678926467, 0.0, 25.763765816688537, 0.0, 0.0, 0.0], 'rewardMean': 0.7498659737077711, 'totalEpisodes': 114, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.56689655418191
'totalSteps': 15360, 'rewardStep': 0.751428622200831, 'errorList': [], 'lossList': [0.0, -1.3288545924425126, 0.0, 25.078127331733704, 0.0, 0.0, 0.0], 'rewardMean': 0.7689998078736402, 'totalEpisodes': 123, 'stepsPerEpisode': 44, 'rewardPerEpisode': 33.62808427997254
'totalSteps': 16640, 'rewardStep': 0.8185098723539709, 'errorList': [], 'lossList': [0.0, -1.327886661887169, 0.0, 8.330799403190612, 0.0, 0.0, 0.0], 'rewardMean': 0.7671411897144259, 'totalEpisodes': 129, 'stepsPerEpisode': 211, 'rewardPerEpisode': 179.1231619678421
'totalSteps': 17920, 'rewardStep': 0.7541602430693519, 'errorList': [], 'lossList': [0.0, -1.3235939931869507, 0.0, 7.221255601644516, 0.0, 0.0, 0.0], 'rewardMean': 0.7549302208820319, 'totalEpisodes': 134, 'stepsPerEpisode': 128, 'rewardPerEpisode': 115.13443372122957
'totalSteps': 19200, 'rewardStep': 0.6179561933852427, 'errorList': [], 'lossList': [0.0, -1.3201703017950057, 0.0, 5.228227140903473, 0.0, 0.0, 0.0], 'rewardMean': 0.7330214891106788, 'totalEpisodes': 140, 'stepsPerEpisode': 232, 'rewardPerEpisode': 186.1676542920848
'totalSteps': 20480, 'rewardStep': 0.7822535813478799, 'errorList': [], 'lossList': [0.0, -1.3330102419853211, 0.0, 4.976474224328995, 0.0, 0.0, 0.0], 'rewardMean': 0.7148468655909449, 'totalEpisodes': 144, 'stepsPerEpisode': 63, 'rewardPerEpisode': 49.86874391111626
'totalSteps': 21760, 'rewardStep': 0.8349601458639911, 'errorList': [], 'lossList': [0.0, -1.318580538034439, 0.0, 4.167605526745319, 0.0, 0.0, 0.0], 'rewardMean': 0.7167076030458249, 'totalEpisodes': 146, 'stepsPerEpisode': 647, 'rewardPerEpisode': 579.5782005064896
'totalSteps': 23040, 'rewardStep': 0.9289984166812657, 'errorList': [], 'lossList': [0.0, -1.2916444379091263, 0.0, 3.297380405366421, 0.0, 0.0, 0.0], 'rewardMean': 0.7615636771321583, 'totalEpisodes': 148, 'stepsPerEpisode': 124, 'rewardPerEpisode': 115.3808199116177
'totalSteps': 24320, 'rewardStep': 0.9338618293399807, 'errorList': [0.22347029956195283, 1.3043036333257763, 2.197102292544995, 0.5834089124309104, 0.20065866288912027, 3.3973083206832104, 2.0747510425146407, 0.8620438718201939, 0.7219565688581766, 1.8722619078638387, 0.5906644681384776, 0.42504678435991633, 0.7792017507648282, 1.3597870693374907, 0.5860153042483174, 1.943141814039981, 1.7904952080372003, 1.7371138237066142, 1.765475981706402, 1.6836019649658567, 0.8120641123524776, 1.9156725687481382, 1.1684992774348109, 2.174679100209243, 1.1478522648825307, 1.81798137430433, 1.2152575737512081, 1.0525927369680543, 1.8574949699709542, 1.135164602392595, 2.2442806785615868, 1.8311279712683348, 0.35970993755153696, 1.3280509729538592, 1.276977109021997, 0.7604158646157242, 0.7542734003265059, 0.19125427795028788, 0.9460824435206885, 1.233098923509645, 1.0353542402180234, 1.7615019437482506, 1.5696739895513456, 2.213555310548769, 0.9714634216810801, 2.6373949085732313, 1.2965818726582836, 0.3626993325007419, 1.4947520053008712, 1.2087397197532417], 'lossList': [0.0, -1.2708347964286804, 0.0, 2.2822450757026673, 0.0, 0.0, 0.0], 'rewardMean': 0.7833632439687829, 'totalEpisodes': 151, 'stepsPerEpisode': 83, 'rewardPerEpisode': 76.64226971718024, 'successfulTests': 1
'totalSteps': 25600, 'rewardStep': 0.4866316902729048, 'errorList': [], 'lossList': [0.0, -1.262265157699585, 0.0, 2.1999579456448557, 0.0, 0.0, 0.0], 'rewardMean': 0.7796326609545258, 'totalEpisodes': 152, 'stepsPerEpisode': 791, 'rewardPerEpisode': 591.0204198912082
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=24320, timeSpent=84.94
