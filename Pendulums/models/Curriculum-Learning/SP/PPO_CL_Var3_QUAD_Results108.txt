#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 108
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.7124976816101856, 'errorList': [], 'lossList': [0.0, -1.4119347524642945, 0.0, 24.57520259022713, 0.0, 0.0, 0.0], 'rewardMean': 0.5284264365778202, 'totalEpisodes': 14, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.973837013923628
'totalSteps': 3840, 'rewardStep': 0.8765651354571475, 'errorList': [], 'lossList': [0.0, -1.3982174384593964, 0.0, 27.880076110363007, 0.0, 0.0, 0.0], 'rewardMean': 0.644472669537596, 'totalEpisodes': 19, 'stepsPerEpisode': 298, 'rewardPerEpisode': 210.8664007502082
'totalSteps': 5120, 'rewardStep': 0.7909439197302948, 'errorList': [], 'lossList': [0.0, -1.3978592884540557, 0.0, 43.03654253959656, 0.0, 0.0, 0.0], 'rewardMean': 0.6810904820857706, 'totalEpisodes': 23, 'stepsPerEpisode': 76, 'rewardPerEpisode': 69.01867468307817
'totalSteps': 6400, 'rewardStep': 0.6082470432882748, 'errorList': [], 'lossList': [0.0, -1.3915772169828415, 0.0, 34.48164708137512, 0.0, 0.0, 0.0], 'rewardMean': 0.6665217943262715, 'totalEpisodes': 27, 'stepsPerEpisode': 166, 'rewardPerEpisode': 120.89033901167741
'totalSteps': 7680, 'rewardStep': 0.9134771641351475, 'errorList': [], 'lossList': [0.0, -1.38499780356884, 0.0, 45.50327810287476, 0.0, 0.0, 0.0], 'rewardMean': 0.7076810226277508, 'totalEpisodes': 33, 'stepsPerEpisode': 58, 'rewardPerEpisode': 49.88777234118457
'totalSteps': 8960, 'rewardStep': 0.799267818515774, 'errorList': [], 'lossList': [0.0, -1.3769094455242157, 0.0, 29.902417018413544, 0.0, 0.0, 0.0], 'rewardMean': 0.7207648506117541, 'totalEpisodes': 39, 'stepsPerEpisode': 69, 'rewardPerEpisode': 58.43188995099419
'totalSteps': 10240, 'rewardStep': 0.6540728425251993, 'errorList': [], 'lossList': [0.0, -1.375132365822792, 0.0, 225.89581489562988, 0.0, 0.0, 0.0], 'rewardMean': 0.7124283496009347, 'totalEpisodes': 69, 'stepsPerEpisode': 102, 'rewardPerEpisode': 82.77834150538719
'totalSteps': 11520, 'rewardStep': 0.7929702863095485, 'errorList': [], 'lossList': [0.0, -1.3760896003246308, 0.0, 93.8384688949585, 0.0, 0.0, 0.0], 'rewardMean': 0.7213774536796697, 'totalEpisodes': 92, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.182764659022629
'totalSteps': 12800, 'rewardStep': 0.4457977819950544, 'errorList': [], 'lossList': [0.0, -1.3774523055553436, 0.0, 28.85570620059967, 0.0, 0.0, 0.0], 'rewardMean': 0.6938194865112081, 'totalEpisodes': 108, 'stepsPerEpisode': 92, 'rewardPerEpisode': 74.3861247193292
'totalSteps': 14080, 'rewardStep': 0.8783131435136662, 'errorList': [], 'lossList': [0.0, -1.3749271965026855, 0.0, 35.69669453620911, 0.0, 0.0, 0.0], 'rewardMean': 0.7472152817080292, 'totalEpisodes': 120, 'stepsPerEpisode': 28, 'rewardPerEpisode': 24.194405183075705
'totalSteps': 15360, 'rewardStep': 0.7441006281597734, 'errorList': [], 'lossList': [0.0, -1.3620459371805191, 0.0, 23.180974397659302, 0.0, 0.0, 0.0], 'rewardMean': 0.750375576362988, 'totalEpisodes': 125, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.678364280662032
'totalSteps': 16640, 'rewardStep': 0.8445392664885536, 'errorList': [], 'lossList': [0.0, -1.3422322106361388, 0.0, 9.903789162635803, 0.0, 0.0, 0.0], 'rewardMean': 0.7471729894661288, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1031.282733712041
'totalSteps': 17920, 'rewardStep': 0.8838397558802827, 'errorList': [], 'lossList': [0.0, -1.3040911364555359, 0.0, 5.857017306685448, 0.0, 0.0, 0.0], 'rewardMean': 0.7564625730811274, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 982.3085824128509
'totalSteps': 19200, 'rewardStep': 0.8499699697432261, 'errorList': [], 'lossList': [0.0, -1.2672056794166564, 0.0, 5.0251352190971375, 0.0, 0.0, 0.0], 'rewardMean': 0.7806348657266226, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.3428431114435
'totalSteps': 20480, 'rewardStep': 0.9802857621497717, 'errorList': [0.018211576007385395, 0.01346019972770974, 0.015431020629943119, 0.017199080833082817, 0.020469415193333465, 0.018537351539559876, 0.020087625892139107, 0.015447753121002562, 0.014413978727144549, 0.018819616175984304, 0.017430799630104864, 0.018449416622095753, 0.016999888072099702, 0.01651745384270851, 0.01477847386762308, 0.013469020211994608, 0.017352561316327272, 0.020146206747041352, 0.017666593975631, 0.020816970260131195, 0.015273273553016203, 0.021256982806574936, 0.02834667266716943, 0.018401750354676522, 0.013726607702067229, 0.014799900718559042, 0.017330208127678697, 0.017837211576752224, 0.019953913235968224, 0.017997900442442216, 0.014002891932043472, 0.02024057466016085, 0.017115520181643105, 0.01724196324367249, 0.022608118304122966, 0.01774629147280519, 0.01997883485443555, 0.0191765685857774, 0.015874055397775276, 0.016679506748003496, 0.014024657655208219, 0.023146863699484727, 0.01672605345416941, 0.01522924836507875, 0.01827414822305016, 0.018884828892986684, 0.013398241547414627, 0.014845615543857707, 0.021008934585548096, 0.018703568729437865], 'lossList': [0.0, -1.235565441250801, 0.0, 4.381564867347479, 0.0, 0.0, 0.0], 'rewardMean': 0.787315725528085, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.3461609655403, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=71.16
