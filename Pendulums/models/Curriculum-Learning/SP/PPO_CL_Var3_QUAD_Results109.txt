#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 109
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8724849624091926, 'errorList': [], 'lossList': [0.0, -1.4163323736190796, 0.0, 29.53377354621887, 0.0, 0.0, 0.0], 'rewardMean': 0.6182775340923906, 'totalEpisodes': 21, 'stepsPerEpisode': 37, 'rewardPerEpisode': 30.568191012337834
'totalSteps': 3840, 'rewardStep': 0.5544807457873056, 'errorList': [], 'lossList': [0.0, -1.4351095843315125, 0.0, 24.987614204883574, 0.0, 0.0, 0.0], 'rewardMean': 0.5970119379906956, 'totalEpisodes': 27, 'stepsPerEpisode': 169, 'rewardPerEpisode': 116.88234004514122
'totalSteps': 5120, 'rewardStep': 0.5504123928506148, 'errorList': [], 'lossList': [0.0, -1.4352264380455018, 0.0, 24.28390302181244, 0.0, 0.0, 0.0], 'rewardMean': 0.5853620517056755, 'totalEpisodes': 29, 'stepsPerEpisode': 281, 'rewardPerEpisode': 202.29749962683485
'totalSteps': 6400, 'rewardStep': 0.7012050388077606, 'errorList': [], 'lossList': [0.0, -1.4129407393932343, 0.0, 31.28938133478165, 0.0, 0.0, 0.0], 'rewardMean': 0.6085306491260924, 'totalEpisodes': 31, 'stepsPerEpisode': 867, 'rewardPerEpisode': 658.9351073481678
'totalSteps': 7680, 'rewardStep': 0.8830893212625741, 'errorList': [], 'lossList': [0.0, -1.4076676207780838, 0.0, 33.67155282139778, 0.0, 0.0, 0.0], 'rewardMean': 0.6542904278155061, 'totalEpisodes': 33, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.521210085236317
'totalSteps': 8960, 'rewardStep': 0.8253150241214816, 'errorList': [], 'lossList': [0.0, -1.3993923622369766, 0.0, 59.55336364269257, 0.0, 0.0, 0.0], 'rewardMean': 0.678722513002074, 'totalEpisodes': 37, 'stepsPerEpisode': 331, 'rewardPerEpisode': 259.06196209167723
'totalSteps': 10240, 'rewardStep': 0.6324237037758117, 'errorList': [], 'lossList': [0.0, -1.39750721514225, 0.0, 228.3348401260376, 0.0, 0.0, 0.0], 'rewardMean': 0.6729351618487912, 'totalEpisodes': 62, 'stepsPerEpisode': 15, 'rewardPerEpisode': 9.28328111459993
'totalSteps': 11520, 'rewardStep': 0.5960884186107952, 'errorList': [], 'lossList': [0.0, -1.3936996841430664, 0.0, 87.1557851600647, 0.0, 0.0, 0.0], 'rewardMean': 0.6643966348223471, 'totalEpisodes': 89, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.565997043320156
'totalSteps': 12800, 'rewardStep': 0.5992858483270328, 'errorList': [], 'lossList': [0.0, -1.3815882164239883, 0.0, 54.00758897781372, 0.0, 0.0, 0.0], 'rewardMean': 0.6578855561728157, 'totalEpisodes': 109, 'stepsPerEpisode': 44, 'rewardPerEpisode': 36.92013873856479
'totalSteps': 14080, 'rewardStep': 0.6950533380934742, 'errorList': [], 'lossList': [0.0, -1.368213813304901, 0.0, 20.689848251342774, 0.0, 0.0, 0.0], 'rewardMean': 0.6909838794046044, 'totalEpisodes': 126, 'stepsPerEpisode': 139, 'rewardPerEpisode': 111.20264768966811
'totalSteps': 15360, 'rewardStep': 0.7329248301868602, 'errorList': [], 'lossList': [0.0, -1.3596391427516936, 0.0, 13.58827423810959, 0.0, 0.0, 0.0], 'rewardMean': 0.6770278661823712, 'totalEpisodes': 134, 'stepsPerEpisode': 85, 'rewardPerEpisode': 72.78071830945251
'totalSteps': 16640, 'rewardStep': 0.25823601527116347, 'errorList': [], 'lossList': [0.0, -1.3476534795761108, 0.0, 19.393502159118654, 0.0, 0.0, 0.0], 'rewardMean': 0.6474033931307569, 'totalEpisodes': 143, 'stepsPerEpisode': 149, 'rewardPerEpisode': 115.22726726515997
'totalSteps': 17920, 'rewardStep': 0.5595874321873872, 'errorList': [], 'lossList': [0.0, -1.3467187261581421, 0.0, 15.113488018512726, 0.0, 0.0, 0.0], 'rewardMean': 0.6483208970644341, 'totalEpisodes': 148, 'stepsPerEpisode': 267, 'rewardPerEpisode': 219.92190226860285
'totalSteps': 19200, 'rewardStep': 0.46556246777861987, 'errorList': [], 'lossList': [0.0, -1.3253522980213166, 0.0, 16.90625822067261, 0.0, 0.0, 0.0], 'rewardMean': 0.6247566399615201, 'totalEpisodes': 152, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.4139338793142047
'totalSteps': 20480, 'rewardStep': 0.7965617708348136, 'errorList': [], 'lossList': [0.0, -1.3030272281169892, 0.0, 6.630746632814407, 0.0, 0.0, 0.0], 'rewardMean': 0.6161038849187439, 'totalEpisodes': 154, 'stepsPerEpisode': 353, 'rewardPerEpisode': 296.046605212794
'totalSteps': 21760, 'rewardStep': 0.7493427485185671, 'errorList': [], 'lossList': [0.0, -1.2896445488929749, 0.0, 4.002114691734314, 0.0, 0.0, 0.0], 'rewardMean': 0.6085066573584526, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.28269895039
'totalSteps': 23040, 'rewardStep': 0.8481728160708976, 'errorList': [], 'lossList': [0.0, -1.2673742949962616, 0.0, 2.3607964074611663, 0.0, 0.0, 0.0], 'rewardMean': 0.630081568587961, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.0381360649787
'totalSteps': 24320, 'rewardStep': 0.8390271930275052, 'errorList': [], 'lossList': [0.0, -1.249001522064209, 0.0, 2.067272200733423, 0.0, 0.0, 0.0], 'rewardMean': 0.654375446029632, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.498075608074
'totalSteps': 25600, 'rewardStep': 0.9611602248064856, 'errorList': [0.06050743375843173, 0.05798803378143085, 0.06491637318550514, 0.06164369159223035, 0.060425544320100104, 0.06091179844883053, 0.062272459776027154, 0.06477958668218943, 0.05723232095667673, 0.09258026139524464, 0.06191815261535928, 0.06502491504535413, 0.059474800016357104, 0.06173885084327975, 0.0857410186705479, 0.0636810565333775, 0.07243538746632726, 0.07303181695481868, 0.07330988617556271, 0.057684740999236504, 0.05933008531891302, 0.07357857971872978, 0.07174408306888923, 0.09538483150493067, 0.059543486296113994, 0.05821949513762646, 0.05710137614690635, 0.06591426058569533, 0.061223426391845845, 0.061452328686845525, 0.06274085364679426, 0.06317218748242534, 0.07006672936331658, 0.06356804531477654, 0.0630882983727472, 0.06521192119206097, 0.06342250783798986, 0.08509758706596074, 0.06193897624936014, 0.06237581251037014, 0.06365681064243767, 0.061389886026230175, 0.07473053379850543, 0.07372221811805821, 0.06323882690485112, 0.06005067290983501, 0.06419887308986715, 0.06523375232076628, 0.10057163089108974, 0.05837963633989847], 'lossList': [0.0, -1.2296384716033935, 0.0, 2.1315322514623403, 0.0, 0.0, 0.0], 'rewardMean': 0.6905628836775775, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.908831735796, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=83.27
