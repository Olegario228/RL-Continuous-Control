#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 110
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9239350883578689, 'errorList': [], 'lossList': [0.0, -1.4074956214427947, 0.0, 26.918961077928543, 0.0, 0.0, 0.0], 'rewardMean': 0.8618670244284159, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 382.8385982460872
'totalSteps': 3840, 'rewardStep': 0.6235822982740704, 'errorList': [], 'lossList': [0.0, -1.402626360654831, 0.0, 37.06405975103378, 0.0, 0.0, 0.0], 'rewardMean': 0.7824387823769673, 'totalEpisodes': 12, 'stepsPerEpisode': 264, 'rewardPerEpisode': 200.59126165206723
'totalSteps': 5120, 'rewardStep': 0.719973602453337, 'errorList': [], 'lossList': [0.0, -1.4086267781257629, 0.0, 26.041575319767, 0.0, 0.0, 0.0], 'rewardMean': 0.7668224873960597, 'totalEpisodes': 13, 'stepsPerEpisode': 472, 'rewardPerEpisode': 351.5680826199611
'totalSteps': 6400, 'rewardStep': 0.8759794464541478, 'errorList': [], 'lossList': [0.0, -1.4028687429428102, 0.0, 20.398933976590634, 0.0, 0.0, 0.0], 'rewardMean': 0.7886538792076774, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.0215036723783
'totalSteps': 7680, 'rewardStep': 0.8408120177664282, 'errorList': [], 'lossList': [0.0, -1.3793377232551576, 0.0, 25.417584584355353, 0.0, 0.0, 0.0], 'rewardMean': 0.7973469023008025, 'totalEpisodes': 14, 'stepsPerEpisode': 419, 'rewardPerEpisode': 331.55813598870304
'totalSteps': 8960, 'rewardStep': 0.6550536964452178, 'errorList': [], 'lossList': [0.0, -1.3492659837007523, 0.0, 40.36381358146667, 0.0, 0.0, 0.0], 'rewardMean': 0.7770193014642903, 'totalEpisodes': 16, 'stepsPerEpisode': 472, 'rewardPerEpisode': 344.7090647026004
'totalSteps': 10240, 'rewardStep': 0.8975912549260163, 'errorList': [], 'lossList': [0.0, -1.3457411158084869, 0.0, 335.99060485839846, 0.0, 0.0, 0.0], 'rewardMean': 0.7920907956470062, 'totalEpisodes': 50, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.565858538453963
'totalSteps': 11520, 'rewardStep': 0.5887997126105684, 'errorList': [], 'lossList': [0.0, -1.3451781821250917, 0.0, 119.88060760498047, 0.0, 0.0, 0.0], 'rewardMean': 0.7695028975318465, 'totalEpisodes': 81, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2025216741297555
'totalSteps': 12800, 'rewardStep': 0.369216988314958, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.686416109391757, 'totalEpisodes': 99, 'stepsPerEpisode': 43, 'rewardPerEpisode': 28.253464045573025
'totalSteps': 14080, 'rewardStep': 0.6912671425169593, 'errorList': [], 'lossList': [0.0, -1.346885898709297, 0.0, 59.28332777023316, 0.0, 0.0, 0.0], 'rewardMean': 0.663149314807666, 'totalEpisodes': 129, 'stepsPerEpisode': 36, 'rewardPerEpisode': 32.39949729268144
'totalSteps': 15360, 'rewardStep': 0.8643085270508096, 'errorList': [], 'lossList': [0.0, -1.3477485764026642, 0.0, 39.811419858932496, 0.0, 0.0, 0.0], 'rewardMean': 0.68722193768534, 'totalEpisodes': 145, 'stepsPerEpisode': 70, 'rewardPerEpisode': 53.61337742438717
'totalSteps': 16640, 'rewardStep': 0.7600522043882506, 'errorList': [], 'lossList': [0.0, -1.3450904703140258, 0.0, 18.216630845069886, 0.0, 0.0, 0.0], 'rewardMean': 0.6912297978788313, 'totalEpisodes': 153, 'stepsPerEpisode': 114, 'rewardPerEpisode': 95.23749593069527
'totalSteps': 17920, 'rewardStep': 0.8741026243911525, 'errorList': [], 'lossList': [0.0, -1.3471381425857545, 0.0, 10.430290735960007, 0.0, 0.0, 0.0], 'rewardMean': 0.6910421156725318, 'totalEpisodes': 158, 'stepsPerEpisode': 346, 'rewardPerEpisode': 233.45773741522032
'totalSteps': 19200, 'rewardStep': 0.8141575141435328, 'errorList': [], 'lossList': [0.0, -1.3472289228439331, 0.0, 11.997880182266236, 0.0, 0.0, 0.0], 'rewardMean': 0.6883766653102423, 'totalEpisodes': 161, 'stepsPerEpisode': 143, 'rewardPerEpisode': 124.2292989658729
'totalSteps': 20480, 'rewardStep': 0.9343098416876097, 'errorList': [0.17739376074686045, 0.18160791271821553, 0.1717725700670108, 0.1807957377676022, 0.2606683835290724, 0.23063458215523183, 0.1834170328428841, 0.17352627643460095, 0.18552170450974648, 0.16847287941711417, 0.18141378672317288, 0.21346738937932838, 0.1831172419823681, 0.2704534783611435, 0.1964116145541809, 0.17742006898064103, 0.17671396560279717, 0.17999868451791937, 0.19233227446618997, 0.1816442451576294, 0.17172126118915912, 0.1788387920251104, 0.23003175316623553, 0.2100793855943534, 0.17072165690316154, 0.1716058538412911, 0.17343485817856744, 0.18663301804522667, 0.17168790664744157, 0.18282852154503315, 0.18172438337269234, 0.1862536448372745, 0.1801131811107323, 0.22846167746366858, 0.17283943922272405, 0.22192260849762357, 0.22722845937726718, 0.1838174910421481, 0.16897032035224793, 0.20008137031112766, 0.21200903429383433, 0.17718080869368164, 0.17848861756097636, 0.17642911352799837, 0.17692731519826577, 0.19544419541669153, 0.17459100780367198, 0.1861352627240533, 0.17986595502683778, 0.17144062156966072], 'lossList': [0.0, -1.3396680825948715, 0.0, 6.437604269981384, 0.0, 0.0, 0.0], 'rewardMean': 0.7163022798344814, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1023.8339316503411, 'successfulTests': 39
'totalSteps': 21760, 'rewardStep': 0.8174089164312235, 'errorList': [], 'lossList': [0.0, -1.3472011965513229, 0.0, 4.461307483613491, 0.0, 0.0, 0.0], 'rewardMean': 0.7082840459850023, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1056.1982534335082
'totalSteps': 23040, 'rewardStep': 0.729563990928389, 'errorList': [], 'lossList': [0.0, -1.3502891200780869, 0.0, 2.499764819741249, 0.0, 0.0, 0.0], 'rewardMean': 0.7223604738167843, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.2393066926986
'totalSteps': 24320, 'rewardStep': 0.6615861979883366, 'errorList': [], 'lossList': [0.0, -1.3201219898462295, 0.0, 1.258089330866933, 0.0, 0.0, 0.0], 'rewardMean': 0.7515973947841221, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1023.8891673060426
'totalSteps': 25600, 'rewardStep': 0.9021171902728202, 'errorList': [], 'lossList': [0.0, -1.2909337091445923, 0.0, 2.8385408840328457, 0.0, 0.0, 0.0], 'rewardMean': 0.8048874149799083, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.42962164105
#maxSuccessfulTests=39, maxSuccessfulTestsAtStep=20480, timeSpent=83.82
