#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 111
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.6995081324885791, 'errorList': [], 'lossList': [0.0, -1.437814500927925, 0.0, 32.757448132038114, 0.0, 0.0, 0.0], 'rewardMean': 0.608151017094016, 'totalEpisodes': 12, 'stepsPerEpisode': 76, 'rewardPerEpisode': 62.17313230922579
'totalSteps': 3840, 'rewardStep': 0.8590776591025964, 'errorList': [], 'lossList': [0.0, -1.444501217007637, 0.0, 26.403338966965677, 0.0, 0.0, 0.0], 'rewardMean': 0.6917932310968761, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.8708102397464
'totalSteps': 5120, 'rewardStep': 0.6426993097109618, 'errorList': [], 'lossList': [0.0, -1.4206834328174591, 0.0, 25.538962019085883, 0.0, 0.0, 0.0], 'rewardMean': 0.6795197507503975, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.6187509639598
'totalSteps': 6400, 'rewardStep': 0.8211753510325623, 'errorList': [], 'lossList': [0.0, -1.4133038705587386, 0.0, 21.734474670886993, 0.0, 0.0, 0.0], 'rewardMean': 0.7078508708068305, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.0724309686557
'totalSteps': 7680, 'rewardStep': 0.8725106058381366, 'errorList': [], 'lossList': [0.0, -1.3848576325178146, 0.0, 16.4023678021878, 0.0, 0.0, 0.0], 'rewardMean': 0.7352941599787148, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.127834306995
'totalSteps': 8960, 'rewardStep': 0.9287896494457422, 'errorList': [], 'lossList': [0.0, -1.3593607330322266, 0.0, 27.679407309293747, 0.0, 0.0, 0.0], 'rewardMean': 0.7629363727597188, 'totalEpisodes': 13, 'stepsPerEpisode': 449, 'rewardPerEpisode': 378.0206225965042
'totalSteps': 10240, 'rewardStep': 0.9274461705189144, 'errorList': [], 'lossList': [0.0, -1.3477837061882019, 0.0, 464.4882817077637, 0.0, 0.0, 0.0], 'rewardMean': 0.7835000974796182, 'totalEpisodes': 53, 'stepsPerEpisode': 28, 'rewardPerEpisode': 26.804771241923852
'totalSteps': 11520, 'rewardStep': 0.736464093290064, 'errorList': [], 'lossList': [0.0, -1.3454229533672333, 0.0, 169.89271438598632, 0.0, 0.0, 0.0], 'rewardMean': 0.77827387479189, 'totalEpisodes': 97, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.2352628922948914
'totalSteps': 12800, 'rewardStep': 0.5070998895373006, 'errorList': [], 'lossList': [0.0, -1.3393105673789978, 0.0, 95.72082954406739, 0.0, 0.0, 0.0], 'rewardMean': 0.7511564762664311, 'totalEpisodes': 128, 'stepsPerEpisode': 26, 'rewardPerEpisode': 18.82862636578932
'totalSteps': 14080, 'rewardStep': 0.923477832787286, 'errorList': [], 'lossList': [0.0, -1.337051064968109, 0.0, 62.09024398803711, 0.0, 0.0, 0.0], 'rewardMean': 0.7918248693752143, 'totalEpisodes': 157, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.906938157963552
'totalSteps': 15360, 'rewardStep': 0.8677373431959495, 'errorList': [], 'lossList': [0.0, -1.3459756219387053, 0.0, 27.068781089782714, 0.0, 0.0, 0.0], 'rewardMean': 0.8086477904459514, 'totalEpisodes': 171, 'stepsPerEpisode': 52, 'rewardPerEpisode': 45.58538993828784
'totalSteps': 16640, 'rewardStep': 0.8468348906249894, 'errorList': [], 'lossList': [0.0, -1.3545730769634248, 0.0, 29.05665551185608, 0.0, 0.0, 0.0], 'rewardMean': 0.8074235135981909, 'totalEpisodes': 187, 'stepsPerEpisode': 92, 'rewardPerEpisode': 78.7561190433929
'totalSteps': 17920, 'rewardStep': 0.9592252014242518, 'errorList': [136.88724472150363, 16.119616324346406, 140.35844809841197, 111.43830930134554, 144.49988553698245, 41.92861820370327, 1.7485478534827967, 40.18461754593908, 82.99728338861613, 153.95476677597972, 105.13799250541193, 154.03784234505142, 112.64171047721193, 166.73818399383865, 132.76211861340795, 156.80872737984754, 59.90419708245354, 152.44268181811395, 190.15211662853505, 148.4616875912712, 36.964105003039, 152.63552450054755, 149.85535642573595, 39.032760509022914, 172.97637127351481, 125.36579723904407, 99.19960279147314, 102.5349133449456, 191.96307917236012, 111.85794351636358, 110.13772769662816, 111.65199910411188, 18.110231852714065, 101.3294165118969, 172.35814113556413, 166.1374621521925, 33.50744421783326, 186.1217685323283, 94.65792616557435, 89.3527456641782, 108.90014621371353, 142.79680099482692, 20.848029006070295, 71.66013486822001, 181.73913860378582, 146.19651583043895, 155.00805256214284, 168.13758772613102, 105.86825639763644, 167.20044903548873], 'lossList': [0.0, -1.3511963021755218, 0.0, 14.932524619102479, 0.0, 0.0, 0.0], 'rewardMean': 0.8390761027695197, 'totalEpisodes': 197, 'stepsPerEpisode': 57, 'rewardPerEpisode': 49.655269680764185, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.829244104849277, 'errorList': [], 'lossList': [0.0, -1.345477774143219, 0.0, 14.081487131118774, 0.0, 0.0, 0.0], 'rewardMean': 0.8398829781511912, 'totalEpisodes': 203, 'stepsPerEpisode': 80, 'rewardPerEpisode': 67.2483868495641
'totalSteps': 20480, 'rewardStep': 0.5632680723149964, 'errorList': [], 'lossList': [0.0, -1.3466516768932342, 0.0, 11.356608924865723, 0.0, 0.0, 0.0], 'rewardMean': 0.8089587247988771, 'totalEpisodes': 212, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.1595565589380392
'totalSteps': 21760, 'rewardStep': 0.7581113328521354, 'errorList': [], 'lossList': [0.0, -1.3418614447116852, 0.0, 11.629241862297057, 0.0, 0.0, 0.0], 'rewardMean': 0.7918908931395165, 'totalEpisodes': 220, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.30050704378631
'totalSteps': 23040, 'rewardStep': 0.7139720275354394, 'errorList': [], 'lossList': [0.0, -1.334568286538124, 0.0, 7.112922296524048, 0.0, 0.0, 0.0], 'rewardMean': 0.770543478841169, 'totalEpisodes': 225, 'stepsPerEpisode': 98, 'rewardPerEpisode': 79.49451124220366
'totalSteps': 24320, 'rewardStep': 0.7336765044558717, 'errorList': [], 'lossList': [0.0, -1.333751569390297, 0.0, 6.841610342264175, 0.0, 0.0, 0.0], 'rewardMean': 0.7702647199577497, 'totalEpisodes': 229, 'stepsPerEpisode': 79, 'rewardPerEpisode': 62.65303307893974
'totalSteps': 25600, 'rewardStep': 0.9529948205519716, 'errorList': [0.4549954860798375, 0.4797206532761998, 0.4988895423924698, 0.5001547629299518, 1.6227161270382526, 0.5637490144507922, 0.7531877437297666, 0.6697370553131676, 0.8772924875394165, 0.6208447087486425, 0.48818918222824165, 0.46460833973696947, 0.6926136271521119, 0.6162965207269121, 0.3638004147679657, 0.35571308258780326, 0.4577303838439361, 1.2517344828234478, 0.3531505575645683, 1.1059786997872298, 0.4381955206542576, 0.6894780872568672, 0.45578038505854024, 0.4041533202760054, 0.44271612450505016, 0.37998422348801514, 0.41586339322090876, 0.9409762376561248, 0.478140961492723, 0.5976311652964297, 1.1745336582630086, 0.7696656294493834, 0.5295479702843078, 0.39176562706893636, 0.37781727485186367, 0.7516644431780456, 0.3574266464255533, 0.5681443961883215, 0.47016263688704457, 0.41616746712311686, 0.5009469873131451, 0.4608980175192126, 1.3075088834942732, 0.7924805473423955, 0.4718997695362296, 0.6610135027061107, 0.49624006822565264, 0.5662622495802867, 0.7167013612840712, 0.740642786183828], 'lossList': [0.0, -1.3193978136777877, 0.0, 4.463431786298752, 0.0, 0.0, 0.0], 'rewardMean': 0.8148542130592169, 'totalEpisodes': 231, 'stepsPerEpisode': 305, 'rewardPerEpisode': 270.2285487492062, 'successfulTests': 0
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=104.36
