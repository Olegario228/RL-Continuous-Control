#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 112
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.8123150792230498, 'errorList': [], 'lossList': [0.0, -1.4439324110746383, 0.0, 25.23493540406227, 0.0, 0.0, 0.0], 'rewardMean': 0.6251728357144619, 'totalEpisodes': 11, 'stepsPerEpisode': 900, 'rewardPerEpisode': 602.4056284014007
'totalSteps': 3840, 'rewardStep': 0.8941665628062639, 'errorList': [], 'lossList': [0.0, -1.4489585638046265, 0.0, 37.40398364543915, 0.0, 0.0, 0.0], 'rewardMean': 0.7148374114117293, 'totalEpisodes': 14, 'stepsPerEpisode': 489, 'rewardPerEpisode': 389.4982915144996
'totalSteps': 5120, 'rewardStep': 0.535662468165228, 'errorList': [], 'lossList': [0.0, -1.4287671542167664, 0.0, 17.0651116669178, 0.0, 0.0, 0.0], 'rewardMean': 0.670043675600104, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 861.0184974721769
'totalSteps': 6400, 'rewardStep': 0.33349753268101523, 'errorList': [], 'lossList': [0.0, -1.408471091389656, 0.0, 42.08582535505295, 0.0, 0.0, 0.0], 'rewardMean': 0.6027344470162863, 'totalEpisodes': 17, 'stepsPerEpisode': 186, 'rewardPerEpisode': 140.36875818734921
'totalSteps': 7680, 'rewardStep': 0.5543384993598923, 'errorList': [], 'lossList': [0.0, -1.4090964192152022, 0.0, 62.427431778907774, 0.0, 0.0, 0.0], 'rewardMean': 0.5946684557402206, 'totalEpisodes': 22, 'stepsPerEpisode': 286, 'rewardPerEpisode': 208.73149034397315
'totalSteps': 8960, 'rewardStep': 0.7611459336468589, 'errorList': [], 'lossList': [0.0, -1.4161994302272796, 0.0, 73.52789543151856, 0.0, 0.0, 0.0], 'rewardMean': 0.6184509525840262, 'totalEpisodes': 28, 'stepsPerEpisode': 134, 'rewardPerEpisode': 92.02897658637015
'totalSteps': 10240, 'rewardStep': 0.5687678849219157, 'errorList': [], 'lossList': [0.0, -1.4112461918592454, 0.0, 298.58683502197266, 0.0, 0.0, 0.0], 'rewardMean': 0.6122405691262622, 'totalEpisodes': 68, 'stepsPerEpisode': 26, 'rewardPerEpisode': 13.899374419251297
'totalSteps': 11520, 'rewardStep': 0.9378508571543934, 'errorList': [239.71138204880825, 278.6421211081919, 210.90292611568236, 315.84987022621317, 215.45017176635662, 274.0140673764512, 322.69403414885954, 199.79267803016648, 282.9755663968961, 268.0149976733093, 317.32783713138315, 243.44309157846138, 330.0954231551275, 320.5150601927923, 297.3256420257883, 305.172923967007, 103.80621815598465, 244.62681197208616, 334.09648387903354, 262.8866526040076, 224.9977427683512, 250.85556968394184, 212.5008652034468, 127.0673635675946, 302.7456540108451, 261.32773531695904, 329.6992207709167, 210.28774506449247, 237.74471395939287, 278.6079947647006, 267.1469941222341, 317.9057806831158, 276.1777962724702, 183.82586719157007, 344.04728894698275, 329.8953136394345, 297.91699146355614, 294.5551921951387, 189.47625968118618, 239.25987585914828, 277.0533173640447, 330.999300243543, 218.9994567130597, 325.9845050783549, 302.9853636616979, 266.4053650025299, 293.44127171027117, 242.6903154488088, 313.51981988813156, 273.4120992727718], 'lossList': [0.0, -1.4034522134065628, 0.0, 106.60164920806885, 0.0, 0.0, 0.0], 'rewardMean': 0.6484194900182768, 'totalEpisodes': 95, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.833101692134768, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.7978489401550264, 'errorList': [], 'lossList': [0.0, -1.4063940757513047, 0.0, 41.076348056793215, 0.0, 0.0, 0.0], 'rewardMean': 0.6633624350319518, 'totalEpisodes': 113, 'stepsPerEpisode': 72, 'rewardPerEpisode': 64.4720181094136
'totalSteps': 14080, 'rewardStep': 0.735374106055931, 'errorList': [], 'lossList': [0.0, -1.397514197230339, 0.0, 21.766300370693205, 0.0, 0.0, 0.0], 'rewardMean': 0.6930967864169575, 'totalEpisodes': 118, 'stepsPerEpisode': 274, 'rewardPerEpisode': 202.8218311007134
'totalSteps': 15360, 'rewardStep': 0.7942876761365065, 'errorList': [], 'lossList': [0.0, -1.3715233719348907, 0.0, 66.91539814949036, 0.0, 0.0, 0.0], 'rewardMean': 0.6912940461083033, 'totalEpisodes': 128, 'stepsPerEpisode': 36, 'rewardPerEpisode': 29.31515328123696
'totalSteps': 16640, 'rewardStep': 0.8734199402167778, 'errorList': [], 'lossList': [0.0, -1.3599686002731324, 0.0, 14.2290509223938, 0.0, 0.0, 0.0], 'rewardMean': 0.6892193838493544, 'totalEpisodes': 132, 'stepsPerEpisode': 110, 'rewardPerEpisode': 92.50842172039627
'totalSteps': 17920, 'rewardStep': 0.29651020025600877, 'errorList': [], 'lossList': [0.0, -1.3431885945796966, 0.0, 13.516304125785828, 0.0, 0.0, 0.0], 'rewardMean': 0.6653041570584325, 'totalEpisodes': 135, 'stepsPerEpisode': 489, 'rewardPerEpisode': 315.32431451350567
'totalSteps': 19200, 'rewardStep': 0.8323631823223887, 'errorList': [], 'lossList': [0.0, -1.3182391101121902, 0.0, 20.634164148569106, 0.0, 0.0, 0.0], 'rewardMean': 0.7151907220225701, 'totalEpisodes': 137, 'stepsPerEpisode': 759, 'rewardPerEpisode': 650.5790113592741
'totalSteps': 20480, 'rewardStep': 0.46674777694793307, 'errorList': [], 'lossList': [0.0, -1.2946079581975938, 0.0, 13.197375835180283, 0.0, 0.0, 0.0], 'rewardMean': 0.7064316497813741, 'totalEpisodes': 139, 'stepsPerEpisode': 442, 'rewardPerEpisode': 331.2760725516684
'totalSteps': 21760, 'rewardStep': 0.47620120936694255, 'errorList': [], 'lossList': [0.0, -1.2827527713775635, 0.0, 7.59679447889328, 0.0, 0.0, 0.0], 'rewardMean': 0.6779371773533824, 'totalEpisodes': 141, 'stepsPerEpisode': 402, 'rewardPerEpisode': 284.958491739647
'totalSteps': 23040, 'rewardStep': 0.6187243334888233, 'errorList': [], 'lossList': [0.0, -1.260598425269127, 0.0, 2.5523565089702607, 0.0, 0.0, 0.0], 'rewardMean': 0.6829328222100732, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 627.7639630282913
'totalSteps': 24320, 'rewardStep': 0.4995580843434729, 'errorList': [], 'lossList': [0.0, -1.2587796258926391, 0.0, 4.477497301697731, 0.0, 0.0, 0.0], 'rewardMean': 0.6391035449289812, 'totalEpisodes': 143, 'stepsPerEpisode': 290, 'rewardPerEpisode': 214.96275033608921
'totalSteps': 25600, 'rewardStep': 0.7733648672051375, 'errorList': [], 'lossList': [0.0, -1.2572853565216064, 0.0, 3.3140537679195403, 0.0, 0.0, 0.0], 'rewardMean': 0.6366551376339923, 'totalEpisodes': 145, 'stepsPerEpisode': 361, 'rewardPerEpisode': 308.3684617671489
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=82.23
