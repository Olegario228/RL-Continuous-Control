#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 114
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.864417666605857, 'errorList': [], 'lossList': [0.0, -1.4232667148113252, 0.0, 29.945975478887558, 0.0, 0.0, 0.0], 'rewardMean': 0.7681665363746002, 'totalEpisodes': 13, 'stepsPerEpisode': 317, 'rewardPerEpisode': 251.91180006605455
'totalSteps': 3840, 'rewardStep': 0.7273263214908876, 'errorList': [], 'lossList': [0.0, -1.4396926659345626, 0.0, 30.767467167377472, 0.0, 0.0, 0.0], 'rewardMean': 0.7545531314133628, 'totalEpisodes': 16, 'stepsPerEpisode': 171, 'rewardPerEpisode': 129.1297816389703
'totalSteps': 5120, 'rewardStep': 0.6043217501929694, 'errorList': [], 'lossList': [0.0, -1.4441898328065872, 0.0, 20.942350497245787, 0.0, 0.0, 0.0], 'rewardMean': 0.7169952861082645, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 939.7434640516649
'totalSteps': 6400, 'rewardStep': 0.809868661650707, 'errorList': [], 'lossList': [0.0, -1.4208572101593018, 0.0, 33.43790386199951, 0.0, 0.0, 0.0], 'rewardMean': 0.735569961216753, 'totalEpisodes': 18, 'stepsPerEpisode': 866, 'rewardPerEpisode': 668.1943377083392
'totalSteps': 7680, 'rewardStep': 0.9035764480056094, 'errorList': [], 'lossList': [0.0, -1.4145493870973587, 0.0, 23.02619988322258, 0.0, 0.0, 0.0], 'rewardMean': 0.763571042348229, 'totalEpisodes': 19, 'stepsPerEpisode': 428, 'rewardPerEpisode': 317.92942734802443
'totalSteps': 8960, 'rewardStep': 0.7914242211310026, 'errorList': [], 'lossList': [0.0, -1.3842687702178955, 0.0, 81.356870803833, 0.0, 0.0, 0.0], 'rewardMean': 0.7675500678886252, 'totalEpisodes': 24, 'stepsPerEpisode': 345, 'rewardPerEpisode': 253.3234754038652
'totalSteps': 10240, 'rewardStep': 0.6536630964496264, 'errorList': [], 'lossList': [0.0, -1.3793891996145249, 0.0, 290.26482624053955, 0.0, 0.0, 0.0], 'rewardMean': 0.7533141964587504, 'totalEpisodes': 52, 'stepsPerEpisode': 56, 'rewardPerEpisode': 47.40933608115137
'totalSteps': 11520, 'rewardStep': 0.8747650087004305, 'errorList': [], 'lossList': [0.0, -1.3780549603700638, 0.0, 183.55688247680663, 0.0, 0.0, 0.0], 'rewardMean': 0.7668087311522704, 'totalEpisodes': 77, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.7330436885013296
'totalSteps': 12800, 'rewardStep': 0.7342079469169203, 'errorList': [], 'lossList': [0.0, -1.3755772197246552, 0.0, 90.78153734207153, 0.0, 0.0, 0.0], 'rewardMean': 0.7635486527287354, 'totalEpisodes': 98, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.309938157768697
'totalSteps': 14080, 'rewardStep': 0.7365791388133205, 'errorList': [], 'lossList': [0.0, -1.3708467024564743, 0.0, 49.40528314590454, 0.0, 0.0, 0.0], 'rewardMean': 0.7700150259957332, 'totalEpisodes': 115, 'stepsPerEpisode': 69, 'rewardPerEpisode': 52.346742129700516
'totalSteps': 15360, 'rewardStep': 0.6648571344555847, 'errorList': [], 'lossList': [0.0, -1.3666416203975678, 0.0, 39.33075259208679, 0.0, 0.0, 0.0], 'rewardMean': 0.7500589727807059, 'totalEpisodes': 127, 'stepsPerEpisode': 138, 'rewardPerEpisode': 119.7239673560668
'totalSteps': 16640, 'rewardStep': 0.5564951973912001, 'errorList': [], 'lossList': [0.0, -1.3582899194955826, 0.0, 34.20639120101929, 0.0, 0.0, 0.0], 'rewardMean': 0.7329758603707371, 'totalEpisodes': 137, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.319094085710915
'totalSteps': 17920, 'rewardStep': 0.7436896503427592, 'errorList': [], 'lossList': [0.0, -1.3454548084735871, 0.0, 23.958134968280792, 0.0, 0.0, 0.0], 'rewardMean': 0.7469126503857161, 'totalEpisodes': 142, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.161611049068902
'totalSteps': 19200, 'rewardStep': 0.5855814065744964, 'errorList': [], 'lossList': [0.0, -1.3349738031625749, 0.0, 12.455106073617936, 0.0, 0.0, 0.0], 'rewardMean': 0.724483924878095, 'totalEpisodes': 147, 'stepsPerEpisode': 125, 'rewardPerEpisode': 98.17382726049836
'totalSteps': 20480, 'rewardStep': 0.9062347909310436, 'errorList': [], 'lossList': [0.0, -1.3240558183193207, 0.0, 8.286760985851288, 0.0, 0.0, 0.0], 'rewardMean': 0.7247497591706383, 'totalEpisodes': 151, 'stepsPerEpisode': 403, 'rewardPerEpisode': 348.52587507199553
'totalSteps': 21760, 'rewardStep': 0.7680408573328975, 'errorList': [], 'lossList': [0.0, -1.3179563641548158, 0.0, 5.807203456163406, 0.0, 0.0, 0.0], 'rewardMean': 0.7224114227908279, 'totalEpisodes': 154, 'stepsPerEpisode': 107, 'rewardPerEpisode': 88.62170179864636
'totalSteps': 23040, 'rewardStep': 0.7609492239410336, 'errorList': [], 'lossList': [0.0, -1.3221124917268754, 0.0, 5.295805748701095, 0.0, 0.0, 0.0], 'rewardMean': 0.7331400355399686, 'totalEpisodes': 157, 'stepsPerEpisode': 99, 'rewardPerEpisode': 77.4848135750391
'totalSteps': 24320, 'rewardStep': 0.7391337388898687, 'errorList': [], 'lossList': [0.0, -1.3243490982055663, 0.0, 3.806618290543556, 0.0, 0.0, 0.0], 'rewardMean': 0.7195769085589125, 'totalEpisodes': 159, 'stepsPerEpisode': 347, 'rewardPerEpisode': 295.49189092000097
'totalSteps': 25600, 'rewardStep': 0.9692667118397114, 'errorList': [0.027385765284970432, 0.03133641721947018, 0.027982756890198426, 0.028479874364144033, 0.028463979672104626, 0.027575120123544884, 0.027886312134995424, 0.02754538399717126, 0.055485533037860975, 0.04548226189105003, 0.0282832808356338, 0.046767486398414354, 0.06748382601078608, 0.047738415989089095, 0.03330371014473614, 0.028056973143300874, 0.06471820770664616, 0.08866383185204062, 0.03266685118347872, 0.028242458447041315, 0.03387448675825304, 0.027191925066009322, 0.027641176689567833, 0.028497946429786997, 0.028135195105539388, 0.03485745397980279, 0.027611478856329464, 0.027650556293005835, 0.028046094407558477, 0.02837928335020212, 0.027801114764329688, 0.027673051052650895, 0.035317736335203444, 0.028147296979234824, 0.05512414081616126, 0.028332707720525124, 0.02721795130653522, 0.027394208139600883, 0.040608092977513145, 0.056451996394789816, 0.027854976815869733, 0.02794100710120844, 0.03665831268068363, 0.055166626468811424, 0.05328158401542984, 0.032742755630253896, 0.028084041943779493, 0.027314086653036813, 0.05747903962905813, 0.03546540099559081], 'lossList': [0.0, -1.313605015873909, 0.0, 3.6194812798500062, 0.0, 0.0, 0.0], 'rewardMean': 0.7430827850511916, 'totalEpisodes': 160, 'stepsPerEpisode': 568, 'rewardPerEpisode': 490.29441901845456, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.61
