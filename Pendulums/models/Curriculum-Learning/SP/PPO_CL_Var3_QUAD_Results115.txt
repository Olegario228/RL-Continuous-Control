#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 115
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9349963128797502, 'errorList': [], 'lossList': [0.0, -1.4349378436803817, 0.0, 30.241871619224547, 0.0, 0.0, 0.0], 'rewardMean': 0.9152889520439587, 'totalEpisodes': 8, 'stepsPerEpisode': 530, 'rewardPerEpisode': 376.87648648120023
'totalSteps': 3840, 'rewardStep': 0.7456302174131709, 'errorList': [], 'lossList': [0.0, -1.4202284055948258, 0.0, 31.40852157354355, 0.0, 0.0, 0.0], 'rewardMean': 0.8587360405003627, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 201.63440566288736
'totalSteps': 5120, 'rewardStep': 0.7495069105404504, 'errorList': [], 'lossList': [0.0, -1.4155706518888473, 0.0, 24.211753877401353, 0.0, 0.0, 0.0], 'rewardMean': 0.8314287580103846, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.8240783112693
'totalSteps': 6400, 'rewardStep': 0.9436575683960641, 'errorList': [], 'lossList': [0.0, -1.3997744876146316, 0.0, 20.41408282279968, 0.0, 0.0, 0.0], 'rewardMean': 0.8538745200875205, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.8496035035962
'totalSteps': 7680, 'rewardStep': 0.6634795774792036, 'errorList': [], 'lossList': [0.0, -1.3817337387800217, 0.0, 11.403034805953503, 0.0, 0.0, 0.0], 'rewardMean': 0.822142029652801, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.4473106971118
'totalSteps': 8960, 'rewardStep': 0.702436640884857, 'errorList': [], 'lossList': [0.0, -1.336613456606865, 0.0, 59.37293909311295, 0.0, 0.0, 0.0], 'rewardMean': 0.805041259828809, 'totalEpisodes': 14, 'stepsPerEpisode': 80, 'rewardPerEpisode': 71.29332678637273
'totalSteps': 10240, 'rewardStep': 0.8519696321497632, 'errorList': [], 'lossList': [0.0, -1.3338825207948686, 0.0, 314.71329948425296, 0.0, 0.0, 0.0], 'rewardMean': 0.8109073063689283, 'totalEpisodes': 43, 'stepsPerEpisode': 64, 'rewardPerEpisode': 50.20724732657244
'totalSteps': 11520, 'rewardStep': 0.9129241343277485, 'errorList': [], 'lossList': [0.0, -1.3339878976345063, 0.0, 197.5099383544922, 0.0, 0.0, 0.0], 'rewardMean': 0.8222425094754638, 'totalEpisodes': 88, 'stepsPerEpisode': 32, 'rewardPerEpisode': 28.551953528406457
'totalSteps': 12800, 'rewardStep': 0.37999895155760377, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7264598897186215, 'totalEpisodes': 128, 'stepsPerEpisode': 16, 'rewardPerEpisode': 9.174784955052662
'totalSteps': 14080, 'rewardStep': 0.7934979159143551, 'errorList': [], 'lossList': [0.0, -1.3334635430574417, 0.0, 93.71260236740112, 0.0, 0.0, 0.0], 'rewardMean': 0.712310050022082, 'totalEpisodes': 159, 'stepsPerEpisode': 37, 'rewardPerEpisode': 33.90845839547469
'totalSteps': 15360, 'rewardStep': 0.9633216396887414, 'errorList': [338.7396648474624, 289.76537091885615, 321.1418000183759, 329.5182706868207, 296.0877856695577, 330.5793503340349, 213.54401433392346, 338.6516262075168, 317.7981850489041, 293.46825120671014, 209.4878021064636, 319.27061173562635, 316.5880044215391, 298.50664205980723, 330.4341122454025, 326.3926958191769, 311.54601155648373, 280.05635435114243, 292.6850705553849, 276.9329522891709, 214.13302535440266, 276.20819449804895, 335.9376779671031, 330.05371765149897, 300.5235121456644, 291.6267399339842, 307.10521486123986, 320.3480672381304, 322.160815847773, 311.9439100249743, 319.2589384086811, 255.4179331991761, 312.92267725764486, 170.628185211636, 324.1575348438812, 288.6231795869988, 342.8631102770915, 318.1715963082285, 284.4452606684255, 336.1653777893564, 338.5971028594939, 301.2525518152518, 328.7417586927026, 225.79646556159057, 273.7070302271492, 252.60781903219515, 303.91445054540316, 309.19065349062276, 320.2542543958155, 262.6560408407902], 'lossList': [0.0, -1.3300673133134842, 0.0, 92.79886196136475, 0.0, 0.0, 0.0], 'rewardMean': 0.734079192249639, 'totalEpisodes': 199, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.262688529385353, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6804189400202193, 'errorList': [], 'lossList': [0.0, -1.3312121438980102, 0.0, 55.42394347190857, 0.0, 0.0, 0.0], 'rewardMean': 0.7271703951976161, 'totalEpisodes': 221, 'stepsPerEpisode': 201, 'rewardPerEpisode': 175.2853621020325
'totalSteps': 17920, 'rewardStep': 0.8193456068053683, 'errorList': [], 'lossList': [0.0, -1.3327454221248627, 0.0, 43.63063341140747, 0.0, 0.0, 0.0], 'rewardMean': 0.7147391990385463, 'totalEpisodes': 237, 'stepsPerEpisode': 36, 'rewardPerEpisode': 28.546767309657472
'totalSteps': 19200, 'rewardStep': 0.6031332108173242, 'errorList': [], 'lossList': [0.0, -1.3259755885601043, 0.0, 29.644871754646303, 0.0, 0.0, 0.0], 'rewardMean': 0.7087045623723583, 'totalEpisodes': 247, 'stepsPerEpisode': 30, 'rewardPerEpisode': 17.428235105837658
'totalSteps': 20480, 'rewardStep': 0.7872097941921171, 'errorList': [], 'lossList': [0.0, -1.3192102801799774, 0.0, 12.374615790843963, 0.0, 0.0, 0.0], 'rewardMean': 0.7171818777030845, 'totalEpisodes': 253, 'stepsPerEpisode': 164, 'rewardPerEpisode': 129.78453740344347
'totalSteps': 21760, 'rewardStep': 0.7417796577098255, 'errorList': [], 'lossList': [0.0, -1.3013641315698623, 0.0, 7.732010402679443, 0.0, 0.0, 0.0], 'rewardMean': 0.7061628802590907, 'totalEpisodes': 259, 'stepsPerEpisode': 73, 'rewardPerEpisode': 54.965879817823016
'totalSteps': 23040, 'rewardStep': 0.6295926045996945, 'errorList': [], 'lossList': [0.0, -1.2879247695207596, 0.0, 8.492645845413207, 0.0, 0.0, 0.0], 'rewardMean': 0.6778297272862852, 'totalEpisodes': 261, 'stepsPerEpisode': 210, 'rewardPerEpisode': 172.46536406157168
'totalSteps': 24320, 'rewardStep': 0.6478653051913994, 'errorList': [], 'lossList': [0.0, -1.2665159392356873, 0.0, 5.7600760170817376, 0.0, 0.0, 0.0], 'rewardMean': 0.7046163626496649, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1030.487834433166
'totalSteps': 25600, 'rewardStep': 0.7946489686031697, 'errorList': [], 'lossList': [0.0, -1.249576872587204, 0.0, 4.435715947151184, 0.0, 0.0, 0.0], 'rewardMean': 0.7460813643542215, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.9359880921309
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=79.25
