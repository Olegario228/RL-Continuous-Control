#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 116
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7636948187986704, 'errorList': [], 'lossList': [0.0, -1.4494908154010773, 0.0, 36.324383888244625, 0.0, 0.0, 0.0], 'rewardMean': 0.6611712686375127, 'totalEpisodes': 12, 'stepsPerEpisode': 67, 'rewardPerEpisode': 56.90214933657757
'totalSteps': 3840, 'rewardStep': 0.8984012541260026, 'errorList': [], 'lossList': [0.0, -1.4670674008131028, 0.0, 28.46269616365433, 0.0, 0.0, 0.0], 'rewardMean': 0.7402479304670093, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.5981780845394
'totalSteps': 5120, 'rewardStep': 0.5055826189523664, 'errorList': [], 'lossList': [0.0, -1.4430218708515168, 0.0, 26.044916573762894, 0.0, 0.0, 0.0], 'rewardMean': 0.6815816025883485, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.1607777006343
'totalSteps': 6400, 'rewardStep': 0.8955305290837299, 'errorList': [], 'lossList': [0.0, -1.4415414696931839, 0.0, 20.166529378890992, 0.0, 0.0, 0.0], 'rewardMean': 0.7243713878874247, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.7147283892134
'totalSteps': 7680, 'rewardStep': 0.888142283806116, 'errorList': [], 'lossList': [0.0, -1.4212849873304367, 0.0, 16.316859366595747, 0.0, 0.0, 0.0], 'rewardMean': 0.7516665372072066, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.9487953746277
'totalSteps': 8960, 'rewardStep': 0.9313051186038428, 'errorList': [], 'lossList': [0.0, -1.4095945376157761, 0.0, 27.35022533774376, 0.0, 0.0, 0.0], 'rewardMean': 0.7773291916924404, 'totalEpisodes': 13, 'stepsPerEpisode': 791, 'rewardPerEpisode': 648.8401199844352
'totalSteps': 10240, 'rewardStep': 0.5541690042499465, 'errorList': [], 'lossList': [0.0, -1.399219752550125, 0.0, 591.1708465576172, 0.0, 0.0, 0.0], 'rewardMean': 0.7494341682621286, 'totalEpisodes': 56, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.854428346802566
'totalSteps': 11520, 'rewardStep': 0.5534511467259513, 'errorList': [], 'lossList': [0.0, -1.3977604389190674, 0.0, 201.82659408569336, 0.0, 0.0, 0.0], 'rewardMean': 0.7276582769803311, 'totalEpisodes': 104, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.005776275221884
'totalSteps': 12800, 'rewardStep': 0.6856755321491218, 'errorList': [], 'lossList': [0.0, -1.3954195004701615, 0.0, 88.14715892791747, 0.0, 0.0, 0.0], 'rewardMean': 0.7234600024972102, 'totalEpisodes': 152, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.104053536691928
'totalSteps': 14080, 'rewardStep': 0.7624160423848408, 'errorList': [], 'lossList': [0.0, -1.3924962639808656, 0.0, 66.36273376464844, 0.0, 0.0, 0.0], 'rewardMean': 0.7438368348880589, 'totalEpisodes': 186, 'stepsPerEpisode': 115, 'rewardPerEpisode': 100.49128433685975
'totalSteps': 15360, 'rewardStep': 0.7342472983367935, 'errorList': [], 'lossList': [0.0, -1.3962077850103378, 0.0, 40.15762638092041, 0.0, 0.0, 0.0], 'rewardMean': 0.7408920828418711, 'totalEpisodes': 208, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.9130195136051267
'totalSteps': 16640, 'rewardStep': 0.49119921353465157, 'errorList': [], 'lossList': [0.0, -1.4028775614500046, 0.0, 24.991401267051696, 0.0, 0.0, 0.0], 'rewardMean': 0.7001718787827361, 'totalEpisodes': 224, 'stepsPerEpisode': 42, 'rewardPerEpisode': 29.511248931152068
'totalSteps': 17920, 'rewardStep': 0.5869857240997629, 'errorList': [], 'lossList': [0.0, -1.3995943254232406, 0.0, 18.202181143760683, 0.0, 0.0, 0.0], 'rewardMean': 0.7083121892974756, 'totalEpisodes': 232, 'stepsPerEpisode': 153, 'rewardPerEpisode': 117.75602111749858
'totalSteps': 19200, 'rewardStep': 0.9816726055162728, 'errorList': [139.07564699901624, 19.988409427990153, 51.303745956103306, 61.2006006028773, 32.09105547534139, 75.09731207141327, 24.180455307410213, 3.3301726326471255, 6.3746026618137455, 54.79525863178094, 3.5115965754502114, 122.13848829647601, 11.160333021194717, 73.30250208051221, 108.19269000628073, 79.90720229749755, 124.30492340641919, 0.17919683241498988, 125.99175005294592, 47.85687541001284, 97.15944908902519, 12.038952409346239, 54.15262443507724, 136.8064777572294, 35.85345261749813, 29.661993358118032, 119.877544289184, 137.36172380652164, 157.56280092384534, 104.07113218040317, 18.1725922877723, 49.292568911712976, 49.126281894978774, 77.0720520631497, 114.77890985767577, 166.16252715058818, 47.20245119485638, 126.50850695440984, 139.120213266928, 39.13781935729122, 163.5620038222321, 81.41761683312329, 7.14406518656368, 9.901074420368756, 84.55886273146655, 64.9255022251704, 84.23139985770824, 135.794798163724, 9.037801282318352, 125.9762913620519], 'lossList': [0.0, -1.3857769745588302, 0.0, 20.884754056930543, 0.0, 0.0, 0.0], 'rewardMean': 0.71692639694073, 'totalEpisodes': 241, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.389282034443934, 'successfulTests': 1
'totalSteps': 20480, 'rewardStep': 0.6245394189561235, 'errorList': [], 'lossList': [0.0, -1.3732163339853287, 0.0, 13.911745312213897, 0.0, 0.0, 0.0], 'rewardMean': 0.6905661104557307, 'totalEpisodes': 247, 'stepsPerEpisode': 82, 'rewardPerEpisode': 68.27181414851395
'totalSteps': 21760, 'rewardStep': 0.45625232520512216, 'errorList': [], 'lossList': [0.0, -1.3496042835712432, 0.0, 10.945931458473206, 0.0, 0.0, 0.0], 'rewardMean': 0.6430608311158588, 'totalEpisodes': 252, 'stepsPerEpisode': 492, 'rewardPerEpisode': 414.9990300923626
'totalSteps': 23040, 'rewardStep': 0.8920317271689443, 'errorList': [], 'lossList': [0.0, -1.340977303981781, 0.0, 9.542410628795624, 0.0, 0.0, 0.0], 'rewardMean': 0.6768471034077584, 'totalEpisodes': 257, 'stepsPerEpisode': 48, 'rewardPerEpisode': 39.80365909276876
'totalSteps': 24320, 'rewardStep': 0.5102770871375059, 'errorList': [], 'lossList': [0.0, -1.3297009640932083, 0.0, 6.545145078897476, 0.0, 0.0, 0.0], 'rewardMean': 0.6725296974489139, 'totalEpisodes': 261, 'stepsPerEpisode': 203, 'rewardPerEpisode': 150.86583454822522
'totalSteps': 25600, 'rewardStep': 0.832342549232327, 'errorList': [], 'lossList': [0.0, -1.2889845538139344, 0.0, 4.293565421700477, 0.0, 0.0, 0.0], 'rewardMean': 0.6871963991572343, 'totalEpisodes': 262, 'stepsPerEpisode': 1274, 'rewardPerEpisode': 1006.902961661026
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=19200, timeSpent=82.97
