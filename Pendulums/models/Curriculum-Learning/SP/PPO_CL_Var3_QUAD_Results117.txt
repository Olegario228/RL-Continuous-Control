#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 117
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.853443489341011, 'errorList': [], 'lossList': [0.0, -1.4428993797302245, 0.0, 27.420334918498995, 0.0, 0.0, 0.0], 'rewardMean': 0.6656146322819045, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 652.5338135831677
'totalSteps': 3840, 'rewardStep': 0.8559374375720307, 'errorList': [], 'lossList': [0.0, -1.4603899198770522, 0.0, 41.48236214876175, 0.0, 0.0, 0.0], 'rewardMean': 0.7290555673786132, 'totalEpisodes': 12, 'stepsPerEpisode': 667, 'rewardPerEpisode': 511.9470711272257
'totalSteps': 5120, 'rewardStep': 0.823693101100567, 'errorList': [], 'lossList': [0.0, -1.4645235180854796, 0.0, 23.190952165126802, 0.0, 0.0, 0.0], 'rewardMean': 0.7527149508091017, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 957.9520103125293
'totalSteps': 6400, 'rewardStep': 0.4975247585313322, 'errorList': [], 'lossList': [0.0, -1.4448924469947815, 0.0, 28.323975256085397, 0.0, 0.0, 0.0], 'rewardMean': 0.7016769123535478, 'totalEpisodes': 13, 'stepsPerEpisode': 575, 'rewardPerEpisode': 437.89916036280107
'totalSteps': 7680, 'rewardStep': 0.641626484771439, 'errorList': [], 'lossList': [0.0, -1.440283932685852, 0.0, 47.84225492715836, 0.0, 0.0, 0.0], 'rewardMean': 0.6916685077565297, 'totalEpisodes': 16, 'stepsPerEpisode': 285, 'rewardPerEpisode': 224.21093093438716
'totalSteps': 8960, 'rewardStep': 0.9025443192810282, 'errorList': [], 'lossList': [0.0, -1.4380162841081618, 0.0, 73.53780177593231, 0.0, 0.0, 0.0], 'rewardMean': 0.7217936236886009, 'totalEpisodes': 21, 'stepsPerEpisode': 358, 'rewardPerEpisode': 252.00592043695468
'totalSteps': 10240, 'rewardStep': 0.6806885725879561, 'errorList': [], 'lossList': [0.0, -1.4376130819320678, 0.0, 422.51472633361817, 0.0, 0.0, 0.0], 'rewardMean': 0.7166554923010202, 'totalEpisodes': 67, 'stepsPerEpisode': 43, 'rewardPerEpisode': 31.788942301579404
'totalSteps': 11520, 'rewardStep': 0.6464329296775798, 'errorList': [], 'lossList': [0.0, -1.4380331414937972, 0.0, 168.48055904388428, 0.0, 0.0, 0.0], 'rewardMean': 0.7088529853428602, 'totalEpisodes': 112, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.923765149106154
'totalSteps': 12800, 'rewardStep': 0.9718730590504898, 'errorList': [290.9379326255915, 249.35158462216828, 267.70637652776503, 297.6849147279572, 282.7697184727675, 317.6280796793258, 307.9345550626855, 307.92006479031477, 309.4770403160291, 278.20464171071177, 257.4477959785879, 314.22435193033897, 282.33589116971183, 157.1958055257693, 253.70712741712273, 324.1621895511486, 305.02806223662884, 171.3642275115327, 330.0034855526873, 306.9799317510353, 299.2796942814538, 213.41300617181295, 279.99026291100125, 264.85197503694263, 304.27617125246667, 326.4837970564622, 303.31225611526247, 299.9006700606149, 254.23219874324667, 304.53016017394106, 307.3902418686696, 295.1407578107881, 308.6855555543107, 276.7353641177453, 275.0080883609528, 274.1776692051352, 308.4086226392934, 297.7028721806, 205.9322006235403, 300.69656974224523, 285.00440417458304, 287.2429160633157, 310.2184480400106, 302.2950179715977, 288.690481168953, 301.4286945603952, 322.6922138084862, 315.066836055289, 210.19177800137905, 320.2851204618147], 'lossList': [0.0, -1.441143189072609, 0.0, 96.45392086029052, 0.0, 0.0, 0.0], 'rewardMean': 0.7351549927136232, 'totalEpisodes': 145, 'stepsPerEpisode': 36, 'rewardPerEpisode': 33.10259496388965, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.8492849647428855, 'errorList': [], 'lossList': [0.0, -1.441689555644989, 0.0, 60.103460502624515, 0.0, 0.0, 0.0], 'rewardMean': 0.772304911665632, 'totalEpisodes': 163, 'stepsPerEpisode': 44, 'rewardPerEpisode': 37.64800817506052
'totalSteps': 15360, 'rewardStep': 0.28634806022884157, 'errorList': [], 'lossList': [0.0, -1.4359141492843628, 0.0, 39.61262166976929, 0.0, 0.0, 0.0], 'rewardMean': 0.715595368754415, 'totalEpisodes': 183, 'stepsPerEpisode': 70, 'rewardPerEpisode': 43.4167251106535
'totalSteps': 16640, 'rewardStep': 0.9587649538055056, 'errorList': [77.4371487342483, 107.9472012764579, 43.174208024384306, 122.08749228963836, 229.50716381058635, 49.847135538687745, 193.8763832488215, 167.32298404601977, 177.49248774060825, 190.88789005382608, 183.07197107767695, 205.38897282441934, 226.16045720811312, 206.98197307729814, 209.68319450684092, 89.92600859292186, 140.38429819600356, 176.57384418895091, 106.920923138865, 188.7673921709717, 154.2834698865448, 243.31950715633587, 122.18626660759107, 189.7197570994189, 238.24892408142853, 93.4953154015872, 119.92503557463469, 198.43703216649692, 224.86404087890955, 131.3676836262724, 185.13074794278538, 143.59864197514202, 241.2614251070376, 117.54277370098565, 140.32952749786557, 132.85552338140437, 127.73153777295998, 182.27113234348238, 176.4785991207038, 126.449182035962, 236.9856950896828, 172.70573944889463, 144.23610865880124, 204.28873893411674, 172.29606596432654, 161.7297381652051, 182.79006945843182, 32.813011301543234, 178.07511382926674, 85.34296788930985], 'lossList': [0.0, -1.4291349726915359, 0.0, 29.168680682182313, 0.0, 0.0, 0.0], 'rewardMean': 0.7258781203777624, 'totalEpisodes': 197, 'stepsPerEpisode': 29, 'rewardPerEpisode': 22.97314184619745, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7173083581253875, 'errorList': [], 'lossList': [0.0, -1.419845582842827, 0.0, 29.19238986492157, 0.0, 0.0, 0.0], 'rewardMean': 0.7152396460802446, 'totalEpisodes': 208, 'stepsPerEpisode': 68, 'rewardPerEpisode': 51.82685417178317
'totalSteps': 19200, 'rewardStep': 0.29618675619854623, 'errorList': [], 'lossList': [0.0, -1.4065583753585815, 0.0, 15.281830804347992, 0.0, 0.0, 0.0], 'rewardMean': 0.6951058458469659, 'totalEpisodes': 212, 'stepsPerEpisode': 211, 'rewardPerEpisode': 158.0922169971887
'totalSteps': 20480, 'rewardStep': 0.43287818373091047, 'errorList': [], 'lossList': [0.0, -1.389649679660797, 0.0, 23.88500130414963, 0.0, 0.0, 0.0], 'rewardMean': 0.6742310157429131, 'totalEpisodes': 218, 'stepsPerEpisode': 145, 'rewardPerEpisode': 116.55729181467967
'totalSteps': 21760, 'rewardStep': 0.8439527255228868, 'errorList': [], 'lossList': [0.0, -1.3885338288545608, 0.0, 10.543016108870507, 0.0, 0.0, 0.0], 'rewardMean': 0.6683718563670988, 'totalEpisodes': 221, 'stepsPerEpisode': 113, 'rewardPerEpisode': 95.60867093995085
'totalSteps': 23040, 'rewardStep': 0.9640846251927432, 'errorList': [0.1425533696297443, 0.11772003834837749, 0.1500931713105152, 0.1733917157128616, 0.09730450110591204, 0.10261082166284267, 0.10892804127765449, 0.10857574712129733, 0.08338308353225758, 0.10493264936992963, 0.15305713499938878, 0.1680323267739236, 0.1169025150161093, 0.1086176494989435, 0.18725998074383546, 0.1572687665796749, 0.11634832463236551, 0.1482866245949257, 0.1551434522651015, 0.11367625057158476, 0.13954102553581962, 0.11157379260350134, 0.20556481445327573, 0.1147612271348904, 0.14729625676188857, 0.156912103994111, 0.09914092265179131, 0.10527790244520183, 0.16611471818833254, 0.08282418130905056, 0.12302408524348378, 0.09727482971703529, 0.08794417919902324, 0.21785403677835874, 0.09698860812871259, 0.10513145264118207, 0.19956177806230818, 0.10114229925787765, 0.0963405556080008, 0.07487508190347118, 0.13063171880386945, 0.12843353051125078, 0.15315176478374734, 0.17144565941872017, 0.11792917633364924, 0.10404070021982988, 0.09488637724769435, 0.07711876027401023, 0.2384890769201418, 0.12895139439745357], 'lossList': [0.0, -1.3869943720102311, 0.0, 7.334726457744837, 0.0, 0.0, 0.0], 'rewardMean': 0.6967114616275777, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.8567551571257, 'successfulTests': 47
'totalSteps': 24320, 'rewardStep': 0.855018080805863, 'errorList': [], 'lossList': [0.0, -1.3771075409650804, 0.0, 4.940633583515883, 0.0, 0.0, 0.0], 'rewardMean': 0.717569976740406, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.6368638472466
'totalSteps': 25600, 'rewardStep': 0.7389404563839651, 'errorList': [], 'lossList': [0.0, -1.3656557792425155, 0.0, 3.2420257851481438, 0.0, 0.0, 0.0], 'rewardMean': 0.6942767164737534, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.7604903229962
#maxSuccessfulTests=47, maxSuccessfulTestsAtStep=23040, timeSpent=123.97
