#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 118
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.584428305370429, 'errorList': [], 'lossList': [0.0, -1.421633991599083, 0.0, 30.649561190605162, 0.0, 0.0, 0.0], 'rewardMean': 0.7213202069629843, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.336423008290957
'totalSteps': 3840, 'rewardStep': 0.9722910166714054, 'errorList': [], 'lossList': [0.0, -1.4209716266393662, 0.0, 30.876007508039475, 0.0, 0.0, 0.0], 'rewardMean': 0.804977143532458, 'totalEpisodes': 18, 'stepsPerEpisode': 488, 'rewardPerEpisode': 388.87376763131886
'totalSteps': 5120, 'rewardStep': 0.8165575309777219, 'errorList': [], 'lossList': [0.0, -1.4165548413991929, 0.0, 28.838525250554085, 0.0, 0.0, 0.0], 'rewardMean': 0.8078722403937739, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.538119289132
'totalSteps': 6400, 'rewardStep': 0.6732784974173236, 'errorList': [], 'lossList': [0.0, -1.4003856682777405, 0.0, 18.65757246494293, 0.0, 0.0, 0.0], 'rewardMean': 0.7809534917984838, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.3891547432274
'totalSteps': 7680, 'rewardStep': 0.9155986685626789, 'errorList': [], 'lossList': [0.0, -1.3735873115062713, 0.0, 14.201774365007877, 0.0, 0.0, 0.0], 'rewardMean': 0.8033943545925163, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.2469445998772
'totalSteps': 8960, 'rewardStep': 0.917744694194175, 'errorList': [], 'lossList': [0.0, -1.3635230642557143, 0.0, 25.504828273653985, 0.0, 0.0, 0.0], 'rewardMean': 0.8197301173927533, 'totalEpisodes': 19, 'stepsPerEpisode': 789, 'rewardPerEpisode': 613.0869824711212
'totalSteps': 10240, 'rewardStep': 0.9535512096637857, 'errorList': [179.1174056696866, 157.1445574841495, 170.04492329868341, 176.4833850794565, 152.49563152778592, 191.30054033555018, 165.50749673931028, 171.205745965037, 174.2599140785676, 191.94327154836515, 162.53167138075472, 189.7254231150582, 186.20745812570763, 182.6163465679456, 170.1040364683704, 180.8162726915401, 189.56984501308708, 165.84190506663754, 194.48058973916025, 192.04089894020672, 189.0523803093848, 181.5153563135577, 178.01054846349552, 175.78652753947623, 187.1363228214364, 179.97524546519895, 170.96952228656752, 181.88423225636774, 179.33875396642387, 154.50781140094676, 128.05285913554943, 185.57608528790803, 169.83131924332648, 180.4738375725216, 175.36835604379195, 181.38562295796817, 163.96731015043903, 164.02720354663558, 186.70320567660573, 158.2437257196001, 178.79890698623336, 173.3952592947657, 164.50373171417897, 173.30369065192562, 174.3436866604157, 174.16217974812704, 177.54421911909188, 172.54636051391915, 190.49910758162275, 172.8136908259722], 'lossList': [0.0, -1.3544107687473297, 0.0, 564.288923034668, 0.0, 0.0, 0.0], 'rewardMean': 0.8364577539266324, 'totalEpisodes': 56, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.573413800793949, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.47713453720171406, 'errorList': [], 'lossList': [0.0, -1.3533375668525696, 0.0, 300.84164558410646, 0.0, 0.0, 0.0], 'rewardMean': 0.7965329520683081, 'totalEpisodes': 98, 'stepsPerEpisode': 10, 'rewardPerEpisode': 5.330317351354921
'totalSteps': 12800, 'rewardStep': 0.8525369898535056, 'errorList': [], 'lossList': [0.0, -1.354078614115715, 0.0, 89.8859004020691, 0.0, 0.0, 0.0], 'rewardMean': 0.8021333558468278, 'totalEpisodes': 138, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.822468260888332
'totalSteps': 14080, 'rewardStep': 0.5018707453462629, 'errorList': [], 'lossList': [0.0, -1.359504742026329, 0.0, 36.371864500045774, 0.0, 0.0, 0.0], 'rewardMean': 0.7664992195259003, 'totalEpisodes': 176, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.477579994602952
'totalSteps': 15360, 'rewardStep': 0.8327630832114705, 'errorList': [], 'lossList': [0.0, -1.3584656816720964, 0.0, 31.374386224746704, 0.0, 0.0, 0.0], 'rewardMean': 0.7913326973100043, 'totalEpisodes': 202, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.930851397656325
'totalSteps': 16640, 'rewardStep': 0.603269220824682, 'errorList': [], 'lossList': [0.0, -1.3517442071437835, 0.0, 24.616092500686644, 0.0, 0.0, 0.0], 'rewardMean': 0.754430517725332, 'totalEpisodes': 212, 'stepsPerEpisode': 63, 'rewardPerEpisode': 46.39000437773365
'totalSteps': 17920, 'rewardStep': 0.6590274415634254, 'errorList': [], 'lossList': [0.0, -1.341042841076851, 0.0, 12.087568593025207, 0.0, 0.0, 0.0], 'rewardMean': 0.7386775087839024, 'totalEpisodes': 217, 'stepsPerEpisode': 46, 'rewardPerEpisode': 28.74924251927355
'totalSteps': 19200, 'rewardStep': 0.7403525788160582, 'errorList': [], 'lossList': [0.0, -1.3240087735652923, 0.0, 21.33970645904541, 0.0, 0.0, 0.0], 'rewardMean': 0.7453849169237758, 'totalEpisodes': 222, 'stepsPerEpisode': 72, 'rewardPerEpisode': 54.45074184597165
'totalSteps': 20480, 'rewardStep': 0.7768829083690764, 'errorList': [], 'lossList': [0.0, -1.2868278324604034, 0.0, 6.939658741950989, 0.0, 0.0, 0.0], 'rewardMean': 0.7315133409044157, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 986.354540585654
'totalSteps': 21760, 'rewardStep': 0.823517871029111, 'errorList': [], 'lossList': [0.0, -1.2567210257053376, 0.0, 5.331439580619335, 0.0, 0.0, 0.0], 'rewardMean': 0.7220906585879092, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1018.1134453321497
'totalSteps': 23040, 'rewardStep': 0.8849221168032007, 'errorList': [], 'lossList': [0.0, -1.2574488002061843, 0.0, 2.750891897380352, 0.0, 0.0, 0.0], 'rewardMean': 0.7152277493018506, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.453611701863
'totalSteps': 24320, 'rewardStep': 0.5782820979198231, 'errorList': [], 'lossList': [0.0, -1.2289419335126877, 0.0, 2.69523330911994, 0.0, 0.0, 0.0], 'rewardMean': 0.7253425053736616, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1026.8679019136998
'totalSteps': 25600, 'rewardStep': 0.9454721898685811, 'errorList': [0.461289870493786, 0.4530913788524224, 0.5746372964916544, 0.4657634331898753, 0.4725641257993058, 0.5443568077552673, 0.43286909350076574, 0.5636191552874691, 0.41360735527857667, 0.4302460526262734, 0.5532633159752242, 0.4815644079774696, 0.6499340729798869, 0.6026762530761617, 0.61135467684364, 0.6139810056276119, 0.5550705718070104, 0.4423098096770593, 0.42315636023724157, 0.551053862889463, 0.44396606546192025, 0.43264419136351656, 0.40316569121197143, 0.47613241745937357, 0.3483478376814663, 0.3611301018093825, 0.537569015630273, 0.4865870142615011, 0.7473187401040585, 0.4452125468453833, 0.4943989936092947, 0.6156949295898638, 0.40799995684037726, 0.5727398853503509, 0.37612110755290606, 0.3797008739997939, 0.4779235063366465, 0.6213118046564933, 0.5710088607041592, 0.4999577132483489, 0.5836319469440174, 0.44845793395985545, 0.3732883802616656, 0.6337447751062895, 0.5391926714940709, 0.3581800050566644, 0.521217986126102, 0.5747391741885763, 0.49905557184053256, 0.5641224227972045], 'lossList': [0.0, -1.2217259234189988, 0.0, 1.6879710622876882, 0.0, 0.0, 0.0], 'rewardMean': 0.7346360253751691, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 971.1958667828692, 'successfulTests': 0
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=103.55
