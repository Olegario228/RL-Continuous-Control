#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 119
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8774416257891215, 'errorList': [], 'lossList': [0.0, -1.422117441892624, 0.0, 33.2656111830473, 0.0, 0.0, 0.0], 'rewardMean': 0.8003962748342113, 'totalEpisodes': 13, 'stepsPerEpisode': 314, 'rewardPerEpisode': 261.5423866780401
'totalSteps': 3840, 'rewardStep': 0.7629152110439346, 'errorList': [], 'lossList': [0.0, -1.424037474989891, 0.0, 32.186503787040714, 0.0, 0.0, 0.0], 'rewardMean': 0.787902586904119, 'totalEpisodes': 15, 'stepsPerEpisode': 198, 'rewardPerEpisode': 133.8766849199856
'totalSteps': 5120, 'rewardStep': 0.610187263747378, 'errorList': [], 'lossList': [0.0, -1.4268530577421188, 0.0, 17.891702266931535, 0.0, 0.0, 0.0], 'rewardMean': 0.7434737561149337, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 894.4481152629075
'totalSteps': 6400, 'rewardStep': 0.8157030757540168, 'errorList': [], 'lossList': [0.0, -1.4213601791858672, 0.0, 16.375137779712677, 0.0, 0.0, 0.0], 'rewardMean': 0.7579196200427504, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 929.1506119571919
'totalSteps': 7680, 'rewardStep': 0.9237882158033396, 'errorList': [], 'lossList': [0.0, -1.4203107833862305, 0.0, 49.14006365537644, 0.0, 0.0, 0.0], 'rewardMean': 0.7855643860028486, 'totalEpisodes': 19, 'stepsPerEpisode': 204, 'rewardPerEpisode': 146.23411867901822
'totalSteps': 8960, 'rewardStep': 0.6648955772483321, 'errorList': [], 'lossList': [0.0, -1.4099439907073974, 0.0, 130.22321075439453, 0.0, 0.0, 0.0], 'rewardMean': 0.7683259847522034, 'totalEpisodes': 30, 'stepsPerEpisode': 212, 'rewardPerEpisode': 154.27791701404868
'totalSteps': 10240, 'rewardStep': 0.6572636996478964, 'errorList': [], 'lossList': [0.0, -1.4063983243703841, 0.0, 372.9804263305664, 0.0, 0.0, 0.0], 'rewardMean': 0.754443199114165, 'totalEpisodes': 89, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.71110305712524
'totalSteps': 11520, 'rewardStep': 0.9194817299171784, 'errorList': [], 'lossList': [0.0, -1.3998954272270203, 0.0, 139.02511322021485, 0.0, 0.0, 0.0], 'rewardMean': 0.7727808136478331, 'totalEpisodes': 125, 'stepsPerEpisode': 33, 'rewardPerEpisode': 29.393492882888214
'totalSteps': 12800, 'rewardStep': 0.5259628253923149, 'errorList': [], 'lossList': [0.0, -1.3869387710094452, 0.0, 78.08301528930664, 0.0, 0.0, 0.0], 'rewardMean': 0.7480990148222814, 'totalEpisodes': 148, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.358548975939593
'totalSteps': 14080, 'rewardStep': 0.7927266418216996, 'errorList': [], 'lossList': [0.0, -1.3746374756097794, 0.0, 54.79678330421448, 0.0, 0.0, 0.0], 'rewardMean': 0.7550365866165213, 'totalEpisodes': 166, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.91738384929011
'totalSteps': 15360, 'rewardStep': 0.9033850335342811, 'errorList': [], 'lossList': [0.0, -1.3635415488481522, 0.0, 51.94597524642944, 0.0, 0.0, 0.0], 'rewardMean': 0.7576309273910372, 'totalEpisodes': 181, 'stepsPerEpisode': 53, 'rewardPerEpisode': 46.23838200242096
'totalSteps': 16640, 'rewardStep': 0.4585211345243848, 'errorList': [], 'lossList': [0.0, -1.354824683070183, 0.0, 53.59062838554382, 0.0, 0.0, 0.0], 'rewardMean': 0.7271915197390822, 'totalEpisodes': 192, 'stepsPerEpisode': 150, 'rewardPerEpisode': 124.03272311604883
'totalSteps': 17920, 'rewardStep': 0.40064972610684113, 'errorList': [], 'lossList': [0.0, -1.3490467709302902, 0.0, 28.670238802433015, 0.0, 0.0, 0.0], 'rewardMean': 0.7062377659750284, 'totalEpisodes': 198, 'stepsPerEpisode': 113, 'rewardPerEpisode': 78.19293763019614
'totalSteps': 19200, 'rewardStep': 0.6156186828557948, 'errorList': [], 'lossList': [0.0, -1.3302690559625625, 0.0, 11.861680827140809, 0.0, 0.0, 0.0], 'rewardMean': 0.6862293266852062, 'totalEpisodes': 203, 'stepsPerEpisode': 318, 'rewardPerEpisode': 260.333996807011
'totalSteps': 20480, 'rewardStep': 0.8830749913843434, 'errorList': [], 'lossList': [0.0, -1.2935547906160354, 0.0, 8.827784869670868, 0.0, 0.0, 0.0], 'rewardMean': 0.6821580042433066, 'totalEpisodes': 208, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.416647954225814
'totalSteps': 21760, 'rewardStep': 0.5341116028078268, 'errorList': [], 'lossList': [0.0, -1.2862815266847611, 0.0, 6.983635378479957, 0.0, 0.0, 0.0], 'rewardMean': 0.6690796067992562, 'totalEpisodes': 209, 'stepsPerEpisode': 269, 'rewardPerEpisode': 207.65846908899516
'totalSteps': 23040, 'rewardStep': 0.28854678669569733, 'errorList': [], 'lossList': [0.0, -1.3092004263401031, 0.0, 7.403272050023079, 0.0, 0.0, 0.0], 'rewardMean': 0.6322079155040362, 'totalEpisodes': 210, 'stepsPerEpisode': 1241, 'rewardPerEpisode': 938.5834565652806
'totalSteps': 24320, 'rewardStep': 0.7022062717750623, 'errorList': [], 'lossList': [0.0, -1.316224000453949, 0.0, 4.709046242088079, 0.0, 0.0, 0.0], 'rewardMean': 0.6104803696898247, 'totalEpisodes': 211, 'stepsPerEpisode': 1275, 'rewardPerEpisode': 1111.963319663249
'totalSteps': 25600, 'rewardStep': 0.9714717570133677, 'errorList': [0.05966602322606306, 0.059680460924947276, 0.05971212699949129, 0.05959340876835686, 0.059648065378390625, 0.05966939872985651, 0.059694139652938946, 0.05971575483060991, 0.059652691977057436, 0.059606106949071605, 0.05963501073805156, 0.059682423176060956, 0.07340936895616408, 0.059692770257543284, 0.06008044602180418, 0.059587933148746476, 0.05972860000326116, 0.08081634294829894, 0.06459377620496656, 0.05953967927079137, 0.065989494319033, 0.06121173960881407, 0.059558323599846356, 0.05972144854306188, 0.059724751942749975, 0.059652866033120214, 0.05951860431736655, 0.05957838743608422, 0.05961677839984813, 0.05958937867102894, 0.05962690225964254, 0.05954513846033138, 0.05953570391135336, 0.08812572594757097, 0.05947244994344543, 0.0693661990708723, 0.059732889372144264, 0.07833818662206193, 0.05969579463739339, 0.059726040688779396, 0.05957547136767222, 0.05952573952749985, 0.06697230677057427, 0.05958961373067848, 0.06514131687602397, 0.08520473108111744, 0.0633173450234729, 0.06834803326177945, 0.08563237277527025, 0.05949216892230317], 'lossList': [0.0, -1.2936933344602586, 0.0, 2.7416817148029806, 0.0, 0.0, 0.0], 'rewardMean': 0.6550312628519299, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.3784883436306, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.11
