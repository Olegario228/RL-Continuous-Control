#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 120
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9079446285596902, 'errorList': [], 'lossList': [0.0, -1.4367833656072617, 0.0, 31.739554685950278, 0.0, 0.0, 0.0], 'rewardMean': 0.9113826295702592, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 382.11092827064755
'totalSteps': 3840, 'rewardStep': 0.7104010928811353, 'errorList': [], 'lossList': [0.0, -1.4213279372453689, 0.0, 30.054393784999846, 0.0, 0.0, 0.0], 'rewardMean': 0.8443887840072178, 'totalEpisodes': 12, 'stepsPerEpisode': 268, 'rewardPerEpisode': 208.71707130856865
'totalSteps': 5120, 'rewardStep': 0.7277285626236999, 'errorList': [], 'lossList': [0.0, -1.4192569494247436, 0.0, 24.65233992934227, 0.0, 0.0, 0.0], 'rewardMean': 0.8152237286613384, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.8443734351807
'totalSteps': 6400, 'rewardStep': 0.9318408054025372, 'errorList': [], 'lossList': [0.0, -1.4121494919061661, 0.0, 22.481941214203836, 0.0, 0.0, 0.0], 'rewardMean': 0.8385471440095781, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.8929403338777
'totalSteps': 7680, 'rewardStep': 0.6319690294836813, 'errorList': [], 'lossList': [0.0, -1.3960595989227296, 0.0, 13.212992311120033, 0.0, 0.0, 0.0], 'rewardMean': 0.8041174582552619, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.6420565576575
'totalSteps': 8960, 'rewardStep': 0.7458845289183875, 'errorList': [], 'lossList': [0.0, -1.3452129286527634, 0.0, 24.878285244107246, 0.0, 0.0, 0.0], 'rewardMean': 0.7957984683499941, 'totalEpisodes': 13, 'stepsPerEpisode': 473, 'rewardPerEpisode': 370.755082332144
'totalSteps': 10240, 'rewardStep': 0.9755503372664492, 'errorList': [84.16646525562324, 76.05409213696734, 82.86913488164316, 92.84775095267311, 88.56985730173145, 89.69669715096111, 91.67880875257556, 93.02835080083123, 88.28031559947159, 93.41183572738058, 85.45671253337113, 91.5730300182354, 89.8957950536895, 93.88442390404965, 93.98377526234735, 90.0444038867505, 91.75781896939684, 93.06685263730108, 84.75954946541464, 86.47313481579339, 92.75653508540364, 91.93999173330025, 89.74618820123435, 95.25173905770714, 91.98344014231235, 85.57942670188308, 94.35463234083633, 75.09975073414563, 94.93530329556565, 86.74933833699394, 86.44732024370863, 88.82030345097226, 96.63895891121268, 86.64898084282798, 77.42060662339843, 84.76691266399664, 91.46997485730772, 90.032060953558, 94.76374209990098, 92.59755943192759, 91.06447372778868, 93.25727066329533, 93.96184313449594, 90.2747206840899, 89.82116287224957, 75.66122262615232, 83.06556167784463, 87.56356857053656, 78.83143279096987, 93.53736986126961], 'lossList': [0.0, -1.3427290511131287, 0.0, 475.7140689849854, 0.0, 0.0, 0.0], 'rewardMean': 0.8182674519645511, 'totalEpisodes': 57, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.760242917490439, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8436495349668736, 'errorList': [], 'lossList': [0.0, -1.3402570879459381, 0.0, 165.53067794799804, 0.0, 0.0, 0.0], 'rewardMean': 0.8210876834092535, 'totalEpisodes': 99, 'stepsPerEpisode': 72, 'rewardPerEpisode': 60.06555998837107
'totalSteps': 12800, 'rewardStep': 0.7993058140207469, 'errorList': [], 'lossList': [0.0, -1.3390159898996352, 0.0, 79.16074369430542, 0.0, 0.0, 0.0], 'rewardMean': 0.8189094964704029, 'totalEpisodes': 141, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.583444489174218
'totalSteps': 14080, 'rewardStep': 0.8363576570141338, 'errorList': [], 'lossList': [0.0, -1.3361925101280212, 0.0, 49.017834711074826, 0.0, 0.0, 0.0], 'rewardMean': 0.8110631991137336, 'totalEpisodes': 174, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.5517000099485285
'totalSteps': 15360, 'rewardStep': 0.3926689190488376, 'errorList': [], 'lossList': [0.0, -1.3319077914953232, 0.0, 33.4865402507782, 0.0, 0.0, 0.0], 'rewardMean': 0.7595356281626481, 'totalEpisodes': 196, 'stepsPerEpisode': 73, 'rewardPerEpisode': 45.940723078306114
'totalSteps': 16640, 'rewardStep': 0.6144571333800266, 'errorList': [], 'lossList': [0.0, -1.337915152311325, 0.0, 18.30849036693573, 0.0, 0.0, 0.0], 'rewardMean': 0.7499412322125373, 'totalEpisodes': 207, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.349443899210775
'totalSteps': 17920, 'rewardStep': 0.8890505504007558, 'errorList': [], 'lossList': [0.0, -1.340715201497078, 0.0, 15.655265369415282, 0.0, 0.0, 0.0], 'rewardMean': 0.7660734309902428, 'totalEpisodes': 216, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.84214431099974
'totalSteps': 19200, 'rewardStep': 0.4383220030077488, 'errorList': [], 'lossList': [0.0, -1.3218585735559463, 0.0, 19.45084103345871, 0.0, 0.0, 0.0], 'rewardMean': 0.716721550750764, 'totalEpisodes': 221, 'stepsPerEpisode': 193, 'rewardPerEpisode': 150.8563202158107
'totalSteps': 20480, 'rewardStep': 0.5779327318397346, 'errorList': [], 'lossList': [0.0, -1.3151651799678803, 0.0, 10.519257068634033, 0.0, 0.0, 0.0], 'rewardMean': 0.7113179209863694, 'totalEpisodes': 225, 'stepsPerEpisode': 266, 'rewardPerEpisode': 204.9034217870669
'totalSteps': 21760, 'rewardStep': 0.7038062911879803, 'errorList': [], 'lossList': [0.0, -1.3249760109186173, 0.0, 9.24888302564621, 0.0, 0.0, 0.0], 'rewardMean': 0.7071100972133287, 'totalEpisodes': 228, 'stepsPerEpisode': 318, 'rewardPerEpisode': 248.9519353267678
'totalSteps': 23040, 'rewardStep': 0.4149809229260023, 'errorList': [], 'lossList': [0.0, -1.3201581418514252, 0.0, 6.416511545181274, 0.0, 0.0, 0.0], 'rewardMean': 0.651053155779284, 'totalEpisodes': 230, 'stepsPerEpisode': 788, 'rewardPerEpisode': 598.6241916075292
'totalSteps': 24320, 'rewardStep': 0.837577969384175, 'errorList': [], 'lossList': [0.0, -1.297432342171669, 0.0, 4.433977844119072, 0.0, 0.0, 0.0], 'rewardMean': 0.6504459992210141, 'totalEpisodes': 230, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.22967264286
'totalSteps': 25600, 'rewardStep': 0.8657612028741425, 'errorList': [], 'lossList': [0.0, -1.2664149981737136, 0.0, 2.5296978041529656, 0.0, 0.0, 0.0], 'rewardMean': 0.6570915381063538, 'totalEpisodes': 230, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.3550396068736
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.78
