#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 122
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.867800723925594, 'errorList': [], 'lossList': [0.0, -1.4503482329845427, 0.0, 33.24829265832901, 0.0, 0.0, 0.0], 'rewardMean': 0.6854507859148029, 'totalEpisodes': 12, 'stepsPerEpisode': 904, 'rewardPerEpisode': 665.3829166030083
'totalSteps': 3840, 'rewardStep': 0.8047238870653316, 'errorList': [], 'lossList': [0.0, -1.4693389022350312, 0.0, 47.44745897054672, 0.0, 0.0, 0.0], 'rewardMean': 0.7252084862983125, 'totalEpisodes': 15, 'stepsPerEpisode': 486, 'rewardPerEpisode': 409.00380588603224
'totalSteps': 5120, 'rewardStep': 0.8230263716397692, 'errorList': [], 'lossList': [0.0, -1.4636801266670227, 0.0, 23.945595930814743, 0.0, 0.0, 0.0], 'rewardMean': 0.7496629576336766, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 976.1881321482351
'totalSteps': 6400, 'rewardStep': 0.5802381720370586, 'errorList': [], 'lossList': [0.0, -1.4418052017688752, 0.0, 21.88202044904232, 0.0, 0.0, 0.0], 'rewardMean': 0.715778000514353, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1034.8408365652795
'totalSteps': 7680, 'rewardStep': 0.664842113295698, 'errorList': [], 'lossList': [0.0, -1.419052215218544, 0.0, 11.679588382840157, 0.0, 0.0, 0.0], 'rewardMean': 0.7072886859779106, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 986.7352539943889
'totalSteps': 8960, 'rewardStep': 0.8599429003512529, 'errorList': [], 'lossList': [0.0, -1.4144056922197341, 0.0, 24.55634240269661, 0.0, 0.0, 0.0], 'rewardMean': 0.729096430888388, 'totalEpisodes': 16, 'stepsPerEpisode': 362, 'rewardPerEpisode': 271.104361568083
'totalSteps': 10240, 'rewardStep': 0.7969240501252469, 'errorList': [], 'lossList': [0.0, -1.396967556476593, 0.0, 536.8162594604493, 0.0, 0.0, 0.0], 'rewardMean': 0.7375748832929954, 'totalEpisodes': 58, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.095931124072296
'totalSteps': 11520, 'rewardStep': 0.632742938800335, 'errorList': [], 'lossList': [0.0, -1.3958396190404891, 0.0, 281.15012470245364, 0.0, 0.0, 0.0], 'rewardMean': 0.7259268894604776, 'totalEpisodes': 96, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.0714864727223303
'totalSteps': 12800, 'rewardStep': 0.6485562883365221, 'errorList': [], 'lossList': [0.0, -1.3951485353708266, 0.0, 157.43056282043457, 0.0, 0.0, 0.0], 'rewardMean': 0.718189829348082, 'totalEpisodes': 130, 'stepsPerEpisode': 36, 'rewardPerEpisode': 29.747026677149726
'totalSteps': 14080, 'rewardStep': 0.8394025941962616, 'errorList': [], 'lossList': [0.0, -1.392122745513916, 0.0, 112.56687503814697, 0.0, 0.0, 0.0], 'rewardMean': 0.7518200039773071, 'totalEpisodes': 172, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.785666216576317
'totalSteps': 15360, 'rewardStep': 0.94882640883601, 'errorList': [259.0745970194065, 248.24504552755036, 234.97201287287518, 244.8278007500188, 231.93009234157097, 221.89560103744805, 133.3495782707154, 215.38074255178145, 250.2784741855874, 245.05868474780775, 207.36893078409668, 236.76137279730872, 232.37373471331242, 230.99273657418996, 244.6846761488026, 242.0593982648513, 203.06116525970168, 226.36652180123085, 237.84433365034099, 174.6988357377477, 252.00869417440651, 232.25660363453827, 259.4566244958712, 264.73268185047453, 241.73032561871437, 235.56375546530353, 234.02322750191624, 254.16705734177486, 197.11252389240028, 213.37809244321375, 245.9359043123282, 218.42311552544476, 217.86207134868442, 232.1987210663567, 237.66707553043898, 233.83702738711426, 235.96332158790148, 160.1794797188804, 259.79724645052727, 235.06520266503102, 236.80666652099717, 173.78886459223503, 241.55246001983448, 243.29728908314806, 213.81801969326682, 233.8372394139082, 250.47113149643158, 249.6508450267521, 252.71583061384328, 242.45224745799428], 'lossList': [0.0, -1.3780121928453446, 0.0, 49.36643959999085, 0.0, 0.0, 0.0], 'rewardMean': 0.7599225724683486, 'totalEpisodes': 203, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.492329822930085, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7304413933447037, 'errorList': [], 'lossList': [0.0, -1.3668608087301255, 0.0, 41.22998149394989, 0.0, 0.0, 0.0], 'rewardMean': 0.7524943230962857, 'totalEpisodes': 226, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.03986104093707
'totalSteps': 17920, 'rewardStep': 0.888354798911637, 'errorList': [], 'lossList': [0.0, -1.3588510698080063, 0.0, 34.04866958141327, 0.0, 0.0, 0.0], 'rewardMean': 0.7590271658234725, 'totalEpisodes': 247, 'stepsPerEpisode': 33, 'rewardPerEpisode': 25.385027375500055
'totalSteps': 19200, 'rewardStep': 0.487070859188591, 'errorList': [], 'lossList': [0.0, -1.347640637755394, 0.0, 28.45505009174347, 0.0, 0.0, 0.0], 'rewardMean': 0.7497104345386257, 'totalEpisodes': 262, 'stepsPerEpisode': 57, 'rewardPerEpisode': 39.62331094960815
'totalSteps': 20480, 'rewardStep': 0.9302926408026568, 'errorList': [141.69715589426596, 248.33863270183846, 138.40402561485385, 164.50208337960916, 262.64551417144, 110.10409084479642, 265.1029115116481, 182.1929088907918, 156.2670406618158, 253.15408989868814, 203.33275202871633, 188.18746501768976, 217.48838854307698, 222.8424773555045, 229.8779179920473, 243.2952230736766, 189.30846302612738, 121.91871619996225, 193.57865410024127, 269.00844031872174, 245.12256704548315, 242.0732186387633, 144.6365663858445, 267.33394375175834, 231.25454335545493, 223.75063759059358, 150.33106021031276, 256.13792045441414, 220.19305476135767, 261.94109299913237, 238.65225802754316, 204.664071231947, 208.3758151828515, 146.70089619039547, 214.15817173356936, 214.53040183126012, 250.63379933813877, 225.53639467377707, 222.49808030173978, 167.8348453614435, 240.2908588687407, 216.32528256786577, 237.03459158623343, 248.5865826866749, 239.73362650134138, 123.02692196674649, 237.8523135915757, 22.658405584170126, 216.37972163124576, 255.4429417578909], 'lossList': [0.0, -1.3434965419769287, 0.0, 22.921092882156373, 0.0, 0.0, 0.0], 'rewardMean': 0.7762554872893216, 'totalEpisodes': 273, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.98617753542464, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.616811365558329, 'errorList': [], 'lossList': [0.0, -1.3459348040819168, 0.0, 20.154639492034914, 0.0, 0.0, 0.0], 'rewardMean': 0.7519423338100293, 'totalEpisodes': 281, 'stepsPerEpisode': 86, 'rewardPerEpisode': 69.51352121959022
'totalSteps': 23040, 'rewardStep': 0.5174688118151821, 'errorList': [], 'lossList': [0.0, -1.3596007746458054, 0.0, 8.725446988344192, 0.0, 0.0, 0.0], 'rewardMean': 0.7239968099790228, 'totalEpisodes': 288, 'stepsPerEpisode': 132, 'rewardPerEpisode': 105.51585946238555
'totalSteps': 24320, 'rewardStep': 0.367622601921253, 'errorList': [], 'lossList': [0.0, -1.3806527894735336, 0.0, 7.696960663795471, 0.0, 0.0, 0.0], 'rewardMean': 0.6974847762911147, 'totalEpisodes': 294, 'stepsPerEpisode': 107, 'rewardPerEpisode': 78.05257178697201
'totalSteps': 25600, 'rewardStep': 0.6123747446700871, 'errorList': [], 'lossList': [0.0, -1.3865442597866058, 0.0, 8.636053354740143, 0.0, 0.0, 0.0], 'rewardMean': 0.6938666219244711, 'totalEpisodes': 299, 'stepsPerEpisode': 261, 'rewardPerEpisode': 210.68265543159237
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=102.11
