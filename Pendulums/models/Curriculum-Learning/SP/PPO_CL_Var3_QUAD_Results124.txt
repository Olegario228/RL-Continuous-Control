#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 124
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8243630883251907, 'errorList': [], 'lossList': [0.0, -1.4214487344026565, 0.0, 28.137301906347275, 0.0, 0.0, 0.0], 'rewardMean': 0.7868269763122359, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 773.560054939282
'totalSteps': 3840, 'rewardStep': 0.7588478041738064, 'errorList': [], 'lossList': [0.0, -1.435687341094017, 0.0, 34.032317683696746, 0.0, 0.0, 0.0], 'rewardMean': 0.777500585599426, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 124.07923240532392
'totalSteps': 5120, 'rewardStep': 0.6626586148307168, 'errorList': [], 'lossList': [0.0, -1.4315830194950103, 0.0, 23.435836504101754, 0.0, 0.0, 0.0], 'rewardMean': 0.7487900929072486, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.4626254692578
'totalSteps': 6400, 'rewardStep': 0.8995760998514782, 'errorList': [], 'lossList': [0.0, -1.4210929721593857, 0.0, 17.36631903409958, 0.0, 0.0, 0.0], 'rewardMean': 0.7789472942960946, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 986.589203887033
'totalSteps': 7680, 'rewardStep': 0.8885357201854587, 'errorList': [], 'lossList': [0.0, -1.417688073515892, 0.0, 10.98262708902359, 0.0, 0.0, 0.0], 'rewardMean': 0.7972120319443219, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 956.5475411138925
'totalSteps': 8960, 'rewardStep': 0.589603395827816, 'errorList': [], 'lossList': [0.0, -1.4057530999183654, 0.0, 159.30251930236815, 0.0, 0.0, 0.0], 'rewardMean': 0.7675536553562496, 'totalEpisodes': 26, 'stepsPerEpisode': 43, 'rewardPerEpisode': 31.81054911794044
'totalSteps': 10240, 'rewardStep': 0.5947330230703981, 'errorList': [], 'lossList': [0.0, -1.4050319331884384, 0.0, 316.7273021697998, 0.0, 0.0, 0.0], 'rewardMean': 0.7459510763205182, 'totalEpisodes': 83, 'stepsPerEpisode': 22, 'rewardPerEpisode': 13.510469451637547
'totalSteps': 11520, 'rewardStep': 0.9053522538885428, 'errorList': [], 'lossList': [0.0, -1.4013994818925857, 0.0, 114.62906478881835, 0.0, 0.0, 0.0], 'rewardMean': 0.7636623182725208, 'totalEpisodes': 134, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.968834611688127
'totalSteps': 12800, 'rewardStep': 0.6205830154732521, 'errorList': [], 'lossList': [0.0, -1.397960923910141, 0.0, 73.30276809692383, 0.0, 0.0, 0.0], 'rewardMean': 0.749354387992594, 'totalEpisodes': 173, 'stepsPerEpisode': 54, 'rewardPerEpisode': 46.51911080645629
'totalSteps': 14080, 'rewardStep': 0.7107292136856398, 'errorList': [], 'lossList': [0.0, -1.3974815046787261, 0.0, 50.71446425437927, 0.0, 0.0, 0.0], 'rewardMean': 0.7454982229312299, 'totalEpisodes': 192, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.8811311797624946
'totalSteps': 15360, 'rewardStep': 0.8691665544753387, 'errorList': [], 'lossList': [0.0, -1.3946936643123626, 0.0, 45.730979285240174, 0.0, 0.0, 0.0], 'rewardMean': 0.7499785695462446, 'totalEpisodes': 204, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.31383675557444
'totalSteps': 16640, 'rewardStep': 0.4339107835113559, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6946100843480637, 'totalEpisodes': 215, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.381740272289552
'totalSteps': 17920, 'rewardStep': 0.6690021575997516, 'errorList': [], 'lossList': [0.0, -1.3893209737539292, 0.0, 19.59050064086914, 0.0, 0.0, 0.0], 'rewardMean': 0.671552690122891, 'totalEpisodes': 223, 'stepsPerEpisode': 46, 'rewardPerEpisode': 37.172092054918565
'totalSteps': 19200, 'rewardStep': 0.742953166378675, 'errorList': [], 'lossList': [0.0, -1.385685510635376, 0.0, 32.66908356666565, 0.0, 0.0, 0.0], 'rewardMean': 0.6569944347422126, 'totalEpisodes': 231, 'stepsPerEpisode': 108, 'rewardPerEpisode': 85.60264121038725
'totalSteps': 20480, 'rewardStep': 0.9351082740657899, 'errorList': [1.1856092884394054, 0.6043118897718143, 1.4843730197020957, 0.5728930595201628, 1.1055250378343568, 7.23643284052135, 2.1396193831706873, 2.5370680957675606, 1.967664031463371, 6.256447150566972, 3.3278430154316316, 0.07330868077213126, 1.5235069787860385, 11.210225665554761, 3.4384621353145217, 5.1651269560392095, 7.396315673118415, 8.528950979709142, 3.6416048613971648, 4.708967273406607, 2.6624414717880107, 14.726810062524544, 11.577140249224257, 5.416359887189983, 4.664571931901425, 0.07057935803925305, 10.305826871788394, 1.25300363742166, 1.491244355359353, 17.035850725040678, 2.9863642171386164, 0.6402265928265335, 8.156342599721858, 14.136882265369609, 0.11256327275271501, 0.9237310825204246, 6.444587709008055, 1.6668581458346856, 2.148429010153599, 10.226819851013637, 4.526509234459567, 0.41453065987335513, 14.873316007431646, 7.837884651784891, 5.649847519317556, 2.448545857759594, 3.7708083752189845, 6.755616684116284, 0.3115011451979659, 2.375974578242242], 'lossList': [0.0, -1.3681349951028823, 0.0, 21.226333689689636, 0.0, 0.0, 0.0], 'rewardMean': 0.69154492256601, 'totalEpisodes': 237, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.75977375687569, 'successfulTests': 3
'totalSteps': 21760, 'rewardStep': 0.6326538560199542, 'errorList': [], 'lossList': [0.0, -1.351045538187027, 0.0, 8.150962718725204, 0.0, 0.0, 0.0], 'rewardMean': 0.6953370058609656, 'totalEpisodes': 240, 'stepsPerEpisode': 76, 'rewardPerEpisode': 60.889262990228545
'totalSteps': 23040, 'rewardStep': 0.8611865715370542, 'errorList': [], 'lossList': [0.0, -1.3475806123018266, 0.0, 33.49996560573578, 0.0, 0.0, 0.0], 'rewardMean': 0.6909204376258168, 'totalEpisodes': 244, 'stepsPerEpisode': 63, 'rewardPerEpisode': 55.506416068076874
'totalSteps': 24320, 'rewardStep': 0.7133985166849665, 'errorList': [], 'lossList': [0.0, -1.3469407695531845, 0.0, 5.884937131404877, 0.0, 0.0, 0.0], 'rewardMean': 0.7002019877469883, 'totalEpisodes': 246, 'stepsPerEpisode': 133, 'rewardPerEpisode': 105.88181466801376
'totalSteps': 25600, 'rewardStep': 0.8079988954548855, 'errorList': [], 'lossList': [0.0, -1.3233215641975402, 0.0, 3.1815870116651057, 0.0, 0.0, 0.0], 'rewardMean': 0.7099289559239128, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.0882453144527
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=20480, timeSpent=80.65
