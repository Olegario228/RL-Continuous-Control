#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 125
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7364060124991547, 'errorList': [], 'lossList': [0.0, -1.413213089108467, 0.0, 32.52741421699524, 0.0, 0.0, 0.0], 'rewardMean': 0.5903143767373362, 'totalEpisodes': 56, 'stepsPerEpisode': 22, 'rewardPerEpisode': 13.422156146000228
'totalSteps': 3840, 'rewardStep': 0.15663513273506863, 'errorList': [], 'lossList': [0.0, -1.4036792558431626, 0.0, 38.7712001991272, 0.0, 0.0, 0.0], 'rewardMean': 0.44575462873658034, 'totalEpisodes': 73, 'stepsPerEpisode': 92, 'rewardPerEpisode': 64.57683318048849
'totalSteps': 5120, 'rewardStep': 0.6524737755120187, 'errorList': [], 'lossList': [0.0, -1.3911950451135635, 0.0, 31.57042675256729, 0.0, 0.0, 0.0], 'rewardMean': 0.4974344154304399, 'totalEpisodes': 80, 'stepsPerEpisode': 192, 'rewardPerEpisode': 145.41958084690694
'totalSteps': 6400, 'rewardStep': 0.5971825063109654, 'errorList': [], 'lossList': [0.0, -1.3866122138500214, 0.0, 29.28097244977951, 0.0, 0.0, 0.0], 'rewardMean': 0.5173840336065451, 'totalEpisodes': 87, 'stepsPerEpisode': 421, 'rewardPerEpisode': 306.56345213567135
'totalSteps': 7680, 'rewardStep': 0.8596871027576434, 'errorList': [], 'lossList': [0.0, -1.3804596227407455, 0.0, 42.943434188365934, 0.0, 0.0, 0.0], 'rewardMean': 0.5744345451317281, 'totalEpisodes': 91, 'stepsPerEpisode': 311, 'rewardPerEpisode': 217.34685422886398
'totalSteps': 8960, 'rewardStep': 0.5803871177035576, 'errorList': [], 'lossList': [0.0, -1.3604708617925645, 0.0, 29.288699131011963, 0.0, 0.0, 0.0], 'rewardMean': 0.5752849126419894, 'totalEpisodes': 96, 'stepsPerEpisode': 270, 'rewardPerEpisode': 210.25145246922068
'totalSteps': 10240, 'rewardStep': 0.6606925828499577, 'errorList': [], 'lossList': [0.0, -1.3680255711078644, 0.0, 107.59392150878907, 0.0, 0.0, 0.0], 'rewardMean': 0.5859608714179855, 'totalEpisodes': 108, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.380837893471568
'totalSteps': 11520, 'rewardStep': 0.6116960914625764, 'errorList': [], 'lossList': [0.0, -1.3610973817110061, 0.0, 127.52084266662598, 0.0, 0.0, 0.0], 'rewardMean': 0.588820340311829, 'totalEpisodes': 132, 'stepsPerEpisode': 32, 'rewardPerEpisode': 20.507633570436592
'totalSteps': 12800, 'rewardStep': 0.12908434921516265, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5113329020261267, 'totalEpisodes': 144, 'stepsPerEpisode': 121, 'rewardPerEpisode': 87.28219692688921
'totalSteps': 14080, 'rewardStep': 0.5127419342264414, 'errorList': [], 'lossList': [0.0, -1.34691286444664, 0.0, 30.51766167163849, 0.0, 0.0, 0.0], 'rewardMean': 0.48896649419885535, 'totalEpisodes': 158, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.3155063321791616
'totalSteps': 15360, 'rewardStep': 0.48155150130969004, 'errorList': [], 'lossList': [0.0, -1.3278251206874847, 0.0, 37.21811806678772, 0.0, 0.0, 0.0], 'rewardMean': 0.5214581310563176, 'totalEpisodes': 169, 'stepsPerEpisode': 69, 'rewardPerEpisode': 53.221134853054316
'totalSteps': 16640, 'rewardStep': 0.8009633283428881, 'errorList': [], 'lossList': [0.0, -1.3136100721359254, 0.0, 13.404594633579254, 0.0, 0.0, 0.0], 'rewardMean': 0.5363070863394046, 'totalEpisodes': 175, 'stepsPerEpisode': 182, 'rewardPerEpisode': 159.18891407021263
'totalSteps': 17920, 'rewardStep': 0.8918048560511597, 'errorList': [], 'lossList': [0.0, -1.308849561214447, 0.0, 35.65461403846741, 0.0, 0.0, 0.0], 'rewardMean': 0.565769321313424, 'totalEpisodes': 178, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.244965905645515
'totalSteps': 19200, 'rewardStep': 0.8777664109418899, 'errorList': [], 'lossList': [0.0, -1.2990463882684709, 0.0, 21.46747578859329, 0.0, 0.0, 0.0], 'rewardMean': 0.5675772521318485, 'totalEpisodes': 180, 'stepsPerEpisode': 823, 'rewardPerEpisode': 660.7128033278201
'totalSteps': 20480, 'rewardStep': 0.9038922814782403, 'errorList': [], 'lossList': [0.0, -1.280406339764595, 0.0, 4.4712146177887915, 0.0, 0.0, 0.0], 'rewardMean': 0.599927768509317, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1041.055666828962
'totalSteps': 21760, 'rewardStep': 0.9018570770471243, 'errorList': [], 'lossList': [0.0, -1.2622262966632842, 0.0, 3.4382561679184436, 0.0, 0.0, 0.0], 'rewardMean': 0.6240442179290335, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.3733773224762
'totalSteps': 23040, 'rewardStep': 0.7958604107151732, 'errorList': [], 'lossList': [0.0, -1.23442979991436, 0.0, 3.2875140362977984, 0.0, 0.0, 0.0], 'rewardMean': 0.6424606498542932, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.5310765252073
'totalSteps': 24320, 'rewardStep': 0.8176319367615112, 'errorList': [], 'lossList': [0.0, -1.199821624159813, 0.0, 2.7042437171936036, 0.0, 0.0, 0.0], 'rewardMean': 0.7113154086089282, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.1247956813872
'totalSteps': 25600, 'rewardStep': 0.9483160437556868, 'errorList': [0.04882182969156729, 0.043726009404119186, 0.043205976377414385, 0.030034976821946963, 0.03576630121465421, 0.05582177436622233, 0.07419828730157196, 0.02526709183336642, 0.044969746737315984, 0.04055619553891685, 0.043249400817226086, 0.03112990609169845, 0.03909627103300809, 0.02811114417427429, 0.039106421218951436, 0.04006893614974314, 0.040004784829502794, 0.03283157752506213, 0.043664902895269156, 0.0524191373095042, 0.029095905833507397, 0.030708319282443435, 0.049700737570851035, 0.04884296765279116, 0.030486404933831267, 0.02802815312787543, 0.07030146880205529, 0.0338729211649298, 0.03378528638906336, 0.05465321620530441, 0.05248306556301217, 0.03175999744811745, 0.03288766184422683, 0.040090066643104764, 0.060534400936741395, 0.04480278923558521, 0.046172487089718064, 0.028728246107766753, 0.051193338375450974, 0.06203423191911239, 0.04106183454172999, 0.02843139343428238, 0.06699954853978657, 0.066087328946804, 0.05285875223512945, 0.03910433189553045, 0.03578076573157479, 0.031132503990494026, 0.06463509364162266, 0.04273238358647543], 'lossList': [0.0, -1.1739396500587462, 0.0, 1.919560113698244, 0.0, 0.0, 0.0], 'rewardMean': 0.7932385780629806, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1194.5898663508146, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=79.91
