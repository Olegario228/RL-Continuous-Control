#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 126
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5972493745241301, 'errorList': [], 'lossList': [0.0, -1.4164019596576691, 0.0, 30.292427682876585, 0.0, 0.0, 0.0], 'rewardMean': 0.7061497536827688, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.31051132640419
'totalSteps': 3840, 'rewardStep': 0.7186219917258226, 'errorList': [], 'lossList': [0.0, -1.4014815646409988, 0.0, 36.99622437000275, 0.0, 0.0, 0.0], 'rewardMean': 0.7103071663637867, 'totalEpisodes': 66, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.227038351723973
'totalSteps': 5120, 'rewardStep': 0.8164676352500215, 'errorList': [], 'lossList': [0.0, -1.394279931783676, 0.0, 31.361532034873964, 0.0, 0.0, 0.0], 'rewardMean': 0.7368472835853455, 'totalEpisodes': 75, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.042116718273221
'totalSteps': 6400, 'rewardStep': 0.44259386687431174, 'errorList': [], 'lossList': [0.0, -1.384840789437294, 0.0, 27.08897104740143, 0.0, 0.0, 0.0], 'rewardMean': 0.6779966002431388, 'totalEpisodes': 81, 'stepsPerEpisode': 82, 'rewardPerEpisode': 63.7746343940949
'totalSteps': 7680, 'rewardStep': 0.5550044748528977, 'errorList': [], 'lossList': [0.0, -1.3687539237737656, 0.0, 16.362085542678834, 0.0, 0.0, 0.0], 'rewardMean': 0.6574979126780985, 'totalEpisodes': 83, 'stepsPerEpisode': 357, 'rewardPerEpisode': 265.3978287381039
'totalSteps': 8960, 'rewardStep': 0.3202755721942498, 'errorList': [], 'lossList': [0.0, -1.3477141493558884, 0.0, 46.62044508457184, 0.0, 0.0, 0.0], 'rewardMean': 0.6093232926089772, 'totalEpisodes': 88, 'stepsPerEpisode': 445, 'rewardPerEpisode': 320.08115151102754
'totalSteps': 10240, 'rewardStep': 0.7827146228238395, 'errorList': [], 'lossList': [0.0, -1.3383865851163863, 0.0, 28.338630386590957, 0.0, 0.0, 0.0], 'rewardMean': 0.6309972088858351, 'totalEpisodes': 92, 'stepsPerEpisode': 157, 'rewardPerEpisode': 126.01470681162327
'totalSteps': 11520, 'rewardStep': 0.804790160078892, 'errorList': [], 'lossList': [0.0, -1.3282784199714661, 0.0, 152.47599475860596, 0.0, 0.0, 0.0], 'rewardMean': 0.6503075367961748, 'totalEpisodes': 112, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.782561523589653
'totalSteps': 12800, 'rewardStep': 0.8782887477113114, 'errorList': [], 'lossList': [0.0, -1.321811599135399, 0.0, 68.31385623931885, 0.0, 0.0, 0.0], 'rewardMean': 0.6731056578876884, 'totalEpisodes': 127, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8782887477113114
'totalSteps': 14080, 'rewardStep': 0.41369301715830087, 'errorList': [], 'lossList': [0.0, -1.3192069506645203, 0.0, 13.255754482746124, 0.0, 0.0, 0.0], 'rewardMean': 0.6329699463193778, 'totalEpisodes': 132, 'stepsPerEpisode': 169, 'rewardPerEpisode': 117.25665644730596
'totalSteps': 15360, 'rewardStep': 0.5680605549488608, 'errorList': [], 'lossList': [0.0, -1.321460480093956, 0.0, 15.07677788734436, 0.0, 0.0, 0.0], 'rewardMean': 0.6300510643618509, 'totalEpisodes': 138, 'stepsPerEpisode': 76, 'rewardPerEpisode': 48.95689703076247
'totalSteps': 16640, 'rewardStep': 0.6808666571476202, 'errorList': [], 'lossList': [0.0, -1.305481081008911, 0.0, 20.90359281539917, 0.0, 0.0, 0.0], 'rewardMean': 0.6262755309040305, 'totalEpisodes': 141, 'stepsPerEpisode': 117, 'rewardPerEpisode': 90.85190816807695
'totalSteps': 17920, 'rewardStep': 0.8330855115894581, 'errorList': [], 'lossList': [0.0, -1.25988745033741, 0.0, 4.495728623867035, 0.0, 0.0, 0.0], 'rewardMean': 0.6279373185379742, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1031.9996880600693
'totalSteps': 19200, 'rewardStep': 0.8196286682454946, 'errorList': [], 'lossList': [0.0, -1.2191995334625245, 0.0, 4.350542283058166, 0.0, 0.0, 0.0], 'rewardMean': 0.6656407986750925, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1079.288086441207
'totalSteps': 20480, 'rewardStep': 0.7494634096053353, 'errorList': [], 'lossList': [0.0, -1.1708802902698516, 0.0, 2.6970286275446416, 0.0, 0.0, 0.0], 'rewardMean': 0.6850866921503362, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.9444322776528
'totalSteps': 21760, 'rewardStep': 0.8091587302337248, 'errorList': [], 'lossList': [0.0, -1.0950689041614532, 0.0, 1.592710290774703, 0.0, 0.0, 0.0], 'rewardMean': 0.7339750079542837, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.5863201159727
'totalSteps': 23040, 'rewardStep': 0.9598816371369889, 'errorList': [0.13391386815464765, 0.1499614348562703, 0.1665106158652459, 0.1330268702705665, 0.14901454289598207, 0.16316698742371213, 0.13053596173135346, 0.13828055226088512, 0.1373382441685858, 0.1308750726084715, 0.1412337276574891, 0.13607377529300999, 0.14476678763769366, 0.13000548033096915, 0.1284169152766364, 0.13214899964856647, 0.1453249535020478, 0.1429019595259314, 0.20499088585905026, 0.1342406017879517, 0.12507119398293226, 0.14925073666186317, 0.13392152621358577, 0.18762019021103035, 0.1312003917805725, 0.14637596374261097, 0.1372093458842547, 0.1399076570931859, 0.1304760995760451, 0.13154824072794713, 0.1476916859185796, 0.16625454617666263, 0.20919906756521242, 0.1295843598930868, 0.191179620298214, 0.12500389499293693, 0.1385978695958317, 0.12905894630691161, 0.1414999311549674, 0.14484574080291224, 0.1398666799693953, 0.13733347387638625, 0.1401353670458082, 0.1329895653515617, 0.14100143427800008, 0.1470030586710679, 0.14571872592002338, 0.131995057218548, 0.1499738074546013, 0.1426203944982901], 'lossList': [0.0, -1.0528582113981246, 0.0, 1.7689493223652244, 0.0, 0.0, 0.0], 'rewardMean': 0.7516917093855987, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.7729316720595, 'successfulTests': 48
'totalSteps': 24320, 'rewardStep': 0.9034024223696759, 'errorList': [], 'lossList': [0.0, -1.016512365937233, 0.0, 0.6781015606969595, 0.0, 0.0, 0.0], 'rewardMean': 0.7615529356146771, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.2697534112965
'totalSteps': 25600, 'rewardStep': 0.990039708946787, 'errorList': [0.028138119157500828, 0.10013123797274538, 0.14701158927216634, 0.2345815456837609, 0.08193327648331405, 0.31282124664570016, 0.09896346060750004, 0.08719934095432022, 0.08816871958691246, 0.0954356982248023, 0.07094721213762549, 0.11975811194200979, 0.23396949783082138, 0.2004990845201138, 0.020523248853632024, 0.04318537608127953, 0.13672674457398518, 0.23204636494175504, 0.4119809057439499, 0.18581214186393025, 0.16558943149508262, 0.17249195110610824, 0.06743128602724616, 0.20062453122255763, 0.1729312315836409, 0.042028637702171696, 0.1763859477602294, 0.11387175230740389, 0.24157101903790001, 0.10835449366657411, 0.09193903822017664, 0.033831479965428374, 0.094450371022969, 0.08479412649851709, 0.03954841034374667, 0.04843953632849772, 0.14494135604297465, 0.032926732695610736, 0.12912502577140023, 0.01922615816346453, 0.18322076629105125, 0.10130697756730586, 0.17279731435601958, 0.1210111333571741, 0.057816913430784496, 0.07646034018461538, 0.14953253851476217, 0.14045269638919408, 0.17383388881113032, 0.05972842824330897], 'lossList': [0.0, -0.9806237390637398, 0.0, 1.056695450283587, 0.0, 0.0, 0.0], 'rewardMean': 0.7727280317382246, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1223.2266733844015, 'successfulTests': 42
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=23040, timeSpent=103.11
