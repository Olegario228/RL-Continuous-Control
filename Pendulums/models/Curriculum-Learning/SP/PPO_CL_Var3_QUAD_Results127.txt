#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 127
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.8488878465474039, 'errorList': [], 'lossList': [0.0, -1.4196446764469146, 0.0, 35.431823053359984, 0.0, 0.0, 0.0], 'rewardMean': 0.8322625941655567, 'totalEpisodes': 68, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.120485558636612
'totalSteps': 3840, 'rewardStep': 0.5266156090764045, 'errorList': [], 'lossList': [0.0, -1.4046370273828506, 0.0, 31.471248168945312, 0.0, 0.0, 0.0], 'rewardMean': 0.7303802658025059, 'totalEpisodes': 84, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.649716849027264
'totalSteps': 5120, 'rewardStep': 0.1510030926148085, 'errorList': [], 'lossList': [0.0, -1.3888691860437392, 0.0, 18.558848052024842, 0.0, 0.0, 0.0], 'rewardMean': 0.5855359725055815, 'totalEpisodes': 93, 'stepsPerEpisode': 190, 'rewardPerEpisode': 126.18640550038349
'totalSteps': 6400, 'rewardStep': 0.6668025341361887, 'errorList': [], 'lossList': [0.0, -1.384702115058899, 0.0, 32.45684299468994, 0.0, 0.0, 0.0], 'rewardMean': 0.601789284831703, 'totalEpisodes': 103, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2831929459006142
'totalSteps': 7680, 'rewardStep': 0.8119983014901638, 'errorList': [], 'lossList': [0.0, -1.3649760448932649, 0.0, 51.1315950012207, 0.0, 0.0, 0.0], 'rewardMean': 0.6368241209414465, 'totalEpisodes': 109, 'stepsPerEpisode': 283, 'rewardPerEpisode': 224.99399854654703
'totalSteps': 8960, 'rewardStep': 0.5611585476468733, 'errorList': [], 'lossList': [0.0, -1.3538296157121659, 0.0, 15.482407470941544, 0.0, 0.0, 0.0], 'rewardMean': 0.626014753327936, 'totalEpisodes': 109, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1014.2163194568017
'totalSteps': 10240, 'rewardStep': 0.7189129945856888, 'errorList': [], 'lossList': [0.0, -1.332374596595764, 0.0, 43.370078892707824, 0.0, 0.0, 0.0], 'rewardMean': 0.6376270334851551, 'totalEpisodes': 112, 'stepsPerEpisode': 425, 'rewardPerEpisode': 346.5574255923258
'totalSteps': 11520, 'rewardStep': 0.8492338595139253, 'errorList': [], 'lossList': [0.0, -1.3350908547639846, 0.0, 55.05096793651581, 0.0, 0.0, 0.0], 'rewardMean': 0.6611389030439074, 'totalEpisodes': 118, 'stepsPerEpisode': 90, 'rewardPerEpisode': 75.45537927187493
'totalSteps': 12800, 'rewardStep': 0.2279430466311676, 'errorList': [], 'lossList': [0.0, -1.3370002698898316, 0.0, 65.47301003456116, 0.0, 0.0, 0.0], 'rewardMean': 0.6178193174026335, 'totalEpisodes': 125, 'stepsPerEpisode': 142, 'rewardPerEpisode': 91.08344171776524
'totalSteps': 14080, 'rewardStep': 0.7907851326208638, 'errorList': [], 'lossList': [0.0, -1.3332279175519943, 0.0, 12.369567606449127, 0.0, 0.0, 0.0], 'rewardMean': 0.615334096486349, 'totalEpisodes': 133, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.189929993318156
'totalSteps': 15360, 'rewardStep': 0.7189649148212199, 'errorList': [], 'lossList': [0.0, -1.3260256481170654, 0.0, 26.85345328807831, 0.0, 0.0, 0.0], 'rewardMean': 0.6023418033137306, 'totalEpisodes': 141, 'stepsPerEpisode': 100, 'rewardPerEpisode': 79.19539140506625
'totalSteps': 16640, 'rewardStep': 0.5895758549169703, 'errorList': [], 'lossList': [0.0, -1.3142600244283675, 0.0, 6.50267413854599, 0.0, 0.0, 0.0], 'rewardMean': 0.608637827897787, 'totalEpisodes': 147, 'stepsPerEpisode': 150, 'rewardPerEpisode': 123.68809318303457
'totalSteps': 17920, 'rewardStep': 0.42273259861545276, 'errorList': [], 'lossList': [0.0, -1.2989035975933074, 0.0, 6.408663016557694, 0.0, 0.0, 0.0], 'rewardMean': 0.6358107784978515, 'totalEpisodes': 154, 'stepsPerEpisode': 142, 'rewardPerEpisode': 114.20784849824867
'totalSteps': 19200, 'rewardStep': 0.46126414793015336, 'errorList': [], 'lossList': [0.0, -1.2960868269205092, 0.0, 5.272926588058471, 0.0, 0.0, 0.0], 'rewardMean': 0.6152569398772479, 'totalEpisodes': 161, 'stepsPerEpisode': 122, 'rewardPerEpisode': 93.28561054646931
'totalSteps': 20480, 'rewardStep': 0.3729657585421842, 'errorList': [], 'lossList': [0.0, -1.3057820665836335, 0.0, 2.448110682964325, 0.0, 0.0, 0.0], 'rewardMean': 0.5713536855824499, 'totalEpisodes': 166, 'stepsPerEpisode': 181, 'rewardPerEpisode': 137.5426440054048
'totalSteps': 21760, 'rewardStep': 0.6482695623767801, 'errorList': [], 'lossList': [0.0, -1.30722210586071, 0.0, 2.7688739782571794, 0.0, 0.0, 0.0], 'rewardMean': 0.5800647870554406, 'totalEpisodes': 170, 'stepsPerEpisode': 244, 'rewardPerEpisode': 203.1211978051258
'totalSteps': 23040, 'rewardStep': 0.9153860637358606, 'errorList': [], 'lossList': [0.0, -1.3054559761285782, 0.0, 2.0762030360102655, 0.0, 0.0, 0.0], 'rewardMean': 0.5997120939704578, 'totalEpisodes': 173, 'stepsPerEpisode': 353, 'rewardPerEpisode': 305.4412691355014
'totalSteps': 24320, 'rewardStep': 0.9843199964985275, 'errorList': [0.1905070155250836, 0.21522132344306844, 0.18720001026244956, 0.19971129529705836, 0.19367649795990513, 0.1640758882206979, 0.17280518211128482, 0.1889947296957003, 0.18840318249191665, 0.2512909061531553, 0.23026068229770894, 0.22627658029968692, 0.18063025131793095, 0.18848487915207807, 0.23448599720592117, 0.1926470890804932, 0.3712417507944498, 0.17629223994317753, 0.1964892081211291, 0.20407831923947817, 0.30220310004035256, 0.18944473342614848, 0.16528140609167175, 0.18568278460011975, 0.1853784455019874, 0.2025598727831591, 0.19873225862711427, 0.20373584472932946, 0.20588483833656, 0.23126293424663855, 0.2667822570544052, 0.19292806279477587, 0.22767112985027987, 0.22647300521285674, 0.18292595241893778, 0.18820905978704294, 0.18723948403642515, 0.17171570447374257, 0.186306505299838, 0.30296121874268434, 0.20663082605165206, 0.18108590969938054, 0.19024854147845513, 0.17997774012801915, 0.19130460233727306, 0.19197690687267005, 0.20306970198586546, 0.2767381546787625, 0.16712284573427977, 0.19159326949451336], 'lossList': [0.0, -1.3021123349666595, 0.0, 2.2468988290429115, 0.0, 0.0, 0.0], 'rewardMean': 0.613220707668918, 'totalEpisodes': 175, 'stepsPerEpisode': 122, 'rewardPerEpisode': 111.53799432782351, 'successfulTests': 31
'totalSteps': 25600, 'rewardStep': 0.8681513036594727, 'errorList': [], 'lossList': [0.0, -1.3149372732639313, 0.0, 3.7232931458950045, 0.0, 0.0, 0.0], 'rewardMean': 0.6772415333717485, 'totalEpisodes': 176, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.13977506130822
#maxSuccessfulTests=31, maxSuccessfulTestsAtStep=24320, timeSpent=82.12
