#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 128
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.5442940404511202, 'errorList': [], 'lossList': [0.0, -1.4377784490585328, 0.0, 32.257827382087704, 0.0, 0.0, 0.0], 'rewardMean': 0.5246166335957525, 'totalEpisodes': 57, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.94539583074498
'totalSteps': 3840, 'rewardStep': 0.929434558503881, 'errorList': [], 'lossList': [0.0, -1.4341219907999039, 0.0, 39.13741998672485, 0.0, 0.0, 0.0], 'rewardMean': 0.659555941898462, 'totalEpisodes': 71, 'stepsPerEpisode': 48, 'rewardPerEpisode': 37.98541899719437
'totalSteps': 5120, 'rewardStep': 0.8829083684374782, 'errorList': [], 'lossList': [0.0, -1.4287728852033614, 0.0, 34.481549489498136, 0.0, 0.0, 0.0], 'rewardMean': 0.7153940485332161, 'totalEpisodes': 79, 'stepsPerEpisode': 71, 'rewardPerEpisode': 62.58502259165104
'totalSteps': 6400, 'rewardStep': 0.5491785777015601, 'errorList': [], 'lossList': [0.0, -1.4265405678749083, 0.0, 21.361196653842924, 0.0, 0.0, 0.0], 'rewardMean': 0.6821509543668849, 'totalEpisodes': 85, 'stepsPerEpisode': 158, 'rewardPerEpisode': 105.97158373741559
'totalSteps': 7680, 'rewardStep': 0.8043382810064835, 'errorList': [], 'lossList': [0.0, -1.4258746606111528, 0.0, 41.07077021121979, 0.0, 0.0, 0.0], 'rewardMean': 0.702515508806818, 'totalEpisodes': 94, 'stepsPerEpisode': 85, 'rewardPerEpisode': 64.33978586487162
'totalSteps': 8960, 'rewardStep': 0.5958862569616887, 'errorList': [], 'lossList': [0.0, -1.420702667236328, 0.0, 24.745428259372712, 0.0, 0.0, 0.0], 'rewardMean': 0.6872827585432281, 'totalEpisodes': 100, 'stepsPerEpisode': 146, 'rewardPerEpisode': 94.49278277100062
'totalSteps': 10240, 'rewardStep': 0.6155668451290977, 'errorList': [], 'lossList': [0.0, -1.4135445195436478, 0.0, 44.67106064319611, 0.0, 0.0, 0.0], 'rewardMean': 0.6783182693664618, 'totalEpisodes': 107, 'stepsPerEpisode': 281, 'rewardPerEpisode': 219.9869291599763
'totalSteps': 11520, 'rewardStep': 0.6901397075605631, 'errorList': [], 'lossList': [0.0, -1.4118181091547013, 0.0, 61.67829319000244, 0.0, 0.0, 0.0], 'rewardMean': 0.6796317624991397, 'totalEpisodes': 123, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.416739735905472
'totalSteps': 12800, 'rewardStep': 0.5607944191202805, 'errorList': [], 'lossList': [0.0, -1.4215267676115035, 0.0, 21.40498939037323, 0.0, 0.0, 0.0], 'rewardMean': 0.6677480281612538, 'totalEpisodes': 133, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.147302666189402
'totalSteps': 14080, 'rewardStep': 0.7128999087880024, 'errorList': [], 'lossList': [0.0, -1.3988099378347396, 0.0, 11.78008842945099, 0.0, 0.0, 0.0], 'rewardMean': 0.6885440963660155, 'totalEpisodes': 140, 'stepsPerEpisode': 103, 'rewardPerEpisode': 72.77692283919116
'totalSteps': 15360, 'rewardStep': 0.41789855479777094, 'errorList': [], 'lossList': [0.0, -1.3783594703674316, 0.0, 10.697808780670165, 0.0, 0.0, 0.0], 'rewardMean': 0.6759045478006807, 'totalEpisodes': 143, 'stepsPerEpisode': 546, 'rewardPerEpisode': 420.02539723876066
'totalSteps': 16640, 'rewardStep': 0.9234432405478418, 'errorList': [], 'lossList': [0.0, -1.3694384068250656, 0.0, 6.591060625314713, 0.0, 0.0, 0.0], 'rewardMean': 0.6753054160050768, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 917.6929479530361
'totalSteps': 17920, 'rewardStep': 0.8409691453640139, 'errorList': [], 'lossList': [0.0, -1.3177778393030166, 0.0, 4.726501307487488, 0.0, 0.0, 0.0], 'rewardMean': 0.6711114936977303, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.6058414079769
'totalSteps': 19200, 'rewardStep': 0.8861864753702252, 'errorList': [], 'lossList': [0.0, -1.2706958502531052, 0.0, 2.875770125091076, 0.0, 0.0, 0.0], 'rewardMean': 0.7048122834645968, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.2733731851165
'totalSteps': 20480, 'rewardStep': 0.9636390693684769, 'errorList': [0.012294349474848162, 0.009462691096405986, 0.006813002615324009, 0.008453176150298452, 0.0028255138628451707, 0.00515332236268533, 0.013433616872446472, 0.016199673562801033, 0.017787032842598943, 0.004480699934735333, 0.01561383186881461, 0.016724562693820763, 0.01630168535496217, 0.012654561345592593, 0.006810573364974763, 0.005379645562627563, 0.01753678643605966, 0.005490361343113044, 0.00273576190125062, 0.015684593323289604, 0.0024607817815706827, 0.011412718609023428, 0.015412417874981457, 0.005020467734479329, 0.02414011114071704, 0.010330025302545327, 0.007231256031999412, 0.015395127991716212, 0.01035257580581279, 0.0063981577603524145, 0.005102434058799453, 0.010471161212892661, 0.01016258214911398, 0.007554884137415611, 0.010835271378842024, 0.004247295318545433, 0.005263148268437947, 0.005252579019256551, 0.019746380953289747, 0.0029250269696610683, 0.006718869981459019, 0.010369264588305633, 0.0010196747121667359, 0.007991050165306811, 0.002450840248424136, 0.019890360077061818, 0.010313349183619222, 0.021477628445919553, 0.001438071573672027, 0.006726457341365897], 'lossList': [0.0, -1.2371380758285522, 0.0, 2.6162682585418224, 0.0, 0.0, 0.0], 'rewardMean': 0.7207423623007961, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.8717360228036, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=68.6
