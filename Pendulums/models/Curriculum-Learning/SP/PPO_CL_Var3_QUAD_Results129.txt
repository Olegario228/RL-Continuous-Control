#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 129
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7751780416446926, 'errorList': [], 'lossList': [0.0, -1.4121422708034514, 0.0, 33.86977728843689, 0.0, 0.0, 0.0], 'rewardMean': 0.8481340878980499, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.534093782678
'totalSteps': 3840, 'rewardStep': 0.8554192927010101, 'errorList': [], 'lossList': [0.0, -1.4162498909235, 0.0, 44.20946449279785, 0.0, 0.0, 0.0], 'rewardMean': 0.8505624894990366, 'totalEpisodes': 88, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.75659977962265
'totalSteps': 5120, 'rewardStep': 0.7732270171557504, 'errorList': [], 'lossList': [0.0, -1.4161993730068207, 0.0, 30.176337015628814, 0.0, 0.0, 0.0], 'rewardMean': 0.8312286214132151, 'totalEpisodes': 95, 'stepsPerEpisode': 40, 'rewardPerEpisode': 33.1289529904867
'totalSteps': 6400, 'rewardStep': 0.8297187875437658, 'errorList': [], 'lossList': [0.0, -1.417870522737503, 0.0, 33.467891914844515, 0.0, 0.0, 0.0], 'rewardMean': 0.8309266546393251, 'totalEpisodes': 101, 'stepsPerEpisode': 349, 'rewardPerEpisode': 270.3933652081314
'totalSteps': 7680, 'rewardStep': 0.9123390546862549, 'errorList': [], 'lossList': [0.0, -1.4032655119895936, 0.0, 39.85224018573761, 0.0, 0.0, 0.0], 'rewardMean': 0.8444953879804801, 'totalEpisodes': 108, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.093989635809017
'totalSteps': 8960, 'rewardStep': 0.4848727943390395, 'errorList': [], 'lossList': [0.0, -1.39119880259037, 0.0, 58.729250555038455, 0.0, 0.0, 0.0], 'rewardMean': 0.7931207317459885, 'totalEpisodes': 116, 'stepsPerEpisode': 327, 'rewardPerEpisode': 242.95537489076787
'totalSteps': 10240, 'rewardStep': 0.8667840649683678, 'errorList': [], 'lossList': [0.0, -1.3801663428545, 0.0, 61.16567825317383, 0.0, 0.0, 0.0], 'rewardMean': 0.802328648398786, 'totalEpisodes': 123, 'stepsPerEpisode': 154, 'rewardPerEpisode': 124.44809876947598
'totalSteps': 11520, 'rewardStep': 0.6778318706193702, 'errorList': [], 'lossList': [0.0, -1.3632496440410613, 0.0, 117.34927118301391, 0.0, 0.0, 0.0], 'rewardMean': 0.7884956730899622, 'totalEpisodes': 149, 'stepsPerEpisode': 40, 'rewardPerEpisode': 30.19042592900128
'totalSteps': 12800, 'rewardStep': 0.8616905403323819, 'errorList': [], 'lossList': [0.0, -1.3588534384965896, 0.0, 73.43886661529541, 0.0, 0.0, 0.0], 'rewardMean': 0.795815159814204, 'totalEpisodes': 172, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.226726945587586
'totalSteps': 14080, 'rewardStep': 0.5868946142782446, 'errorList': [], 'lossList': [0.0, -1.3476796716451644, 0.0, 26.12768846988678, 0.0, 0.0, 0.0], 'rewardMean': 0.7623956078268878, 'totalEpisodes': 183, 'stepsPerEpisode': 100, 'rewardPerEpisode': 71.05598383341986
'totalSteps': 15360, 'rewardStep': 0.520864239894095, 'errorList': [], 'lossList': [0.0, -1.3437716633081436, 0.0, 30.893678855895995, 0.0, 0.0, 0.0], 'rewardMean': 0.736964227651828, 'totalEpisodes': 188, 'stepsPerEpisode': 313, 'rewardPerEpisode': 251.72221545976078
'totalSteps': 16640, 'rewardStep': 0.18059970155957705, 'errorList': [], 'lossList': [0.0, -1.3490182065963745, 0.0, 25.48515820980072, 0.0, 0.0, 0.0], 'rewardMean': 0.6694822685376847, 'totalEpisodes': 192, 'stepsPerEpisode': 447, 'rewardPerEpisode': 335.2325577484471
'totalSteps': 17920, 'rewardStep': 0.7311916350183804, 'errorList': [], 'lossList': [0.0, -1.3243947637081146, 0.0, 11.802220911383628, 0.0, 0.0, 0.0], 'rewardMean': 0.6652787303239476, 'totalEpisodes': 195, 'stepsPerEpisode': 348, 'rewardPerEpisode': 287.0076517400956
'totalSteps': 19200, 'rewardStep': 0.8862388904249505, 'errorList': [], 'lossList': [0.0, -1.2955131775140762, 0.0, 6.118845196962357, 0.0, 0.0, 0.0], 'rewardMean': 0.6709307406120661, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 940.1995723074915
'totalSteps': 20480, 'rewardStep': 0.7234248650498335, 'errorList': [], 'lossList': [0.0, -1.2679258996248246, 0.0, 3.9875110098719597, 0.0, 0.0, 0.0], 'rewardMean': 0.6520393216484239, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.3855213139733
'totalSteps': 21760, 'rewardStep': 0.8322607500331599, 'errorList': [], 'lossList': [0.0, -1.2489531862735748, 0.0, 3.313619233071804, 0.0, 0.0, 0.0], 'rewardMean': 0.686778117217836, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.6176260978013
'totalSteps': 23040, 'rewardStep': 0.853648948382641, 'errorList': [], 'lossList': [0.0, -1.2271307837963104, 0.0, 2.0008546525985, 0.0, 0.0, 0.0], 'rewardMean': 0.6854646055592634, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.8964956670593
'totalSteps': 24320, 'rewardStep': 0.8843966872395209, 'errorList': [], 'lossList': [0.0, -1.2141520595550537, 0.0, 1.5232139674946665, 0.0, 0.0, 0.0], 'rewardMean': 0.7061210872212784, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.1466166169014
'totalSteps': 25600, 'rewardStep': 0.9656657186341719, 'errorList': [0.07673043957627414, 0.06758456401896508, 0.06317238105959212, 0.09649263721876702, 0.07262336350377414, 0.06504503909315025, 0.06589570732349355, 0.06457803240025137, 0.06642822751890827, 0.07782851362596266, 0.06535030396278474, 0.06615695953321513, 0.05919955840381427, 0.06150170291497977, 0.06595220473516411, 0.11367894029204159, 0.06611296549046862, 0.05783753572327254, 0.0599926807544712, 0.12711917244586812, 0.05772535619235399, 0.06019289940810074, 0.12039174340217569, 0.06564183987301221, 0.059302877165365454, 0.06721963485638456, 0.1473036953075879, 0.11299263029414537, 0.10375008303033917, 0.10628453622408672, 0.07567058792353802, 0.12483744727685396, 0.1512914335824076, 0.1020522314801554, 0.18681105676118684, 0.059479057934583066, 0.06102208868858056, 0.06453812817910884, 0.06661716419106656, 0.059999494300440055, 0.1214138865617124, 0.12671645615447663, 0.059119606772693435, 0.10030001502165829, 0.05946164311138755, 0.11767936886764968, 0.06147994488721843, 0.05993260601759, 0.1076089935158279, 0.176797855108445], 'lossList': [0.0, -1.1892878752946854, 0.0, 1.8824919916689395, 0.0, 0.0, 0.0], 'rewardMean': 0.7165186050514575, 'totalEpisodes': 195, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1194.8755805214191, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.8
