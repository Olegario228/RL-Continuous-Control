#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 13
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6113436455576633, 'errorList': [], 'lossList': [0.0, -1.418630239367485, 0.0, 26.976813066005707, 0.0, 0.0, 0.0], 'rewardMean': 0.6883347392523597, 'totalEpisodes': 16, 'stepsPerEpisode': 43, 'rewardPerEpisode': 30.374728522067723
'totalSteps': 3840, 'rewardStep': 0.886896531131728, 'errorList': [], 'lossList': [0.0, -1.4027723169326782, 0.0, 28.04003488063812, 0.0, 0.0, 0.0], 'rewardMean': 0.754522003212149, 'totalEpisodes': 18, 'stepsPerEpisode': 541, 'rewardPerEpisode': 385.9880453242152
'totalSteps': 5120, 'rewardStep': 0.7644515847559634, 'errorList': [], 'lossList': [0.0, -1.3969396561384202, 0.0, 55.39999134063721, 0.0, 0.0, 0.0], 'rewardMean': 0.7570043985981026, 'totalEpisodes': 25, 'stepsPerEpisode': 79, 'rewardPerEpisode': 71.7752551170692
'totalSteps': 6400, 'rewardStep': 0.9339544876902267, 'errorList': [175.14553994499042, 147.43995678089814, 176.58936257103613, 178.01443859496, 163.10530458029143, 155.55002172590682, 170.86313610682842, 157.54604624600603, 161.21137414203136, 168.32035288672094, 167.5393313954892, 167.64448384961696, 184.0692598123139, 166.98833142276243, 182.21996288825142, 184.83275351609402, 174.12486897699932, 155.17242281721772, 167.12044815166362, 139.6798430871863, 153.87915960973248, 169.64807015969106, 179.00204016058478, 161.9401553702713, 171.26074683369814, 180.3099779469217, 152.12581758195068, 170.2892972464269, 167.83222701152408, 172.17453896382034, 152.54387094043796, 163.9415893675429, 173.85528435107662, 173.95387887463062, 156.24327650649983, 170.44298922831118, 173.09339821980703, 157.70605693016535, 142.9989215058204, 169.81653875062554, 185.33932318147643, 165.11096410266413, 174.26820786591045, 177.17505115889784, 123.91831727851064, 177.6260230056159, 151.04024617779322, 160.1381610594741, 146.76199416987075, 158.36684348731043], 'lossList': [0.0, -1.395898626446724, 0.0, 174.50677017211913, 0.0, 0.0, 0.0], 'rewardMean': 0.7923944164165275, 'totalEpisodes': 79, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.086500485511745, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.5774875641706849, 'errorList': [], 'lossList': [0.0, -1.382072076201439, 0.0, 89.20252193450928, 0.0, 0.0, 0.0], 'rewardMean': 0.756576607708887, 'totalEpisodes': 128, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.763144928952556
'totalSteps': 8960, 'rewardStep': 0.5555820032068908, 'errorList': [], 'lossList': [0.0, -1.3751434129476547, 0.0, 61.76270290374756, 0.0, 0.0, 0.0], 'rewardMean': 0.7278630927800304, 'totalEpisodes': 167, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5555820032068908
'totalSteps': 10240, 'rewardStep': 0.7616577291602403, 'errorList': [], 'lossList': [0.0, -1.3635696160793305, 0.0, 56.90945009231567, 0.0, 0.0, 0.0], 'rewardMean': 0.7320874223275567, 'totalEpisodes': 187, 'stepsPerEpisode': 141, 'rewardPerEpisode': 125.47233043930575
'totalSteps': 11520, 'rewardStep': 0.8958691768001589, 'errorList': [], 'lossList': [0.0, -1.3447885447740555, 0.0, 37.676033086776734, 0.0, 0.0, 0.0], 'rewardMean': 0.7502853950467347, 'totalEpisodes': 196, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.622619947370875
'totalSteps': 12800, 'rewardStep': 0.7269573516868992, 'errorList': [], 'lossList': [0.0, -1.3342493683099748, 0.0, 17.318452224731445, 0.0, 0.0, 0.0], 'rewardMean': 0.7479525907107512, 'totalEpisodes': 201, 'stepsPerEpisode': 96, 'rewardPerEpisode': 77.69272232508715
'totalSteps': 14080, 'rewardStep': 0.9505259110193631, 'errorList': [3.919544006656068, 5.680997722781278, 2.6313630012910822, 3.149870486890084, 4.570252605432562, 6.267630074051482, 3.673105192440159, 1.8382396210857577, 0.6433525285989823, 0.675426313714557, 1.9075077969449192, 2.039960331454707, 4.605982803652055, 1.6458366699782234, 3.2172228481605027, 0.8342634442974989, 0.24662194377329194, 5.1493476416509125, 3.4277559079862523, 1.62960757714124, 4.287216321677838, 0.6006232192472138, 2.361808173593287, 2.3584913253265274, 3.515843539037568, 0.23211987372225862, 1.1616729085522624, 2.548428257065293, 2.3259011604320734, 5.673150361227418, 1.6364573513054217, 1.0558493213921112, 6.49603430037422, 1.9788500051544382, 2.6448815696408987, 5.301473993680577, 0.2992730288645953, 0.505935609517569, 4.543746058277752, 2.4868630185611234, 0.6819620847008451, 0.13888616570464432, 2.856520688000123, 0.4461387429630412, 4.613683222495338, 1.1278444316207548, 3.5338437429952925, 2.3410086691144434, 5.816307148437086, 6.2035261669722574], 'lossList': [0.0, -1.3444537717103957, 0.0, 13.26637854218483, 0.0, 0.0, 0.0], 'rewardMean': 0.7664725985179819, 'totalEpisodes': 205, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.800324396150206, 'successfulTests': 1
'totalSteps': 15360, 'rewardStep': 0.6936737591758633, 'errorList': [], 'lossList': [0.0, -1.3553148710727692, 0.0, 8.809822733402251, 0.0, 0.0, 0.0], 'rewardMean': 0.7747056098798019, 'totalEpisodes': 210, 'stepsPerEpisode': 57, 'rewardPerEpisode': 40.32830893512747
'totalSteps': 16640, 'rewardStep': 0.7328448288390117, 'errorList': [], 'lossList': [0.0, -1.343107505440712, 0.0, 9.969671595096589, 0.0, 0.0, 0.0], 'rewardMean': 0.7593004396505302, 'totalEpisodes': 213, 'stepsPerEpisode': 123, 'rewardPerEpisode': 101.9413701616838
'totalSteps': 17920, 'rewardStep': 0.8032134383333143, 'errorList': [], 'lossList': [0.0, -1.3251541316509248, 0.0, 6.3413114786148075, 0.0, 0.0, 0.0], 'rewardMean': 0.7631766250082653, 'totalEpisodes': 218, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.293456044189895
'totalSteps': 19200, 'rewardStep': 0.8853647919826386, 'errorList': [], 'lossList': [0.0, -1.3178316074609757, 0.0, 5.268735099434853, 0.0, 0.0, 0.0], 'rewardMean': 0.7583176554375065, 'totalEpisodes': 221, 'stepsPerEpisode': 76, 'rewardPerEpisode': 63.24670302562345
'totalSteps': 20480, 'rewardStep': 0.6652175457193068, 'errorList': [], 'lossList': [0.0, -1.313138118982315, 0.0, 3.0555588638782503, 0.0, 0.0, 0.0], 'rewardMean': 0.7670906535923686, 'totalEpisodes': 222, 'stepsPerEpisode': 586, 'rewardPerEpisode': 424.86125227674574
'totalSteps': 21760, 'rewardStep': 0.8994413589161961, 'errorList': [], 'lossList': [0.0, -1.2963675379753112, 0.0, 2.596536072790623, 0.0, 0.0, 0.0], 'rewardMean': 0.8014765891632992, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 958.2077686088229
'totalSteps': 23040, 'rewardStep': 0.7753234039387914, 'errorList': [], 'lossList': [0.0, -1.2549777644872666, 0.0, 1.677588210105896, 0.0, 0.0, 0.0], 'rewardMean': 0.8028431566411542, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.0507340221873
'totalSteps': 24320, 'rewardStep': 0.777460447250027, 'errorList': [], 'lossList': [0.0, -1.2222881537675858, 0.0, 1.203618000820279, 0.0, 0.0, 0.0], 'rewardMean': 0.7910022836861411, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.052663978516
'totalSteps': 25600, 'rewardStep': 0.9532550946901959, 'errorList': [0.021576425147901233, 0.03370967294225724, 0.03891452916160796, 0.01808944479952164, 0.021622258626471564, 0.02149323608638397, 0.055643466074215386, 0.03388572425876776, 0.03245376309047509, 0.02629419598767083, 0.01919309183948637, 0.09568396066501882, 0.021047678742297637, 0.06384414147667607, 0.015526471700799713, 0.04985636418937388, 0.01920375403200853, 0.059105071674031034, 0.06116660139950888, 0.08245107349105707, 0.045217391200574775, 0.041983221896751036, 0.01617158500849365, 0.03655617969347978, 0.026081714337860613, 0.019852603469266, 0.01821019794148647, 0.05541359978962432, 0.056667240878065496, 0.029986436001433137, 0.018653124633968576, 0.045997783457710766, 0.030102544616115442, 0.024089107978885207, 0.05717320074319694, 0.032629990801264774, 0.022520060780004268, 0.023528002446601158, 0.03144265131930833, 0.05849636285982894, 0.02269951127464353, 0.01519704856082497, 0.02378531177284597, 0.03877747674757958, 0.05285425182881169, 0.030712074236906726, 0.016761287634867145, 0.019213589448052567, 0.058933971537165536, 0.029225462173628303], 'lossList': [0.0, -1.2068811613321304, 0.0, 1.0255093048326671, 0.0, 0.0, 0.0], 'rewardMean': 0.8136320579864709, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1178.2979614616268, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=120.09
