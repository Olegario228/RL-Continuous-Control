#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 131
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6198643409336853, 'errorList': [], 'lossList': [0.0, -1.4236961072683334, 0.0, 27.09020174026489, 0.0, 0.0, 0.0], 'rewardMean': 0.7915769464221976, 'totalEpisodes': 16, 'stepsPerEpisode': 359, 'rewardPerEpisode': 252.655278143475
'totalSteps': 3840, 'rewardStep': 0.8025490960388084, 'errorList': [], 'lossList': [0.0, -1.4268264448642731, 0.0, 21.67763866662979, 0.0, 0.0, 0.0], 'rewardMean': 0.7952343296277345, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 247.1155734554917
'totalSteps': 5120, 'rewardStep': 0.6021662596725221, 'errorList': [], 'lossList': [0.0, -1.4322392839193343, 0.0, 21.76698417186737, 0.0, 0.0, 0.0], 'rewardMean': 0.7469673121389314, 'totalEpisodes': 24, 'stepsPerEpisode': 70, 'rewardPerEpisode': 48.936158796957585
'totalSteps': 6400, 'rewardStep': 0.6098474106770959, 'errorList': [], 'lossList': [0.0, -1.4271924251317978, 0.0, 21.247647724151612, 0.0, 0.0, 0.0], 'rewardMean': 0.7195433318465643, 'totalEpisodes': 26, 'stepsPerEpisode': 304, 'rewardPerEpisode': 187.864047549819
'totalSteps': 7680, 'rewardStep': 0.6713566128295293, 'errorList': [], 'lossList': [0.0, -1.3964197140932084, 0.0, 11.186662551760673, 0.0, 0.0, 0.0], 'rewardMean': 0.7115122120103918, 'totalEpisodes': 26, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.8839768984354
'totalSteps': 8960, 'rewardStep': 0.8411560362381254, 'errorList': [], 'lossList': [0.0, -1.3642153298854829, 0.0, 15.028884925842284, 0.0, 0.0, 0.0], 'rewardMean': 0.7300327583286395, 'totalEpisodes': 27, 'stepsPerEpisode': 449, 'rewardPerEpisode': 361.16942015983415
'totalSteps': 10240, 'rewardStep': 0.7150662728511042, 'errorList': [], 'lossList': [0.0, -1.3458790975809096, 0.0, 51.45156428337097, 0.0, 0.0, 0.0], 'rewardMean': 0.7281619476439476, 'totalEpisodes': 31, 'stepsPerEpisode': 56, 'rewardPerEpisode': 38.97102680132126
'totalSteps': 11520, 'rewardStep': 0.6021265160782215, 'errorList': [], 'lossList': [0.0, -1.3424617874622344, 0.0, 293.88602012634277, 0.0, 0.0, 0.0], 'rewardMean': 0.7141580108033113, 'totalEpisodes': 62, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.987461549494697
'totalSteps': 12800, 'rewardStep': 0.7124490212564367, 'errorList': [], 'lossList': [0.0, -1.3417168140411377, 0.0, 116.91901798248291, 0.0, 0.0, 0.0], 'rewardMean': 0.7139871118486238, 'totalEpisodes': 89, 'stepsPerEpisode': 74, 'rewardPerEpisode': 59.797141507879886
'totalSteps': 14080, 'rewardStep': 0.8360046665807549, 'errorList': [], 'lossList': [0.0, -1.3366780829429628, 0.0, 39.160151829719545, 0.0, 0.0, 0.0], 'rewardMean': 0.7012586233156284, 'totalEpisodes': 106, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.486518978476633
'totalSteps': 15360, 'rewardStep': 0.45643134678723624, 'errorList': [], 'lossList': [0.0, -1.3295566684007645, 0.0, 15.072330405712128, 0.0, 0.0, 0.0], 'rewardMean': 0.6849153239009835, 'totalEpisodes': 111, 'stepsPerEpisode': 177, 'rewardPerEpisode': 129.51776963777044
'totalSteps': 16640, 'rewardStep': 0.4366299863441291, 'errorList': [], 'lossList': [0.0, -1.313281136751175, 0.0, 38.110183358192444, 0.0, 0.0, 0.0], 'rewardMean': 0.6483234129315155, 'totalEpisodes': 117, 'stepsPerEpisode': 172, 'rewardPerEpisode': 129.79000811866644
'totalSteps': 17920, 'rewardStep': 0.5807051423909932, 'errorList': [], 'lossList': [0.0, -1.2796704381704331, 0.0, 10.085024287700653, 0.0, 0.0, 0.0], 'rewardMean': 0.6461773012033627, 'totalEpisodes': 122, 'stepsPerEpisode': 233, 'rewardPerEpisode': 163.08065031268464
'totalSteps': 19200, 'rewardStep': 0.9440474600519352, 'errorList': [2.645532210615609, 1.0687188706016395, 4.342536194529327, 5.317248827344619, 5.53043439023921, 3.9965627692546426, 1.695307865854955, 3.831830604569112, 3.668767160277752, 5.086384307960697, 2.5994434931648613, 1.1587197279755876, 3.4932742600848146, 5.552978583918766, 5.315506929714321, 6.940301092196982, 2.28316069939954, 4.935034570481505, 1.9120001082413987, 3.9473721040778647, 2.779323742667236, 2.4320622959558285, 2.570636889278143, 5.305576188794004, 2.724504470566263, 1.6859379957047083, 2.427089553545181, 6.916053534132256, 1.4998685819651745, 5.657009336810268, 2.551464689277907, 3.416738707860224, 2.2635834244086106, 2.584875007981738, 0.9042436383595588, 6.623331071654923, 1.6490995164226407, 5.359893360579777, 1.5301394962312738, 5.796098871624694, 5.457773048487404, 5.300573230319948, 3.290188962986558, 7.262785352782742, 4.5812716263392925, 3.630544506165329, 5.345209616787979, 1.921682350477547, 5.507287995228752, 4.262681441859047], 'lossList': [0.0, -1.2417417973279954, 0.0, 16.13223314523697, 0.0, 0.0, 0.0], 'rewardMean': 0.6795973061408466, 'totalEpisodes': 129, 'stepsPerEpisode': 46, 'rewardPerEpisode': 41.30032046088772, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.6193584273543029, 'errorList': [], 'lossList': [0.0, -1.2186475825309753, 0.0, 6.618516414165497, 0.0, 0.0, 0.0], 'rewardMean': 0.6743974875933241, 'totalEpisodes': 133, 'stepsPerEpisode': 99, 'rewardPerEpisode': 69.84897782560043
'totalSteps': 21760, 'rewardStep': 0.794501532501382, 'errorList': [], 'lossList': [0.0, -1.2053385931253433, 0.0, 7.827148866653443, 0.0, 0.0, 0.0], 'rewardMean': 0.6697320372196496, 'totalEpisodes': 136, 'stepsPerEpisode': 72, 'rewardPerEpisode': 50.646471715507644
'totalSteps': 23040, 'rewardStep': 0.6310477338005904, 'errorList': [], 'lossList': [0.0, -1.1910505956411361, 0.0, 5.745047832727432, 0.0, 0.0, 0.0], 'rewardMean': 0.6613301833145983, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 992.7763056315523
'totalSteps': 24320, 'rewardStep': 0.6077371824023263, 'errorList': [], 'lossList': [0.0, -1.167109859585762, 0.0, 4.975615125894547, 0.0, 0.0, 0.0], 'rewardMean': 0.6618912499470087, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 830.4573855618086
'totalSteps': 25600, 'rewardStep': 0.9138185348451954, 'errorList': [], 'lossList': [0.0, -1.1357631868124007, 0.0, 1.4919542273879052, 0.0, 0.0, 0.0], 'rewardMean': 0.6820282013058846, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.9032280384627
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.23
