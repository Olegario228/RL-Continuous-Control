#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 134
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8728080237662469, 'errorList': [], 'lossList': [0.0, -1.4150770044326781, 0.0, 27.33979842185974, 0.0, 0.0, 0.0], 'rewardMean': 0.6184390647709177, 'totalEpisodes': 21, 'stepsPerEpisode': 37, 'rewardPerEpisode': 30.579136477324717
'totalSteps': 3840, 'rewardStep': 0.5658957791374124, 'errorList': [], 'lossList': [0.0, -1.4308610665798187, 0.0, 28.464538366794585, 0.0, 0.0, 0.0], 'rewardMean': 0.6009246362264159, 'totalEpisodes': 29, 'stepsPerEpisode': 182, 'rewardPerEpisode': 120.50812201117319
'totalSteps': 5120, 'rewardStep': 0.5489054457277796, 'errorList': [], 'lossList': [0.0, -1.4340275478363038, 0.0, 13.098313295841217, 0.0, 0.0, 0.0], 'rewardMean': 0.5879198386017568, 'totalEpisodes': 29, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 830.4866835589498
'totalSteps': 6400, 'rewardStep': 0.704221212749388, 'errorList': [], 'lossList': [0.0, -1.4263639843463898, 0.0, 31.347619965076447, 0.0, 0.0, 0.0], 'rewardMean': 0.611180113431283, 'totalEpisodes': 31, 'stepsPerEpisode': 867, 'rewardPerEpisode': 670.2118950379631
'totalSteps': 7680, 'rewardStep': 0.7649926254472545, 'errorList': [], 'lossList': [0.0, -1.4116993242502212, 0.0, 23.938474049568175, 0.0, 0.0, 0.0], 'rewardMean': 0.6368155321006116, 'totalEpisodes': 32, 'stepsPerEpisode': 428, 'rewardPerEpisode': 320.83160065916445
'totalSteps': 8960, 'rewardStep': 0.9040030413896782, 'errorList': [], 'lossList': [0.0, -1.385682662129402, 0.0, 32.94229287147522, 0.0, 0.0, 0.0], 'rewardMean': 0.6749851762847641, 'totalEpisodes': 34, 'stepsPerEpisode': 310, 'rewardPerEpisode': 248.8825393723644
'totalSteps': 10240, 'rewardStep': 0.8335930618417151, 'errorList': [], 'lossList': [0.0, -1.379010174870491, 0.0, 82.9957759475708, 0.0, 0.0, 0.0], 'rewardMean': 0.6948111619793829, 'totalEpisodes': 40, 'stepsPerEpisode': 82, 'rewardPerEpisode': 68.79962118425098
'totalSteps': 11520, 'rewardStep': 0.8291305502205932, 'errorList': [], 'lossList': [0.0, -1.3724409025907516, 0.0, 201.8651912689209, 0.0, 0.0, 0.0], 'rewardMean': 0.7097355384506285, 'totalEpisodes': 73, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.700572864306867
'totalSteps': 12800, 'rewardStep': 0.564109472164797, 'errorList': [], 'lossList': [0.0, -1.3694881635904312, 0.0, 100.77731880187989, 0.0, 0.0, 0.0], 'rewardMean': 0.6951729318220454, 'totalEpisodes': 98, 'stepsPerEpisode': 64, 'rewardPerEpisode': 44.27435968936856
'totalSteps': 14080, 'rewardStep': 0.5031290111798591, 'errorList': [], 'lossList': [0.0, -1.3710664063692093, 0.0, 48.17135940551758, 0.0, 0.0, 0.0], 'rewardMean': 0.7090788223624724, 'totalEpisodes': 105, 'stepsPerEpisode': 139, 'rewardPerEpisode': 108.12144434514614
'totalSteps': 15360, 'rewardStep': 0.8193557325486398, 'errorList': [], 'lossList': [0.0, -1.36849260866642, 0.0, 48.40888138771057, 0.0, 0.0, 0.0], 'rewardMean': 0.7037335932407117, 'totalEpisodes': 113, 'stepsPerEpisode': 374, 'rewardPerEpisode': 261.1302883098935
'totalSteps': 16640, 'rewardStep': 0.4191302799619531, 'errorList': [], 'lossList': [0.0, -1.3622657990455627, 0.0, 27.149671382904053, 0.0, 0.0, 0.0], 'rewardMean': 0.6890570433231658, 'totalEpisodes': 118, 'stepsPerEpisode': 447, 'rewardPerEpisode': 296.7468087803347
'totalSteps': 17920, 'rewardStep': 0.7277251531712013, 'errorList': [], 'lossList': [0.0, -1.3479052466154098, 0.0, 11.2033631503582, 0.0, 0.0, 0.0], 'rewardMean': 0.706939014067508, 'totalEpisodes': 119, 'stepsPerEpisode': 208, 'rewardPerEpisode': 160.8006033184825
'totalSteps': 19200, 'rewardStep': 0.7805045364106391, 'errorList': [], 'lossList': [0.0, -1.3243325263261796, 0.0, 6.472342250943184, 0.0, 0.0, 0.0], 'rewardMean': 0.7145673464336331, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 964.0999351367783
'totalSteps': 20480, 'rewardStep': 0.7892769972053955, 'errorList': [], 'lossList': [0.0, -1.2842988777160644, 0.0, 3.9088290032744406, 0.0, 0.0, 0.0], 'rewardMean': 0.7169957836094472, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 924.1107521542052
'totalSteps': 21760, 'rewardStep': 0.8763099216112793, 'errorList': [], 'lossList': [0.0, -1.2473686176538468, 0.0, 3.735945194065571, 0.0, 0.0, 0.0], 'rewardMean': 0.7142264716316074, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.8423124525318
'totalSteps': 23040, 'rewardStep': 0.9089193353368767, 'errorList': [], 'lossList': [0.0, -1.2434611332416534, 0.0, 2.7791154816746713, 0.0, 0.0, 0.0], 'rewardMean': 0.7217590989811234, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.8060764587478
'totalSteps': 24320, 'rewardStep': 0.8462968231760571, 'errorList': [], 'lossList': [0.0, -1.221124683022499, 0.0, 1.929309899099171, 0.0, 0.0, 0.0], 'rewardMean': 0.7234757262766698, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.215976378006
'totalSteps': 25600, 'rewardStep': 0.9827676695059494, 'errorList': [0.07457851768247938, 0.046374671919319085, 0.045012727959677624, 0.04784345222062123, 0.0478172229704121, 0.04503434046574791, 0.04586693639353376, 0.04574448552009945, 0.04673236118523453, 0.0629457230444778, 0.06649025891232564, 0.0452702607346961, 0.0446200533750746, 0.04876980015770327, 0.04860558312071641, 0.058492446929698905, 0.06143144990185699, 0.07159094212473371, 0.051954568926859894, 0.04471169188264711, 0.050021619001634064, 0.046861205015502394, 0.04987082778232945, 0.046957735168410876, 0.04730638978940961, 0.04586493384976952, 0.04597064952022718, 0.060506547698610975, 0.04621612723186704, 0.07977012838208739, 0.050165986357685904, 0.04974781681753527, 0.0471855914669273, 0.05269243687827189, 0.04683046259070035, 0.04762867134713032, 0.04679799547541825, 0.0829966679287585, 0.04471983961192533, 0.04786500734994562, 0.06010759509222675, 0.05791699539144395, 0.04623233997255291, 0.0445897569144239, 0.04921612251012062, 0.04688656284019864, 0.046399033863699396, 0.0464106675386926, 0.048342737000061836, 0.04904753677087223], 'lossList': [0.0, -1.1808586400747298, 0.0, 1.8776945281773805, 0.0, 0.0, 0.0], 'rewardMean': 0.765341546010785, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1191.3833121773578, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.83
