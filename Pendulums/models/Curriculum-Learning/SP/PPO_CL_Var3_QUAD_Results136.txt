#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 136
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7000445658223015, 'errorList': [], 'lossList': [0.0, -1.4378761506080628, 0.0, 32.823497290611265, 0.0, 0.0, 0.0], 'rewardMean': 0.6084192337608771, 'totalEpisodes': 12, 'stepsPerEpisode': 76, 'rewardPerEpisode': 62.203764465395636
'totalSteps': 3840, 'rewardStep': 0.8602516308066789, 'errorList': [], 'lossList': [0.0, -1.4444838488101959, 0.0, 26.6231791472435, 0.0, 0.0, 0.0], 'rewardMean': 0.6923633661094777, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 968.1800323446813
'totalSteps': 5120, 'rewardStep': 0.6438361675204465, 'errorList': [], 'lossList': [0.0, -1.4199292850494385, 0.0, 26.186295030713083, 0.0, 0.0, 0.0], 'rewardMean': 0.6802315664622198, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.6186251728723
'totalSteps': 6400, 'rewardStep': 0.8410966389361897, 'errorList': [], 'lossList': [0.0, -1.4140122920274734, 0.0, 21.861056965589523, 0.0, 0.0, 0.0], 'rewardMean': 0.7124045809570138, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.0546717476302
'totalSteps': 7680, 'rewardStep': 0.8696267930498726, 'errorList': [], 'lossList': [0.0, -1.3886793106794357, 0.0, 17.089685423746705, 0.0, 0.0, 0.0], 'rewardMean': 0.7386082829724901, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.9875696914137
'totalSteps': 8960, 'rewardStep': 0.9539802263054039, 'errorList': [], 'lossList': [0.0, -1.37690887093544, 0.0, 8.4542059892416, 0.0, 0.0, 0.0], 'rewardMean': 0.7693757034486207, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1065.5145121123448
'totalSteps': 10240, 'rewardStep': 0.9298736374394615, 'errorList': [], 'lossList': [0.0, -1.3515806365013123, 0.0, 5.078722496330738, 0.0, 0.0, 0.0], 'rewardMean': 0.789437945197476, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1029.6934012841275
'totalSteps': 11520, 'rewardStep': 0.6945981324340095, 'errorList': [], 'lossList': [0.0, -1.3419824653863908, 0.0, 623.653197631836, 0.0, 0.0, 0.0], 'rewardMean': 0.7789001882237575, 'totalEpisodes': 44, 'stepsPerEpisode': 70, 'rewardPerEpisode': 60.776136233034144
'totalSteps': 12800, 'rewardStep': 0.5257093433564601, 'errorList': [], 'lossList': [0.0, -1.3424198752641678, 0.0, 264.98811050415037, 0.0, 0.0, 0.0], 'rewardMean': 0.7535811037370278, 'totalEpisodes': 67, 'stepsPerEpisode': 25, 'rewardPerEpisode': 18.093306817012362
'totalSteps': 14080, 'rewardStep': 0.6188606465077194, 'errorList': [], 'lossList': [0.0, -1.3418006873130799, 0.0, 113.25499979019165, 0.0, 0.0, 0.0], 'rewardMean': 0.7637877782178543, 'totalEpisodes': 96, 'stepsPerEpisode': 63, 'rewardPerEpisode': 55.423093639422035
'totalSteps': 15360, 'rewardStep': 0.783593099040012, 'errorList': [], 'lossList': [0.0, -1.3388888102769851, 0.0, 47.13957016944885, 0.0, 0.0, 0.0], 'rewardMean': 0.7721426315396254, 'totalEpisodes': 123, 'stepsPerEpisode': 58, 'rewardPerEpisode': 50.47865401292065
'totalSteps': 16640, 'rewardStep': 0.8949163769409743, 'errorList': [], 'lossList': [0.0, -1.3325815069675446, 0.0, 28.790082025527955, 0.0, 0.0, 0.0], 'rewardMean': 0.7756091061530549, 'totalEpisodes': 141, 'stepsPerEpisode': 24, 'rewardPerEpisode': 22.71167988866798
'totalSteps': 17920, 'rewardStep': 0.7461045002623502, 'errorList': [], 'lossList': [0.0, -1.3240990471839904, 0.0, 22.636965317726137, 0.0, 0.0, 0.0], 'rewardMean': 0.7858359394272454, 'totalEpisodes': 149, 'stepsPerEpisode': 69, 'rewardPerEpisode': 57.91570476747183
'totalSteps': 19200, 'rewardStep': 0.93639931030443, 'errorList': [3.54115141786181, 3.4402640368038497, 2.5201440151219847, 3.7850583094860415, 3.0171578246784705, 2.1631113439385152, 3.281513862281133, 3.445587819066954, 3.727702326637702, 2.9757780621444136, 3.726841832373215, 1.0369742171930643, 3.6529164823697107, 2.225055138420305, 2.467549350403142, 3.642633058581391, 3.1774031304550214, 2.3542056031424, 2.4866767796529663, 3.2008595503752946, 3.0546051645385854, 3.5017243656599444, 1.9249537049588512, 2.1812685242245955, 3.569351926657015, 1.6945293853759371, 3.585720715951805, 3.5219004751616705, 2.8664527350192763, 3.406360942830752, 1.7782461835605534, 2.4698492472881983, 2.6710394902344614, 2.426033333129633, 2.793066216768954, 3.5621084576510955, 3.5350055671617233, 2.626456907805484, 2.1538557411681354, 1.0352357638542393, 1.8803927576417048, 3.3488488529061597, 3.3843495086964186, 3.2316938080371087, 2.4242157536348827, 2.601220748032569, 1.412654408478796, 3.3641700772370218, 1.5239678268203967, 3.351775919046356], 'lossList': [0.0, -1.3149342620372773, 0.0, 21.916880180835722, 0.0, 0.0, 0.0], 'rewardMean': 0.7953662065640694, 'totalEpisodes': 157, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.92887274395093, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.6963112220257147, 'errorList': [], 'lossList': [0.0, -1.3000404155254364, 0.0, 10.758584818840028, 0.0, 0.0, 0.0], 'rewardMean': 0.7780346494616536, 'totalEpisodes': 159, 'stepsPerEpisode': 220, 'rewardPerEpisode': 183.93720114408399
'totalSteps': 21760, 'rewardStep': 0.8479447087643869, 'errorList': [], 'lossList': [0.0, -1.2745060068368912, 0.0, 19.171730320453644, 0.0, 0.0, 0.0], 'rewardMean': 0.7674310977075518, 'totalEpisodes': 161, 'stepsPerEpisode': 723, 'rewardPerEpisode': 611.7904822637231
'totalSteps': 23040, 'rewardStep': 0.8193294136877809, 'errorList': [], 'lossList': [0.0, -1.2647422850131989, 0.0, 11.73793343424797, 0.0, 0.0, 0.0], 'rewardMean': 0.7563766753323837, 'totalEpisodes': 163, 'stepsPerEpisode': 77, 'rewardPerEpisode': 62.521814703436775
'totalSteps': 24320, 'rewardStep': 0.5536985239226055, 'errorList': [], 'lossList': [0.0, -1.2690005946159362, 0.0, 2.9387856340408325, 0.0, 0.0, 0.0], 'rewardMean': 0.7422867144812434, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 849.150608990265
'totalSteps': 25600, 'rewardStep': 0.882533290305573, 'errorList': [], 'lossList': [0.0, -1.231685163974762, 0.0, 2.0666066387295725, 0.0, 0.0, 0.0], 'rewardMean': 0.7779691091761547, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.0131509586723
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=78.17
