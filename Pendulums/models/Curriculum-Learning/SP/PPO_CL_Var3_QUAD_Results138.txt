#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 138
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6545623066100972, 'errorList': [], 'lossList': [0.0, -1.4220104396343232, 0.0, 28.143821687698363, 0.0, 0.0, 0.0], 'rewardMean': 0.7099440697785766, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.082099983710116
'totalSteps': 3840, 'rewardStep': 0.9788904971773907, 'errorList': [], 'lossList': [0.0, -1.4124725741147994, 0.0, 28.961936374902724, 0.0, 0.0, 0.0], 'rewardMean': 0.7995928789115146, 'totalEpisodes': 18, 'stepsPerEpisode': 488, 'rewardPerEpisode': 374.0085245807097
'totalSteps': 5120, 'rewardStep': 0.7849682420759347, 'errorList': [], 'lossList': [0.0, -1.3965895849466323, 0.0, 25.80490006387234, 0.0, 0.0, 0.0], 'rewardMean': 0.7959367197026197, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.181170081126
'totalSteps': 6400, 'rewardStep': 0.6072054531356178, 'errorList': [], 'lossList': [0.0, -1.384770479798317, 0.0, 16.93236580848694, 0.0, 0.0, 0.0], 'rewardMean': 0.7581904663892194, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.9684548173193
'totalSteps': 7680, 'rewardStep': 0.8621439157328556, 'errorList': [], 'lossList': [0.0, -1.36528586268425, 0.0, 13.758628809452057, 0.0, 0.0, 0.0], 'rewardMean': 0.7755160412798254, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.0949259181327
'totalSteps': 8960, 'rewardStep': 0.8354122968696983, 'errorList': [], 'lossList': [0.0, -1.3544546043872834, 0.0, 25.837890555262565, 0.0, 0.0, 0.0], 'rewardMean': 0.7840726492212359, 'totalEpisodes': 19, 'stepsPerEpisode': 788, 'rewardPerEpisode': 617.4816578561359
'totalSteps': 10240, 'rewardStep': 0.8837827057921727, 'errorList': [], 'lossList': [0.0, -1.3430007189512252, 0.0, 83.83526210784912, 0.0, 0.0, 0.0], 'rewardMean': 0.7965364062926029, 'totalEpisodes': 24, 'stepsPerEpisode': 139, 'rewardPerEpisode': 118.7588242942742
'totalSteps': 11520, 'rewardStep': 0.9874375708492515, 'errorList': [348.8804454720299, 345.8537435889294, 356.0799313735447, 323.2231069354409, 369.88726331907657, 358.4068292905447, 383.0598020091416, 315.0233280521905, 376.2031797698927, 382.182476641754, 280.09468795991444, 249.73609124976403, 362.79973624698505, 234.56610114889128, 390.94337811528504, 251.11757901593603, 238.63784948768125, 374.8456434602942, 370.6899375893269, 359.00478795875546, 395.86714897386815, 389.78767738555507, 364.1247155716841, 346.90494858944066, 356.85595374379807, 299.53894851456533, 385.8600577823443, 358.17359231426485, 395.13366176224275, 411.870088773141, 378.32319131379114, 285.23439049136783, 371.8615710733795, 256.28619312337054, 349.85019375150694, 349.6214400883218, 358.7153343338798, 363.05529195521757, 304.7725351455144, 384.95575011522635, 389.6727231148652, 334.8576929211153, 299.5775785017232, 385.29541011221096, 375.63543980955393, 358.9689571370437, 308.25748504985535, 313.92597420906293, 387.57035465222464, 366.7322296917567], 'lossList': [0.0, -1.3430703693628312, 0.0, 463.6166688537598, 0.0, 0.0, 0.0], 'rewardMean': 0.8177476467988972, 'totalEpisodes': 65, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.739848803675306, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.6361030944289288, 'errorList': [], 'lossList': [0.0, -1.3410053008794784, 0.0, 179.5131365966797, 0.0, 0.0, 0.0], 'rewardMean': 0.7995831915619004, 'totalEpisodes': 101, 'stepsPerEpisode': 40, 'rewardPerEpisode': 26.79301365971764
'totalSteps': 14080, 'rewardStep': 0.5619367099328452, 'errorList': [], 'lossList': [0.0, -1.3383442980051041, 0.0, 69.15909617424012, 0.0, 0.0, 0.0], 'rewardMean': 0.7792442792604792, 'totalEpisodes': 120, 'stepsPerEpisode': 162, 'rewardPerEpisode': 132.5413510466288
'totalSteps': 15360, 'rewardStep': 0.5583025085636883, 'errorList': [], 'lossList': [0.0, -1.33352345764637, 0.0, 62.77253426551819, 0.0, 0.0, 0.0], 'rewardMean': 0.7696182994558384, 'totalEpisodes': 131, 'stepsPerEpisode': 46, 'rewardPerEpisode': 28.96740401197608
'totalSteps': 16640, 'rewardStep': 0.6065753271938608, 'errorList': [], 'lossList': [0.0, -1.3295700430870057, 0.0, 21.533011114597322, 0.0, 0.0, 0.0], 'rewardMean': 0.7323867824574852, 'totalEpisodes': 134, 'stepsPerEpisode': 144, 'rewardPerEpisode': 92.07072667421951
'totalSteps': 17920, 'rewardStep': 0.7292321532243426, 'errorList': [], 'lossList': [0.0, -1.3349323785305023, 0.0, 8.288513576984405, 0.0, 0.0, 0.0], 'rewardMean': 0.7268131735723262, 'totalEpisodes': 137, 'stepsPerEpisode': 212, 'rewardPerEpisode': 178.94724104875255
'totalSteps': 19200, 'rewardStep': 0.5133109155919571, 'errorList': [], 'lossList': [0.0, -1.3338610941171647, 0.0, 10.38237804889679, 0.0, 0.0, 0.0], 'rewardMean': 0.71742371981796, 'totalEpisodes': 139, 'stepsPerEpisode': 225, 'rewardPerEpisode': 143.9104188938927
'totalSteps': 20480, 'rewardStep': 0.7939157456586045, 'errorList': [], 'lossList': [0.0, -1.3176317298412323, 0.0, 24.928764200210573, 0.0, 0.0, 0.0], 'rewardMean': 0.7106009028105349, 'totalEpisodes': 141, 'stepsPerEpisode': 948, 'rewardPerEpisode': 653.6624999056108
'totalSteps': 21760, 'rewardStep': 0.7940661276404546, 'errorList': [], 'lossList': [0.0, -1.3109057569503784, 0.0, 11.599264708161353, 0.0, 0.0, 0.0], 'rewardMean': 0.7064662858876105, 'totalEpisodes': 142, 'stepsPerEpisode': 883, 'rewardPerEpisode': 628.153681202013
'totalSteps': 23040, 'rewardStep': 0.579948752392715, 'errorList': [], 'lossList': [0.0, -1.3077374666929245, 0.0, 1.9058391520380973, 0.0, 0.0, 0.0], 'rewardMean': 0.6760828905476648, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.6205220029907
'totalSteps': 24320, 'rewardStep': 0.653482575640995, 'errorList': [], 'lossList': [0.0, -1.2743170082569122, 0.0, 2.3132145692408086, 0.0, 0.0, 0.0], 'rewardMean': 0.6426873910268391, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.9706861935806
'totalSteps': 25600, 'rewardStep': 0.9280239231677703, 'errorList': [], 'lossList': [0.0, -1.2460250359773637, 0.0, 2.0723158521205187, 0.0, 0.0, 0.0], 'rewardMean': 0.6718794739007233, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.8617175785075
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=72.95
