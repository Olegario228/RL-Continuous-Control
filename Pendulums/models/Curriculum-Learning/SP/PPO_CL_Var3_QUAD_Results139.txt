#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 139
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8642542992003552, 'errorList': [], 'lossList': [0.0, -1.4232628375291825, 0.0, 30.018079788684844, 0.0, 0.0, 0.0], 'rewardMean': 0.7680848526718493, 'totalEpisodes': 13, 'stepsPerEpisode': 317, 'rewardPerEpisode': 252.12487540345728
'totalSteps': 3840, 'rewardStep': 0.7403426377606603, 'errorList': [], 'lossList': [0.0, -1.4385573834180831, 0.0, 31.32720592021942, 0.0, 0.0, 0.0], 'rewardMean': 0.758837447701453, 'totalEpisodes': 16, 'stepsPerEpisode': 170, 'rewardPerEpisode': 128.66919897081254
'totalSteps': 5120, 'rewardStep': 0.6273436046093894, 'errorList': [], 'lossList': [0.0, -1.4441673636436463, 0.0, 22.33675212264061, 0.0, 0.0, 0.0], 'rewardMean': 0.7259639869284371, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 955.4046058842837
'totalSteps': 6400, 'rewardStep': 0.8179851410062368, 'errorList': [], 'lossList': [0.0, -1.424668173789978, 0.0, 34.990654816627504, 0.0, 0.0, 0.0], 'rewardMean': 0.744368217743997, 'totalEpisodes': 18, 'stepsPerEpisode': 866, 'rewardPerEpisode': 686.5742695970686
'totalSteps': 7680, 'rewardStep': 0.9827729268788484, 'errorList': [], 'lossList': [0.0, -1.421746466755867, 0.0, 11.899266305565835, 0.0, 0.0, 0.0], 'rewardMean': 0.7841023359331389, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 999.5198268641269
'totalSteps': 8960, 'rewardStep': 0.7528702084700327, 'errorList': [], 'lossList': [0.0, -1.4064811235666275, 0.0, 33.681146466732024, 0.0, 0.0, 0.0], 'rewardMean': 0.7796406034384095, 'totalEpisodes': 20, 'stepsPerEpisode': 310, 'rewardPerEpisode': 240.72098873558718
'totalSteps': 10240, 'rewardStep': 0.8008732360443616, 'errorList': [], 'lossList': [0.0, -1.3980105477571487, 0.0, 93.65876182079315, 0.0, 0.0, 0.0], 'rewardMean': 0.7822946825141535, 'totalEpisodes': 25, 'stepsPerEpisode': 410, 'rewardPerEpisode': 275.8099185108079
'totalSteps': 11520, 'rewardStep': 0.9156049639491342, 'errorList': [], 'lossList': [0.0, -1.386852040886879, 0.0, 316.4176026916504, 0.0, 0.0, 0.0], 'rewardMean': 0.7971069360069291, 'totalEpisodes': 64, 'stepsPerEpisode': 31, 'rewardPerEpisode': 27.908349466427442
'totalSteps': 12800, 'rewardStep': 0.8955259209854303, 'errorList': [], 'lossList': [0.0, -1.387297316789627, 0.0, 161.33077095031737, 0.0, 0.0, 0.0], 'rewardMean': 0.8069488345047791, 'totalEpisodes': 93, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8955259209854303
'totalSteps': 14080, 'rewardStep': 0.5290625003901395, 'errorList': [], 'lossList': [0.0, -1.391915135383606, 0.0, 66.41636768341064, 0.0, 0.0, 0.0], 'rewardMean': 0.7926635439294588, 'totalEpisodes': 110, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.803838599878463
'totalSteps': 15360, 'rewardStep': 0.7559756678645132, 'errorList': [], 'lossList': [0.0, -1.3960675525665283, 0.0, 50.567903175354004, 0.0, 0.0, 0.0], 'rewardMean': 0.7818356807958746, 'totalEpisodes': 126, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.5347647572863308
'totalSteps': 16640, 'rewardStep': 0.6500982056265375, 'errorList': [], 'lossList': [0.0, -1.396405538916588, 0.0, 52.377042322158815, 0.0, 0.0, 0.0], 'rewardMean': 0.7728112375824624, 'totalEpisodes': 141, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.835067728378686
'totalSteps': 17920, 'rewardStep': 0.6929233291864028, 'errorList': [], 'lossList': [0.0, -1.3879247087240218, 0.0, 33.75863811016083, 0.0, 0.0, 0.0], 'rewardMean': 0.7793692100401637, 'totalEpisodes': 148, 'stepsPerEpisode': 76, 'rewardPerEpisode': 64.35983694754151
'totalSteps': 19200, 'rewardStep': 0.6763069207298065, 'errorList': [], 'lossList': [0.0, -1.378996232151985, 0.0, 12.601880197525025, 0.0, 0.0, 0.0], 'rewardMean': 0.7652013880125207, 'totalEpisodes': 151, 'stepsPerEpisode': 367, 'rewardPerEpisode': 274.1620405474
'totalSteps': 20480, 'rewardStep': 0.696089900471752, 'errorList': [], 'lossList': [0.0, -1.3683611577749253, 0.0, 11.44868058681488, 0.0, 0.0, 0.0], 'rewardMean': 0.736533085371811, 'totalEpisodes': 154, 'stepsPerEpisode': 109, 'rewardPerEpisode': 76.36380796597471
'totalSteps': 21760, 'rewardStep': 0.8860330318382128, 'errorList': [], 'lossList': [0.0, -1.366229248046875, 0.0, 11.782590671777726, 0.0, 0.0, 0.0], 'rewardMean': 0.7498493677086289, 'totalEpisodes': 156, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.497786981924664
'totalSteps': 23040, 'rewardStep': 0.8339731837807329, 'errorList': [], 'lossList': [0.0, -1.3703027027845383, 0.0, 29.254750580787658, 0.0, 0.0, 0.0], 'rewardMean': 0.7531593624822662, 'totalEpisodes': 158, 'stepsPerEpisode': 620, 'rewardPerEpisode': 481.5175440749126
'totalSteps': 24320, 'rewardStep': 0.6641722350267777, 'errorList': [], 'lossList': [0.0, -1.363527112007141, 0.0, 3.3557894909381867, 0.0, 0.0, 0.0], 'rewardMean': 0.7280160895900305, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.8199268222158
'totalSteps': 25600, 'rewardStep': 0.9682072132299193, 'errorList': [0.04261236613385588, 0.041252436069314134, 0.03086706727958939, 0.06822103380106466, 0.030883220885374714, 0.030921254105147396, 0.030920980329966517, 0.030844358572059263, 0.06789497600958229, 0.058319385111617765, 0.05084812579187567, 0.030890649217121962, 0.05152226183582291, 0.030847304532466605, 0.06339703052073296, 0.08129352189397702, 0.030945607607943137, 0.06057767399051767, 0.04380152372379159, 0.043522793959739375, 0.030762853602064228, 0.037329966272019136, 0.030900387856993658, 0.03175613176555885, 0.041018214931022874, 0.030873240338712176, 0.030808503751170686, 0.030816375876845273, 0.038550690358947155, 0.037423048223183486, 0.06298334868541358, 0.030803040575771885, 0.030866973296253656, 0.06379345752508452, 0.033446573642616384, 0.03423101588424, 0.03095649493197985, 0.039761002017935775, 0.031374215839087255, 0.030847872816777408, 0.03315445976214833, 0.030804839808637918, 0.030896221925447154, 0.03500203861753431, 0.030837959497368204, 0.03078444398345322, 0.04663657900351075, 0.030833427929034263, 0.04980859528372886, 0.03092423128105117], 'lossList': [0.0, -1.3420544576644897, 0.0, 3.4154732367396354, 0.0, 0.0, 0.0], 'rewardMean': 0.7352842188144794, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.7971257329868, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=75.51
