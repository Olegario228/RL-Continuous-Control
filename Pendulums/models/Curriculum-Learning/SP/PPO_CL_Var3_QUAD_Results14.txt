#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 14
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8798275821151775, 'errorList': [], 'lossList': [0.0, -1.4219279837608338, 0.0, 29.818697711229323, 0.0, 0.0, 0.0], 'rewardMean': 0.7758714941292604, 'totalEpisodes': 13, 'stepsPerEpisode': 318, 'rewardPerEpisode': 250.8757530646766
'totalSteps': 3840, 'rewardStep': 0.5877694232961979, 'errorList': [], 'lossList': [0.0, -1.431461877822876, 0.0, 26.583825602531434, 0.0, 0.0, 0.0], 'rewardMean': 0.7131708038515728, 'totalEpisodes': 16, 'stepsPerEpisode': 186, 'rewardPerEpisode': 127.69651472280442
'totalSteps': 5120, 'rewardStep': 0.4467437379273784, 'errorList': [], 'lossList': [0.0, -1.4296118193864822, 0.0, 63.92550700187683, 0.0, 0.0, 0.0], 'rewardMean': 0.6465640373705243, 'totalEpisodes': 28, 'stepsPerEpisode': 237, 'rewardPerEpisode': 171.5402851770571
'totalSteps': 6400, 'rewardStep': 0.4800327758862393, 'errorList': [], 'lossList': [0.0, -1.4226975375413895, 0.0, 144.02632801055907, 0.0, 0.0, 0.0], 'rewardMean': 0.6132577850736672, 'totalEpisodes': 85, 'stepsPerEpisode': 39, 'rewardPerEpisode': 27.292633932682673
'totalSteps': 7680, 'rewardStep': 0.8536498922369992, 'errorList': [], 'lossList': [0.0, -1.4112357372045516, 0.0, 86.18380016326904, 0.0, 0.0, 0.0], 'rewardMean': 0.6533231362675559, 'totalEpisodes': 133, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.654455296574966
'totalSteps': 8960, 'rewardStep': 0.5566824836926082, 'errorList': [], 'lossList': [0.0, -1.401575564146042, 0.0, 54.01939558982849, 0.0, 0.0, 0.0], 'rewardMean': 0.6395173287568491, 'totalEpisodes': 157, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.728544188211774
'totalSteps': 10240, 'rewardStep': 0.5422208050592676, 'errorList': [], 'lossList': [0.0, -1.3903554141521455, 0.0, 40.619587173461916, 0.0, 0.0, 0.0], 'rewardMean': 0.6273552632946514, 'totalEpisodes': 168, 'stepsPerEpisode': 200, 'rewardPerEpisode': 138.29701651993696
'totalSteps': 11520, 'rewardStep': 0.5723209041138901, 'errorList': [], 'lossList': [0.0, -1.3745447099208832, 0.0, 27.975105156898497, 0.0, 0.0, 0.0], 'rewardMean': 0.6212403344967891, 'totalEpisodes': 174, 'stepsPerEpisode': 155, 'rewardPerEpisode': 107.22506861930638
'totalSteps': 12800, 'rewardStep': 0.7092272224041853, 'errorList': [], 'lossList': [0.0, -1.3598424619436265, 0.0, 40.80296026706696, 0.0, 0.0, 0.0], 'rewardMean': 0.6300390232875286, 'totalEpisodes': 183, 'stepsPerEpisode': 89, 'rewardPerEpisode': 62.01346391745365
'totalSteps': 14080, 'rewardStep': 0.9247990884616244, 'errorList': [], 'lossList': [0.0, -1.3396593368053435, 0.0, 16.450104457139968, 0.0, 0.0, 0.0], 'rewardMean': 0.6553273915193568, 'totalEpisodes': 186, 'stepsPerEpisode': 314, 'rewardPerEpisode': 271.301500636212
'totalSteps': 15360, 'rewardStep': 0.8592754345463967, 'errorList': [], 'lossList': [0.0, -1.3122746616601944, 0.0, 32.69309614658356, 0.0, 0.0, 0.0], 'rewardMean': 0.6532721767624786, 'totalEpisodes': 193, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.058155371935254
'totalSteps': 16640, 'rewardStep': 0.35214045588872517, 'errorList': [], 'lossList': [0.0, -1.2931498104333878, 0.0, 7.887362710237503, 0.0, 0.0, 0.0], 'rewardMean': 0.6297092800217314, 'totalEpisodes': 196, 'stepsPerEpisode': 171, 'rewardPerEpisode': 123.02204228603865
'totalSteps': 17920, 'rewardStep': 0.8250822055118676, 'errorList': [], 'lossList': [0.0, -1.288871129155159, 0.0, 7.954335314631462, 0.0, 0.0, 0.0], 'rewardMean': 0.6675431267801804, 'totalEpisodes': 200, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.446401343014592
'totalSteps': 19200, 'rewardStep': 0.9108500376792595, 'errorList': [], 'lossList': [0.0, -1.2937731170654296, 0.0, 3.6201858618855476, 0.0, 0.0, 0.0], 'rewardMean': 0.7106248529594825, 'totalEpisodes': 203, 'stepsPerEpisode': 158, 'rewardPerEpisode': 141.933501379562
'totalSteps': 20480, 'rewardStep': 0.9903715707268069, 'errorList': [0.0853129337307878, 1.1851789030169517, 0.13453051370234234, 0.7800817975552666, 2.948470762022601, 3.8824871660076488, 0.3530718767677461, 1.0693028372263822, 1.8651555249096154, 1.610817059070953, 2.552872929787747, 0.9761548694522568, 3.2173882749589677, 0.48399920373058647, 1.8482786312385795, 1.3945904363007056, 1.5865562268953186, 3.6250287945018327, 1.9717634391755745, 3.31106027626588, 0.3080592361564368, 0.7777595007949774, 0.4734326535983621, 3.6381699424521297, 0.09493074551108674, 2.3296583814127905, 0.5436763498012951, 3.053923367628848, 2.497273453391164, 1.6355629335095687, 1.676449021319527, 1.3236037167177888, 0.2536830446410476, 2.22497535465526, 1.6513440840310964, 1.5857048561160403, 0.1212488083011127, 0.946335025491423, 4.116712203927106, 0.06444265031527696, 0.4409863608301161, 1.2650041319449106, 1.5725324663112863, 0.06651706560970014, 0.5310210177243224, 2.667135126582689, 1.8703457692789585, 1.0889080274073881, 2.1630438304295776, 0.1655943446521499], 'lossList': [0.0, -1.2918624663352967, 0.0, 5.70947588801384, 0.0, 0.0, 0.0], 'rewardMean': 0.7242970208084631, 'totalEpisodes': 206, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.02618994548688, 'successfulTests': 7
'totalSteps': 21760, 'rewardStep': 0.8903333551760925, 'errorList': [], 'lossList': [0.0, -1.2773682475090027, 0.0, 2.7869639936089516, 0.0, 0.0, 0.0], 'rewardMean': 0.7576621079568115, 'totalEpisodes': 208, 'stepsPerEpisode': 254, 'rewardPerEpisode': 222.09548930911092
'totalSteps': 23040, 'rewardStep': 0.7819072965226517, 'errorList': [], 'lossList': [0.0, -1.266015992164612, 0.0, 1.9781985077261925, 0.0, 0.0, 0.0], 'rewardMean': 0.78163075710315, 'totalEpisodes': 210, 'stepsPerEpisode': 318, 'rewardPerEpisode': 270.1779987220811
'totalSteps': 24320, 'rewardStep': 0.6801398334519451, 'errorList': [], 'lossList': [0.0, -1.270939285159111, 0.0, 2.0913096004724503, 0.0, 0.0, 0.0], 'rewardMean': 0.7924126500369556, 'totalEpisodes': 212, 'stepsPerEpisode': 295, 'rewardPerEpisode': 230.79895957357505
'totalSteps': 25600, 'rewardStep': 0.6675723669382698, 'errorList': [], 'lossList': [0.0, -1.2599333643913269, 0.0, 0.9057846508920193, 0.0, 0.0, 0.0], 'rewardMean': 0.7882471644903639, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1047.8880182747641
#maxSuccessfulTests=7, maxSuccessfulTestsAtStep=20480, timeSpent=74.32
