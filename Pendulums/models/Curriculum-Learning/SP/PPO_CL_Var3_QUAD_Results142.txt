#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 142
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.8538111018349037, 'errorList': [], 'lossList': [0.0, -1.4428253823518753, 0.0, 27.467425585985183, 0.0, 0.0, 0.0], 'rewardMean': 0.6657984385288509, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 652.9733864511031
'totalSteps': 3840, 'rewardStep': 0.8534972442047359, 'errorList': [], 'lossList': [0.0, -1.4601209634542465, 0.0, 41.86897357940674, 0.0, 0.0, 0.0], 'rewardMean': 0.7283647070874792, 'totalEpisodes': 12, 'stepsPerEpisode': 667, 'rewardPerEpisode': 513.6664909996364
'totalSteps': 5120, 'rewardStep': 0.8425173533940825, 'errorList': [], 'lossList': [0.0, -1.4650429719686509, 0.0, 23.786618852615355, 0.0, 0.0, 0.0], 'rewardMean': 0.75690286866413, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 966.3675131915658
'totalSteps': 6400, 'rewardStep': 0.5348410115885109, 'errorList': [], 'lossList': [0.0, -1.4472281247377397, 0.0, 28.96703348696232, 0.0, 0.0, 0.0], 'rewardMean': 0.7124904972490063, 'totalEpisodes': 13, 'stepsPerEpisode': 574, 'rewardPerEpisode': 442.73527753882723
'totalSteps': 7680, 'rewardStep': 0.6695708368958309, 'errorList': [], 'lossList': [0.0, -1.4434437495470047, 0.0, 27.417536237239837, 0.0, 0.0, 0.0], 'rewardMean': 0.705337220523477, 'totalEpisodes': 14, 'stepsPerEpisode': 829, 'rewardPerEpisode': 641.4231960075172
'totalSteps': 8960, 'rewardStep': 0.8399072106483921, 'errorList': [], 'lossList': [0.0, -1.4629956716299057, 0.0, 39.39673291325569, 0.0, 0.0, 0.0], 'rewardMean': 0.7245615048270364, 'totalEpisodes': 16, 'stepsPerEpisode': 363, 'rewardPerEpisode': 262.1468015725919
'totalSteps': 10240, 'rewardStep': 0.7849627936526045, 'errorList': [], 'lossList': [0.0, -1.462597854733467, 0.0, 36.33600703954696, 0.0, 0.0, 0.0], 'rewardMean': 0.7321116659302322, 'totalEpisodes': 19, 'stepsPerEpisode': 273, 'rewardPerEpisode': 208.47187359968785
'totalSteps': 11520, 'rewardStep': 0.7087837581274884, 'errorList': [], 'lossList': [0.0, -1.464223735332489, 0.0, 475.13669212341307, 0.0, 0.0, 0.0], 'rewardMean': 0.7295196761743719, 'totalEpisodes': 63, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.630875633305678
'totalSteps': 12800, 'rewardStep': 0.7755727431233175, 'errorList': [], 'lossList': [0.0, -1.4630453866720199, 0.0, 178.4058249282837, 0.0, 0.0, 0.0], 'rewardMean': 0.7341249828692664, 'totalEpisodes': 106, 'stepsPerEpisode': 76, 'rewardPerEpisode': 68.84498247675154
'totalSteps': 14080, 'rewardStep': 0.8336512460108285, 'errorList': [], 'lossList': [0.0, -1.4618636113405228, 0.0, 85.00372089385986, 0.0, 0.0, 0.0], 'rewardMean': 0.7697115299480696, 'totalEpisodes': 141, 'stepsPerEpisode': 22, 'rewardPerEpisode': 20.199085793526052
'totalSteps': 15360, 'rewardStep': 0.5032003435710508, 'errorList': [], 'lossList': [0.0, -1.4674013191461563, 0.0, 49.40039722442627, 0.0, 0.0, 0.0], 'rewardMean': 0.7346504541216842, 'totalEpisodes': 161, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.442046578416985
'totalSteps': 16640, 'rewardStep': 0.5870619518446354, 'errorList': [], 'lossList': [0.0, -1.4772916370630265, 0.0, 31.544140028953553, 0.0, 0.0, 0.0], 'rewardMean': 0.7080069248856742, 'totalEpisodes': 173, 'stepsPerEpisode': 105, 'rewardPerEpisode': 81.40729583250148
'totalSteps': 17920, 'rewardStep': 0.6284881692160335, 'errorList': [], 'lossList': [0.0, -1.471345857977867, 0.0, 34.62734390735626, 0.0, 0.0, 0.0], 'rewardMean': 0.6866040064678691, 'totalEpisodes': 180, 'stepsPerEpisode': 86, 'rewardPerEpisode': 69.2507030097903
'totalSteps': 19200, 'rewardStep': 0.9215579391419844, 'errorList': [], 'lossList': [0.0, -1.4463229709863663, 0.0, 16.821470119953155, 0.0, 0.0, 0.0], 'rewardMean': 0.7252756992232166, 'totalEpisodes': 183, 'stepsPerEpisode': 84, 'rewardPerEpisode': 76.56336540135729
'totalSteps': 20480, 'rewardStep': 0.6607535189613086, 'errorList': [], 'lossList': [0.0, -1.421899829506874, 0.0, 18.48260462999344, 0.0, 0.0, 0.0], 'rewardMean': 0.7243939674297643, 'totalEpisodes': 185, 'stepsPerEpisode': 442, 'rewardPerEpisode': 353.6390916572454
'totalSteps': 21760, 'rewardStep': 0.7181525562585805, 'errorList': [], 'lossList': [0.0, -1.397917714715004, 0.0, 8.683895356059075, 0.0, 0.0, 0.0], 'rewardMean': 0.7122185019907831, 'totalEpisodes': 188, 'stepsPerEpisode': 46, 'rewardPerEpisode': 37.58777792574797
'totalSteps': 23040, 'rewardStep': 0.892751479544491, 'errorList': [], 'lossList': [0.0, -1.379599494934082, 0.0, 5.705702960789203, 0.0, 0.0, 0.0], 'rewardMean': 0.7229973705799717, 'totalEpisodes': 190, 'stepsPerEpisode': 119, 'rewardPerEpisode': 108.9654558825177
'totalSteps': 24320, 'rewardStep': 0.8701207005354152, 'errorList': [], 'lossList': [0.0, -1.3618667429685594, 0.0, 2.7298930698633193, 0.0, 0.0, 0.0], 'rewardMean': 0.7391310648207645, 'totalEpisodes': 191, 'stepsPerEpisode': 980, 'rewardPerEpisode': 827.3628162913708
'totalSteps': 25600, 'rewardStep': 0.4630778358766291, 'errorList': [], 'lossList': [0.0, -1.347059519290924, 0.0, 2.8399053397774696, 0.0, 0.0, 0.0], 'rewardMean': 0.7078815740960958, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.0634002374558
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=53.04
