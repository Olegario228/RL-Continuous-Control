#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 143
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5844740517567187, 'errorList': [], 'lossList': [0.0, -1.4216894906759263, 0.0, 30.713602151870727, 0.0, 0.0, 0.0], 'rewardMean': 0.7213430801561291, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.355871653066867
'totalSteps': 3840, 'rewardStep': 0.9719090722824756, 'errorList': [], 'lossList': [0.0, -1.4212014549970626, 0.0, 31.081967223882675, 0.0, 0.0, 0.0], 'rewardMean': 0.804865077531578, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 387.9935067201007
'totalSteps': 5120, 'rewardStep': 0.8201044155973318, 'errorList': [], 'lossList': [0.0, -1.417560755610466, 0.0, 29.214772979319097, 0.0, 0.0, 0.0], 'rewardMean': 0.8086749120480164, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.0263467423129
'totalSteps': 6400, 'rewardStep': 0.7120523804303016, 'errorList': [], 'lossList': [0.0, -1.399943642616272, 0.0, 19.965761312842368, 0.0, 0.0, 0.0], 'rewardMean': 0.7893504057244736, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.9353270086117
'totalSteps': 7680, 'rewardStep': 0.9598505315800168, 'errorList': [], 'lossList': [0.0, -1.3748533880710603, 0.0, 15.468609986901283, 0.0, 0.0, 0.0], 'rewardMean': 0.8177670933670641, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.8536888051456
'totalSteps': 8960, 'rewardStep': 0.8061194283631584, 'errorList': [], 'lossList': [0.0, -1.3731969040632248, 0.0, 8.480465006232262, 0.0, 0.0, 0.0], 'rewardMean': 0.8161031412236489, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.5545314209437
'totalSteps': 10240, 'rewardStep': 0.9332356801862586, 'errorList': [192.23911750533958, 167.19966944571578, 199.10516992869745, 219.15000291731346, 202.787352742394, 214.8699249847468, 215.92415791716965, 178.31577254390416, 151.24138876057475, 198.8578027531709, 192.9706045445612, 158.5481156546508, 219.42228986155212, 208.29288638760488, 180.0984893602447, 201.97129432164553, 211.95008461529466, 218.4633213661004, 207.24269030440487, 194.31135401247425, 208.41310604744552, 205.06617430135174, 222.43457552494598, 203.19102572904285, 196.50618557516685, 184.08019601996023, 210.77214258396293, 185.62575725436685, 173.11246262844406, 198.8794640247928, 208.73612895298476, 168.95961461502256, 203.42931721862914, 202.59630248905538, 157.4710481978654, 201.68640650639372, 203.78146649011632, 225.2151571455003, 203.18859457440996, 179.09788848299291, 166.83833751574812, 209.15473031891477, 161.10640518043164, 225.05353609471362, 167.577088234643, 217.2212090367477, 208.83874607982978, 205.61844817165704, 192.0097394004335, 211.28469823334274], 'lossList': [0.0, -1.3614162826538085, 0.0, 52.172944991588594, 0.0, 0.0, 0.0], 'rewardMean': 0.8307447085939752, 'totalEpisodes': 20, 'stepsPerEpisode': 138, 'rewardPerEpisode': 124.54156553222877, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6340921601989922, 'errorList': [], 'lossList': [0.0, -1.3603383833169937, 0.0, 325.8929421615601, 0.0, 0.0, 0.0], 'rewardMean': 0.8088944254389772, 'totalEpisodes': 53, 'stepsPerEpisode': 38, 'rewardPerEpisode': 29.940688543211674
'totalSteps': 12800, 'rewardStep': 0.5589400593903682, 'errorList': [], 'lossList': [0.0, -1.3581441897153854, 0.0, 287.67831092834473, 0.0, 0.0, 0.0], 'rewardMean': 0.7838989888341162, 'totalEpisodes': 91, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.820736094999774
'totalSteps': 14080, 'rewardStep': 0.8104862303737811, 'errorList': [], 'lossList': [0.0, -1.355687854886055, 0.0, 68.08478651046752, 0.0, 0.0, 0.0], 'rewardMean': 0.7791264010159404, 'totalEpisodes': 126, 'stepsPerEpisode': 38, 'rewardPerEpisode': 32.114135508110046
'totalSteps': 15360, 'rewardStep': 0.5668830636779054, 'errorList': [], 'lossList': [0.0, -1.3562763440608978, 0.0, 28.76232752799988, 0.0, 0.0, 0.0], 'rewardMean': 0.7773673022080589, 'totalEpisodes': 162, 'stepsPerEpisode': 29, 'rewardPerEpisode': 22.590609965530458
'totalSteps': 16640, 'rewardStep': 0.7963395663574744, 'errorList': [], 'lossList': [0.0, -1.3554590719938278, 0.0, 21.284379286766054, 0.0, 0.0, 0.0], 'rewardMean': 0.7598103516155589, 'totalEpisodes': 178, 'stepsPerEpisode': 47, 'rewardPerEpisode': 40.7094980474169
'totalSteps': 17920, 'rewardStep': 0.6519218670518069, 'errorList': [], 'lossList': [0.0, -1.3530311781167983, 0.0, 16.784684736728668, 0.0, 0.0, 0.0], 'rewardMean': 0.7429920967610063, 'totalEpisodes': 188, 'stepsPerEpisode': 128, 'rewardPerEpisode': 100.88594218059922
'totalSteps': 19200, 'rewardStep': 0.7451064110983989, 'errorList': [], 'lossList': [0.0, -1.3492572355270385, 0.0, 23.235089778900146, 0.0, 0.0, 0.0], 'rewardMean': 0.7462974998278161, 'totalEpisodes': 193, 'stepsPerEpisode': 241, 'rewardPerEpisode': 148.03289381132763
'totalSteps': 20480, 'rewardStep': 0.7676156814437118, 'errorList': [], 'lossList': [0.0, -1.3320551490783692, 0.0, 15.439735586643218, 0.0, 0.0, 0.0], 'rewardMean': 0.7270740148141857, 'totalEpisodes': 195, 'stepsPerEpisode': 307, 'rewardPerEpisode': 236.44161335163062
'totalSteps': 21760, 'rewardStep': 0.9015236219777802, 'errorList': [], 'lossList': [0.0, -1.3100947552919389, 0.0, 6.591083163022995, 0.0, 0.0, 0.0], 'rewardMean': 0.7366144341756478, 'totalEpisodes': 200, 'stepsPerEpisode': 87, 'rewardPerEpisode': 81.1010895391687
'totalSteps': 23040, 'rewardStep': 0.5066930350201733, 'errorList': [], 'lossList': [0.0, -1.3029257106781005, 0.0, 5.17639619231224, 0.0, 0.0, 0.0], 'rewardMean': 0.6939601696590392, 'totalEpisodes': 203, 'stepsPerEpisode': 179, 'rewardPerEpisode': 137.83179846346096
'totalSteps': 24320, 'rewardStep': 0.6496638782677514, 'errorList': [], 'lossList': [0.0, -1.29040977537632, 0.0, 4.517414702773094, 0.0, 0.0, 0.0], 'rewardMean': 0.695517341465915, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 860.1100416405399
'totalSteps': 25600, 'rewardStep': 0.9505717240242341, 'errorList': [0.10323507798219013, 0.09883944783165431, 0.09946333635664527, 0.10191003924273442, 0.13332133197894352, 0.13995559008377775, 0.09835103010180062, 0.1161941658811829, 0.10056569445422874, 0.15415822633155782, 0.10427961015538374, 0.09283685450087863, 0.10091413431073316, 0.09633309704260035, 0.10368181490196186, 0.10084827268278786, 0.10289940354469537, 0.0971136612872408, 0.09683343569202991, 0.09423664543870547, 0.09956320351976089, 0.09872346626291457, 0.10205228225422784, 0.1708415520373554, 0.1005582406670733, 0.10339760861143764, 0.1204333900995489, 0.09758549322158339, 0.09647173635081083, 0.1004871089504997, 0.10235788636744379, 0.09762806605319581, 0.09955338012518185, 0.1033743534566833, 0.09646578094050604, 0.16103648089761913, 0.10019097724110067, 0.15527032111781267, 0.10044724102781227, 0.09689605206348918, 0.14554809865967863, 0.10162827916566611, 0.14250879368203673, 0.16449433496416718, 0.20046444099349064, 0.15754920474842846, 0.10197220432066119, 0.10313061388304266, 0.09647655235417404, 0.10066708881024071], 'lossList': [0.0, -1.2513824945688248, 0.0, 2.7546825413405895, 0.0, 0.0, 0.0], 'rewardMean': 0.7346805079293017, 'totalEpisodes': 203, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.2077727210367, 'successfulTests': 49
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=25600, timeSpent=71.87
