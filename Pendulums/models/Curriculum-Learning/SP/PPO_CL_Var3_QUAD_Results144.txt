#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 144
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8780611204406642, 'errorList': [], 'lossList': [0.0, -1.4221542847156525, 0.0, 33.320491117239, 0.0, 0.0, 0.0], 'rewardMean': 0.8007060221599825, 'totalEpisodes': 13, 'stepsPerEpisode': 314, 'rewardPerEpisode': 261.69067384353565
'totalSteps': 3840, 'rewardStep': 0.7592869052503398, 'errorList': [], 'lossList': [0.0, -1.4245879346132277, 0.0, 32.308069385290146, 0.0, 0.0, 0.0], 'rewardMean': 0.7868996498567683, 'totalEpisodes': 15, 'stepsPerEpisode': 198, 'rewardPerEpisode': 134.29153094672733
'totalSteps': 5120, 'rewardStep': 0.6203461047757995, 'errorList': [], 'lossList': [0.0, -1.4281752634048461, 0.0, 18.502803905010225, 0.0, 0.0, 0.0], 'rewardMean': 0.7452612635865261, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 903.3207044452593
'totalSteps': 6400, 'rewardStep': 0.8498371809704522, 'errorList': [], 'lossList': [0.0, -1.4214864242076874, 0.0, 16.558759784698488, 0.0, 0.0, 0.0], 'rewardMean': 0.7661764470633113, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 931.4908023702078
'totalSteps': 7680, 'rewardStep': 0.8078920818377671, 'errorList': [], 'lossList': [0.0, -1.4267335027456283, 0.0, 25.494725831747054, 0.0, 0.0, 0.0], 'rewardMean': 0.773129052859054, 'totalEpisodes': 17, 'stepsPerEpisode': 214, 'rewardPerEpisode': 149.71166843691438
'totalSteps': 8960, 'rewardStep': 0.6009183574457725, 'errorList': [], 'lossList': [0.0, -1.4273266929388047, 0.0, 80.51930438995362, 0.0, 0.0, 0.0], 'rewardMean': 0.7485275249428709, 'totalEpisodes': 23, 'stepsPerEpisode': 214, 'rewardPerEpisode': 157.57321169956
'totalSteps': 10240, 'rewardStep': 0.7829647954402613, 'errorList': [], 'lossList': [0.0, -1.420924469232559, 0.0, 165.8276382446289, 0.0, 0.0, 0.0], 'rewardMean': 0.7528321837550447, 'totalEpisodes': 36, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.57654959573526
'totalSteps': 11520, 'rewardStep': 0.5491563989771692, 'errorList': [], 'lossList': [0.0, -1.4153722548484802, 0.0, 277.356131439209, 0.0, 0.0, 0.0], 'rewardMean': 0.7302015410019475, 'totalEpisodes': 75, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.154464291319051
'totalSteps': 12800, 'rewardStep': 0.8029146971305792, 'errorList': [], 'lossList': [0.0, -1.4099240177869796, 0.0, 116.62465166091918, 0.0, 0.0, 0.0], 'rewardMean': 0.7374728566148107, 'totalEpisodes': 106, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.2214768387169537
'totalSteps': 14080, 'rewardStep': 0.9174000613291793, 'errorList': [], 'lossList': [0.0, -1.4057785332202912, 0.0, 105.84450077056884, 0.0, 0.0, 0.0], 'rewardMean': 0.7568777703597984, 'totalEpisodes': 132, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.137156986891798
'totalSteps': 15360, 'rewardStep': 0.5970818090039277, 'errorList': [], 'lossList': [0.0, -1.4043281346559524, 0.0, 50.05570695877075, 0.0, 0.0, 0.0], 'rewardMean': 0.7287798392161248, 'totalEpisodes': 144, 'stepsPerEpisode': 84, 'rewardPerEpisode': 61.56627363542233
'totalSteps': 16640, 'rewardStep': 0.9390872308922332, 'errorList': [1.2385585171482876, 74.41100560363647, 96.72862102358965, 107.10462760796457, 107.46895950981012, 2.2835194284887184, 16.112954121121483, 72.5950630077342, 123.39630704799383, 48.155859253858836, 5.784307443280983, 24.1942509800067, 35.15876613337604, 67.93169221418492, 33.609918041314685, 110.3421581820869, 31.272751025042613, 42.25773847080564, 71.00571765906273, 112.05668128018456, 95.51409120155093, 64.84042950599606, 63.7462251349793, 132.17010804381462, 43.83616143469133, 125.2500602000229, 51.82808017458573, 75.82796474278103, 113.9336920340126, 8.666093741475937, 28.81357289010997, 91.82830582453327, 66.79601620586196, 83.35173960007178, 133.16530081307937, 97.20782239357918, 133.70744837836114, 141.29055078909406, 99.75687624168229, 150.48737921951403, 7.338806228992446, 12.230604866754659, 100.28613572675764, 126.61139510568218, 84.38420678408932, 149.0164379589905, 2.529889201912287, 49.47869278342886, 102.08582016944551, 144.96698093678535], 'lossList': [0.0, -1.404617395401001, 0.0, 79.52107191085815, 0.0, 0.0, 0.0], 'rewardMean': 0.7467598717803141, 'totalEpisodes': 161, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.3486347427677465, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.062421512778382815, 'errorList': [], 'lossList': [0.0, -1.403716779947281, 0.0, 15.911462483406067, 0.0, 0.0, 0.0], 'rewardMean': 0.6909674125805724, 'totalEpisodes': 169, 'stepsPerEpisode': 100, 'rewardPerEpisode': 53.76442590360368
'totalSteps': 19200, 'rewardStep': 0.6224702238819095, 'errorList': [], 'lossList': [0.0, -1.3873587936162948, 0.0, 30.28206624865532, 0.0, 0.0, 0.0], 'rewardMean': 0.6682307168717182, 'totalEpisodes': 175, 'stepsPerEpisode': 289, 'rewardPerEpisode': 236.6844100484617
'totalSteps': 20480, 'rewardStep': 0.8847110997594982, 'errorList': [], 'lossList': [0.0, -1.3713661468029021, 0.0, 18.601900422573088, 0.0, 0.0, 0.0], 'rewardMean': 0.6759126186638913, 'totalEpisodes': 182, 'stepsPerEpisode': 61, 'rewardPerEpisode': 50.065425894870906
'totalSteps': 21760, 'rewardStep': 0.7462342788119003, 'errorList': [], 'lossList': [0.0, -1.3565816849470138, 0.0, 8.53948405265808, 0.0, 0.0, 0.0], 'rewardMean': 0.690444210800504, 'totalEpisodes': 183, 'stepsPerEpisode': 676, 'rewardPerEpisode': 566.525711416816
'totalSteps': 23040, 'rewardStep': 0.5870707520529792, 'errorList': [], 'lossList': [0.0, -1.347901627421379, 0.0, 8.021847577095032, 0.0, 0.0, 0.0], 'rewardMean': 0.6708548064617759, 'totalEpisodes': 185, 'stepsPerEpisode': 284, 'rewardPerEpisode': 196.61222070549437
'totalSteps': 24320, 'rewardStep': 0.8174746774054401, 'errorList': [], 'lossList': [0.0, -1.3330113005638122, 0.0, 6.764542481899261, 0.0, 0.0, 0.0], 'rewardMean': 0.6976866343046029, 'totalEpisodes': 187, 'stepsPerEpisode': 189, 'rewardPerEpisode': 170.307694912362
'totalSteps': 25600, 'rewardStep': 0.7545995513784771, 'errorList': [], 'lossList': [0.0, -1.3225498884916305, 0.0, 5.000160249471665, 0.0, 0.0, 0.0], 'rewardMean': 0.6928551197293927, 'totalEpisodes': 188, 'stepsPerEpisode': 307, 'rewardPerEpisode': 243.38793956986922
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=63.16
