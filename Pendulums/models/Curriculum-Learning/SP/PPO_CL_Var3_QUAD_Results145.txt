#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 145
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9087049753418702, 'errorList': [], 'lossList': [0.0, -1.436891384124756, 0.0, 31.78382812857628, 0.0, 0.0, 0.0], 'rewardMean': 0.9117628029613492, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 382.3757565791294
'totalSteps': 3840, 'rewardStep': 0.7160349532816503, 'errorList': [], 'lossList': [0.0, -1.4219212394952774, 0.0, 30.58448260307312, 0.0, 0.0, 0.0], 'rewardMean': 0.8465201864014494, 'totalEpisodes': 12, 'stepsPerEpisode': 267, 'rewardPerEpisode': 208.92269818916503
'totalSteps': 5120, 'rewardStep': 0.7284753276613462, 'errorList': [], 'lossList': [0.0, -1.4208747249841691, 0.0, 25.1212877368927, 0.0, 0.0, 0.0], 'rewardMean': 0.8170089717164236, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 985.0401253082139
'totalSteps': 6400, 'rewardStep': 0.9437407807119212, 'errorList': [], 'lossList': [0.0, -1.4132634180784225, 0.0, 23.448724660873413, 0.0, 0.0, 0.0], 'rewardMean': 0.8423553335155232, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.8950808610434
'totalSteps': 7680, 'rewardStep': 0.6481049077053009, 'errorList': [], 'lossList': [0.0, -1.3970028448104859, 0.0, 14.789767116457224, 0.0, 0.0, 0.0], 'rewardMean': 0.8099802625471528, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.3909705117853
'totalSteps': 8960, 'rewardStep': 0.8603009216745926, 'errorList': [], 'lossList': [0.0, -1.3430363607406617, 0.0, 10.051210871934892, 0.0, 0.0, 0.0], 'rewardMean': 0.817168928136787, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.7575009152235
'totalSteps': 10240, 'rewardStep': 0.8526745980367776, 'errorList': [], 'lossList': [0.0, -1.334753737449646, 0.0, 177.72236129760742, 0.0, 0.0, 0.0], 'rewardMean': 0.8216071368742859, 'totalEpisodes': 21, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.369643170233074
'totalSteps': 11520, 'rewardStep': 0.8692006366918232, 'errorList': [], 'lossList': [0.0, -1.3377663904428483, 0.0, 655.4877406311035, 0.0, 0.0, 0.0], 'rewardMean': 0.8268953035206789, 'totalEpisodes': 73, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.506744301701243
'totalSteps': 12800, 'rewardStep': 0.29871917461234143, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7124675450329965, 'totalEpisodes': 120, 'stepsPerEpisode': 64, 'rewardPerEpisode': 48.79569736111232
'totalSteps': 14080, 'rewardStep': 0.7243404773417302, 'errorList': [], 'lossList': [0.0, -1.3376659047603607, 0.0, 381.55152130126953, 0.0, 0.0, 0.0], 'rewardMean': 0.6940310952329825, 'totalEpisodes': 167, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.978008927796285
'totalSteps': 15360, 'rewardStep': 0.5335214905926715, 'errorList': [], 'lossList': [0.0, -1.335194862484932, 0.0, 122.72522291183472, 0.0, 0.0, 0.0], 'rewardMean': 0.6757797489640847, 'totalEpisodes': 208, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.0763796352594968
'totalSteps': 16640, 'rewardStep': 0.6348582178142208, 'errorList': [], 'lossList': [0.0, -1.335150814652443, 0.0, 73.11561393737793, 0.0, 0.0, 0.0], 'rewardMean': 0.666418037979372, 'totalEpisodes': 240, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.689534136619747
'totalSteps': 17920, 'rewardStep': 0.9363297134455787, 'errorList': [252.56348698034793, 280.7916992466633, 251.01164800971475, 267.7674297301644, 229.63822592674143, 254.45761384098762, 273.051854128571, 280.17323098069573, 140.77667761059007, 275.03460119118233, 286.00052420103833, 281.36884099190195, 246.44230888157608, 250.8117253752416, 284.66278239574416, 247.14169159512807, 197.7219470788026, 270.8504908422013, 207.69131628890491, 277.7412150502425, 239.15592719927844, 266.3952417684181, 268.55252163393413, 274.47659081098925, 269.1934491788401, 244.69739223039957, 274.0358074556104, 248.4880435506449, 175.80785135565614, 252.08370160090473, 264.5262584219672, 275.1346822157442, 268.7461350034225, 287.3810691256145, 224.50150481560445, 203.62611692011947, 275.24662782914373, 239.849519488426, 252.05885933969236, 267.51805863794715, 247.7303455239717, 263.0567081796164, 236.92531762658012, 241.18941205711656, 192.0541800491447, 243.64445880640943, 250.52508766285527, 236.84996663475962, 256.0346686990851, 269.94950667595583], 'lossList': [0.0, -1.3385385644435883, 0.0, 25.552189955711366, 0.0, 0.0, 0.0], 'rewardMean': 0.6656769312527379, 'totalEpisodes': 260, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.819460142783395, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.7583156838111678, 'errorList': [], 'lossList': [0.0, -1.3365085512399673, 0.0, 43.93040601730347, 0.0, 0.0, 0.0], 'rewardMean': 0.6766980088633245, 'totalEpisodes': 290, 'stepsPerEpisode': 36, 'rewardPerEpisode': 26.439557536933
'totalSteps': 20480, 'rewardStep': 0.9358739194296974, 'errorList': [46.9683175020596, 45.844330826473914, 71.4540551484116, 12.496884561045794, 49.24725318344996, 14.79629198037417, 13.777956108860046, 21.85323243487787, 10.578174616152918, 43.326165960901044, 7.997200459969072, 12.556907806587297, 9.78599309817855, 8.086971784961644, 60.40436074411839, 15.433918465208727, 24.09404552735246, 47.986001486725996, 47.41004702554487, 9.93309231776395, 15.485434796081217, 28.61477036293604, 11.468811182442197, 19.980121257010378, 43.319964865613926, 11.468629076749709, 10.68126952069674, 42.10856589449512, 2.870036139741287, 14.804760805289122, 21.774763469335895, 17.467249739873026, 10.928979655208384, 4.013187811501449, 34.86030868984164, 7.9405203518911955, 43.41838538913664, 19.270736904384524, 2.9775109078074973, 46.676717759654224, 8.532729190206298, 11.878807181111428, 12.25271398025407, 48.16132591296935, 19.872393351505185, 22.8823238508828, 58.814335667750576, 12.31088470965442, 57.361096665582295, 8.305916078898465], 'lossList': [0.0, -1.3353650969266893, 0.0, 30.644275813102723, 0.0, 0.0, 0.0], 'rewardMean': 0.684255308638835, 'totalEpisodes': 310, 'stepsPerEpisode': 108, 'rewardPerEpisode': 89.48605803100938, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8723508924050819, 'errorList': [], 'lossList': [0.0, -1.3299155908823013, 0.0, 30.283556356430054, 0.0, 0.0, 0.0], 'rewardMean': 0.6862229380756656, 'totalEpisodes': 318, 'stepsPerEpisode': 60, 'rewardPerEpisode': 53.38824730305972
'totalSteps': 23040, 'rewardStep': 0.6973104663082226, 'errorList': [], 'lossList': [0.0, -1.3151427912712097, 0.0, 19.909537992477418, 0.0, 0.0, 0.0], 'rewardMean': 0.6690339210373054, 'totalEpisodes': 321, 'stepsPerEpisode': 404, 'rewardPerEpisode': 310.9973703901843
'totalSteps': 24320, 'rewardStep': 0.4939249348039666, 'errorList': [], 'lossList': [0.0, -1.281712207198143, 0.0, 7.96254675924778, 0.0, 0.0, 0.0], 'rewardMean': 0.688554497056468, 'totalEpisodes': 321, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 923.2203938337576
'totalSteps': 25600, 'rewardStep': 0.5847231326576072, 'errorList': [], 'lossList': [0.0, -1.2493751657009124, 0.0, 9.41120400905609, 0.0, 0.0, 0.0], 'rewardMean': 0.7171548928609944, 'totalEpisodes': 321, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 831.0976109213095
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=68.75
