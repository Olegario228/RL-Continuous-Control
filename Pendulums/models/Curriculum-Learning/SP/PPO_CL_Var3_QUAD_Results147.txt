#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 147
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8662504886108687, 'errorList': [], 'lossList': [0.0, -1.4421055901050568, 0.0, 30.042565891742708, 0.0, 0.0, 0.0], 'rewardMean': 0.6846756682574402, 'totalEpisodes': 10, 'stepsPerEpisode': 905, 'rewardPerEpisode': 674.6439946850139
'totalSteps': 3840, 'rewardStep': 0.8184453479729354, 'errorList': [], 'lossList': [0.0, -1.4604449236392976, 0.0, 42.71524368286133, 0.0, 0.0, 0.0], 'rewardMean': 0.7292655614959386, 'totalEpisodes': 12, 'stepsPerEpisode': 670, 'rewardPerEpisode': 522.3824941211965
'totalSteps': 5120, 'rewardStep': 0.8891204176077244, 'errorList': [], 'lossList': [0.0, -1.461834117770195, 0.0, 26.79695129275322, 0.0, 0.0, 0.0], 'rewardMean': 0.7692292755238851, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.0799872034172
'totalSteps': 6400, 'rewardStep': 0.6518035742653419, 'errorList': [], 'lossList': [0.0, -1.4516490012407304, 0.0, 22.546081656217574, 0.0, 0.0, 0.0], 'rewardMean': 0.7457441352721764, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.6503701962124
'totalSteps': 7680, 'rewardStep': 0.7120797404545762, 'errorList': [], 'lossList': [0.0, -1.4347963738441467, 0.0, 12.813220042586327, 0.0, 0.0, 0.0], 'rewardMean': 0.7401334028025763, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.5811790569786
'totalSteps': 8960, 'rewardStep': 0.8518571218004403, 'errorList': [], 'lossList': [0.0, -1.4265231114625931, 0.0, 10.82314218789339, 0.0, 0.0, 0.0], 'rewardMean': 0.7560939340879855, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.643737123198
'totalSteps': 10240, 'rewardStep': 0.8228515204329252, 'errorList': [], 'lossList': [0.0, -1.3962056314945221, 0.0, 7.138064379096031, 0.0, 0.0, 0.0], 'rewardMean': 0.764438632381103, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.7301866185696
'totalSteps': 11520, 'rewardStep': 0.8328632955635316, 'errorList': [], 'lossList': [0.0, -1.3874605703353882, 0.0, 703.3179454040527, 0.0, 0.0, 0.0], 'rewardMean': 0.7720413727347062, 'totalEpisodes': 51, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.700380595708111
'totalSteps': 12800, 'rewardStep': 0.6324971199363918, 'errorList': [], 'lossList': [0.0, -1.3867080563306808, 0.0, 433.32673484802245, 0.0, 0.0, 0.0], 'rewardMean': 0.7580869474548747, 'totalEpisodes': 83, 'stepsPerEpisode': 24, 'rewardPerEpisode': 15.613219196999644
'totalSteps': 14080, 'rewardStep': 0.7693946015162041, 'errorList': [], 'lossList': [0.0, -1.3858714556694032, 0.0, 355.0763471221924, 0.0, 0.0, 0.0], 'rewardMean': 0.7847163228160939, 'totalEpisodes': 123, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.10748021710378
'totalSteps': 15360, 'rewardStep': 0.8768648334782871, 'errorList': [], 'lossList': [0.0, -1.3859203988313675, 0.0, 122.99841997146606, 0.0, 0.0, 0.0], 'rewardMean': 0.7857777573028357, 'totalEpisodes': 159, 'stepsPerEpisode': 21, 'rewardPerEpisode': 19.746095980917236
'totalSteps': 16640, 'rewardStep': 0.5343339730968716, 'errorList': [], 'lossList': [0.0, -1.3882310253381729, 0.0, 75.11888319015503, 0.0, 0.0, 0.0], 'rewardMean': 0.7573666198152293, 'totalEpisodes': 193, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.50695686473016
'totalSteps': 17920, 'rewardStep': 0.9631655634316628, 'errorList': [225.96397974625114, 187.60978780711056, 178.1340520428665, 177.5988696029612, 283.829176449439, 231.81685077463834, 184.07346659610158, 312.7498994384581, 321.6475583026871, 276.0464362864105, 266.3220756261401, 282.5881184969902, 182.50832315075584, 274.2261641547459, 233.53776013438718, 301.3111557963746, 217.2233170662317, 334.28663552436115, 305.2904534601553, 305.95093806751925, 292.12263103222296, 239.73818616445453, 223.99787134801625, 299.16460809472113, 323.63693370453495, 231.953802937165, 282.17949115345203, 325.3641054040286, 263.2376644029239, 319.14525483670184, 249.17226035598375, 308.16131946258616, 321.0691336392656, 256.00156229784, 286.1688406516161, 327.95048247341066, 310.10503037055133, 261.58541603165355, 205.03342983419512, 250.53422337687334, 319.4249601954975, 278.06377379742537, 212.81233200136595, 314.5175541861422, 330.0792649034809, 254.09938900956374, 328.04847627652236, 298.76585328833875, 240.60676656320163, 239.90824165156153], 'lossList': [0.0, -1.382381848692894, 0.0, 42.31515019416809, 0.0, 0.0, 0.0], 'rewardMean': 0.7647711343976233, 'totalEpisodes': 222, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.783110977766114, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.7157437405721768, 'errorList': [], 'lossList': [0.0, -1.3727396845817565, 0.0, 35.171173105239866, 0.0, 0.0, 0.0], 'rewardMean': 0.7711651510283069, 'totalEpisodes': 242, 'stepsPerEpisode': 39, 'rewardPerEpisode': 29.637224948534282
'totalSteps': 20480, 'rewardStep': 0.6622812219578801, 'errorList': [], 'lossList': [0.0, -1.36797856092453, 0.0, 19.004051237106324, 0.0, 0.0, 0.0], 'rewardMean': 0.7661852991786371, 'totalEpisodes': 255, 'stepsPerEpisode': 43, 'rewardPerEpisode': 32.40944503432029
'totalSteps': 21760, 'rewardStep': 0.42655109715393136, 'errorList': [], 'lossList': [0.0, -1.3652172303199768, 0.0, 23.985253410339354, 0.0, 0.0, 0.0], 'rewardMean': 0.7236546967139862, 'totalEpisodes': 265, 'stepsPerEpisode': 169, 'rewardPerEpisode': 136.0468678087186
'totalSteps': 23040, 'rewardStep': 0.1971168840351495, 'errorList': [], 'lossList': [0.0, -1.3558847481012344, 0.0, 11.41875066280365, 0.0, 0.0, 0.0], 'rewardMean': 0.6610812330742086, 'totalEpisodes': 271, 'stepsPerEpisode': 215, 'rewardPerEpisode': 155.73874438507937
'totalSteps': 24320, 'rewardStep': 0.862818078874685, 'errorList': [], 'lossList': [0.0, -1.3456982201337815, 0.0, 12.84715917825699, 0.0, 0.0, 0.0], 'rewardMean': 0.6640767114053241, 'totalEpisodes': 277, 'stepsPerEpisode': 64, 'rewardPerEpisode': 55.129766916921106
'totalSteps': 25600, 'rewardStep': 0.4836103411250057, 'errorList': [], 'lossList': [0.0, -1.3393932485580444, 0.0, 8.204093868732452, 0.0, 0.0, 0.0], 'rewardMean': 0.6491880335241855, 'totalEpisodes': 280, 'stepsPerEpisode': 118, 'rewardPerEpisode': 85.94539792883813
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=52.85
