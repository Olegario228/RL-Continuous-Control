#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 148
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5734814774633837, 'errorList': [], 'lossList': [0.0, -1.4228311014175414, 0.0, 32.396563688516615, 0.0, 0.0, 0.0], 'rewardMean': 0.7362059466450621, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.191808663248466
'totalSteps': 3840, 'rewardStep': 0.9624419183609149, 'errorList': [], 'lossList': [0.0, -1.4237687772512435, 0.0, 34.81024846792221, 0.0, 0.0, 0.0], 'rewardMean': 0.811617937217013, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 400.5345918296024
'totalSteps': 5120, 'rewardStep': 0.8390188011292605, 'errorList': [], 'lossList': [0.0, -1.4204721838235854, 0.0, 31.43943962931633, 0.0, 0.0, 0.0], 'rewardMean': 0.8184681531950749, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.1527166643057
'totalSteps': 6400, 'rewardStep': 0.7810443102554049, 'errorList': [], 'lossList': [0.0, -1.4008024144172668, 0.0, 22.501428109407424, 0.0, 0.0, 0.0], 'rewardMean': 0.8109833846071408, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.1625406338153
'totalSteps': 7680, 'rewardStep': 0.9848464594247467, 'errorList': [], 'lossList': [0.0, -1.3828698599338531, 0.0, 16.007688530385494, 0.0, 0.0, 0.0], 'rewardMean': 0.8399605637434084, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.5160852152708
'totalSteps': 8960, 'rewardStep': 0.8011389852927687, 'errorList': [], 'lossList': [0.0, -1.3662160402536392, 0.0, 9.266811757385732, 0.0, 0.0, 0.0], 'rewardMean': 0.8344146239647456, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.148120388039
'totalSteps': 10240, 'rewardStep': 0.936242962223277, 'errorList': [203.57921125618577, 176.44353085922077, 211.93217062417872, 230.7604038739896, 214.47339604592142, 225.8640259365237, 228.1032990951818, 181.14634102570474, 170.28300776916737, 211.6777953843888, 202.60691483432598, 171.51598612014314, 231.2995181988107, 221.30774054845364, 192.68048073025255, 211.49318201712538, 225.69741370869278, 230.29108099561583, 219.85887218518502, 205.6382091686237, 221.37080965958052, 217.34414582322182, 234.61482583172332, 215.33211748365324, 205.82226247946033, 192.5066829208037, 221.78420197176095, 200.07169647397265, 173.7331661466729, 210.28204411619097, 221.63994779694406, 183.26367808525148, 213.84315665218705, 213.31503268367857, 163.33330291710195, 213.03551991664187, 215.97063109201235, 237.6093633262274, 214.16231714485082, 187.34580351406345, 177.08248961419127, 221.07365009484585, 152.4738836507612, 237.58492928750357, 175.36432513353782, 228.50344208633572, 221.9649311831619, 218.18091129362605, 201.62368570050253, 222.38628545495416], 'lossList': [0.0, -1.3477314299345016, 0.0, 5.665117074549198, 0.0, 0.0, 0.0], 'rewardMean': 0.8471431662470621, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.7565096167814, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5771450536892613, 'errorList': [], 'lossList': [0.0, -1.3440455621480942, 0.0, 660.8036643981934, 0.0, 0.0, 0.0], 'rewardMean': 0.8171433759628619, 'totalEpisodes': 50, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.103678977136294
'totalSteps': 12800, 'rewardStep': 0.6052781133085958, 'errorList': [], 'lossList': [0.0, -1.3439808773994446, 0.0, 450.29700561523435, 0.0, 0.0, 0.0], 'rewardMean': 0.7959568496974353, 'totalEpisodes': 88, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.097763466757752
'totalSteps': 14080, 'rewardStep': 0.678127953672878, 'errorList': [], 'lossList': [0.0, -1.3425832504034043, 0.0, 180.95707321166992, 0.0, 0.0, 0.0], 'rewardMean': 0.7738766034820491, 'totalEpisodes': 119, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.043070075786641
'totalSteps': 15360, 'rewardStep': 0.6902235494576336, 'errorList': [], 'lossList': [0.0, -1.3410068845748901, 0.0, 108.1249026107788, 0.0, 0.0, 0.0], 'rewardMean': 0.7855508106814741, 'totalEpisodes': 150, 'stepsPerEpisode': 61, 'rewardPerEpisode': 52.09799423196617
'totalSteps': 16640, 'rewardStep': 0.8228257411792852, 'errorList': [], 'lossList': [0.0, -1.339865846633911, 0.0, 100.18262273788451, 0.0, 0.0, 0.0], 'rewardMean': 0.7715891929633112, 'totalEpisodes': 178, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.6564283082343088
'totalSteps': 17920, 'rewardStep': 0.3944469739599519, 'errorList': [], 'lossList': [0.0, -1.335829485654831, 0.0, 56.777186460494995, 0.0, 0.0, 0.0], 'rewardMean': 0.7271320102463803, 'totalEpisodes': 197, 'stepsPerEpisode': 41, 'rewardPerEpisode': 20.371461896664623
'totalSteps': 19200, 'rewardStep': 0.9517404874432306, 'errorList': [114.86322943769672, 40.640497675989636, 186.59382358521887, 240.44003808821455, 107.7728588155413, 266.4623181775651, 198.20437370629168, 277.7905039859257, 245.55585094232063, 244.49479000310146, 116.66803500806603, 94.10512209088299, 225.87236570174372, 246.67300253549263, 73.78492681499036, 220.1273824410704, 160.17534968825638, 242.02659412128725, 250.0783039799434, 234.99905201442175, 182.51198071264423, 74.96558817125326, 205.60570205249047, 241.8426291110878, 42.876423052472596, 117.23118754380927, 282.26266720038774, 266.5059596342385, 234.00051075385772, 95.89820201695943, 277.51547675068633, 202.1828054054217, 267.9265645773603, 48.40585636272283, 38.9122282154784, 119.4823895963379, 235.4645571461611, 201.82226414993903, 254.23405998450423, 203.09480492463413, 217.404147162642, 245.53663995015043, 276.9718136468565, 64.96465363081163, 108.70921686662508, 108.42218372002266, 270.61847161299477, 234.23358818467776, 43.18425183704761, 119.0807791240345], 'lossList': [0.0, -1.333028439283371, 0.0, 35.34618987083435, 0.0, 0.0, 0.0], 'rewardMean': 0.7442016279651629, 'totalEpisodes': 218, 'stepsPerEpisode': 76, 'rewardPerEpisode': 69.24665593490137, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.7120781810510568, 'errorList': [], 'lossList': [0.0, -1.3306326055526734, 0.0, 14.455353446006775, 0.0, 0.0, 0.0], 'rewardMean': 0.7169248001277938, 'totalEpisodes': 231, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7120781810510568
'totalSteps': 21760, 'rewardStep': 0.9095355624621922, 'errorList': [], 'lossList': [0.0, -1.3186831629276277, 0.0, 11.774377830028534, 0.0, 0.0, 0.0], 'rewardMean': 0.7277644578447363, 'totalEpisodes': 239, 'stepsPerEpisode': 146, 'rewardPerEpisode': 131.08388203823725
'totalSteps': 23040, 'rewardStep': 0.5908599578488765, 'errorList': [], 'lossList': [0.0, -1.3023317050933838, 0.0, 9.10966391324997, 0.0, 0.0, 0.0], 'rewardMean': 0.6932261574072962, 'totalEpisodes': 246, 'stepsPerEpisode': 67, 'rewardPerEpisode': 43.104603818699644
'totalSteps': 24320, 'rewardStep': 0.46427907981971706, 'errorList': [], 'lossList': [0.0, -1.2878034484386445, 0.0, 16.358876156806947, 0.0, 0.0, 0.0], 'rewardMean': 0.6819395600203417, 'totalEpisodes': 249, 'stepsPerEpisode': 50, 'rewardPerEpisode': 36.82156874356248
'totalSteps': 25600, 'rewardStep': 0.3828509934449706, 'errorList': [], 'lossList': [0.0, -1.2868650370836259, 0.0, 11.490969721078873, 0.0, 0.0, 0.0], 'rewardMean': 0.6596968480339792, 'totalEpisodes': 251, 'stepsPerEpisode': 770, 'rewardPerEpisode': 608.405743981084
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=59.19
