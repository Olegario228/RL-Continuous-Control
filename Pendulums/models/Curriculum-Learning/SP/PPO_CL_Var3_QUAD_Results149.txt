#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 149
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8235348665983694, 'errorList': [], 'lossList': [0.0, -1.4212338054180145, 0.0, 28.196929371356966, 0.0, 0.0, 0.0], 'rewardMean': 0.7864128654488252, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 774.2286778047021
'totalSteps': 3840, 'rewardStep': 0.7650444976172296, 'errorList': [], 'lossList': [0.0, -1.4351186394691466, 0.0, 34.38810375928879, 0.0, 0.0, 0.0], 'rewardMean': 0.7792900761716267, 'totalEpisodes': 14, 'stepsPerEpisode': 160, 'rewardPerEpisode': 124.12871363207263
'totalSteps': 5120, 'rewardStep': 0.6957442332659612, 'errorList': [], 'lossList': [0.0, -1.4314742225408554, 0.0, 25.071438367962838, 0.0, 0.0, 0.0], 'rewardMean': 0.7584036154452103, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 993.7253342277286
'totalSteps': 6400, 'rewardStep': 0.9099310891360634, 'errorList': [], 'lossList': [0.0, -1.4271194487810135, 0.0, 16.88903454899788, 0.0, 0.0, 0.0], 'rewardMean': 0.7887091101833809, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 993.9631613923004
'totalSteps': 7680, 'rewardStep': 0.8814680663612725, 'errorList': [], 'lossList': [0.0, -1.4246221369504928, 0.0, 12.430484606027603, 0.0, 0.0, 0.0], 'rewardMean': 0.8041689362130295, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 985.2637900075665
'totalSteps': 8960, 'rewardStep': 0.6250418763880098, 'errorList': [], 'lossList': [0.0, -1.4039125090837479, 0.0, 37.71153578639031, 0.0, 0.0, 0.0], 'rewardMean': 0.7785793562380267, 'totalEpisodes': 16, 'stepsPerEpisode': 1124, 'rewardPerEpisode': 811.7684686088643
'totalSteps': 10240, 'rewardStep': 0.8719719710489605, 'errorList': [], 'lossList': [0.0, -1.3894374215602874, 0.0, 110.64741752624512, 0.0, 0.0, 0.0], 'rewardMean': 0.7902534330893934, 'totalEpisodes': 22, 'stepsPerEpisode': 57, 'rewardPerEpisode': 45.70369475036533
'totalSteps': 11520, 'rewardStep': 0.511927028955792, 'errorList': [], 'lossList': [0.0, -1.3832894003391265, 0.0, 536.0424584960938, 0.0, 0.0, 0.0], 'rewardMean': 0.7593282770745488, 'totalEpisodes': 70, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.511927028955792
'totalSteps': 12800, 'rewardStep': 0.6549479100671077, 'errorList': [], 'lossList': [0.0, -1.381091131567955, 0.0, 191.7661192703247, 0.0, 0.0, 0.0], 'rewardMean': 0.7488902403738047, 'totalEpisodes': 103, 'stepsPerEpisode': 38, 'rewardPerEpisode': 22.674375999918844
'totalSteps': 14080, 'rewardStep': 0.7146460761265608, 'errorList': [], 'lossList': [0.0, -1.3774072355031968, 0.0, 130.76965045928955, 0.0, 0.0, 0.0], 'rewardMean': 0.7454257615565327, 'totalEpisodes': 134, 'stepsPerEpisode': 57, 'rewardPerEpisode': 44.243118463832495
'totalSteps': 15360, 'rewardStep': 0.6705346919219632, 'errorList': [], 'lossList': [0.0, -1.3787707436084746, 0.0, 90.34364562988281, 0.0, 0.0, 0.0], 'rewardMean': 0.7301257440888922, 'totalEpisodes': 158, 'stepsPerEpisode': 57, 'rewardPerEpisode': 43.542950883096765
'totalSteps': 16640, 'rewardStep': 0.1840525320896555, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6208573774185041, 'totalEpisodes': 172, 'stepsPerEpisode': 83, 'rewardPerEpisode': 54.39426051668821
'totalSteps': 17920, 'rewardStep': 0.6181146657120891, 'errorList': [], 'lossList': [0.0, -1.3745425993204117, 0.0, 52.72799331665039, 0.0, 0.0, 0.0], 'rewardMean': 0.5916757350761067, 'totalEpisodes': 183, 'stepsPerEpisode': 81, 'rewardPerEpisode': 65.26021548705076
'totalSteps': 19200, 'rewardStep': 0.39881948510001203, 'errorList': [], 'lossList': [0.0, -1.3654344832897187, 0.0, 51.362179522514346, 0.0, 0.0, 0.0], 'rewardMean': 0.5434108769499806, 'totalEpisodes': 193, 'stepsPerEpisode': 172, 'rewardPerEpisode': 118.09186378833172
'totalSteps': 20480, 'rewardStep': 0.9760634931431569, 'errorList': [1.8660619042552387, 1.4322034050627916, 1.8490820660842147, 2.3368930567458297, 2.1333208630615648, 1.849217762697981, 2.1870149077718546, 2.058810468274037, 1.9935587482744332, 2.6840638083409067, 1.923138343096303, 2.4149727159152574, 2.5543257141068367, 2.30032824030413, 2.0663415420411955, 2.6793452309837673, 2.3113840056637556, 1.83965653978995, 2.5529903949041466, 2.5574053486412405, 2.211710884754635, 2.5331818940056645, 2.254169217298785, 1.6535691864244706, 1.572700350193577, 1.8367988673778606, 2.5295456497351827, 2.208030006270064, 1.829656402391306, 2.2312227573317127, 2.735301912538505, 2.283816846310867, 2.211762943854513, 2.73517055409931, 1.9013732830192238, 1.7890291776484084, 2.15335745406329, 2.126598092636794, 2.6150178703577196, 2.3164005390854503, 2.318937427597149, 2.5874716351319886, 2.5892879485562257, 2.644401853220475, 2.6519969101628185, 2.084406593433329, 1.644354694586385, 2.5733094242115953, 2.4307156435828436, 2.2549255061915083], 'lossList': [0.0, -1.3524758863449096, 0.0, 29.34374143600464, 0.0, 0.0, 0.0], 'rewardMean': 0.5785130386254954, 'totalEpisodes': 200, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.78611476085028, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.7471454563787997, 'errorList': [], 'lossList': [0.0, -1.34320570230484, 0.0, 13.046874996423721, 0.0, 0.0, 0.0], 'rewardMean': 0.5660303871584793, 'totalEpisodes': 204, 'stepsPerEpisode': 74, 'rewardPerEpisode': 64.05975150894969
'totalSteps': 23040, 'rewardStep': 0.4773171471171703, 'errorList': [], 'lossList': [0.0, -1.3386591148376465, 0.0, 28.17315676689148, 0.0, 0.0, 0.0], 'rewardMean': 0.562569398974617, 'totalEpisodes': 206, 'stepsPerEpisode': 315, 'rewardPerEpisode': 166.38251758011
'totalSteps': 24320, 'rewardStep': 0.7011738875778223, 'errorList': [], 'lossList': [0.0, -1.3291583001613616, 0.0, 4.998738077282906, 0.0, 0.0, 0.0], 'rewardMean': 0.5671919967256885, 'totalEpisodes': 206, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 882.4715562188214
'totalSteps': 25600, 'rewardStep': 0.7966161431114407, 'errorList': [], 'lossList': [0.0, -1.3033520096540452, 0.0, 3.2541605058312415, 0.0, 0.0, 0.0], 'rewardMean': 0.5753890034241765, 'totalEpisodes': 206, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 960.3789173672525
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=49.45
