#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 15
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9342153040089045, 'errorList': [], 'lossList': [0.0, -1.4350808668136597, 0.0, 29.524906636476516, 0.0, 0.0, 0.0], 'rewardMean': 0.9148984476085358, 'totalEpisodes': 8, 'stepsPerEpisode': 530, 'rewardPerEpisode': 373.8561509233166
'totalSteps': 3840, 'rewardStep': 0.7068526029320407, 'errorList': [], 'lossList': [0.0, -1.4218777841329575, 0.0, 28.526218614578248, 0.0, 0.0, 0.0], 'rewardMean': 0.8455498327163707, 'totalEpisodes': 12, 'stepsPerEpisode': 259, 'rewardPerEpisode': 191.44358761814524
'totalSteps': 5120, 'rewardStep': 0.7180329429126137, 'errorList': [], 'lossList': [0.0, -1.4245619511604308, 0.0, 70.71785312652588, 0.0, 0.0, 0.0], 'rewardMean': 0.8136706102654315, 'totalEpisodes': 23, 'stepsPerEpisode': 153, 'rewardPerEpisode': 116.83942194772644
'totalSteps': 6400, 'rewardStep': 0.8581660619705377, 'errorList': [], 'lossList': [0.0, -1.4300472742319108, 0.0, 168.15068714141844, 0.0, 0.0, 0.0], 'rewardMean': 0.8225697006064527, 'totalEpisodes': 96, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.4978063325963604
'totalSteps': 7680, 'rewardStep': 0.7298136291943442, 'errorList': [], 'lossList': [0.0, -1.4217774564027785, 0.0, 70.51066816329956, 0.0, 0.0, 0.0], 'rewardMean': 0.8071103553711012, 'totalEpisodes': 151, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.799283813387982
'totalSteps': 8960, 'rewardStep': 0.8897020981722034, 'errorList': [], 'lossList': [0.0, -1.398936163187027, 0.0, 46.993035011291504, 0.0, 0.0, 0.0], 'rewardMean': 0.8189091757712587, 'totalEpisodes': 180, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8897020981722034
'totalSteps': 10240, 'rewardStep': 0.8142828421786729, 'errorList': [], 'lossList': [0.0, -1.3794605183601378, 0.0, 48.764243631362916, 0.0, 0.0, 0.0], 'rewardMean': 0.8183308840721855, 'totalEpisodes': 195, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.509190217723992
'totalSteps': 11520, 'rewardStep': 0.7451527424656256, 'errorList': [], 'lossList': [0.0, -1.3692178565263748, 0.0, 36.6528569316864, 0.0, 0.0, 0.0], 'rewardMean': 0.8101999794492344, 'totalEpisodes': 205, 'stepsPerEpisode': 69, 'rewardPerEpisode': 59.66596155263856
'totalSteps': 12800, 'rewardStep': 0.6617523800744701, 'errorList': [], 'lossList': [0.0, -1.3599856221675872, 0.0, 19.136532179117204, 0.0, 0.0, 0.0], 'rewardMean': 0.795355219511758, 'totalEpisodes': 211, 'stepsPerEpisode': 121, 'rewardPerEpisode': 102.87491677484951
'totalSteps': 14080, 'rewardStep': 0.7231092772068385, 'errorList': [], 'lossList': [0.0, -1.3552819091081618, 0.0, 24.41075604557991, 0.0, 0.0, 0.0], 'rewardMean': 0.7781079881116252, 'totalEpisodes': 215, 'stepsPerEpisode': 213, 'rewardPerEpisode': 176.94925044198555
'totalSteps': 15360, 'rewardStep': 0.822330574393451, 'errorList': [], 'lossList': [0.0, -1.3524328774213792, 0.0, 12.274966391324996, 0.0, 0.0, 0.0], 'rewardMean': 0.7669195151500798, 'totalEpisodes': 219, 'stepsPerEpisode': 69, 'rewardPerEpisode': 63.81469398177293
'totalSteps': 16640, 'rewardStep': 0.8279872859859321, 'errorList': [], 'lossList': [0.0, -1.3607715183496476, 0.0, 8.042983500361442, 0.0, 0.0, 0.0], 'rewardMean': 0.7790329834554689, 'totalEpisodes': 222, 'stepsPerEpisode': 260, 'rewardPerEpisode': 229.23210832328132
'totalSteps': 17920, 'rewardStep': 0.668067316957293, 'errorList': [], 'lossList': [0.0, -1.3524483245611192, 0.0, 3.226889285445213, 0.0, 0.0, 0.0], 'rewardMean': 0.7740364208599368, 'totalEpisodes': 227, 'stepsPerEpisode': 145, 'rewardPerEpisode': 110.10518536226165
'totalSteps': 19200, 'rewardStep': 0.6424944039593867, 'errorList': [], 'lossList': [0.0, -1.3289276564121246, 0.0, 4.5717104536294935, 0.0, 0.0, 0.0], 'rewardMean': 0.7524692550588218, 'totalEpisodes': 230, 'stepsPerEpisode': 221, 'rewardPerEpisode': 170.26253741268852
'totalSteps': 20480, 'rewardStep': 0.944693963359935, 'errorList': [9.714029154260869, 11.581794054144508, 5.88876833459293, 3.7763720939644956, 9.758318275916041, 6.618101091478261, 4.364888012243866, 13.137499935209581, 3.1129082379863866, 6.827343493352727, 4.859089499286385, 0.5862413690446526, 10.485154491192349, 1.7824796838263943, 7.125879888327739, 10.049596889269111, 5.771449842778132, 3.1949618805891005, 1.8705506710993052, 9.136975264772394, 6.629946887559181, 5.274095031695504, 13.05745968049524, 2.984217252224344, 3.685941616402586, 4.642588703251867, 5.0746043428508525, 4.402351428921854, 6.228513170932283, 0.9660303140037434, 10.939113756700568, 8.983784654889023, 14.927146842263499, 12.209860444082844, 1.7581743196345323, 1.9877538312550815, 11.216923810302033, 6.9660063686381495, 10.318944138774496, 10.508168072571424, 5.770531154890857, 5.680696124195876, 7.078178698588139, 13.789405134170117, 7.699767982805302, 3.0890566349929953, 8.57244481189765, 3.432371556818187, 7.139156750555354, 6.161214500569891], 'lossList': [0.0, -1.2977094984054565, 0.0, 3.5565649551153182, 0.0, 0.0, 0.0], 'rewardMean': 0.7739572884753808, 'totalEpisodes': 232, 'stepsPerEpisode': 688, 'rewardPerEpisode': 602.5222829622571, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.9209303849666315, 'errorList': [], 'lossList': [0.0, -1.2838257437944411, 0.0, 5.232763745188713, 0.0, 0.0, 0.0], 'rewardMean': 0.7770801171548236, 'totalEpisodes': 236, 'stepsPerEpisode': 144, 'rewardPerEpisode': 126.75683181517998
'totalSteps': 23040, 'rewardStep': 0.6179830228495109, 'errorList': [], 'lossList': [0.0, -1.3011077594757081, 0.0, 3.69157982647419, 0.0, 0.0, 0.0], 'rewardMean': 0.7574501352219075, 'totalEpisodes': 239, 'stepsPerEpisode': 130, 'rewardPerEpisode': 106.32175888066936
'totalSteps': 24320, 'rewardStep': 0.7699903017169037, 'errorList': [], 'lossList': [0.0, -1.307279611825943, 0.0, 3.899235936999321, 0.0, 0.0, 0.0], 'rewardMean': 0.7599338911470352, 'totalEpisodes': 243, 'stepsPerEpisode': 155, 'rewardPerEpisode': 129.62715284488294
'totalSteps': 25600, 'rewardStep': 0.8968099666937748, 'errorList': [], 'lossList': [0.0, -1.3049932718276978, 0.0, 3.644018529653549, 0.0, 0.0, 0.0], 'rewardMean': 0.7834396498089659, 'totalEpisodes': 245, 'stepsPerEpisode': 85, 'rewardPerEpisode': 73.30023589120749
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=76.71
