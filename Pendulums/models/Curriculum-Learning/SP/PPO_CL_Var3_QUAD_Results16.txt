#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 16
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7500298467469649, 'errorList': [], 'lossList': [0.0, -1.4449572098255157, 0.0, 35.80020132064819, 0.0, 0.0, 0.0], 'rewardMean': 0.65433878261166, 'totalEpisodes': 12, 'stepsPerEpisode': 72, 'rewardPerEpisode': 61.505558098190626
'totalSteps': 3840, 'rewardStep': 0.8819127387740987, 'errorList': [], 'lossList': [0.0, -1.459716727733612, 0.0, 25.9310118830204, 0.0, 0.0, 0.0], 'rewardMean': 0.7301967679991396, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 949.0703040635274
'totalSteps': 5120, 'rewardStep': 0.8550904418362365, 'errorList': [], 'lossList': [0.0, -1.4451111561059953, 0.0, 34.613796856403354, 0.0, 0.0, 0.0], 'rewardMean': 0.7614201864584138, 'totalEpisodes': 15, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.861845683700505
'totalSteps': 6400, 'rewardStep': 0.8915119627729174, 'errorList': [], 'lossList': [0.0, -1.4420530492067336, 0.0, 206.96060050964354, 0.0, 0.0, 0.0], 'rewardMean': 0.7874385417213146, 'totalEpisodes': 67, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.694210870387626
'totalSteps': 7680, 'rewardStep': 0.6418328717934031, 'errorList': [], 'lossList': [0.0, -1.4332414120435715, 0.0, 78.75488210678101, 0.0, 0.0, 0.0], 'rewardMean': 0.7631709300666626, 'totalEpisodes': 108, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.26860576163662
'totalSteps': 8960, 'rewardStep': 0.7435392344969537, 'errorList': [], 'lossList': [0.0, -1.4203295904397963, 0.0, 50.59995142936707, 0.0, 0.0, 0.0], 'rewardMean': 0.7603664021281328, 'totalEpisodes': 147, 'stepsPerEpisode': 45, 'rewardPerEpisode': 39.80523796615835
'totalSteps': 10240, 'rewardStep': 0.4759347888602132, 'errorList': [], 'lossList': [0.0, -1.4115895521640778, 0.0, 24.10601821422577, 0.0, 0.0, 0.0], 'rewardMean': 0.7248124504696428, 'totalEpisodes': 159, 'stepsPerEpisode': 157, 'rewardPerEpisode': 108.68477881406703
'totalSteps': 11520, 'rewardStep': 0.7501560031413477, 'errorList': [], 'lossList': [0.0, -1.3995496433973313, 0.0, 31.854904971122743, 0.0, 0.0, 0.0], 'rewardMean': 0.7276284007664989, 'totalEpisodes': 170, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.851278365657391
'totalSteps': 12800, 'rewardStep': 0.4040908719351253, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6798189632292385, 'totalEpisodes': 179, 'stepsPerEpisode': 278, 'rewardPerEpisode': 190.37812931331078
'totalSteps': 14080, 'rewardStep': 0.7758567685203747, 'errorList': [], 'lossList': [0.0, -1.3733325111865997, 0.0, 27.74244576692581, 0.0, 0.0, 0.0], 'rewardMean': 0.6824016554065795, 'totalEpisodes': 184, 'stepsPerEpisode': 226, 'rewardPerEpisode': 179.2205836498362
'totalSteps': 15360, 'rewardStep': 0.7972431470878509, 'errorList': [], 'lossList': [0.0, -1.3691153836250305, 0.0, 10.418117892742156, 0.0, 0.0, 0.0], 'rewardMean': 0.6739346962379548, 'totalEpisodes': 188, 'stepsPerEpisode': 106, 'rewardPerEpisode': 80.25510615285606
'totalSteps': 16640, 'rewardStep': 0.4286866026050082, 'errorList': [], 'lossList': [0.0, -1.384734178185463, 0.0, 8.493944107294082, 0.0, 0.0, 0.0], 'rewardMean': 0.631294312314832, 'totalEpisodes': 190, 'stepsPerEpisode': 513, 'rewardPerEpisode': 361.14123788404135
'totalSteps': 17920, 'rewardStep': 0.37751709435857694, 'errorList': [], 'lossList': [0.0, -1.362879360318184, 0.0, 5.884115296602249, 0.0, 0.0, 0.0], 'rewardMean': 0.5798948254733979, 'totalEpisodes': 192, 'stepsPerEpisode': 419, 'rewardPerEpisode': 281.5112846636768
'totalSteps': 19200, 'rewardStep': 0.9092827501520033, 'errorList': [], 'lossList': [0.0, -1.319575417637825, 0.0, 6.276660578250885, 0.0, 0.0, 0.0], 'rewardMean': 0.606639813309258, 'totalEpisodes': 194, 'stepsPerEpisode': 116, 'rewardPerEpisode': 94.85552169916444
'totalSteps': 20480, 'rewardStep': 0.7618074581497316, 'errorList': [], 'lossList': [0.0, -1.2819715303182602, 0.0, 4.342022234201432, 0.0, 0.0, 0.0], 'rewardMean': 0.6084666356745358, 'totalEpisodes': 196, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.57334887902224
'totalSteps': 21760, 'rewardStep': 0.6230744749237005, 'errorList': [], 'lossList': [0.0, -1.2319510209560394, 0.0, 2.615374225974083, 0.0, 0.0, 0.0], 'rewardMean': 0.6231806042808844, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 962.3813722841609
'totalSteps': 23040, 'rewardStep': 0.9285739039144689, 'errorList': [], 'lossList': [0.0, -1.1963360387086868, 0.0, 2.6047378373891115, 0.0, 0.0, 0.0], 'rewardMean': 0.6410223943581965, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.4099892890742
'totalSteps': 24320, 'rewardStep': 0.831876378851363, 'errorList': [], 'lossList': [0.0, -1.1699200814962387, 0.0, 1.9672487488389014, 0.0, 0.0, 0.0], 'rewardMean': 0.6838009450498204, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.9588982433852
'totalSteps': 25600, 'rewardStep': 0.9537269098521477, 'errorList': [0.030464452453947723, 0.06150462126375133, 0.14934996950212437, 0.1603395655270957, 0.03776374208729336, 0.15070975931566605, 0.16356662517924514, 0.03639113786234144, 0.0333272544080935, 0.10096851943402696, 0.13313805737523862, 0.03192425203057115, 0.1093795255112718, 0.058591886862520315, 0.07680783284406655, 0.0880359449092495, 0.22180090152459594, 0.030521511717485017, 0.16082391065398288, 0.0509335134797318, 0.10625412493267401, 0.07999651301245517, 0.13547949292455447, 0.12624629006224805, 0.030025138526134004, 0.1399906320898482, 0.03378034673044685, 0.10463718794038293, 0.11195980348366916, 0.03157738503414183, 0.032676812400131566, 0.16325547534667254, 0.02865976988422402, 0.031818704412860686, 0.033086729464209666, 0.03475270644090379, 0.12102811649895769, 0.03201965959810847, 0.03216772463690585, 0.03203168411662368, 0.08912166340492492, 0.07707681060775255, 0.04895744578913308, 0.03046521435756619, 0.07019368521839787, 0.14976331294799064, 0.15397658591552912, 0.09952200188647471, 0.07932729675806262, 0.09470500322516699], 'lossList': [0.0, -1.1216186875104903, 0.0, 1.3281103513762356, 0.0, 0.0, 0.0], 'rewardMean': 0.7387645488415225, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.8281992479494, 'successfulTests': 49
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=25600, timeSpent=77.51
