#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 17
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.8482618755507305, 'errorList': [], 'lossList': [0.0, -1.4448001652956008, 0.0, 30.85931431055069, 0.0, 0.0, 0.0], 'rewardMean': 0.6630238253867644, 'totalEpisodes': 12, 'stepsPerEpisode': 898, 'rewardPerEpisode': 636.7226841775991
'totalSteps': 3840, 'rewardStep': 0.9203993182913687, 'errorList': [], 'lossList': [0.0, -1.4544023245573043, 0.0, 35.11497160434723, 0.0, 0.0, 0.0], 'rewardMean': 0.7488156563549658, 'totalEpisodes': 15, 'stepsPerEpisode': 491, 'rewardPerEpisode': 385.07301632676985
'totalSteps': 5120, 'rewardStep': 0.3826449241673579, 'errorList': [], 'lossList': [0.0, -1.4394847106933595, 0.0, 51.742845640182495, 0.0, 0.0, 0.0], 'rewardMean': 0.6572729733080638, 'totalEpisodes': 24, 'stepsPerEpisode': 68, 'rewardPerEpisode': 36.14756527497607
'totalSteps': 6400, 'rewardStep': 0.5851200575955621, 'errorList': [], 'lossList': [0.0, -1.4278228497505188, 0.0, 160.06961212158203, 0.0, 0.0, 0.0], 'rewardMean': 0.6428423901655634, 'totalEpisodes': 78, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2033739727741861
'totalSteps': 7680, 'rewardStep': 0.7605503119695148, 'errorList': [], 'lossList': [0.0, -1.422133868932724, 0.0, 76.3599088859558, 0.0, 0.0, 0.0], 'rewardMean': 0.6624603771328886, 'totalEpisodes': 132, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.204801084795275
'totalSteps': 8960, 'rewardStep': 0.8127083747320659, 'errorList': [], 'lossList': [0.0, -1.4169259268045424, 0.0, 53.3365905380249, 0.0, 0.0, 0.0], 'rewardMean': 0.683924376789914, 'totalEpisodes': 159, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.680834464428098
'totalSteps': 10240, 'rewardStep': 0.5104412838349073, 'errorList': [], 'lossList': [0.0, -1.408781611919403, 0.0, 26.81612437725067, 0.0, 0.0, 0.0], 'rewardMean': 0.6622389901705381, 'totalEpisodes': 173, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.6206735251267097
'totalSteps': 11520, 'rewardStep': 0.810464661535332, 'errorList': [], 'lossList': [0.0, -1.3968160778284073, 0.0, 33.23221029758454, 0.0, 0.0, 0.0], 'rewardMean': 0.6787085092110707, 'totalEpisodes': 185, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.653076116785263
'totalSteps': 12800, 'rewardStep': 0.8574996631012666, 'errorList': [], 'lossList': [0.0, -1.3893458008766175, 0.0, 29.950420989990235, 0.0, 0.0, 0.0], 'rewardMean': 0.6965876246000903, 'totalEpisodes': 192, 'stepsPerEpisode': 156, 'rewardPerEpisode': 126.02642570411857
'totalSteps': 14080, 'rewardStep': 0.8314506125132087, 'errorList': [], 'lossList': [0.0, -1.3796143651008606, 0.0, 12.378246958255767, 0.0, 0.0, 0.0], 'rewardMean': 0.7319541083291314, 'totalEpisodes': 197, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.191826259084117
'totalSteps': 15360, 'rewardStep': 0.7941741850870369, 'errorList': [], 'lossList': [0.0, -1.3861314487457275, 0.0, 22.438028366565703, 0.0, 0.0, 0.0], 'rewardMean': 0.726545339282762, 'totalEpisodes': 201, 'stepsPerEpisode': 96, 'rewardPerEpisode': 76.63506516027165
'totalSteps': 16640, 'rewardStep': 0.9027275710966899, 'errorList': [], 'lossList': [0.0, -1.3863596934080125, 0.0, 7.431491528749466, 0.0, 0.0, 0.0], 'rewardMean': 0.7247781645632941, 'totalEpisodes': 205, 'stepsPerEpisode': 189, 'rewardPerEpisode': 165.12817526164665
'totalSteps': 17920, 'rewardStep': 0.8414566005711531, 'errorList': [], 'lossList': [0.0, -1.3966542154550552, 0.0, 6.805288997292519, 0.0, 0.0, 0.0], 'rewardMean': 0.7706593322036737, 'totalEpisodes': 208, 'stepsPerEpisode': 309, 'rewardPerEpisode': 277.3901103782035
'totalSteps': 19200, 'rewardStep': 0.930952978202348, 'errorList': [1.339746654847992, 0.20098374385014356, 0.9796889965771224, 0.6824910376746619, 0.12408206660275849, 2.384394739915457, 0.9619169288671965, 0.5973209144960339, 0.4579152107437621, 0.27934926442880176, 1.790093573298657, 2.1360264782377016, 1.7765470271695758, 0.25657246013268664, 0.416234245307279, 1.1918132348990635, 1.554765528419726, 1.037926575142107, 1.0683697655479145, 0.9314347339563247, 0.3347530645565881, 1.1976682046524256, 1.4312985542013963, 0.6122280940135264, 0.3716183048717743, 0.161467735216398, 0.775097230505885, 0.9320674762590098, 1.782126700749944, 0.6878940132357275, 0.13002029118406205, 1.4765101812270005, 0.7377576520081006, 0.9027112297965104, 0.12267623511343748, 1.6125210029688992, 0.6344014174852182, 0.35600874608459276, 0.8660526146393597, 1.8159127530435952, 0.6115927723892048, 1.7964926688529321, 1.1983052622846477, 0.44472803235810743, 0.8747356004824276, 0.8531466577900418, 0.6709496962397382, 2.7079485520742534, 1.4309320881989673, 0.5788882397919503], 'lossList': [0.0, -1.3762561774253845, 0.0, 8.200040893554688, 0.0, 0.0, 0.0], 'rewardMean': 0.8052426242643523, 'totalEpisodes': 211, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.673629158158388, 'successfulTests': 4
'totalSteps': 20480, 'rewardStep': 0.9118025217116408, 'errorList': [], 'lossList': [0.0, -1.347998861670494, 0.0, 4.390439204573632, 0.0, 0.0, 0.0], 'rewardMean': 0.8203678452385649, 'totalEpisodes': 214, 'stepsPerEpisode': 105, 'rewardPerEpisode': 88.33194673836458
'totalSteps': 21760, 'rewardStep': 0.522965254732414, 'errorList': [], 'lossList': [0.0, -1.3213099145889282, 0.0, 3.3221772226691244, 0.0, 0.0, 0.0], 'rewardMean': 0.7913935332385997, 'totalEpisodes': 215, 'stepsPerEpisode': 831, 'rewardPerEpisode': 706.309771789515
'totalSteps': 23040, 'rewardStep': 0.8832285004933669, 'errorList': [], 'lossList': [0.0, -1.3151145684719086, 0.0, 2.021935762017965, 0.0, 0.0, 0.0], 'rewardMean': 0.8286722549044457, 'totalEpisodes': 218, 'stepsPerEpisode': 88, 'rewardPerEpisode': 76.94160529776596
'totalSteps': 24320, 'rewardStep': 0.9781303570090314, 'errorList': [7.238847259442092, 3.6859402443904883, 9.226374170488485, 3.8373900505914347, 4.971713743457655, 8.186599872989788, 4.895521165927217, 3.75307692208962, 5.4141277899977585, 3.0784044774061528, 6.493204393407221, 8.563196225737046, 4.429840875346931, 9.528399743770686, 3.1516364321464327, 6.09990820736257, 10.690078797720167, 12.336767484601793, 3.3550798010404503, 8.470809678121052, 4.107287816965239, 7.841135713771954, 4.609571962835794, 10.735569631272782, 10.616628483667977, 12.122608279589272, 12.76336582724907, 4.981104110210872, 3.30983213712051, 9.085871741064992, 3.6652805319414914, 5.416395605273499, 7.1401745209561875, 8.285431127908232, 6.357874861045871, 5.023053429271539, 7.729083667168408, 4.170428565137725, 13.294040312602347, 7.026157864463685, 8.933906789153026, 6.493203418751235, 9.027189756085233, 5.727294444994644, 11.542388482617502, 3.2812241120969574, 4.402084313310472, 9.327489346354099, 8.477999595952662, 11.399589285210201], 'lossList': [0.0, -1.3084933006763457, 0.0, 2.9519134280085564, 0.0, 0.0, 0.0], 'rewardMean': 0.8454388244518156, 'totalEpisodes': 219, 'stepsPerEpisode': 198, 'rewardPerEpisode': 171.63337713416743, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.743896717817966, 'errorList': [], 'lossList': [0.0, -1.306979656815529, 0.0, 3.369899473488331, 0.0, 0.0, 0.0], 'rewardMean': 0.8340785299234856, 'totalEpisodes': 223, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.278791689437257
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=19200, timeSpent=98.58
