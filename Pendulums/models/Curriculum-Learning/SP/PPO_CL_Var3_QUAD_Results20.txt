#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 20
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.8983640198931809, 'errorList': [], 'lossList': [0.0, -1.4375572824478149, 0.0, 30.88102098107338, 0.0, 0.0, 0.0], 'rewardMean': 0.9065923252370045, 'totalEpisodes': 8, 'stepsPerEpisode': 529, 'rewardPerEpisode': 376.2430223529151
'totalSteps': 3840, 'rewardStep': 0.7608484766961245, 'errorList': [], 'lossList': [0.0, -1.4219058656692505, 0.0, 30.363131403923035, 0.0, 0.0, 0.0], 'rewardMean': 0.8580110423900446, 'totalEpisodes': 12, 'stepsPerEpisode': 259, 'rewardPerEpisode': 197.07615073962052
'totalSteps': 5120, 'rewardStep': 0.6693180927809308, 'errorList': [], 'lossList': [0.0, -1.41737278342247, 0.0, 49.90223464488983, 0.0, 0.0, 0.0], 'rewardMean': 0.810837804987766, 'totalEpisodes': 18, 'stepsPerEpisode': 173, 'rewardPerEpisode': 135.2603194063345
'totalSteps': 6400, 'rewardStep': 0.5948836077433934, 'errorList': [], 'lossList': [0.0, -1.4146185779571534, 0.0, 198.7789115142822, 0.0, 0.0, 0.0], 'rewardMean': 0.7676469655388916, 'totalEpisodes': 84, 'stepsPerEpisode': 28, 'rewardPerEpisode': 19.652291963990898
'totalSteps': 7680, 'rewardStep': 0.7712173526258523, 'errorList': [], 'lossList': [0.0, -1.4134915989637375, 0.0, 93.41665103912354, 0.0, 0.0, 0.0], 'rewardMean': 0.768242030053385, 'totalEpisodes': 136, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.410824478576039
'totalSteps': 8960, 'rewardStep': 0.7891502292606967, 'errorList': [], 'lossList': [0.0, -1.3945206004381179, 0.0, 61.05773849487305, 0.0, 0.0, 0.0], 'rewardMean': 0.7712289156544295, 'totalEpisodes': 182, 'stepsPerEpisode': 26, 'rewardPerEpisode': 17.37513798825401
'totalSteps': 10240, 'rewardStep': 0.8040552660472216, 'errorList': [], 'lossList': [0.0, -1.3662353992462157, 0.0, 56.97222885131836, 0.0, 0.0, 0.0], 'rewardMean': 0.7753322094535285, 'totalEpisodes': 207, 'stepsPerEpisode': 12, 'rewardPerEpisode': 11.044337842241273
'totalSteps': 11520, 'rewardStep': 0.8403955628166119, 'errorList': [], 'lossList': [0.0, -1.3389436846971512, 0.0, 53.8843713760376, 0.0, 0.0, 0.0], 'rewardMean': 0.7825614709383156, 'totalEpisodes': 222, 'stepsPerEpisode': 49, 'rewardPerEpisode': 42.28828616608836
'totalSteps': 12800, 'rewardStep': 0.7691715845570233, 'errorList': [], 'lossList': [0.0, -1.3214515900611878, 0.0, 44.63145117759704, 0.0, 0.0, 0.0], 'rewardMean': 0.7812224823001863, 'totalEpisodes': 231, 'stepsPerEpisode': 42, 'rewardPerEpisode': 28.442855860461542
'totalSteps': 14080, 'rewardStep': 0.43356581831778784, 'errorList': [], 'lossList': [0.0, -1.3100580883026123, 0.0, 32.546990983486175, 0.0, 0.0, 0.0], 'rewardMean': 0.7330970010738823, 'totalEpisodes': 236, 'stepsPerEpisode': 216, 'rewardPerEpisode': 134.4210946358102
'totalSteps': 15360, 'rewardStep': 0.781212004758074, 'errorList': [], 'lossList': [0.0, -1.3152487194538116, 0.0, 32.1344554233551, 0.0, 0.0, 0.0], 'rewardMean': 0.7213817995603716, 'totalEpisodes': 242, 'stepsPerEpisode': 69, 'rewardPerEpisode': 59.68495002017242
'totalSteps': 16640, 'rewardStep': 0.9474756597844607, 'errorList': [0.7470862971381117, 0.2763340433290616, 0.8810825582025487, 0.10440063635768045, 0.8497195979571891, 0.9511784084816373, 1.0815227956098072, 0.7956818725713408, 1.6725495897836828, 0.38716468963407935, 0.05165938438163288, 1.2433324768101401, 0.7782685532219074, 0.2121273706371261, 0.09571398773606399, 0.8317832114817756, 3.0849216822612338, 3.9157539621703923, 1.371048446888171, 3.6431661049042092, 3.1454586426582294, 0.9452753439973014, 1.2162713020903568, 0.25632067652208357, 1.9052808951135813, 1.7358912500587311, 1.7247176476286321, 1.7759076208375417, 0.40245320262712453, 0.5592409101810555, 3.3858308852258485, 1.795231266209483, 0.6763083915261517, 0.8971799775254427, 0.21176679179088748, 0.9447292170907727, 1.2566682496018025, 4.901849334229949, 1.6494196932589464, 0.5881376389370054, 1.1470994726925268, 0.6327634899394665, 2.7480240394683237, 4.000844154751219, 2.060016476265257, 1.3797801252484563, 3.691945170786187, 2.972450138343122, 3.6639608307713556, 1.9982686132116538], 'lossList': [0.0, -1.325567650794983, 0.0, 20.543359845280648, 0.0, 0.0, 0.0], 'rewardMean': 0.7400445178692052, 'totalEpisodes': 244, 'stepsPerEpisode': 170, 'rewardPerEpisode': 152.8473598184274, 'successfulTests': 3
'totalSteps': 17920, 'rewardStep': 0.9311516261672667, 'errorList': [2.1317793006595998, 0.5260991561494682, 0.4889196606345777, 0.5355377528912358, 0.21411708299881468, 1.7495033731059537, 0.25086864548403615, 0.12723518627857117, 1.7135583229764744, 0.12638425901195288, 2.362443458333159, 1.2919853663696073, 2.2263896460905643, 0.19894298283133385, 2.0479081249751796, 0.9950628690042812, 1.1597758811427372, 1.3373786494309359, 0.6654378832001776, 0.291126741388429, 1.1247694504550032, 1.4412409513084408, 1.082009414468323, 0.3379620610533438, 2.1843082821904884, 1.1132959652201193, 0.4718850675532175, 1.3878441466883205, 0.3178713333176668, 0.33474118288839927, 0.4694752911085013, 1.7180576429326238, 1.3035833120203393, 0.7091108220590746, 0.2396047745915751, 1.2686187845846284, 1.3351668225537783, 1.2815941427550686, 1.7816368213132445, 0.18339317992116422, 1.1713034093589456, 2.473206246239678, 1.6628347238545134, 0.1723293955250623, 0.1268453838472684, 1.9412978366157327, 1.2551244609373686, 1.2073621270454213, 0.3534132414008503, 0.2467829700387841], 'lossList': [0.0, -1.322610393166542, 0.0, 14.710712978839874, 0.0, 0.0, 0.0], 'rewardMean': 0.7662278712078389, 'totalEpisodes': 247, 'stepsPerEpisode': 48, 'rewardPerEpisode': 36.86484791256268, 'successfulTests': 6
'totalSteps': 19200, 'rewardStep': 0.5846280212312045, 'errorList': [], 'lossList': [0.0, -1.310806023478508, 0.0, 19.33786800146103, 0.0, 0.0, 0.0], 'rewardMean': 0.7652023125566199, 'totalEpisodes': 249, 'stepsPerEpisode': 407, 'rewardPerEpisode': 319.3365235251344
'totalSteps': 20480, 'rewardStep': 0.6140800880817443, 'errorList': [], 'lossList': [0.0, -1.2951457571983338, 0.0, 12.671405987739563, 0.0, 0.0, 0.0], 'rewardMean': 0.7494885861022091, 'totalEpisodes': 252, 'stepsPerEpisode': 363, 'rewardPerEpisode': 270.8502751993375
'totalSteps': 21760, 'rewardStep': 0.8608827547603294, 'errorList': [], 'lossList': [0.0, -1.2795115780830384, 0.0, 8.94912079334259, 0.0, 0.0, 0.0], 'rewardMean': 0.7566618386521723, 'totalEpisodes': 257, 'stepsPerEpisode': 70, 'rewardPerEpisode': 56.812884235445225
'totalSteps': 23040, 'rewardStep': 0.3299929283291581, 'errorList': [], 'lossList': [0.0, -1.2675816369056703, 0.0, 3.7882688003778457, 0.0, 0.0, 0.0], 'rewardMean': 0.7092556048803661, 'totalEpisodes': 259, 'stepsPerEpisode': 455, 'rewardPerEpisode': 334.297410656586
'totalSteps': 24320, 'rewardStep': 0.9214515510114245, 'errorList': [], 'lossList': [0.0, -1.2600830525159836, 0.0, 3.8600383265316487, 0.0, 0.0, 0.0], 'rewardMean': 0.7173612036998473, 'totalEpisodes': 260, 'stepsPerEpisode': 1206, 'rewardPerEpisode': 1040.2538024896444
'totalSteps': 25600, 'rewardStep': 0.7270323571818482, 'errorList': [], 'lossList': [0.0, -1.2463002997636794, 0.0, 1.9713199201226235, 0.0, 0.0, 0.0], 'rewardMean': 0.7131472809623298, 'totalEpisodes': 260, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.0310359921646
#maxSuccessfulTests=6, maxSuccessfulTestsAtStep=17920, timeSpent=104.84
