#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 21
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.840985145642346, 'errorList': [], 'lossList': [0.0, -1.4493587213754653, 0.0, 38.99321163654327, 0.0, 0.0, 0.0], 'rewardMean': 0.7170814826112527, 'totalEpisodes': 12, 'stepsPerEpisode': 66, 'rewardPerEpisode': 59.70571894917941
'totalSteps': 3840, 'rewardStep': 0.8880932251458458, 'errorList': [], 'lossList': [0.0, -1.4640239316225052, 0.0, 30.429792145490648, 0.0, 0.0, 0.0], 'rewardMean': 0.7740853967894504, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.3855545540193
'totalSteps': 5120, 'rewardStep': 0.9153928716517173, 'errorList': [], 'lossList': [0.0, -1.4507713788747787, 0.0, 35.34827413678169, 0.0, 0.0, 0.0], 'rewardMean': 0.8094122655050171, 'totalEpisodes': 14, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.068483028772104
'totalSteps': 6400, 'rewardStep': 0.46063149904459727, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6931520100182106, 'totalEpisodes': 67, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.79282423667561
'totalSteps': 7680, 'rewardStep': 0.506115205272579, 'errorList': [], 'lossList': [0.0, -1.443889132142067, 0.0, 242.96383224487306, 0.0, 0.0, 0.0], 'rewardMean': 0.6664324664831203, 'totalEpisodes': 115, 'stepsPerEpisode': 20, 'rewardPerEpisode': 11.960965913368364
'totalSteps': 8960, 'rewardStep': 0.8454634060702981, 'errorList': [], 'lossList': [0.0, -1.4400585597753526, 0.0, 90.22003620147706, 0.0, 0.0, 0.0], 'rewardMean': 0.6888113339315175, 'totalEpisodes': 158, 'stepsPerEpisode': 93, 'rewardPerEpisode': 78.05651882948054
'totalSteps': 10240, 'rewardStep': 0.7640196277273162, 'errorList': [], 'lossList': [0.0, -1.4340602195262908, 0.0, 48.40786725997925, 0.0, 0.0, 0.0], 'rewardMean': 0.6971678110199396, 'totalEpisodes': 194, 'stepsPerEpisode': 28, 'rewardPerEpisode': 17.98740525075812
'totalSteps': 11520, 'rewardStep': 0.4768867839810225, 'errorList': [], 'lossList': [0.0, -1.4284605014324188, 0.0, 30.407334275245667, 0.0, 0.0, 0.0], 'rewardMean': 0.675139708316048, 'totalEpisodes': 210, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.070077095173042
'totalSteps': 12800, 'rewardStep': 0.5320151566904321, 'errorList': [], 'lossList': [0.0, -1.4060830557346344, 0.0, 20.53089791059494, 0.0, 0.0, 0.0], 'rewardMean': 0.6690234420270752, 'totalEpisodes': 220, 'stepsPerEpisode': 133, 'rewardPerEpisode': 102.66983965359584
'totalSteps': 14080, 'rewardStep': 0.6375962195461122, 'errorList': [], 'lossList': [0.0, -1.3949021357297897, 0.0, 22.68474962711334, 0.0, 0.0, 0.0], 'rewardMean': 0.6486845494174518, 'totalEpisodes': 229, 'stepsPerEpisode': 158, 'rewardPerEpisode': 128.92623563650966
'totalSteps': 15360, 'rewardStep': 0.9735820669216223, 'errorList': [6.033480945979355, 13.22992371883218, 18.233178169140352, 16.950128867337092, 8.932009004547053, 15.121243624443514, 1.7880394513125424, 16.474245311407547, 0.35647749566974507, 15.291732285115117, 45.976671836654674, 4.022277536746213, 75.63221460577391, 6.082856799858675, 15.82487921707186, 4.4134715704565055, 2.48009254978625, 1.2396001632803253, 13.391875982347273, 4.687195433323663, 4.0653965382225605, 10.614115355772745, 77.03583833696987, 13.95978112186412, 18.873378982605423, 5.432436530351357, 12.852865342526883, 11.933949464320678, 1.9853130737506548, 1.4522524534122403, 2.3562833345886904, 4.193465134003858, 3.8187091331520278, 36.967716799073024, 57.06000804450182, 47.34952247745322, 34.8329387229469, 6.191417397043112, 30.843299189738907, 8.971596653074386, 14.080503789945833, 19.15779103975026, 5.481184228005836, 24.205209716903717, 48.715624806604175, 3.2641879948263015, 3.1712224610598336, 3.962240864927094, 20.549970183845808, 53.277207344691675], 'lossList': [0.0, -1.383325344324112, 0.0, 12.183162950277328, 0.0, 0.0, 0.0], 'rewardMean': 0.6572334335950294, 'totalEpisodes': 233, 'stepsPerEpisode': 64, 'rewardPerEpisode': 55.90906026908133, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7877460182943218, 'errorList': [], 'lossList': [0.0, -1.3754809486865998, 0.0, 11.963906079530716, 0.0, 0.0, 0.0], 'rewardMean': 0.6444687482592899, 'totalEpisodes': 237, 'stepsPerEpisode': 135, 'rewardPerEpisode': 108.27715689132066
'totalSteps': 17920, 'rewardStep': 0.858650971731976, 'errorList': [], 'lossList': [0.0, -1.3633381909132003, 0.0, 7.6920480012893675, 0.0, 0.0, 0.0], 'rewardMean': 0.6842706955280277, 'totalEpisodes': 241, 'stepsPerEpisode': 228, 'rewardPerEpisode': 196.6941667652922
'totalSteps': 19200, 'rewardStep': 0.8273506663264116, 'errorList': [], 'lossList': [0.0, -1.3454456967115402, 0.0, 4.928304196596145, 0.0, 0.0, 0.0], 'rewardMean': 0.7209426122562091, 'totalEpisodes': 243, 'stepsPerEpisode': 717, 'rewardPerEpisode': 609.2030741249584
'totalSteps': 20480, 'rewardStep': 0.21317704082370825, 'errorList': [], 'lossList': [0.0, -1.3416855084896087, 0.0, 5.81136285007, 0.0, 0.0, 0.0], 'rewardMean': 0.6916487958113221, 'totalEpisodes': 244, 'stepsPerEpisode': 623, 'rewardPerEpisode': 508.25909221362446
'totalSteps': 21760, 'rewardStep': 0.5657558575946301, 'errorList': [], 'lossList': [0.0, -1.3226254099607468, 0.0, 3.8559240144491196, 0.0, 0.0, 0.0], 'rewardMean': 0.6636780409637553, 'totalEpisodes': 245, 'stepsPerEpisode': 1278, 'rewardPerEpisode': 970.0697850973011
'totalSteps': 23040, 'rewardStep': 0.832083825495467, 'errorList': [], 'lossList': [0.0, -1.292711016535759, 0.0, 3.5144426155090334, 0.0, 0.0, 0.0], 'rewardMean': 0.6704844607405704, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.632311725142
'totalSteps': 24320, 'rewardStep': 0.8487764135592742, 'errorList': [], 'lossList': [0.0, -1.2580120128393173, 0.0, 1.6599347323179245, 0.0, 0.0, 0.0], 'rewardMean': 0.7076734236983955, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.0808758162477
'totalSteps': 25600, 'rewardStep': 0.9142264298919076, 'errorList': [], 'lossList': [0.0, -1.1899586719274522, 0.0, 1.060509524643421, 0.0, 0.0, 0.0], 'rewardMean': 0.745894551018543, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.758894844608
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.53
