#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 22
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8657007970189358, 'errorList': [], 'lossList': [0.0, -1.4500640577077866, 0.0, 32.88606756925583, 0.0, 0.0, 0.0], 'rewardMean': 0.6844008224614737, 'totalEpisodes': 12, 'stepsPerEpisode': 904, 'rewardPerEpisode': 660.6355819773647
'totalSteps': 3840, 'rewardStep': 0.8268942711529932, 'errorList': [], 'lossList': [0.0, -1.4692161148786544, 0.0, 43.016193799972534, 0.0, 0.0, 0.0], 'rewardMean': 0.7318986386919802, 'totalEpisodes': 15, 'stepsPerEpisode': 489, 'rewardPerEpisode': 399.6525548367102
'totalSteps': 5120, 'rewardStep': 0.6651354931895949, 'errorList': [], 'lossList': [0.0, -1.4647150486707687, 0.0, 38.30325713634491, 0.0, 0.0, 0.0], 'rewardMean': 0.7152078523163838, 'totalEpisodes': 20, 'stepsPerEpisode': 69, 'rewardPerEpisode': 45.92746817076126
'totalSteps': 6400, 'rewardStep': 0.5304161465983049, 'errorList': [], 'lossList': [0.0, -1.4522661501169205, 0.0, 212.27464637756347, 0.0, 0.0, 0.0], 'rewardMean': 0.678249511172768, 'totalEpisodes': 82, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.85639720066801
'totalSteps': 7680, 'rewardStep': 0.4126596782070723, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6023667017539979, 'totalEpisodes': 138, 'stepsPerEpisode': 19, 'rewardPerEpisode': 11.250477867201143
'totalSteps': 8960, 'rewardStep': 0.9694296854253458, 'errorList': [190.95281399891795, 200.5670812602412, 212.8005884321905, 193.5693561520517, 168.9015839560909, 202.56932132771232, 175.13802157031546, 205.68136921181147, 222.13547500509378, 194.06411503954084, 214.1907608708326, 214.14397473525173, 207.2487591644201, 218.23256741695326, 209.54082222942236, 219.42855271019852, 218.36890933047684, 158.078464336608, 172.1023936938279, 221.34318450147137, 201.31953730729467, 198.4349288294681, 213.79901503619956, 215.63597671296742, 217.1833574615886, 213.90147520327315, 135.01269108641642, 198.62018863907699, 167.5966466887385, 136.7006652049136, 191.23515531633342, 211.50696204664277, 195.55372021656217, 182.0856967743453, 198.2955721234135, 210.9733799582528, 182.93851782857396, 196.34529221324465, 129.87636608416966, 126.57511500583087, 188.91181104687135, 216.02720223164837, 181.38955706118966, 155.11297656959582, 212.96876636858792, 188.25281475164545, 219.70941652810953, 217.59418641819352, 198.75959440563895, 197.79415925694155], 'lossList': [0.0, -1.4520173460245132, 0.0, 93.50473279953003, 0.0, 0.0, 0.0], 'rewardMean': 0.6482495747129163, 'totalEpisodes': 184, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.597827788910093, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.5911895816126325, 'errorList': [], 'lossList': [0.0, -1.442195464372635, 0.0, 70.34451045989991, 0.0, 0.0, 0.0], 'rewardMean': 0.6419095754795515, 'totalEpisodes': 223, 'stepsPerEpisode': 58, 'rewardPerEpisode': 49.33434834423775
'totalSteps': 11520, 'rewardStep': 0.8885197034427638, 'errorList': [], 'lossList': [0.0, -1.4413340502977372, 0.0, 46.153634519577025, 0.0, 0.0, 0.0], 'rewardMean': 0.6665705882758728, 'totalEpisodes': 244, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.312566532300398
'totalSteps': 12800, 'rewardStep': 0.8039850896499307, 'errorList': [], 'lossList': [0.0, -1.441404173374176, 0.0, 49.835153903961185, 0.0, 0.0, 0.0], 'rewardMean': 0.6966590124504647, 'totalEpisodes': 257, 'stepsPerEpisode': 106, 'rewardPerEpisode': 84.93307484030278
'totalSteps': 14080, 'rewardStep': 0.6769724721778921, 'errorList': [], 'lossList': [0.0, -1.4394998776912689, 0.0, 35.10831911087036, 0.0, 0.0, 0.0], 'rewardMean': 0.6777861799663603, 'totalEpisodes': 265, 'stepsPerEpisode': 43, 'rewardPerEpisode': 31.71408625427709
'totalSteps': 15360, 'rewardStep': 0.9296445844895167, 'errorList': [], 'lossList': [0.0, -1.4242426359653473, 0.0, 14.527130680084229, 0.0, 0.0, 0.0], 'rewardMean': 0.6880612113000126, 'totalEpisodes': 271, 'stepsPerEpisode': 202, 'rewardPerEpisode': 163.90003256181686
'totalSteps': 16640, 'rewardStep': 0.9047774506450532, 'errorList': [], 'lossList': [0.0, -1.4271207416057587, 0.0, 15.234938459396362, 0.0, 0.0, 0.0], 'rewardMean': 0.7120254070455585, 'totalEpisodes': 276, 'stepsPerEpisode': 61, 'rewardPerEpisode': 51.71100485141376
'totalSteps': 17920, 'rewardStep': 0.8203574050575809, 'errorList': [], 'lossList': [0.0, -1.4252282345294953, 0.0, 11.565576402544975, 0.0, 0.0, 0.0], 'rewardMean': 0.741019532891486, 'totalEpisodes': 280, 'stepsPerEpisode': 85, 'rewardPerEpisode': 72.02882300319295
'totalSteps': 19200, 'rewardStep': 0.7348671264645803, 'errorList': [], 'lossList': [0.0, -1.3995940846204757, 0.0, 24.84987260222435, 0.0, 0.0, 0.0], 'rewardMean': 0.7732402777172369, 'totalEpisodes': 284, 'stepsPerEpisode': 251, 'rewardPerEpisode': 204.68672907348173
'totalSteps': 20480, 'rewardStep': 0.7648934934369851, 'errorList': [], 'lossList': [0.0, -1.3768469470739364, 0.0, 4.663576845824719, 0.0, 0.0, 0.0], 'rewardMean': 0.8084636592402281, 'totalEpisodes': 288, 'stepsPerEpisode': 201, 'rewardPerEpisode': 161.32384806953553
'totalSteps': 21760, 'rewardStep': 0.8529853958755906, 'errorList': [], 'lossList': [0.0, -1.3512281095981598, 0.0, 4.188486943244934, 0.0, 0.0, 0.0], 'rewardMean': 0.7968192302852526, 'totalEpisodes': 291, 'stepsPerEpisode': 37, 'rewardPerEpisode': 34.515020257503906
'totalSteps': 23040, 'rewardStep': 0.883203076728654, 'errorList': [], 'lossList': [0.0, -1.3304980820417405, 0.0, 2.913788052499294, 0.0, 0.0, 0.0], 'rewardMean': 0.8260205797968547, 'totalEpisodes': 293, 'stepsPerEpisode': 466, 'rewardPerEpisode': 385.1791282819988
'totalSteps': 24320, 'rewardStep': 0.7647732728326513, 'errorList': [], 'lossList': [0.0, -1.31702163875103, 0.0, 3.069441479444504, 0.0, 0.0, 0.0], 'rewardMean': 0.8136459367358434, 'totalEpisodes': 295, 'stepsPerEpisode': 206, 'rewardPerEpisode': 177.05467325652782
'totalSteps': 25600, 'rewardStep': 0.6734711011807365, 'errorList': [], 'lossList': [0.0, -1.3003814244270324, 0.0, 5.0273139905929565, 0.0, 0.0, 0.0], 'rewardMean': 0.8005945378889241, 'totalEpisodes': 298, 'stepsPerEpisode': 454, 'rewardPerEpisode': 340.2821681453455
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.73
