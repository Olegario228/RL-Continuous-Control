#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 23
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5390165548587871, 'errorList': [], 'lossList': [0.0, -1.4220072418451308, 0.0, 31.56872453331947, 0.0, 0.0, 0.0], 'rewardMean': 0.7189734853427637, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 31.891960949200257
'totalSteps': 3840, 'rewardStep': 0.9497703692133851, 'errorList': [], 'lossList': [0.0, -1.4147843265533446, 0.0, 30.83180460691452, 0.0, 0.0, 0.0], 'rewardMean': 0.7959057799663042, 'totalEpisodes': 18, 'stepsPerEpisode': 489, 'rewardPerEpisode': 388.37293263698535
'totalSteps': 5120, 'rewardStep': 0.7441647065981846, 'errorList': [], 'lossList': [0.0, -1.4053344708681106, 0.0, 37.52299365520477, 0.0, 0.0, 0.0], 'rewardMean': 0.7829705116242742, 'totalEpisodes': 21, 'stepsPerEpisode': 77, 'rewardPerEpisode': 69.01585523827909
'totalSteps': 6400, 'rewardStep': 0.8186099417285363, 'errorList': [], 'lossList': [0.0, -1.4102988868951798, 0.0, 266.30651401519776, 0.0, 0.0, 0.0], 'rewardMean': 0.7900983976451267, 'totalEpisodes': 89, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.90736271055844
'totalSteps': 7680, 'rewardStep': 0.6714940534396778, 'errorList': [], 'lossList': [0.0, -1.405691933631897, 0.0, 76.1517042350769, 0.0, 0.0, 0.0], 'rewardMean': 0.7703310069442185, 'totalEpisodes': 151, 'stepsPerEpisode': 60, 'rewardPerEpisode': 46.19345057277263
'totalSteps': 8960, 'rewardStep': 0.7429797074332147, 'errorList': [], 'lossList': [0.0, -1.4063041812181474, 0.0, 58.79466510772705, 0.0, 0.0, 0.0], 'rewardMean': 0.7664236784426465, 'totalEpisodes': 193, 'stepsPerEpisode': 75, 'rewardPerEpisode': 62.2016916483153
'totalSteps': 10240, 'rewardStep': 0.8185344745550057, 'errorList': [], 'lossList': [0.0, -1.4033893436193465, 0.0, 39.825306816101076, 0.0, 0.0, 0.0], 'rewardMean': 0.7729375279566915, 'totalEpisodes': 205, 'stepsPerEpisode': 33, 'rewardPerEpisode': 28.04409362302594
'totalSteps': 11520, 'rewardStep': 0.5767714783070171, 'errorList': [], 'lossList': [0.0, -1.3862910401821136, 0.0, 43.52642378807068, 0.0, 0.0, 0.0], 'rewardMean': 0.7511413002178388, 'totalEpisodes': 216, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.4871276811763026
'totalSteps': 12800, 'rewardStep': 0.4698041251027658, 'errorList': [], 'lossList': [0.0, -1.3689802211523057, 0.0, 26.456766152381896, 0.0, 0.0, 0.0], 'rewardMean': 0.7230075827063315, 'totalEpisodes': 220, 'stepsPerEpisode': 265, 'rewardPerEpisode': 180.58969142110016
'totalSteps': 14080, 'rewardStep': 0.7959477231255074, 'errorList': [], 'lossList': [0.0, -1.352914689183235, 0.0, 12.255353949069978, 0.0, 0.0, 0.0], 'rewardMean': 0.7127093134362082, 'totalEpisodes': 222, 'stepsPerEpisode': 332, 'rewardPerEpisode': 264.13938097847574
'totalSteps': 15360, 'rewardStep': 0.7656130277849177, 'errorList': [], 'lossList': [0.0, -1.3416891115903855, 0.0, 8.230629397630691, 0.0, 0.0, 0.0], 'rewardMean': 0.7353689607288212, 'totalEpisodes': 226, 'stepsPerEpisode': 120, 'rewardPerEpisode': 104.56754463031668
'totalSteps': 16640, 'rewardStep': 0.39814341888022237, 'errorList': [], 'lossList': [0.0, -1.3435920536518098, 0.0, 6.257722138762474, 0.0, 0.0, 0.0], 'rewardMean': 0.6802062656955049, 'totalEpisodes': 228, 'stepsPerEpisode': 552, 'rewardPerEpisode': 430.91644337812653
'totalSteps': 17920, 'rewardStep': 0.7553031151821114, 'errorList': [], 'lossList': [0.0, -1.3496538907289506, 0.0, 4.264116326570511, 0.0, 0.0, 0.0], 'rewardMean': 0.6813201065538975, 'totalEpisodes': 230, 'stepsPerEpisode': 592, 'rewardPerEpisode': 490.7935235440613
'totalSteps': 19200, 'rewardStep': 0.9205083676056908, 'errorList': [], 'lossList': [0.0, -1.336388068795204, 0.0, 5.143127277493477, 0.0, 0.0, 0.0], 'rewardMean': 0.691509949141613, 'totalEpisodes': 233, 'stepsPerEpisode': 220, 'rewardPerEpisode': 183.931543676901
'totalSteps': 20480, 'rewardStep': 0.7790166822672773, 'errorList': [], 'lossList': [0.0, -1.333054288625717, 0.0, 4.950907807350159, 0.0, 0.0, 0.0], 'rewardMean': 0.702262212024373, 'totalEpisodes': 235, 'stepsPerEpisode': 280, 'rewardPerEpisode': 237.87486147806567
'totalSteps': 21760, 'rewardStep': 0.7275090292714088, 'errorList': [], 'lossList': [0.0, -1.320548951625824, 0.0, 2.8590130615234375, 0.0, 0.0, 0.0], 'rewardMean': 0.7007151442081925, 'totalEpisodes': 236, 'stepsPerEpisode': 929, 'rewardPerEpisode': 720.0597390883712
'totalSteps': 23040, 'rewardStep': 0.7651178006041532, 'errorList': [], 'lossList': [0.0, -1.2882977002859115, 0.0, 1.9993209648132324, 0.0, 0.0, 0.0], 'rewardMean': 0.695373476813107, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 920.9338430479005
'totalSteps': 24320, 'rewardStep': 0.7140323126735543, 'errorList': [], 'lossList': [0.0, -1.260249787569046, 0.0, 4.133223456740379, 0.0, 0.0, 0.0], 'rewardMean': 0.7090995602497608, 'totalEpisodes': 238, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.12320928551007
'totalSteps': 25600, 'rewardStep': 0.970967329866374, 'errorList': [0.07430208661293704, 0.07461299812615443, 0.08243893473298546, 0.074589284235823, 0.07427279770875476, 0.07464872756413307, 0.12747714712111508, 0.07463795299528123, 0.0744126863381669, 0.0746542606578402, 0.07422071149536706, 0.07465290408145757, 0.07453581832027854, 0.07460197469314626, 0.13224509578293747, 0.0745670951270512, 0.07430390206681965, 0.07455395878133375, 0.10471425440220744, 0.07419630609681649, 0.07458260491212097, 0.07406798700261097, 0.07461313531495044, 0.08181931317080923, 0.07446957355154546, 0.07467302358347243, 0.08306667526568758, 0.12068498972536337, 0.0745409605592799, 0.12818465946574534, 0.1240884476159896, 0.07464349280010996, 0.07434466668767625, 0.07464006632247117, 0.07441255492741322, 0.07450643682112557, 0.07421758700835966, 0.07451706861325781, 0.09617989010501808, 0.12678821318620087, 0.0744796050183111, 0.07446736321495957, 0.07431027697424253, 0.07724811481042619, 0.07462062693923818, 0.07459632560481484, 0.11820156533219796, 0.07432839567223569, 0.07416568709816358, 0.09456600867731639], 'lossList': [0.0, -1.2388788288831711, 0.0, 1.2906652131676675, 0.0, 0.0, 0.0], 'rewardMean': 0.7592158807261218, 'totalEpisodes': 238, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.472987811792, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=84.11
