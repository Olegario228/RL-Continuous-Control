#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 24
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8962317427690479, 'errorList': [], 'lossList': [0.0, -1.4224282896518707, 0.0, 35.622966019511225, 0.0, 0.0, 0.0], 'rewardMean': 0.8227613035341645, 'totalEpisodes': 13, 'stepsPerEpisode': 310, 'rewardPerEpisode': 263.2697389094221
'totalSteps': 3840, 'rewardStep': 0.6782027862239971, 'errorList': [], 'lossList': [0.0, -1.4274212938547135, 0.0, 31.29931357383728, 0.0, 0.0, 0.0], 'rewardMean': 0.7745751310974421, 'totalEpisodes': 16, 'stepsPerEpisode': 177, 'rewardPerEpisode': 126.22678023303291
'totalSteps': 5120, 'rewardStep': 0.43539493004049123, 'errorList': [], 'lossList': [0.0, -1.4359297955036163, 0.0, 43.11230990409851, 0.0, 0.0, 0.0], 'rewardMean': 0.6897800808332044, 'totalEpisodes': 21, 'stepsPerEpisode': 241, 'rewardPerEpisode': 171.2228567519277
'totalSteps': 6400, 'rewardStep': 0.4378579410126981, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.605806034226369, 'totalEpisodes': 83, 'stepsPerEpisode': 15, 'rewardPerEpisode': 7.722115694947764
'totalSteps': 7680, 'rewardStep': 0.7085480979126938, 'errorList': [], 'lossList': [0.0, -1.4396702927350997, 0.0, 246.81444511413574, 0.0, 0.0, 0.0], 'rewardMean': 0.620483471895844, 'totalEpisodes': 154, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.609920510453526
'totalSteps': 8960, 'rewardStep': 0.9464456582084706, 'errorList': [171.89970714920614, 162.46449075425963, 168.45108216231594, 154.99619886676848, 171.77147749274943, 145.67974598752153, 176.97881247250552, 163.3395988360971, 156.86322390014962, 168.20404321490767, 124.51723371886452, 159.96943225988414, 140.1845329124392, 170.6861389426172, 157.28980346932465, 156.03120157544234, 152.95381175256145, 159.22024559093498, 157.5366342781831, 161.06118495212695, 157.5198135688719, 172.6869914725874, 153.2042743928124, 148.62284211520816, 146.3887225758546, 172.65629456675273, 170.07949264163446, 145.1149654456958, 174.89725392936188, 176.15143420858993, 87.80240631383681, 169.26328796019007, 149.41861710929143, 163.79949238827936, 160.9582677861785, 161.5059989950125, 175.44941444171278, 156.05611134302788, 157.76213737149394, 156.62054153420533, 164.7986540362586, 173.41026276447775, 164.1309342322142, 171.49699843284202, 149.72333976853142, 147.46777277773126, 172.9950885180095, 160.64356407368788, 154.86757600828227, 160.08597270684263], 'lossList': [0.0, -1.4374532121419907, 0.0, 94.19433631896973, 0.0, 0.0, 0.0], 'rewardMean': 0.6612287451849224, 'totalEpisodes': 213, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.426470197443122, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.7991519427885279, 'errorList': [], 'lossList': [0.0, -1.4390606415271758, 0.0, 68.95848752975463, 0.0, 0.0, 0.0], 'rewardMean': 0.6765535449186564, 'totalEpisodes': 263, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.092731844436663
'totalSteps': 11520, 'rewardStep': 0.8492297650297679, 'errorList': [], 'lossList': [0.0, -1.4469096130132675, 0.0, 46.49580661773682, 0.0, 0.0, 0.0], 'rewardMean': 0.6938211669297675, 'totalEpisodes': 296, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.753376483291191
'totalSteps': 12800, 'rewardStep': 0.6711707497549553, 'errorList': [], 'lossList': [0.0, -1.4500264543294907, 0.0, 45.75103345870971, 0.0, 0.0, 0.0], 'rewardMean': 0.6860091554753348, 'totalEpisodes': 307, 'stepsPerEpisode': 67, 'rewardPerEpisode': 53.03089703200726
'totalSteps': 14080, 'rewardStep': 0.6617530383705756, 'errorList': [], 'lossList': [0.0, -1.4426505464315413, 0.0, 43.613415637016296, 0.0, 0.0, 0.0], 'rewardMean': 0.6625612850354875, 'totalEpisodes': 315, 'stepsPerEpisode': 360, 'rewardPerEpisode': 260.4240430611385
'totalSteps': 15360, 'rewardStep': 0.8246634434499273, 'errorList': [], 'lossList': [0.0, -1.4164544123411178, 0.0, 65.4150329208374, 0.0, 0.0, 0.0], 'rewardMean': 0.6772073507580806, 'totalEpisodes': 325, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8246634434499273
'totalSteps': 16640, 'rewardStep': 0.35994842525902515, 'errorList': [], 'lossList': [0.0, -1.4028711354732513, 0.0, 21.589029333591462, 0.0, 0.0, 0.0], 'rewardMean': 0.669662700279934, 'totalEpisodes': 331, 'stepsPerEpisode': 158, 'rewardPerEpisode': 118.23369512792574
'totalSteps': 17920, 'rewardStep': 0.6823876413211535, 'errorList': [], 'lossList': [0.0, -1.3971505379676818, 0.0, 24.683391473293305, 0.0, 0.0, 0.0], 'rewardMean': 0.6941156703107796, 'totalEpisodes': 336, 'stepsPerEpisode': 55, 'rewardPerEpisode': 47.01962959152325
'totalSteps': 19200, 'rewardStep': 0.91594028703296, 'errorList': [], 'lossList': [0.0, -1.383493074774742, 0.0, 10.57277148127556, 0.0, 0.0, 0.0], 'rewardMean': 0.7419239049128057, 'totalEpisodes': 340, 'stepsPerEpisode': 78, 'rewardPerEpisode': 70.16950911608166
'totalSteps': 20480, 'rewardStep': 0.7948818512225633, 'errorList': [], 'lossList': [0.0, -1.3628699898719787, 0.0, 10.092323538064957, 0.0, 0.0, 0.0], 'rewardMean': 0.7505572802437926, 'totalEpisodes': 344, 'stepsPerEpisode': 137, 'rewardPerEpisode': 107.01832269602481
'totalSteps': 21760, 'rewardStep': 0.6686597192023609, 'errorList': [], 'lossList': [0.0, -1.3524046260118485, 0.0, 7.013066682219505, 0.0, 0.0, 0.0], 'rewardMean': 0.7227786863431815, 'totalEpisodes': 347, 'stepsPerEpisode': 687, 'rewardPerEpisode': 604.039876966462
'totalSteps': 23040, 'rewardStep': 0.7082304897731058, 'errorList': [], 'lossList': [0.0, -1.333563083410263, 0.0, 6.449426866769791, 0.0, 0.0, 0.0], 'rewardMean': 0.7136865410416395, 'totalEpisodes': 351, 'stepsPerEpisode': 220, 'rewardPerEpisode': 178.95769662763198
'totalSteps': 24320, 'rewardStep': 0.7603529040269748, 'errorList': [], 'lossList': [0.0, -1.339470204114914, 0.0, 4.440596899986267, 0.0, 0.0, 0.0], 'rewardMean': 0.7047988549413601, 'totalEpisodes': 353, 'stepsPerEpisode': 364, 'rewardPerEpisode': 293.98686322455444
'totalSteps': 25600, 'rewardStep': 0.6584288376012248, 'errorList': [], 'lossList': [0.0, -1.3368567460775376, 0.0, 2.2881731727719306, 0.0, 0.0, 0.0], 'rewardMean': 0.7035246637259871, 'totalEpisodes': 353, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 927.4839214274733
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.69
