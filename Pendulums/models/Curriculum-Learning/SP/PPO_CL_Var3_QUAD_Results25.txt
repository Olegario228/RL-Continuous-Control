#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 25
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7232637238312791, 'errorList': [], 'lossList': [0.0, -1.4116801339387894, 0.0, 32.21066035270691, 0.0, 0.0, 0.0], 'rewardMean': 0.5837432324033984, 'totalEpisodes': 56, 'stepsPerEpisode': 23, 'rewardPerEpisode': 14.674651230762665
'totalSteps': 3840, 'rewardStep': 0.5665025322802231, 'errorList': [], 'lossList': [0.0, -1.4020038276910782, 0.0, 40.60594400405884, 0.0, 0.0, 0.0], 'rewardMean': 0.5779963323623399, 'totalEpisodes': 78, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.3715239825200776
'totalSteps': 5120, 'rewardStep': 0.7124734771493717, 'errorList': [], 'lossList': [0.0, -1.3866936004161834, 0.0, 42.60483675956726, 0.0, 0.0, 0.0], 'rewardMean': 0.611615618559098, 'totalEpisodes': 93, 'stepsPerEpisode': 68, 'rewardPerEpisode': 53.77897072454398
'totalSteps': 6400, 'rewardStep': 0.8072021451943603, 'errorList': [], 'lossList': [0.0, -1.368797828555107, 0.0, 86.73422451019287, 0.0, 0.0, 0.0], 'rewardMean': 0.6507329238861504, 'totalEpisodes': 121, 'stepsPerEpisode': 34, 'rewardPerEpisode': 27.976710107471966
'totalSteps': 7680, 'rewardStep': 0.9217750314874626, 'errorList': [], 'lossList': [0.0, -1.3763978415727616, 0.0, 57.99040289878845, 0.0, 0.0, 0.0], 'rewardMean': 0.6959066084863691, 'totalEpisodes': 145, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.794537490513306
'totalSteps': 8960, 'rewardStep': 0.8434302416296503, 'errorList': [], 'lossList': [0.0, -1.384734752178192, 0.0, 49.76058033943176, 0.0, 0.0, 0.0], 'rewardMean': 0.7169814132211235, 'totalEpisodes': 163, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.094065141617964
'totalSteps': 10240, 'rewardStep': 0.16002358823728435, 'errorList': [], 'lossList': [0.0, -1.3734591919183732, 0.0, 36.49577407360077, 0.0, 0.0, 0.0], 'rewardMean': 0.6473616850981436, 'totalEpisodes': 173, 'stepsPerEpisode': 110, 'rewardPerEpisode': 69.74180875128671
'totalSteps': 11520, 'rewardStep': 0.7052214030257666, 'errorList': [], 'lossList': [0.0, -1.360498034954071, 0.0, 23.987319767475128, 0.0, 0.0, 0.0], 'rewardMean': 0.6537905426456573, 'totalEpisodes': 177, 'stepsPerEpisode': 69, 'rewardPerEpisode': 56.045726568063145
'totalSteps': 12800, 'rewardStep': 0.7090001738366124, 'errorList': [], 'lossList': [0.0, -1.3507953202724456, 0.0, 9.2329319190979, 0.0, 0.0, 0.0], 'rewardMean': 0.6593115057647527, 'totalEpisodes': 181, 'stepsPerEpisode': 108, 'rewardPerEpisode': 88.90743457158479
'totalSteps': 14080, 'rewardStep': 0.7388332280715394, 'errorList': [], 'lossList': [0.0, -1.3415499484539033, 0.0, 6.808580414056778, 0.0, 0.0, 0.0], 'rewardMean': 0.6887725544743549, 'totalEpisodes': 183, 'stepsPerEpisode': 160, 'rewardPerEpisode': 139.0573805302218
'totalSteps': 15360, 'rewardStep': 0.8601892689622569, 'errorList': [], 'lossList': [0.0, -1.325292876958847, 0.0, 3.5655488395690917, 0.0, 0.0, 0.0], 'rewardMean': 0.7024651089874527, 'totalEpisodes': 184, 'stepsPerEpisode': 37, 'rewardPerEpisode': 32.76088316253067
'totalSteps': 16640, 'rewardStep': 0.9606160738598701, 'errorList': [0.0822878359159669, 0.15348604925235146, 0.12619801521967053, 0.16322417788074225, 0.09859184246281824, 0.12083417251810025, 0.07414492275433782, 0.07225814087494213, 0.0611298484729753, 0.07594663857376337, 0.0859141540539752, 0.18088108407526146, 0.09833465610448719, 0.1315541843208801, 0.09799919526597185, 0.11296735445942664, 0.054764844157121836, 0.1142562572664707, 0.1049153076211337, 0.08652750697504513, 0.08230396125950917, 0.12597301959595436, 0.08832604502552822, 0.06857031485409437, 0.08807811957385137, 0.08208255588426208, 0.06372377976840717, 0.1299184264090285, 0.07381747534120671, 0.05192531918814327, 0.07632220265417487, 0.08924596403593056, 0.10237559320123121, 0.06195563683648242, 0.056871170657383466, 0.11475069064536418, 0.08767521896403825, 0.07795596968861693, 0.0608666037343632, 0.14448610529517988, 0.09267054746315405, 0.08931851996282605, 0.060059730457887484, 0.07754530143764309, 0.09294818390223444, 0.12221722814882446, 0.10137812154708838, 0.05216607565284052, 0.1421111388325036, 0.07944943702288092], 'lossList': [0.0, -1.3246634125709533, 0.0, 5.196538105159998, 0.0, 0.0, 0.0], 'rewardMean': 0.7418764631454174, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.5651705222213, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=16640, timeSpent=62.17
