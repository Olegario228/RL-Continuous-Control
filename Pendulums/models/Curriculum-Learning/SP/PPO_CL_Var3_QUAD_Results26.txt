#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 26
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.596125050607127, 'errorList': [], 'lossList': [0.0, -1.416792528629303, 0.0, 31.175913162231446, 0.0, 0.0, 0.0], 'rewardMean': 0.7055875917242671, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.25154980103212
'totalSteps': 3840, 'rewardStep': 0.6286988209564052, 'errorList': [], 'lossList': [0.0, -1.3977047008275987, 0.0, 45.289755773544314, 0.0, 0.0, 0.0], 'rewardMean': 0.6799580014683131, 'totalEpisodes': 72, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.787578554102355
'totalSteps': 5120, 'rewardStep': 0.5219115874132607, 'errorList': [], 'lossList': [0.0, -1.383754436969757, 0.0, 36.5742834854126, 0.0, 0.0, 0.0], 'rewardMean': 0.64044639795455, 'totalEpisodes': 83, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.274575469429736
'totalSteps': 6400, 'rewardStep': 0.5447572648049253, 'errorList': [], 'lossList': [0.0, -1.3743004912137986, 0.0, 79.56313858032226, 0.0, 0.0, 0.0], 'rewardMean': 0.6213085713246251, 'totalEpisodes': 98, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.981764748372149
'totalSteps': 7680, 'rewardStep': 0.9327335805720608, 'errorList': [3.90125496657572, 0.11216755518468417, 27.86695918971803, 6.119762830097878, 22.746138435210483, 1.3376541068483818, 25.500946201168468, 1.7638238344875938, 1.912433228567267, 11.599569012208251, 2.8399144988593794, 4.750592369060027, 7.024212207995783, 6.075162224435009, 10.760318776811499, 14.033590197504472, 8.515042386606094, 21.16516729499479, 12.226077425704226, 5.583329990549146, 2.6134490553099647, 2.4646994081051137, 2.0459143597619596, 9.06576746322851, 13.19918790981373, 3.431830258303704, 10.14172704594941, 2.417829160504141, 9.504812961414492, 10.025480535490388, 4.987359603751182, 5.355881550409985, 5.901414871137055, 11.708276530069156, 23.20737385406798, 4.1564439880343365, 7.709220001660851, 7.873277489655881, 3.666638930247586, 4.538405187783174, 2.697328144855491, 8.15745062253888, 14.737885932697006, 1.9133680831921405, 4.508881048519355, 1.4149805903095358, 6.733972487429599, 5.524067243003038, 3.176309613353834, 18.453644979305345], 'lossList': [0.0, -1.3777723544836045, 0.0, 86.83184064865112, 0.0, 0.0, 0.0], 'rewardMean': 0.673212739532531, 'totalEpisodes': 114, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.2106922360112735, 'successfulTests': 1
'totalSteps': 8960, 'rewardStep': 0.6652863956299657, 'errorList': [], 'lossList': [0.0, -1.3762657916545868, 0.0, 78.29459844589233, 0.0, 0.0, 0.0], 'rewardMean': 0.6720804046893074, 'totalEpisodes': 132, 'stepsPerEpisode': 111, 'rewardPerEpisode': 94.55249080234891
'totalSteps': 10240, 'rewardStep': 0.7705503712069346, 'errorList': [], 'lossList': [0.0, -1.3672518944740295, 0.0, 27.948938808441163, 0.0, 0.0, 0.0], 'rewardMean': 0.6843891505040108, 'totalEpisodes': 136, 'stepsPerEpisode': 54, 'rewardPerEpisode': 45.884734364311434
'totalSteps': 11520, 'rewardStep': 0.2200022183918171, 'errorList': [], 'lossList': [0.0, -1.350143637061119, 0.0, 36.74871708393097, 0.0, 0.0, 0.0], 'rewardMean': 0.6327906024915448, 'totalEpisodes': 140, 'stepsPerEpisode': 392, 'rewardPerEpisode': 274.373705324305
'totalSteps': 12800, 'rewardStep': 0.7745695707219893, 'errorList': [], 'lossList': [0.0, -1.3282543194293976, 0.0, 20.684527223110198, 0.0, 0.0, 0.0], 'rewardMean': 0.6469684993145892, 'totalEpisodes': 144, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.031293334888545
'totalSteps': 14080, 'rewardStep': 0.4226257911247841, 'errorList': [], 'lossList': [0.0, -1.3018319243192673, 0.0, 8.802167219519616, 0.0, 0.0, 0.0], 'rewardMean': 0.607726065142927, 'totalEpisodes': 145, 'stepsPerEpisode': 911, 'rewardPerEpisode': 682.4207771400859
'totalSteps': 15360, 'rewardStep': 0.7618483421222668, 'errorList': [], 'lossList': [0.0, -1.275610259771347, 0.0, 7.009337533116341, 0.0, 0.0, 0.0], 'rewardMean': 0.624298394294441, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.3769736832586
'totalSteps': 16640, 'rewardStep': 0.7991866014371514, 'errorList': [], 'lossList': [0.0, -1.2495091807842256, 0.0, 2.6563531348109244, 0.0, 0.0, 0.0], 'rewardMean': 0.6413471723425156, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 999.3813077954635
'totalSteps': 17920, 'rewardStep': 0.8496505292599883, 'errorList': [], 'lossList': [0.0, -1.1990954720973968, 0.0, 2.675020953863859, 0.0, 0.0, 0.0], 'rewardMean': 0.6741210665271884, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.7008388915933
'totalSteps': 19200, 'rewardStep': 0.8672226497017508, 'errorList': [], 'lossList': [0.0, -1.1655485951900482, 0.0, 2.654646922536194, 0.0, 0.0, 0.0], 'rewardMean': 0.7063676050168709, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.7703752805708
'totalSteps': 20480, 'rewardStep': 0.924138229153062, 'errorList': [], 'lossList': [0.0, -1.122607581615448, 0.0, 1.7798836099728943, 0.0, 0.0, 0.0], 'rewardMean': 0.7055080698749709, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.803072320759
'totalSteps': 21760, 'rewardStep': 0.8726774908306556, 'errorList': [], 'lossList': [0.0, -1.0618223601579666, 0.0, 1.2206278849951924, 0.0, 0.0, 0.0], 'rewardMean': 0.72624717939504, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1203.757076423044
'totalSteps': 23040, 'rewardStep': 0.9854469631060313, 'errorList': [0.048076186744687804, 0.05020908358078112, 0.03560561041202934, 0.03960662947036515, 0.0657524967866563, 0.03932440555558224, 0.04663356418701964, 0.05332245395712484, 0.031140662489184203, 0.05070302223916988, 0.03412378473900231, 0.030560915868066453, 0.042748771444988294, 0.04611509064487415, 0.05972457357255849, 0.03845237527341404, 0.05209024775167992, 0.03045193663519138, 0.04243185519980499, 0.037983958180170814, 0.06125558724976306, 0.05334973548643877, 0.03801814425478204, 0.05973243267222195, 0.045088064042423374, 0.03010358957829891, 0.05214226282373945, 0.03669367895762422, 0.03229968156047327, 0.040355517991656584, 0.03306782107851373, 0.032400810023844144, 0.05710991428345637, 0.05142305036105133, 0.031311976267867976, 0.028664242486263505, 0.04914438418624639, 0.036021638883493634, 0.04917468912851002, 0.038978810827461324, 0.04074396375405046, 0.05505880360641136, 0.05281701138727389, 0.0777200774129645, 0.04802242965216824, 0.04278151284514197, 0.034042430861999234, 0.03777016763618318, 0.034216490246221455, 0.033107237215839275], 'lossList': [0.0, -1.029440212249756, 0.0, 0.7909258876275271, 0.0, 0.0, 0.0], 'rewardMean': 0.7477368385849497, 'totalEpisodes': 145, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1206.0600501757192, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=97.99
