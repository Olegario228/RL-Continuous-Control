#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 27
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.677385419428941, 'errorList': [], 'lossList': [0.0, -1.4194670647382737, 0.0, 36.00688628196716, 0.0, 0.0, 0.0], 'rewardMean': 0.7465113806063253, 'totalEpisodes': 70, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.7313427014204965
'totalSteps': 3840, 'rewardStep': 0.7838181697050043, 'errorList': [], 'lossList': [0.0, -1.393928188085556, 0.0, 34.20284688472748, 0.0, 0.0, 0.0], 'rewardMean': 0.7589469769725516, 'totalEpisodes': 88, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.185960959519186
'totalSteps': 5120, 'rewardStep': 0.6583699293902496, 'errorList': [], 'lossList': [0.0, -1.3636180579662323, 0.0, 40.98209943771362, 0.0, 0.0, 0.0], 'rewardMean': 0.7338027150769761, 'totalEpisodes': 104, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.66234599555905
'totalSteps': 6400, 'rewardStep': 0.7528246175883235, 'errorList': [], 'lossList': [0.0, -1.3559000325202941, 0.0, 60.016509790420535, 0.0, 0.0, 0.0], 'rewardMean': 0.7376070955792456, 'totalEpisodes': 116, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.311320967587676
'totalSteps': 7680, 'rewardStep': 0.7203161411048253, 'errorList': [], 'lossList': [0.0, -1.345798870921135, 0.0, 104.78216888427734, 0.0, 0.0, 0.0], 'rewardMean': 0.7347252698335089, 'totalEpisodes': 146, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.271359929937121
'totalSteps': 8960, 'rewardStep': 0.4689276760416159, 'errorList': [], 'lossList': [0.0, -1.3397656047344209, 0.0, 35.733217148780824, 0.0, 0.0, 0.0], 'rewardMean': 0.6967541850060955, 'totalEpisodes': 159, 'stepsPerEpisode': 68, 'rewardPerEpisode': 47.63325463267276
'totalSteps': 10240, 'rewardStep': 0.48423737778073017, 'errorList': [], 'lossList': [0.0, -1.3309481412172317, 0.0, 21.546305718421937, 0.0, 0.0, 0.0], 'rewardMean': 0.670189584102925, 'totalEpisodes': 166, 'stepsPerEpisode': 184, 'rewardPerEpisode': 152.9391158063799
'totalSteps': 11520, 'rewardStep': 0.7602555262095765, 'errorList': [], 'lossList': [0.0, -1.3259698837995528, 0.0, 30.19296540260315, 0.0, 0.0, 0.0], 'rewardMean': 0.680196911003664, 'totalEpisodes': 174, 'stepsPerEpisode': 79, 'rewardPerEpisode': 62.70556129279754
'totalSteps': 12800, 'rewardStep': 0.8942953465511004, 'errorList': [], 'lossList': [0.0, -1.325987485051155, 0.0, 35.01995317697525, 0.0, 0.0, 0.0], 'rewardMean': 0.7016067545584076, 'totalEpisodes': 183, 'stepsPerEpisode': 187, 'rewardPerEpisode': 158.68796564184606
'totalSteps': 14080, 'rewardStep': 0.40152226375194305, 'errorList': [], 'lossList': [0.0, -1.3112378054857254, 0.0, 6.407445409297943, 0.0, 0.0, 0.0], 'rewardMean': 0.660195246755231, 'totalEpisodes': 188, 'stepsPerEpisode': 133, 'rewardPerEpisode': 96.94737682840127
'totalSteps': 15360, 'rewardStep': 0.8893005388536316, 'errorList': [], 'lossList': [0.0, -1.2706270360946654, 0.0, 7.009413738250732, 0.0, 0.0, 0.0], 'rewardMean': 0.6813867586976999, 'totalEpisodes': 190, 'stepsPerEpisode': 1049, 'rewardPerEpisode': 764.5385450543438
'totalSteps': 16640, 'rewardStep': 0.6331167996836493, 'errorList': [], 'lossList': [0.0, -1.246520676612854, 0.0, 7.107489901781082, 0.0, 0.0, 0.0], 'rewardMean': 0.6663166216955645, 'totalEpisodes': 191, 'stepsPerEpisode': 134, 'rewardPerEpisode': 93.2964544319098
'totalSteps': 17920, 'rewardStep': 0.8507814801440987, 'errorList': [], 'lossList': [0.0, -1.2433949828147888, 0.0, 4.381112993955612, 0.0, 0.0, 0.0], 'rewardMean': 0.6855577767709494, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 938.3670954896191
'totalSteps': 19200, 'rewardStep': 0.925608564383613, 'errorList': [], 'lossList': [0.0, -1.2328686344623565, 0.0, 4.0443856191635135, 0.0, 0.0, 0.0], 'rewardMean': 0.7028361714504784, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.9357226723046
'totalSteps': 20480, 'rewardStep': 0.9611091267352727, 'errorList': [0.0787642651418621, 0.09272075760995686, 0.09830838897726711, 0.09836445309571, 0.08721714252102174, 0.09287036358999048, 0.0790722387947328, 0.08455123548342278, 0.07401404534882833, 0.09040535989568503, 0.09043234053049774, 0.08231300623768595, 0.0990979115108486, 0.08984580054707396, 0.0893978458184493, 0.08608491661323668, 0.08475771805670437, 0.09002635528742652, 0.08688619470430917, 0.07879014798405681, 0.09000296621908682, 0.09469399760127067, 0.08895070729571079, 0.09314018463713984, 0.08808223748907269, 0.10834064799651794, 0.09280453244014042, 0.09507725264395393, 0.09684933746634372, 0.08328159010794792, 0.07956324974738439, 0.09212334756559237, 0.08915602293865606, 0.08418275313303446, 0.08530129609796466, 0.09244998476385147, 0.0776774154846533, 0.0873004283684023, 0.07833187201069776, 0.09525776065212184, 0.09903640817401013, 0.0812770209973487, 0.08148439915470533, 0.09940993282357001, 0.09057736801131454, 0.09435728476762241, 0.07926353325095484, 0.07872002581698287, 0.09835039295160333, 0.0866120262195653], 'lossList': [0.0, -1.194198459982872, 0.0, 2.438335576951504, 0.0, 0.0, 0.0], 'rewardMean': 0.7269154700135232, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.875384116184, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=71.21
