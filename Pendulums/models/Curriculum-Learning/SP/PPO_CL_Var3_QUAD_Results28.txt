#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 28
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.5256649664292901, 'errorList': [], 'lossList': [0.0, -1.4316237461566925, 0.0, 33.42303265571594, 0.0, 0.0, 0.0], 'rewardMean': 0.5153020965848374, 'totalEpisodes': 58, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.82151790407299
'totalSteps': 3840, 'rewardStep': 0.8207126677075025, 'errorList': [], 'lossList': [0.0, -1.428173829317093, 0.0, 44.54964725494385, 0.0, 0.0, 0.0], 'rewardMean': 0.6171056202923925, 'totalEpisodes': 77, 'stepsPerEpisode': 51, 'rewardPerEpisode': 39.486015031674384
'totalSteps': 5120, 'rewardStep': 0.8269436324704672, 'errorList': [], 'lossList': [0.0, -1.4284092259407044, 0.0, 44.12074373245239, 0.0, 0.0, 0.0], 'rewardMean': 0.6695651233369111, 'totalEpisodes': 88, 'stepsPerEpisode': 71, 'rewardPerEpisode': 51.309486890233835
'totalSteps': 6400, 'rewardStep': 0.7964696986481952, 'errorList': [], 'lossList': [0.0, -1.4180301010608674, 0.0, 53.57234227657318, 0.0, 0.0, 0.0], 'rewardMean': 0.6949460383991679, 'totalEpisodes': 96, 'stepsPerEpisode': 157, 'rewardPerEpisode': 106.89133152234464
'totalSteps': 7680, 'rewardStep': 0.7733474864186417, 'errorList': [], 'lossList': [0.0, -1.4029231566190719, 0.0, 124.83127494812011, 0.0, 0.0, 0.0], 'rewardMean': 0.7080129464024135, 'totalEpisodes': 125, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.234854830578918
'totalSteps': 8960, 'rewardStep': 0.8939636948259977, 'errorList': [], 'lossList': [0.0, -1.4106803500652314, 0.0, 49.77776733398437, 0.0, 0.0, 0.0], 'rewardMean': 0.7345773390343542, 'totalEpisodes': 139, 'stepsPerEpisode': 107, 'rewardPerEpisode': 77.85135860178774
'totalSteps': 10240, 'rewardStep': 0.6961169792350196, 'errorList': [], 'lossList': [0.0, -1.4120438092947005, 0.0, 23.952361989021302, 0.0, 0.0, 0.0], 'rewardMean': 0.7297697940594373, 'totalEpisodes': 145, 'stepsPerEpisode': 163, 'rewardPerEpisode': 133.79634152067203
'totalSteps': 11520, 'rewardStep': 0.48272260757570284, 'errorList': [], 'lossList': [0.0, -1.4029831629991532, 0.0, 15.17678045988083, 0.0, 0.0, 0.0], 'rewardMean': 0.7023201066723557, 'totalEpisodes': 147, 'stepsPerEpisode': 332, 'rewardPerEpisode': 226.73997743545178
'totalSteps': 12800, 'rewardStep': 0.7986372878389826, 'errorList': [], 'lossList': [0.0, -1.381551239490509, 0.0, 8.187958294451237, 0.0, 0.0, 0.0], 'rewardMean': 0.7119518247890183, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 947.1909411426669
'totalSteps': 14080, 'rewardStep': 0.8268656809911621, 'errorList': [], 'lossList': [0.0, -1.3478831881284714, 0.0, 5.55503919839859, 0.0, 0.0, 0.0], 'rewardMean': 0.7441444702140961, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 916.154994090835
'totalSteps': 15360, 'rewardStep': 0.8343540575976676, 'errorList': [], 'lossList': [0.0, -1.3314539706707, 0.0, 6.94698977202177, 0.0, 0.0, 0.0], 'rewardMean': 0.775013379330934, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.6237053087898
'totalSteps': 16640, 'rewardStep': 0.7721888273987876, 'errorList': [], 'lossList': [0.0, -1.3164627730846405, 0.0, 4.232966322302818, 0.0, 0.0, 0.0], 'rewardMean': 0.7701609953000623, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.0105373387041
'totalSteps': 17920, 'rewardStep': 0.881480436439589, 'errorList': [], 'lossList': [0.0, -1.2860987836122513, 0.0, 3.401409670934081, 0.0, 0.0, 0.0], 'rewardMean': 0.7756146756969746, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.1613373329485
'totalSteps': 19200, 'rewardStep': 0.8192257440857638, 'errorList': [], 'lossList': [0.0, -1.2761847192049027, 0.0, 2.8550406542420386, 0.0, 0.0, 0.0], 'rewardMean': 0.7778902802407315, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.5822526671395
'totalSteps': 20480, 'rewardStep': 0.9748555911058022, 'errorList': [0.05383036477593814, 0.03975907103465729, 0.06865289565602214, 0.07533391584629356, 0.050897192402390386, 0.018507831979302617, 0.07473445860171107, 0.059060605338403044, 0.06663320538046474, 0.07338742230659936, 0.03658686542398934, 0.015668452431535994, 0.01842670122588108, 0.021581734266582813, 0.00667443047056602, 0.03743807792045196, 0.0429089668526961, 0.09296024896056986, 0.05676365947371945, 0.08081631510089066, 0.04122520817273207, 0.0178143015004623, 0.1308148092043804, 0.020131431994781908, 0.01804656904870447, 0.01761443493304086, 0.06601171331642618, 0.10928975563327328, 0.04706969764980807, 0.05819837894707088, 0.02285724043645977, 0.050901077991255955, 0.05389837778167463, 0.01822693121703847, 0.12348834867286164, 0.05788966873437477, 0.060992749883694845, 0.07014103438513203, 0.04142768491718537, 0.03562512024505544, 0.041650847481812144, 0.09468185929893036, 0.0462570344174263, 0.1100405879183438, 0.08833418763268969, 0.06758393661290833, 0.02944491652923151, 0.017747442442640045, 0.055338670673479765, 0.026016796526613588], 'lossList': [0.0, -1.256615617275238, 0.0, 1.7362000780180096, 0.0, 0.0, 0.0], 'rewardMean': 0.7980410907094475, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.9187460170679, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=71.36
