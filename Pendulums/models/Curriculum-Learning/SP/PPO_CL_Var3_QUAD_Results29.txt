#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 29
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.748412670571308, 'errorList': [], 'lossList': [0.0, -1.4062894600629807, 0.0, 34.68720324516296, 0.0, 0.0, 0.0], 'rewardMean': 0.8347514023613576, 'totalEpisodes': 69, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.766263999295486
'totalSteps': 3840, 'rewardStep': 0.8786673673413894, 'errorList': [], 'lossList': [0.0, -1.4063318347930909, 0.0, 46.31501863479614, 0.0, 0.0, 0.0], 'rewardMean': 0.8493900573547015, 'totalEpisodes': 94, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.078177207361705
'totalSteps': 5120, 'rewardStep': 0.8366546516644816, 'errorList': [], 'lossList': [0.0, -1.3921078896522523, 0.0, 35.18492151260376, 0.0, 0.0, 0.0], 'rewardMean': 0.8462062059321466, 'totalEpisodes': 106, 'stepsPerEpisode': 42, 'rewardPerEpisode': 30.411493846593785
'totalSteps': 6400, 'rewardStep': 0.6479474618254843, 'errorList': [], 'lossList': [0.0, -1.3696116232872009, 0.0, 71.33064739227295, 0.0, 0.0, 0.0], 'rewardMean': 0.8065544571108141, 'totalEpisodes': 121, 'stepsPerEpisode': 214, 'rewardPerEpisode': 137.2656621002701
'totalSteps': 7680, 'rewardStep': 0.7530356679747301, 'errorList': [], 'lossList': [0.0, -1.3665978282690048, 0.0, 88.16865968704224, 0.0, 0.0, 0.0], 'rewardMean': 0.7976346589214668, 'totalEpisodes': 146, 'stepsPerEpisode': 76, 'rewardPerEpisode': 56.30408618377784
'totalSteps': 8960, 'rewardStep': 0.7775774402882641, 'errorList': [], 'lossList': [0.0, -1.3631635135412217, 0.0, 68.42661945343018, 0.0, 0.0, 0.0], 'rewardMean': 0.7947693419738665, 'totalEpisodes': 164, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.062379690425674
'totalSteps': 10240, 'rewardStep': 0.874523157186929, 'errorList': [], 'lossList': [0.0, -1.3535908633470535, 0.0, 26.373142142295837, 0.0, 0.0, 0.0], 'rewardMean': 0.8047385688754993, 'totalEpisodes': 168, 'stepsPerEpisode': 48, 'rewardPerEpisode': 37.73628534055408
'totalSteps': 11520, 'rewardStep': 0.5991063109415187, 'errorList': [], 'lossList': [0.0, -1.3264534455537795, 0.0, 8.801664237976075, 0.0, 0.0, 0.0], 'rewardMean': 0.7818905402161681, 'totalEpisodes': 169, 'stepsPerEpisode': 774, 'rewardPerEpisode': 541.8491855177459
'totalSteps': 12800, 'rewardStep': 0.8695031094302269, 'errorList': [], 'lossList': [0.0, -1.3015596616268157, 0.0, 32.433646912574766, 0.0, 0.0, 0.0], 'rewardMean': 0.7906517971375739, 'totalEpisodes': 175, 'stepsPerEpisode': 64, 'rewardPerEpisode': 56.564818472636006
'totalSteps': 14080, 'rewardStep': 0.5821190687002262, 'errorList': [], 'lossList': [0.0, -1.2879499614238739, 0.0, 9.549025683403014, 0.0, 0.0, 0.0], 'rewardMean': 0.7567546905924558, 'totalEpisodes': 177, 'stepsPerEpisode': 342, 'rewardPerEpisode': 272.86852169346133
'totalSteps': 15360, 'rewardStep': 0.9648595913001163, 'errorList': [7.824931642836768, 7.027664983286397, 7.4545294448963855, 4.345826578234913, 6.01727434633424, 2.9373391196042, 3.431124041608068, 5.658038176289029, 5.133507261261522, 0.47685297793285897, 2.3585897121051898, 1.5517953736387762, 4.199023183833644, 7.89776628132161, 6.893853189064434, 1.4130951500037474, 7.649321696895133, 4.3369236221183325, 5.151117416251385, 0.4646926937010342, 2.5989121504865746, 3.174669779425306, 5.300002888974315, 5.565439224442615, 1.711755942756987, 0.49254331037324506, 6.282978846693347, 3.0354868536081283, 5.347800067764751, 6.8268714192455775, 2.447353936363405, 0.4100417759261577, 2.18500667634667, 6.73444974687312, 7.004290440537508, 5.440478695947288, 6.142327690018891, 2.731146826046488, 5.88976765886129, 7.853485599987021, 6.250229807680599, 7.494015788640353, 8.17121119416929, 3.4053158639992076, 4.166057242578994, 4.751360549551989, 3.4203893800423164, 8.288684761281912, 3.1355211184141685, 4.506846615878308], 'lossList': [0.0, -1.2627771615982055, 0.0, 8.570561400055885, 0.0, 0.0, 0.0], 'rewardMean': 0.7783993826653367, 'totalEpisodes': 181, 'stepsPerEpisode': 303, 'rewardPerEpisode': 278.48595555879456, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.3312687424985743, 'errorList': [], 'lossList': [0.0, -1.2411024755239486, 0.0, 3.625037321448326, 0.0, 0.0, 0.0], 'rewardMean': 0.7236595201810552, 'totalEpisodes': 185, 'stepsPerEpisode': 150, 'rewardPerEpisode': 99.86803699957065
'totalSteps': 17920, 'rewardStep': 0.7035648407692051, 'errorList': [], 'lossList': [0.0, -1.2407396507263184, 0.0, 4.429578147530556, 0.0, 0.0, 0.0], 'rewardMean': 0.7103505390915276, 'totalEpisodes': 188, 'stepsPerEpisode': 416, 'rewardPerEpisode': 336.09326354654297
'totalSteps': 19200, 'rewardStep': 0.8448783061287624, 'errorList': [], 'lossList': [0.0, -1.2252979522943497, 0.0, 1.5951604235172272, 0.0, 0.0, 0.0], 'rewardMean': 0.7300436235218554, 'totalEpisodes': 190, 'stepsPerEpisode': 186, 'rewardPerEpisode': 158.19294686329798
'totalSteps': 20480, 'rewardStep': 0.8191802531744057, 'errorList': [], 'lossList': [0.0, -1.231227765083313, 0.0, 1.7322033190727233, 0.0, 0.0, 0.0], 'rewardMean': 0.7366580820418228, 'totalEpisodes': 192, 'stepsPerEpisode': 344, 'rewardPerEpisode': 284.80284715623964
'totalSteps': 21760, 'rewardStep': 0.876927270568319, 'errorList': [], 'lossList': [0.0, -1.2314053732156753, 0.0, 3.7247571411728857, 0.0, 0.0, 0.0], 'rewardMean': 0.7465930650698284, 'totalEpisodes': 193, 'stepsPerEpisode': 148, 'rewardPerEpisode': 136.6653506776198
'totalSteps': 23040, 'rewardStep': 0.7987401128186304, 'errorList': [], 'lossList': [0.0, -1.2271620458364487, 0.0, 1.8317786794900894, 0.0, 0.0, 0.0], 'rewardMean': 0.7390147606329985, 'totalEpisodes': 194, 'stepsPerEpisode': 260, 'rewardPerEpisode': 215.66538487690815
'totalSteps': 24320, 'rewardStep': 0.8692327975374544, 'errorList': [], 'lossList': [0.0, -1.208684927225113, 0.0, 1.353771316409111, 0.0, 0.0, 0.0], 'rewardMean': 0.766027409292592, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 992.3103734580525
'totalSteps': 25600, 'rewardStep': 0.7349912364511764, 'errorList': [], 'lossList': [0.0, -1.1913756209611892, 0.0, 4.709509582221508, 0.0, 0.0, 0.0], 'rewardMean': 0.752576221994687, 'totalEpisodes': 195, 'stepsPerEpisode': 579, 'rewardPerEpisode': 490.57153682758354
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.33
