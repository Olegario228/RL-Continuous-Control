#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.47398090027360634, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.48430034242919917, 'totalEpisodes': 62, 'stepsPerEpisode': 35, 'rewardPerEpisode': 25.2940630532828
'totalSteps': 3840, 'rewardStep': 0.8978835884971612, 'errorList': [], 'lossList': [0.0, -1.4313257944583893, 0.0, 33.49548372268677, 0.0, 0.0, 0.0], 'rewardMean': 0.5876961539461897, 'totalEpisodes': 92, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.62224565735566
'totalSteps': 5120, 'rewardStep': 0.8000200761540837, 'errorList': [], 'lossList': [0.0, -1.4195465236902236, 0.0, 48.75353920936585, 0.0, 0.0, 0.0], 'rewardMean': 0.6301609383877684, 'totalEpisodes': 117, 'stepsPerEpisode': 74, 'rewardPerEpisode': 60.604437674344766
'totalSteps': 6400, 'rewardStep': 0.8922087009127263, 'errorList': [], 'lossList': [0.0, -1.411713849902153, 0.0, 70.09340496063233, 0.0, 0.0, 0.0], 'rewardMean': 0.6738355654752614, 'totalEpisodes': 163, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.618284093598492
'totalSteps': 7680, 'rewardStep': 0.6050152540636485, 'errorList': [], 'lossList': [0.0, -1.4082842463254928, 0.0, 48.055155668258664, 0.0, 0.0, 0.0], 'rewardMean': 0.6640040924164595, 'totalEpisodes': 183, 'stepsPerEpisode': 41, 'rewardPerEpisode': 22.209059142118683
'totalSteps': 8960, 'rewardStep': 0.889146559918705, 'errorList': [], 'lossList': [0.0, -1.3924309015274048, 0.0, 45.87167000770569, 0.0, 0.0, 0.0], 'rewardMean': 0.6921469008542402, 'totalEpisodes': 195, 'stepsPerEpisode': 69, 'rewardPerEpisode': 56.88508997761027
'totalSteps': 10240, 'rewardStep': 0.9055082807027423, 'errorList': [], 'lossList': [0.0, -1.3789574003219605, 0.0, 26.627540000677108, 0.0, 0.0, 0.0], 'rewardMean': 0.7158537208374072, 'totalEpisodes': 201, 'stepsPerEpisode': 176, 'rewardPerEpisode': 152.71099314710526
'totalSteps': 11520, 'rewardStep': 0.5301239587586275, 'errorList': [], 'lossList': [0.0, -1.3648955899477004, 0.0, 20.344188933372497, 0.0, 0.0, 0.0], 'rewardMean': 0.6972807446295292, 'totalEpisodes': 205, 'stepsPerEpisode': 343, 'rewardPerEpisode': 273.0567475204195
'totalSteps': 12800, 'rewardStep': 0.474188851676207, 'errorList': [], 'lossList': [0.0, -1.3627698570489883, 0.0, 12.324526967406273, 0.0, 0.0, 0.0], 'rewardMean': 0.6942057071231114, 'totalEpisodes': 207, 'stepsPerEpisode': 164, 'rewardPerEpisode': 125.32329108578074
'totalSteps': 14080, 'rewardStep': 0.610966507962081, 'errorList': [], 'lossList': [0.0, -1.3523010182380677, 0.0, 8.2350845515728, 0.0, 0.0, 0.0], 'rewardMean': 0.7079042678919588, 'totalEpisodes': 208, 'stepsPerEpisode': 1244, 'rewardPerEpisode': 896.6943374056801
'totalSteps': 15360, 'rewardStep': 0.7297167614180066, 'errorList': [], 'lossList': [0.0, -1.3238296073675155, 0.0, 4.34313634455204, 0.0, 0.0, 0.0], 'rewardMean': 0.7334778540063989, 'totalEpisodes': 209, 'stepsPerEpisode': 545, 'rewardPerEpisode': 384.1788336629689
'totalSteps': 16640, 'rewardStep': 0.8811295922023711, 'errorList': [], 'lossList': [0.0, -1.2920164918899537, 0.0, 4.978763956874609, 0.0, 0.0, 0.0], 'rewardMean': 0.7318024543769199, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.5686009285955
'totalSteps': 17920, 'rewardStep': 0.91045537931831, 'errorList': [], 'lossList': [0.0, -1.2701625227928162, 0.0, 2.504109103679657, 0.0, 0.0, 0.0], 'rewardMean': 0.7428459846933425, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.725398898291
'totalSteps': 19200, 'rewardStep': 0.9282906798291828, 'errorList': [], 'lossList': [0.0, -1.2462857836484909, 0.0, 2.5322628330439327, 0.0, 0.0, 0.0], 'rewardMean': 0.7464541825849882, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.5990069631769
'totalSteps': 20480, 'rewardStep': 0.9822598071056869, 'errorList': [0.11189039053635207, 0.06763009479258231, 0.11598343526288779, 0.08777855521985768, 0.08945793450885878, 0.08465021658454926, 0.148742237450128, 0.08758455882687016, 0.10786325602838456, 0.08638150307076693, 0.08180992976011296, 0.09170523896861175, 0.09698685583747325, 0.09577746222028724, 0.0871606988062482, 0.0828455684605625, 0.0963556099176674, 0.0871076817209866, 0.10719851060867464, 0.0893398119525084, 0.09027846926856962, 0.13977442375026342, 0.09280108368384118, 0.1034320987831209, 0.09870753832081107, 0.07520635959738474, 0.08245655541384482, 0.11792464458632321, 0.08417899910200413, 0.10547278923572034, 0.07490370593331866, 0.07141809765263352, 0.10474077756671223, 0.08625346680081321, 0.11462656510620116, 0.0901178167052437, 0.09646198463182035, 0.07402253015569561, 0.11074191533043441, 0.09388949987963095, 0.08446316667002024, 0.07850165012825513, 0.08349646592028769, 0.07484768807805343, 0.12717563213056196, 0.09482545647759513, 0.07792338996160766, 0.09287264233337103, 0.07258276019518498, 0.101559909663901], 'lossList': [0.0, -1.21338394343853, 0.0, 2.2204862171411515, 0.0, 0.0, 0.0], 'rewardMean': 0.784178637889192, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.969272680217, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=60.25
