#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 31
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6017000625979079, 'errorList': [], 'lossList': [0.0, -1.4181553667783737, 0.0, 26.26806542634964, 0.0, 0.0, 0.0], 'rewardMean': 0.7824948072543088, 'totalEpisodes': 16, 'stepsPerEpisode': 359, 'rewardPerEpisode': 251.15419556997307
'totalSteps': 3840, 'rewardStep': 0.8595169851479072, 'errorList': [], 'lossList': [0.0, -1.416436905860901, 0.0, 27.523747441768645, 0.0, 0.0, 0.0], 'rewardMean': 0.8081688665521748, 'totalEpisodes': 22, 'stepsPerEpisode': 299, 'rewardPerEpisode': 184.53013080502734
'totalSteps': 5120, 'rewardStep': 0.9356343305931826, 'errorList': [], 'lossList': [0.0, -1.4103928768634797, 0.0, 41.78747223854065, 0.0, 0.0, 0.0], 'rewardMean': 0.8400352325624267, 'totalEpisodes': 30, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.308464987292423
'totalSteps': 6400, 'rewardStep': 0.8898687912596774, 'errorList': [], 'lossList': [0.0, -1.3958740162849426, 0.0, 81.95484210968017, 0.0, 0.0, 0.0], 'rewardMean': 0.8500019443018768, 'totalEpisodes': 44, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.7693510162013855
'totalSteps': 7680, 'rewardStep': 0.8306243046291895, 'errorList': [], 'lossList': [0.0, -1.3842090028524399, 0.0, 114.07380697250366, 0.0, 0.0, 0.0], 'rewardMean': 0.8467723376897623, 'totalEpisodes': 75, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.52906821738079
'totalSteps': 8960, 'rewardStep': 0.5813530203171364, 'errorList': [], 'lossList': [0.0, -1.3782938545942307, 0.0, 65.97056777954101, 0.0, 0.0, 0.0], 'rewardMean': 0.8088552923508158, 'totalEpisodes': 96, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.736532547431745
'totalSteps': 10240, 'rewardStep': 0.7233178192996639, 'errorList': [], 'lossList': [0.0, -1.367613610625267, 0.0, 38.81488070487976, 0.0, 0.0, 0.0], 'rewardMean': 0.7981631082194218, 'totalEpisodes': 104, 'stepsPerEpisode': 55, 'rewardPerEpisode': 40.49164266522983
'totalSteps': 11520, 'rewardStep': 0.4612709016494875, 'errorList': [], 'lossList': [0.0, -1.3482835102081299, 0.0, 34.72278917551041, 0.0, 0.0, 0.0], 'rewardMean': 0.7607306408227624, 'totalEpisodes': 108, 'stepsPerEpisode': 392, 'rewardPerEpisode': 311.6579296378069
'totalSteps': 12800, 'rewardStep': 0.7471327078704807, 'errorList': [], 'lossList': [0.0, -1.3180695766210555, 0.0, 18.79271771788597, 0.0, 0.0, 0.0], 'rewardMean': 0.7593708475275343, 'totalEpisodes': 110, 'stepsPerEpisode': 830, 'rewardPerEpisode': 646.3012263920817
'totalSteps': 14080, 'rewardStep': 0.400772774341209, 'errorList': [], 'lossList': [0.0, -1.2678128147125245, 0.0, 8.317397838830948, 0.0, 0.0, 0.0], 'rewardMean': 0.7031191697705842, 'totalEpisodes': 113, 'stepsPerEpisode': 256, 'rewardPerEpisode': 128.4532003475511
'totalSteps': 15360, 'rewardStep': 0.8444647156891101, 'errorList': [], 'lossList': [0.0, -1.2550382733345031, 0.0, 8.204655941128731, 0.0, 0.0, 0.0], 'rewardMean': 0.7273956350797044, 'totalEpisodes': 116, 'stepsPerEpisode': 325, 'rewardPerEpisode': 273.1082650521456
'totalSteps': 16640, 'rewardStep': 0.8023469337586653, 'errorList': [], 'lossList': [0.0, -1.2492248153686523, 0.0, 4.516611414551735, 0.0, 0.0, 0.0], 'rewardMean': 0.7216786299407801, 'totalEpisodes': 118, 'stepsPerEpisode': 144, 'rewardPerEpisode': 115.50816289538079
'totalSteps': 17920, 'rewardStep': 0.7042903444589752, 'errorList': [], 'lossList': [0.0, -1.2071917635202407, 0.0, 3.193381019756198, 0.0, 0.0, 0.0], 'rewardMean': 0.6985442313273594, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.8208965831332
'totalSteps': 19200, 'rewardStep': 0.8052546893361655, 'errorList': [], 'lossList': [0.0, -1.1700238728523253, 0.0, 2.6438261407613752, 0.0, 0.0, 0.0], 'rewardMean': 0.6900828211350083, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.8696416650803
'totalSteps': 20480, 'rewardStep': 0.8697547175321987, 'errorList': [], 'lossList': [0.0, -1.1163964849710464, 0.0, 1.641849337927997, 0.0, 0.0, 0.0], 'rewardMean': 0.6939958624253092, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.9272621947355
'totalSteps': 21760, 'rewardStep': 0.8161625602269386, 'errorList': [], 'lossList': [0.0, -1.0577193582057953, 0.0, 1.1182855079323053, 0.0, 0.0, 0.0], 'rewardMean': 0.7174768164162895, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.9150666728647
'totalSteps': 23040, 'rewardStep': 0.9814210584579095, 'errorList': [0.10053424668155828, 0.1371901636336701, 0.09042540827687989, 0.134376115427241, 0.09738999095599025, 0.09487684710097095, 0.12292401151273748, 0.1317644269156597, 0.16321520944038304, 0.09635539565505195, 0.09148317457991303, 0.1363776245145031, 0.16115456878669068, 0.1370579003240453, 0.1238568203535441, 0.1441652903016413, 0.12894492651635, 0.17065089694993854, 0.09939553642797157, 0.17423135006776844, 0.11676058151597767, 0.08905450445357471, 0.10591353014044047, 0.14429401474242354, 0.16163412839305794, 0.09360812979608873, 0.15072941767646283, 0.13445964430952803, 0.11440928923044615, 0.10417488909563892, 0.11212764564851278, 0.16779441702719805, 0.13849259379480125, 0.1743026903238949, 0.09170177121565658, 0.1187638537136413, 0.14471268525262182, 0.1661117275858418, 0.14754285754090313, 0.08777837806358221, 0.14670323897125034, 0.11480225801056469, 0.12168878795184898, 0.1295132488200453, 0.1278614396783342, 0.105590321630839, 0.12526062662447557, 0.16247782655878523, 0.1409907293558548, 0.08844978136640928], 'lossList': [0.0, -1.0267585545778275, 0.0, 1.0810766365565359, 0.0, 0.0, 0.0], 'rewardMean': 0.7432871403321141, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.0074767524716, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=77.54
