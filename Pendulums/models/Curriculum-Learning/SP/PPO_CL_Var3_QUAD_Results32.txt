#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 32
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.540073652740004, 'errorList': [], 'lossList': [0.0, -1.4189491379261017, 0.0, 24.778790514469147, 0.0, 0.0, 0.0], 'rewardMean': 0.6669218269600776, 'totalEpisodes': 21, 'stepsPerEpisode': 134, 'rewardPerEpisode': 80.0244832743085
'totalSteps': 3840, 'rewardStep': 0.7506017458645106, 'errorList': [], 'lossList': [0.0, -1.4051984816789627, 0.0, 28.619895050525667, 0.0, 0.0, 0.0], 'rewardMean': 0.6948151332615553, 'totalEpisodes': 27, 'stepsPerEpisode': 78, 'rewardPerEpisode': 58.93333301917025
'totalSteps': 5120, 'rewardStep': 0.8430260259678446, 'errorList': [], 'lossList': [0.0, -1.3870141309499742, 0.0, 20.818590824604033, 0.0, 0.0, 0.0], 'rewardMean': 0.7318678564381276, 'totalEpisodes': 33, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.143055516700741
'totalSteps': 6400, 'rewardStep': 0.6033599750893389, 'errorList': [], 'lossList': [0.0, -1.3931339144706727, 0.0, 92.06213735580444, 0.0, 0.0, 0.0], 'rewardMean': 0.7061662801683698, 'totalEpisodes': 49, 'stepsPerEpisode': 44, 'rewardPerEpisode': 30.966101349011186
'totalSteps': 7680, 'rewardStep': 0.8107177784510744, 'errorList': [], 'lossList': [0.0, -1.3860542285442352, 0.0, 130.29055019378663, 0.0, 0.0, 0.0], 'rewardMean': 0.7235915298821539, 'totalEpisodes': 82, 'stepsPerEpisode': 42, 'rewardPerEpisode': 34.29144312443623
'totalSteps': 8960, 'rewardStep': 0.9695604235667727, 'errorList': [205.34331851566895, 213.10837159560546, 234.47288204517187, 133.55574715092013, 196.2838361792823, 221.06094175880443, 254.1582734873781, 187.32800830097838, 138.70069693123423, 222.4064684798958, 152.22817076572125, 224.62520707128127, 118.90326682196167, 250.40571524367593, 157.81831762720824, 104.72365467825415, 176.14972519488427, 206.6488276877449, 216.69388257803544, 179.8218825122491, 212.60311905697918, 57.24356237581737, 73.17607908352602, 215.03334205638666, 176.20250031445659, 124.05665690577585, 44.31795823028221, 178.33239070082288, 195.59443037408352, 226.79635563042845, 191.93895781104297, 205.98145579440222, 234.4218567480769, 217.8827853097108, 44.77038321271575, 227.38183198876908, 198.2633751788123, 231.9660749335093, 237.506242058216, 185.98701432949534, 195.31093243272318, 171.13328538818794, 217.61206717809998, 223.87013623375074, 185.63473747501646, 236.51393734892608, 179.86820882966813, 189.01750009148543, 167.77639670197416, 194.8309117517205], 'lossList': [0.0, -1.3762812745571136, 0.0, 64.97525375366212, 0.0, 0.0, 0.0], 'rewardMean': 0.7587299432656709, 'totalEpisodes': 110, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.919399111059104, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.9862924211073595, 'errorList': [87.12959645437758, 22.939141688642056, 73.49382135798126, 82.32528319146965, 176.11931731519576, 94.5087656890524, 164.84098411117543, 121.36311749369005, 109.53908272049034, 93.32206495775011, 94.68760332622661, 128.88397811489457, 119.85979664072566, 16.74828304011299, 19.347158996711283, 160.4095710475, 98.18324288670244, 68.41868320259951, 49.22474887755341, 5.176223551379842, 139.0363777083319, 9.98924689907504, 114.08650442759343, 29.068816628834526, 5.5699238379566784, 140.02879563789506, 65.81139223144935, 140.73280329414717, 86.99334212948932, 35.84413010003137, 6.3515801116442, 99.4656384366135, 141.89496213111238, 171.54452263391323, 65.72228940508609, 126.42234324191514, 68.30377777905413, 45.04852169286631, 92.4264362352414, 77.56112539018442, 54.66179735325609, 46.91739860298958, 48.185077128655024, 112.34164694428318, 70.59491039465762, 154.16570352010615, 99.39917468405012, 78.258252914005, 48.48533078300816, 122.82523950810058], 'lossList': [0.0, -1.3673852914571762, 0.0, 28.076320943832396, 0.0, 0.0, 0.0], 'rewardMean': 0.787175252995882, 'totalEpisodes': 124, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.658010681913695, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5805258346110717, 'errorList': [], 'lossList': [0.0, -1.3692974013090133, 0.0, 19.513373012542726, 0.0, 0.0, 0.0], 'rewardMean': 0.7642142065086808, 'totalEpisodes': 132, 'stepsPerEpisode': 90, 'rewardPerEpisode': 65.29345763243029
'totalSteps': 12800, 'rewardStep': 0.3403987158342608, 'errorList': [], 'lossList': [0.0, -1.3805500382184983, 0.0, 9.407823672294617, 0.0, 0.0, 0.0], 'rewardMean': 0.7218326574412388, 'totalEpisodes': 138, 'stepsPerEpisode': 146, 'rewardPerEpisode': 96.76969869882731
'totalSteps': 14080, 'rewardStep': 0.632208809311886, 'errorList': [], 'lossList': [0.0, -1.3992916291952133, 0.0, 8.067783558368683, 0.0, 0.0, 0.0], 'rewardMean': 0.7056765382544123, 'totalEpisodes': 143, 'stepsPerEpisode': 103, 'rewardPerEpisode': 75.35368069309294
'totalSteps': 15360, 'rewardStep': 0.5663035237843634, 'errorList': [], 'lossList': [0.0, -1.4030419951677322, 0.0, 26.311423959732057, 0.0, 0.0, 0.0], 'rewardMean': 0.7082995253588484, 'totalEpisodes': 147, 'stepsPerEpisode': 315, 'rewardPerEpisode': 193.50238845719477
'totalSteps': 16640, 'rewardStep': 0.7961887121340123, 'errorList': [], 'lossList': [0.0, -1.3691851592063904, 0.0, 6.630621566772461, 0.0, 0.0, 0.0], 'rewardMean': 0.7128582219857985, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 993.0471263533445
'totalSteps': 17920, 'rewardStep': 0.8405732130056827, 'errorList': [], 'lossList': [0.0, -1.3067389434576036, 0.0, 5.002021998763085, 0.0, 0.0, 0.0], 'rewardMean': 0.7126129406895823, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.7330878176247
'totalSteps': 19200, 'rewardStep': 0.881975000928919, 'errorList': [], 'lossList': [0.0, -1.274127893447876, 0.0, 3.5074490985274314, 0.0, 0.0, 0.0], 'rewardMean': 0.7404744432735402, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.248134097396
'totalSteps': 20480, 'rewardStep': 0.986185073446973, 'errorList': [0.07238424595693158, 0.13103735037828065, 0.14043405517936455, 0.12721794924255087, 0.09405570828083117, 0.11158051550574322, 0.07829910229524552, 0.09603117473086753, 0.07180508793328386, 0.1022204570146742, 0.11723134917327656, 0.08171509485039444, 0.1408267866173569, 0.11727572933757716, 0.09282208957959583, 0.08472855058267274, 0.09604491698414179, 0.09086465255214002, 0.10184492953419587, 0.07856246635425333, 0.10730755211183698, 0.12497902889283143, 0.10578713098111513, 0.13445010254982054, 0.10001836163251097, 0.1729191898522279, 0.10085696732649174, 0.12686112570454283, 0.13191350525172038, 0.09094539547107351, 0.08590713857521999, 0.09899177653609526, 0.08582425040604216, 0.08983427386575893, 0.08993549478033845, 0.1262777109175374, 0.07360318206379045, 0.10312698642082398, 0.07671034480785881, 0.10058237013609687, 0.13858324068087824, 0.074995047042226, 0.08311837733313777, 0.13989781748975458, 0.11546797853237382, 0.12518107536086456, 0.07490522684510788, 0.07332156548364478, 0.13912302694333958, 0.08409448007265649], 'lossList': [0.0, -1.2245190298557282, 0.0, 3.4176537304371597, 0.0, 0.0, 0.0], 'rewardMean': 0.7580211727731301, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.9819997019529, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=113.18
