#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 33
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.7057919108336482, 'errorList': [], 'lossList': [0.0, -1.4129135578870773, 0.0, 23.580652205944062, 0.0, 0.0, 0.0], 'rewardMean': 0.5250735511895516, 'totalEpisodes': 14, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.03161268357925
'totalSteps': 3840, 'rewardStep': 0.8420249491740875, 'errorList': [], 'lossList': [0.0, -1.3969401717185974, 0.0, 31.784714653491974, 0.0, 0.0, 0.0], 'rewardMean': 0.6307240171843969, 'totalEpisodes': 21, 'stepsPerEpisode': 273, 'rewardPerEpisode': 191.2015629084658
'totalSteps': 5120, 'rewardStep': 0.7539356955087753, 'errorList': [], 'lossList': [0.0, -1.3881064695119858, 0.0, 39.98949733257294, 0.0, 0.0, 0.0], 'rewardMean': 0.6615269367654915, 'totalEpisodes': 26, 'stepsPerEpisode': 76, 'rewardPerEpisode': 61.63309497104161
'totalSteps': 6400, 'rewardStep': 0.6371186479521418, 'errorList': [], 'lossList': [0.0, -1.381395891904831, 0.0, 72.07544389724731, 0.0, 0.0, 0.0], 'rewardMean': 0.6566452790028215, 'totalEpisodes': 38, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.057561647369006
'totalSteps': 7680, 'rewardStep': 0.7697377845594612, 'errorList': [], 'lossList': [0.0, -1.379612797498703, 0.0, 170.01359043121337, 0.0, 0.0, 0.0], 'rewardMean': 0.6754940299289282, 'totalEpisodes': 82, 'stepsPerEpisode': 83, 'rewardPerEpisode': 63.94706343912258
'totalSteps': 8960, 'rewardStep': 0.9266040432399216, 'errorList': [], 'lossList': [0.0, -1.3852970439195633, 0.0, 71.82961463928223, 0.0, 0.0, 0.0], 'rewardMean': 0.7113668889733559, 'totalEpisodes': 104, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.177279440226236
'totalSteps': 10240, 'rewardStep': 0.17762990787583266, 'errorList': [], 'lossList': [0.0, -1.366577935218811, 0.0, 34.968848738670346, 0.0, 0.0, 0.0], 'rewardMean': 0.6446497663361654, 'totalEpisodes': 113, 'stepsPerEpisode': 140, 'rewardPerEpisode': 92.81532881948735
'totalSteps': 11520, 'rewardStep': 0.6621521259305236, 'errorList': [], 'lossList': [0.0, -1.347975299358368, 0.0, 18.22556723833084, 0.0, 0.0, 0.0], 'rewardMean': 0.6465944729577608, 'totalEpisodes': 122, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.3005641999776296
'totalSteps': 12800, 'rewardStep': 0.5920178650638233, 'errorList': [], 'lossList': [0.0, -1.3603709572553635, 0.0, 29.05123574733734, 0.0, 0.0, 0.0], 'rewardMean': 0.641136812168367, 'totalEpisodes': 128, 'stepsPerEpisode': 169, 'rewardPerEpisode': 141.25586472860627
'totalSteps': 14080, 'rewardStep': 0.655031071006692, 'errorList': [], 'lossList': [0.0, -1.3627319425344466, 0.0, 12.229517924785615, 0.0, 0.0, 0.0], 'rewardMean': 0.6722044001144908, 'totalEpisodes': 130, 'stepsPerEpisode': 284, 'rewardPerEpisode': 199.99559136418296
'totalSteps': 15360, 'rewardStep': 0.5252540910030776, 'errorList': [], 'lossList': [0.0, -1.3488427889347077, 0.0, 24.840023047924042, 0.0, 0.0, 0.0], 'rewardMean': 0.6541506181314337, 'totalEpisodes': 132, 'stepsPerEpisode': 511, 'rewardPerEpisode': 424.4411151120059
'totalSteps': 16640, 'rewardStep': 0.7437683916321215, 'errorList': [], 'lossList': [0.0, -1.3345517718791962, 0.0, 8.297726299762726, 0.0, 0.0, 0.0], 'rewardMean': 0.644324962377237, 'totalEpisodes': 133, 'stepsPerEpisode': 422, 'rewardPerEpisode': 339.8905661551146
'totalSteps': 17920, 'rewardStep': 0.6750834048780021, 'errorList': [], 'lossList': [0.0, -1.3038808161020279, 0.0, 4.347852179408074, 0.0, 0.0, 0.0], 'rewardMean': 0.6364397333141598, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 940.9436554758123
'totalSteps': 19200, 'rewardStep': 0.6784802717651285, 'errorList': [], 'lossList': [0.0, -1.2728424191474914, 0.0, 6.13715561747551, 0.0, 0.0, 0.0], 'rewardMean': 0.6405758956954584, 'totalEpisodes': 134, 'stepsPerEpisode': 304, 'rewardPerEpisode': 230.62932219060664
'totalSteps': 20480, 'rewardStep': 0.8246300861292948, 'errorList': [], 'lossList': [0.0, -1.2613559663295746, 0.0, 17.352459819316863, 0.0, 0.0, 0.0], 'rewardMean': 0.6460651258524417, 'totalEpisodes': 135, 'stepsPerEpisode': 949, 'rewardPerEpisode': 758.3539273290489
'totalSteps': 21760, 'rewardStep': 0.8150899262009634, 'errorList': [], 'lossList': [0.0, -1.2410035866498947, 0.0, 2.9473186835646628, 0.0, 0.0, 0.0], 'rewardMean': 0.6349137141485459, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 930.7386278377896
'totalSteps': 23040, 'rewardStep': 0.7396928443607755, 'errorList': [], 'lossList': [0.0, -1.2011881124973298, 0.0, 1.6279406040161848, 0.0, 0.0, 0.0], 'rewardMean': 0.6911200077970403, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.7145247301437
'totalSteps': 24320, 'rewardStep': 0.8168330946093048, 'errorList': [], 'lossList': [0.0, -1.1609965038299561, 0.0, 1.5098973932117223, 0.0, 0.0, 0.0], 'rewardMean': 0.7065881046649183, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.4055930059114
'totalSteps': 25600, 'rewardStep': 0.9774101746431684, 'errorList': [0.05175264223783737, 0.06807766767616616, 0.04010430988383471, 0.04265627570442554, 0.05394347224449787, 0.08174965756161441, 0.07037976392630507, 0.04204016477944463, 0.044146638642241755, 0.046508601446281175, 0.05278329298170794, 0.056447875699429675, 0.0751612036073954, 0.10027467348429894, 0.05451289040910589, 0.04997670605166306, 0.039195586380887076, 0.0653982742660869, 0.07358510386705616, 0.08444847775177398, 0.09426414428917845, 0.06955845239112596, 0.05684975119834451, 0.04265490197672869, 0.0718267275795162, 0.05855295037487591, 0.05817361254243414, 0.0798270143133312, 0.04510528564948417, 0.045048066911446465, 0.060924641922173, 0.0884792683610929, 0.06290062038159606, 0.05375438731772037, 0.0424985041885151, 0.04980679627153254, 0.0739526518770592, 0.03888879483678094, 0.05608094568948582, 0.04753698419888194, 0.05702399910360555, 0.04877362170237077, 0.05738195228554899, 0.08577647622772876, 0.0678076303794804, 0.0640455426181071, 0.049737408517280694, 0.0375156068468968, 0.05451657008381796, 0.06498947560735682], 'lossList': [0.0, -1.1370910769701004, 0.0, 1.1854596276208758, 0.0, 0.0, 0.0], 'rewardMean': 0.7451273356228528, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1183.043233360237, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.1
