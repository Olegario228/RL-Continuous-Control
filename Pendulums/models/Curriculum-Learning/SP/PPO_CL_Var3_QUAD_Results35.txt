#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 35
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9203215160966355, 'errorList': [], 'lossList': [0.0, -1.4143338626623154, 0.0, 26.359307066202163, 0.0, 0.0, 0.0], 'rewardMean': 0.8600602382977993, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 381.19053618746534
'totalSteps': 3840, 'rewardStep': 0.574092715381082, 'errorList': [], 'lossList': [0.0, -1.4229253643751145, 0.0, 37.43027774333954, 0.0, 0.0, 0.0], 'rewardMean': 0.7647377306588936, 'totalEpisodes': 12, 'stepsPerEpisode': 153, 'rewardPerEpisode': 120.9221561466254
'totalSteps': 5120, 'rewardStep': 0.7529574263175469, 'errorList': [], 'lossList': [0.0, -1.428107230067253, 0.0, 27.109685204029084, 0.0, 0.0, 0.0], 'rewardMean': 0.7617926545735569, 'totalEpisodes': 14, 'stepsPerEpisode': 471, 'rewardPerEpisode': 340.8709617306208
'totalSteps': 6400, 'rewardStep': 0.5405905981083825, 'errorList': [], 'lossList': [0.0, -1.425786269903183, 0.0, 88.3591602897644, 0.0, 0.0, 0.0], 'rewardMean': 0.717552243280522, 'totalEpisodes': 24, 'stepsPerEpisode': 104, 'rewardPerEpisode': 78.27556740682887
'totalSteps': 7680, 'rewardStep': 0.8871257136298001, 'errorList': [], 'lossList': [0.0, -1.4298248499631883, 0.0, 217.0876039505005, 0.0, 0.0, 0.0], 'rewardMean': 0.745814488338735, 'totalEpisodes': 81, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.781866586173786
'totalSteps': 8960, 'rewardStep': 0.9093634346062764, 'errorList': [], 'lossList': [0.0, -1.427497217655182, 0.0, 91.36426527023315, 0.0, 0.0, 0.0], 'rewardMean': 0.7691786235198123, 'totalEpisodes': 125, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.4599396085455085
'totalSteps': 10240, 'rewardStep': 0.7239299039116627, 'errorList': [], 'lossList': [0.0, -1.4225376349687577, 0.0, 51.82236639022827, 0.0, 0.0, 0.0], 'rewardMean': 0.7635225335687936, 'totalEpisodes': 148, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.991643709311862
'totalSteps': 11520, 'rewardStep': 0.46628131132390715, 'errorList': [], 'lossList': [0.0, -1.4075017100572587, 0.0, 50.550664768218994, 0.0, 0.0, 0.0], 'rewardMean': 0.7304957310971396, 'totalEpisodes': 165, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.043603857857114
'totalSteps': 12800, 'rewardStep': 0.3615116205965135, 'errorList': [], 'lossList': [0.0, -1.3874586123228072, 0.0, 19.115155885219576, 0.0, 0.0, 0.0], 'rewardMean': 0.6935973200470771, 'totalEpisodes': 172, 'stepsPerEpisode': 122, 'rewardPerEpisode': 82.79665987810476
'totalSteps': 14080, 'rewardStep': 0.6202463751421163, 'errorList': [], 'lossList': [0.0, -1.3726562070846557, 0.0, 28.327004382610323, 0.0, 0.0, 0.0], 'rewardMean': 0.6756420615113924, 'totalEpisodes': 179, 'stepsPerEpisode': 58, 'rewardPerEpisode': 50.89888420256902
'totalSteps': 15360, 'rewardStep': 0.67114272720544, 'errorList': [], 'lossList': [0.0, -1.3904126346111298, 0.0, 13.63913807630539, 0.0, 0.0, 0.0], 'rewardMean': 0.6507241826222728, 'totalEpisodes': 183, 'stepsPerEpisode': 142, 'rewardPerEpisode': 116.44123073242363
'totalSteps': 16640, 'rewardStep': 0.9006989433487271, 'errorList': [], 'lossList': [0.0, -1.4162013185024263, 0.0, 8.698807327747344, 0.0, 0.0, 0.0], 'rewardMean': 0.6833848054190372, 'totalEpisodes': 184, 'stepsPerEpisode': 135, 'rewardPerEpisode': 123.29725881421436
'totalSteps': 17920, 'rewardStep': 0.8638136072592403, 'errorList': [], 'lossList': [0.0, -1.394291655421257, 0.0, 6.977138602882624, 0.0, 0.0, 0.0], 'rewardMean': 0.6944704235132066, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.4474334139218
'totalSteps': 19200, 'rewardStep': 0.8722183673246421, 'errorList': [], 'lossList': [0.0, -1.3508155173063279, 0.0, 4.507554158270359, 0.0, 0.0, 0.0], 'rewardMean': 0.7276332004348326, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.014843732033
'totalSteps': 20480, 'rewardStep': 0.891076610121061, 'errorList': [], 'lossList': [0.0, -1.3319778287410735, 0.0, 3.648207125812769, 0.0, 0.0, 0.0], 'rewardMean': 0.7280282900839586, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.9141878208395
'totalSteps': 21760, 'rewardStep': 0.9737364860869601, 'errorList': [0.13756073826278767, 0.1498231493610823, 0.1406678062168252, 0.2775683939862468, 0.1630699344207923, 0.14772400959150872, 0.18673402435194034, 0.13809653865298233, 0.12678092591444834, 0.15844680709769599, 0.1580317689913311, 0.12740380233564783, 0.11668335948898426, 0.14191565489741215, 0.13156270518122123, 0.16573039610511003, 0.1345341160699025, 0.18462785536724743, 0.14601329648190606, 0.14795772183846098, 0.14587583227866105, 0.14507959375570254, 0.15986802038190567, 0.1332341576490406, 0.16219349400675967, 0.14485460954442397, 0.1450726979524231, 0.1452860050817591, 0.2500387940209974, 0.143564924494936, 0.14713946951805776, 0.1306765591083886, 0.16035636497057976, 0.11640409108613398, 0.15228766561389007, 0.15475016183505402, 0.1512620326154156, 0.1507974980558648, 0.1472792045162142, 0.16316392150902442, 0.12481186643030061, 0.13867568101894326, 0.1432639551611384, 0.2734788159386867, 0.15393073520126352, 0.1262347107632182, 0.16098615159416765, 0.12325653782302409, 0.2090286622607308, 0.16371742026944675], 'lossList': [0.0, -1.325759842991829, 0.0, 3.4219036607444284, 0.0, 0.0, 0.0], 'rewardMean': 0.734465595232027, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.8561365861062, 'successfulTests': 46
'totalSteps': 23040, 'rewardStep': 0.8000940055528261, 'errorList': [], 'lossList': [0.0, -1.307334936261177, 0.0, 1.5646909171342849, 0.0, 0.0, 0.0], 'rewardMean': 0.7420820053961434, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.371833090269
'totalSteps': 24320, 'rewardStep': 0.8924462698871879, 'errorList': [], 'lossList': [0.0, -1.2727978807687759, 0.0, 1.0805934881046415, 0.0, 0.0, 0.0], 'rewardMean': 0.7846985012524714, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.6247128513585
'totalSteps': 25600, 'rewardStep': 0.8274765585306794, 'errorList': [], 'lossList': [0.0, -1.2463458734750748, 0.0, 0.668045681938529, 0.0, 0.0, 0.0], 'rewardMean': 0.8312949950458879, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.6483022765915
#maxSuccessfulTests=46, maxSuccessfulTestsAtStep=21760, timeSpent=81.25
