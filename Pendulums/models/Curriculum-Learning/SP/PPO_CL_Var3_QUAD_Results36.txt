#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 36
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7654203814203566, 'errorList': [], 'lossList': [0.0, -1.4420830816030503, 0.0, 32.63007572650909, 0.0, 0.0, 0.0], 'rewardMean': 0.6411071415599048, 'totalEpisodes': 12, 'stepsPerEpisode': 78, 'rewardPerEpisode': 66.9722151197293
'totalSteps': 3840, 'rewardStep': 0.8788200332672299, 'errorList': [], 'lossList': [0.0, -1.4576596379280091, 0.0, 26.54335523724556, 0.0, 0.0, 0.0], 'rewardMean': 0.7203447721290132, 'totalEpisodes': 13, 'stepsPerEpisode': 1262, 'rewardPerEpisode': 912.5454432460554
'totalSteps': 5120, 'rewardStep': 0.8349285088332358, 'errorList': [], 'lossList': [0.0, -1.4318596041202545, 0.0, 33.27831966638565, 0.0, 0.0, 0.0], 'rewardMean': 0.7489907063050688, 'totalEpisodes': 16, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.828071481613168
'totalSteps': 6400, 'rewardStep': 0.9240884899504989, 'errorList': [], 'lossList': [0.0, -1.413667688369751, 0.0, 74.97121681213379, 0.0, 0.0, 0.0], 'rewardMean': 0.7840102630341548, 'totalEpisodes': 25, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.434571853240095
'totalSteps': 7680, 'rewardStep': 0.5290681704473486, 'errorList': [], 'lossList': [0.0, -1.4030167573690415, 0.0, 195.4931620788574, 0.0, 0.0, 0.0], 'rewardMean': 0.741519914269687, 'totalEpisodes': 68, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.0944890336522637
'totalSteps': 8960, 'rewardStep': 0.7740582541081072, 'errorList': [], 'lossList': [0.0, -1.3975559198856353, 0.0, 73.34485492706298, 0.0, 0.0, 0.0], 'rewardMean': 0.7461682485323184, 'totalEpisodes': 99, 'stepsPerEpisode': 29, 'rewardPerEpisode': 26.068999496585928
'totalSteps': 10240, 'rewardStep': 0.7058380030265339, 'errorList': [], 'lossList': [0.0, -1.3963236314058305, 0.0, 49.847140331268314, 0.0, 0.0, 0.0], 'rewardMean': 0.7411269678440955, 'totalEpisodes': 131, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7058380030265339
'totalSteps': 11520, 'rewardStep': 0.5221735801341656, 'errorList': [], 'lossList': [0.0, -1.3883288472890853, 0.0, 32.06516480445862, 0.0, 0.0, 0.0], 'rewardMean': 0.7167988136541033, 'totalEpisodes': 142, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.24463562159406
'totalSteps': 12800, 'rewardStep': 0.4658090449079043, 'errorList': [], 'lossList': [0.0, -1.3508298736810684, 0.0, 30.019933161735533, 0.0, 0.0, 0.0], 'rewardMean': 0.6916998367794833, 'totalEpisodes': 151, 'stepsPerEpisode': 89, 'rewardPerEpisode': 51.54663047845665
'totalSteps': 14080, 'rewardStep': 0.6842940106165651, 'errorList': [], 'lossList': [0.0, -1.328850491642952, 0.0, 20.50086193561554, 0.0, 0.0, 0.0], 'rewardMean': 0.7084498476711946, 'totalEpisodes': 158, 'stepsPerEpisode': 167, 'rewardPerEpisode': 135.43232617863168
'totalSteps': 15360, 'rewardStep': 0.802154281267316, 'errorList': [], 'lossList': [0.0, -1.3336980438232422, 0.0, 8.823224185705184, 0.0, 0.0, 0.0], 'rewardMean': 0.7121232376558905, 'totalEpisodes': 164, 'stepsPerEpisode': 118, 'rewardPerEpisode': 92.35954525089564
'totalSteps': 16640, 'rewardStep': 0.8805339161058066, 'errorList': [], 'lossList': [0.0, -1.3301767331361771, 0.0, 6.5958668541908265, 0.0, 0.0, 0.0], 'rewardMean': 0.7122946259397482, 'totalEpisodes': 169, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.637694646360675
'totalSteps': 17920, 'rewardStep': 0.5141986048019038, 'errorList': [], 'lossList': [0.0, -1.3246170115470886, 0.0, 6.228531565368176, 0.0, 0.0, 0.0], 'rewardMean': 0.680221635536615, 'totalEpisodes': 170, 'stepsPerEpisode': 812, 'rewardPerEpisode': 675.0890211870933
'totalSteps': 19200, 'rewardStep': 0.9197437832160356, 'errorList': [], 'lossList': [0.0, -1.3209241193532943, 0.0, 4.866189664304256, 0.0, 0.0, 0.0], 'rewardMean': 0.6797871648631687, 'totalEpisodes': 171, 'stepsPerEpisode': 1120, 'rewardPerEpisode': 967.5543532836338
'totalSteps': 20480, 'rewardStep': 0.49510633623066597, 'errorList': [], 'lossList': [0.0, -1.3000568771362304, 0.0, 2.8798299020528795, 0.0, 0.0, 0.0], 'rewardMean': 0.6763909814415004, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 983.4406656955638
'totalSteps': 21760, 'rewardStep': 0.7368906308435896, 'errorList': [], 'lossList': [0.0, -1.279970921278, 0.0, 1.8706724534928798, 0.0, 0.0, 0.0], 'rewardMean': 0.6726742191150487, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.936138135582
'totalSteps': 23040, 'rewardStep': 0.8907240297879893, 'errorList': [], 'lossList': [0.0, -1.2577704310417175, 0.0, 2.3783123349398374, 0.0, 0.0, 0.0], 'rewardMean': 0.6911628217911943, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.8638004449517
'totalSteps': 24320, 'rewardStep': 0.8594425872118551, 'errorList': [], 'lossList': [0.0, -1.2220561182498932, 0.0, 1.0757507829368114, 0.0, 0.0, 0.0], 'rewardMean': 0.7248897224989631, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1136.7036963801445
'totalSteps': 25600, 'rewardStep': 0.979210288886984, 'errorList': [0.2244191067402465, 0.22807257424427876, 0.20236203527575947, 0.22329171025354685, 0.21018304880958524, 0.309842025837251, 0.2506477728352113, 0.20663744165067183, 0.29503746962537686, 0.23997052179175668, 0.214915128901024, 0.2539464805951178, 0.2542681416878084, 0.22479064821239536, 0.2003459428270131, 0.2504476206099419, 0.25136385262978167, 0.23675133204628998, 0.2600643852705751, 0.2593233017403374, 0.21163015333544133, 0.21007620240726363, 0.19875714844734524, 0.32001025158790003, 0.2084081251254376, 0.2307184152102276, 0.2845530128653167, 0.1988856400313502, 0.25050815295268397, 0.2174677982156895, 0.2637170271601397, 0.2263907148474571, 0.21400376301113969, 0.29706376432182285, 0.18631086795975785, 0.2114708203766703, 0.395414415947442, 0.2366450817406095, 0.20565653784596638, 0.3362069592118028, 0.22211141148790747, 0.21125886713410985, 0.19984337915866185, 0.1910475146103369, 0.21769578797726405, 0.256171408871717, 0.20755833923798067, 0.21029484941710225, 0.19030485156790977, 0.21750134949813157], 'lossList': [0.0, -1.1657723933458328, 0.0, 1.287801977545023, 0.0, 0.0, 0.0], 'rewardMean': 0.7762298468968711, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1205.0487275243433, 'successfulTests': 6
#maxSuccessfulTests=6, maxSuccessfulTestsAtStep=25600, timeSpent=84.98
