#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 39
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8661002520882264, 'errorList': [], 'lossList': [0.0, -1.4225112920999528, 0.0, 29.403774397373198, 0.0, 0.0, 0.0], 'rewardMean': 0.7690078291157849, 'totalEpisodes': 13, 'stepsPerEpisode': 317, 'rewardPerEpisode': 250.47782916578092
'totalSteps': 3840, 'rewardStep': 0.6982245714046844, 'errorList': [], 'lossList': [0.0, -1.4354460042715074, 0.0, 31.069573485851286, 0.0, 0.0, 0.0], 'rewardMean': 0.7454134098787515, 'totalEpisodes': 16, 'stepsPerEpisode': 182, 'rewardPerEpisode': 130.90655388121047
'totalSteps': 5120, 'rewardStep': 0.5208455964053378, 'errorList': [], 'lossList': [0.0, -1.4398198068141936, 0.0, 30.280402648448945, 0.0, 0.0, 0.0], 'rewardMean': 0.6892714565103981, 'totalEpisodes': 19, 'stepsPerEpisode': 281, 'rewardPerEpisode': 196.56389742413188
'totalSteps': 6400, 'rewardStep': 0.9826416535498909, 'errorList': [209.6238392215433, 142.1563544198172, 212.19082810205873, 205.65512226932066, 202.75476128502075, 190.26902244075947, 210.52400665734035, 201.0317226093239, 207.07928339439903, 213.73784935214832, 168.38257847208735, 205.15193216395593, 210.33109156425235, 134.81399247933786, 197.59247028525823, 206.4635821451026, 215.28058919563827, 171.00314194117436, 196.3687641076047, 214.27201114979442, 181.3447489961172, 206.01499117469248, 215.4816887338762, 190.48000209947375, 212.48507638407668, 209.76138750469954, 194.85215018007602, 199.1918384638023, 215.91358334588386, 214.03801878726716, 210.59176448602102, 216.07447826188138, 187.38121840377252, 177.4976041342776, 214.5265879277519, 196.89293637634034, 212.35760675053837, 219.61609713018362, 207.38324231103408, 202.37015011465508, 211.235759469546, 196.16558763544253, 191.07544242343448, 218.38691156555836, 135.33926993260954, 212.7170534771088, 219.8194714653425, 212.60670117919997, 195.89772364682116, 202.10735706518338], 'lossList': [0.0, -1.4385396760702134, 0.0, 114.49267093658447, 0.0, 0.0, 0.0], 'rewardMean': 0.7479454959182966, 'totalEpisodes': 39, 'stepsPerEpisode': 34, 'rewardPerEpisode': 31.59199763016281, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.5872525587462012, 'errorList': [], 'lossList': [0.0, -1.4336540377140046, 0.0, 131.6318469619751, 0.0, 0.0, 0.0], 'rewardMean': 0.7211633397229473, 'totalEpisodes': 90, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.5960292515310797
'totalSteps': 8960, 'rewardStep': 0.8558227448628434, 'errorList': [], 'lossList': [0.0, -1.4258534187078475, 0.0, 58.85956489562988, 0.0, 0.0, 0.0], 'rewardMean': 0.7404003976000754, 'totalEpisodes': 139, 'stepsPerEpisode': 28, 'rewardPerEpisode': 18.191944424118713
'totalSteps': 10240, 'rewardStep': 0.5268056500183923, 'errorList': [], 'lossList': [0.0, -1.4085196816921235, 0.0, 42.28314949035644, 0.0, 0.0, 0.0], 'rewardMean': 0.713701054152365, 'totalEpisodes': 152, 'stepsPerEpisode': 152, 'rewardPerEpisode': 110.92378066007628
'totalSteps': 11520, 'rewardStep': 0.9057815492241903, 'errorList': [], 'lossList': [0.0, -1.3826016134023666, 0.0, 36.47429058074951, 0.0, 0.0, 0.0], 'rewardMean': 0.7350433313825678, 'totalEpisodes': 162, 'stepsPerEpisode': 78, 'rewardPerEpisode': 64.94587711969041
'totalSteps': 12800, 'rewardStep': 0.8470051606609746, 'errorList': [], 'lossList': [0.0, -1.36460211455822, 0.0, 31.233058280944825, 0.0, 0.0, 0.0], 'rewardMean': 0.7462395143104085, 'totalEpisodes': 169, 'stepsPerEpisode': 64, 'rewardPerEpisode': 56.48329042729423
'totalSteps': 14080, 'rewardStep': 0.5779720799618631, 'errorList': [], 'lossList': [0.0, -1.3554843419790268, 0.0, 16.398852000236513, 0.0, 0.0, 0.0], 'rewardMean': 0.7368451816922604, 'totalEpisodes': 173, 'stepsPerEpisode': 139, 'rewardPerEpisode': 97.46386635745039
'totalSteps': 15360, 'rewardStep': 0.7809621477221282, 'errorList': [], 'lossList': [0.0, -1.3662360411882402, 0.0, 29.506346697807313, 0.0, 0.0, 0.0], 'rewardMean': 0.7283313712556506, 'totalEpisodes': 180, 'stepsPerEpisode': 258, 'rewardPerEpisode': 221.63555513458502
'totalSteps': 16640, 'rewardStep': 0.5892917390656504, 'errorList': [], 'lossList': [0.0, -1.3568497806787492, 0.0, 10.989186687469482, 0.0, 0.0, 0.0], 'rewardMean': 0.7174380880217474, 'totalEpisodes': 182, 'stepsPerEpisode': 297, 'rewardPerEpisode': 241.81822337974415
'totalSteps': 17920, 'rewardStep': 0.7199431862712022, 'errorList': [], 'lossList': [0.0, -1.3323069113492965, 0.0, 7.4936094319820405, 0.0, 0.0, 0.0], 'rewardMean': 0.7373478470083337, 'totalEpisodes': 186, 'stepsPerEpisode': 98, 'rewardPerEpisode': 85.24375165225162
'totalSteps': 19200, 'rewardStep': 0.7595590357653456, 'errorList': [], 'lossList': [0.0, -1.316901233792305, 0.0, 7.3513221991062165, 0.0, 0.0, 0.0], 'rewardMean': 0.7150395852298791, 'totalEpisodes': 191, 'stepsPerEpisode': 310, 'rewardPerEpisode': 268.216815992871
'totalSteps': 20480, 'rewardStep': 0.9356282534134482, 'errorList': [1.816459901169316, 5.931400685023386, 2.3559784587264434, 5.978655552022535, 1.189363010540429, 1.2012672650796, 4.495970490933682, 6.635552706670411, 1.3829911800610584, 0.5234685883723205, 0.38773395823322804, 1.4306940569722362, 3.44366938125402, 4.3373423833856695, 8.142592305545694, 0.6124539038521005, 5.491252522926755, 3.0329526683237575, 1.1837207727348278, 0.49174823153984387, 8.120478784717196, 5.657450946809674, 0.5998580891226636, 1.103252802515599, 0.8887749382513223, 4.351273737794907, 5.836222070795682, 1.9975899031781172, 2.174245915572097, 0.35811301561778963, 3.5354815055170654, 2.794877956307229, 2.726440286649294, 6.883195454561752, 2.3969659561371386, 3.8558415418883616, 5.2868499756670735, 1.0931080482655835, 7.397137648365385, 0.5191163077295196, 0.6067048997840282, 8.672061402114553, 6.652574236151184, 2.7630590795415317, 4.4098175126017765, 7.586824784088397, 4.975248456590603, 6.605234982903838, 1.5891432981378992, 0.14371459210163465], 'lossList': [0.0, -1.2784447741508485, 0.0, 5.7187261837720875, 0.0, 0.0, 0.0], 'rewardMean': 0.7498771546966039, 'totalEpisodes': 195, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.574552152548105, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.5861524231817101, 'errorList': [], 'lossList': [0.0, -1.249280630350113, 0.0, 4.483650854229927, 0.0, 0.0, 0.0], 'rewardMean': 0.7229101225284905, 'totalEpisodes': 199, 'stepsPerEpisode': 249, 'rewardPerEpisode': 197.38954766356463
'totalSteps': 23040, 'rewardStep': 0.665299834488458, 'errorList': [], 'lossList': [0.0, -1.2508500987291336, 0.0, 4.08719464302063, 0.0, 0.0, 0.0], 'rewardMean': 0.7367595409754971, 'totalEpisodes': 202, 'stepsPerEpisode': 154, 'rewardPerEpisode': 118.39699394629224
'totalSteps': 24320, 'rewardStep': 0.4139622467059681, 'errorList': [], 'lossList': [0.0, -1.2405574864149094, 0.0, 3.3239752721786497, 0.0, 0.0, 0.0], 'rewardMean': 0.6875776107236748, 'totalEpisodes': 204, 'stepsPerEpisode': 616, 'rewardPerEpisode': 475.27600071096026
'totalSteps': 25600, 'rewardStep': 0.48484760650771463, 'errorList': [], 'lossList': [0.0, -1.2207230293750764, 0.0, 2.017078556418419, 0.0, 0.0, 0.0], 'rewardMean': 0.6513618553083489, 'totalEpisodes': 206, 'stepsPerEpisode': 514, 'rewardPerEpisode': 368.1562591443814
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=20480, timeSpent=104.88
