#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.5827392687472346, 'errorList': [], 'lossList': [0.0, -1.4101531916856767, 0.0, 34.053767137527466, 0.0, 0.0, 0.0], 'rewardMean': 0.7519147014493208, 'totalEpisodes': 70, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.505954603848345
'totalSteps': 3840, 'rewardStep': 0.9466195977622777, 'errorList': [], 'lossList': [0.0, -1.4053219509124757, 0.0, 43.94052324295044, 0.0, 0.0, 0.0], 'rewardMean': 0.8168163335536397, 'totalEpisodes': 93, 'stepsPerEpisode': 57, 'rewardPerEpisode': 44.22407567596588
'totalSteps': 5120, 'rewardStep': 0.7656001228808283, 'errorList': [], 'lossList': [0.0, -1.3859781390428543, 0.0, 49.35141241073608, 0.0, 0.0, 0.0], 'rewardMean': 0.8040122808854369, 'totalEpisodes': 108, 'stepsPerEpisode': 47, 'rewardPerEpisode': 34.63582735992697
'totalSteps': 6400, 'rewardStep': 0.8820501982669878, 'errorList': [], 'lossList': [0.0, -1.3822384834289552, 0.0, 77.0120818901062, 0.0, 0.0, 0.0], 'rewardMean': 0.8196198643617472, 'totalEpisodes': 144, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.6207542030489805
'totalSteps': 7680, 'rewardStep': 0.5018577144074694, 'errorList': [], 'lossList': [0.0, -1.3888994479179382, 0.0, 49.76572986602783, 0.0, 0.0, 0.0], 'rewardMean': 0.7666595060360342, 'totalEpisodes': 160, 'stepsPerEpisode': 34, 'rewardPerEpisode': 21.578878479677847
'totalSteps': 8960, 'rewardStep': 0.7322172576747833, 'errorList': [], 'lossList': [0.0, -1.3824782711267471, 0.0, 52.41583926200867, 0.0, 0.0, 0.0], 'rewardMean': 0.7617391848415698, 'totalEpisodes': 175, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.47844340392729
'totalSteps': 10240, 'rewardStep': 0.7280228527404661, 'errorList': [], 'lossList': [0.0, -1.3652546697854995, 0.0, 27.360669083595276, 0.0, 0.0, 0.0], 'rewardMean': 0.7575246433289318, 'totalEpisodes': 180, 'stepsPerEpisode': 74, 'rewardPerEpisode': 54.63348194055281
'totalSteps': 11520, 'rewardStep': 0.284176274433369, 'errorList': [], 'lossList': [0.0, -1.353678719997406, 0.0, 11.742158257961274, 0.0, 0.0, 0.0], 'rewardMean': 0.7049303801183137, 'totalEpisodes': 185, 'stepsPerEpisode': 144, 'rewardPerEpisode': 99.3135817360848
'totalSteps': 12800, 'rewardStep': 0.6406350857471635, 'errorList': [], 'lossList': [0.0, -1.3506978225708008, 0.0, 37.86373079299927, 0.0, 0.0, 0.0], 'rewardMean': 0.6985008506811987, 'totalEpisodes': 193, 'stepsPerEpisode': 65, 'rewardPerEpisode': 50.92998214240173
'totalSteps': 14080, 'rewardStep': 0.8205172050537767, 'errorList': [], 'lossList': [0.0, -1.3337917053699493, 0.0, 23.572773401737212, 0.0, 0.0, 0.0], 'rewardMean': 0.6884435577714356, 'totalEpisodes': 196, 'stepsPerEpisode': 1080, 'rewardPerEpisode': 848.0654676236437
'totalSteps': 15360, 'rewardStep': 0.7008135999015831, 'errorList': [], 'lossList': [0.0, -1.3200276678800582, 0.0, 21.71136034488678, 0.0, 0.0, 0.0], 'rewardMean': 0.7002509908868704, 'totalEpisodes': 199, 'stepsPerEpisode': 759, 'rewardPerEpisode': 600.5580664117348
'totalSteps': 16640, 'rewardStep': 0.6140164973364246, 'errorList': [], 'lossList': [0.0, -1.3094688260555267, 0.0, 5.997500810027122, 0.0, 0.0, 0.0], 'rewardMean': 0.6669906808442853, 'totalEpisodes': 202, 'stepsPerEpisode': 141, 'rewardPerEpisode': 124.06655071880012
'totalSteps': 17920, 'rewardStep': 0.34306579999400827, 'errorList': [], 'lossList': [0.0, -1.2860954600572585, 0.0, 6.392440071105957, 0.0, 0.0, 0.0], 'rewardMean': 0.6247372485556032, 'totalEpisodes': 206, 'stepsPerEpisode': 517, 'rewardPerEpisode': 414.9266991602239
'totalSteps': 19200, 'rewardStep': 0.7386285741288763, 'errorList': [], 'lossList': [0.0, -1.2649925059080125, 0.0, 2.9534872606396676, 0.0, 0.0, 0.0], 'rewardMean': 0.6103950861417919, 'totalEpisodes': 207, 'stepsPerEpisode': 1270, 'rewardPerEpisode': 881.9885665301584
'totalSteps': 20480, 'rewardStep': 0.9432493690169927, 'errorList': [0.18483026126860935, 0.09710387631117909, 0.16813577775833707, 0.1463840677875012, 0.1268426878737324, 0.16053531349815078, 0.07969053554716671, 0.1652176075457608, 0.11477381814001981, 0.16790364403636301, 0.05739355535392629, 0.0554298787508186, 0.07356584231949746, 0.023467627812062377, 0.17913928043461239, 0.1636692034850843, 0.08159587731021467, 0.05215062191930259, 0.13741889771140284, 0.08416283026377222, 0.22389690776414625, 0.11712816312996309, 0.10189476750982095, 0.0730092516750321, 0.12495794809160908, 0.15077200832609833, 0.0989087224598619, 0.06542653593173231, 0.058233453006897105, 0.11935196589236133, 0.0573810375933089, 0.027950948518249953, 0.09213225960227105, 0.2141174559523388, 0.016657235667610536, 0.08821857847518776, 0.06727182933344353, 0.1335148073467745, 0.11515458602053023, 0.10968853425857929, 0.07581583130289768, 0.2084625086241631, 0.10301398098227828, 0.0830197849121146, 0.2599066285274368, 0.12174447713843153, 0.1030851776084283, 0.13859698397720133, 0.06075246821325402, 0.13144655902799413], 'lossList': [0.0, -1.2567923039197921, 0.0, 1.7104107044637202, 0.0, 0.0, 0.0], 'rewardMean': 0.6545342516027444, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.9641299260718, 'successfulTests': 46
'totalSteps': 21760, 'rewardStep': 0.8601880116509177, 'errorList': [], 'lossList': [0.0, -1.2369776856899262, 0.0, 1.4314603339880705, 0.0, 0.0, 0.0], 'rewardMean': 0.6673313270003577, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.004304077817
'totalSteps': 23040, 'rewardStep': 0.8521806480521803, 'errorList': [], 'lossList': [0.0, -1.224580838084221, 0.0, 1.2025109660625457, 0.0, 0.0, 0.0], 'rewardMean': 0.6797471065315293, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.2675945466183
'totalSteps': 24320, 'rewardStep': 0.9207059911059329, 'errorList': [], 'lossList': [0.0, -1.2137636786699295, 0.0, 0.9458387340418994, 0.0, 0.0, 0.0], 'rewardMean': 0.7434000781987856, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.657274423387
'totalSteps': 25600, 'rewardStep': 0.9895259422260134, 'errorList': [0.034611168090814846, 0.01688028657198765, 0.01667161723170744, 0.0424710302096528, 0.07471293351823477, 0.07138710572479429, 0.028233910839786342, 0.02075783475644111, 0.05518337278817202, 0.08916620254416045, 0.07398713574606187, 0.030967458921876787, 0.07766514890942008, 0.12008272894254889, 0.08895258084384974, 0.04505941789025193, 0.09828183120188397, 0.041792515114102695, 0.07804824642788066, 0.024459596830648366, 0.11492867065354573, 0.04236707190048446, 0.1242170982176055, 0.030351996635631033, 0.04990812616369514, 0.05944086862232291, 0.01874313799752346, 0.032726684576625545, 0.05445515415493371, 0.06308049593643515, 0.029863355063571524, 0.05214241608734288, 0.028694815938997507, 0.0871941164663883, 0.047372694442933884, 0.03505981251678119, 0.04119191162773279, 0.0833010601662116, 0.020761479305690844, 0.0922901529844097, 0.07661692368434757, 0.08812577575984686, 0.06855869289638158, 0.02495369398595995, 0.03311258335038421, 0.09290702977440232, 0.10875507112301556, 0.016392158001413377, 0.031170677857006265, 0.13140868591868862], 'lossList': [0.0, -1.1872879654169082, 0.0, 0.7547533017955721, 0.0, 0.0, 0.0], 'rewardMean': 0.7782891638466707, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.5418278939649, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=94.84
