#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 40
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.934300634733822, 'errorList': [], 'lossList': [0.0, -1.4350119984149934, 0.0, 29.79505714535713, 0.0, 0.0, 0.0], 'rewardMean': 0.9149411129709946, 'totalEpisodes': 8, 'stepsPerEpisode': 530, 'rewardPerEpisode': 375.2016448308542
'totalSteps': 3840, 'rewardStep': 0.730520188130831, 'errorList': [], 'lossList': [0.0, -1.4228237026929855, 0.0, 29.729782700538635, 0.0, 0.0, 0.0], 'rewardMean': 0.8534674713576068, 'totalEpisodes': 12, 'stepsPerEpisode': 258, 'rewardPerEpisode': 195.3323934063496
'totalSteps': 5120, 'rewardStep': 0.6946566849592876, 'errorList': [], 'lossList': [0.0, -1.4182269567251204, 0.0, 29.266003737449648, 0.0, 0.0, 0.0], 'rewardMean': 0.813764774758027, 'totalEpisodes': 14, 'stepsPerEpisode': 271, 'rewardPerEpisode': 210.65447664361224
'totalSteps': 6400, 'rewardStep': 0.6769655336839763, 'errorList': [], 'lossList': [0.0, -1.4000625520944596, 0.0, 88.30992057800293, 0.0, 0.0, 0.0], 'rewardMean': 0.7864049265432168, 'totalEpisodes': 26, 'stepsPerEpisode': 178, 'rewardPerEpisode': 126.73599854289603
'totalSteps': 7680, 'rewardStep': 0.9585884699825077, 'errorList': [185.93156567178409, 206.76738577056813, 174.90275173512617, 205.7494224081521, 190.27751603543135, 194.6276815360161, 191.70434289715837, 206.91935124880231, 184.59967348420201, 206.46421001631703, 202.66455446116618, 193.03920747852328, 200.64992930798567, 186.92298995155284, 196.82361791275073, 195.05780748238502, 191.48971146811041, 201.25662618479944, 186.31639725238924, 170.66776380851078, 204.7858738066627, 184.4299656581653, 179.82889015471673, 198.98192416246368, 205.91965334340625, 183.34828398164538, 190.53612807630518, 194.85237602807206, 189.99641904025484, 195.25806790716666, 171.30121515311944, 202.8365908234811, 190.83621439330568, 97.28274090982521, 204.08994860051996, 196.67187682636643, 197.71587705227714, 184.45825797807248, 189.40154045701246, 188.3597257668168, 169.43291423152917, 198.0718585005714, 191.67426035336248, 146.85519788864215, 152.53806027131927, 199.12830465669958, 206.9534366001772, 201.9652783042492, 186.94903416772925, 188.8767363782918], 'lossList': [0.0, -1.3975107312202453, 0.0, 244.04791763305664, 0.0, 0.0, 0.0], 'rewardMean': 0.8151021837830986, 'totalEpisodes': 83, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.189067889394725, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.9151222596168537, 'errorList': [], 'lossList': [0.0, -1.3821775281429292, 0.0, 98.44312530517578, 0.0, 0.0, 0.0], 'rewardMean': 0.8293907660450637, 'totalEpisodes': 119, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.412713042188733
'totalSteps': 10240, 'rewardStep': 0.38357103481958443, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7303197146616237, 'totalEpisodes': 151, 'stepsPerEpisode': 151, 'rewardPerEpisode': 115.00700321161172
'totalSteps': 11520, 'rewardStep': 0.7459474083042799, 'errorList': [], 'lossList': [0.0, -1.3672732657194138, 0.0, 61.81400159835815, 0.0, 0.0, 0.0], 'rewardMean': 0.7318824840258893, 'totalEpisodes': 175, 'stepsPerEpisode': 68, 'rewardPerEpisode': 52.75872150247363
'totalSteps': 12800, 'rewardStep': 0.6266748705423304, 'errorList': [], 'lossList': [0.0, -1.3522725880146027, 0.0, 53.910597476959225, 0.0, 0.0, 0.0], 'rewardMean': 0.7049918119593058, 'totalEpisodes': 189, 'stepsPerEpisode': 123, 'rewardPerEpisode': 86.18236908216599
'totalSteps': 14080, 'rewardStep': 0.23429485042821813, 'errorList': [], 'lossList': [0.0, -1.3387580627202988, 0.0, 29.31177960038185, 0.0, 0.0, 0.0], 'rewardMean': 0.6349912335287453, 'totalEpisodes': 194, 'stepsPerEpisode': 194, 'rewardPerEpisode': 152.959938360641
'totalSteps': 15360, 'rewardStep': 0.7281747067007066, 'errorList': [], 'lossList': [0.0, -1.3395935577154159, 0.0, 17.49769241094589, 0.0, 0.0, 0.0], 'rewardMean': 0.6347566853857329, 'totalEpisodes': 201, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.027517191390366
'totalSteps': 16640, 'rewardStep': 0.8909380771323248, 'errorList': [], 'lossList': [0.0, -1.3187554305791855, 0.0, 11.218043643236161, 0.0, 0.0, 0.0], 'rewardMean': 0.6543848246030366, 'totalEpisodes': 205, 'stepsPerEpisode': 105, 'rewardPerEpisode': 94.32331971963299
'totalSteps': 17920, 'rewardStep': 0.4360793693560637, 'errorList': [], 'lossList': [0.0, -1.2934201574325561, 0.0, 6.116716194152832, 0.0, 0.0, 0.0], 'rewardMean': 0.6302962081702453, 'totalEpisodes': 210, 'stepsPerEpisode': 209, 'rewardPerEpisode': 146.082375576829
'totalSteps': 19200, 'rewardStep': 0.803166809056819, 'errorList': [], 'lossList': [0.0, -1.2857048416137695, 0.0, 11.231504962444305, 0.0, 0.0, 0.0], 'rewardMean': 0.6147540420776764, 'totalEpisodes': 215, 'stepsPerEpisode': 63, 'rewardPerEpisode': 52.09609604554778
'totalSteps': 20480, 'rewardStep': 0.9496020795644328, 'errorList': [1.5485355698201115, 0.6052527603914312, 0.45412284538183456, 3.212850265880384, 0.18423137678917376, 0.34955776466876537, 3.505974056051767, 0.600698215236931, 0.4373286280962102, 2.29477224537506, 1.321111335021566, 0.7828401654639003, 4.134711304833696, 5.345634117489981, 2.500121798753531, 1.5920625180861256, 0.37171565454863476, 3.6344163085180923, 1.3903369143600304, 1.3068372668142405, 3.1540860506170487, 1.1512327723925675, 0.3801856215380621, 0.5069402121188484, 3.726887991147445, 1.5461895532798593, 0.9864603960002074, 1.4490758596444684, 1.152743890988626, 1.8252329581762268, 0.3838169718857747, 2.703694818668744, 0.2938302031057565, 4.221397048585743, 0.657290704332139, 1.8828741676806673, 0.32121203707032997, 1.1299874345750605, 2.155900957899603, 1.4042785946639789, 1.3929732727679278, 2.7440083596612546, 4.1161899634128245, 0.441431910795031, 1.0523446121578708, 3.5352127283623687, 2.916184459821468, 0.5465206583765941, 0.8135917356867086, 0.7629865707270372], 'lossList': [0.0, -1.2976651072502137, 0.0, 6.190696012973786, 0.0, 0.0, 0.0], 'rewardMean': 0.6182020240724344, 'totalEpisodes': 218, 'stepsPerEpisode': 90, 'rewardPerEpisode': 68.726678879155, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.7335176348991164, 'errorList': [], 'lossList': [0.0, -1.3041003197431564, 0.0, 4.8479134482145305, 0.0, 0.0, 0.0], 'rewardMean': 0.6531966840803876, 'totalEpisodes': 220, 'stepsPerEpisode': 195, 'rewardPerEpisode': 157.5143648346957
'totalSteps': 23040, 'rewardStep': 0.6316747388079541, 'errorList': [], 'lossList': [0.0, -1.288881253004074, 0.0, 3.6344368660449984, 0.0, 0.0, 0.0], 'rewardMean': 0.6780070544792245, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 882.6098432732197
'totalSteps': 24320, 'rewardStep': 0.84915124685039, 'errorList': [], 'lossList': [0.0, -1.2653341728448868, 0.0, 3.5679630535840987, 0.0, 0.0, 0.0], 'rewardMean': 0.6883274383338355, 'totalEpisodes': 221, 'stepsPerEpisode': 531, 'rewardPerEpisode': 425.41754833282414
'totalSteps': 25600, 'rewardStep': 0.7322042350328862, 'errorList': [], 'lossList': [0.0, -1.2494116252660752, 0.0, 2.1050412786006927, 0.0, 0.0, 0.0], 'rewardMean': 0.6988803747828911, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.2531525062743
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=20480, timeSpent=104.04
