#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 41
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7380486177223677, 'errorList': [], 'lossList': [0.0, -1.4462494283914566, 0.0, 36.020457713603975, 0.0, 0.0, 0.0], 'rewardMean': 0.6483481680993615, 'totalEpisodes': 12, 'stepsPerEpisode': 69, 'rewardPerEpisode': 57.74873433591924
'totalSteps': 3840, 'rewardStep': 0.8912609584733034, 'errorList': [], 'lossList': [0.0, -1.4638072901964188, 0.0, 27.345401606559754, 0.0, 0.0, 0.0], 'rewardMean': 0.7293190982240088, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 963.5438108156758
'totalSteps': 5120, 'rewardStep': 0.6029484423369488, 'errorList': [], 'lossList': [0.0, -1.4510455679893495, 0.0, 20.933794082999228, 0.0, 0.0, 0.0], 'rewardMean': 0.6977264342522438, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 955.2407891798689
'totalSteps': 6400, 'rewardStep': 0.8587325652926546, 'errorList': [], 'lossList': [0.0, -1.4372728043794631, 0.0, 75.73736232757568, 0.0, 0.0, 0.0], 'rewardMean': 0.729927660460326, 'totalEpisodes': 19, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.6483882873546625
'totalSteps': 7680, 'rewardStep': 0.5831773812362283, 'errorList': [], 'lossList': [0.0, -1.4212751358747482, 0.0, 200.78105976104737, 0.0, 0.0, 0.0], 'rewardMean': 0.705469280589643, 'totalEpisodes': 55, 'stepsPerEpisode': 38, 'rewardPerEpisode': 26.990944198422408
'totalSteps': 8960, 'rewardStep': 0.7790038286494849, 'errorList': [], 'lossList': [0.0, -1.418647713661194, 0.0, 93.90791261672973, 0.0, 0.0, 0.0], 'rewardMean': 0.7159742160267631, 'totalEpisodes': 85, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.576703085927607
'totalSteps': 10240, 'rewardStep': 0.6130122330104664, 'errorList': [], 'lossList': [0.0, -1.4110101401805877, 0.0, 58.43993444442749, 0.0, 0.0, 0.0], 'rewardMean': 0.7031039681497262, 'totalEpisodes': 103, 'stepsPerEpisode': 114, 'rewardPerEpisode': 83.18637962049027
'totalSteps': 11520, 'rewardStep': 0.9441994396496405, 'errorList': [7.1948470402741735, 6.9341856364058065, 4.013846905144077, 7.602352767403937, 6.570750523185727, 8.7666124562642, 7.646106734470998, 6.322797415844238, 0.6362341707022962, 8.866106922828113, 7.272753244811678, 0.8441652297551924, 7.258848197264921, 8.50538533280519, 8.382159556661279, 2.9799129371796726, 2.266836510866131, 1.6078582766302312, 5.006975917684542, 2.8133176237039517, 6.961494103280254, 5.799993308682352, 1.135493446101277, 4.770625627004482, 3.96177437909642, 2.63651547904003, 0.8068441843189569, 7.365014914143369, 8.075025988747813, 1.2724926471038682, 0.9345644133065201, 4.644130673028532, 5.990416147815779, 5.362518910541058, 5.930217185567072, 6.363674894084974, 1.99946670836854, 4.896599876733915, 1.4390833759204642, 7.173124200318284, 3.4493802586651583, 5.567997964065365, 8.656894885785928, 0.5838540535185233, 4.919865253379062, 1.494916197123203, 3.6358243495536327, 1.915083938711082, 4.957229860412496, 1.0345129412631615], 'lossList': [0.0, -1.3860812789201737, 0.0, 51.4519282245636, 0.0, 0.0, 0.0], 'rewardMean': 0.7298923538719388, 'totalEpisodes': 118, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9441994396496405, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.5646640302133811, 'errorList': [], 'lossList': [0.0, -1.3510325330495834, 0.0, 35.29494160175324, 0.0, 0.0, 0.0], 'rewardMean': 0.7133695215060831, 'totalEpisodes': 125, 'stepsPerEpisode': 132, 'rewardPerEpisode': 90.85074616559503
'totalSteps': 14080, 'rewardStep': 0.638838849921128, 'errorList': [], 'lossList': [0.0, -1.3182497161626816, 0.0, 12.74788400888443, 0.0, 0.0, 0.0], 'rewardMean': 0.7213886346505605, 'totalEpisodes': 130, 'stepsPerEpisode': 291, 'rewardPerEpisode': 228.38905748357354
'totalSteps': 15360, 'rewardStep': 0.9021103593914166, 'errorList': [], 'lossList': [0.0, -1.307175206542015, 0.0, 9.556380590200424, 0.0, 0.0, 0.0], 'rewardMean': 0.7377948088174653, 'totalEpisodes': 134, 'stepsPerEpisode': 266, 'rewardPerEpisode': 221.94651024035028
'totalSteps': 16640, 'rewardStep': 0.9006292106212826, 'errorList': [], 'lossList': [0.0, -1.29747317135334, 0.0, 9.68194410443306, 0.0, 0.0, 0.0], 'rewardMean': 0.7387316340322633, 'totalEpisodes': 137, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.832295455407138
'totalSteps': 17920, 'rewardStep': 0.4764098035273416, 'errorList': [], 'lossList': [0.0, -1.297363618016243, 0.0, 5.235176531672478, 0.0, 0.0, 0.0], 'rewardMean': 0.7260777701513026, 'totalEpisodes': 138, 'stepsPerEpisode': 232, 'rewardPerEpisode': 155.124223106701
'totalSteps': 19200, 'rewardStep': 0.7465814281879855, 'errorList': [], 'lossList': [0.0, -1.2643124049901961, 0.0, 3.169278138875961, 0.0, 0.0, 0.0], 'rewardMean': 0.7148626564408356, 'totalEpisodes': 140, 'stepsPerEpisode': 614, 'rewardPerEpisode': 413.84926022140615
'totalSteps': 20480, 'rewardStep': 0.8601563700234289, 'errorList': [], 'lossList': [0.0, -1.1970129400491714, 0.0, 2.21372795984149, 0.0, 0.0, 0.0], 'rewardMean': 0.7425605553195556, 'totalEpisodes': 140, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.3381770089238
'totalSteps': 21760, 'rewardStep': 0.8247691838639721, 'errorList': [], 'lossList': [0.0, -1.1335512787103652, 0.0, 1.8405760580301285, 0.0, 0.0, 0.0], 'rewardMean': 0.7471370908410043, 'totalEpisodes': 140, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.7536084735614
'totalSteps': 23040, 'rewardStep': 0.9526345045194514, 'errorList': [0.08924522427818238, 0.08707624968833985, 0.06867388952051084, 0.0982774022771882, 0.11135087185117055, 0.09198569659292936, 0.10087793052620934, 0.10485625562484224, 0.07812733930569149, 0.07866073757746031, 0.08558913126134023, 0.10098242944420355, 0.08199099508859542, 0.07716034971696165, 0.08180461047396209, 0.09966386867291897, 0.08197849891703533, 0.08126160489286759, 0.07355928237599894, 0.06758600195361808, 0.0977509448048745, 0.0682975491861965, 0.09911423127150497, 0.09391937307409137, 0.07928871481170821, 0.09630635816245849, 0.07409232730776595, 0.09429123654939583, 0.08156747620910655, 0.09720468878689158, 0.06226246844373608, 0.07077236336196137, 0.08827061633463001, 0.0639688419566421, 0.06677529883199418, 0.08272832928352379, 0.06929044699081469, 0.09640211376155024, 0.07494209801978512, 0.10020914925626744, 0.10095789623097438, 0.08921997715155527, 0.066233323577499, 0.08933820386322756, 0.08844854562210422, 0.08070903264358138, 0.08745930444651051, 0.09517051055773568, 0.09874374167805795, 0.07163133426677022], 'lossList': [0.0, -1.1153642457723618, 0.0, 1.90601645976305, 0.0, 0.0, 0.0], 'rewardMean': 0.7810993179919028, 'totalEpisodes': 140, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.767374822285, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=96.33
