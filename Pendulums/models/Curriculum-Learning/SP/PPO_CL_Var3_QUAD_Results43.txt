#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 43
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5841152101023863, 'errorList': [], 'lossList': [0.0, -1.421685341000557, 0.0, 30.222408400774, 0.0, 0.0, 0.0], 'rewardMean': 0.721163659328963, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.209705116181816
'totalSteps': 3840, 'rewardStep': 0.9622510118862264, 'errorList': [], 'lossList': [0.0, -1.4213661235570907, 0.0, 28.66413658618927, 0.0, 0.0, 0.0], 'rewardMean': 0.8015261101813841, 'totalEpisodes': 18, 'stepsPerEpisode': 489, 'rewardPerEpisode': 380.1253079019022
'totalSteps': 5120, 'rewardStep': 0.75547994396717, 'errorList': [], 'lossList': [0.0, -1.420074262022972, 0.0, 29.480418340563773, 0.0, 0.0, 0.0], 'rewardMean': 0.7900145686278306, 'totalEpisodes': 19, 'stepsPerEpisode': 181, 'rewardPerEpisode': 152.02697499837217
'totalSteps': 6400, 'rewardStep': 0.6060483211542043, 'errorList': [], 'lossList': [0.0, -1.417067386507988, 0.0, 94.15003913879394, 0.0, 0.0, 0.0], 'rewardMean': 0.7532213191331053, 'totalEpisodes': 29, 'stepsPerEpisode': 26, 'rewardPerEpisode': 18.399718789359458
'totalSteps': 7680, 'rewardStep': 0.6986962713659256, 'errorList': [], 'lossList': [0.0, -1.4111079800128936, 0.0, 215.9517461013794, 0.0, 0.0, 0.0], 'rewardMean': 0.7441338111719086, 'totalEpisodes': 85, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.149779387548746
'totalSteps': 8960, 'rewardStep': 0.46916934866535265, 'errorList': [], 'lossList': [0.0, -1.4053170049190522, 0.0, 60.16272336959839, 0.0, 0.0, 0.0], 'rewardMean': 0.704853173670972, 'totalEpisodes': 125, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.514254516526663
'totalSteps': 10240, 'rewardStep': 0.6796472865522586, 'errorList': [], 'lossList': [0.0, -1.403889930844307, 0.0, 40.913684587478635, 0.0, 0.0, 0.0], 'rewardMean': 0.701702437781133, 'totalEpisodes': 164, 'stepsPerEpisode': 33, 'rewardPerEpisode': 23.875988928960833
'totalSteps': 11520, 'rewardStep': 0.7731179976598249, 'errorList': [], 'lossList': [0.0, -1.3941796398162842, 0.0, 27.9229385471344, 0.0, 0.0, 0.0], 'rewardMean': 0.7096374999898765, 'totalEpisodes': 194, 'stepsPerEpisode': 94, 'rewardPerEpisode': 81.77585505049126
'totalSteps': 12800, 'rewardStep': 0.9117920296707903, 'errorList': [], 'lossList': [0.0, -1.3810358029603957, 0.0, 34.896413717269894, 0.0, 0.0, 0.0], 'rewardMean': 0.7298529529579679, 'totalEpisodes': 203, 'stepsPerEpisode': 27, 'rewardPerEpisode': 19.668494994468166
'totalSteps': 14080, 'rewardStep': 0.7524418696511906, 'errorList': [], 'lossList': [0.0, -1.3639777433872222, 0.0, 25.04985980987549, 0.0, 0.0, 0.0], 'rewardMean': 0.7192759290675329, 'totalEpisodes': 206, 'stepsPerEpisode': 220, 'rewardPerEpisode': 141.1043169904738
'totalSteps': 15360, 'rewardStep': 0.5042517949114675, 'errorList': [], 'lossList': [0.0, -1.337338621020317, 0.0, 13.253433446884156, 0.0, 0.0, 0.0], 'rewardMean': 0.7112895875484411, 'totalEpisodes': 208, 'stepsPerEpisode': 543, 'rewardPerEpisode': 356.57432613580175
'totalSteps': 16640, 'rewardStep': 0.8344880495480673, 'errorList': [], 'lossList': [0.0, -1.308063987493515, 0.0, 10.10166882365942, 0.0, 0.0, 0.0], 'rewardMean': 0.6985132913146251, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.8002247606505
'totalSteps': 17920, 'rewardStep': 0.9219683448136701, 'errorList': [], 'lossList': [0.0, -1.283345147371292, 0.0, 9.150091842636465, 0.0, 0.0, 0.0], 'rewardMean': 0.7151621313992751, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.4570419490742
'totalSteps': 19200, 'rewardStep': 0.8378122045458946, 'errorList': [], 'lossList': [0.0, -1.2579718351364135, 0.0, 16.323543607592583, 0.0, 0.0, 0.0], 'rewardMean': 0.7383385197384442, 'totalEpisodes': 210, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.836106194014954
'totalSteps': 20480, 'rewardStep': 0.9624648634904803, 'errorList': [0.7180483723161931, 0.49874709991307564, 0.6353921970009865, 0.12094960258638687, 0.16056636468659505, 0.9416725958984185, 1.2774069664258463, 0.7403932646440341, 0.7041896714185066, 0.12620889408056435, 0.1781206543742362, 0.32705373946979377, 0.6100862150113346, 0.579682408156706, 0.7321854546011377, 1.0838792152118606, 1.233724131218202, 0.24892503574422215, 0.616510023863901, 0.718662661544342, 0.5287694094295022, 1.046531830303115, 0.9235541680304716, 0.9505135451320955, 1.059591948333895, 0.6494054556015751, 0.6441937076813802, 0.24392383978773297, 0.31848218225444896, 0.3254408054968593, 1.2255820151361674, 0.08225083908524344, 0.4505811558152761, 0.3405993002260123, 0.5907700215016001, 0.7569816906517368, 1.233452263601297, 1.2408064345247993, 0.39178024514071885, 0.846109904967488, 0.22596225431507294, 0.05806874511914999, 0.17979695826793612, 0.8386807518118815, 1.0303929391460973, 1.1440886305504117, 1.0462504298540591, 0.6471097374734219, 1.3364290775951568, 1.0610715164236117], 'lossList': [0.0, -1.263770927786827, 0.0, 9.274659931063653, 0.0, 0.0, 0.0], 'rewardMean': 0.7647153789508997, 'totalEpisodes': 211, 'stepsPerEpisode': 39, 'rewardPerEpisode': 34.6115878996502, 'successfulTests': 7
'totalSteps': 21760, 'rewardStep': 0.4530723905880421, 'errorList': [], 'lossList': [0.0, -1.2614319902658462, 0.0, 6.004497816860676, 0.0, 0.0, 0.0], 'rewardMean': 0.7631056831431687, 'totalEpisodes': 212, 'stepsPerEpisode': 726, 'rewardPerEpisode': 475.2196419833027
'totalSteps': 23040, 'rewardStep': 0.49696951090789854, 'errorList': [], 'lossList': [0.0, -1.2675916808843612, 0.0, 5.372648904025555, 0.0, 0.0, 0.0], 'rewardMean': 0.7448379055787326, 'totalEpisodes': 213, 'stepsPerEpisode': 1263, 'rewardPerEpisode': 875.8133821178611
'totalSteps': 24320, 'rewardStep': 0.8035156052041791, 'errorList': [], 'lossList': [0.0, -1.2711598199605942, 0.0, 1.7382785385847093, 0.0, 0.0, 0.0], 'rewardMean': 0.747877666333168, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.044576131239
'totalSteps': 25600, 'rewardStep': 0.7696619819359729, 'errorList': [], 'lossList': [0.0, -1.270016993880272, 0.0, 0.5244405271112919, 0.0, 0.0, 0.0], 'rewardMean': 0.7336646615596863, 'totalEpisodes': 213, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1001.3364737749719
#maxSuccessfulTests=7, maxSuccessfulTestsAtStep=20480, timeSpent=82.04
