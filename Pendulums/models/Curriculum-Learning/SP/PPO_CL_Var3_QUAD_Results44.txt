#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 44
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8733076119111836, 'errorList': [], 'lossList': [0.0, -1.4268915903568269, 0.0, 33.309914875030515, 0.0, 0.0, 0.0], 'rewardMean': 0.7983292678952423, 'totalEpisodes': 13, 'stepsPerEpisode': 314, 'rewardPerEpisode': 260.53927527871997
'totalSteps': 3840, 'rewardStep': 0.7954348524764895, 'errorList': [], 'lossList': [0.0, -1.4389088159799577, 0.0, 31.26890241622925, 0.0, 0.0, 0.0], 'rewardMean': 0.7973644627556581, 'totalEpisodes': 16, 'stepsPerEpisode': 167, 'rewardPerEpisode': 127.87022683321705
'totalSteps': 5120, 'rewardStep': 0.5008728053697207, 'errorList': [], 'lossList': [0.0, -1.4415322786569595, 0.0, 16.189001621007918, 0.0, 0.0, 0.0], 'rewardMean': 0.7232415484091737, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 881.3145145035883
'totalSteps': 6400, 'rewardStep': 0.8827228131648993, 'errorList': [], 'lossList': [0.0, -1.4144709312915802, 0.0, 113.74500911712647, 0.0, 0.0, 0.0], 'rewardMean': 0.7551378013603187, 'totalEpisodes': 32, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.51122189426127
'totalSteps': 7680, 'rewardStep': 0.5606026586517342, 'errorList': [], 'lossList': [0.0, -1.4060918986797333, 0.0, 224.42188743591308, 0.0, 0.0, 0.0], 'rewardMean': 0.7227152775755546, 'totalEpisodes': 91, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.49604179782463
'totalSteps': 8960, 'rewardStep': 0.6136739565637315, 'errorList': [], 'lossList': [0.0, -1.4032588237524033, 0.0, 127.97074481964111, 0.0, 0.0, 0.0], 'rewardMean': 0.7071379460024371, 'totalEpisodes': 155, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.852578262662533
'totalSteps': 10240, 'rewardStep': 0.8500031903699974, 'errorList': [], 'lossList': [0.0, -1.3983300071954727, 0.0, 54.544046287536624, 0.0, 0.0, 0.0], 'rewardMean': 0.7249961015483821, 'totalEpisodes': 209, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.781976779846136
'totalSteps': 11520, 'rewardStep': 0.9239562580144587, 'errorList': [], 'lossList': [0.0, -1.3866285121440887, 0.0, 44.5273436832428, 0.0, 0.0, 0.0], 'rewardMean': 0.7471027856001684, 'totalEpisodes': 234, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.995266170377093
'totalSteps': 12800, 'rewardStep': 0.5795733040053437, 'errorList': [], 'lossList': [0.0, -1.3663400650024413, 0.0, 41.787664642333986, 0.0, 0.0, 0.0], 'rewardMean': 0.7303498374406858, 'totalEpisodes': 248, 'stepsPerEpisode': 67, 'rewardPerEpisode': 50.59107943471246
'totalSteps': 14080, 'rewardStep': 0.7726873186494209, 'errorList': [], 'lossList': [0.0, -1.3480014091730117, 0.0, 36.22262990474701, 0.0, 0.0, 0.0], 'rewardMean': 0.7352834769176979, 'totalEpisodes': 255, 'stepsPerEpisode': 106, 'rewardPerEpisode': 77.30781120636397
'totalSteps': 15360, 'rewardStep': 0.8106894488589245, 'errorList': [], 'lossList': [0.0, -1.3351545870304107, 0.0, 38.48602236747742, 0.0, 0.0, 0.0], 'rewardMean': 0.729021660612472, 'totalEpisodes': 262, 'stepsPerEpisode': 49, 'rewardPerEpisode': 40.5897813362753
'totalSteps': 16640, 'rewardStep': 0.38896829376940006, 'errorList': [], 'lossList': [0.0, -1.334707595705986, 0.0, 25.25880543231964, 0.0, 0.0, 0.0], 'rewardMean': 0.688375004741763, 'totalEpisodes': 269, 'stepsPerEpisode': 137, 'rewardPerEpisode': 99.01086765857009
'totalSteps': 17920, 'rewardStep': 0.46908849098824723, 'errorList': [], 'lossList': [0.0, -1.3292487519979477, 0.0, 30.130982298851013, 0.0, 0.0, 0.0], 'rewardMean': 0.6851965733036158, 'totalEpisodes': 277, 'stepsPerEpisode': 121, 'rewardPerEpisode': 94.99060380482948
'totalSteps': 19200, 'rewardStep': 0.86106635446736, 'errorList': [], 'lossList': [0.0, -1.3052905535697936, 0.0, 14.201278805732727, 0.0, 0.0, 0.0], 'rewardMean': 0.6830309274338617, 'totalEpisodes': 283, 'stepsPerEpisode': 84, 'rewardPerEpisode': 69.9655428115258
'totalSteps': 20480, 'rewardStep': 0.9795736457879347, 'errorList': [16.526730438580877, 9.573062893580378, 17.60376163567917, 8.219020484287654, 11.308411746390238, 11.896211832233584, 5.553938171200161, 9.702224707294294, 14.530986274028605, 21.518506202925888, 3.7631379299878325, 6.894356301318582, 8.425271182532176, 16.15275107875564, 3.507181216513912, 26.691085725422166, 0.35792625097032665, 18.660845950427454, 15.395082077694001, 15.460843360269068, 9.305992775214946, 1.1647702136611764, 22.224548658980968, 14.88227002614078, 21.284709352221967, 7.2245954816597635, 0.49460215131544033, 16.05964503379419, 18.827331057337414, 26.338616763382422, 13.824137888801676, 16.362551721180182, 29.610580280848403, 8.46611904023209, 11.201912208438147, 6.943500655192358, 14.859844644858535, 9.375872461519783, 13.292765986213775, 12.858246290103175, 15.707221825091429, 9.076949065509503, 5.6137574818725, 10.413259827245358, 21.872197953293618, 17.984100188130448, 11.640140513916002, 3.9869879185496644, 6.590380309441094, 18.219877432964275], 'lossList': [0.0, -1.2814438843727112, 0.0, 11.664198353290558, 0.0, 0.0, 0.0], 'rewardMean': 0.7249280261474819, 'totalEpisodes': 290, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.406521749496388, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8186752714331428, 'errorList': [], 'lossList': [0.0, -1.2614054715633392, 0.0, 5.133190684318542, 0.0, 0.0, 0.0], 'rewardMean': 0.7454281576344229, 'totalEpisodes': 296, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.93547452191571
'totalSteps': 23040, 'rewardStep': 0.7862355017460474, 'errorList': [], 'lossList': [0.0, -1.2462285989522934, 0.0, 10.894977017641068, 0.0, 0.0, 0.0], 'rewardMean': 0.739051388772028, 'totalEpisodes': 299, 'stepsPerEpisode': 81, 'rewardPerEpisode': 64.98784809305313
'totalSteps': 24320, 'rewardStep': 0.8116529638625002, 'errorList': [], 'lossList': [0.0, -1.2367798793315887, 0.0, 3.3339754486083986, 0.0, 0.0, 0.0], 'rewardMean': 0.7278210593568321, 'totalEpisodes': 304, 'stepsPerEpisode': 102, 'rewardPerEpisode': 92.67415633932907
'totalSteps': 25600, 'rewardStep': 0.5514067731654302, 'errorList': [], 'lossList': [0.0, -1.2395777755975723, 0.0, 3.1895654994249343, 0.0, 0.0, 0.0], 'rewardMean': 0.7250044062728409, 'totalEpisodes': 307, 'stepsPerEpisode': 264, 'rewardPerEpisode': 190.78075956838575
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.17
