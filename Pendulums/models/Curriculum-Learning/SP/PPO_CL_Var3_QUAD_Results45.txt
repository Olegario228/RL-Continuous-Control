#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 45
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9017034606688734, 'errorList': [], 'lossList': [0.0, -1.4372288572788239, 0.0, 31.117247829437257, 0.0, 0.0, 0.0], 'rewardMean': 0.9082620456248507, 'totalEpisodes': 8, 'stepsPerEpisode': 529, 'rewardPerEpisode': 377.6042513738456
'totalSteps': 3840, 'rewardStep': 0.7726347467335641, 'errorList': [], 'lossList': [0.0, -1.4214826941490173, 0.0, 31.641873145103453, 0.0, 0.0, 0.0], 'rewardMean': 0.8630529459944217, 'totalEpisodes': 12, 'stepsPerEpisode': 258, 'rewardPerEpisode': 201.06854539987327
'totalSteps': 5120, 'rewardStep': 0.6923864614491735, 'errorList': [], 'lossList': [0.0, -1.4174434727430343, 0.0, 26.269844735860826, 0.0, 0.0, 0.0], 'rewardMean': 0.8203863248581097, 'totalEpisodes': 13, 'stepsPerEpisode': 472, 'rewardPerEpisode': 358.3392935481151
'totalSteps': 6400, 'rewardStep': 0.9512581952151207, 'errorList': [78.55855960732478, 71.49753188484242, 82.4831454849441, 84.37130764216646, 76.05124785451565, 81.21014071238866, 80.98880882333631, 76.76370325740751, 60.862132136285375, 79.21594102412966, 64.90833917891732, 77.14792247722222, 77.4904472118606, 72.55176315101308, 78.19522035266786, 76.19200480975992, 78.56716609048777, 73.7954077109403, 80.37934914751499, 80.36104191656025, 67.98973150581908, 82.63191197595513, 82.55758963374954, 77.54168839964333, 76.53580433933313, 77.40591652633742, 80.80232730809635, 76.22337148196168, 78.11292568224394, 82.93408110692395, 83.2768898380988, 81.33370996687982, 72.27172736738612, 82.36756866088665, 75.60791067060515, 74.407607134468, 82.1747246654805, 76.80359747568309, 82.23648301815149, 84.01766375939614, 77.3895264217383, 76.35892015606606, 73.77696335575313, 74.51522246285836, 79.78732726099231, 81.54707392108996, 76.86172140388115, 70.29683445281897, 75.16718874817386, 75.97711135101206], 'lossList': [0.0, -1.4049807143211366, 0.0, 60.41779902458191, 0.0, 0.0, 0.0], 'rewardMean': 0.8465606989295118, 'totalEpisodes': 19, 'stepsPerEpisode': 208, 'rewardPerEpisode': 162.05626160481847, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.6061261319874809, 'errorList': [], 'lossList': [0.0, -1.40517914891243, 0.0, 254.03106983184816, 0.0, 0.0, 0.0], 'rewardMean': 0.80648827110584, 'totalEpisodes': 72, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.847651293275383
'totalSteps': 8960, 'rewardStep': 0.5524308545653526, 'errorList': [], 'lossList': [0.0, -1.3960029184818268, 0.0, 96.41281116485595, 0.0, 0.0, 0.0], 'rewardMean': 0.770194354457199, 'totalEpisodes': 128, 'stepsPerEpisode': 20, 'rewardPerEpisode': 10.054999770731479
'totalSteps': 10240, 'rewardStep': 0.8928213671158733, 'errorList': [], 'lossList': [0.0, -1.38476378262043, 0.0, 61.988950443267825, 0.0, 0.0, 0.0], 'rewardMean': 0.7855227310395333, 'totalEpisodes': 160, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.6655899365496785
'totalSteps': 11520, 'rewardStep': 0.5037405894317544, 'errorList': [], 'lossList': [0.0, -1.3662747544050218, 0.0, 40.17439914703369, 0.0, 0.0, 0.0], 'rewardMean': 0.7542136041942246, 'totalEpisodes': 174, 'stepsPerEpisode': 67, 'rewardPerEpisode': 55.00319769072175
'totalSteps': 12800, 'rewardStep': 0.9080806507073009, 'errorList': [], 'lossList': [0.0, -1.3376826459169389, 0.0, 29.99845880508423, 0.0, 0.0, 0.0], 'rewardMean': 0.7696003088455322, 'totalEpisodes': 184, 'stepsPerEpisode': 47, 'rewardPerEpisode': 41.12459379172382
'totalSteps': 14080, 'rewardStep': 0.6501033239629725, 'errorList': [], 'lossList': [0.0, -1.313009151816368, 0.0, 19.947120912075043, 0.0, 0.0, 0.0], 'rewardMean': 0.7431285781837466, 'totalEpisodes': 189, 'stepsPerEpisode': 212, 'rewardPerEpisode': 158.75950044594737
'totalSteps': 15360, 'rewardStep': 0.30845036544401716, 'errorList': [], 'lossList': [0.0, -1.2907155352830886, 0.0, 16.703083245754243, 0.0, 0.0, 0.0], 'rewardMean': 0.683803268661261, 'totalEpisodes': 196, 'stepsPerEpisode': 185, 'rewardPerEpisode': 138.05553688376492
'totalSteps': 16640, 'rewardStep': 0.8263237164063378, 'errorList': [], 'lossList': [0.0, -1.2634202986955643, 0.0, 15.355163729190826, 0.0, 0.0, 0.0], 'rewardMean': 0.6891721656285383, 'totalEpisodes': 203, 'stepsPerEpisode': 56, 'rewardPerEpisode': 41.46976845755217
'totalSteps': 17920, 'rewardStep': 0.9111668914241943, 'errorList': [], 'lossList': [0.0, -1.251070435643196, 0.0, 6.727850530147553, 0.0, 0.0, 0.0], 'rewardMean': 0.7110502086260404, 'totalEpisodes': 206, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.746230543754155
'totalSteps': 19200, 'rewardStep': 0.7128453992557328, 'errorList': [], 'lossList': [0.0, -1.2462527447938918, 0.0, 16.392371994256973, 0.0, 0.0, 0.0], 'rewardMean': 0.6872089290301017, 'totalEpisodes': 209, 'stepsPerEpisode': 428, 'rewardPerEpisode': 364.0832955182822
'totalSteps': 20480, 'rewardStep': 0.8151396443827553, 'errorList': [], 'lossList': [0.0, -1.2587818044424057, 0.0, 4.515452318191528, 0.0, 0.0, 0.0], 'rewardMean': 0.7081102802696292, 'totalEpisodes': 212, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.985605822405105
'totalSteps': 21760, 'rewardStep': 0.8764853242743639, 'errorList': [], 'lossList': [0.0, -1.2667502915859223, 0.0, 3.129628519117832, 0.0, 0.0, 0.0], 'rewardMean': 0.7405157272405302, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.3518689789335
'totalSteps': 23040, 'rewardStep': 0.7373494281750901, 'errorList': [], 'lossList': [0.0, -1.2428888386487962, 0.0, 2.1645321275293825, 0.0, 0.0, 0.0], 'rewardMean': 0.7249685333464518, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.976593311377
'totalSteps': 24320, 'rewardStep': 0.7986711046551189, 'errorList': [], 'lossList': [0.0, -1.208987484574318, 0.0, 1.9488387422263622, 0.0, 0.0, 0.0], 'rewardMean': 0.7544615848687883, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.7626381625878
'totalSteps': 25600, 'rewardStep': 0.9270347556485775, 'errorList': [], 'lossList': [0.0, -1.1527306300401687, 0.0, 1.5325320316851139, 0.0, 0.0, 0.0], 'rewardMean': 0.7563569953629161, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1178.731608877443
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.48
