#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 46
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8196083962831074, 'errorList': [], 'lossList': [0.0, -1.4501610457897187, 0.0, 38.77483886241913, 0.0, 0.0, 0.0], 'rewardMean': 0.7063931079316335, 'totalEpisodes': 12, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.40007676129911
'totalSteps': 3840, 'rewardStep': 0.8971215392622387, 'errorList': [], 'lossList': [0.0, -1.4657599878311158, 0.0, 30.58737414062023, 0.0, 0.0, 0.0], 'rewardMean': 0.7699692517085018, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1006.1488129298762
'totalSteps': 5120, 'rewardStep': 0.5651619995233738, 'errorList': [], 'lossList': [0.0, -1.4460194778442383, 0.0, 24.912892896533013, 0.0, 0.0, 0.0], 'rewardMean': 0.7187674386622198, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1000.895485310661
'totalSteps': 6400, 'rewardStep': 0.6001849844536272, 'errorList': [], 'lossList': [0.0, -1.4337127423286438, 0.0, 46.24680958032608, 0.0, 0.0, 0.0], 'rewardMean': 0.6950509478205013, 'totalEpisodes': 15, 'stepsPerEpisode': 84, 'rewardPerEpisode': 68.02623003256105
'totalSteps': 7680, 'rewardStep': 0.816323153047425, 'errorList': [], 'lossList': [0.0, -1.411267222762108, 0.0, 313.9417085266113, 0.0, 0.0, 0.0], 'rewardMean': 0.7152629820249885, 'totalEpisodes': 64, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.246106846675634
'totalSteps': 8960, 'rewardStep': 0.7014820874893877, 'errorList': [], 'lossList': [0.0, -1.412132608294487, 0.0, 103.29608974456787, 0.0, 0.0, 0.0], 'rewardMean': 0.7132942828056169, 'totalEpisodes': 105, 'stepsPerEpisode': 24, 'rewardPerEpisode': 16.40749803137849
'totalSteps': 10240, 'rewardStep': 0.8404956568665801, 'errorList': [], 'lossList': [0.0, -1.404358160495758, 0.0, 74.73334735870361, 0.0, 0.0, 0.0], 'rewardMean': 0.7291944545632374, 'totalEpisodes': 132, 'stepsPerEpisode': 27, 'rewardPerEpisode': 24.60417285172397
'totalSteps': 11520, 'rewardStep': 0.49899087756965044, 'errorList': [], 'lossList': [0.0, -1.3977122443914414, 0.0, 41.50093234062195, 0.0, 0.0, 0.0], 'rewardMean': 0.7036162793417278, 'totalEpisodes': 145, 'stepsPerEpisode': 85, 'rewardPerEpisode': 63.307963823473976
'totalSteps': 12800, 'rewardStep': 0.9337513800960541, 'errorList': [32.71778883656473, 25.36455410607403, 76.94020978508152, 81.67411586802133, 25.91129310660156, 72.07806561192442, 45.656163890975975, 13.294752644259686, 66.21822765365016, 51.58083169064162, 79.65943719007767, 66.84547269509086, 68.30276381297023, 68.03357694728147, 27.525047049324275, 21.5880033715074, 25.389669873668538, 24.074014654775155, 73.89988273647909, 37.554919228999914, 69.25072804835081, 48.20041825935811, 99.77825331815336, 99.89330589519805, 3.844870496689894, 27.34153602650871, 29.046783614899432, 91.1397316816532, 73.29341012714102, 64.54455455780075, 36.363155996031445, 27.276190456767004, 74.67485987924594, 48.2404157579461, 47.10080359168028, 43.81124803940612, 4.152471735897843, 42.20540935999335, 39.78816676780715, 73.51855757954631, 78.51899200606533, 83.39786331428104, 33.49618029616944, 87.2407493969663, 68.44205121501113, 99.67889880411035, 54.3521924321293, 0.39012894960202327, 70.45490045063309, 9.066948320909653], 'lossList': [0.0, -1.3815186327695848, 0.0, 58.70752998352051, 0.0, 0.0, 0.0], 'rewardMean': 0.7266297894171604, 'totalEpisodes': 162, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.779702367861457, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.41052423567142315, 'errorList': [], 'lossList': [0.0, -1.3651412814855575, 0.0, 22.53407837152481, 0.0, 0.0, 0.0], 'rewardMean': 0.7083644310262868, 'totalEpisodes': 168, 'stepsPerEpisode': 126, 'rewardPerEpisode': 80.32526212089147
'totalSteps': 15360, 'rewardStep': 0.5096866368806292, 'errorList': [], 'lossList': [0.0, -1.3531029200553895, 0.0, 9.88239631652832, 0.0, 0.0, 0.0], 'rewardMean': 0.6773722550860389, 'totalEpisodes': 172, 'stepsPerEpisode': 160, 'rewardPerEpisode': 111.90556252295904
'totalSteps': 16640, 'rewardStep': 0.7548280762333133, 'errorList': [], 'lossList': [0.0, -1.3132680350542068, 0.0, 9.40203894853592, 0.0, 0.0, 0.0], 'rewardMean': 0.6631429087831464, 'totalEpisodes': 180, 'stepsPerEpisode': 121, 'rewardPerEpisode': 97.20477143504799
'totalSteps': 17920, 'rewardStep': 0.8186223220763201, 'errorList': [], 'lossList': [0.0, -1.293269885778427, 0.0, 14.06572619497776, 0.0, 0.0, 0.0], 'rewardMean': 0.6884889410384409, 'totalEpisodes': 182, 'stepsPerEpisode': 926, 'rewardPerEpisode': 755.3566399989157
'totalSteps': 19200, 'rewardStep': 0.7390984205474792, 'errorList': [], 'lossList': [0.0, -1.3013987338542938, 0.0, 4.755839769244194, 0.0, 0.0, 0.0], 'rewardMean': 0.7023802846478262, 'totalEpisodes': 186, 'stepsPerEpisode': 149, 'rewardPerEpisode': 117.97527571122609
'totalSteps': 20480, 'rewardStep': 0.7849720620483599, 'errorList': [], 'lossList': [0.0, -1.2951085674762726, 0.0, 4.36268568366766, 0.0, 0.0, 0.0], 'rewardMean': 0.6992451755479197, 'totalEpisodes': 188, 'stepsPerEpisode': 156, 'rewardPerEpisode': 136.33885667214244
'totalSteps': 21760, 'rewardStep': 0.4789837717766001, 'errorList': [], 'lossList': [0.0, -1.2755725640058517, 0.0, 25.920825488567353, 0.0, 0.0, 0.0], 'rewardMean': 0.676995343976641, 'totalEpisodes': 189, 'stepsPerEpisode': 820, 'rewardPerEpisode': 586.9536796010416
'totalSteps': 23040, 'rewardStep': 0.7624772561232172, 'errorList': [], 'lossList': [0.0, -1.2381604355573654, 0.0, 2.5637698274850846, 0.0, 0.0, 0.0], 'rewardMean': 0.6691935039023047, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 927.4302228421644
'totalSteps': 24320, 'rewardStep': 0.7772906814159002, 'errorList': [], 'lossList': [0.0, -1.2262393534183502, 0.0, 2.698452618420124, 0.0, 0.0, 0.0], 'rewardMean': 0.6970234842869296, 'totalEpisodes': 191, 'stepsPerEpisode': 193, 'rewardPerEpisode': 171.73654532751715
'totalSteps': 25600, 'rewardStep': 0.8676297855807138, 'errorList': [], 'lossList': [0.0, -1.2007082879543305, 0.0, 1.5015074789524079, 0.0, 0.0, 0.0], 'rewardMean': 0.6904113248353957, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.818071197918
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.92
