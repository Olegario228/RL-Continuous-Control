#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 47
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8665393003045015, 'errorList': [], 'lossList': [0.0, -1.4422493171691895, 0.0, 29.31101459622383, 0.0, 0.0, 0.0], 'rewardMean': 0.6848200741042567, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 667.5919646394857
'totalSteps': 3840, 'rewardStep': 0.834940955376786, 'errorList': [], 'lossList': [0.0, -1.4614481377601622, 0.0, 42.542749230861666, 0.0, 0.0, 0.0], 'rewardMean': 0.7348603678617666, 'totalEpisodes': 12, 'stepsPerEpisode': 667, 'rewardPerEpisode': 516.4178173471076
'totalSteps': 5120, 'rewardStep': 0.845377429241248, 'errorList': [], 'lossList': [0.0, -1.461582155227661, 0.0, 21.526002287864685, 0.0, 0.0, 0.0], 'rewardMean': 0.7624896332066369, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 945.6105298745986
'totalSteps': 6400, 'rewardStep': 0.5965852072576946, 'errorList': [], 'lossList': [0.0, -1.430266553759575, 0.0, 105.916875, 0.0, 0.0, 0.0], 'rewardMean': 0.7293087480168484, 'totalEpisodes': 25, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.6769823394890695
'totalSteps': 7680, 'rewardStep': 0.5579383809709698, 'errorList': [], 'lossList': [0.0, -1.4250291109085083, 0.0, 228.7942848587036, 0.0, 0.0, 0.0], 'rewardMean': 0.7007470201758687, 'totalEpisodes': 95, 'stepsPerEpisode': 18, 'rewardPerEpisode': 11.666202999012365
'totalSteps': 8960, 'rewardStep': 0.8287045911891936, 'errorList': [], 'lossList': [0.0, -1.4167408615350723, 0.0, 77.20714611053467, 0.0, 0.0, 0.0], 'rewardMean': 0.7190266731777722, 'totalEpisodes': 136, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.924830127195467
'totalSteps': 10240, 'rewardStep': 0.6950000126457522, 'errorList': [], 'lossList': [0.0, -1.4143270361423492, 0.0, 66.68485263824464, 0.0, 0.0, 0.0], 'rewardMean': 0.7160233406112697, 'totalEpisodes': 170, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.947603116694975
'totalSteps': 11520, 'rewardStep': 0.46814794921558295, 'errorList': [], 'lossList': [0.0, -1.419618495106697, 0.0, 61.98429039001465, 0.0, 0.0, 0.0], 'rewardMean': 0.6884816304561934, 'totalEpisodes': 188, 'stepsPerEpisode': 16, 'rewardPerEpisode': 7.9438146420492615
'totalSteps': 12800, 'rewardStep': 0.7685860873779343, 'errorList': [], 'lossList': [0.0, -1.4142941451072693, 0.0, 36.79303902149201, 0.0, 0.0, 0.0], 'rewardMean': 0.6964920761483675, 'totalEpisodes': 200, 'stepsPerEpisode': 67, 'rewardPerEpisode': 55.05397953190322
'totalSteps': 14080, 'rewardStep': 0.7310886627661493, 'errorList': [], 'lossList': [0.0, -1.4046151447296142, 0.0, 18.828841633796692, 0.0, 0.0, 0.0], 'rewardMean': 0.7192908576345812, 'totalEpisodes': 208, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.85162459194929
'totalSteps': 15360, 'rewardStep': 0.46345147427132355, 'errorList': [], 'lossList': [0.0, -1.4028398764133454, 0.0, 13.213264698982238, 0.0, 0.0, 0.0], 'rewardMean': 0.6789820750312634, 'totalEpisodes': 215, 'stepsPerEpisode': 204, 'rewardPerEpisode': 155.88948090750696
'totalSteps': 16640, 'rewardStep': 0.9585968025841154, 'errorList': [28.61639202595605, 183.2273588285703, 90.94310815902118, 24.26741040166838, 48.37247951793453, 143.82448365385164, 86.95540433954088, 30.217744104522236, 119.63358770218704, 124.1827416773957, 107.2060526336835, 68.93788779617185, 95.96100081892787, 195.57482654075696, 99.91442491208372, 167.29498603753555, 177.79972648754696, 30.7674277413272, 18.65564134670782, 117.57962278310437, 39.247235939249265, 59.3315505213572, 69.58110273340826, 165.13957256894125, 85.39174488868889, 131.38953144404198, 184.69496274435792, 143.16690897436723, 125.99115545363047, 59.18916645932188, 39.89307915483425, 64.47596838685931, 96.83316041723646, 102.01383054310274, 53.643877649503786, 113.12050400636053, 107.84940919743529, 152.43432541370814, 151.9848260204318, 51.90744981472049, 66.97807956241277, 174.73560844226276, 140.19254018431002, 102.38320406242589, 170.0620505087148, 105.93134351205418, 19.563705902961914, 95.93684841169886, 151.82681227043017, 122.7578408452382], 'lossList': [0.0, -1.4060351705551148, 0.0, 10.64276176214218, 0.0, 0.0, 0.0], 'rewardMean': 0.6913476597519963, 'totalEpisodes': 221, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.016185414499233, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7116841150949844, 'errorList': [], 'lossList': [0.0, -1.3952170866727829, 0.0, 6.413010519742966, 0.0, 0.0, 0.0], 'rewardMean': 0.67797832833737, 'totalEpisodes': 228, 'stepsPerEpisode': 48, 'rewardPerEpisode': 39.4275354013973
'totalSteps': 19200, 'rewardStep': 0.16087550801266143, 'errorList': [], 'lossList': [0.0, -1.3505751121044158, 0.0, 12.649473958015442, 0.0, 0.0, 0.0], 'rewardMean': 0.6344073584128667, 'totalEpisodes': 234, 'stepsPerEpisode': 270, 'rewardPerEpisode': 210.67044917869208
'totalSteps': 20480, 'rewardStep': 0.7014478467024516, 'errorList': [], 'lossList': [0.0, -1.314621503353119, 0.0, 5.4990895372629165, 0.0, 0.0, 0.0], 'rewardMean': 0.6487583049860148, 'totalEpisodes': 238, 'stepsPerEpisode': 410, 'rewardPerEpisode': 361.01431046888536
'totalSteps': 21760, 'rewardStep': 0.7237342633745129, 'errorList': [], 'lossList': [0.0, -1.3100708383321762, 0.0, 4.387742965519428, 0.0, 0.0, 0.0], 'rewardMean': 0.6382612722045468, 'totalEpisodes': 240, 'stepsPerEpisode': 357, 'rewardPerEpisode': 298.3443863035181
'totalSteps': 23040, 'rewardStep': 0.13021183401507586, 'errorList': [], 'lossList': [0.0, -1.3000521832704544, 0.0, 3.148845906853676, 0.0, 0.0, 0.0], 'rewardMean': 0.5817824543414792, 'totalEpisodes': 241, 'stepsPerEpisode': 1010, 'rewardPerEpisode': 825.8252461293235
'totalSteps': 24320, 'rewardStep': 0.957833878717068, 'errorList': [0.04792157924965554, 0.04239447688282379, 0.04850546559363212, 0.04263222483815475, 0.04152201505583651, 0.04648245225715979, 0.041316487789697945, 0.04210352966219179, 0.04109921483703503, 0.04281352679757732, 0.042547035902695055, 0.04364314211694533, 0.04142586562360941, 0.046634309683879306, 0.04251467672058106, 0.041014467279610875, 0.04893733274912113, 0.06069192937217903, 0.042299821724199785, 0.04628530556243523, 0.04243235496250391, 0.04228152108571539, 0.04150931433226876, 0.06004385549591068, 0.05679523907001903, 0.05995842448931612, 0.06354771095604919, 0.04106763297541488, 0.04239727262520648, 0.04093315046815742, 0.04243952534774055, 0.040799214923338656, 0.04329619321608114, 0.04514864877737967, 0.04039849894697833, 0.04158203755161234, 0.04444335982193693, 0.04178192908514857, 0.0642999879061844, 0.040653273533438145, 0.04512513716309278, 0.042893663390223886, 0.042898007700269296, 0.040969923488753314, 0.06005675230173304, 0.042570156190230524, 0.04222063334673602, 0.045052631005021944, 0.040400150155508394, 0.061059026159460406], 'lossList': [0.0, -1.2689481264352798, 0.0, 1.671240180656314, 0.0, 0.0, 0.0], 'rewardMean': 0.6307510472916277, 'totalEpisodes': 242, 'stepsPerEpisode': 1278, 'rewardPerEpisode': 1150.6265829668992, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=101.89
