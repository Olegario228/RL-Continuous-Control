#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 48
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5399419098901257, 'errorList': [], 'lossList': [0.0, -1.4219946801662444, 0.0, 31.908417702913283, 0.0, 0.0, 0.0], 'rewardMean': 0.719436162858433, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 32.00524476522591
'totalSteps': 3840, 'rewardStep': 0.952313692043573, 'errorList': [], 'lossList': [0.0, -1.4146406292915343, 0.0, 32.42353110432625, 0.0, 0.0, 0.0], 'rewardMean': 0.7970620059201464, 'totalEpisodes': 18, 'stepsPerEpisode': 489, 'rewardPerEpisode': 394.5104963401561
'totalSteps': 5120, 'rewardStep': 0.7832897679383878, 'errorList': [], 'lossList': [0.0, -1.4017951810359954, 0.0, 24.95351285278797, 0.0, 0.0, 0.0], 'rewardMean': 0.7936189464247068, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.7394440997846
'totalSteps': 6400, 'rewardStep': 0.6944634946543826, 'errorList': [], 'lossList': [0.0, -1.3819014179706572, 0.0, 89.67444597244263, 0.0, 0.0, 0.0], 'rewardMean': 0.7737878560706419, 'totalEpisodes': 27, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.37035699291814
'totalSteps': 7680, 'rewardStep': 0.5108601859542483, 'errorList': [], 'lossList': [0.0, -1.3685576021671295, 0.0, 259.0985009765625, 0.0, 0.0, 0.0], 'rewardMean': 0.7299665777179096, 'totalEpisodes': 80, 'stepsPerEpisode': 41, 'rewardPerEpisode': 32.2931686927679
'totalSteps': 8960, 'rewardStep': 0.7867435739285311, 'errorList': [], 'lossList': [0.0, -1.366316003203392, 0.0, 86.82242460250855, 0.0, 0.0, 0.0], 'rewardMean': 0.7380775771765699, 'totalEpisodes': 131, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.435861966041324
'totalSteps': 10240, 'rewardStep': 0.7348019695831571, 'errorList': [], 'lossList': [0.0, -1.3634132581949234, 0.0, 51.0296392250061, 0.0, 0.0, 0.0], 'rewardMean': 0.7376681262273933, 'totalEpisodes': 168, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.868132838648025
'totalSteps': 11520, 'rewardStep': 0.5252155252553067, 'errorList': [], 'lossList': [0.0, -1.3560195183753967, 0.0, 30.166501960754395, 0.0, 0.0, 0.0], 'rewardMean': 0.7140622816749392, 'totalEpisodes': 199, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.309982104419428
'totalSteps': 12800, 'rewardStep': 0.45717163225844437, 'errorList': [], 'lossList': [0.0, -1.3472741371393204, 0.0, 26.13172287940979, 0.0, 0.0, 0.0], 'rewardMean': 0.6883732167332898, 'totalEpisodes': 215, 'stepsPerEpisode': 6, 'rewardPerEpisode': 2.7953079692093104
'totalSteps': 14080, 'rewardStep': 0.8313430611657099, 'errorList': [], 'lossList': [0.0, -1.3356332278251648, 0.0, 23.82302137851715, 0.0, 0.0, 0.0], 'rewardMean': 0.6816144812671867, 'totalEpisodes': 221, 'stepsPerEpisode': 70, 'rewardPerEpisode': 60.0675494933489
'totalSteps': 15360, 'rewardStep': 0.822196672580765, 'errorList': [], 'lossList': [0.0, -1.3248195058107377, 0.0, 13.932839069366455, 0.0, 0.0, 0.0], 'rewardMean': 0.7098399575362506, 'totalEpisodes': 227, 'stepsPerEpisode': 83, 'rewardPerEpisode': 74.10839014634523
'totalSteps': 16640, 'rewardStep': 0.36887003595976753, 'errorList': [], 'lossList': [0.0, -1.3029297006130218, 0.0, 7.3819534206390385, 0.0, 0.0, 0.0], 'rewardMean': 0.6514955919278701, 'totalEpisodes': 231, 'stepsPerEpisode': 175, 'rewardPerEpisode': 130.19231120172856
'totalSteps': 17920, 'rewardStep': 0.9300808354482769, 'errorList': [0.921291784402652, 8.69706790104395, 6.60088319335507, 9.215955044968142, 0.4454332430145427, 2.5841192788655496, 1.2012062294339225, 0.5855192594475314, 0.31130967285128097, 1.133706414470401, 1.657381627579972, 3.056266366374913, 4.903345505975027, 0.14668707178720586, 0.4926658858547201, 16.98806267350428, 2.482661773648561, 1.3194450208950017, 5.146873393056085, 0.8388634831678555, 8.517089234427717, 0.5442069471559314, 4.111771436487322, 13.302737111552425, 11.130370752223767, 6.507362247528911, 0.6408912549464872, 8.499077563119457, 9.761478282504315, 0.9926204537854951, 10.729186906003525, 10.466216513845193, 0.5703148964071493, 0.17740120516647967, 2.1960503750297793, 34.40656767067732, 0.17491595207562863, 0.5748171067495116, 1.6779602784460361, 8.287309721384354, 25.841174796584, 9.618970285091073, 2.9495287113761597, 1.0854779470878184, 2.199502995237729, 9.107247866237623, 0.5094719847665491, 1.302212018214175, 3.6111894366198203, 1.9536806462678145], 'lossList': [0.0, -1.2935874778032304, 0.0, 7.913933587670326, 0.0, 0.0, 0.0], 'rewardMean': 0.666174698678859, 'totalEpisodes': 235, 'stepsPerEpisode': 389, 'rewardPerEpisode': 348.80105817545444, 'successfulTests': 3
'totalSteps': 19200, 'rewardStep': 0.5888464364260173, 'errorList': [], 'lossList': [0.0, -1.297922304868698, 0.0, 5.977693932056427, 0.0, 0.0, 0.0], 'rewardMean': 0.6556129928560225, 'totalEpisodes': 237, 'stepsPerEpisode': 244, 'rewardPerEpisode': 194.5628737045354
'totalSteps': 20480, 'rewardStep': 0.8916594212318201, 'errorList': [], 'lossList': [0.0, -1.2883098435401916, 0.0, 3.856209804713726, 0.0, 0.0, 0.0], 'rewardMean': 0.6936929163837796, 'totalEpisodes': 240, 'stepsPerEpisode': 216, 'rewardPerEpisode': 196.64746298299116
'totalSteps': 21760, 'rewardStep': 0.9385604504679599, 'errorList': [0.43247603740604684, 0.25341350215407654, 0.31104200028871226, 0.8529022368321404, 42.81727519465854, 15.196962605748583, 0.7319567430476266, 0.5438022136169269, 3.783823338055365, 0.2610568442382233, 0.8567911239652451, 0.25079009092626614, 0.3186148093574225, 0.763093105024312, 0.21829769183597375, 0.3174036117962332, 0.35184370827308314, 24.95213790299144, 0.26504558568658393, 0.42496554185359453, 20.17061235619767, 0.22128174198877587, 0.25960675363627317, 20.47660656300104, 0.38322209135356705, 0.2010579843627775, 0.1938316574234345, 6.757980050501717, 0.5592156148612544, 0.20992654890413026, 0.22423128974578055, 100.69251954362939, 2.5579076738138204, 0.2818588788976507, 0.33894508985510424, 0.301874395732467, 0.2989918100301418, 0.35490956957995795, 3.095730817764987, 0.217499921545456, 0.42314626816402606, 0.2672952824432236, 0.23927487065919703, 116.52761812015015, 0.23092668862571344, 0.23889185474627053, 0.27119665649769326, 22.517328200068732, 0.5507990342270243, 0.2699852444644658], 'lossList': [0.0, -1.2851056623458863, 0.0, 4.7707080060243605, 0.0, 0.0, 0.0], 'rewardMean': 0.7088746040377225, 'totalEpisodes': 242, 'stepsPerEpisode': 382, 'rewardPerEpisode': 340.7001733847602, 'successfulTests': 1
'totalSteps': 23040, 'rewardStep': 0.5117235262520239, 'errorList': [], 'lossList': [0.0, -1.298025124669075, 0.0, 2.686521174311638, 0.0, 0.0, 0.0], 'rewardMean': 0.6865667597046092, 'totalEpisodes': 245, 'stepsPerEpisode': 552, 'rewardPerEpisode': 464.8640716700451
'totalSteps': 24320, 'rewardStep': 0.7831315112367793, 'errorList': [], 'lossList': [0.0, -1.2732347786426543, 0.0, 1.8739022627472877, 0.0, 0.0, 0.0], 'rewardMean': 0.7123583583027564, 'totalEpisodes': 246, 'stepsPerEpisode': 1257, 'rewardPerEpisode': 1077.123407935921
'totalSteps': 25600, 'rewardStep': 0.9275296167033023, 'errorList': [], 'lossList': [0.0, -1.2378734076023101, 0.0, 1.2020709121972322, 0.0, 0.0, 0.0], 'rewardMean': 0.7593941567472422, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1140.0504752539725
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=17920, timeSpent=104.21
