#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 49
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.891684713006701, 'errorList': [], 'lossList': [0.0, -1.4103160661458969, 0.0, 35.857446759939194, 0.0, 0.0, 0.0], 'rewardMean': 0.8204877886529911, 'totalEpisodes': 13, 'stepsPerEpisode': 303, 'rewardPerEpisode': 255.39610328027229
'totalSteps': 3840, 'rewardStep': 0.764721966157415, 'errorList': [], 'lossList': [0.0, -1.3969255584478377, 0.0, 32.33547274589539, 0.0, 0.0, 0.0], 'rewardMean': 0.8018991811544657, 'totalEpisodes': 16, 'stepsPerEpisode': 165, 'rewardPerEpisode': 128.219138047209
'totalSteps': 5120, 'rewardStep': 0.5791298991682037, 'errorList': [], 'lossList': [0.0, -1.3963031679391862, 0.0, 17.195506180524827, 0.0, 0.0, 0.0], 'rewardMean': 0.7462068606579002, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 898.000491809959
'totalSteps': 6400, 'rewardStep': 0.8550156407056727, 'errorList': [], 'lossList': [0.0, -1.381338171362877, 0.0, 110.05030683517457, 0.0, 0.0, 0.0], 'rewardMean': 0.7679686166674548, 'totalEpisodes': 31, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.345841718669647
'totalSteps': 7680, 'rewardStep': 0.8026971038740236, 'errorList': [], 'lossList': [0.0, -1.3811092990636826, 0.0, 242.63466606140136, 0.0, 0.0, 0.0], 'rewardMean': 0.7737566978685496, 'totalEpisodes': 96, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.2771651732086635
'totalSteps': 8960, 'rewardStep': 0.4019515428502176, 'errorList': [], 'lossList': [0.0, -1.378817389011383, 0.0, 134.00129413604736, 0.0, 0.0, 0.0], 'rewardMean': 0.7206416757230736, 'totalEpisodes': 161, 'stepsPerEpisode': 26, 'rewardPerEpisode': 15.429430302508532
'totalSteps': 10240, 'rewardStep': 0.9756133534168382, 'errorList': [213.29631807669784, 206.84591511847063, 197.9379622187407, 206.91568651567485, 132.1720201577667, 196.58464883730477, 177.44052082668756, 126.31344883822243, 160.95185110722917, 211.40157860891816, 205.95974532103077, 193.9444349042921, 179.09061726336358, 208.00711512492904, 212.81010457698105, 107.24229514267434, 211.36169429124553, 187.0494464399752, 185.33385083583354, 195.63040692556336, 172.53850991202, 214.65752253154486, 223.69434904299217, 179.40430245804347, 217.23832529914532, 207.84463221995549, 81.9607913711987, 194.7646342136845, 111.16521672132852, 167.12430978980981, 172.39512014612657, 139.5169220694201, 224.35646877614684, 191.85208879673957, 218.64539665305222, 184.63953968534406, 144.42726936637854, 211.94547881899686, 217.29312009615288, 186.46877838250222, 198.64902169182844, 52.45884476894451, 187.7723486979715, 226.2690486406873, 153.4601942497658, 227.05842195700578, 222.62425557768807, 210.08085378373764, 196.98370930355256, 208.1612331094502], 'lossList': [0.0, -1.3683855885267258, 0.0, 62.571064453125, 0.0, 0.0, 0.0], 'rewardMean': 0.752513135434794, 'totalEpisodes': 211, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.55803113550349, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9291075276928258, 'errorList': [], 'lossList': [0.0, -1.362245079278946, 0.0, 58.49773706436157, 0.0, 0.0, 0.0], 'rewardMean': 0.7721347345745753, 'totalEpisodes': 248, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.969610438949862
'totalSteps': 12800, 'rewardStep': 0.5390010364936448, 'errorList': [], 'lossList': [0.0, -1.3608469611406326, 0.0, 45.65697672843933, 0.0, 0.0, 0.0], 'rewardMean': 0.7488213647664823, 'totalEpisodes': 261, 'stepsPerEpisode': 64, 'rewardPerEpisode': 44.07911355582228
'totalSteps': 14080, 'rewardStep': 0.9269376424854627, 'errorList': [], 'lossList': [0.0, -1.3522927165031433, 0.0, 38.278712100982666, 0.0, 0.0, 0.0], 'rewardMean': 0.7665860425851004, 'totalEpisodes': 270, 'stepsPerEpisode': 99, 'rewardPerEpisode': 80.68957610572366
'totalSteps': 15360, 'rewardStep': 0.8084098183555016, 'errorList': [], 'lossList': [0.0, -1.3423802584409714, 0.0, 49.147937536239624, 0.0, 0.0, 0.0], 'rewardMean': 0.7582585531199806, 'totalEpisodes': 279, 'stepsPerEpisode': 155, 'rewardPerEpisode': 135.84766621506108
'totalSteps': 16640, 'rewardStep': 0.48194420073760347, 'errorList': [], 'lossList': [0.0, -1.3168194198608398, 0.0, 38.58985934734344, 0.0, 0.0, 0.0], 'rewardMean': 0.7299807765779994, 'totalEpisodes': 285, 'stepsPerEpisode': 239, 'rewardPerEpisode': 195.28172083974673
'totalSteps': 17920, 'rewardStep': 0.7943485866319506, 'errorList': [], 'lossList': [0.0, -1.2935919326543808, 0.0, 17.597017464637755, 0.0, 0.0, 0.0], 'rewardMean': 0.7515026453243742, 'totalEpisodes': 290, 'stepsPerEpisode': 89, 'rewardPerEpisode': 69.58175806806803
'totalSteps': 19200, 'rewardStep': 0.716930867330234, 'errorList': [], 'lossList': [0.0, -1.291097465157509, 0.0, 11.88445584654808, 0.0, 0.0, 0.0], 'rewardMean': 0.7376941679868303, 'totalEpisodes': 295, 'stepsPerEpisode': 75, 'rewardPerEpisode': 59.01790143884079
'totalSteps': 20480, 'rewardStep': 0.7797555074325224, 'errorList': [], 'lossList': [0.0, -1.3065927439928056, 0.0, 6.099615366458893, 0.0, 0.0, 0.0], 'rewardMean': 0.7354000083426802, 'totalEpisodes': 300, 'stepsPerEpisode': 224, 'rewardPerEpisode': 185.62776456926284
'totalSteps': 21760, 'rewardStep': 0.7950866320429655, 'errorList': [], 'lossList': [0.0, -1.3106104469299316, 0.0, 6.238265070915222, 0.0, 0.0, 0.0], 'rewardMean': 0.7747135172619549, 'totalEpisodes': 304, 'stepsPerEpisode': 235, 'rewardPerEpisode': 203.14894277724107
'totalSteps': 23040, 'rewardStep': 0.8579168296528679, 'errorList': [], 'lossList': [0.0, -1.3028444474935532, 0.0, 5.809573430418968, 0.0, 0.0, 0.0], 'rewardMean': 0.7629438648855579, 'totalEpisodes': 307, 'stepsPerEpisode': 61, 'rewardPerEpisode': 53.19751107880674
'totalSteps': 24320, 'rewardStep': 0.5562070892217775, 'errorList': [], 'lossList': [0.0, -1.3015500384569167, 0.0, 4.432278883457184, 0.0, 0.0, 0.0], 'rewardMean': 0.725653821038453, 'totalEpisodes': 309, 'stepsPerEpisode': 170, 'rewardPerEpisode': 124.68801395782549
'totalSteps': 25600, 'rewardStep': 0.5432789954730242, 'errorList': [], 'lossList': [0.0, -1.321059422492981, 0.0, 3.539133283495903, 0.0, 0.0, 0.0], 'rewardMean': 0.726081616936391, 'totalEpisodes': 311, 'stepsPerEpisode': 596, 'rewardPerEpisode': 461.7373979876984
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.38
