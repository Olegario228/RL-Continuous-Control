#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 5
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.9045543692635666, 'errorList': [], 'lossList': [0.0, -1.390825167298317, 0.0, 29.100388922691344, 0.0, 0.0, 0.0], 'rewardMean': 0.7576122341027085, 'totalEpisodes': 18, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.30781177425368
'totalSteps': 3840, 'rewardStep': 0.531473265358778, 'errorList': [], 'lossList': [0.0, -1.3722243720293046, 0.0, 31.490766978263856, 0.0, 0.0, 0.0], 'rewardMean': 0.6822325778547317, 'totalEpisodes': 30, 'stepsPerEpisode': 154, 'rewardPerEpisode': 121.86386937557874
'totalSteps': 5120, 'rewardStep': 0.47342813326807126, 'errorList': [], 'lossList': [0.0, -1.3654039895534515, 0.0, 41.73625512123108, 0.0, 0.0, 0.0], 'rewardMean': 0.6300314667080666, 'totalEpisodes': 39, 'stepsPerEpisode': 191, 'rewardPerEpisode': 116.54736592554954
'totalSteps': 6400, 'rewardStep': 0.9536550086080791, 'errorList': [245.1925970872245, 244.98583582955035, 245.96564164195215, 229.0563307215998, 278.5569256095699, 277.2642348169592, 268.9176856893849, 253.70019918030408, 197.96603408367392, 242.74789017375028, 201.25387547724912, 235.59456314410212, 219.27945811313782, 201.01350081789653, 220.45561498377938, 239.43371114995176, 249.18303441398348, 232.70350515459702, 246.66935025573358, 265.09294198929155, 262.40095285598676, 250.09643195766682, 203.929238761509, 237.32953767441253, 248.22917534778753, 257.6923322061415, 205.02695142783824, 147.66842615725744, 241.43768509833163, 283.05774422582715, 224.71736640957803, 237.37115252184148, 249.65837886010257, 235.01397777133118, 221.44333380747597, 280.4084798199556, 65.99732980917499, 228.66408019948574, 158.5628980249427, 252.15893017057564, 122.66487485271598, 139.67257812228905, 277.11967896558934, 221.0878852285348, 218.93900660135458, 115.2118141295875, 107.2627838180558, 251.95571369489028, 250.6393863374688, 213.40738535654512], 'lossList': [0.0, -1.3576923602819442, 0.0, 117.00294307708741, 0.0, 0.0, 0.0], 'rewardMean': 0.6947561750880691, 'totalEpisodes': 72, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.645397969871081, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.7118537238213904, 'errorList': [], 'lossList': [0.0, -1.3521089220046998, 0.0, 93.05216218948364, 0.0, 0.0, 0.0], 'rewardMean': 0.6976057665436226, 'totalEpisodes': 96, 'stepsPerEpisode': 70, 'rewardPerEpisode': 52.04250161554337
'totalSteps': 8960, 'rewardStep': 0.8589350576273336, 'errorList': [], 'lossList': [0.0, -1.3613674730062484, 0.0, 61.115189199447634, 0.0, 0.0, 0.0], 'rewardMean': 0.7206528081270099, 'totalEpisodes': 111, 'stepsPerEpisode': 101, 'rewardPerEpisode': 83.19780523405467
'totalSteps': 10240, 'rewardStep': 0.7956260327708318, 'errorList': [], 'lossList': [0.0, -1.3706093287467958, 0.0, 52.29798909187317, 0.0, 0.0, 0.0], 'rewardMean': 0.7300244612074877, 'totalEpisodes': 122, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.7921980245936706
'totalSteps': 11520, 'rewardStep': 0.6764332071220113, 'errorList': [], 'lossList': [0.0, -1.3592598468065262, 0.0, 48.94473055362701, 0.0, 0.0, 0.0], 'rewardMean': 0.7240698774202126, 'totalEpisodes': 128, 'stepsPerEpisode': 70, 'rewardPerEpisode': 62.039672641677726
'totalSteps': 12800, 'rewardStep': 0.49264278922348936, 'errorList': [], 'lossList': [0.0, -1.3337006878852844, 0.0, 30.787317774295808, 0.0, 0.0, 0.0], 'rewardMean': 0.7009271686005402, 'totalEpisodes': 130, 'stepsPerEpisode': 395, 'rewardPerEpisode': 287.19759992687904
'totalSteps': 14080, 'rewardStep': 0.6873630136342042, 'errorList': [], 'lossList': [0.0, -1.3351344901323319, 0.0, 34.70383933067322, 0.0, 0.0, 0.0], 'rewardMean': 0.7085964600697755, 'totalEpisodes': 134, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.919870524651277
'totalSteps': 15360, 'rewardStep': 0.6228742159465768, 'errorList': [], 'lossList': [0.0, -1.356997444629669, 0.0, 11.077850812673569, 0.0, 0.0, 0.0], 'rewardMean': 0.6804284447380766, 'totalEpisodes': 137, 'stepsPerEpisode': 232, 'rewardPerEpisode': 192.06764159169094
'totalSteps': 16640, 'rewardStep': 0.7788217267632352, 'errorList': [], 'lossList': [0.0, -1.3679862517118453, 0.0, 7.044207661449909, 0.0, 0.0, 0.0], 'rewardMean': 0.7051632908785223, 'totalEpisodes': 138, 'stepsPerEpisode': 709, 'rewardPerEpisode': 569.789506897966
'totalSteps': 17920, 'rewardStep': 0.7850142133177328, 'errorList': [], 'lossList': [0.0, -1.3597725415229798, 0.0, 3.4863187232613564, 0.0, 0.0, 0.0], 'rewardMean': 0.7363218988834884, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 982.8475895542712
'totalSteps': 19200, 'rewardStep': 0.8419941913219369, 'errorList': [], 'lossList': [0.0, -1.3435874342918397, 0.0, 3.123083608001471, 0.0, 0.0, 0.0], 'rewardMean': 0.7251558171548742, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.9041823901262
'totalSteps': 20480, 'rewardStep': 0.9281545588224822, 'errorList': [], 'lossList': [0.0, -1.3184297078847884, 0.0, 2.472420580983162, 0.0, 0.0, 0.0], 'rewardMean': 0.7467859006549833, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.453430185533
'totalSteps': 21760, 'rewardStep': 0.8059537688944178, 'errorList': [], 'lossList': [0.0, -1.297724912762642, 0.0, 1.3559431785345077, 0.0, 0.0, 0.0], 'rewardMean': 0.7414877717816919, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1012.6145217444554
'totalSteps': 23040, 'rewardStep': 0.8069924615903876, 'errorList': [], 'lossList': [0.0, -1.2918958514928818, 0.0, 1.1758980133384467, 0.0, 0.0, 0.0], 'rewardMean': 0.7426244146636474, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.7519882768054
'totalSteps': 24320, 'rewardStep': 0.6948127928328993, 'errorList': [], 'lossList': [0.0, -1.2594476163387298, 0.0, 0.46951481301337483, 0.0, 0.0, 0.0], 'rewardMean': 0.7444623732347363, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.62803290418
'totalSteps': 25600, 'rewardStep': 0.9319818135161524, 'errorList': [0.1752883946610625, 0.10602162588941455, 0.13164458288150777, 0.0999843391344082, 0.14033458414350203, 0.16189637898243373, 0.08559731911577782, 0.1267529564780638, 0.12048900581828453, 0.11621531848164879, 0.13670606147786063, 0.1154941558719503, 0.13076791193467457, 0.07867109523218241, 0.06642027792201971, 0.08307262038344411, 0.19980151167239876, 0.08499307887917622, 0.1274726359878463, 0.10589784950172586, 0.09669087113836271, 0.09228561871516246, 0.0956996178328298, 0.10851091157844442, 0.19024671447677366, 0.13602648158093203, 0.191809998827235, 0.1313829735953647, 0.1879108820846451, 0.09306461363059916, 0.09081915954503542, 0.10291512337549234, 0.10071342183348242, 0.11699477618834692, 0.14367207161501663, 0.06568973401412119, 0.15097986892129042, 0.11221798595020613, 0.10242345877716467, 0.09576746113146804, 0.07685092285405067, 0.07758513156967851, 0.11206883222924227, 0.058439127895747275, 0.1103048481467509, 0.10486736393755608, 0.09729738360893486, 0.08659549492333438, 0.11359628083576753, 0.11677678619183697], 'lossList': [0.0, -1.2088256144523621, 0.0, 0.7695615337789059, 0.0, 0.0, 0.0], 'rewardMean': 0.7883962756640026, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.1194203988823, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=101.42
