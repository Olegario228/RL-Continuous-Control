#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 51
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5965333132827025, 'errorList': [], 'lossList': [0.0, -1.4177787899971008, 0.0, 31.300232582092285, 0.0, 0.0, 0.0], 'rewardMean': 0.705791723062055, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.27629771583806
'totalSteps': 3840, 'rewardStep': 0.6158922659963918, 'errorList': [], 'lossList': [0.0, -1.3946469134092332, 0.0, 46.24772470474243, 0.0, 0.0, 0.0], 'rewardMean': 0.6758252373735005, 'totalEpisodes': 72, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.028468789533811
'totalSteps': 5120, 'rewardStep': 0.6287496123020667, 'errorList': [], 'lossList': [0.0, -1.3713402152061462, 0.0, 38.66108852386475, 0.0, 0.0, 0.0], 'rewardMean': 0.6640563311056421, 'totalEpisodes': 83, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.622840075329038
'totalSteps': 6400, 'rewardStep': 0.8428123020353332, 'errorList': [], 'lossList': [0.0, -1.3623939424753189, 0.0, 49.57935013771057, 0.0, 0.0, 0.0], 'rewardMean': 0.6998075252915803, 'totalEpisodes': 90, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8428123020353332
'totalSteps': 7680, 'rewardStep': 0.5454210701666875, 'errorList': [], 'lossList': [0.0, -1.34888934135437, 0.0, 49.98527459144592, 0.0, 0.0, 0.0], 'rewardMean': 0.6740764494374315, 'totalEpisodes': 98, 'stepsPerEpisode': 82, 'rewardPerEpisode': 60.55507006621018
'totalSteps': 8960, 'rewardStep': 0.975872673088657, 'errorList': [68.6828071059594, 46.92551306928492, 101.63175813913946, 114.25904860292974, 63.12020070687908, 124.43640321377552, 78.9776661332727, 28.074241651041525, 18.598947989725826, 43.57146603147747, 79.13932336941726, 96.12266304079638, 68.35841716422384, 116.30812194783918, 114.02676142463078, 37.11182445159794, 120.13334238284583, 74.79053276191752, 79.41390074880118, 119.53911151932547, 128.92068579066165, 35.940192670460654, 59.75705489223437, 133.7300862071383, 41.86202290206029, 31.399137676063322, 28.364648079750527, 42.94572349284975, 14.878181541906551, 13.091026453857719, 123.90576630669965, 99.85702564544097, 62.88979296718623, 54.40908813509207, 34.86781382783605, 25.950011059500262, 57.2568605468622, 142.62399314095234, 99.91344153628646, 117.80689140159843, 74.9259201088686, 29.998482874454655, 132.92581067684964, 60.00312821929558, 58.74529085216076, 51.64276414044556, 23.646604106056945, 148.43013261290622, 33.50561445026249, 128.193675077873], 'lossList': [0.0, -1.3380913436412811, 0.0, 91.39261114120484, 0.0, 0.0, 0.0], 'rewardMean': 0.7171901956733208, 'totalEpisodes': 119, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.253422107860908, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.23983475281336153, 'errorList': [], 'lossList': [0.0, -1.3373044276237487, 0.0, 34.89396820545196, 0.0, 0.0, 0.0], 'rewardMean': 0.6575207653158259, 'totalEpisodes': 129, 'stepsPerEpisode': 238, 'rewardPerEpisode': 171.0797361551762
'totalSteps': 11520, 'rewardStep': 0.7518356441799139, 'errorList': [], 'lossList': [0.0, -1.3335042649507522, 0.0, 54.37391808509827, 0.0, 0.0, 0.0], 'rewardMean': 0.6680001963007246, 'totalEpisodes': 140, 'stepsPerEpisode': 54, 'rewardPerEpisode': 44.14696408769928
'totalSteps': 12800, 'rewardStep': 0.6143603715996825, 'errorList': [], 'lossList': [0.0, -1.3007836043834686, 0.0, 35.57138083934784, 0.0, 0.0, 0.0], 'rewardMean': 0.6626362138306203, 'totalEpisodes': 147, 'stepsPerEpisode': 274, 'rewardPerEpisode': 219.46742327751116
'totalSteps': 14080, 'rewardStep': 0.7771493358397571, 'errorList': [], 'lossList': [0.0, -1.2702573442459106, 0.0, 9.3290099132061, 0.0, 0.0, 0.0], 'rewardMean': 0.6588461341304555, 'totalEpisodes': 150, 'stepsPerEpisode': 231, 'rewardPerEpisode': 189.4309825610817
'totalSteps': 15360, 'rewardStep': 0.5788436416259283, 'errorList': [], 'lossList': [0.0, -1.251738604903221, 0.0, 5.399504109323025, 0.0, 0.0, 0.0], 'rewardMean': 0.6570771669647779, 'totalEpisodes': 152, 'stepsPerEpisode': 525, 'rewardPerEpisode': 404.91494867019264
'totalSteps': 16640, 'rewardStep': 0.8115788151614131, 'errorList': [], 'lossList': [0.0, -1.230554221868515, 0.0, 5.082742303013801, 0.0, 0.0, 0.0], 'rewardMean': 0.6766458218812801, 'totalEpisodes': 155, 'stepsPerEpisode': 119, 'rewardPerEpisode': 105.4079722803435
'totalSteps': 17920, 'rewardStep': 0.8273599910245454, 'errorList': [], 'lossList': [0.0, -1.2340496850013734, 0.0, 3.7937852239608763, 0.0, 0.0, 0.0], 'rewardMean': 0.696506859753528, 'totalEpisodes': 158, 'stepsPerEpisode': 38, 'rewardPerEpisode': 33.19093902578404
'totalSteps': 19200, 'rewardStep': 0.9458938112040514, 'errorList': [0.8339057643223393, 2.124269002809715, 1.7188984782057823, 0.945942683665516, 1.084262444579528, 1.7320703949646354, 1.098855864202038, 1.0748708702412104, 1.0575959925653127, 1.123673194128096, 0.8470693802670438, 1.9213040380825015, 1.9732421910897175, 1.5019808845139786, 2.420463456865778, 2.2141327199963574, 0.41705446256306167, 2.2307802124569687, 1.2609297321782351, 1.7814162429332876, 2.1105821168603973, 1.928076551715214, 1.1674861312219857, 1.8148596801330943, 0.8116690596076539, 1.3527925662494271, 2.0844603117658886, 0.5954349372860077, 0.9252798013053621, 1.130467832496991, 1.6670191994709995, 2.0178999962338637, 1.9575963170909894, 1.119852896021828, 1.2247701372268933, 1.1688013504177603, 2.4303848168680244, 0.8129142914798565, 1.2788934267556664, 1.387727384216315, 1.5953773591977756, 1.3109487473728594, 1.5200764358324421, 0.9909702425310837, 1.4429735299719593, 1.1370903512255466, 1.3739988063539121, 2.361525406319141, 0.8757561235155729, 0.49130868470892486], 'lossList': [0.0, -1.2408552652597427, 0.0, 4.754214578866959, 0.0, 0.0, 0.0], 'rewardMean': 0.7068150106703998, 'totalEpisodes': 161, 'stepsPerEpisode': 70, 'rewardPerEpisode': 62.530677673955715, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.7152453638591757, 'errorList': [], 'lossList': [0.0, -1.2288578766584397, 0.0, 3.482950298190117, 0.0, 0.0, 0.0], 'rewardMean': 0.7237974400396486, 'totalEpisodes': 164, 'stepsPerEpisode': 55, 'rewardPerEpisode': 48.097259900164985
'totalSteps': 21760, 'rewardStep': 0.4555984023788902, 'errorList': [], 'lossList': [0.0, -1.216378163099289, 0.0, 2.9196196097135543, 0.0, 0.0, 0.0], 'rewardMean': 0.671770012968672, 'totalEpisodes': 165, 'stepsPerEpisode': 515, 'rewardPerEpisode': 368.26580926257395
'totalSteps': 23040, 'rewardStep': 0.478843028332395, 'errorList': [], 'lossList': [0.0, -1.2171941465139389, 0.0, 2.224100799560547, 0.0, 0.0, 0.0], 'rewardMean': 0.6956708405205753, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 910.7145335288578
'totalSteps': 24320, 'rewardStep': 0.8035366691066566, 'errorList': [], 'lossList': [0.0, -1.2002595233917237, 0.0, 1.2491837194561959, 0.0, 0.0, 0.0], 'rewardMean': 0.7008409430132496, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.2180338209419
'totalSteps': 25600, 'rewardStep': 0.8751923779424502, 'errorList': [], 'lossList': [0.0, -1.1452579295635223, 0.0, 0.7503107610344887, 0.0, 0.0, 0.0], 'rewardMean': 0.7269241436475262, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1062.2675671912602
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=103.69
