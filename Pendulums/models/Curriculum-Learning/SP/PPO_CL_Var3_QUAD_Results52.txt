#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 52
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.8471362754049409, 'errorList': [], 'lossList': [0.0, -1.4188964724540711, 0.0, 35.69211591720581, 0.0, 0.0, 0.0], 'rewardMean': 0.8313868085943252, 'totalEpisodes': 68, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.114271109384592
'totalSteps': 3840, 'rewardStep': 0.49119541684876755, 'errorList': [], 'lossList': [0.0, -1.4037146538496017, 0.0, 34.30624825000763, 0.0, 0.0, 0.0], 'rewardMean': 0.7179896780124727, 'totalEpisodes': 84, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.31202327758344
'totalSteps': 5120, 'rewardStep': 0.4973962164086543, 'errorList': [], 'lossList': [0.0, -1.385992488861084, 0.0, 31.004609491825104, 0.0, 0.0, 0.0], 'rewardMean': 0.6628413126115181, 'totalEpisodes': 95, 'stepsPerEpisode': 44, 'rewardPerEpisode': 27.526806619467628
'totalSteps': 6400, 'rewardStep': 0.5917375754682471, 'errorList': [], 'lossList': [0.0, -1.3799347507953643, 0.0, 54.640839385986325, 0.0, 0.0, 0.0], 'rewardMean': 0.6486205651828639, 'totalEpisodes': 107, 'stepsPerEpisode': 50, 'rewardPerEpisode': 38.394706925879994
'totalSteps': 7680, 'rewardStep': 0.8725342426510817, 'errorList': [], 'lossList': [0.0, -1.3723101282119752, 0.0, 113.64090341567993, 0.0, 0.0, 0.0], 'rewardMean': 0.6859395114275668, 'totalEpisodes': 132, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.863775048596096
'totalSteps': 8960, 'rewardStep': 0.6310202976111589, 'errorList': [], 'lossList': [0.0, -1.3819299173355102, 0.0, 62.30574928283691, 0.0, 0.0, 0.0], 'rewardMean': 0.6780939094537943, 'totalEpisodes': 144, 'stepsPerEpisode': 132, 'rewardPerEpisode': 96.63812041106343
'totalSteps': 10240, 'rewardStep': 0.6242376540749535, 'errorList': [], 'lossList': [0.0, -1.3764436858892442, 0.0, 25.01748811721802, 0.0, 0.0, 0.0], 'rewardMean': 0.6713618775314392, 'totalEpisodes': 151, 'stepsPerEpisode': 141, 'rewardPerEpisode': 119.8816954413893
'totalSteps': 11520, 'rewardStep': 0.9207461782285173, 'errorList': [], 'lossList': [0.0, -1.3627025294303894, 0.0, 42.90185120105743, 0.0, 0.0, 0.0], 'rewardMean': 0.699071244275559, 'totalEpisodes': 160, 'stepsPerEpisode': 69, 'rewardPerEpisode': 62.07708174907772
'totalSteps': 12800, 'rewardStep': 0.7247047571484276, 'errorList': [], 'lossList': [0.0, -1.374655555486679, 0.0, 28.1453613948822, 0.0, 0.0, 0.0], 'rewardMean': 0.7016345955628458, 'totalEpisodes': 169, 'stepsPerEpisode': 185, 'rewardPerEpisode': 144.34048284245682
'totalSteps': 14080, 'rewardStep': 0.46771389097495436, 'errorList': [], 'lossList': [0.0, -1.367415913939476, 0.0, 8.895492017269135, 0.0, 0.0, 0.0], 'rewardMean': 0.6668422504819703, 'totalEpisodes': 172, 'stepsPerEpisode': 537, 'rewardPerEpisode': 419.48494312545256
'totalSteps': 15360, 'rewardStep': 0.782686132194523, 'errorList': [], 'lossList': [0.0, -1.346663203239441, 0.0, 8.585264497995377, 0.0, 0.0, 0.0], 'rewardMean': 0.6603972361609285, 'totalEpisodes': 175, 'stepsPerEpisode': 769, 'rewardPerEpisode': 551.9308891295668
'totalSteps': 16640, 'rewardStep': 0.6992018947545457, 'errorList': [], 'lossList': [0.0, -1.3294367974996566, 0.0, 4.315387486815452, 0.0, 0.0, 0.0], 'rewardMean': 0.6811978839515064, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 911.4549851672891
'totalSteps': 17920, 'rewardStep': 0.8337574958911843, 'errorList': [], 'lossList': [0.0, -1.2961607533693313, 0.0, 3.953584226965904, 0.0, 0.0, 0.0], 'rewardMean': 0.7148340118997594, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.30457791439
'totalSteps': 19200, 'rewardStep': 0.939642878144085, 'errorList': [0.01962947240018028, 0.062442975820236606, 0.007620063717590961, 0.027652065663756796, 0.05428752743326666, 0.07424679543794457, 0.029035557968460426, 0.044791574454255634, 0.037177499735816445, 0.04796148754858566, 0.06023180834101505, 0.07092398257712457, 0.04495287608386506, 0.040005619279941416, 0.03662617004800185, 0.020834793899386316, 0.04381918958633608, 0.052547992211280224, 0.00453168945900122, 0.03436555065106226, 0.039543804816501625, 0.018179475242174486, 0.04744654072116886, 0.020365529535562452, 0.02817881266681968, 0.06445772210194693, 0.016685151085071532, 0.051294407653383015, 0.06194184612551718, 0.02054458626534456, 0.07241192403520469, 0.03563673188780183, 0.03369745282370353, 0.024780138776007463, 0.06059326359306318, 0.035780107452503866, 0.02831424633575614, 0.03020908097358352, 0.033923218938726823, 0.053384830707010625, 0.037170408047412215, 0.045435399698666286, 0.041135012083127934, 0.0251996887172215, 0.015629073974179886, 0.016111716272918127, 0.0526327002609212, 0.09661926382444859, 0.026416743876670257, 0.025676535569147087], 'lossList': [0.0, -1.267734386920929, 0.0, 4.160862760841846, 0.0, 0.0, 0.0], 'rewardMean': 0.7496245421673431, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1116.123156681655, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=68.05
