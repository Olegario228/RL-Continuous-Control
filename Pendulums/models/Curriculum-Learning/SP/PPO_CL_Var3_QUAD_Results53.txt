#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 53
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.539289994451557, 'errorList': [], 'lossList': [0.0, -1.4378273886442186, 0.0, 32.056147470474244, 0.0, 0.0, 0.0], 'rewardMean': 0.5221146105959709, 'totalEpisodes': 57, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.931767152278283
'totalSteps': 3840, 'rewardStep': 0.9107590199444451, 'errorList': [], 'lossList': [0.0, -1.4331001824140548, 0.0, 42.943211765289306, 0.0, 0.0, 0.0], 'rewardMean': 0.6516627470454622, 'totalEpisodes': 75, 'stepsPerEpisode': 49, 'rewardPerEpisode': 37.703848068500236
'totalSteps': 5120, 'rewardStep': 0.9454254652816433, 'errorList': [], 'lossList': [0.0, -1.4232219505310058, 0.0, 38.70396107673645, 0.0, 0.0, 0.0], 'rewardMean': 0.7251034266045076, 'totalEpisodes': 84, 'stepsPerEpisode': 71, 'rewardPerEpisode': 60.72284524050741
'totalSteps': 6400, 'rewardStep': 0.5848009592307719, 'errorList': [], 'lossList': [0.0, -1.4145273756980896, 0.0, 54.69945499420166, 0.0, 0.0, 0.0], 'rewardMean': 0.6970429331297604, 'totalEpisodes': 96, 'stepsPerEpisode': 166, 'rewardPerEpisode': 118.28431742813854
'totalSteps': 7680, 'rewardStep': 0.9148822585999272, 'errorList': [], 'lossList': [0.0, -1.4113388931751252, 0.0, 97.7674842453003, 0.0, 0.0, 0.0], 'rewardMean': 0.7333494873747881, 'totalEpisodes': 112, 'stepsPerEpisode': 86, 'rewardPerEpisode': 70.0160756196793
'totalSteps': 8960, 'rewardStep': 0.894681956559546, 'errorList': [], 'lossList': [0.0, -1.399956681728363, 0.0, 51.02960412979126, 0.0, 0.0, 0.0], 'rewardMean': 0.7563969829726106, 'totalEpisodes': 123, 'stepsPerEpisode': 69, 'rewardPerEpisode': 59.12038040327714
'totalSteps': 10240, 'rewardStep': 0.5438233648867287, 'errorList': [], 'lossList': [0.0, -1.3920028966665268, 0.0, 26.849442596435548, 0.0, 0.0, 0.0], 'rewardMean': 0.7298252807118755, 'totalEpisodes': 130, 'stepsPerEpisode': 164, 'rewardPerEpisode': 121.49219814081694
'totalSteps': 11520, 'rewardStep': 0.6288622646127779, 'errorList': [], 'lossList': [0.0, -1.3879883831739426, 0.0, 12.026533406972884, 0.0, 0.0, 0.0], 'rewardMean': 0.7186071678119758, 'totalEpisodes': 133, 'stepsPerEpisode': 116, 'rewardPerEpisode': 91.29588317201238
'totalSteps': 12800, 'rewardStep': 0.7782553851517963, 'errorList': [], 'lossList': [0.0, -1.390659298300743, 0.0, 12.619382160902024, 0.0, 0.0, 0.0], 'rewardMean': 0.7245719895459578, 'totalEpisodes': 136, 'stepsPerEpisode': 177, 'rewardPerEpisode': 153.09029366750423
'totalSteps': 14080, 'rewardStep': 0.517037462084963, 'errorList': [], 'lossList': [0.0, -1.3899868410825729, 0.0, 7.545695998072624, 0.0, 0.0, 0.0], 'rewardMean': 0.7257818130804157, 'totalEpisodes': 137, 'stepsPerEpisode': 527, 'rewardPerEpisode': 384.5070494187087
'totalSteps': 15360, 'rewardStep': 0.6972302247319522, 'errorList': [], 'lossList': [0.0, -1.3694110709428786, 0.0, 4.747923696637153, 0.0, 0.0, 0.0], 'rewardMean': 0.7415758361084552, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 895.3707192303601
'totalSteps': 16640, 'rewardStep': 0.8653213803988723, 'errorList': [], 'lossList': [0.0, -1.3395978742837906, 0.0, 3.912206659913063, 0.0, 0.0, 0.0], 'rewardMean': 0.7370320721538979, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.996839499658
'totalSteps': 17920, 'rewardStep': 0.8562122598269072, 'errorList': [], 'lossList': [0.0, -1.3063360273838043, 0.0, 2.5894873526692392, 0.0, 0.0, 0.0], 'rewardMean': 0.7281107516084242, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.5156738371168
'totalSteps': 19200, 'rewardStep': 0.9074693251734083, 'errorList': [], 'lossList': [0.0, -1.288982561826706, 0.0, 1.303913625329733, 0.0, 0.0, 0.0], 'rewardMean': 0.7603775882026879, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.4070273653408
'totalSteps': 20480, 'rewardStep': 0.972855920607981, 'errorList': [0.017434880840705477, 0.016479111111834196, 0.02876014401633478, 0.018867690713813615, 0.034599630383717696, 0.04396787751890373, 0.02537751668546973, 0.05973202253773035, 0.033775317918127185, 0.017571210252914807, 0.05445038643197014, 0.023147949137227973, 0.03539512632142297, 0.01851050943200892, 0.018501001312454246, 0.038470823325253214, 0.019434735399133787, 0.02057465108897959, 0.023901162615237417, 0.016055162116634214, 0.03716940744946923, 0.035266309974870215, 0.04228817172695621, 0.015877258211703334, 0.054654706676043725, 0.015476145655825212, 0.0305747464721294, 0.0324233276283505, 0.029093005061977338, 0.018240425817115845, 0.026071232608124887, 0.019067415815526245, 0.02286278175408512, 0.04417929428547932, 0.027104360715334265, 0.04620027516912259, 0.023568166472465304, 0.030960789099676162, 0.03859665681485042, 0.03538991862350068, 0.01746793707068895, 0.06720004578517573, 0.032601609904915854, 0.020954042948546556, 0.027165540860530454, 0.0410028944635873, 0.02859032366579087, 0.017173114235483275, 0.032444455350115586, 0.03659183536767106], 'lossList': [0.0, -1.2557993906736373, 0.0, 1.4247591204196215, 0.0, 0.0, 0.0], 'rewardMean': 0.7661749544034933, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.913614805081, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=71.95
