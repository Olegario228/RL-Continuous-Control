#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 54
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7485961745826508, 'errorList': [], 'lossList': [0.0, -1.4050948071479796, 0.0, 34.69511636734009, 0.0, 0.0, 0.0], 'rewardMean': 0.8348431543670289, 'totalEpisodes': 69, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.771695158194817
'totalSteps': 3840, 'rewardStep': 0.8703704867368137, 'errorList': [], 'lossList': [0.0, -1.4007585960626603, 0.0, 41.25835922241211, 0.0, 0.0, 0.0], 'rewardMean': 0.8466855984902906, 'totalEpisodes': 92, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.84768237795047
'totalSteps': 5120, 'rewardStep': 0.9062571162635141, 'errorList': [], 'lossList': [0.0, -1.3880423158407211, 0.0, 32.51791934967041, 0.0, 0.0, 0.0], 'rewardMean': 0.8615784779335964, 'totalEpisodes': 103, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.08840848193001
'totalSteps': 6400, 'rewardStep': 0.7405758916521152, 'errorList': [], 'lossList': [0.0, -1.373760005235672, 0.0, 40.537197880744934, 0.0, 0.0, 0.0], 'rewardMean': 0.8373779606773002, 'totalEpisodes': 116, 'stepsPerEpisode': 32, 'rewardPerEpisode': 25.43013103015583
'totalSteps': 7680, 'rewardStep': 0.46002192267458447, 'errorList': [], 'lossList': [0.0, -1.3690044099092484, 0.0, 74.75302430152892, 0.0, 0.0, 0.0], 'rewardMean': 0.7744852876768475, 'totalEpisodes': 129, 'stepsPerEpisode': 131, 'rewardPerEpisode': 93.03194343882274
'totalSteps': 8960, 'rewardStep': 0.6161766352169933, 'errorList': [], 'lossList': [0.0, -1.3706647300720214, 0.0, 99.18116004943847, 0.0, 0.0, 0.0], 'rewardMean': 0.7518697658968684, 'totalEpisodes': 157, 'stepsPerEpisode': 59, 'rewardPerEpisode': 46.15951735337187
'totalSteps': 10240, 'rewardStep': 0.7355439625550697, 'errorList': [], 'lossList': [0.0, -1.3726323974132537, 0.0, 41.158025617599485, 0.0, 0.0, 0.0], 'rewardMean': 0.7498290404791436, 'totalEpisodes': 169, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.896360314957546
'totalSteps': 11520, 'rewardStep': 0.7567639008590464, 'errorList': [], 'lossList': [0.0, -1.3569612008333207, 0.0, 17.342998785972597, 0.0, 0.0, 0.0], 'rewardMean': 0.750599580521355, 'totalEpisodes': 173, 'stepsPerEpisode': 288, 'rewardPerEpisode': 236.90853683853942
'totalSteps': 12800, 'rewardStep': 0.7704840255804728, 'errorList': [], 'lossList': [0.0, -1.3225619328022002, 0.0, 7.770092409849167, 0.0, 0.0, 0.0], 'rewardMean': 0.7525880250272667, 'totalEpisodes': 177, 'stepsPerEpisode': 111, 'rewardPerEpisode': 83.37460380081772
'totalSteps': 14080, 'rewardStep': 0.6183526543477886, 'errorList': [], 'lossList': [0.0, -1.3020664310455323, 0.0, 11.754371777176857, 0.0, 0.0, 0.0], 'rewardMean': 0.7223142770469048, 'totalEpisodes': 179, 'stepsPerEpisode': 747, 'rewardPerEpisode': 485.6625076610851
'totalSteps': 15360, 'rewardStep': 0.7690966664124161, 'errorList': [], 'lossList': [0.0, -1.282729601264, 0.0, 6.928302309513092, 0.0, 0.0, 0.0], 'rewardMean': 0.7243643262298815, 'totalEpisodes': 180, 'stepsPerEpisode': 794, 'rewardPerEpisode': 631.2977192259319
'totalSteps': 16640, 'rewardStep': 0.6943014460864374, 'errorList': [], 'lossList': [0.0, -1.2546852481365205, 0.0, 4.083095069825649, 0.0, 0.0, 0.0], 'rewardMean': 0.7067574221648438, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.8651221958895
'totalSteps': 17920, 'rewardStep': 0.8425102763267702, 'errorList': [], 'lossList': [0.0, -1.2245535939931869, 0.0, 2.941226307153702, 0.0, 0.0, 0.0], 'rewardMean': 0.7003827381711695, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.5144416892492
'totalSteps': 19200, 'rewardStep': 0.8353471569853065, 'errorList': [], 'lossList': [0.0, -1.2073748159408568, 0.0, 2.138161120414734, 0.0, 0.0, 0.0], 'rewardMean': 0.7098598647044885, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1087.024004769331
'totalSteps': 20480, 'rewardStep': 0.9913922309993026, 'errorList': [0.18552803369776208, 0.15194445105112758, 0.19513960239141986, 0.2247682442407041, 0.3036423793034443, 0.16507674879533343, 0.17746102763438631, 0.1648445474440057, 0.16814514098662361, 0.16528931251373757, 0.18065140864655974, 0.29611704795831423, 0.17549361343459907, 0.1599697192174744, 0.17002264386921084, 0.14547176546441962, 0.16585939473252923, 0.1935633637075533, 0.1817569746532925, 0.17185080175696768, 0.15174683607728473, 0.16385189123639896, 0.1483159740889187, 0.1995303708091949, 0.1750200821551241, 0.14348310629621736, 0.17119048309894777, 0.14535958574013125, 0.1754658864062147, 0.16955872855062437, 0.15400705244779878, 0.18595839184905555, 0.24959834419419996, 0.2179001518629304, 0.24174414401910918, 0.16129800285859844, 0.19152173771544073, 0.28669901244463164, 0.16979074058278068, 0.15080902388697368, 0.17985142562603468, 0.1777914069472286, 0.18356094859775174, 0.14580854445385558, 0.17225060860763197, 0.16866424301974825, 0.17358710538818495, 0.16146271173684887, 0.16379542859286636, 0.1633377538246013], 'lossList': [0.0, -1.1940448611974717, 0.0, 2.5212054232507946, 0.0, 0.0, 0.0], 'rewardMean': 0.7629968955369603, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.0327828592158, 'successfulTests': 43
'totalSteps': 21760, 'rewardStep': 0.8880281462245231, 'errorList': [], 'lossList': [0.0, -1.1920571672916411, 0.0, 1.2403250532969832, 0.0, 0.0, 0.0], 'rewardMean': 0.7901820466377134, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.2058466152887
'totalSteps': 23040, 'rewardStep': 0.9064529155872204, 'errorList': [], 'lossList': [0.0, -1.198694770336151, 0.0, 0.8514283576980233, 0.0, 0.0, 0.0], 'rewardMean': 0.8072729419409285, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.8358592966401
'totalSteps': 24320, 'rewardStep': 0.9268936258248162, 'errorList': [], 'lossList': [0.0, -1.1775733506679535, 0.0, 0.795717911515385, 0.0, 0.0, 0.0], 'rewardMean': 0.8242859144375053, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1204.6972404620717
'totalSteps': 25600, 'rewardStep': 0.9805193705975358, 'errorList': [0.04357178459535457, 0.09372876128849957, 0.04183274993035869, 0.0406714308926354, 0.05248422840728824, 0.06385866103365137, 0.09671626919011092, 0.06395794350134622, 0.07032992648394794, 0.040635527931129775, 0.07625375508755133, 0.01626628473745391, 0.0743206983370371, 0.08835985494428829, 0.0476092292899234, 0.03356450412874866, 0.05909432268874628, 0.05223588840453167, 0.127212889717485, 0.09890416034694616, 0.12094138577849926, 0.054286078178818836, 0.049206885314530366, 0.07654736773172362, 0.03943194922942428, 0.05571348929348302, 0.08970674308679552, 0.048886593641332056, 0.05319548984114574, 0.028168948323192628, 0.07117837634838146, 0.14292264026833953, 0.01994350359781098, 0.030266972430714696, 0.12629155811061604, 0.14467647599503683, 0.007781784868699655, 0.007536840757532498, 0.024446153378635493, 0.04717445176533984, 0.0982875872493855, 0.041471026066260325, 0.08932941098336045, 0.08423989872812036, 0.029238645141032568, 0.043546266792097776, 0.017790100862741104, 0.013832806032222327, 0.02716155318376542, 0.029748100443919175], 'lossList': [0.0, -1.1532177990674972, 0.0, 0.5537903995998203, 0.0, 0.0, 0.0], 'rewardMean': 0.8452894489392117, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1207.5516164575006, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=104.12
