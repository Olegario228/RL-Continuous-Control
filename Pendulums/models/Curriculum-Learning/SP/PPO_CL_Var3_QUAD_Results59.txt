#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 59
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.874329911222801, 'errorList': [], 'lossList': [0.0, -1.4158003008365632, 0.0, 28.031501228809358, 0.0, 0.0, 0.0], 'rewardMean': 0.6192000084991949, 'totalEpisodes': 19, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.585559272034455
'totalSteps': 3840, 'rewardStep': 0.6519490990522214, 'errorList': [], 'lossList': [0.0, -1.427988002896309, 0.0, 25.670550005435942, 0.0, 0.0, 0.0], 'rewardMean': 0.6301163720168704, 'totalEpisodes': 25, 'stepsPerEpisode': 161, 'rewardPerEpisode': 112.12381368475195
'totalSteps': 5120, 'rewardStep': 0.46796139596438063, 'errorList': [], 'lossList': [0.0, -1.4268799674510957, 0.0, 24.082236607074737, 0.0, 0.0, 0.0], 'rewardMean': 0.5895776280037479, 'totalEpisodes': 29, 'stepsPerEpisode': 281, 'rewardPerEpisode': 186.5630473471299
'totalSteps': 6400, 'rewardStep': 0.6981315115585838, 'errorList': [], 'lossList': [0.0, -1.4065275442600251, 0.0, 43.7541789150238, 0.0, 0.0, 0.0], 'rewardMean': 0.6112884047147151, 'totalEpisodes': 36, 'stepsPerEpisode': 350, 'rewardPerEpisode': 247.95543471714373
'totalSteps': 7680, 'rewardStep': 0.5771940329621289, 'errorList': [], 'lossList': [0.0, -1.3949834018945695, 0.0, 104.70169036865235, 0.0, 0.0, 0.0], 'rewardMean': 0.6056060094226174, 'totalEpisodes': 49, 'stepsPerEpisode': 35, 'rewardPerEpisode': 21.308796621828872
'totalSteps': 8960, 'rewardStep': 0.8785837106127585, 'errorList': [], 'lossList': [0.0, -1.389756777882576, 0.0, 179.25618339538573, 0.0, 0.0, 0.0], 'rewardMean': 0.6446028238783519, 'totalEpisodes': 92, 'stepsPerEpisode': 63, 'rewardPerEpisode': 48.45451617341588
'totalSteps': 10240, 'rewardStep': 0.49075779842381373, 'errorList': [], 'lossList': [0.0, -1.3833462172746658, 0.0, 80.47616403579713, 0.0, 0.0, 0.0], 'rewardMean': 0.6253721956965346, 'totalEpisodes': 120, 'stepsPerEpisode': 21, 'rewardPerEpisode': 11.000495479242463
'totalSteps': 11520, 'rewardStep': 0.8760099668108006, 'errorList': [], 'lossList': [0.0, -1.3816044580936433, 0.0, 34.979602694511414, 0.0, 0.0, 0.0], 'rewardMean': 0.6532208369314529, 'totalEpisodes': 136, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.19349483685561
'totalSteps': 12800, 'rewardStep': 0.6430855053708779, 'errorList': [], 'lossList': [0.0, -1.3759062558412551, 0.0, 29.46628972053528, 0.0, 0.0, 0.0], 'rewardMean': 0.6522073037753955, 'totalEpisodes': 144, 'stepsPerEpisode': 62, 'rewardPerEpisode': 47.40295390319384
'totalSteps': 14080, 'rewardStep': 0.869591993867886, 'errorList': [], 'lossList': [0.0, -1.3739572364091872, 0.0, 9.815868184566497, 0.0, 0.0, 0.0], 'rewardMean': 0.7027594925846252, 'totalEpisodes': 147, 'stepsPerEpisode': 137, 'rewardPerEpisode': 114.95720355324033
'totalSteps': 15360, 'rewardStep': 0.6119707170140546, 'errorList': [], 'lossList': [0.0, -1.3893586385250092, 0.0, 40.08561021089554, 0.0, 0.0, 0.0], 'rewardMean': 0.6765235731637507, 'totalEpisodes': 152, 'stepsPerEpisode': 805, 'rewardPerEpisode': 578.2889570071211
'totalSteps': 16640, 'rewardStep': 0.5195244242106551, 'errorList': [], 'lossList': [0.0, -1.3799522626399994, 0.0, 30.01957150220871, 0.0, 0.0, 0.0], 'rewardMean': 0.6632811056795939, 'totalEpisodes': 155, 'stepsPerEpisode': 449, 'rewardPerEpisode': 339.7021122022064
'totalSteps': 17920, 'rewardStep': 0.6502764669283284, 'errorList': [], 'lossList': [0.0, -1.344351944923401, 0.0, 4.998882802128792, 0.0, 0.0, 0.0], 'rewardMean': 0.6815126127759887, 'totalEpisodes': 156, 'stepsPerEpisode': 556, 'rewardPerEpisode': 403.09721311943946
'totalSteps': 19200, 'rewardStep': 0.9108896251864549, 'errorList': [], 'lossList': [0.0, -1.3104417043924332, 0.0, 4.912818977534771, 0.0, 0.0, 0.0], 'rewardMean': 0.7027884241387758, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.3348124061491
'totalSteps': 20480, 'rewardStep': 0.7663217548155689, 'errorList': [], 'lossList': [0.0, -1.2670873039960862, 0.0, 2.892648501396179, 0.0, 0.0, 0.0], 'rewardMean': 0.7217011963241199, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 923.5203299279262
'totalSteps': 21760, 'rewardStep': 0.8295760753107293, 'errorList': [], 'lossList': [0.0, -1.2502910667657852, 0.0, 2.827705165594816, 0.0, 0.0, 0.0], 'rewardMean': 0.716800432793917, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.0773330881636
'totalSteps': 23040, 'rewardStep': 0.8500359707311703, 'errorList': [], 'lossList': [0.0, -1.2454225879907608, 0.0, 1.7488943592458963, 0.0, 0.0, 0.0], 'rewardMean': 0.7527282500246526, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1084.7707180404802
'totalSteps': 24320, 'rewardStep': 0.9035517604200844, 'errorList': [], 'lossList': [0.0, -1.2253463089466095, 0.0, 1.8078762888535858, 0.0, 0.0, 0.0], 'rewardMean': 0.755482429385581, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.361979623305
'totalSteps': 25600, 'rewardStep': 0.9654229460647269, 'errorList': [0.07477710543925277, 0.07799373597371773, 0.06553487381145247, 0.0705559956927272, 0.06999610213210626, 0.06977051374196166, 0.11383686644329517, 0.07328922583983374, 0.10122289788585098, 0.09915120580548381, 0.08937333422730248, 0.08920518036623713, 0.09819454307512301, 0.07916906789885922, 0.1101786405345494, 0.09045677735713553, 0.08007677579296799, 0.1283932787701914, 0.06541933627646698, 0.07616178668609666, 0.09070359185620026, 0.07292742540269398, 0.0799976186859808, 0.0992292348750269, 0.07523110399044393, 0.07285280793083987, 0.06535940052520652, 0.06861234351256199, 0.07191480864340491, 0.0652396262132152, 0.10801883904411293, 0.07513422408856531, 0.07117804144327292, 0.07140184983913767, 0.07241389746646439, 0.07943800768939845, 0.11619421868649793, 0.07583122942222888, 0.06491986938908113, 0.0722076930246697, 0.07640166061674904, 0.07247921369980784, 0.08143151798959895, 0.07973498831828185, 0.06628257602030577, 0.12384204652662295, 0.07469885792901566, 0.0929493065122308, 0.11712897457454939, 0.07965261999689377], 'lossList': [0.0, -1.2026068824529648, 0.0, 1.5967817019671202, 0.0, 0.0, 0.0], 'rewardMean': 0.7877161734549659, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.1549570633367, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=84.32
