#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 6
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.5887661770836823, 'errorList': [], 'lossList': [0.0, -1.4171436947584153, 0.0, 26.077654943466186, 0.0, 0.0, 0.0], 'rewardMean': 0.7760278644971961, 'totalEpisodes': 16, 'stepsPerEpisode': 359, 'rewardPerEpisode': 250.1086476856821
'totalSteps': 3840, 'rewardStep': 0.47037740245121734, 'errorList': [], 'lossList': [0.0, -1.414244042634964, 0.0, 28.315385642051698, 0.0, 0.0, 0.0], 'rewardMean': 0.6741443771485365, 'totalEpisodes': 24, 'stepsPerEpisode': 195, 'rewardPerEpisode': 134.4508337646465
'totalSteps': 5120, 'rewardStep': 0.5364004375191282, 'errorList': [], 'lossList': [0.0, -1.3975857853889466, 0.0, 44.132102785110476, 0.0, 0.0, 0.0], 'rewardMean': 0.6397083922411844, 'totalEpisodes': 36, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.121028658990448
'totalSteps': 6400, 'rewardStep': 0.7310445602006144, 'errorList': [], 'lossList': [0.0, -1.380854100584984, 0.0, 118.70377586364746, 0.0, 0.0, 0.0], 'rewardMean': 0.6579756258330705, 'totalEpisodes': 69, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.346988585370264
'totalSteps': 7680, 'rewardStep': 0.7128428440785849, 'errorList': [], 'lossList': [0.0, -1.3843793785572052, 0.0, 39.38231620311737, 0.0, 0.0, 0.0], 'rewardMean': 0.6671201622073228, 'totalEpisodes': 78, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.48574681731516
'totalSteps': 8960, 'rewardStep': 0.27584608700541235, 'errorList': [], 'lossList': [0.0, -1.3778728461265564, 0.0, 45.74673181533814, 0.0, 0.0, 0.0], 'rewardMean': 0.6112238657499071, 'totalEpisodes': 89, 'stepsPerEpisode': 108, 'rewardPerEpisode': 64.3432076067223
'totalSteps': 10240, 'rewardStep': 0.8191736886863852, 'errorList': [], 'lossList': [0.0, -1.3579101568460465, 0.0, 27.603825516700745, 0.0, 0.0, 0.0], 'rewardMean': 0.6372175936169668, 'totalEpisodes': 97, 'stepsPerEpisode': 54, 'rewardPerEpisode': 45.61221459535304
'totalSteps': 11520, 'rewardStep': 0.5167007084130741, 'errorList': [], 'lossList': [0.0, -1.3457253694534301, 0.0, 47.825944681167606, 0.0, 0.0, 0.0], 'rewardMean': 0.623826828594312, 'totalEpisodes': 106, 'stepsPerEpisode': 80, 'rewardPerEpisode': 60.82853303932523
'totalSteps': 12800, 'rewardStep': 0.7579887776496763, 'errorList': [], 'lossList': [0.0, -1.3224391007423402, 0.0, 25.482319650650023, 0.0, 0.0, 0.0], 'rewardMean': 0.6372430234998485, 'totalEpisodes': 114, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.397147730646317
'totalSteps': 14080, 'rewardStep': 0.24265287972110855, 'errorList': [], 'lossList': [0.0, -1.3054065656661988, 0.0, 16.151072257757185, 0.0, 0.0, 0.0], 'rewardMean': 0.5651793562808884, 'totalEpisodes': 118, 'stepsPerEpisode': 248, 'rewardPerEpisode': 179.94415027308486
'totalSteps': 15360, 'rewardStep': 0.7586467235825074, 'errorList': [], 'lossList': [0.0, -1.3073558622598649, 0.0, 11.341131768226624, 0.0, 0.0, 0.0], 'rewardMean': 0.582167410930771, 'totalEpisodes': 124, 'stepsPerEpisode': 143, 'rewardPerEpisode': 111.8120087482731
'totalSteps': 16640, 'rewardStep': 0.7623738126035056, 'errorList': [], 'lossList': [0.0, -1.2949704712629317, 0.0, 8.485195702314376, 0.0, 0.0, 0.0], 'rewardMean': 0.6113670519459997, 'totalEpisodes': 128, 'stepsPerEpisode': 282, 'rewardPerEpisode': 228.13590851621382
'totalSteps': 17920, 'rewardStep': 0.8990890665887643, 'errorList': [], 'lossList': [0.0, -1.2824627500772476, 0.0, 6.844549803733826, 0.0, 0.0, 0.0], 'rewardMean': 0.6476359148529633, 'totalEpisodes': 131, 'stepsPerEpisode': 482, 'rewardPerEpisode': 422.20961163214014
'totalSteps': 19200, 'rewardStep': 0.9050104177379756, 'errorList': [], 'lossList': [0.0, -1.2645312446355819, 0.0, 5.526593862175941, 0.0, 0.0, 0.0], 'rewardMean': 0.6650325006066995, 'totalEpisodes': 134, 'stepsPerEpisode': 117, 'rewardPerEpisode': 96.2851814687683
'totalSteps': 20480, 'rewardStep': 0.9032924117354271, 'errorList': [], 'lossList': [0.0, -1.2236903399229049, 0.0, 6.179836175441742, 0.0, 0.0, 0.0], 'rewardMean': 0.6840774573723836, 'totalEpisodes': 137, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.852940080626253
'totalSteps': 21760, 'rewardStep': 0.5652046631025271, 'errorList': [], 'lossList': [0.0, -1.189232115149498, 0.0, 5.502944544553757, 0.0, 0.0, 0.0], 'rewardMean': 0.7130133149820951, 'totalEpisodes': 138, 'stepsPerEpisode': 559, 'rewardPerEpisode': 443.0582175765119
'totalSteps': 23040, 'rewardStep': 0.7852704933165668, 'errorList': [], 'lossList': [0.0, -1.176273876428604, 0.0, 2.2441695287823675, 0.0, 0.0, 0.0], 'rewardMean': 0.7096229954451132, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 957.7702063987317
'totalSteps': 24320, 'rewardStep': 0.6874676294384123, 'errorList': [], 'lossList': [0.0, -1.164342269897461, 0.0, 1.4480511102080345, 0.0, 0.0, 0.0], 'rewardMean': 0.726699687547647, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1079.9423175971945
'totalSteps': 25600, 'rewardStep': 0.9649195329469148, 'errorList': [0.11188827583013616, 0.11677710322123454, 0.1255545954921535, 0.10953801267763977, 0.11724583835077841, 0.15519899440395524, 0.1339845732970407, 0.12979892812754668, 0.15804137713431107, 0.12293357374430974, 0.11560426821070054, 0.132772404560579, 0.1402801217965475, 0.12117113563041124, 0.1202588393700317, 0.13763007465711832, 0.12973352792155696, 0.11571052661328912, 0.13870865524941914, 0.1391596920273211, 0.13213585757901622, 0.12145321897683675, 0.12274932308228768, 0.15053698382782457, 0.12192352571865242, 0.12485108548731381, 0.13785518599871519, 0.13427294145010454, 0.13736779965828103, 0.11852068392644416, 0.1328659058535339, 0.11007498661404384, 0.11387507196180871, 0.15756262068996624, 0.13434434636529488, 0.12300376755884536, 0.18158988168913243, 0.1254502697736971, 0.12912409293040503, 0.15673907097083006, 0.12122753773085143, 0.1189354576627813, 0.12240303326850017, 0.13480254868895253, 0.12523513240559953, 0.134691867412026, 0.12130738423586382, 0.1281895511651774, 0.1346948178204001, 0.11546310582105866], 'lossList': [0.0, -1.129207391142845, 0.0, 0.8875063280016184, 0.0, 0.0, 0.0], 'rewardMean': 0.747392763077371, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.78912030518, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=79.78
