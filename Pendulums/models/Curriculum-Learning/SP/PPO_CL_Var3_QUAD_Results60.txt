#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 60
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9222118009620719, 'errorList': [], 'lossList': [0.0, -1.4074676847457885, 0.0, 26.728652129769326, 0.0, 0.0, 0.0], 'rewardMean': 0.8610053807305175, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 381.9830179980953
'totalSteps': 3840, 'rewardStep': 0.607075662980918, 'errorList': [], 'lossList': [0.0, -1.4022896057367324, 0.0, 35.620902247428894, 0.0, 0.0, 0.0], 'rewardMean': 0.776362141480651, 'totalEpisodes': 12, 'stepsPerEpisode': 262, 'rewardPerEpisode': 197.00484233647106
'totalSteps': 5120, 'rewardStep': 0.6840175052105475, 'errorList': [], 'lossList': [0.0, -1.403834241628647, 0.0, 26.682963614463805, 0.0, 0.0, 0.0], 'rewardMean': 0.7532759824131251, 'totalEpisodes': 14, 'stepsPerEpisode': 471, 'rewardPerEpisode': 335.50823138316247
'totalSteps': 6400, 'rewardStep': 0.7493303030788767, 'errorList': [], 'lossList': [0.0, -1.4062317913770677, 0.0, 19.64779090166092, 0.0, 0.0, 0.0], 'rewardMean': 0.7524868465462754, 'totalEpisodes': 15, 'stepsPerEpisode': 594, 'rewardPerEpisode': 419.4058531050969
'totalSteps': 7680, 'rewardStep': 0.6190055684899686, 'errorList': [], 'lossList': [0.0, -1.406718521118164, 0.0, 122.74431804656983, 0.0, 0.0, 0.0], 'rewardMean': 0.7302399668702243, 'totalEpisodes': 29, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.075271149741533
'totalSteps': 8960, 'rewardStep': 0.5814311777572987, 'errorList': [], 'lossList': [0.0, -1.3969083857536315, 0.0, 199.55188457489012, 0.0, 0.0, 0.0], 'rewardMean': 0.7089815684255206, 'totalEpisodes': 73, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5814311777572987
'totalSteps': 10240, 'rewardStep': 0.8583405042936963, 'errorList': [], 'lossList': [0.0, -1.3806120133399964, 0.0, 104.83415893554688, 0.0, 0.0, 0.0], 'rewardMean': 0.7276514354090426, 'totalEpisodes': 105, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.374183582266232
'totalSteps': 11520, 'rewardStep': 0.6205777046853977, 'errorList': [], 'lossList': [0.0, -1.3607890224456787, 0.0, 60.96877180099487, 0.0, 0.0, 0.0], 'rewardMean': 0.7157543542175265, 'totalEpisodes': 128, 'stepsPerEpisode': 47, 'rewardPerEpisode': 32.09098952324642
'totalSteps': 12800, 'rewardStep': 0.8904331562406171, 'errorList': [], 'lossList': [0.0, -1.3409640604257584, 0.0, 29.42190460205078, 0.0, 0.0, 0.0], 'rewardMean': 0.7332222344198357, 'totalEpisodes': 141, 'stepsPerEpisode': 141, 'rewardPerEpisode': 118.35298554091226
'totalSteps': 14080, 'rewardStep': 0.6259354304544106, 'errorList': [], 'lossList': [0.0, -1.3018106788396835, 0.0, 21.966521432399748, 0.0, 0.0, 0.0], 'rewardMean': 0.7158358814153802, 'totalEpisodes': 150, 'stepsPerEpisode': 65, 'rewardPerEpisode': 57.411496621831525
'totalSteps': 15360, 'rewardStep': 0.632406045138428, 'errorList': [], 'lossList': [0.0, -1.2899024575948714, 0.0, 26.715394339561463, 0.0, 0.0, 0.0], 'rewardMean': 0.686855305833016, 'totalEpisodes': 158, 'stepsPerEpisode': 73, 'rewardPerEpisode': 51.56163172800488
'totalSteps': 16640, 'rewardStep': 0.44719134475367894, 'errorList': [], 'lossList': [0.0, -1.2847725731134414, 0.0, 6.615904750823975, 0.0, 0.0, 0.0], 'rewardMean': 0.670866874010292, 'totalEpisodes': 164, 'stepsPerEpisode': 154, 'rewardPerEpisode': 110.36990122611584
'totalSteps': 17920, 'rewardStep': 0.6983212608064949, 'errorList': [], 'lossList': [0.0, -1.2663188236951828, 0.0, 8.424183422327042, 0.0, 0.0, 0.0], 'rewardMean': 0.6722972495698867, 'totalEpisodes': 169, 'stepsPerEpisode': 133, 'rewardPerEpisode': 97.80103395480556
'totalSteps': 19200, 'rewardStep': 0.8630169879021545, 'errorList': [], 'lossList': [0.0, -1.2661799800395965, 0.0, 13.971598552465439, 0.0, 0.0, 0.0], 'rewardMean': 0.6836659180522145, 'totalEpisodes': 173, 'stepsPerEpisode': 255, 'rewardPerEpisode': 224.29018055168513
'totalSteps': 20480, 'rewardStep': 0.5791950930168075, 'errorList': [], 'lossList': [0.0, -1.2654209047555924, 0.0, 6.080843563079834, 0.0, 0.0, 0.0], 'rewardMean': 0.6796848705048986, 'totalEpisodes': 176, 'stepsPerEpisode': 296, 'rewardPerEpisode': 215.65701668095858
'totalSteps': 21760, 'rewardStep': 0.9671519701464124, 'errorList': [0.04351962627831811, 0.27321700225750595, 0.07667454177116648, 0.5778289771453196, 0.5414098186050953, 0.06607413121495662, 0.26722848270356364, 0.19905480037708687, 0.19759117887746572, 0.7534926170290592, 0.055994657562372166, 0.37915447176164574, 0.06509560089348738, 0.1819752037204767, 0.07665976170030937, 0.2664614589053041, 0.15938864886832319, 0.09668554001834957, 0.07157829674683769, 0.28363515560775304, 0.40229595615878094, 0.10448907535255744, 0.14599983105867068, 0.2729530676575518, 0.0278550830864944, 0.2752692258588389, 0.1073852517758628, 0.3113017495747829, 0.18283567997192418, 0.057287665245960524, 0.2123180628433706, 0.15980614158410944, 0.4361095106776312, 0.311591364609161, 0.23434830737722248, 0.3080398896731139, 0.38743350777108554, 0.11295408023488923, 0.24616291838169888, 0.03669248892138574, 0.48072640422824736, 0.2795887916728614, 0.3251493598662945, 0.18261751129306927, 0.1499848009095541, 0.21966418390585413, 0.2205764137481866, 0.4906039953087033, 0.09623075868188449, 0.07917235990589322], 'lossList': [0.0, -1.2579325133562087, 0.0, 8.895938575267792, 0.0, 0.0, 0.0], 'rewardMean': 0.7182569497438097, 'totalEpisodes': 179, 'stepsPerEpisode': 62, 'rewardPerEpisode': 57.644482831004474, 'successfulTests': 25
'totalSteps': 23040, 'rewardStep': 0.4937586272841915, 'errorList': [], 'lossList': [0.0, -1.2432574099302292, 0.0, 3.879072805047035, 0.0, 0.0, 0.0], 'rewardMean': 0.6817987620428594, 'totalEpisodes': 179, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.6621846546575
'totalSteps': 24320, 'rewardStep': 0.735236447897674, 'errorList': [], 'lossList': [0.0, -1.2297587037086486, 0.0, 1.629555508196354, 0.0, 0.0, 0.0], 'rewardMean': 0.6932646363640869, 'totalEpisodes': 179, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.5444750264426
'totalSteps': 25600, 'rewardStep': 0.8812176011113062, 'errorList': [], 'lossList': [0.0, -1.1832529956102371, 0.0, 1.2787244754284621, 0.0, 0.0, 0.0], 'rewardMean': 0.6923430808511558, 'totalEpisodes': 179, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.6565976088614
#maxSuccessfulTests=25, maxSuccessfulTestsAtStep=21760, timeSpent=84.11
