#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 61
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7105809878267002, 'errorList': [], 'lossList': [0.0, -1.4370950800180435, 0.0, 32.55418267250061, 0.0, 0.0, 0.0], 'rewardMean': 0.6136874447630765, 'totalEpisodes': 12, 'stepsPerEpisode': 77, 'rewardPerEpisode': 63.53543528967793
'totalSteps': 3840, 'rewardStep': 0.8547419037741172, 'errorList': [], 'lossList': [0.0, -1.442376959323883, 0.0, 25.48109269440174, 0.0, 0.0, 0.0], 'rewardMean': 0.69403893110009, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 956.9714052929102
'totalSteps': 5120, 'rewardStep': 0.686020243529396, 'errorList': [], 'lossList': [0.0, -1.415510824918747, 0.0, 23.035499017834663, 0.0, 0.0, 0.0], 'rewardMean': 0.6920342592074165, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 982.6497575705911
'totalSteps': 6400, 'rewardStep': 0.6573423682340603, 'errorList': [], 'lossList': [0.0, -1.4056555390357972, 0.0, 27.467387633323668, 0.0, 0.0, 0.0], 'rewardMean': 0.6850958810127452, 'totalEpisodes': 13, 'stepsPerEpisode': 305, 'rewardPerEpisode': 227.8075186387698
'totalSteps': 7680, 'rewardStep': 0.9812331349620221, 'errorList': [171.95502942335682, 149.96453989800557, 177.33617661971422, 163.34785495043857, 170.8416870045062, 156.96183405899433, 132.142222337001, 175.95254886811642, 154.4995756656939, 174.1661727913106, 148.08467880540402, 169.15419850907531, 176.39621802690735, 166.90988648221642, 176.36843885814253, 154.691608669622, 169.10590119697954, 165.49806861324214, 142.8087463473411, 164.36226722204083, 174.41379547346324, 159.86921317967924, 175.94890282919798, 138.86586549509082, 158.89606471716405, 156.83636781128678, 160.0271510430312, 159.2326207631868, 172.64136279549587, 154.07973697577464, 169.12941511184277, 141.0669397739309, 162.50708358271407, 167.1900430237307, 153.21445073473564, 167.36803723497906, 151.53770505616717, 168.39870510737342, 166.61889003860855, 158.31958459593446, 168.3799593646326, 134.88531708283497, 142.3293727231096, 176.68201146644978, 159.83118852773728, 162.29479195295838, 164.84879925925213, 174.12754511354515, 161.9441502345113, 172.25325975336517], 'lossList': [0.0, -1.3748133385181427, 0.0, 56.58607895851135, 0.0, 0.0, 0.0], 'rewardMean': 0.7344520900042913, 'totalEpisodes': 17, 'stepsPerEpisode': 1065, 'rewardPerEpisode': 829.2234029872163, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.7745890841805683, 'errorList': [], 'lossList': [0.0, -1.372731899023056, 0.0, 273.5065675354004, 0.0, 0.0, 0.0], 'rewardMean': 0.7401859463151881, 'totalEpisodes': 62, 'stepsPerEpisode': 41, 'rewardPerEpisode': 36.509736957312526
'totalSteps': 10240, 'rewardStep': 0.5860508963210793, 'errorList': [], 'lossList': [0.0, -1.3671173912286758, 0.0, 139.84107654571534, 0.0, 0.0, 0.0], 'rewardMean': 0.7209190650659245, 'totalEpisodes': 107, 'stepsPerEpisode': 28, 'rewardPerEpisode': 16.622202630492055
'totalSteps': 11520, 'rewardStep': 0.8293142646633604, 'errorList': [], 'lossList': [0.0, -1.3600856828689576, 0.0, 70.46915615081787, 0.0, 0.0, 0.0], 'rewardMean': 0.7329629761323063, 'totalEpisodes': 138, 'stepsPerEpisode': 34, 'rewardPerEpisode': 29.042067899702875
'totalSteps': 12800, 'rewardStep': 0.8035463687099617, 'errorList': [], 'lossList': [0.0, -1.3458217126131058, 0.0, 58.34970652580261, 0.0, 0.0, 0.0], 'rewardMean': 0.7400213153900719, 'totalEpisodes': 158, 'stepsPerEpisode': 35, 'rewardPerEpisode': 24.196040721730714
'totalSteps': 14080, 'rewardStep': 0.947525352602325, 'errorList': [102.88609281883616, 108.03251994117109, 63.532378423296755, 65.43611660571425, 31.311047720463037, 81.05389829102553, 82.53588449034116, 118.54070339866064, 126.28429735157698, 68.4639535975335, 62.56275923175175, 82.93292937197585, 3.6861324253730694, 148.1939612668508, 45.8894634311011, 90.41012124280115, 56.22798110624835, 151.8936200533359, 21.724882718747455, 13.805775573946518, 31.626091610031878, 66.5262124894753, 99.50586846053461, 79.80121027596628, 134.8994813637045, 42.03673137684945, 14.638954057917063, 137.4451812808748, 135.83971148927287, 80.3833234933917, 87.4971277007701, 135.40888073952289, 86.23487093132655, 144.2437424615878, 132.56538170249786, 85.12664903491549, 25.91571774990073, 73.64846040343208, 59.30542099445974, 50.374870560059975, 7.34286530523926, 71.21357973877433, 119.19838374747233, 77.27545425890915, 71.99443334362401, 60.770335371485665, 120.51390475590355, 109.42476908762883, 0.7309856266244847, 53.895700967497795], 'lossList': [0.0, -1.3281531977653502, 0.0, 35.40626790523529, 0.0, 0.0, 0.0], 'rewardMean': 0.783094460480359, 'totalEpisodes': 167, 'stepsPerEpisode': 104, 'rewardPerEpisode': 85.48955640999628, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.4040020498398428, 'errorList': [], 'lossList': [0.0, -1.3212664073705673, 0.0, 13.168060700893403, 0.0, 0.0, 0.0], 'rewardMean': 0.7524365666816732, 'totalEpisodes': 174, 'stepsPerEpisode': 72, 'rewardPerEpisode': 40.084370356854784
'totalSteps': 16640, 'rewardStep': 0.8514902941882038, 'errorList': [], 'lossList': [0.0, -1.3112855887413024, 0.0, 22.477933679819106, 0.0, 0.0, 0.0], 'rewardMean': 0.7521114057230819, 'totalEpisodes': 180, 'stepsPerEpisode': 136, 'rewardPerEpisode': 115.32460284477294
'totalSteps': 17920, 'rewardStep': 0.7027771697676413, 'errorList': [], 'lossList': [0.0, -1.3038297629356383, 0.0, 20.93651278138161, 0.0, 0.0, 0.0], 'rewardMean': 0.7537870983469065, 'totalEpisodes': 186, 'stepsPerEpisode': 146, 'rewardPerEpisode': 107.61536863943186
'totalSteps': 19200, 'rewardStep': 0.7042873228864536, 'errorList': [], 'lossList': [0.0, -1.3039108324050903, 0.0, 5.972543464899063, 0.0, 0.0, 0.0], 'rewardMean': 0.7584815938121457, 'totalEpisodes': 190, 'stepsPerEpisode': 246, 'rewardPerEpisode': 200.8974301452843
'totalSteps': 20480, 'rewardStep': 0.7244333737810768, 'errorList': [], 'lossList': [0.0, -1.3031362360715866, 0.0, 4.876377346515656, 0.0, 0.0, 0.0], 'rewardMean': 0.7328016176940513, 'totalEpisodes': 195, 'stepsPerEpisode': 233, 'rewardPerEpisode': 189.06170538407537
'totalSteps': 21760, 'rewardStep': 0.6821932908416547, 'errorList': [], 'lossList': [0.0, -1.2763973504304886, 0.0, 4.984830486774444, 0.0, 0.0, 0.0], 'rewardMean': 0.72356203836016, 'totalEpisodes': 197, 'stepsPerEpisode': 438, 'rewardPerEpisode': 381.71780766970227
'totalSteps': 23040, 'rewardStep': 0.7446472252985782, 'errorList': [], 'lossList': [0.0, -1.249296534061432, 0.0, 4.5631380927562715, 0.0, 0.0, 0.0], 'rewardMean': 0.7394216712579099, 'totalEpisodes': 198, 'stepsPerEpisode': 182, 'rewardPerEpisode': 150.187926988863
'totalSteps': 24320, 'rewardStep': 0.5978584898634827, 'errorList': [], 'lossList': [0.0, -1.2309523898363113, 0.0, 3.3712831145524977, 0.0, 0.0, 0.0], 'rewardMean': 0.7162760937779221, 'totalEpisodes': 199, 'stepsPerEpisode': 1088, 'rewardPerEpisode': 906.6740799434078
'totalSteps': 25600, 'rewardStep': 0.9448609960279896, 'errorList': [0.03816586497457167, 0.019793007770847682, 0.018192851656181373, 0.024122752964428843, 0.004209617751891011, 0.030835554577116626, 0.024743463750852686, 0.012210057710838126, 0.008952344498512768, 0.043346722399946074, 0.007646825723496391, 0.013879598887027118, 0.017167442985940487, 0.028920655418531702, 0.00926278364441705, 0.011692294023823184, 0.004536836729952227, 0.018138765903756095, 0.011061342029115226, 0.029910771670231512, 0.020508401092206297, 0.008560488659894457, 0.027309570686167038, 0.013602536629636551, 0.013524972270525687, 0.023744207099375286, 0.018273583222707897, 0.03982307551768254, 0.01136490884456473, 0.021793380860801296, 0.01677390584003689, 0.022196903285120438, 0.022895410316411846, 0.030014415740047155, 0.006966266840203619, 0.00788526589993117, 0.008366685648018126, 0.008257261135169688, 0.025746199397156003, 0.03758918178036765, 0.02019379329011126, 0.00543758898961016, 0.024610845112001303, 0.008166133337218794, 0.009140100558278089, 0.011376106753916863, 0.004969563886143298, 0.020429329065554204, 0.02470968264034148, 0.034667075950059266], 'lossList': [0.0, -1.1931169790029525, 0.0, 2.156843929886818, 0.0, 0.0, 0.0], 'rewardMean': 0.7304075565097248, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.9127143365763, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=125.78
