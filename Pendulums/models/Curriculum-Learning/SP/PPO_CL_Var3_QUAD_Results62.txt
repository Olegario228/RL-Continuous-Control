#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 62
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.807776612384743, 'errorList': [], 'lossList': [0.0, -1.44378703892231, 0.0, 25.701321488618852, 0.0, 0.0, 0.0], 'rewardMean': 0.6229036022953085, 'totalEpisodes': 11, 'stepsPerEpisode': 899, 'rewardPerEpisode': 608.7541224022671
'totalSteps': 3840, 'rewardStep': 0.9033372508524196, 'errorList': [], 'lossList': [0.0, -1.4488963854312897, 0.0, 35.37890683174133, 0.0, 0.0, 0.0], 'rewardMean': 0.716381485147679, 'totalEpisodes': 14, 'stepsPerEpisode': 488, 'rewardPerEpisode': 382.8827155594896
'totalSteps': 5120, 'rewardStep': 0.5914355085297514, 'errorList': [], 'lossList': [0.0, -1.4240037655830384, 0.0, 25.94870174407959, 0.0, 0.0, 0.0], 'rewardMean': 0.685144990993197, 'totalEpisodes': 17, 'stepsPerEpisode': 67, 'rewardPerEpisode': 43.3683245106976
'totalSteps': 6400, 'rewardStep': 0.7841016613018372, 'errorList': [], 'lossList': [0.0, -1.410955496430397, 0.0, 53.15079874038696, 0.0, 0.0, 0.0], 'rewardMean': 0.704936325054925, 'totalEpisodes': 23, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.2781734581154236
'totalSteps': 7680, 'rewardStep': 0.8394711495516389, 'errorList': [], 'lossList': [0.0, -1.391617906689644, 0.0, 153.6336865234375, 0.0, 0.0, 0.0], 'rewardMean': 0.7273587958043773, 'totalEpisodes': 44, 'stepsPerEpisode': 49, 'rewardPerEpisode': 34.67305291641176
'totalSteps': 8960, 'rewardStep': 0.5461409367981601, 'errorList': [], 'lossList': [0.0, -1.3843692779541015, 0.0, 175.0158553314209, 0.0, 0.0, 0.0], 'rewardMean': 0.7014705302320606, 'totalEpisodes': 83, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.893144575977601
'totalSteps': 10240, 'rewardStep': 0.4994686480800809, 'errorList': [], 'lossList': [0.0, -1.3843629616498947, 0.0, 89.3291901397705, 0.0, 0.0, 0.0], 'rewardMean': 0.6762202949630631, 'totalEpisodes': 107, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.4994686480800809
'totalSteps': 11520, 'rewardStep': 0.7001863721979279, 'errorList': [], 'lossList': [0.0, -1.397222791314125, 0.0, 49.2628946685791, 0.0, 0.0, 0.0], 'rewardMean': 0.6788831924336036, 'totalEpisodes': 121, 'stepsPerEpisode': 67, 'rewardPerEpisode': 54.86683600944138
'totalSteps': 12800, 'rewardStep': 0.8274923335183558, 'errorList': [], 'lossList': [0.0, -1.3964442145824432, 0.0, 39.7823334312439, 0.0, 0.0, 0.0], 'rewardMean': 0.6937441065420789, 'totalEpisodes': 127, 'stepsPerEpisode': 143, 'rewardPerEpisode': 112.06404519882301
'totalSteps': 14080, 'rewardStep': 0.5956754777630907, 'errorList': [], 'lossList': [0.0, -1.382922541499138, 0.0, 19.77315480709076, 0.0, 0.0, 0.0], 'rewardMean': 0.7095085950978005, 'totalEpisodes': 128, 'stepsPerEpisode': 456, 'rewardPerEpisode': 264.95034923839756
'totalSteps': 15360, 'rewardStep': 0.887756145556468, 'errorList': [], 'lossList': [0.0, -1.3599642604589461, 0.0, 28.736404390335082, 0.0, 0.0, 0.0], 'rewardMean': 0.717506548414973, 'totalEpisodes': 131, 'stepsPerEpisode': 80, 'rewardPerEpisode': 62.25863888882155
'totalSteps': 16640, 'rewardStep': 0.7921070441031401, 'errorList': [], 'lossList': [0.0, -1.3546670055389405, 0.0, 8.302365152835845, 0.0, 0.0, 0.0], 'rewardMean': 0.7063835277400451, 'totalEpisodes': 132, 'stepsPerEpisode': 111, 'rewardPerEpisode': 90.55560001953002
'totalSteps': 17920, 'rewardStep': 0.7920406151797749, 'errorList': [], 'lossList': [0.0, -1.3488701021671294, 0.0, 5.3725612679123875, 0.0, 0.0, 0.0], 'rewardMean': 0.7264440384050475, 'totalEpisodes': 132, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 976.6819344899776
'totalSteps': 19200, 'rewardStep': 0.9208823320818477, 'errorList': [], 'lossList': [0.0, -1.3329920077323913, 0.0, 9.893185911476612, 0.0, 0.0, 0.0], 'rewardMean': 0.7401221054830486, 'totalEpisodes': 133, 'stepsPerEpisode': 758, 'rewardPerEpisode': 631.8186303654498
'totalSteps': 20480, 'rewardStep': 0.8895319095771596, 'errorList': [], 'lossList': [0.0, -1.2944256228208542, 0.0, 4.6967785635590555, 0.0, 0.0, 0.0], 'rewardMean': 0.7451281814856006, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.7616393768824
'totalSteps': 21760, 'rewardStep': 0.8601002479025499, 'errorList': [], 'lossList': [0.0, -1.2399832600355147, 0.0, 3.7392516597360372, 0.0, 0.0, 0.0], 'rewardMean': 0.7765241125960396, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.045247106091
'totalSteps': 23040, 'rewardStep': 0.9464042711769831, 'errorList': [0.037561147493085235, 0.07920909911776293, 0.09824708381412098, 0.03893893654918861, 0.06318274555973895, 0.04611483613711847, 0.02339578953237496, 0.13317306159451742, 0.024147750723646937, 0.07208872192905216, 0.06923625034035209, 0.06178980233782, 0.09693061514409826, 0.0695820968143574, 0.07107402273768215, 0.04277861139030492, 0.08351122860464913, 0.07860506058704127, 0.044723351225154286, 0.061875885504410845, 0.0992973845112428, 0.03788965459119905, 0.030061985920731945, 0.03469262499471014, 0.07691778157003377, 0.04989334800584654, 0.022330850535192008, 0.021948888503734654, 0.09366971283891469, 0.10577873977185265, 0.043424414276921, 0.09878720678673458, 0.1135330080504585, 0.06407304881366527, 0.05709593456018561, 0.09640855097583559, 0.027613958977259073, 0.04130275362125414, 0.09531989403733174, 0.08230458612720529, 0.009639130139937674, 0.061958202046595454, 0.05889428262119773, 0.04558175702406529, 0.04148830895480745, 0.02875931594222422, 0.009753774897233012, 0.05942473053471311, 0.02783146810650209, 0.007418483469424102], 'lossList': [0.0, -1.1910300260782243, 0.0, 1.8432632004469633, 0.0, 0.0, 0.0], 'rewardMean': 0.8212176749057297, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.807945173876, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=78.35
