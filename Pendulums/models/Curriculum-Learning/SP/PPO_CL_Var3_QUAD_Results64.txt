#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 64
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8651594708188728, 'errorList': [], 'lossList': [0.0, -1.4234577441215515, 0.0, 29.696680133342742, 0.0, 0.0, 0.0], 'rewardMean': 0.7685374384811081, 'totalEpisodes': 13, 'stepsPerEpisode': 317, 'rewardPerEpisode': 251.1692087600497
'totalSteps': 3840, 'rewardStep': 0.7158656253590128, 'errorList': [], 'lossList': [0.0, -1.4387696117162705, 0.0, 30.482686364650725, 0.0, 0.0, 0.0], 'rewardMean': 0.7509801674404096, 'totalEpisodes': 16, 'stepsPerEpisode': 172, 'rewardPerEpisode': 128.50603513930847
'totalSteps': 5120, 'rewardStep': 0.5604252458201373, 'errorList': [], 'lossList': [0.0, -1.4421150290966034, 0.0, 19.128466376662253, 0.0, 0.0, 0.0], 'rewardMean': 0.7033414370353415, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 915.175237849209
'totalSteps': 6400, 'rewardStep': 0.7299489762237492, 'errorList': [], 'lossList': [0.0, -1.419771341085434, 0.0, 29.800419883728026, 0.0, 0.0, 0.0], 'rewardMean': 0.7086629448730231, 'totalEpisodes': 18, 'stepsPerEpisode': 867, 'rewardPerEpisode': 631.5300163289547
'totalSteps': 7680, 'rewardStep': 0.819228586209269, 'errorList': [], 'lossList': [0.0, -1.4163153702020645, 0.0, 133.5058437347412, 0.0, 0.0, 0.0], 'rewardMean': 0.7270905517623975, 'totalEpisodes': 33, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.681756308047578
'totalSteps': 8960, 'rewardStep': 0.8790786263082238, 'errorList': [], 'lossList': [0.0, -1.4105128610134126, 0.0, 201.75764072418212, 0.0, 0.0, 0.0], 'rewardMean': 0.7488031338403726, 'totalEpisodes': 82, 'stepsPerEpisode': 29, 'rewardPerEpisode': 21.981515538212545
'totalSteps': 10240, 'rewardStep': 0.8203666698520381, 'errorList': [], 'lossList': [0.0, -1.4052496153116225, 0.0, 118.09490180969239, 0.0, 0.0, 0.0], 'rewardMean': 0.7577485758418308, 'totalEpisodes': 107, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.1506991546793865
'totalSteps': 11520, 'rewardStep': 0.42861127197984433, 'errorList': [], 'lossList': [0.0, -1.3983449059724808, 0.0, 92.48924085617065, 0.0, 0.0, 0.0], 'rewardMean': 0.72117776430161, 'totalEpisodes': 137, 'stepsPerEpisode': 41, 'rewardPerEpisode': 24.774231637394216
'totalSteps': 12800, 'rewardStep': 0.8819946195879234, 'errorList': [], 'lossList': [0.0, -1.392816663980484, 0.0, 61.18997007369995, 0.0, 0.0, 0.0], 'rewardMean': 0.7372594498302414, 'totalEpisodes': 156, 'stepsPerEpisode': 35, 'rewardPerEpisode': 25.278924466360554
'totalSteps': 14080, 'rewardStep': 0.5838788309363716, 'errorList': [], 'lossList': [0.0, -1.3907962542772294, 0.0, 28.375725688934327, 0.0, 0.0, 0.0], 'rewardMean': 0.7284557923095443, 'totalEpisodes': 169, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.0977561080679163
'totalSteps': 15360, 'rewardStep': 0.7631118614740992, 'errorList': [], 'lossList': [0.0, -1.3842902338504792, 0.0, 30.89097201347351, 0.0, 0.0, 0.0], 'rewardMean': 0.7182510313750669, 'totalEpisodes': 179, 'stepsPerEpisode': 64, 'rewardPerEpisode': 51.19430815485586
'totalSteps': 16640, 'rewardStep': 0.6500803795426383, 'errorList': [], 'lossList': [0.0, -1.370980168581009, 0.0, 27.613405857086182, 0.0, 0.0, 0.0], 'rewardMean': 0.7116725067934294, 'totalEpisodes': 188, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.125364371114145
'totalSteps': 17920, 'rewardStep': 0.5617300567281664, 'errorList': [], 'lossList': [0.0, -1.38157790184021, 0.0, 23.789943709373475, 0.0, 0.0, 0.0], 'rewardMean': 0.7118029878842324, 'totalEpisodes': 195, 'stepsPerEpisode': 131, 'rewardPerEpisode': 109.67948661576705
'totalSteps': 19200, 'rewardStep': 0.5409358205825094, 'errorList': [], 'lossList': [0.0, -1.3965012919902802, 0.0, 7.940828393697739, 0.0, 0.0, 0.0], 'rewardMean': 0.6929016723201084, 'totalEpisodes': 200, 'stepsPerEpisode': 273, 'rewardPerEpisode': 222.55525227914757
'totalSteps': 20480, 'rewardStep': 0.929263331534338, 'errorList': [], 'lossList': [0.0, -1.3794685173034669, 0.0, 7.960338164567947, 0.0, 0.0, 0.0], 'rewardMean': 0.7039051468526153, 'totalEpisodes': 204, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.32554625457794
'totalSteps': 21760, 'rewardStep': 0.3038247417358066, 'errorList': [], 'lossList': [0.0, -1.3641662788391113, 0.0, 7.160642060041428, 0.0, 0.0, 0.0], 'rewardMean': 0.6463797583953735, 'totalEpisodes': 206, 'stepsPerEpisode': 252, 'rewardPerEpisode': 185.47231880793018
'totalSteps': 23040, 'rewardStep': 0.7722660343371719, 'errorList': [], 'lossList': [0.0, -1.3563565689325332, 0.0, 28.860133286714554, 0.0, 0.0, 0.0], 'rewardMean': 0.6415696948438869, 'totalEpisodes': 209, 'stepsPerEpisode': 621, 'rewardPerEpisode': 489.1287634369483
'totalSteps': 24320, 'rewardStep': 0.7455949903200747, 'errorList': [], 'lossList': [0.0, -1.3427459442615508, 0.0, 2.8245737341046335, 0.0, 0.0, 0.0], 'rewardMean': 0.6732680666779098, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.4445306199138
'totalSteps': 25600, 'rewardStep': 0.9577370162705823, 'errorList': [0.0691280212819814, 0.06290992741000154, 0.06494030179790007, 0.06036649833904667, 0.06103544061950513, 0.06336716570257876, 0.06360083781725148, 0.06540157007377374, 0.06487346384760996, 0.0603263477705403, 0.06071287873604309, 0.06824293230194657, 0.07344485068491304, 0.06281573275217216, 0.07042572032870731, 0.06796361617981689, 0.06949246269287092, 0.07605807594836027, 0.06782060900698056, 0.06067460038982844, 0.06741175991930734, 0.06593472917303086, 0.06285838273973604, 0.07164512108676499, 0.06498598054638625, 0.06245682960402646, 0.06063472120312586, 0.06761010129472028, 0.06031589131504866, 0.06041839940934037, 0.06916177817891943, 0.06053876672346865, 0.06731603608175574, 0.07760555894337792, 0.06077483596798594, 0.06953893371700963, 0.06540143091682295, 0.07354722654502882, 0.06848168784432174, 0.06614643766297046, 0.06593252116276249, 0.06059879499464071, 0.06959195516366408, 0.06040078017014109, 0.06719128495311107, 0.07565187297896933, 0.07033078265134854, 0.07146264817563154, 0.07582609381852595, 0.0660407561298253], 'lossList': [0.0, -1.3142652356624602, 0.0, 3.166986977607012, 0.0, 0.0, 0.0], 'rewardMean': 0.6808423063461758, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.764668178656, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.83
