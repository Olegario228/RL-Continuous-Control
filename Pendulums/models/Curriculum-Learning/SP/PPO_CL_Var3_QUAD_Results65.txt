#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 65
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9345545582036154, 'errorList': [], 'lossList': [0.0, -1.4349319595098495, 0.0, 30.065678749084473, 0.0, 0.0, 0.0], 'rewardMean': 0.9150680747058912, 'totalEpisodes': 8, 'stepsPerEpisode': 530, 'rewardPerEpisode': 376.0044338435734
'totalSteps': 3840, 'rewardStep': 0.7393538369322503, 'errorList': [], 'lossList': [0.0, -1.4201282250881195, 0.0, 30.27417725086212, 0.0, 0.0, 0.0], 'rewardMean': 0.8564966621146777, 'totalEpisodes': 11, 'stepsPerEpisode': 258, 'rewardPerEpisode': 199.34326991395798
'totalSteps': 5120, 'rewardStep': 0.7362381532440654, 'errorList': [], 'lossList': [0.0, -1.4156903791427613, 0.0, 27.52331710100174, 0.0, 0.0, 0.0], 'rewardMean': 0.8264320348970247, 'totalEpisodes': 12, 'stepsPerEpisode': 473, 'rewardPerEpisode': 365.1364041836242
'totalSteps': 6400, 'rewardStep': 0.9170324595516395, 'errorList': [], 'lossList': [0.0, -1.4032795381546022, 0.0, 28.276030846834182, 0.0, 0.0, 0.0], 'rewardMean': 0.8445521198279475, 'totalEpisodes': 13, 'stepsPerEpisode': 207, 'rewardPerEpisode': 172.5717550758294
'totalSteps': 7680, 'rewardStep': 0.8384845449027623, 'errorList': [], 'lossList': [0.0, -1.3670234549045563, 0.0, 126.9606969833374, 0.0, 0.0, 0.0], 'rewardMean': 0.8435408573404167, 'totalEpisodes': 25, 'stepsPerEpisode': 73, 'rewardPerEpisode': 57.35238315329286
'totalSteps': 8960, 'rewardStep': 0.7161828827470215, 'errorList': [], 'lossList': [0.0, -1.3585347425937653, 0.0, 358.0722250366211, 0.0, 0.0, 0.0], 'rewardMean': 0.8253468609699316, 'totalEpisodes': 76, 'stepsPerEpisode': 27, 'rewardPerEpisode': 19.29223827978685
'totalSteps': 10240, 'rewardStep': 0.9343573408488266, 'errorList': [322.79262236915156, 290.44123016392973, 262.89287789938254, 327.5082248295932, 326.5430404957904, 318.1649299305339, 298.2902173299541, 333.14819691983996, 344.69096290876035, 342.9501714500803, 314.2280857587222, 331.9415493633308, 298.89985150596823, 294.42421617790217, 330.34200652035577, 255.56496881224896, 324.1696912552357, 321.88259206539897, 341.2166018990987, 296.9534944051473, 315.37607524108864, 323.9357839251765, 321.595718866602, 332.0302049954598, 287.64213300254977, 238.4431211477943, 328.28917035915185, 300.5391593962774, 336.33109779059663, 278.8924787737661, 324.4454891592806, 296.0405215731823, 285.8473463504124, 345.65810446137374, 304.20437338714555, 214.18151735831174, 315.3565827574588, 286.4853044300303, 291.42037188534755, 318.5945269099379, 322.5434315605156, 339.2633715290684, 343.663344649024, 327.1590058234057, 304.44662819213073, 330.0610023462333, 303.59632499455864, 306.3131637587221, 322.2038818285439, 348.00686460635455], 'lossList': [0.0, -1.3529608875513077, 0.0, 191.89601184844972, 0.0, 0.0, 0.0], 'rewardMean': 0.8389731709547934, 'totalEpisodes': 112, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9343573408488266, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8630695474218498, 'errorList': [], 'lossList': [0.0, -1.3490370792150497, 0.0, 101.00719512939453, 0.0, 0.0, 0.0], 'rewardMean': 0.8416505461177997, 'totalEpisodes': 140, 'stepsPerEpisode': 42, 'rewardPerEpisode': 32.97450814692781
'totalSteps': 12800, 'rewardStep': 0.7220143818007592, 'errorList': [], 'lossList': [0.0, -1.3538812655210495, 0.0, 95.60610769271851, 0.0, 0.0, 0.0], 'rewardMean': 0.8296869296860956, 'totalEpisodes': 172, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.091292193911082
'totalSteps': 14080, 'rewardStep': 0.5244491629377664, 'errorList': [], 'lossList': [0.0, -1.3466067552566527, 0.0, 48.17881383895874, 0.0, 0.0, 0.0], 'rewardMean': 0.7925736868590556, 'totalEpisodes': 182, 'stepsPerEpisode': 62, 'rewardPerEpisode': 49.025540561851194
'totalSteps': 15360, 'rewardStep': 0.44434498999110494, 'errorList': [], 'lossList': [0.0, -1.3346921694278717, 0.0, 44.652506742477414, 0.0, 0.0, 0.0], 'rewardMean': 0.7435527300378045, 'totalEpisodes': 190, 'stepsPerEpisode': 68, 'rewardPerEpisode': 46.22946346620556
'totalSteps': 16640, 'rewardStep': 0.8191763534756059, 'errorList': [], 'lossList': [0.0, -1.3283723938465117, 0.0, 24.413171303272247, 0.0, 0.0, 0.0], 'rewardMean': 0.7515349816921402, 'totalEpisodes': 194, 'stepsPerEpisode': 83, 'rewardPerEpisode': 70.69524025593066
'totalSteps': 17920, 'rewardStep': 0.6388470349351728, 'errorList': [], 'lossList': [0.0, -1.331281899213791, 0.0, 14.874841237068177, 0.0, 0.0, 0.0], 'rewardMean': 0.7417958698612509, 'totalEpisodes': 195, 'stepsPerEpisode': 912, 'rewardPerEpisode': 646.699201138651
'totalSteps': 19200, 'rewardStep': 0.75189732569635, 'errorList': [], 'lossList': [0.0, -1.319824174642563, 0.0, 29.571844971179964, 0.0, 0.0, 0.0], 'rewardMean': 0.725282356475722, 'totalEpisodes': 198, 'stepsPerEpisode': 818, 'rewardPerEpisode': 664.2259915661639
'totalSteps': 20480, 'rewardStep': 0.6882977219319046, 'errorList': [], 'lossList': [0.0, -1.3174645298719405, 0.0, 10.035831973552703, 0.0, 0.0, 0.0], 'rewardMean': 0.7102636741786362, 'totalEpisodes': 200, 'stepsPerEpisode': 449, 'rewardPerEpisode': 342.34212926384464
'totalSteps': 21760, 'rewardStep': 0.5039942956910433, 'errorList': [], 'lossList': [0.0, -1.315180272459984, 0.0, 7.879878816604614, 0.0, 0.0, 0.0], 'rewardMean': 0.6890448154730384, 'totalEpisodes': 203, 'stepsPerEpisode': 466, 'rewardPerEpisode': 285.1696412371021
'totalSteps': 23040, 'rewardStep': 0.6288435207703886, 'errorList': [], 'lossList': [0.0, -1.306222054362297, 0.0, 8.532779103517532, 0.0, 0.0, 0.0], 'rewardMean': 0.6584934334651946, 'totalEpisodes': 204, 'stepsPerEpisode': 510, 'rewardPerEpisode': 385.74966339275056
'totalSteps': 24320, 'rewardStep': 0.8681908219856709, 'errorList': [], 'lossList': [0.0, -1.3021820098161698, 0.0, 6.690497574806213, 0.0, 0.0, 0.0], 'rewardMean': 0.6590055609215767, 'totalEpisodes': 210, 'stepsPerEpisode': 120, 'rewardPerEpisode': 94.69773529017613
'totalSteps': 25600, 'rewardStep': 0.6608465504717571, 'errorList': [], 'lossList': [0.0, -1.292121097445488, 0.0, 6.395219980478287, 0.0, 0.0, 0.0], 'rewardMean': 0.6528887777886765, 'totalEpisodes': 212, 'stepsPerEpisode': 754, 'rewardPerEpisode': 531.8641272689873
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=85.61
