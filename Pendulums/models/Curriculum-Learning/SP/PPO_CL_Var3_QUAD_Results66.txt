#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 66
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8015297866855966, 'errorList': [], 'lossList': [0.0, -1.4486198735237121, 0.0, 36.239823503494264, 0.0, 0.0, 0.0], 'rewardMean': 0.6800887525809758, 'totalEpisodes': 12, 'stepsPerEpisode': 68, 'rewardPerEpisode': 59.96463364115983
'totalSteps': 3840, 'rewardStep': 0.8932500517485511, 'errorList': [], 'lossList': [0.0, -1.464823386669159, 0.0, 27.71157702445984, 0.0, 0.0, 0.0], 'rewardMean': 0.7511425189701676, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 980.2361007993868
'totalSteps': 5120, 'rewardStep': 0.47713469809521514, 'errorList': [], 'lossList': [0.0, -1.441158178448677, 0.0, 23.586881679296493, 0.0, 0.0, 0.0], 'rewardMean': 0.6826405637514295, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 964.2934244693776
'totalSteps': 6400, 'rewardStep': 0.6961121215462543, 'errorList': [], 'lossList': [0.0, -1.4376786738634109, 0.0, 25.19838971376419, 0.0, 0.0, 0.0], 'rewardMean': 0.6853348753103944, 'totalEpisodes': 13, 'stepsPerEpisode': 304, 'rewardPerEpisode': 229.93553542050265
'totalSteps': 7680, 'rewardStep': 0.9135713507145492, 'errorList': [], 'lossList': [0.0, -1.4150898069143296, 0.0, 53.39556678771973, 0.0, 0.0, 0.0], 'rewardMean': 0.7233742878777535, 'totalEpisodes': 17, 'stepsPerEpisode': 808, 'rewardPerEpisode': 632.2351257696572
'totalSteps': 8960, 'rewardStep': 0.6359150187404587, 'errorList': [], 'lossList': [0.0, -1.4088444995880127, 0.0, 390.84594444274904, 0.0, 0.0, 0.0], 'rewardMean': 0.7108801065724257, 'totalEpisodes': 75, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.533699166486988
'totalSteps': 10240, 'rewardStep': 0.7629769641911182, 'errorList': [], 'lossList': [0.0, -1.4022183722257615, 0.0, 88.4243238067627, 0.0, 0.0, 0.0], 'rewardMean': 0.7173922137747623, 'totalEpisodes': 128, 'stepsPerEpisode': 29, 'rewardPerEpisode': 26.098304018939032
'totalSteps': 11520, 'rewardStep': 0.4876962141775506, 'errorList': [], 'lossList': [0.0, -1.3927833592891694, 0.0, 59.33470821380615, 0.0, 0.0, 0.0], 'rewardMean': 0.6918704360417388, 'totalEpisodes': 165, 'stepsPerEpisode': 32, 'rewardPerEpisode': 20.535298758630777
'totalSteps': 12800, 'rewardStep': 0.6967360499318562, 'errorList': [], 'lossList': [0.0, -1.380787136554718, 0.0, 38.31210213661194, 0.0, 0.0, 0.0], 'rewardMean': 0.6923569974307505, 'totalEpisodes': 186, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.488383673142263
'totalSteps': 14080, 'rewardStep': 0.8790927537131246, 'errorList': [], 'lossList': [0.0, -1.3802770721912383, 0.0, 37.0086802482605, 0.0, 0.0, 0.0], 'rewardMean': 0.7244015009544276, 'totalEpisodes': 198, 'stepsPerEpisode': 26, 'rewardPerEpisode': 19.65140837237737
'totalSteps': 15360, 'rewardStep': 0.8418407694668746, 'errorList': [], 'lossList': [0.0, -1.383640193939209, 0.0, 16.79788593053818, 0.0, 0.0, 0.0], 'rewardMean': 0.7284325992325552, 'totalEpisodes': 202, 'stepsPerEpisode': 80, 'rewardPerEpisode': 67.97340708368927
'totalSteps': 16640, 'rewardStep': 0.6542433288488787, 'errorList': [], 'lossList': [0.0, -1.3572369652986527, 0.0, 22.27905022621155, 0.0, 0.0, 0.0], 'rewardMean': 0.704531926942588, 'totalEpisodes': 205, 'stepsPerEpisode': 111, 'rewardPerEpisode': 84.37336337935373
'totalSteps': 17920, 'rewardStep': 0.7578707087534817, 'errorList': [], 'lossList': [0.0, -1.3327433502674102, 0.0, 16.266623857021333, 0.0, 0.0, 0.0], 'rewardMean': 0.7326055280084146, 'totalEpisodes': 207, 'stepsPerEpisode': 180, 'rewardPerEpisode': 146.21349686181554
'totalSteps': 19200, 'rewardStep': 0.7490467480805358, 'errorList': [], 'lossList': [0.0, -1.30662368953228, 0.0, 3.031033892631531, 0.0, 0.0, 0.0], 'rewardMean': 0.7378989906618427, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 777.5819782082787
'totalSteps': 20480, 'rewardStep': 0.7284168653455443, 'errorList': [], 'lossList': [0.0, -1.2818315762281418, 0.0, 6.871066382527351, 0.0, 0.0, 0.0], 'rewardMean': 0.7193835421249423, 'totalEpisodes': 209, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.260113256315865
'totalSteps': 21760, 'rewardStep': 0.6784976999175578, 'errorList': [], 'lossList': [0.0, -1.2774070656299592, 0.0, 2.3018047964572905, 0.0, 0.0, 0.0], 'rewardMean': 0.7236418102426522, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 923.9174758680523
'totalSteps': 23040, 'rewardStep': 0.7917207064768137, 'errorList': [], 'lossList': [0.0, -1.2665507149696351, 0.0, 3.1842294809222222, 0.0, 0.0, 0.0], 'rewardMean': 0.7265161844712218, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.6053072314087
'totalSteps': 24320, 'rewardStep': 0.6391579636002805, 'errorList': [], 'lossList': [0.0, -1.2536047959327699, 0.0, 1.9539187170937657, 0.0, 0.0, 0.0], 'rewardMean': 0.7416623594134948, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.829638579437
'totalSteps': 25600, 'rewardStep': 0.9767007337152049, 'errorList': [0.36867772211283817, 0.19572724533223157, 0.30726482902597496, 0.2433136578798367, 0.2986735866691997, 0.19322248371239292, 0.2537443993572154, 0.2356699081465215, 0.26536099718617023, 0.2832932484763276, 0.3561717695250225, 0.17560927446374236, 0.2731231294056159, 0.4044303538111822, 0.30841520463171507, 0.282515498301919, 0.30519663790940094, 0.2670553574168608, 0.26165420388496907, 0.18147097855602767, 0.23004255499464893, 0.2291324691018429, 0.30657958284907366, 0.28490770134825605, 0.2990320109453749, 0.4119716524026438, 0.38892202022051864, 0.2049863182606196, 0.30403644672873614, 0.2762283639136836, 0.2442307601930663, 0.3030408035879006, 0.29429767861705736, 0.2697440953965651, 0.3430662575823055, 0.2508122347458026, 0.34512566263484873, 0.28412744246057103, 0.24125992795913725, 0.2662081475280991, 0.3250926545295139, 0.34803332007543347, 0.2492907235428422, 0.34934037637817056, 0.45477345410375636, 0.35167726960911905, 0.2600137597736479, 0.2394646105164469, 0.28995478972457195, 0.24209806193351513], 'lossList': [0.0, -1.2210363250970842, 0.0, 2.1991035345196726, 0.0, 0.0, 0.0], 'rewardMean': 0.7696588277918297, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.384848897151, 'successfulTests': 4
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=25600, timeSpent=83.32
