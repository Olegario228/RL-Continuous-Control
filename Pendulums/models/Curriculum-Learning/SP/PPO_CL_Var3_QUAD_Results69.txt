#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 69
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8752933981311011, 'errorList': [], 'lossList': [0.0, -1.427434111237526, 0.0, 33.51608197450638, 0.0, 0.0, 0.0], 'rewardMean': 0.799322161005201, 'totalEpisodes': 13, 'stepsPerEpisode': 314, 'rewardPerEpisode': 261.0244020696477
'totalSteps': 3840, 'rewardStep': 0.8245451488565223, 'errorList': [], 'lossList': [0.0, -1.4395597487688065, 0.0, 32.0702174282074, 0.0, 0.0, 0.0], 'rewardMean': 0.8077298236223082, 'totalEpisodes': 16, 'stepsPerEpisode': 166, 'rewardPerEpisode': 128.23588726346082
'totalSteps': 5120, 'rewardStep': 0.578260501180577, 'errorList': [], 'lossList': [0.0, -1.4416165137290955, 0.0, 18.94565005540848, 0.0, 0.0, 0.0], 'rewardMean': 0.7503624930118753, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.4384629795944
'totalSteps': 6400, 'rewardStep': 0.906373401875768, 'errorList': [], 'lossList': [0.0, -1.4267590165138244, 0.0, 49.8840367937088, 0.0, 0.0, 0.0], 'rewardMean': 0.7815646747846539, 'totalEpisodes': 21, 'stepsPerEpisode': 35, 'rewardPerEpisode': 27.033954220721238
'totalSteps': 7680, 'rewardStep': 0.7793092968844905, 'errorList': [], 'lossList': [0.0, -1.4240055358409882, 0.0, 169.65171440124513, 0.0, 0.0, 0.0], 'rewardMean': 0.78118877846796, 'totalEpisodes': 46, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.231751867082906
'totalSteps': 8960, 'rewardStep': 0.6301721550116082, 'errorList': [], 'lossList': [0.0, -1.415656504034996, 0.0, 153.30839275360108, 0.0, 0.0, 0.0], 'rewardMean': 0.7596149751170526, 'totalEpisodes': 117, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.701403656909268
'totalSteps': 10240, 'rewardStep': 0.8545836374179412, 'errorList': [], 'lossList': [0.0, -1.3968703651428223, 0.0, 75.12353986740112, 0.0, 0.0, 0.0], 'rewardMean': 0.7714860579046636, 'totalEpisodes': 163, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.435358080120691
'totalSteps': 11520, 'rewardStep': 0.7273489333282355, 'errorList': [], 'lossList': [0.0, -1.3901127868890761, 0.0, 56.46576261520386, 0.0, 0.0, 0.0], 'rewardMean': 0.7665819329517272, 'totalEpisodes': 184, 'stepsPerEpisode': 60, 'rewardPerEpisode': 51.190305143580076
'totalSteps': 12800, 'rewardStep': 0.77672355805619, 'errorList': [], 'lossList': [0.0, -1.3897578543424607, 0.0, 39.040782318115234, 0.0, 0.0, 0.0], 'rewardMean': 0.7675960954621734, 'totalEpisodes': 197, 'stepsPerEpisode': 62, 'rewardPerEpisode': 51.2898035005941
'totalSteps': 14080, 'rewardStep': 0.6558461783111638, 'errorList': [], 'lossList': [0.0, -1.3861758160591124, 0.0, 22.563993089199066, 0.0, 0.0, 0.0], 'rewardMean': 0.7608456209053598, 'totalEpisodes': 202, 'stepsPerEpisode': 98, 'rewardPerEpisode': 66.84154211406184
'totalSteps': 15360, 'rewardStep': 0.4596552140135494, 'errorList': [], 'lossList': [0.0, -1.390096861720085, 0.0, 40.296964869499206, 0.0, 0.0, 0.0], 'rewardMean': 0.7192818024936046, 'totalEpisodes': 213, 'stepsPerEpisode': 77, 'rewardPerEpisode': 52.66746538814591
'totalSteps': 16640, 'rewardStep': 0.9554481349528958, 'errorList': [108.18623639420896, 122.05942506505205, 28.136553513520596, 143.52108929410144, 100.61460276914819, 117.220838527671, 24.124557846204134, 91.0664307407986, 66.91138957356372, 136.7454489843661, 147.99412805393112, 53.22984280304655, 9.782404843019242, 84.16449974255235, 90.01421755788971, 95.77009269102541, 7.310746646712068, 156.76646507790582, 1.6949771675049379, 121.72369626449247, 22.429176498040604, 68.88668975123994, 143.94848146373033, 132.38621884475882, 95.36442641323615, 96.65626875455746, 82.67791512106344, 153.90675223147977, 15.034025010055474, 60.949324787623986, 147.4289857618867, 78.58321104386214, 111.80321002808473, 98.28712533750995, 10.472640782123632, 115.97831684088644, 132.75755866842056, 112.6730680435778, 64.02631136121106, 100.2414853668405, 39.32245809035292, 38.57046933323715, 132.23935240308037, 124.68493508429479, 3.382712003754619, 18.32996997633445, 85.23709407709059, 74.52177456150112, 114.24145585841124, 7.2088718120629895], 'lossList': [0.0, -1.4028937256336211, 0.0, 23.968258364200594, 0.0, 0.0, 0.0], 'rewardMean': 0.7323721011032419, 'totalEpisodes': 222, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.7771073567526514, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.6944179489780653, 'errorList': [], 'lossList': [0.0, -1.4184190553426743, 0.0, 9.808846199512482, 0.0, 0.0, 0.0], 'rewardMean': 0.7439878458829907, 'totalEpisodes': 228, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.59574702773152
'totalSteps': 19200, 'rewardStep': 0.943830316360454, 'errorList': [0.3836323824302598, 0.23676661175380534, 0.05757075488005576, 0.8262265043649276, 0.6568255554979507, 0.13349991846011636, 0.3308875085080921, 0.034196708910042894, 0.0862460632814253, 0.5013843959225258, 0.11387014920358063, 0.4290085965122676, 0.4369419805001843, 0.21227861112982135, 0.19829450810503946, 0.7576049054079184, 1.1397529977163856, 0.039612839485772225, 0.15536687268715105, 0.2025445013781923, 0.08674709309817273, 0.1279025658929247, 0.27296821794077214, 0.20398726580303578, 0.20127775617083216, 0.1430314678802769, 0.13007206854304987, 0.05810984590830488, 0.716799400658644, 0.1404689891679532, 0.1891389323315337, 0.05807451315276778, 0.537977328257572, 0.7462163190299216, 0.24447982376984678, 0.08351540365835468, 0.3386467843199193, 0.2744263862922058, 0.39156677313616917, 0.7196311057143351, 0.25725188391903003, 0.39755495348259734, 1.248881550440575, 0.871297389448132, 0.05386898496871883, 0.6586929704816461, 0.17837548610077553, 0.10449616703806237, 0.403374530740031, 0.7241106619034843], 'lossList': [0.0, -1.4271817046403885, 0.0, 10.565372607707978, 0.0, 0.0, 0.0], 'rewardMean': 0.7477335373314593, 'totalEpisodes': 233, 'stepsPerEpisode': 28, 'rewardPerEpisode': 25.668898461376003, 'successfulTests': 20
'totalSteps': 20480, 'rewardStep': 0.9231491491316078, 'errorList': [], 'lossList': [0.0, -1.4076821249723435, 0.0, 7.746380026340485, 0.0, 0.0, 0.0], 'rewardMean': 0.7621175225561712, 'totalEpisodes': 234, 'stepsPerEpisode': 887, 'rewardPerEpisode': 694.5986354756279
'totalSteps': 21760, 'rewardStep': 0.8142966287144133, 'errorList': [], 'lossList': [0.0, -1.365923546552658, 0.0, 5.52807713329792, 0.0, 0.0, 0.0], 'rewardMean': 0.7805299699264516, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.4649681887036
'totalSteps': 23040, 'rewardStep': 0.9142050602222135, 'errorList': [], 'lossList': [0.0, -1.3361217510700225, 0.0, 4.090323362648487, 0.0, 0.0, 0.0], 'rewardMean': 0.7864921122068789, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.7471985211084
'totalSteps': 24320, 'rewardStep': 0.8601477517436362, 'errorList': [], 'lossList': [0.0, -1.3154679656028747, 0.0, 2.5787640080600975, 0.0, 0.0, 0.0], 'rewardMean': 0.799771994048419, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1160.0419404428394
'totalSteps': 25600, 'rewardStep': 0.9880893769033591, 'errorList': [0.06789225303722388, 0.05355300105441101, 0.08213309059440017, 0.05986160311491283, 0.053583167418193696, 0.06059668265703763, 0.05372664074005606, 0.05305601198168913, 0.05409117380917073, 0.05407097502893066, 0.053126207166545716, 0.05350844840615472, 0.053823439047090325, 0.05410764483001097, 0.05337431078584274, 0.09311778545261026, 0.05409776481983838, 0.053571682688902014, 0.0939680253663094, 0.05378939558828669, 0.05408845655312702, 0.05359584860697156, 0.0778020540791679, 0.08450131062260632, 0.06914204648219296, 0.05366989584719986, 0.05412631112308169, 0.06512998814654397, 0.05782758940102432, 0.053564940014079715, 0.053309867096755836, 0.07404820327887995, 0.053572317239242134, 0.08619151567874374, 0.053826962663750366, 0.05379934865991541, 0.05308690336061782, 0.05346293662130073, 0.055045327936516286, 0.05313434346521712, 0.05378366816782853, 0.0645715428947536, 0.054063582444877904, 0.05350733030877189, 0.05311521821109834, 0.053521134757234586, 0.054081768386712276, 0.05345710313271524, 0.05368599127489548, 0.053569961262610856], 'lossList': [0.0, -1.2888810950517655, 0.0, 2.4703675030916927, 0.0, 0.0, 0.0], 'rewardMean': 0.8209085759331358, 'totalEpisodes': 234, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1194.5878901403923, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=124.42
