#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 7
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.49565826466689694, 'errorList': [], 'lossList': [0.0, -1.4228890037536621, 0.0, 25.943651375770568, 0.0, 0.0, 0.0], 'rewardMean': 0.6447141329235241, 'totalEpisodes': 24, 'stepsPerEpisode': 133, 'rewardPerEpisode': 75.6847763449928
'totalSteps': 3840, 'rewardStep': 0.7209202237488793, 'errorList': [], 'lossList': [0.0, -1.4219945669174194, 0.0, 41.1178478717804, 0.0, 0.0, 0.0], 'rewardMean': 0.6701161631986424, 'totalEpisodes': 33, 'stepsPerEpisode': 30, 'rewardPerEpisode': 21.501447713799553
'totalSteps': 5120, 'rewardStep': 0.41341804330483506, 'errorList': [], 'lossList': [0.0, -1.398641682267189, 0.0, 60.35824381828308, 0.0, 0.0, 0.0], 'rewardMean': 0.6059416332251906, 'totalEpisodes': 51, 'stepsPerEpisode': 49, 'rewardPerEpisode': 31.662479393000268
'totalSteps': 6400, 'rewardStep': 0.7392959588452014, 'errorList': [], 'lossList': [0.0, -1.3936358487606049, 0.0, 112.40686943054199, 0.0, 0.0, 0.0], 'rewardMean': 0.6326124983491928, 'totalEpisodes': 98, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.24047606399841
'totalSteps': 7680, 'rewardStep': 0.6606935312004348, 'errorList': [], 'lossList': [0.0, -1.3920084255933762, 0.0, 71.02574853897094, 0.0, 0.0, 0.0], 'rewardMean': 0.6372926704910664, 'totalEpisodes': 133, 'stepsPerEpisode': 59, 'rewardPerEpisode': 46.41363582301003
'totalSteps': 8960, 'rewardStep': 0.8855454790521619, 'errorList': [], 'lossList': [0.0, -1.386187850832939, 0.0, 44.19239174842834, 0.0, 0.0, 0.0], 'rewardMean': 0.6727573574283657, 'totalEpisodes': 148, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.600495169410221
'totalSteps': 10240, 'rewardStep': 0.6652488594813638, 'errorList': [], 'lossList': [0.0, -1.3826978117227555, 0.0, 37.98600779056549, 0.0, 0.0, 0.0], 'rewardMean': 0.6718187951849905, 'totalEpisodes': 156, 'stepsPerEpisode': 130, 'rewardPerEpisode': 103.76806970821694
'totalSteps': 11520, 'rewardStep': 0.7896643810478448, 'errorList': [], 'lossList': [0.0, -1.3847752767801285, 0.0, 15.726421990394591, 0.0, 0.0, 0.0], 'rewardMean': 0.6849127491697522, 'totalEpisodes': 164, 'stepsPerEpisode': 124, 'rewardPerEpisode': 104.62010360267888
'totalSteps': 12800, 'rewardStep': 0.9426174890401323, 'errorList': [83.15411219507538, 48.42839473964054, 21.697103932277177, 8.114183451031458, 31.65343317052832, 86.691939732821, 41.096941006563156, 29.29459550219455, 122.20828692070687, 7.735719451321926, 4.872133588118483, 6.142210476776253, 0.1846245077498714, 25.781124992480105, 5.508424744688209, 0.16358467699209323, 109.22502785289612, 92.63657173775232, 13.356389931599674, 72.20757294929217, 48.81272556492113, 101.85687516534678, 27.369487085199804, 62.84693064938106, 16.735912652646114, 30.696489948546397, 76.85981643495491, 28.286469997162445, 72.60141788662945, 8.146101577992775, 67.9885100040627, 13.19462761180641, 12.062042750301273, 4.436283600002446, 37.81804249338421, 66.24684191738747, 46.68416630213474, 86.44289522155174, 1.2876579720925865, 6.9435385854495895, 78.99194757895262, 100.90430573496369, 30.034784957544044, 7.2851823673891785, 27.677442688340005, 27.079325058471557, 77.56441199332914, 71.7261500997461, 47.77709134841262, 16.799690680896358], 'lossList': [0.0, -1.3691918277740478, 0.0, 27.403366327285767, 0.0, 0.0, 0.0], 'rewardMean': 0.7106832231567901, 'totalEpisodes': 170, 'stepsPerEpisode': 141, 'rewardPerEpisode': 124.05284713453393, 'successfulTests': 2
'totalSteps': 14080, 'rewardStep': 0.7607255133118919, 'errorList': [], 'lossList': [0.0, -1.359139276742935, 0.0, 6.0408970010280605, 0.0, 0.0, 0.0], 'rewardMean': 0.7073787743699642, 'totalEpisodes': 177, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.62675821636777
'totalSteps': 15360, 'rewardStep': 0.996168908079383, 'errorList': [1.517103390986179, 6.045994797892535, 6.694990437351465, 2.994633038352131, 10.903951022403252, 0.07941307601025739, 5.52631879903268, 5.201527766789289, 4.308372163371484, 0.7030316747528748, 0.7743825569971827, 0.916021565913752, 3.2363325311151923, 3.7770724154665256, 1.5706550881577692, 0.32514609967194213, 2.806726595838183, 0.6351591791454525, 1.2065812329310617, 5.26911797717322, 0.5956791849241854, 2.1298798564270007, 3.243356017382504, 1.661552750412891, 1.2437419122692173, 3.2924070874336584, 5.249052485633768, 1.294033742180118, 0.102716987124614, 6.226565612207941, 0.4425373158513809, 0.6921716403138742, 2.7151574313164923, 0.5894015270602649, 3.5170143121922526, 7.4282971729273655, 0.49535666689374014, 4.226784053847959, 1.144925505025341, 7.900613574663537, 0.5730288524577726, 1.5001830540643637, 2.2224547028267922, 4.179785869008218, 4.409883493560593, 0.06891230049448147, 5.827930957030603, 0.32217156602452357, 3.8944119028906408, 4.641444056990526], 'lossList': [0.0, -1.3678964531421662, 0.0, 27.622233231067657, 0.0, 0.0, 0.0], 'rewardMean': 0.7574298387112128, 'totalEpisodes': 182, 'stepsPerEpisode': 178, 'rewardPerEpisode': 144.06548351174166, 'successfulTests': 3
'totalSteps': 16640, 'rewardStep': 0.4965271515331107, 'errorList': [], 'lossList': [0.0, -1.3809326314926147, 0.0, 9.981815203428269, 0.0, 0.0, 0.0], 'rewardMean': 0.734990531489636, 'totalEpisodes': 186, 'stepsPerEpisode': 180, 'rewardPerEpisode': 122.86100110991207
'totalSteps': 17920, 'rewardStep': 0.7738876870567158, 'errorList': [], 'lossList': [0.0, -1.389932962656021, 0.0, 4.250037977397442, 0.0, 0.0, 0.0], 'rewardMean': 0.7710374958648241, 'totalEpisodes': 189, 'stepsPerEpisode': 384, 'rewardPerEpisode': 310.5226581169516
'totalSteps': 19200, 'rewardStep': 0.8404126048939246, 'errorList': [], 'lossList': [0.0, -1.3727528327703475, 0.0, 7.534805227220058, 0.0, 0.0, 0.0], 'rewardMean': 0.7811491604696964, 'totalEpisodes': 191, 'stepsPerEpisode': 361, 'rewardPerEpisode': 306.7154799646603
'totalSteps': 20480, 'rewardStep': 0.7641591221881059, 'errorList': [], 'lossList': [0.0, -1.3533561545610429, 0.0, 6.48177610039711, 0.0, 0.0, 0.0], 'rewardMean': 0.7914957195684635, 'totalEpisodes': 193, 'stepsPerEpisode': 264, 'rewardPerEpisode': 224.69482485724103
'totalSteps': 21760, 'rewardStep': 0.7496154832109159, 'errorList': [], 'lossList': [0.0, -1.3292420619726182, 0.0, 2.671323142647743, 0.0, 0.0, 0.0], 'rewardMean': 0.7779027199843388, 'totalEpisodes': 195, 'stepsPerEpisode': 163, 'rewardPerEpisode': 132.72919793410912
'totalSteps': 23040, 'rewardStep': 0.9119117683529059, 'errorList': [], 'lossList': [0.0, -1.3196319502592087, 0.0, 4.284514430761337, 0.0, 0.0, 0.0], 'rewardMean': 0.8025690108714931, 'totalEpisodes': 196, 'stepsPerEpisode': 458, 'rewardPerEpisode': 413.8602691107722
'totalSteps': 24320, 'rewardStep': 0.9283273127281809, 'errorList': [], 'lossList': [0.0, -1.300287230014801, 0.0, 1.0730706810951234, 0.0, 0.0, 0.0], 'rewardMean': 0.8164353040395266, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.375217760091
'totalSteps': 25600, 'rewardStep': 0.7924332086872083, 'errorList': [], 'lossList': [0.0, -1.25591137945652, 0.0, 0.8468888371437788, 0.0, 0.0, 0.0], 'rewardMean': 0.8014168760042344, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.064625226867
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=15360, timeSpent=94.94
