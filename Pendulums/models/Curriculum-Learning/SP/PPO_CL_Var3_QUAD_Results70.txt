#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 70
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.905335996264536, 'errorList': [], 'lossList': [0.0, -1.4370191651582718, 0.0, 31.583871287107467, 0.0, 0.0, 0.0], 'rewardMean': 0.910078313422682, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 381.19190180865024
'totalSteps': 3840, 'rewardStep': 0.7090522725725202, 'errorList': [], 'lossList': [0.0, -1.422214732170105, 0.0, 29.576819164752962, 0.0, 0.0, 0.0], 'rewardMean': 0.8430696331392947, 'totalEpisodes': 12, 'stepsPerEpisode': 267, 'rewardPerEpisode': 206.40675205671823
'totalSteps': 5120, 'rewardStep': 0.7162773421931227, 'errorList': [], 'lossList': [0.0, -1.4208389496803284, 0.0, 22.5294237613678, 0.0, 0.0, 0.0], 'rewardMean': 0.8113715604027517, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 954.6275396368728
'totalSteps': 6400, 'rewardStep': 0.8470416433805444, 'errorList': [], 'lossList': [0.0, -1.4120034915208817, 0.0, 39.91465753316879, 0.0, 0.0, 0.0], 'rewardMean': 0.8185055769983103, 'totalEpisodes': 15, 'stepsPerEpisode': 549, 'rewardPerEpisode': 434.22519183357616
'totalSteps': 7680, 'rewardStep': 0.8070539616644141, 'errorList': [], 'lossList': [0.0, -1.4007456177473068, 0.0, 152.87608280181885, 0.0, 0.0, 0.0], 'rewardMean': 0.816596974442661, 'totalEpisodes': 30, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.755298679965497
'totalSteps': 8960, 'rewardStep': 0.7018881108352154, 'errorList': [], 'lossList': [0.0, -1.396055184006691, 0.0, 269.97846992492674, 0.0, 0.0, 0.0], 'rewardMean': 0.8002099939273116, 'totalEpisodes': 87, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.990960699467495
'totalSteps': 10240, 'rewardStep': 0.9608480545400494, 'errorList': [194.1458357782303, 196.62716805764154, 198.85940806434547, 203.41373748770712, 185.39241486103938, 213.3588910480111, 215.80106431049956, 192.97048752483846, 193.36039801496736, 206.15061698050815, 150.7474013893054, 220.305253315237, 197.42467710852074, 217.48669390611232, 196.84774201916036, 204.3598047303844, 172.9393896721325, 192.4626215269361, 218.19518209877054, 177.24386905092524, 208.13143147728746, 209.16449423006262, 217.0935590130215, 192.86960212951314, 202.33902594168526, 189.3626188344786, 180.7269155848614, 204.18102689502263, 219.14142972088754, 196.0147742446947, 181.14876598592465, 206.84528853021476, 212.40751416778238, 198.65144250380274, 192.79718912416223, 188.15707174717417, 204.54553751647555, 194.0851564469876, 200.21015844606177, 191.28767485864674, 200.5505667700505, 213.34055575559054, 188.78194463758615, 205.5620930647607, 184.5408803372341, 209.3604775314729, 172.65952247496878, 213.52954007935273, 185.279650066703, 205.17723566816375], 'lossList': [0.0, -1.389606603384018, 0.0, 93.05460437774659, 0.0, 0.0, 0.0], 'rewardMean': 0.8202897515039038, 'totalEpisodes': 145, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9608480545400494, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.7181729396009386, 'errorList': [], 'lossList': [0.0, -1.3754073673486709, 0.0, 59.66638887405396, 0.0, 0.0, 0.0], 'rewardMean': 0.808943439070241, 'totalEpisodes': 175, 'stepsPerEpisode': 52, 'rewardPerEpisode': 46.3419408458626
'totalSteps': 12800, 'rewardStep': 0.7736287328960172, 'errorList': [], 'lossList': [0.0, -1.347533488869667, 0.0, 51.707894277572635, 0.0, 0.0, 0.0], 'rewardMean': 0.8054119684528185, 'totalEpisodes': 192, 'stepsPerEpisode': 138, 'rewardPerEpisode': 113.48870267339015
'totalSteps': 14080, 'rewardStep': 0.8404131030105728, 'errorList': [], 'lossList': [0.0, -1.3241888111829758, 0.0, 43.56930818557739, 0.0, 0.0, 0.0], 'rewardMean': 0.7979712156957932, 'totalEpisodes': 208, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.005927969666473
'totalSteps': 15360, 'rewardStep': 0.3911673834876416, 'errorList': [], 'lossList': [0.0, -1.3222217053174972, 0.0, 34.74687263011933, 0.0, 0.0, 0.0], 'rewardMean': 0.7465543544181037, 'totalEpisodes': 220, 'stepsPerEpisode': 68, 'rewardPerEpisode': 47.93354994868767
'totalSteps': 16640, 'rewardStep': 0.18556252642823762, 'errorList': [], 'lossList': [0.0, -1.316049369573593, 0.0, 17.80389580965042, 0.0, 0.0, 0.0], 'rewardMean': 0.6942053798036755, 'totalEpisodes': 227, 'stepsPerEpisode': 290, 'rewardPerEpisode': 227.6810204420405
'totalSteps': 17920, 'rewardStep': 0.6302912281194069, 'errorList': [], 'lossList': [0.0, -1.3122055232524872, 0.0, 25.823714385032655, 0.0, 0.0, 0.0], 'rewardMean': 0.6856067683963037, 'totalEpisodes': 233, 'stepsPerEpisode': 165, 'rewardPerEpisode': 128.0452588093194
'totalSteps': 19200, 'rewardStep': 0.8551550395137458, 'errorList': [], 'lossList': [0.0, -1.3047516751289367, 0.0, 8.556128647327423, 0.0, 0.0, 0.0], 'rewardMean': 0.6864181080096239, 'totalEpisodes': 238, 'stepsPerEpisode': 169, 'rewardPerEpisode': 146.56651752507756
'totalSteps': 20480, 'rewardStep': 0.5965120893880648, 'errorList': [], 'lossList': [0.0, -1.292807007431984, 0.0, 5.659325534105301, 0.0, 0.0, 0.0], 'rewardMean': 0.665363920781989, 'totalEpisodes': 243, 'stepsPerEpisode': 302, 'rewardPerEpisode': 225.04603712913513
'totalSteps': 21760, 'rewardStep': 0.7288253442154515, 'errorList': [], 'lossList': [0.0, -1.2804708993434906, 0.0, 7.1535980725288395, 0.0, 0.0, 0.0], 'rewardMean': 0.6680576441200126, 'totalEpisodes': 246, 'stepsPerEpisode': 485, 'rewardPerEpisode': 409.90905364657874
'totalSteps': 23040, 'rewardStep': 0.5042188951468208, 'errorList': [], 'lossList': [0.0, -1.2681820565462112, 0.0, 5.422788018286228, 0.0, 0.0, 0.0], 'rewardMean': 0.6223947281806896, 'totalEpisodes': 247, 'stepsPerEpisode': 1206, 'rewardPerEpisode': 1022.7850871132858
'totalSteps': 24320, 'rewardStep': 0.7874004283592813, 'errorList': [], 'lossList': [0.0, -1.2309894013404845, 0.0, 3.7376751786470415, 0.0, 0.0, 0.0], 'rewardMean': 0.629317477056524, 'totalEpisodes': 250, 'stepsPerEpisode': 169, 'rewardPerEpisode': 142.39454085794273
'totalSteps': 25600, 'rewardStep': 0.400186586822472, 'errorList': [], 'lossList': [0.0, -1.2087524461746215, 0.0, 3.18543086707592, 0.0, 0.0, 0.0], 'rewardMean': 0.5919732624491696, 'totalEpisodes': 252, 'stepsPerEpisode': 349, 'rewardPerEpisode': 268.36562360046474
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.15
