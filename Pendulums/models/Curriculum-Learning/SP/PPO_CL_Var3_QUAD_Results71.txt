#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 71
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8204261166263663, 'errorList': [], 'lossList': [0.0, -1.449988792538643, 0.0, 38.92621943950653, 0.0, 0.0, 0.0], 'rewardMean': 0.7068019681032629, 'totalEpisodes': 12, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.43897786827749
'totalSteps': 3840, 'rewardStep': 0.9015443122286064, 'errorList': [], 'lossList': [0.0, -1.4658267849683762, 0.0, 31.58429461479187, 0.0, 0.0, 0.0], 'rewardMean': 0.7717160828117108, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1014.943090597172
'totalSteps': 5120, 'rewardStep': 0.560819338910804, 'errorList': [], 'lossList': [0.0, -1.4486551773548126, 0.0, 26.729035602807997, 0.0, 0.0, 0.0], 'rewardMean': 0.7189918968364841, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.5115088954283
'totalSteps': 6400, 'rewardStep': 0.8703073369963358, 'errorList': [], 'lossList': [0.0, -1.4439935100078582, 0.0, 19.430842732191085, 0.0, 0.0, 0.0], 'rewardMean': 0.7492549848684544, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.6381036834193
'totalSteps': 7680, 'rewardStep': 0.8581367179728708, 'errorList': [], 'lossList': [0.0, -1.4167369985580445, 0.0, 36.51939775407314, 0.0, 0.0, 0.0], 'rewardMean': 0.7674019403858572, 'totalEpisodes': 14, 'stepsPerEpisode': 1136, 'rewardPerEpisode': 921.5935170432372
'totalSteps': 8960, 'rewardStep': 0.5839741614748047, 'errorList': [], 'lossList': [0.0, -1.4133916026353837, 0.0, 470.7241946411133, 0.0, 0.0, 0.0], 'rewardMean': 0.7411979719699925, 'totalEpisodes': 59, 'stepsPerEpisode': 41, 'rewardPerEpisode': 27.020513891081976
'totalSteps': 10240, 'rewardStep': 0.8018782414052791, 'errorList': [], 'lossList': [0.0, -1.411831864118576, 0.0, 216.80756401062013, 0.0, 0.0, 0.0], 'rewardMean': 0.7487830056494034, 'totalEpisodes': 108, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.5879498261397975
'totalSteps': 11520, 'rewardStep': 0.6736627966852469, 'errorList': [], 'lossList': [0.0, -1.411698079109192, 0.0, 92.29013715744018, 0.0, 0.0, 0.0], 'rewardMean': 0.7404363157644971, 'totalEpisodes': 154, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.784698527544339
'totalSteps': 12800, 'rewardStep': 0.9111269988761406, 'errorList': [], 'lossList': [0.0, -1.4141845214366913, 0.0, 61.772260208129886, 0.0, 0.0, 0.0], 'rewardMean': 0.7575053840756614, 'totalEpisodes': 194, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.320049340941594
'totalSteps': 14080, 'rewardStep': 0.46882407903852186, 'errorList': [], 'lossList': [0.0, -1.4080755853652953, 0.0, 41.287956972122196, 0.0, 0.0, 0.0], 'rewardMean': 0.7450700100214976, 'totalEpisodes': 211, 'stepsPerEpisode': 95, 'rewardPerEpisode': 77.17755015697091
'totalSteps': 15360, 'rewardStep': 0.7976017397243407, 'errorList': [], 'lossList': [0.0, -1.4015851604938507, 0.0, 20.04025744199753, 0.0, 0.0, 0.0], 'rewardMean': 0.7427875723312951, 'totalEpisodes': 220, 'stepsPerEpisode': 130, 'rewardPerEpisode': 104.77495484672157
'totalSteps': 16640, 'rewardStep': 0.9286182921627428, 'errorList': [], 'lossList': [0.0, -1.391509899497032, 0.0, 13.30585012435913, 0.0, 0.0, 0.0], 'rewardMean': 0.7454949703247087, 'totalEpisodes': 228, 'stepsPerEpisode': 24, 'rewardPerEpisode': 22.362310124086278
'totalSteps': 17920, 'rewardStep': 0.5777297780865134, 'errorList': [], 'lossList': [0.0, -1.3839868170022964, 0.0, 13.185559486150742, 0.0, 0.0, 0.0], 'rewardMean': 0.7471860142422797, 'totalEpisodes': 236, 'stepsPerEpisode': 58, 'rewardPerEpisode': 43.62253696342202
'totalSteps': 19200, 'rewardStep': 0.6952224053153624, 'errorList': [], 'lossList': [0.0, -1.3782415682077407, 0.0, 26.14649400472641, 0.0, 0.0, 0.0], 'rewardMean': 0.7296775210741823, 'totalEpisodes': 248, 'stepsPerEpisode': 18, 'rewardPerEpisode': 11.36947175916445
'totalSteps': 20480, 'rewardStep': 0.9016346975377666, 'errorList': [], 'lossList': [0.0, -1.3507617336511613, 0.0, 7.067494201660156, 0.0, 0.0, 0.0], 'rewardMean': 0.7340273190306719, 'totalEpisodes': 254, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.955595907062733
'totalSteps': 21760, 'rewardStep': 0.7191748045537618, 'errorList': [], 'lossList': [0.0, -1.327219032049179, 0.0, 10.855973958969116, 0.0, 0.0, 0.0], 'rewardMean': 0.7475473833385676, 'totalEpisodes': 258, 'stepsPerEpisode': 116, 'rewardPerEpisode': 99.17928140287609
'totalSteps': 23040, 'rewardStep': 0.33495652070242815, 'errorList': [], 'lossList': [0.0, -1.3269902163743972, 0.0, 5.207257873415947, 0.0, 0.0, 0.0], 'rewardMean': 0.7008552112682824, 'totalEpisodes': 261, 'stepsPerEpisode': 192, 'rewardPerEpisode': 133.39628569939512
'totalSteps': 24320, 'rewardStep': 0.6186690885004007, 'errorList': [], 'lossList': [0.0, -1.3211873584985734, 0.0, 4.862350976467132, 0.0, 0.0, 0.0], 'rewardMean': 0.6953558404497978, 'totalEpisodes': 264, 'stepsPerEpisode': 287, 'rewardPerEpisode': 226.6501458980901
'totalSteps': 25600, 'rewardStep': 0.8643155023650245, 'errorList': [], 'lossList': [0.0, -1.2897184628248215, 0.0, 3.226726610660553, 0.0, 0.0, 0.0], 'rewardMean': 0.6906746907986864, 'totalEpisodes': 266, 'stepsPerEpisode': 503, 'rewardPerEpisode': 426.0301225330739
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=62.33
