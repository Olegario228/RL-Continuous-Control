#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 72
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8670383750234363, 'errorList': [], 'lossList': [0.0, -1.4421435707807542, 0.0, 29.455716516971588, 0.0, 0.0, 0.0], 'rewardMean': 0.685069611463724, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 668.8161939442998
'totalSteps': 3840, 'rewardStep': 0.8330642550584881, 'errorList': [], 'lossList': [0.0, -1.4613844329118728, 0.0, 43.57103371620178, 0.0, 0.0, 0.0], 'rewardMean': 0.7344011593286455, 'totalEpisodes': 12, 'stepsPerEpisode': 666, 'rewardPerEpisode': 521.7584805545076
'totalSteps': 5120, 'rewardStep': 0.9192067841957339, 'errorList': [], 'lossList': [0.0, -1.4633689832687378, 0.0, 26.40578649878502, 0.0, 0.0, 0.0], 'rewardMean': 0.7806025655454176, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1000.1553259519178
'totalSteps': 6400, 'rewardStep': 0.5727436805347209, 'errorList': [], 'lossList': [0.0, -1.4471894562244416, 0.0, 35.4872691321373, 0.0, 0.0, 0.0], 'rewardMean': 0.7390307885432783, 'totalEpisodes': 14, 'stepsPerEpisode': 559, 'rewardPerEpisode': 432.4735840183826
'totalSteps': 7680, 'rewardStep': 0.5100039254372968, 'errorList': [], 'lossList': [0.0, -1.4272643208503724, 0.0, 194.05434524536133, 0.0, 0.0, 0.0], 'rewardMean': 0.7008596446922813, 'totalEpisodes': 35, 'stepsPerEpisode': 48, 'rewardPerEpisode': 36.74931995200851
'totalSteps': 8960, 'rewardStep': 0.8215848460952841, 'errorList': [], 'lossList': [0.0, -1.4220630913972854, 0.0, 234.21195598602296, 0.0, 0.0, 0.0], 'rewardMean': 0.7181061020355675, 'totalEpisodes': 79, 'stepsPerEpisode': 66, 'rewardPerEpisode': 54.23832495281096
'totalSteps': 10240, 'rewardStep': 0.5686997513505354, 'errorList': [], 'lossList': [0.0, -1.4211298960447312, 0.0, 122.11533966064454, 0.0, 0.0, 0.0], 'rewardMean': 0.6994303081999385, 'totalEpisodes': 126, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.9178617141654994
'totalSteps': 11520, 'rewardStep': 0.6056001029000024, 'errorList': [], 'lossList': [0.0, -1.4122112917900085, 0.0, 63.81583641052246, 0.0, 0.0, 0.0], 'rewardMean': 0.6890047298332789, 'totalEpisodes': 152, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.790774822671684
'totalSteps': 12800, 'rewardStep': 0.4060293479486469, 'errorList': [], 'lossList': [0.0, -1.4002209413051605, 0.0, 55.39349044799805, 0.0, 0.0, 0.0], 'rewardMean': 0.6607071916448157, 'totalEpisodes': 177, 'stepsPerEpisode': 128, 'rewardPerEpisode': 96.55032467390251
'totalSteps': 14080, 'rewardStep': 0.6520591195758514, 'errorList': [], 'lossList': [0.0, -1.404702045917511, 0.0, 23.24664251804352, 0.0, 0.0, 0.0], 'rewardMean': 0.6756030188119997, 'totalEpisodes': 190, 'stepsPerEpisode': 109, 'rewardPerEpisode': 79.67159798579681
'totalSteps': 15360, 'rewardStep': 0.8439136390518469, 'errorList': [], 'lossList': [0.0, -1.414429075717926, 0.0, 20.16767683982849, 0.0, 0.0, 0.0], 'rewardMean': 0.6732905452148407, 'totalEpisodes': 199, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.818756903012215
'totalSteps': 16640, 'rewardStep': 0.8773794599371156, 'errorList': [], 'lossList': [0.0, -1.4255681973695755, 0.0, 15.52915373802185, 0.0, 0.0, 0.0], 'rewardMean': 0.6777220657027033, 'totalEpisodes': 205, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.568694029661263
'totalSteps': 17920, 'rewardStep': 0.46293378401316176, 'errorList': [], 'lossList': [0.0, -1.4237473559379579, 0.0, 14.976218657493591, 0.0, 0.0, 0.0], 'rewardMean': 0.6320947656844462, 'totalEpisodes': 210, 'stepsPerEpisode': 309, 'rewardPerEpisode': 251.515126029065
'totalSteps': 19200, 'rewardStep': 0.8593494572458878, 'errorList': [], 'lossList': [0.0, -1.3781021964550018, 0.0, 27.05864198446274, 0.0, 0.0, 0.0], 'rewardMean': 0.6607553433555629, 'totalEpisodes': 216, 'stepsPerEpisode': 217, 'rewardPerEpisode': 182.95418196035078
'totalSteps': 20480, 'rewardStep': 0.8239685901213374, 'errorList': [], 'lossList': [0.0, -1.357915188074112, 0.0, 22.874593065977095, 0.0, 0.0, 0.0], 'rewardMean': 0.6921518098239671, 'totalEpisodes': 218, 'stepsPerEpisode': 86, 'rewardPerEpisode': 75.00879628179194
'totalSteps': 21760, 'rewardStep': 0.8143511899962042, 'errorList': [], 'lossList': [0.0, -1.3428169578313827, 0.0, 7.0039932435750965, 0.0, 0.0, 0.0], 'rewardMean': 0.6914284442140589, 'totalEpisodes': 219, 'stepsPerEpisode': 464, 'rewardPerEpisode': 415.8806738301651
'totalSteps': 23040, 'rewardStep': 0.953471184442198, 'errorList': [0.2732871132339055, 0.28009790977481663, 0.28121051197235714, 0.25810171264271203, 0.2648317846167488, 0.2759064478646219, 0.28047743424824617, 0.2698857362835962, 0.2527308629228297, 0.29729698771491614, 0.27124666242125217, 0.2799913653366613, 0.2612048121124834, 0.26344853670471646, 0.27152687483040167, 0.2530393515069489, 0.26745989356704314, 0.2518126200536047, 0.24665846419099913, 0.2569774942573846, 0.251552900946489, 0.25888665092043595, 0.25543037435422306, 0.2528782709968171, 0.27057021633609296, 0.2748096829408099, 0.24948758279723715, 0.28350126891144617, 0.2619646758822667, 0.254080075440294, 0.2536486786640996, 0.2567455716715116, 0.2677360920797325, 0.2624077746936847, 0.2663216732521276, 0.2555369974082348, 0.2768053912235577, 0.26283739222289143, 0.29295537777248065, 0.286337833347068, 0.350135990273127, 0.2727569239911588, 0.2844704346588511, 0.2640418496527495, 0.32977557473507, 0.28439364923003807, 0.25442128755645477, 0.27631528328734367, 0.2719023304895642, 0.2941841997668993], 'lossList': [0.0, -1.324093943834305, 0.0, 5.572899062633514, 0.0, 0.0, 0.0], 'rewardMean': 0.7299055875232252, 'totalEpisodes': 220, 'stepsPerEpisode': 232, 'rewardPerEpisode': 208.5968306626475, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.7746508726842696, 'errorList': [], 'lossList': [0.0, -1.3211530715227127, 0.0, 2.2369103725254535, 0.0, 0.0, 0.0], 'rewardMean': 0.7468106645016519, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.2747564660128
'totalSteps': 25600, 'rewardStep': 0.8277621637679575, 'errorList': [], 'lossList': [0.0, -1.3179505175352098, 0.0, 1.821246214210987, 0.0, 0.0, 0.0], 'rewardMean': 0.788983946083583, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1116.3864471963004
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.96
