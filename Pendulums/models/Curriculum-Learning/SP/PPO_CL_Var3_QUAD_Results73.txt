#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 73
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5405560914592729, 'errorList': [], 'lossList': [0.0, -1.4221876043081283, 0.0, 32.10894706964493, 0.0, 0.0, 0.0], 'rewardMean': 0.7197432536430066, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 32.07263302460993
'totalSteps': 3840, 'rewardStep': 0.9491021528507775, 'errorList': [], 'lossList': [0.0, -1.4141032588481903, 0.0, 33.37659946680069, 0.0, 0.0, 0.0], 'rewardMean': 0.796196220045597, 'totalEpisodes': 18, 'stepsPerEpisode': 488, 'rewardPerEpisode': 398.79065032929697
'totalSteps': 5120, 'rewardStep': 0.8022585123306115, 'errorList': [], 'lossList': [0.0, -1.3989354252815247, 0.0, 27.86423929929733, 0.0, 0.0, 0.0], 'rewardMean': 0.7977117931168506, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.0196736549021
'totalSteps': 6400, 'rewardStep': 0.5884544502883448, 'errorList': [], 'lossList': [0.0, -1.384591789841652, 0.0, 15.337363922595978, 0.0, 0.0, 0.0], 'rewardMean': 0.7558603245511495, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.4368996516488
'totalSteps': 7680, 'rewardStep': 0.9385787588770299, 'errorList': [71.22220296500231, 70.48486078640224, 74.06549056874664, 79.01563787828584, 63.895246754187475, 70.23298255757324, 70.61985870499551, 68.77702378725401, 69.66566163826795, 79.60974034525594, 69.05594443108441, 68.4471087221138, 77.94702293856788, 76.42045368781037, 79.14511584227543, 69.58607666016768, 79.57966787159319, 78.77330322421368, 64.55170336082601, 71.15373525257107, 73.24018283014782, 71.72497057185167, 68.37884883068944, 82.90928290291599, 74.22479333372785, 69.10712251644071, 71.1499471643213, 71.25739248695173, 66.06548414037272, 81.47849104688144, 66.64865840796811, 69.90517265434983, 63.80512314214975, 64.01479842336381, 73.14728833602491, 76.14336247867567, 78.8403006963204, 64.61190739380511, 71.49677711222454, 69.71675281986376, 73.4622982787881, 74.17623811977734, 67.81498862648648, 78.82846104869411, 67.17863061389193, 63.186078565742996, 71.93034242447636, 69.13331123462883, 67.8362132422519, 65.91497786481465], 'lossList': [0.0, -1.3578896123170852, 0.0, 153.9628720855713, 0.0, 0.0, 0.0], 'rewardMean': 0.7863133969387962, 'totalEpisodes': 32, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.3620800248385, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.6431631009293969, 'errorList': [], 'lossList': [0.0, -1.3569172382354737, 0.0, 285.64900787353514, 0.0, 0.0, 0.0], 'rewardMean': 0.7658633546517392, 'totalEpisodes': 90, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.682314494730734
'totalSteps': 10240, 'rewardStep': 0.6071310618428151, 'errorList': [], 'lossList': [0.0, -1.3492514139413834, 0.0, 93.78794342041016, 0.0, 0.0, 0.0], 'rewardMean': 0.7460218180506236, 'totalEpisodes': 139, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.837343349929334
'totalSteps': 11520, 'rewardStep': 0.7398824821443652, 'errorList': [], 'lossList': [0.0, -1.3449585419893264, 0.0, 46.02736911773682, 0.0, 0.0, 0.0], 'rewardMean': 0.7453396696165949, 'totalEpisodes': 167, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.25319432280161
'totalSteps': 12800, 'rewardStep': 0.6115306480581444, 'errorList': [], 'lossList': [0.0, -1.3492445021867752, 0.0, 29.189261050224303, 0.0, 0.0, 0.0], 'rewardMean': 0.7319587674607498, 'totalEpisodes': 185, 'stepsPerEpisode': 92, 'rewardPerEpisode': 71.86076018734431
'totalSteps': 14080, 'rewardStep': 0.9166359793359194, 'errorList': [], 'lossList': [0.0, -1.3462571507692338, 0.0, 28.257470960617066, 0.0, 0.0, 0.0], 'rewardMean': 0.7337293238116678, 'totalEpisodes': 196, 'stepsPerEpisode': 70, 'rewardPerEpisode': 57.64444161301548
'totalSteps': 15360, 'rewardStep': 0.7666488240347062, 'errorList': [], 'lossList': [0.0, -1.334221841096878, 0.0, 16.64704532146454, 0.0, 0.0, 0.0], 'rewardMean': 0.7563385970692111, 'totalEpisodes': 204, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.061618565194648
'totalSteps': 16640, 'rewardStep': 0.6549724811472315, 'errorList': [], 'lossList': [0.0, -1.3178938162326812, 0.0, 8.384150738716125, 0.0, 0.0, 0.0], 'rewardMean': 0.7269256298988565, 'totalEpisodes': 208, 'stepsPerEpisode': 325, 'rewardPerEpisode': 244.50219914101888
'totalSteps': 17920, 'rewardStep': 0.8626754087512242, 'errorList': [], 'lossList': [0.0, -1.3052016776800155, 0.0, 11.844072396755218, 0.0, 0.0, 0.0], 'rewardMean': 0.7329673195409178, 'totalEpisodes': 212, 'stepsPerEpisode': 165, 'rewardPerEpisode': 145.11838588229935
'totalSteps': 19200, 'rewardStep': 0.5881942069097453, 'errorList': [], 'lossList': [0.0, -1.3178837513923645, 0.0, 10.194666037559509, 0.0, 0.0, 0.0], 'rewardMean': 0.7329412952030578, 'totalEpisodes': 214, 'stepsPerEpisode': 347, 'rewardPerEpisode': 233.55375421409659
'totalSteps': 20480, 'rewardStep': 0.8509995015295047, 'errorList': [], 'lossList': [0.0, -1.3030786734819413, 0.0, 6.8660902452468875, 0.0, 0.0, 0.0], 'rewardMean': 0.7241833694683052, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.279483497712
'totalSteps': 21760, 'rewardStep': 0.7809575029281941, 'errorList': [], 'lossList': [0.0, -1.2690350568294526, 0.0, 4.392428899407387, 0.0, 0.0, 0.0], 'rewardMean': 0.7379628096681851, 'totalEpisodes': 215, 'stepsPerEpisode': 669, 'rewardPerEpisode': 553.1180355385779
'totalSteps': 23040, 'rewardStep': 0.6110782943319614, 'errorList': [], 'lossList': [0.0, -1.252065321803093, 0.0, 0.9957417717576027, 0.0, 0.0, 0.0], 'rewardMean': 0.7383575329170996, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 899.7542212726019
'totalSteps': 24320, 'rewardStep': 0.6451489124368284, 'errorList': [], 'lossList': [0.0, -1.2284223657846451, 0.0, 2.155797515809536, 0.0, 0.0, 0.0], 'rewardMean': 0.728884175946346, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.0621896601463
'totalSteps': 25600, 'rewardStep': 0.9320576778906775, 'errorList': [0.03272201185951866, 0.03862652036498616, 0.03600522998929012, 0.035105733920524275, 0.035464221668162356, 0.03937903898952539, 0.039251557939644355, 0.03178592627583356, 0.03934679805286632, 0.042731485069658764, 0.03690922669056392, 0.06659638544453918, 0.03733748986826509, 0.05124958788016661, 0.031920678598742085, 0.02908734817427081, 0.036919858366688485, 0.04938650943652029, 0.036072763272091905, 0.06098644435739356, 0.026839627249653136, 0.030581830289652192, 0.03234282276323464, 0.04692471502910519, 0.02859156957226064, 0.037817810957850084, 0.03368993619957391, 0.03707651338523125, 0.05243942248275623, 0.026295016294676437, 0.0344089368122422, 0.0490413825980122, 0.03718750466105929, 0.04311864605138497, 0.04706318436538742, 0.04519940021204501, 0.03251871966741024, 0.04106703609699756, 0.027683398543401617, 0.03582713153333993, 0.03961069364025897, 0.031386101748078085, 0.03749700389636225, 0.04744618852436389, 0.05247104910912457, 0.044885215051008946, 0.03358075332611543, 0.03664093953805907, 0.03983089689264345, 0.04022955898678168], 'lossList': [0.0, -1.196574512720108, 0.0, 1.860028659775853, 0.0, 0.0, 0.0], 'rewardMean': 0.7609368789295992, 'totalEpisodes': 215, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.6602289900482, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=101.65
