#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 76
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5968428610159475, 'errorList': [], 'lossList': [0.0, -1.4200267380475997, 0.0, 31.089387125968933, 0.0, 0.0, 0.0], 'rewardMean': 0.7059464969286775, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.29199333121611
'totalSteps': 3840, 'rewardStep': 0.8254084750249987, 'errorList': [], 'lossList': [0.0, -1.3982788103818893, 0.0, 41.07032429695129, 0.0, 0.0, 0.0], 'rewardMean': 0.7457671562941179, 'totalEpisodes': 71, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.977398712579161
'totalSteps': 5120, 'rewardStep': 0.8214273804859585, 'errorList': [], 'lossList': [0.0, -1.3760017275810241, 0.0, 32.5603906583786, 0.0, 0.0, 0.0], 'rewardMean': 0.7646822123420781, 'totalEpisodes': 80, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.555384702822217
'totalSteps': 6400, 'rewardStep': 0.515306682514231, 'errorList': [], 'lossList': [0.0, -1.366793963909149, 0.0, 46.251426467895506, 0.0, 0.0, 0.0], 'rewardMean': 0.7148071063765087, 'totalEpisodes': 87, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.515306682514231
'totalSteps': 7680, 'rewardStep': 0.6081509789415711, 'errorList': [], 'lossList': [0.0, -1.3547783666849136, 0.0, 17.351128898561, 0.0, 0.0, 0.0], 'rewardMean': 0.6970310851373526, 'totalEpisodes': 88, 'stepsPerEpisode': 273, 'rewardPerEpisode': 213.64176302848307
'totalSteps': 8960, 'rewardStep': 0.537199728333277, 'errorList': [], 'lossList': [0.0, -1.3577031475305557, 0.0, 113.42917419433594, 0.0, 0.0, 0.0], 'rewardMean': 0.6741980341653419, 'totalEpisodes': 105, 'stepsPerEpisode': 66, 'rewardPerEpisode': 52.16228301440888
'totalSteps': 10240, 'rewardStep': 0.278342673735486, 'errorList': [], 'lossList': [0.0, -1.352833279967308, 0.0, 99.78834413528442, 0.0, 0.0, 0.0], 'rewardMean': 0.6247161141116097, 'totalEpisodes': 122, 'stepsPerEpisode': 123, 'rewardPerEpisode': 88.7654830828475
'totalSteps': 11520, 'rewardStep': 0.5073843501588262, 'errorList': [], 'lossList': [0.0, -1.3401622241735458, 0.0, 60.35373453140259, 0.0, 0.0, 0.0], 'rewardMean': 0.6116792514501893, 'totalEpisodes': 137, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.031153233601943
'totalSteps': 12800, 'rewardStep': 0.3437183511055528, 'errorList': [], 'lossList': [0.0, -1.3169595092535018, 0.0, 28.188345251083373, 0.0, 0.0, 0.0], 'rewardMean': 0.5848831614157256, 'totalEpisodes': 145, 'stepsPerEpisode': 494, 'rewardPerEpisode': 336.5790772352135
'totalSteps': 14080, 'rewardStep': 0.8045851445640608, 'errorList': [], 'lossList': [0.0, -1.2913181966543197, 0.0, 13.773694014549255, 0.0, 0.0, 0.0], 'rewardMean': 0.5838366625879909, 'totalEpisodes': 150, 'stepsPerEpisode': 231, 'rewardPerEpisode': 187.9198134758736
'totalSteps': 15360, 'rewardStep': 0.9432641948512703, 'errorList': [0.4219078104606643, 0.4965562178922509, 0.37073089093058237, 0.49240992895645147, 0.4146295926777716, 0.39421404765567036, 0.5113842389268418, 0.44895386667455794, 0.40021147943590696, 0.3827097085328413, 0.3447369180415411, 0.44264934540249073, 0.4775049783877195, 0.4055527525432788, 0.37745777082295184, 0.4157688600004599, 0.5627280924241923, 0.4344691060957684, 0.6959026733046693, 0.3778683837986069, 0.4368105125606116, 0.4865236723547796, 0.35779343268860925, 0.47257903950197383, 0.45927411261193063, 0.490109496788778, 0.41889022451689195, 0.38314122087496355, 0.38264788245476755, 0.44825004488857795, 0.3258144522593786, 0.39603228178071154, 0.39959802787140564, 0.3606452681286763, 0.3090008318835584, 0.3970058877928444, 0.3965564860403959, 0.32866962770802804, 0.45057695499907585, 0.3906917200234655, 0.42533563978950756, 0.3799128024133593, 0.41431919822971436, 0.4300129595781192, 0.3876158903389028, 0.4055052228001092, 0.5256155419761981, 0.4415838495845384, 0.3953940431080328, 0.36331353398179556], 'lossList': [0.0, -1.2846544855833053, 0.0, 6.507441697716713, 0.0, 0.0, 0.0], 'rewardMean': 0.6184787959715232, 'totalEpisodes': 152, 'stepsPerEpisode': 192, 'rewardPerEpisode': 154.69534834956917, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6446888133063862, 'errorList': [], 'lossList': [0.0, -1.2751738357543945, 0.0, 4.186783269047737, 0.0, 0.0, 0.0], 'rewardMean': 0.600406829799662, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 947.1955424244472
'totalSteps': 17920, 'rewardStep': 0.786589310926083, 'errorList': [], 'lossList': [0.0, -1.2392516762018204, 0.0, 3.9561433325707913, 0.0, 0.0, 0.0], 'rewardMean': 0.5969230228436745, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.6985795515234
'totalSteps': 19200, 'rewardStep': 0.9493461173602527, 'errorList': [0.05537508111801801, 0.07401056336356415, 0.0738987762083667, 0.08547493849909255, 0.08056347437511989, 0.09306551112797676, 0.06744902520221187, 0.07869579157240757, 0.06979290296171554, 0.0723962519661468, 0.07795187918893562, 0.08460845404939694, 0.07835948213817526, 0.0765441380449901, 0.05500197838972674, 0.06457467120348692, 0.10633290136813146, 0.07143233868860702, 0.06331299332364221, 0.06437508986604436, 0.059189519203939604, 0.07722122541348982, 0.06248904440055632, 0.0690192579949642, 0.06250904883483661, 0.08108415724022972, 0.08827740019361842, 0.05871129963742273, 0.05888088020383878, 0.05515246106954188, 0.06777177286662525, 0.059344570225371776, 0.07606521782110405, 0.06151051911765679, 0.06847999468421051, 0.08180623171419808, 0.06772182299318198, 0.09714757470697458, 0.10132449169656176, 0.06903050811438285, 0.11808398764701623, 0.07752394847144899, 0.0778665066991887, 0.07210271589673556, 0.06683704966246416, 0.08697665407887563, 0.08348674973615508, 0.05546685322695795, 0.07116860823638083, 0.07532545477393368], 'lossList': [0.0, -1.2075379544496536, 0.0, 2.823572828657925, 0.0, 0.0, 0.0], 'rewardMean': 0.6403269663282767, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.2564238603832, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=87.99
