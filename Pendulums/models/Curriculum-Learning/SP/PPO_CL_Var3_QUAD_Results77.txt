#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 77
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.8479432505637031, 'errorList': [], 'lossList': [0.0, -1.4193296921253205, 0.0, 35.590335865020755, 0.0, 0.0, 0.0], 'rewardMean': 0.8317902961737063, 'totalEpisodes': 68, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.11713197076629
'totalSteps': 3840, 'rewardStep': 0.5059791457398264, 'errorList': [], 'lossList': [0.0, -1.404847065806389, 0.0, 37.39580714225769, 0.0, 0.0, 0.0], 'rewardMean': 0.7231865793624129, 'totalEpisodes': 84, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.426441159157473
'totalSteps': 5120, 'rewardStep': 0.49039566418979763, 'errorList': [], 'lossList': [0.0, -1.3900258201360702, 0.0, 30.69641766548157, 0.0, 0.0, 0.0], 'rewardMean': 0.6649888505692592, 'totalEpisodes': 95, 'stepsPerEpisode': 44, 'rewardPerEpisode': 27.56348652725481
'totalSteps': 6400, 'rewardStep': 0.4249498419473078, 'errorList': [], 'lossList': [0.0, -1.3823833572864532, 0.0, 37.37594820022583, 0.0, 0.0, 0.0], 'rewardMean': 0.6169810488448688, 'totalEpisodes': 103, 'stepsPerEpisode': 52, 'rewardPerEpisode': 40.27566306358229
'totalSteps': 7680, 'rewardStep': 0.8592793996829953, 'errorList': [], 'lossList': [0.0, -1.3601456362009048, 0.0, 63.93708498001099, 0.0, 0.0, 0.0], 'rewardMean': 0.65736410731789, 'totalEpisodes': 111, 'stepsPerEpisode': 54, 'rewardPerEpisode': 42.11990465940192
'totalSteps': 8960, 'rewardStep': 0.7371670512582768, 'errorList': [], 'lossList': [0.0, -1.362821819782257, 0.0, 86.96892164230347, 0.0, 0.0, 0.0], 'rewardMean': 0.6687645278808024, 'totalEpisodes': 126, 'stepsPerEpisode': 79, 'rewardPerEpisode': 63.46549107319062
'totalSteps': 10240, 'rewardStep': 0.7923648389359471, 'errorList': [], 'lossList': [0.0, -1.366743374466896, 0.0, 61.14753664016724, 0.0, 0.0, 0.0], 'rewardMean': 0.6842145667626954, 'totalEpisodes': 141, 'stepsPerEpisode': 78, 'rewardPerEpisode': 70.88139903389853
'totalSteps': 11520, 'rewardStep': 0.7085078295596596, 'errorList': [], 'lossList': [0.0, -1.375795847773552, 0.0, 40.09459404945373, 0.0, 0.0, 0.0], 'rewardMean': 0.6869138181845804, 'totalEpisodes': 152, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.43542270478164
'totalSteps': 12800, 'rewardStep': 0.5403255291671881, 'errorList': [], 'lossList': [0.0, -1.372789060473442, 0.0, 18.117529954910278, 0.0, 0.0, 0.0], 'rewardMean': 0.6722549892828412, 'totalEpisodes': 158, 'stepsPerEpisode': 256, 'rewardPerEpisode': 161.31365896050565
'totalSteps': 14080, 'rewardStep': 0.8573018572448667, 'errorList': [], 'lossList': [0.0, -1.351051669716835, 0.0, 13.228899683952331, 0.0, 0.0, 0.0], 'rewardMean': 0.6764214408289569, 'totalEpisodes': 163, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7075953608655439
'totalSteps': 15360, 'rewardStep': 0.9455370859963474, 'errorList': [1.0569845923433319, 1.0647769313846016, 1.387617158269962, 1.0848952514581545, 0.9783890878110376, 1.1659378462096757, 1.553809214292764, 1.170456780856192, 1.1913083361962407, 1.7015752765543861, 1.60082961731334, 1.3432478782973705, 1.0778952224091651, 1.5266899366299984, 0.876758639824141, 1.2855189753720297, 1.107920311780047, 1.4618487282829054, 1.1957461843082215, 1.1268537733199233, 1.0850298024906344, 0.9988809834981844, 1.0148767713783184, 1.3518882398491394, 1.02467639107118, 1.2989786276263335, 1.8596448032984598, 1.1159430440919638, 1.2423853758959704, 2.182322367445295, 1.220311687236407, 1.3809753819149067, 1.0776216031289818, 1.6506473271571802, 1.3830784136715237, 1.1385347833200914, 1.0883971014975893, 1.4426148500353795, 1.27004113577722, 1.6654006385779978, 1.256826395398957, 1.0353526304357352, 1.1342273807459402, 1.4033668645946595, 1.1033287526518962, 1.2338969757013192, 1.0520289532316116, 1.3070455531182288, 1.1439295490771888, 0.9635037653785063], 'lossList': [0.0, -1.346122374534607, 0.0, 9.128523817062378, 0.0, 0.0, 0.0], 'rewardMean': 0.6861808243722213, 'totalEpisodes': 166, 'stepsPerEpisode': 43, 'rewardPerEpisode': 39.46983199234283, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5681387308828658, 'errorList': [], 'lossList': [0.0, -1.3604681497812272, 0.0, 6.7345467096567155, 0.0, 0.0, 0.0], 'rewardMean': 0.6923967828865252, 'totalEpisodes': 167, 'stepsPerEpisode': 176, 'rewardPerEpisode': 119.2578025471929
'totalSteps': 17920, 'rewardStep': 0.5901570196308731, 'errorList': [], 'lossList': [0.0, -1.3594255131483077, 0.0, 6.0350649285316464, 0.0, 0.0, 0.0], 'rewardMean': 0.7023729184306328, 'totalEpisodes': 169, 'stepsPerEpisode': 692, 'rewardPerEpisode': 541.0575330068435
'totalSteps': 19200, 'rewardStep': 0.9560446738036923, 'errorList': [0.06445265217777187, 0.051575953341689317, 0.14945669855361807, 0.09910607668716258, 0.05707351250496385, 0.07157458630215145, 0.16217942351727763, 0.12375521827094667, 0.08717888909519336, 0.04119497496444784, 0.09652473727648829, 0.08470895013577735, 0.04619011454034632, 0.04750169228135147, 0.051749567262223906, 0.03902277708793267, 0.16669857628742082, 0.1463198308245002, 0.05959036465406451, 0.06331960785646616, 0.04798589309569458, 0.05840753655344377, 0.1572337062700414, 0.05124424824741778, 0.05316905284472755, 0.036053116038829755, 0.04640479774124005, 0.04374755097116217, 0.04553575266678397, 0.056430594424082256, 0.03870520676954727, 0.14258594145593517, 0.03778519035150737, 0.10223698466097753, 0.11126332828887212, 0.10587517879657388, 0.028134240397368795, 0.13912965584858508, 0.059521916679413665, 0.05427617047348428, 0.05146287525322091, 0.07657483051987063, 0.06628700477281636, 0.1190478025065181, 0.0786945355289395, 0.051622224390091107, 0.06482193228761218, 0.03642539112967273, 0.03438676527878623, 0.08462353562141456], 'lossList': [0.0, -1.3264207941293717, 0.0, 7.406248098611831, 0.0, 0.0, 0.0], 'rewardMean': 0.7554824016162713, 'totalEpisodes': 170, 'stepsPerEpisode': 759, 'rewardPerEpisode': 625.5183088772216, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=87.32
