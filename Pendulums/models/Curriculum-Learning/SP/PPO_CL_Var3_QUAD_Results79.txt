#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 79
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.820609907478158, 'errorList': [], 'lossList': [0.0, -1.4124705183506012, 0.0, 33.77205814361572, 0.0, 0.0, 0.0], 'rewardMean': 0.8708500208147826, 'totalEpisodes': 67, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.992680606470635
'totalSteps': 3840, 'rewardStep': 0.8755868236894356, 'errorList': [], 'lossList': [0.0, -1.4158238327503205, 0.0, 41.764147663116454, 0.0, 0.0, 0.0], 'rewardMean': 0.8724289551063337, 'totalEpisodes': 92, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.86691949068384
'totalSteps': 5120, 'rewardStep': 0.938526555792414, 'errorList': [], 'lossList': [0.0, -1.400364138484001, 0.0, 27.95239520549774, 0.0, 0.0, 0.0], 'rewardMean': 0.8889533552778537, 'totalEpisodes': 103, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.492700282502625
'totalSteps': 6400, 'rewardStep': 0.7990659283419039, 'errorList': [], 'lossList': [0.0, -1.3791874170303344, 0.0, 31.0066237449646, 0.0, 0.0, 0.0], 'rewardMean': 0.8709758698906637, 'totalEpisodes': 111, 'stepsPerEpisode': 151, 'rewardPerEpisode': 118.82857495808086
'totalSteps': 7680, 'rewardStep': 0.8903509042326821, 'errorList': [], 'lossList': [0.0, -1.3801019847393037, 0.0, 49.27780429840088, 0.0, 0.0, 0.0], 'rewardMean': 0.874205042281, 'totalEpisodes': 118, 'stepsPerEpisode': 48, 'rewardPerEpisode': 36.67642751671147
'totalSteps': 8960, 'rewardStep': 0.7097117116399593, 'errorList': [], 'lossList': [0.0, -1.3729580008983613, 0.0, 124.03276386260987, 0.0, 0.0, 0.0], 'rewardMean': 0.8507059950465656, 'totalEpisodes': 139, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.726336180180086
'totalSteps': 10240, 'rewardStep': 0.5212401640145574, 'errorList': [], 'lossList': [0.0, -1.3592101323604584, 0.0, 91.10185438156128, 0.0, 0.0, 0.0], 'rewardMean': 0.8095227661675647, 'totalEpisodes': 160, 'stepsPerEpisode': 15, 'rewardPerEpisode': 7.5782326150279395
'totalSteps': 11520, 'rewardStep': 0.42750043508816626, 'errorList': [], 'lossList': [0.0, -1.3485663914680481, 0.0, 40.22753914833069, 0.0, 0.0, 0.0], 'rewardMean': 0.767075840492076, 'totalEpisodes': 178, 'stepsPerEpisode': 87, 'rewardPerEpisode': 64.53550901842982
'totalSteps': 12800, 'rewardStep': 0.5489330851008032, 'errorList': [], 'lossList': [0.0, -1.3531034570932388, 0.0, 23.16434760093689, 0.0, 0.0, 0.0], 'rewardMean': 0.7452615649529488, 'totalEpisodes': 185, 'stepsPerEpisode': 64, 'rewardPerEpisode': 42.177273740834615
'totalSteps': 14080, 'rewardStep': 0.7562724457249224, 'errorList': [], 'lossList': [0.0, -1.3510654658079146, 0.0, 13.27877734899521, 0.0, 0.0, 0.0], 'rewardMean': 0.7287797961103003, 'totalEpisodes': 187, 'stepsPerEpisode': 1119, 'rewardPerEpisode': 867.8718882449646
'totalSteps': 15360, 'rewardStep': 0.6240805385306991, 'errorList': [], 'lossList': [0.0, -1.3327733451128005, 0.0, 43.34969916343689, 0.0, 0.0, 0.0], 'rewardMean': 0.7091268592155544, 'totalEpisodes': 191, 'stepsPerEpisode': 859, 'rewardPerEpisode': 583.8165718431725
'totalSteps': 16640, 'rewardStep': 0.5192763165271379, 'errorList': [], 'lossList': [0.0, -1.3120330393314361, 0.0, 15.652945615053177, 0.0, 0.0, 0.0], 'rewardMean': 0.6734958084993246, 'totalEpisodes': 193, 'stepsPerEpisode': 447, 'rewardPerEpisode': 372.4019496589654
'totalSteps': 17920, 'rewardStep': 0.8032861909980213, 'errorList': [], 'lossList': [0.0, -1.2871085911989213, 0.0, 5.973307106196881, 0.0, 0.0, 0.0], 'rewardMean': 0.6599717720198853, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 944.17281889877
'totalSteps': 19200, 'rewardStep': 0.9661429949007505, 'errorList': [0.4798206548121646, 0.43826214873974234, 0.45198433029007784, 0.467613270231446, 0.42794216855591627, 0.47242426760079637, 0.45174494348014654, 0.4603598361448337, 0.44833025437609886, 0.47479752658780605, 0.43481971557186133, 0.45100991202686413, 0.43118338240149495, 0.49806260063255003, 0.4632719327641991, 0.45105517767437847, 0.5232130841302318, 0.49495945021165066, 0.48137574130781036, 0.43332965865813733, 0.4905110000373366, 0.47820331867009613, 0.4480145907001136, 0.47481164511220436, 0.4396343478096301, 0.5875069609451347, 0.44615971936605486, 0.5180168716830358, 0.46769250196481277, 0.470094940268895, 0.45894064923452643, 0.41790215794248264, 0.44888846932910254, 0.49509970465186137, 0.4593032838623695, 0.46104009209815483, 0.4739108098505134, 0.4725047779049178, 0.44954380940727084, 0.46209066188377773, 0.5018091881403937, 0.46939564820049706, 0.44970883766965847, 0.4385911762853036, 0.46481041875962953, 0.49980650051262066, 0.4571454973564795, 0.47344148248557627, 0.46503058455236923, 0.445885825404615], 'lossList': [0.0, -1.2767517131567, 0.0, 5.802555943727493, 0.0, 0.0, 0.0], 'rewardMean': 0.6766794786757699, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.1568568713374, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8701067852190215, 'errorList': [], 'lossList': [0.0, -1.2513014304637908, 0.0, 3.0821152471005915, 0.0, 0.0, 0.0], 'rewardMean': 0.6746550667744039, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.880009760809
'totalSteps': 21760, 'rewardStep': 0.8369081665894366, 'errorList': [], 'lossList': [0.0, -1.2566974133253097, 0.0, 1.1858658219128848, 0.0, 0.0, 0.0], 'rewardMean': 0.6873747122693517, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 939.092579408855
'totalSteps': 23040, 'rewardStep': 0.7785676362930694, 'errorList': [], 'lossList': [0.0, -1.265041144490242, 0.0, 1.092850770652294, 0.0, 0.0, 0.0], 'rewardMean': 0.7131074594972028, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.6778069973375
'totalSteps': 24320, 'rewardStep': 0.867429880295289, 'errorList': [], 'lossList': [0.0, -1.2523936080932616, 0.0, 1.047618196196854, 0.0, 0.0, 0.0], 'rewardMean': 0.757100404017915, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.0823962252089
'totalSteps': 25600, 'rewardStep': 0.9820998651339059, 'errorList': [0.11560090691933185, 0.07788584189292636, 0.07111986307546235, 0.10627211501198802, 0.07907811992996552, 0.06788961726649269, 0.07010301140689598, 0.0710515168164418, 0.07503037735490706, 0.07694958246402442, 0.06528597971461106, 0.08183219589228513, 0.08238716929669028, 0.07181860972528081, 0.0883543552898289, 0.1619147372747396, 0.08692618138952424, 0.08602226145625107, 0.07660005231504179, 0.15068710256890033, 0.0786375048340734, 0.07489454494607345, 0.1509013669464617, 0.08790919846924937, 0.07602928504293226, 0.0743722599068187, 0.1714891898043025, 0.15972917950925838, 0.12019566214307743, 0.11380868777866048, 0.11341704178935424, 0.1407110686121904, 0.21126471084028978, 0.12624575170757582, 0.23072418854153837, 0.0783619997157517, 0.07394037295811388, 0.08501203537322882, 0.08722308892503612, 0.0758947801592023, 0.16458314890119136, 0.14110826097022852, 0.07904788132285297, 0.10699905995717991, 0.07652194245348316, 0.1636210709502101, 0.07781644985577248, 0.07992563355126399, 0.14799929273954274, 0.24202597785932345], 'lossList': [0.0, -1.2284663599729537, 0.0, 1.2540604296699167, 0.0, 0.0, 0.0], 'rewardMean': 0.8004170820212252, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.174211166876, 'successfulTests': 47
#maxSuccessfulTests=47, maxSuccessfulTestsAtStep=25600, timeSpent=104.36
