#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 80
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8709094950336207, 'errorList': [], 'lossList': [0.0, -1.398868477344513, 0.0, 28.3991090965271, 0.0, 0.0, 0.0], 'rewardMean': 0.7407897969877356, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.916343869164361
'totalSteps': 3840, 'rewardStep': 0.658157456086025, 'errorList': [], 'lossList': [0.0, -1.3859186708927154, 0.0, 27.347802000045775, 0.0, 0.0, 0.0], 'rewardMean': 0.7132456833538322, 'totalEpisodes': 24, 'stepsPerEpisode': 154, 'rewardPerEpisode': 129.31111997377292
'totalSteps': 5120, 'rewardStep': 0.7359780789123747, 'errorList': [], 'lossList': [0.0, -1.3888188290596009, 0.0, 30.712135586738587, 0.0, 0.0, 0.0], 'rewardMean': 0.7189287822434678, 'totalEpisodes': 28, 'stepsPerEpisode': 203, 'rewardPerEpisode': 150.70253387357272
'totalSteps': 6400, 'rewardStep': 0.8186743343884836, 'errorList': [], 'lossList': [0.0, -1.3876413774490357, 0.0, 39.42438956260681, 0.0, 0.0, 0.0], 'rewardMean': 0.738877892672471, 'totalEpisodes': 31, 'stepsPerEpisode': 208, 'rewardPerEpisode': 164.91263685303494
'totalSteps': 7680, 'rewardStep': 0.8411656224499938, 'errorList': [], 'lossList': [0.0, -1.3730625325441361, 0.0, 80.374087972641, 0.0, 0.0, 0.0], 'rewardMean': 0.7559258476353915, 'totalEpisodes': 38, 'stepsPerEpisode': 290, 'rewardPerEpisode': 168.60825388332813
'totalSteps': 8960, 'rewardStep': 0.8581403129295977, 'errorList': [], 'lossList': [0.0, -1.3669768834114076, 0.0, 203.50608890533448, 0.0, 0.0, 0.0], 'rewardMean': 0.7705279141059923, 'totalEpisodes': 67, 'stepsPerEpisode': 87, 'rewardPerEpisode': 70.8519635742132
'totalSteps': 10240, 'rewardStep': 0.7198415960742702, 'errorList': [], 'lossList': [0.0, -1.35931815803051, 0.0, 144.64224548339843, 0.0, 0.0, 0.0], 'rewardMean': 0.7641921243520271, 'totalEpisodes': 95, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.065534400689907
'totalSteps': 11520, 'rewardStep': 0.6961677116344415, 'errorList': [], 'lossList': [0.0, -1.350414610505104, 0.0, 79.13041646957397, 0.0, 0.0, 0.0], 'rewardMean': 0.7566338562722953, 'totalEpisodes': 110, 'stepsPerEpisode': 70, 'rewardPerEpisode': 57.171470433068926
'totalSteps': 12800, 'rewardStep': 0.6147635878340802, 'errorList': [], 'lossList': [0.0, -1.3406006652116775, 0.0, 53.07837810516357, 0.0, 0.0, 0.0], 'rewardMean': 0.7424468294284738, 'totalEpisodes': 118, 'stepsPerEpisode': 141, 'rewardPerEpisode': 95.36615153374451
'totalSteps': 14080, 'rewardStep': 0.5451499747384395, 'errorList': [], 'lossList': [0.0, -1.3338305783271789, 0.0, 47.66615830898285, 0.0, 0.0, 0.0], 'rewardMean': 0.7358948170081326, 'totalEpisodes': 121, 'stepsPerEpisode': 379, 'rewardPerEpisode': 302.3383328959807
'totalSteps': 15360, 'rewardStep': 0.7990038310110545, 'errorList': [], 'lossList': [0.0, -1.3353038519620894, 0.0, 68.88481398582458, 0.0, 0.0, 0.0], 'rewardMean': 0.7287042506058761, 'totalEpisodes': 129, 'stepsPerEpisode': 39, 'rewardPerEpisode': 36.10726133966405
'totalSteps': 16640, 'rewardStep': 0.841785580277552, 'errorList': [], 'lossList': [0.0, -1.3355200517177581, 0.0, 29.62207580089569, 0.0, 0.0, 0.0], 'rewardMean': 0.7470670630250288, 'totalEpisodes': 131, 'stepsPerEpisode': 320, 'rewardPerEpisode': 278.9085979520105
'totalSteps': 17920, 'rewardStep': 0.9244178121267577, 'errorList': [], 'lossList': [0.0, -1.3468137794733048, 0.0, 41.33056678295136, 0.0, 0.0, 0.0], 'rewardMean': 0.7659110363464671, 'totalEpisodes': 135, 'stepsPerEpisode': 264, 'rewardPerEpisode': 211.53801970999274
'totalSteps': 19200, 'rewardStep': 0.7899951761016237, 'errorList': [], 'lossList': [0.0, -1.3427817279100418, 0.0, 27.745112711191176, 0.0, 0.0, 0.0], 'rewardMean': 0.763043120517781, 'totalEpisodes': 136, 'stepsPerEpisode': 839, 'rewardPerEpisode': 657.2521948388886
'totalSteps': 20480, 'rewardStep': 0.938209184535153, 'errorList': [0.31734363053124165, 0.4201635980701887, 1.0664439071199767, 1.0713120161453091, 0.8441730741823675, 0.5154154055517072, 0.8034561194757288, 0.6148501360158237, 0.20045964634937877, 0.9085419542777945, 1.0818371531973285, 0.6786352153630958, 0.8449604112057159, 1.8883003655570845, 0.6039538700143107, 0.6540121006312286, 0.26270087326823793, 0.47924009113306787, 0.46178656782474026, 1.3494756324697434, 1.9398888450271663, 1.0204532533828643, 0.7528482715216365, 0.8871034022553906, 0.23551147585757112, 1.2480288822283214, 1.0060273267591824, 0.8729573195109458, 0.32820812541246613, 0.15460974693781457, 1.1822071542422246, 0.9212073516732552, 1.0434221445221772, 0.044711999451203854, 0.6561814439256035, 0.04701370668474468, 1.1922151153386498, 1.0373910787356684, 0.692135914729617, 1.0846984004855917, 0.9538817457632519, 0.39799936464916147, 1.4798187572403214, 1.1154700305566152, 1.5682282839340551, 0.7131023542398589, 0.7038667835210798, 0.9258900890622244, 0.3879909044549905, 0.8767079218332139], 'lossList': [0.0, -1.3595907998085022, 0.0, 6.419326146841049, 0.0, 0.0, 0.0], 'rewardMean': 0.772747476726297, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 893.4272264167881, 'successfulTests': 3
'totalSteps': 21760, 'rewardStep': 0.5715784968094925, 'errorList': [], 'lossList': [0.0, -1.3836987173557282, 0.0, 28.039953691959383, 0.0, 0.0, 0.0], 'rewardMean': 0.7440912951142865, 'totalEpisodes': 137, 'stepsPerEpisode': 353, 'rewardPerEpisode': 217.99770956262003
'totalSteps': 23040, 'rewardStep': 0.6473106092319447, 'errorList': [], 'lossList': [0.0, -1.3874082636833192, 0.0, 7.145078504085541, 0.0, 0.0, 0.0], 'rewardMean': 0.7368381964300539, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.7207382382725
'totalSteps': 24320, 'rewardStep': 0.9373708005080681, 'errorList': [18.84316434614394, 11.293736654977812, 21.90611967355843, 4.773586528489625, 15.84098642734859, 15.162622894154044, 15.777644259834956, 4.268076691701682, 11.098570598359922, 10.55120898170973, 4.320901782625126, 14.528908693441267, 3.199083418044974, 9.886351994499355, 8.618481052603146, 14.374783331073504, 1.9342864326677502, 10.836758088480277, 8.019079798982123, 13.5969974209448, 8.675787928572854, 14.233721447329673, 14.54744853930769, 18.50835746385141, 17.362017049305372, 18.156084570235443, 5.736313190095358, 10.752607532738164, 12.041106874172947, 14.542975639133616, 13.121599605003007, 10.836332459186007, 17.14352419016518, 9.737646849689796, 9.143337571463906, 19.579899660690444, 15.067221224028069, 10.950461512470376, 0.4005211461096752, 19.559370146615912, 6.094984056533754, 0.3690799194122745, 13.836265037445893, 8.565175000687097, 12.850346775200808, 9.997163233166745, 10.804852971588286, 17.663150870328465, 13.761021557999584, 15.30622148426363], 'lossList': [0.0, -1.4114519727230073, 0.0, 130.88111661911012, 0.0, 0.0, 0.0], 'rewardMean': 0.7609585053174166, 'totalEpisodes': 142, 'stepsPerEpisode': 29, 'rewardPerEpisode': 22.471692674633758, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.7268904826022295, 'errorList': [], 'lossList': [0.0, -1.4142068660259246, 0.0, 104.63610627174377, 0.0, 0.0, 0.0], 'rewardMean': 0.7721711947942316, 'totalEpisodes': 148, 'stepsPerEpisode': 95, 'rewardPerEpisode': 74.9747360158855
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=20480, timeSpent=104.11
