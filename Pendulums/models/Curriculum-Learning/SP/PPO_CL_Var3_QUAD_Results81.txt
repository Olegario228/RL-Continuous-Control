#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 81
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6142044171128439, 'errorList': [], 'lossList': [0.0, -1.4177309358119965, 0.0, 26.488723430633545, 0.0, 0.0, 0.0], 'rewardMean': 0.7887469845117768, 'totalEpisodes': 16, 'stepsPerEpisode': 359, 'rewardPerEpisode': 252.18162750311038
'totalSteps': 3840, 'rewardStep': 0.7733261317111562, 'errorList': [], 'lossList': [0.0, -1.412829509973526, 0.0, 28.168435554504395, 0.0, 0.0, 0.0], 'rewardMean': 0.7836067002449033, 'totalEpisodes': 21, 'stepsPerEpisode': 325, 'rewardPerEpisode': 218.92576564359717
'totalSteps': 5120, 'rewardStep': 0.7870854247665725, 'errorList': [], 'lossList': [0.0, -1.4107151973247527, 0.0, 51.48507125854492, 0.0, 0.0, 0.0], 'rewardMean': 0.7844763813753206, 'totalEpisodes': 29, 'stepsPerEpisode': 68, 'rewardPerEpisode': 54.440246938639596
'totalSteps': 6400, 'rewardStep': 0.4901388973879061, 'errorList': [], 'lossList': [0.0, -1.4092320221662522, 0.0, 57.60955809593201, 0.0, 0.0, 0.0], 'rewardMean': 0.7256088845778377, 'totalEpisodes': 37, 'stepsPerEpisode': 83, 'rewardPerEpisode': 61.368119954080804
'totalSteps': 7680, 'rewardStep': 0.9267518583225242, 'errorList': [], 'lossList': [0.0, -1.3921980440616608, 0.0, 37.71627224683762, 0.0, 0.0, 0.0], 'rewardMean': 0.7591327135352856, 'totalEpisodes': 42, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.374608291196097
'totalSteps': 8960, 'rewardStep': 0.9506417514808849, 'errorList': [335.96016138903207, 284.1896394363966, 328.2121732218909, 297.4754328828921, 292.1486973569582, 258.78224521803526, 290.4115048968523, 308.6816719257862, 268.1763231286289, 279.4842663658371, 268.3429171253231, 307.3270562852582, 334.8650750470624, 233.158215153341, 338.6280335927999, 257.011027397714, 321.2669235280941, 230.0849573279124, 270.7893640576639, 286.3968245977783, 303.397535337945, 213.01904952202605, 321.8157213746953, 339.2540504897612, 311.4996124819931, 299.2482382706485, 131.05196127185366, 299.4609755330458, 356.66962529722724, 299.2366144375923, 267.96835140902346, 262.4124150610645, 287.6179097561211, 307.2835798278035, 169.42953415197317, 325.70244281736893, 306.6720438388521, 332.9939883361867, 293.39377813197, 270.2448404648895, 210.21027743910656, 333.492114840735, 243.62775623700392, 309.26875376111894, 318.2536142827112, 279.93782649180116, 315.6430789499406, 296.56130637698504, 274.87237913228194, 301.86654343858186], 'lossList': [0.0, -1.3852653110027313, 0.0, 143.15429515838622, 0.0, 0.0, 0.0], 'rewardMean': 0.7864911475275139, 'totalEpisodes': 58, 'stepsPerEpisode': 65, 'rewardPerEpisode': 54.252894048717785, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.5813096362603873, 'errorList': [], 'lossList': [0.0, -1.3856810104846955, 0.0, 122.463233127594, 0.0, 0.0, 0.0], 'rewardMean': 0.7608434586191231, 'totalEpisodes': 82, 'stepsPerEpisode': 91, 'rewardPerEpisode': 62.91052764176098
'totalSteps': 11520, 'rewardStep': 0.6508485064031256, 'errorList': [], 'lossList': [0.0, -1.3712713086605073, 0.0, 40.64486652374268, 0.0, 0.0, 0.0], 'rewardMean': 0.74862179726179, 'totalEpisodes': 93, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.523900247749651
'totalSteps': 12800, 'rewardStep': 0.7061921031219157, 'errorList': [], 'lossList': [0.0, -1.3509397637844085, 0.0, 22.34689799070358, 0.0, 0.0, 0.0], 'rewardMean': 0.7443788278478026, 'totalEpisodes': 102, 'stepsPerEpisode': 88, 'rewardPerEpisode': 59.891707685372815
'totalSteps': 14080, 'rewardStep': 0.7070882485670247, 'errorList': [], 'lossList': [0.0, -1.352826959490776, 0.0, 12.455364108085632, 0.0, 0.0, 0.0], 'rewardMean': 0.7187586975134341, 'totalEpisodes': 108, 'stepsPerEpisode': 162, 'rewardPerEpisode': 135.3826005974604
'totalSteps': 15360, 'rewardStep': 0.495960681164125, 'errorList': [], 'lossList': [0.0, -1.355533556342125, 0.0, 8.17070242881775, 0.0, 0.0, 0.0], 'rewardMean': 0.7069343239185623, 'totalEpisodes': 113, 'stepsPerEpisode': 185, 'rewardPerEpisode': 132.24430203954412
'totalSteps': 16640, 'rewardStep': 0.8287298373278243, 'errorList': [], 'lossList': [0.0, -1.3610464465618133, 0.0, 5.922772092819214, 0.0, 0.0, 0.0], 'rewardMean': 0.712474694480229, 'totalEpisodes': 117, 'stepsPerEpisode': 412, 'rewardPerEpisode': 327.2805973411279
'totalSteps': 17920, 'rewardStep': 0.8044379620200578, 'errorList': [], 'lossList': [0.0, -1.3556985145807265, 0.0, 5.8195504301786425, 0.0, 0.0, 0.0], 'rewardMean': 0.7142099482055776, 'totalEpisodes': 118, 'stepsPerEpisode': 694, 'rewardPerEpisode': 537.895611285953
'totalSteps': 19200, 'rewardStep': 0.47760746009786986, 'errorList': [], 'lossList': [0.0, -1.3410890513658524, 0.0, 4.391353796124458, 0.0, 0.0, 0.0], 'rewardMean': 0.712956804476574, 'totalEpisodes': 122, 'stepsPerEpisode': 365, 'rewardPerEpisode': 282.6686034928517
'totalSteps': 20480, 'rewardStep': 0.5614739153944225, 'errorList': [], 'lossList': [0.0, -1.330205420255661, 0.0, 2.6855760887265205, 0.0, 0.0, 0.0], 'rewardMean': 0.6764290101837639, 'totalEpisodes': 124, 'stepsPerEpisode': 314, 'rewardPerEpisode': 240.62990871538568
'totalSteps': 21760, 'rewardStep': 0.6310655290064993, 'errorList': [], 'lossList': [0.0, -1.3256991320848466, 0.0, 4.338850331306458, 0.0, 0.0, 0.0], 'rewardMean': 0.6444713879363253, 'totalEpisodes': 124, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 875.7872848940973
'totalSteps': 23040, 'rewardStep': 0.9262939695730572, 'errorList': [], 'lossList': [0.0, -1.3202270257472992, 0.0, 2.0222959512472154, 0.0, 0.0, 0.0], 'rewardMean': 0.6789698212675923, 'totalEpisodes': 124, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.124047385429
'totalSteps': 24320, 'rewardStep': 0.6559358174861645, 'errorList': [], 'lossList': [0.0, -1.2751706790924073, 0.0, 1.2570928076654673, 0.0, 0.0, 0.0], 'rewardMean': 0.6794785523758962, 'totalEpisodes': 124, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1116.1832035140146
'totalSteps': 25600, 'rewardStep': 0.9447367356761439, 'errorList': [0.056337144591257625, 0.059933918075699644, 0.06745301716717726, 0.05489600449621448, 0.06071618593554799, 0.10354411588629138, 0.08165307362487918, 0.07099968961141084, 0.10462154984140225, 0.06561090727933741, 0.059335396426480475, 0.08061103090780482, 0.08629313711905405, 0.06350358156240096, 0.06319354763537469, 0.08424649462075628, 0.08076751385255593, 0.0607489472152899, 0.0791156982037236, 0.08906378978010754, 0.07299569064148384, 0.06390062041159299, 0.06521475062890289, 0.10102460740106575, 0.06426988363515154, 0.06659402406633029, 0.08576694982999739, 0.07495821272039974, 0.0846923378349254, 0.061503672387595666, 0.08310165191719218, 0.059239347028829506, 0.058166122093850116, 0.1051068956398732, 0.07503400324976266, 0.06524210104367224, 0.126491519135122, 0.07393751461138881, 0.0703689029687964, 0.10596689717511912, 0.06358728816449481, 0.06208637130623078, 0.06486016555220987, 0.07544150936849624, 0.06707950867041196, 0.08485949623117175, 0.06383508999225296, 0.0694997097376154, 0.07531379187324184, 0.05881055796922661], 'lossList': [0.0, -1.2268989670276642, 0.0, 1.1286928152292968, 0.0, 0.0, 0.0], 'rewardMean': 0.7033330156313189, 'totalEpisodes': 124, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.1099418787978, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.06
