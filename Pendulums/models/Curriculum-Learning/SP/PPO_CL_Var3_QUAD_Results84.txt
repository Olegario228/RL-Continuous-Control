#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 84
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8720268878651635, 'errorList': [], 'lossList': [0.0, -1.4163150215148925, 0.0, 29.461157369613648, 0.0, 0.0, 0.0], 'rewardMean': 0.6180484968203761, 'totalEpisodes': 21, 'stepsPerEpisode': 37, 'rewardPerEpisode': 30.552855364837185
'totalSteps': 3840, 'rewardStep': 0.5393630108891518, 'errorList': [], 'lossList': [0.0, -1.4316968882083894, 0.0, 32.44537372112274, 0.0, 0.0, 0.0], 'rewardMean': 0.591820001509968, 'totalEpisodes': 28, 'stepsPerEpisode': 168, 'rewardPerEpisode': 114.10685416039063
'totalSteps': 5120, 'rewardStep': 0.39013782805893293, 'errorList': [], 'lossList': [0.0, -1.428633016347885, 0.0, 19.412741956710814, 0.0, 0.0, 0.0], 'rewardMean': 0.5413994581472092, 'totalEpisodes': 30, 'stepsPerEpisode': 340, 'rewardPerEpisode': 213.6475908453615
'totalSteps': 6400, 'rewardStep': 0.552624193546698, 'errorList': [], 'lossList': [0.0, -1.4266430574655533, 0.0, 33.7312987446785, 0.0, 0.0, 0.0], 'rewardMean': 0.543644405227107, 'totalEpisodes': 33, 'stepsPerEpisode': 348, 'rewardPerEpisode': 262.77585320835965
'totalSteps': 7680, 'rewardStep': 0.8535895793214743, 'errorList': [], 'lossList': [0.0, -1.4117247503995896, 0.0, 40.19884905099869, 0.0, 0.0, 0.0], 'rewardMean': 0.5953019342428348, 'totalEpisodes': 37, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.347254264524523
'totalSteps': 8960, 'rewardStep': 0.7096178977093256, 'errorList': [], 'lossList': [0.0, -1.401276286840439, 0.0, 184.5874365234375, 0.0, 0.0, 0.0], 'rewardMean': 0.6116327861666192, 'totalEpisodes': 61, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.185700330931856
'totalSteps': 10240, 'rewardStep': 0.49728141088145206, 'errorList': [], 'lossList': [0.0, -1.4020298916101455, 0.0, 75.9584483718872, 0.0, 0.0, 0.0], 'rewardMean': 0.5973388642559734, 'totalEpisodes': 77, 'stepsPerEpisode': 115, 'rewardPerEpisode': 92.11787412486845
'totalSteps': 11520, 'rewardStep': 0.7733896038334755, 'errorList': [], 'lossList': [0.0, -1.4107343178987504, 0.0, 22.54031163215637, 0.0, 0.0, 0.0], 'rewardMean': 0.6169000575423625, 'totalEpisodes': 88, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.5763332014385365
'totalSteps': 12800, 'rewardStep': 0.8303131464131135, 'errorList': [], 'lossList': [0.0, -1.4042046552896499, 0.0, 45.47471364974976, 0.0, 0.0, 0.0], 'rewardMean': 0.6382413664294375, 'totalEpisodes': 98, 'stepsPerEpisode': 57, 'rewardPerEpisode': 51.318445008201785
'totalSteps': 14080, 'rewardStep': 0.8636537135214051, 'errorList': [], 'lossList': [0.0, -1.3769534158706664, 0.0, 10.356430118083955, 0.0, 0.0, 0.0], 'rewardMean': 0.6881997272040191, 'totalEpisodes': 104, 'stepsPerEpisode': 111, 'rewardPerEpisode': 94.69749482401629
'totalSteps': 15360, 'rewardStep': 0.60026387778083, 'errorList': [], 'lossList': [0.0, -1.3469745177030563, 0.0, 32.47823407888413, 0.0, 0.0, 0.0], 'rewardMean': 0.6610234261955859, 'totalEpisodes': 110, 'stepsPerEpisode': 249, 'rewardPerEpisode': 194.98612323936766
'totalSteps': 16640, 'rewardStep': 0.6004369408829713, 'errorList': [], 'lossList': [0.0, -1.3424220097064972, 0.0, 8.912020125389098, 0.0, 0.0, 0.0], 'rewardMean': 0.6671308191949679, 'totalEpisodes': 114, 'stepsPerEpisode': 203, 'rewardPerEpisode': 171.35341948538803
'totalSteps': 17920, 'rewardStep': 0.4559100329966901, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6640366236337428, 'totalEpisodes': 117, 'stepsPerEpisode': 581, 'rewardPerEpisode': 434.647395327341
'totalSteps': 19200, 'rewardStep': 0.9093927857146962, 'errorList': [], 'lossList': [0.0, -1.3398884010314942, 0.0, 9.11005517244339, 0.0, 0.0, 0.0], 'rewardMean': 0.669616944273065, 'totalEpisodes': 121, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.657235245025129
'totalSteps': 20480, 'rewardStep': 0.9414607217516522, 'errorList': [1.6528346016243243, 0.5199445050827982, 1.080275040253574, 0.9264980887261212, 0.07741862191573966, 0.061284714948083845, 0.7489048264414725, 0.09755096061454344, 0.11865799129654257, 0.24884172290045262, 0.23620583294717964, 0.5122598288642649, 0.33480317036391527, 4.6932033397962085, 0.5175263003701338, 0.259894846767435, 0.3927668054600632, 0.1272632847898024, 1.7682289012676833, 0.4467838445288597, 2.366410777036158, 2.6942604199370224, 0.709881923221502, 2.2793128269899117, 0.5036703666254468, 0.9934518373212807, 0.7639831131374839, 8.310535638522607, 0.345845590754291, 1.6150111585663176, 0.854463090650443, 0.08034206487004389, 0.6246215623393203, 0.2314776075992488, 0.08345501523909378, 0.0937801795864986, 1.8182345892496816, 0.10535925095518692, 0.9088510231438549, 0.8451095959344797, 0.11534554336846695, 1.6605396536636265, 0.6199101004936939, 0.09170048110321774, 1.2981881702554532, 0.9734675235985318, 1.313691532516792, 0.5729511795621869, 1.2732030839837654, 0.3479269176912426], 'lossList': [0.0, -1.3103214710950852, 0.0, 4.805392926335335, 0.0, 0.0, 0.0], 'rewardMean': 0.6928012266772976, 'totalEpisodes': 124, 'stepsPerEpisode': 138, 'rewardPerEpisode': 118.08164152066823, 'successfulTests': 11
'totalSteps': 21760, 'rewardStep': 0.5560350521268964, 'errorList': [], 'lossList': [0.0, -1.2832574820518494, 0.0, 4.51138800740242, 0.0, 0.0, 0.0], 'rewardMean': 0.6986765908018421, 'totalEpisodes': 126, 'stepsPerEpisode': 528, 'rewardPerEpisode': 434.15327705073776
'totalSteps': 23040, 'rewardStep': 0.7920257475576999, 'errorList': [], 'lossList': [0.0, -1.2641396421194075, 0.0, 3.988400903940201, 0.0, 0.0, 0.0], 'rewardMean': 0.7005402051742644, 'totalEpisodes': 127, 'stepsPerEpisode': 1107, 'rewardPerEpisode': 909.1207083433982
'totalSteps': 24320, 'rewardStep': 0.8116837255345767, 'errorList': [], 'lossList': [0.0, -1.236951640844345, 0.0, 1.4384510883688926, 0.0, 0.0, 0.0], 'rewardMean': 0.6986772630864108, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.4821166549063
'totalSteps': 25600, 'rewardStep': 0.9734144945208542, 'errorList': [0.04202787558099703, 0.04599290708849329, 0.04955472565235749, 0.046915570444387183, 0.04756481561277414, 0.04969939375554401, 0.05498132249074707, 0.05244555772678866, 0.04322131841542916, 0.04532189459663095, 0.0512939379202349, 0.04183612000746521, 0.04260700855988431, 0.04891747942094668, 0.041837614125891716, 0.04243609071682176, 0.050380568917884436, 0.05011570744815414, 0.052094825113233545, 0.04187287801074632, 0.04211474173331477, 0.04869888517167766, 0.04984745136102877, 0.048198698250631465, 0.04987262675230733, 0.04912294904297662, 0.051411290186125426, 0.04263569710036355, 0.0469909177661043, 0.042387319557103086, 0.04537921440670571, 0.04406446903764771, 0.050647550451366694, 0.05654439975999619, 0.053378330977694635, 0.04674144790202428, 0.04234288111338599, 0.04695447684482225, 0.047531401234853846, 0.04619192519508978, 0.04204978612989062, 0.042227037564086016, 0.0438614715262991, 0.043386351073591255, 0.051617962476068965, 0.053717815228061086, 0.04190669023965218, 0.053031466571069746, 0.05229492796333371, 0.05172798050423232], 'lossList': [0.0, -1.209549994468689, 0.0, 1.3887400098145009, 0.0, 0.0, 0.0], 'rewardMean': 0.7096533411863557, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.4404933026237, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.42
