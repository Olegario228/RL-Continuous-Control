#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 85
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9234295383614873, 'errorList': [], 'lossList': [0.0, -1.4074133741855621, 0.0, 26.84161251783371, 0.0, 0.0, 0.0], 'rewardMean': 0.8616142494302252, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 382.4916853746715
'totalSteps': 3840, 'rewardStep': 0.619037223492739, 'errorList': [], 'lossList': [0.0, -1.4024624770879746, 0.0, 37.091099526882175, 0.0, 0.0, 0.0], 'rewardMean': 0.7807552407843965, 'totalEpisodes': 12, 'stepsPerEpisode': 262, 'rewardPerEpisode': 198.34141739237845
'totalSteps': 5120, 'rewardStep': 0.6992850982887737, 'errorList': [], 'lossList': [0.0, -1.408173828125, 0.0, 25.712173721790315, 0.0, 0.0, 0.0], 'rewardMean': 0.7603877051604908, 'totalEpisodes': 13, 'stepsPerEpisode': 472, 'rewardPerEpisode': 348.8825784051781
'totalSteps': 6400, 'rewardStep': 0.8495798054273318, 'errorList': [], 'lossList': [0.0, -1.4012440687417984, 0.0, 18.558815419971943, 0.0, 0.0, 0.0], 'rewardMean': 0.778226125213859, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 984.163949630307
'totalSteps': 7680, 'rewardStep': 0.8603433515056987, 'errorList': [], 'lossList': [0.0, -1.3851685220003127, 0.0, 33.06524021148682, 0.0, 0.0, 0.0], 'rewardMean': 0.7919123295958324, 'totalEpisodes': 15, 'stepsPerEpisode': 297, 'rewardPerEpisode': 228.9281477498761
'totalSteps': 8960, 'rewardStep': 0.5494249392213495, 'errorList': [], 'lossList': [0.0, -1.3789934563636779, 0.0, 192.74031589508056, 0.0, 0.0, 0.0], 'rewardMean': 0.7572712738280492, 'totalEpisodes': 33, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.618625364221529
'totalSteps': 10240, 'rewardStep': 0.8807538244391774, 'errorList': [], 'lossList': [0.0, -1.3770356422662735, 0.0, 189.55732421875, 0.0, 0.0, 0.0], 'rewardMean': 0.7727065926544401, 'totalEpisodes': 68, 'stepsPerEpisode': 44, 'rewardPerEpisode': 33.92034345601798
'totalSteps': 11520, 'rewardStep': 0.4254458081719404, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7032544357579402, 'totalEpisodes': 104, 'stepsPerEpisode': 46, 'rewardPerEpisode': 37.21561447620852
'totalSteps': 12800, 'rewardStep': 0.9268380670372031, 'errorList': [], 'lossList': [0.0, -1.3791221445798874, 0.0, 115.3903161239624, 0.0, 0.0, 0.0], 'rewardMean': 0.7159583464117641, 'totalEpisodes': 139, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.674834501570135
'totalSteps': 14080, 'rewardStep': 0.6159870990025749, 'errorList': [], 'lossList': [0.0, -1.3777446722984314, 0.0, 61.50960185050965, 0.0, 0.0, 0.0], 'rewardMean': 0.6852141024758729, 'totalEpisodes': 167, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.65528890015747
'totalSteps': 15360, 'rewardStep': 0.9623503859389955, 'errorList': [213.2470883320476, 165.78197075591683, 245.93712230016257, 222.86083874973218, 228.03694214467643, 228.67472137144128, 235.22308148760206, 104.06736607675236, 225.681216733518, 164.95416679688557, 86.58560809273747, 144.70185146096586, 237.15105948897866, 143.46375818000243, 261.31397874267, 176.5442186229805, 105.30584475512926, 167.06948862406884, 157.4441651305023, 213.8971012182374, 213.73157427881176, 193.3293692466534, 204.40140998811142, 234.49170614350834, 226.96083389393044, 248.71234742409473, 196.81200532479673, 215.47892624036865, 130.84838411069023, 228.95594592644997, 231.97519737453197, 97.60841158912184, 39.619087750897286, 44.76059815102566, 189.88330765070535, 177.65959462949962, 195.89340492530528, 224.77909403527718, 172.70380808054188, 39.537656436255595, 118.13166631706072, 226.64078387455, 231.2214044576745, 228.62594947749753, 80.04109956670227, 227.81176292014337, 170.64159162504765, 189.3473550726621, 137.33707492559876, 175.81553061197255], 'lossList': [0.0, -1.3722357296943664, 0.0, 33.462051162719725, 0.0, 0.0, 0.0], 'rewardMean': 0.7195454187204985, 'totalEpisodes': 187, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.732391009974466, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5565423896793427, 'errorList': [], 'lossList': [0.0, -1.3669641810655593, 0.0, 31.399010753631593, 0.0, 0.0, 0.0], 'rewardMean': 0.7052711478595554, 'totalEpisodes': 200, 'stepsPerEpisode': 70, 'rewardPerEpisode': 48.366480545846336
'totalSteps': 17920, 'rewardStep': 0.625075273615703, 'errorList': [], 'lossList': [0.0, -1.3532796996831893, 0.0, 15.805945563316346, 0.0, 0.0, 0.0], 'rewardMean': 0.6828206946783926, 'totalEpisodes': 207, 'stepsPerEpisode': 271, 'rewardPerEpisode': 208.63688917917284
'totalSteps': 19200, 'rewardStep': 0.8825890742090076, 'errorList': [], 'lossList': [0.0, -1.324778447151184, 0.0, 27.882202882766723, 0.0, 0.0, 0.0], 'rewardMean': 0.6850452669487235, 'totalEpisodes': 216, 'stepsPerEpisode': 77, 'rewardPerEpisode': 63.201983436886145
'totalSteps': 20480, 'rewardStep': 0.7322901937754617, 'errorList': [], 'lossList': [0.0, -1.3173894548416138, 0.0, 8.32339037179947, 0.0, 0.0, 0.0], 'rewardMean': 0.7033317924041346, 'totalEpisodes': 220, 'stepsPerEpisode': 411, 'rewardPerEpisode': 333.7016939820997
'totalSteps': 21760, 'rewardStep': 0.9513353943253835, 'errorList': [9.383892826505344, 1.5095098115067125, 23.468804088787078, 7.510410122704438, 8.08353426199262, 1.4967504613715317, 2.0715507909182698, 10.55643513541201, 7.099634335672773, 0.2718800205334056, 1.0012196883206064, 1.2878225806214332, 0.9891114509467833, 1.5704278646238146, 1.646243880154953, 6.802580701347289, 1.1933783691261692, 17.973663097369123, 19.044796131278876, 1.6998770270202628, 0.1865827433746132, 1.981912997161216, 0.4482883143483514, 9.163214468393015, 8.022664861871107, 6.091853131464612, 8.931743030600623, 1.5831897492849232, 3.2210705843735035, 8.779543888229957, 3.5165169686929088, 16.708770298555837, 9.181519552196836, 2.519446044381024, 0.5927329622053962, 0.5833617163158437, 1.6725394124666937, 22.962565445761133, 1.7363698633644975, 17.03883848360585, 6.081177951134123, 1.2320847792596672, 0.3568160644450906, 12.013924027479717, 14.486136779629772, 0.025999449758047903, 12.311211173754488, 1.8709607356175295, 1.1231303015627105, 1.8329534468094602], 'lossList': [0.0, -1.3233074641227722, 0.0, 20.47373471736908, 0.0, 0.0, 0.0], 'rewardMean': 0.7103899493927552, 'totalEpisodes': 226, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.88616878430886, 'successfulTests': 2
'totalSteps': 23040, 'rewardStep': 0.3026309955594492, 'errorList': [], 'lossList': [0.0, -1.3243379092216492, 0.0, 5.2499056059122085, 0.0, 0.0, 0.0], 'rewardMean': 0.6981084681315062, 'totalEpisodes': 229, 'stepsPerEpisode': 169, 'rewardPerEpisode': 122.58210399258694
'totalSteps': 24320, 'rewardStep': 0.91436774745361, 'errorList': [], 'lossList': [0.0, -1.3243645948171616, 0.0, 5.137549741268158, 0.0, 0.0, 0.0], 'rewardMean': 0.7470006620596731, 'totalEpisodes': 232, 'stepsPerEpisode': 83, 'rewardPerEpisode': 71.60488636096933
'totalSteps': 25600, 'rewardStep': 0.8609481317974695, 'errorList': [], 'lossList': [0.0, -1.3047393572330475, 0.0, 2.9848560498654844, 0.0, 0.0, 0.0], 'rewardMean': 0.7404116685356998, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.5271378456196
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=21760, timeSpent=105.43
