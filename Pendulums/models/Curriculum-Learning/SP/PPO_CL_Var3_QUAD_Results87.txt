#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 87
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.8107189940130267, 'errorList': [], 'lossList': [0.0, -1.4435795265436173, 0.0, 25.175679281949996, 0.0, 0.0, 0.0], 'rewardMean': 0.6243747931094503, 'totalEpisodes': 11, 'stepsPerEpisode': 900, 'rewardPerEpisode': 601.6264226455971
'totalSteps': 3840, 'rewardStep': 0.8922935497760514, 'errorList': [], 'lossList': [0.0, -1.4472895991802215, 0.0, 36.31211127281189, 0.0, 0.0, 0.0], 'rewardMean': 0.7136810453316507, 'totalEpisodes': 14, 'stepsPerEpisode': 489, 'rewardPerEpisode': 387.85200452833783
'totalSteps': 5120, 'rewardStep': 0.5509943692953649, 'errorList': [], 'lossList': [0.0, -1.422908633351326, 0.0, 16.692463824748994, 0.0, 0.0, 0.0], 'rewardMean': 0.6730093763225793, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 857.6877253344161
'totalSteps': 6400, 'rewardStep': 0.26907302437808683, 'errorList': [], 'lossList': [0.0, -1.4004566937685012, 0.0, 40.23541462182999, 0.0, 0.0, 0.0], 'rewardMean': 0.5922221059336807, 'totalEpisodes': 17, 'stepsPerEpisode': 187, 'rewardPerEpisode': 137.8442773335621
'totalSteps': 7680, 'rewardStep': 0.6711206311586434, 'errorList': [], 'lossList': [0.0, -1.3989450353384019, 0.0, 124.41729751586914, 0.0, 0.0, 0.0], 'rewardMean': 0.6053718601378412, 'totalEpisodes': 31, 'stepsPerEpisode': 57, 'rewardPerEpisode': 45.782037997978094
'totalSteps': 8960, 'rewardStep': 0.9188638587182797, 'errorList': [], 'lossList': [0.0, -1.3986882066726685, 0.0, 180.70093776702882, 0.0, 0.0, 0.0], 'rewardMean': 0.6501564313636182, 'totalEpisodes': 60, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.175375603743225
'totalSteps': 10240, 'rewardStep': 0.671358538760211, 'errorList': [], 'lossList': [0.0, -1.3934015119075776, 0.0, 147.2647142791748, 0.0, 0.0, 0.0], 'rewardMean': 0.6528066947881923, 'totalEpisodes': 89, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.166580577017905
'totalSteps': 11520, 'rewardStep': 0.6355799887491295, 'errorList': [], 'lossList': [0.0, -1.389482340812683, 0.0, 65.81162937164306, 0.0, 0.0, 0.0], 'rewardMean': 0.6508926163394075, 'totalEpisodes': 111, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.506903103233138
'totalSteps': 12800, 'rewardStep': 0.5031861713342749, 'errorList': [], 'lossList': [0.0, -1.3806412971019746, 0.0, 37.06192280292511, 0.0, 0.0, 0.0], 'rewardMean': 0.6361219718388943, 'totalEpisodes': 126, 'stepsPerEpisode': 89, 'rewardPerEpisode': 63.07461141976459
'totalSteps': 14080, 'rewardStep': 0.7986135744550931, 'errorList': [], 'lossList': [0.0, -1.3806433802843094, 0.0, 12.20266937494278, 0.0, 0.0, 0.0], 'rewardMean': 0.6721802700638161, 'totalEpisodes': 132, 'stepsPerEpisode': 135, 'rewardPerEpisode': 96.68196471492799
'totalSteps': 15360, 'rewardStep': 0.5522873116905236, 'errorList': [], 'lossList': [0.0, -1.3840726220607757, 0.0, 8.221431107521058, 0.0, 0.0, 0.0], 'rewardMean': 0.6463371018315659, 'totalEpisodes': 137, 'stepsPerEpisode': 144, 'rewardPerEpisode': 103.24076223789933
'totalSteps': 16640, 'rewardStep': 0.9021393219154538, 'errorList': [], 'lossList': [0.0, -1.3832952237129212, 0.0, 6.663690533638, 0.0, 0.0, 0.0], 'rewardMean': 0.6473216790455061, 'totalEpisodes': 144, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.696018578098402
'totalSteps': 17920, 'rewardStep': 0.9345006600856559, 'errorList': [36.58985703985798, 0.38917041287052534, 115.98065906295439, 1.9822969488193563, 6.408051088785745, 28.864942252140413, 3.2376106117616006, 0.8720109646444324, 58.62243776748872, 67.13076022917114, 2.3326204169358955, 5.519767697180866, 27.357384137287667, 12.775039782747797, 0.9588324357315227, 26.61192765783026, 0.38915860691968646, 62.22530240363458, 1.200601082986123, 3.0876294864518243, 59.156473761353595, 1.2650757555139682, 109.03603463243483, 0.45478826398090005, 4.491012514436242, 2.1230301613618807, 1.8100646307539048, 0.3198598570613586, 34.21825126644381, 1.0102665534971422, 0.38098973853184426, 3.4855575190714925, 9.438278297575401, 37.75394907769957, 1.2429160162907993, 94.76917201286666, 0.2667544566152766, 101.94364483435984, 51.74691682826986, 0.2752132127839552, 83.95441380347515, 2.488331008029841, 81.89895006524057, 34.6225200702803, 60.236162059578504, 25.073412198649844, 20.02928190743242, 57.15018359629842, 43.21568745921239, 2.511689082083429], 'lossList': [0.0, -1.3716967958211899, 0.0, 11.831094727516174, 0.0, 0.0, 0.0], 'rewardMean': 0.6856723081245353, 'totalEpisodes': 152, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.267063935834406, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.8321852234925807, 'errorList': [], 'lossList': [0.0, -1.3503053265810012, 0.0, 10.253427197933197, 0.0, 0.0, 0.0], 'rewardMean': 0.7419835280359846, 'totalEpisodes': 154, 'stepsPerEpisode': 755, 'rewardPerEpisode': 581.8932658316926
'totalSteps': 20480, 'rewardStep': 0.7330003126799882, 'errorList': [], 'lossList': [0.0, -1.3293740993738175, 0.0, 4.5616029900312425, 0.0, 0.0, 0.0], 'rewardMean': 0.748171496188119, 'totalEpisodes': 155, 'stepsPerEpisode': 537, 'rewardPerEpisode': 413.9281782393506
'totalSteps': 21760, 'rewardStep': 0.8335435782612345, 'errorList': [], 'lossList': [0.0, -1.3077014780044556, 0.0, 4.823393479734659, 0.0, 0.0, 0.0], 'rewardMean': 0.7396394681424144, 'totalEpisodes': 155, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.1194402308502
'totalSteps': 23040, 'rewardStep': 0.9426617579978962, 'errorList': [0.1956179113938409, 0.18060188742038294, 0.1782558110500838, 0.17124881510926176, 0.19793386861868092, 0.16851924071990146, 0.21564709229349183, 0.17050587585196914, 0.1629394944949608, 0.16022322645978004, 0.2010185203366618, 0.16977088862961293, 0.15327780387659617, 0.15105436782804318, 0.22770945367135198, 0.1734688927056197, 0.1633782923237605, 0.18415057499467594, 0.20390747292695185, 0.15105832228112057, 0.18865534446060908, 0.1680799599163577, 0.15153925058942833, 0.19125072583806432, 0.1658123344982851, 0.15571317209265212, 0.16526272558670502, 0.1710654885757855, 0.17052097087930093, 0.17994837538102967, 0.17608106125790682, 0.18895137987554936, 0.18309289535578113, 0.19087057975024405, 0.15671628874018628, 0.19479152182799714, 0.13300408849730244, 0.17855176899298084, 0.17512751936238907, 0.18307569164453724, 0.16200730331419083, 0.17272423968453643, 0.15830911585036164, 0.18889306732494485, 0.18219836210188292, 0.18093009135514426, 0.16374620014101368, 0.2719419388188242, 0.2589501884967019, 0.14920762999836246], 'lossList': [0.0, -1.2674272221326828, 0.0, 3.580227843374014, 0.0, 0.0, 0.0], 'rewardMean': 0.766769790066183, 'totalEpisodes': 155, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.4033015278446, 'successfulTests': 44
'totalSteps': 24320, 'rewardStep': 0.8840062878461701, 'errorList': [], 'lossList': [0.0, -1.2492056453227998, 0.0, 2.0414666577428577, 0.0, 0.0, 0.0], 'rewardMean': 0.7916124199758872, 'totalEpisodes': 155, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.0431388462878
'totalSteps': 25600, 'rewardStep': 0.8479740945011698, 'errorList': [], 'lossList': [0.0, -1.2259866124391556, 0.0, 1.791543515138328, 0.0, 0.0, 0.0], 'rewardMean': 0.8260912122925766, 'totalEpisodes': 155, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.500802944722
#maxSuccessfulTests=44, maxSuccessfulTestsAtStep=23040, timeSpent=106.65
