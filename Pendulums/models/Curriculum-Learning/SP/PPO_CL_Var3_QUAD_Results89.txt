#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 89
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8646848870204505, 'errorList': [], 'lossList': [0.0, -1.4233570033311844, 0.0, 29.844857996702196, 0.0, 0.0, 0.0], 'rewardMean': 0.7683001465818969, 'totalEpisodes': 13, 'stepsPerEpisode': 317, 'rewardPerEpisode': 251.61167459329465
'totalSteps': 3840, 'rewardStep': 0.7349546861296868, 'errorList': [], 'lossList': [0.0, -1.4385679805278777, 0.0, 30.294396419525146, 0.0, 0.0, 0.0], 'rewardMean': 0.7571849930978268, 'totalEpisodes': 16, 'stepsPerEpisode': 168, 'rewardPerEpisode': 125.24457852953714
'totalSteps': 5120, 'rewardStep': 0.5830464685987606, 'errorList': [], 'lossList': [0.0, -1.4434202647209167, 0.0, 21.181413026452063, 0.0, 0.0, 0.0], 'rewardMean': 0.7136503619730603, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 940.0697758975816
'totalSteps': 6400, 'rewardStep': 0.7859270954028769, 'errorList': [], 'lossList': [0.0, -1.42534898519516, 0.0, 32.25822425365448, 0.0, 0.0, 0.0], 'rewardMean': 0.7281057086590236, 'totalEpisodes': 18, 'stepsPerEpisode': 867, 'rewardPerEpisode': 661.1514951135894
'totalSteps': 7680, 'rewardStep': 0.8871084699964569, 'errorList': [], 'lossList': [0.0, -1.423664585351944, 0.0, 44.057182648181914, 0.0, 0.0, 0.0], 'rewardMean': 0.7546061688819291, 'totalEpisodes': 21, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.508920254296303
'totalSteps': 8960, 'rewardStep': 0.5166247494419915, 'errorList': [], 'lossList': [0.0, -1.4205565589666367, 0.0, 266.87329040527345, 0.0, 0.0, 0.0], 'rewardMean': 0.7206088232476523, 'totalEpisodes': 56, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.328038834788041
'totalSteps': 10240, 'rewardStep': 0.9331252030089705, 'errorList': [236.5483295248877, 249.03801546922523, 234.0703094503175, 221.41287726926774, 218.72984500883325, 224.99955964640804, 272.4089254988221, 235.558246914758, 208.85906424401193, 195.82198861317647, 211.6157437745962, 233.80726138260437, 138.3683003309255, 260.270706187413, 174.38222599079822, 250.15598342441618, 247.4635248792005, 216.92778314755373, 211.9784929732825, 204.75123883529454, 234.6040144609765, 231.7787255625283, 231.670461715692, 252.241471657874, 255.44221989392148, 258.8838687578998, 259.858924628325, 174.90738774519977, 234.143204946214, 221.7082857270566, 157.34043753806654, 217.409856563701, 256.9435993978084, 241.4814875453676, 174.17321811520367, 239.1065252270389, 261.95120669491064, 263.55726108269505, 237.73265668172166, 189.04478599500857, 258.53361915184746, 248.87604264345012, 261.83158043901165, 244.2031760257355, 243.9879578478462, 240.85452663375608, 260.9557156093727, 216.23405659212287, 246.78398530393468, 261.8700824074853], 'lossList': [0.0, -1.4163892370462419, 0.0, 145.15155208587646, 0.0, 0.0, 0.0], 'rewardMean': 0.7471733707178171, 'totalEpisodes': 88, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.210206170080237, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6847931855570825, 'errorList': [], 'lossList': [0.0, -1.4184221404790878, 0.0, 94.33533782958985, 0.0, 0.0, 0.0], 'rewardMean': 0.740242239033291, 'totalEpisodes': 113, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.7058599781868
'totalSteps': 12800, 'rewardStep': 0.7283164225333401, 'errorList': [], 'lossList': [0.0, -1.4135308057069778, 0.0, 58.72583292961121, 0.0, 0.0, 0.0], 'rewardMean': 0.739049657383296, 'totalEpisodes': 132, 'stepsPerEpisode': 64, 'rewardPerEpisode': 50.31937881125262
'totalSteps': 14080, 'rewardStep': 0.0802178920244686, 'errorList': [], 'lossList': [0.0, -1.3902132332324981, 0.0, 22.216262152194975, 0.0, 0.0, 0.0], 'rewardMean': 0.6798799059714085, 'totalEpisodes': 142, 'stepsPerEpisode': 175, 'rewardPerEpisode': 120.1110581972081
'totalSteps': 15360, 'rewardStep': 0.46685399770676206, 'errorList': [], 'lossList': [0.0, -1.3673597580194474, 0.0, 47.41647737026214, 0.0, 0.0, 0.0], 'rewardMean': 0.6400968170400396, 'totalEpisodes': 157, 'stepsPerEpisode': 121, 'rewardPerEpisode': 87.60561726791722
'totalSteps': 16640, 'rewardStep': 0.46425235240498086, 'errorList': [], 'lossList': [0.0, -1.3471052688360214, 0.0, 30.187115325927735, 0.0, 0.0, 0.0], 'rewardMean': 0.613026583667569, 'totalEpisodes': 165, 'stepsPerEpisode': 54, 'rewardPerEpisode': 41.816626629434296
'totalSteps': 17920, 'rewardStep': 0.758896226227912, 'errorList': [], 'lossList': [0.0, -1.3370032858848573, 0.0, 15.405229341983794, 0.0, 0.0, 0.0], 'rewardMean': 0.6306115594304842, 'totalEpisodes': 172, 'stepsPerEpisode': 180, 'rewardPerEpisode': 150.4046751525996
'totalSteps': 19200, 'rewardStep': 0.5504738101809148, 'errorList': [], 'lossList': [0.0, -1.3368185555934906, 0.0, 9.299950275421143, 0.0, 0.0, 0.0], 'rewardMean': 0.6070662309082879, 'totalEpisodes': 175, 'stepsPerEpisode': 259, 'rewardPerEpisode': 208.99905604654262
'totalSteps': 20480, 'rewardStep': 0.8483228602055056, 'errorList': [], 'lossList': [0.0, -1.3192523956298827, 0.0, 5.747926917076111, 0.0, 0.0, 0.0], 'rewardMean': 0.6031876699291928, 'totalEpisodes': 179, 'stepsPerEpisode': 347, 'rewardPerEpisode': 297.66064401417793
'totalSteps': 21760, 'rewardStep': 0.6879021172051797, 'errorList': [], 'lossList': [0.0, -1.3134068876504899, 0.0, 4.501109168529511, 0.0, 0.0, 0.0], 'rewardMean': 0.6203154067055117, 'totalEpisodes': 180, 'stepsPerEpisode': 272, 'rewardPerEpisode': 231.4938094860227
'totalSteps': 23040, 'rewardStep': 0.7858139879763468, 'errorList': [], 'lossList': [0.0, -1.3272855669260024, 0.0, 17.657667735219, 0.0, 0.0, 0.0], 'rewardMean': 0.6055842852022494, 'totalEpisodes': 182, 'stepsPerEpisode': 722, 'rewardPerEpisode': 600.0270276414021
'totalSteps': 24320, 'rewardStep': 0.7566112200670092, 'errorList': [], 'lossList': [0.0, -1.3130700993537903, 0.0, 1.8443467032909393, 0.0, 0.0, 0.0], 'rewardMean': 0.612766088653242, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1045.2787661822401
'totalSteps': 25600, 'rewardStep': 0.974285123372768, 'errorList': [0.05892154428100182, 0.05832912236390327, 0.05826925960864167, 0.0775144394415159, 0.058697300165327364, 0.05844166218647477, 0.05828999974520988, 0.05823679618938768, 0.06209900142421803, 0.07429660617207631, 0.06650928244679592, 0.05845697644080229, 0.05784780802489257, 0.05833124422305358, 0.058193615129979254, 0.08392694839401742, 0.05823308108494319, 0.057593479739874116, 0.0579869831452108, 0.09501589754100073, 0.057848756996669914, 0.05801579179937464, 0.09047599500003603, 0.05832963859390824, 0.058237870101452156, 0.05896416781076467, 0.09896259807629411, 0.08741010269195303, 0.06656400230059563, 0.07575244960821771, 0.07394810869297673, 0.09138575572191754, 0.09925303243520861, 0.057300803499947935, 0.11223463658966504, 0.057825672815424514, 0.05816970599982497, 0.0576104611237204, 0.05838020461445924, 0.058275708004546775, 0.08614314042339044, 0.09787670699484696, 0.057988986033306184, 0.07732764761985945, 0.05787496464138286, 0.057377482707944065, 0.05815951360512804, 0.058005287690811015, 0.05735525173212358, 0.11087422172456231], 'lossList': [0.0, -1.2844349557161332, 0.0, 1.7381884282082318, 0.0, 0.0, 0.0], 'rewardMean': 0.6373629587371848, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.979659522665, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=104.89
