#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 92
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.8529004049006067, 'errorList': [], 'lossList': [0.0, -1.44288330078125, 0.0, 27.35370950937271, 0.0, 0.0, 0.0], 'rewardMean': 0.6653430900617024, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 651.9127100526671
'totalSteps': 3840, 'rewardStep': 0.8603051978151379, 'errorList': [], 'lossList': [0.0, -1.4594763016700745, 0.0, 41.28003453969956, 0.0, 0.0, 0.0], 'rewardMean': 0.7303304593128477, 'totalEpisodes': 12, 'stepsPerEpisode': 668, 'rewardPerEpisode': 512.2197681787922
'totalSteps': 5120, 'rewardStep': 0.8211969309383229, 'errorList': [], 'lossList': [0.0, -1.463728396296501, 0.0, 22.01442218542099, 0.0, 0.0, 0.0], 'rewardMean': 0.7530470772192165, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 947.2487061399653
'totalSteps': 6400, 'rewardStep': 0.47153963751712424, 'errorList': [], 'lossList': [0.0, -1.4362735188007354, 0.0, 35.61994574189186, 0.0, 0.0, 0.0], 'rewardMean': 0.696745589278798, 'totalEpisodes': 14, 'stepsPerEpisode': 558, 'rewardPerEpisode': 422.64785920476595
'totalSteps': 7680, 'rewardStep': 0.6362872397495497, 'errorList': [], 'lossList': [0.0, -1.418938074707985, 0.0, 123.43131193161011, 0.0, 0.0, 0.0], 'rewardMean': 0.68666919769059, 'totalEpisodes': 24, 'stepsPerEpisode': 56, 'rewardPerEpisode': 36.92156831341452
'totalSteps': 8960, 'rewardStep': 0.8379135727101064, 'errorList': [], 'lossList': [0.0, -1.421792802810669, 0.0, 161.90424461364745, 0.0, 0.0, 0.0], 'rewardMean': 0.7082755369790924, 'totalEpisodes': 41, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.07538538923826
'totalSteps': 10240, 'rewardStep': 0.4383132490205034, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6482839174327393, 'totalEpisodes': 79, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.904246181517301
'totalSteps': 11520, 'rewardStep': 0.595821749640383, 'errorList': [], 'lossList': [0.0, -1.421769071817398, 0.0, 179.13293380737304, 0.0, 0.0, 0.0], 'rewardMean': 0.6430377006535035, 'totalEpisodes': 115, 'stepsPerEpisode': 69, 'rewardPerEpisode': 52.30092932039639
'totalSteps': 12800, 'rewardStep': 0.930522658513212, 'errorList': [251.44823493862683, 282.48896814959477, 252.37179562655746, 265.9704730275624, 242.51717051549687, 238.7726251376583, 239.65036714239346, 282.83455466676725, 254.75659308934186, 112.83984368467927, 299.4074511090669, 251.4120406230411, 264.3179111772272, 301.70775127542566, 301.71218715526356, 277.3590455051935, 305.28746573690665, 233.02712582894895, 304.33534184645754, 236.4160127380449, 198.97800508411734, 154.81772361513572, 238.33785760092414, 288.5441420887287, 277.8489889969528, 245.9336990734799, 216.59382464109441, 265.39113105682287, 213.76640790582442, 250.95354863254244, 236.46713414535358, 185.83680257025344, 274.6717737406766, 214.41574875713908, 285.9506270752849, 252.2634705537751, 222.47342657991265, 81.18154205582688, 206.99093075299538, 235.73133383230265, 262.92909288159314, 279.6755640509009, 250.79996374030182, 257.2301718900987, 255.87980186678843, 188.71356097650673, 168.4838291634729, 56.22296328541933, 300.6277616327001, 244.41714933862824], 'lossList': [0.0, -1.4157338041067122, 0.0, 121.6015238571167, 0.0, 0.0, 0.0], 'rewardMean': 0.6883113889825451, 'totalEpisodes': 144, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.8663724527466907, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.49367533348478776, 'errorList': [], 'lossList': [0.0, -1.4216291242837906, 0.0, 96.79971139907838, 0.0, 0.0, 0.0], 'rewardMean': 0.6523888818409631, 'totalEpisodes': 163, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.4173044981688
'totalSteps': 15360, 'rewardStep': 0.6350043066937108, 'errorList': [], 'lossList': [0.0, -1.4315343415737152, 0.0, 87.80276126861573, 0.0, 0.0, 0.0], 'rewardMean': 0.6298587927288204, 'totalEpisodes': 178, 'stepsPerEpisode': 38, 'rewardPerEpisode': 22.469630296232424
'totalSteps': 16640, 'rewardStep': 0.890927147141377, 'errorList': [], 'lossList': [0.0, -1.4355577045679093, 0.0, 37.77918931007385, 0.0, 0.0, 0.0], 'rewardMean': 0.6368318143491257, 'totalEpisodes': 184, 'stepsPerEpisode': 72, 'rewardPerEpisode': 61.552494887403476
'totalSteps': 17920, 'rewardStep': 0.6985136946379218, 'errorList': [], 'lossList': [0.0, -1.4292703402042388, 0.0, 61.61611426353455, 0.0, 0.0, 0.0], 'rewardMean': 0.6595292200612055, 'totalEpisodes': 189, 'stepsPerEpisode': 309, 'rewardPerEpisode': 254.91187805121143
'totalSteps': 19200, 'rewardStep': 0.6592156459833142, 'errorList': [], 'lossList': [0.0, -1.4016608744859695, 0.0, 32.0279527592659, 0.0, 0.0, 0.0], 'rewardMean': 0.6618220606845819, 'totalEpisodes': 192, 'stepsPerEpisode': 518, 'rewardPerEpisode': 398.5868425132701
'totalSteps': 20480, 'rewardStep': 0.735222608620556, 'errorList': [], 'lossList': [0.0, -1.3794974571466445, 0.0, 12.491414719820023, 0.0, 0.0, 0.0], 'rewardMean': 0.6515529642756269, 'totalEpisodes': 195, 'stepsPerEpisode': 51, 'rewardPerEpisode': 42.90547366871097
'totalSteps': 21760, 'rewardStep': 0.46351276253584833, 'errorList': [], 'lossList': [0.0, -1.3730233740806579, 0.0, 10.460751559734344, 0.0, 0.0, 0.0], 'rewardMean': 0.6540729156271614, 'totalEpisodes': 198, 'stepsPerEpisode': 140, 'rewardPerEpisode': 91.02924260864563
'totalSteps': 23040, 'rewardStep': 0.8009055359681548, 'errorList': [], 'lossList': [0.0, -1.3810469621419907, 0.0, 6.638755573034286, 0.0, 0.0, 0.0], 'rewardMean': 0.6903321443219266, 'totalEpisodes': 200, 'stepsPerEpisode': 240, 'rewardPerEpisode': 197.62953156964753
'totalSteps': 24320, 'rewardStep': 0.6467367648428661, 'errorList': [], 'lossList': [0.0, -1.377155939936638, 0.0, 2.1544916889071466, 0.0, 0.0, 0.0], 'rewardMean': 0.6954236458421749, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 935.7841024004686
'totalSteps': 25600, 'rewardStep': 0.5222483712426237, 'errorList': [], 'lossList': [0.0, -1.3655138808488845, 0.0, 11.622655580043793, 0.0, 0.0, 0.0], 'rewardMean': 0.6545962171151161, 'totalEpisodes': 201, 'stepsPerEpisode': 1238, 'rewardPerEpisode': 936.9560428359126
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=82.11
