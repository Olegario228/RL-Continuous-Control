#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 93
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5843641036894014, 'errorList': [], 'lossList': [0.0, -1.4218970161676407, 0.0, 30.55996016263962, 0.0, 0.0, 0.0], 'rewardMean': 0.7212881061224705, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.309305522819685
'totalSteps': 3840, 'rewardStep': 0.970517016632514, 'errorList': [], 'lossList': [0.0, -1.4210473591089248, 0.0, 30.41417692065239, 0.0, 0.0, 0.0], 'rewardMean': 0.8043644096258183, 'totalEpisodes': 18, 'stepsPerEpisode': 488, 'rewardPerEpisode': 387.1818840822633
'totalSteps': 5120, 'rewardStep': 0.8087220984245629, 'errorList': [], 'lossList': [0.0, -1.4164694827795028, 0.0, 27.83260536134243, 0.0, 0.0, 0.0], 'rewardMean': 0.8054538318255045, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.0755473780087
'totalSteps': 6400, 'rewardStep': 0.6303542082312801, 'errorList': [], 'lossList': [0.0, -1.3998497247695922, 0.0, 17.18078737974167, 0.0, 0.0, 0.0], 'rewardMean': 0.7704339071066595, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1026.9238901176598
'totalSteps': 7680, 'rewardStep': 0.8192317972699207, 'errorList': [], 'lossList': [0.0, -1.3696356207132339, 0.0, 53.600734329223634, 0.0, 0.0, 0.0], 'rewardMean': 0.7785668888005364, 'totalEpisodes': 21, 'stepsPerEpisode': 107, 'rewardPerEpisode': 78.48693950933902
'totalSteps': 8960, 'rewardStep': 0.759314294544661, 'errorList': [], 'lossList': [0.0, -1.3697431707382202, 0.0, 228.6344602203369, 0.0, 0.0, 0.0], 'rewardMean': 0.7758165181925543, 'totalEpisodes': 41, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.81028975889635
'totalSteps': 10240, 'rewardStep': 0.5084311088218005, 'errorList': [], 'lossList': [0.0, -1.3668235331773757, 0.0, 251.80716484069825, 0.0, 0.0, 0.0], 'rewardMean': 0.7423933420212101, 'totalEpisodes': 78, 'stepsPerEpisode': 7, 'rewardPerEpisode': 3.6198151102317118
'totalSteps': 11520, 'rewardStep': 0.9624725613848988, 'errorList': [210.33381397667526, 226.86416038472098, 214.35598944457152, 225.67256959137478, 219.8494672142714, 193.3169791403308, 236.05934451456903, 228.8388015205391, 209.73164091748114, 204.9904179292162, 234.30323463169384, 200.02703102308075, 222.3915428311826, 217.77727299447164, 183.19374081420798, 207.78645002869132, 224.82687530812188, 216.45610975583668, 176.2117837551349, 170.17743718984212, 230.0202111852998, 227.9254677681508, 224.11511758878873, 225.5233924584831, 203.71382265273596, 237.19782626146701, 212.03664491710327, 213.04036506892587, 227.0831859416675, 218.64175950360013, 209.42226152068088, 149.07776614306724, 231.72666196279448, 227.51923619096917, 185.49157028065505, 223.74724385544474, 208.20778823801717, 216.77661847917355, 228.84667705935703, 185.70629912386372, 220.868929503604, 227.01014551506856, 202.62551924697348, 181.51519390097303, 216.2228960985981, 216.19583434274736, 223.7336117515876, 218.96521929024962, 192.2201926736685, 221.9235969491019], 'lossList': [0.0, -1.3621427196264266, 0.0, 97.67589656829834, 0.0, 0.0, 0.0], 'rewardMean': 0.7668465886171755, 'totalEpisodes': 113, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.83364239105553, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.7420789063445116, 'errorList': [], 'lossList': [0.0, -1.3634095108509063, 0.0, 58.96474340438843, 0.0, 0.0, 0.0], 'rewardMean': 0.7643698203899091, 'totalEpisodes': 140, 'stepsPerEpisode': 38, 'rewardPerEpisode': 25.77373743744466
'totalSteps': 14080, 'rewardStep': 0.8769803337622372, 'errorList': [], 'lossList': [0.0, -1.362132243514061, 0.0, 41.87364159584045, 0.0, 0.0, 0.0], 'rewardMean': 0.7662466429105788, 'totalEpisodes': 168, 'stepsPerEpisode': 152, 'rewardPerEpisode': 126.4718613097313
'totalSteps': 15360, 'rewardStep': 0.8787374780335583, 'errorList': [], 'lossList': [0.0, -1.3493008929491044, 0.0, 20.404839282035827, 0.0, 0.0, 0.0], 'rewardMean': 0.7956839803449945, 'totalEpisodes': 182, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.006399853178502
'totalSteps': 16640, 'rewardStep': 0.931888446564165, 'errorList': [72.13193167878954, 100.81131174190207, 21.253640258886364, 5.927563819289251, 52.28798592504016, 42.31419672939986, 168.73157024263847, 107.81442725584166, 112.96301388870702, 18.352311369831035, 103.49501828577495, 34.41818000204345, 155.18940258785702, 53.415209933629505, 41.17400891766303, 17.319622926005184, 146.87851281255777, 35.886612638854125, 62.640442662822046, 56.80947679708228, 137.5151116624202, 38.73295977160409, 1.0020647421706528, 55.2705350727122, 81.50198651733088, 35.51967120084771, 39.61716253742234, 56.05909394872405, 114.53900233340532, 1.0488394425324787, 135.26661187786505, 71.57519227430653, 153.99661263614894, 73.31629795723688, 29.19710931016621, 144.32128628627805, 100.90875067272094, 5.340400718482676, 27.112511685791564, 148.4532732249984, 56.683317166313486, 24.276013433584783, 118.54198209972913, 138.39658158577708, 70.51704066259843, 61.102205726303204, 35.29618692486336, 25.4375011455124, 1.684250460760899, 80.23966490905865], 'lossList': [0.0, -1.3338070070743562, 0.0, 15.753140692710877, 0.0, 0.0, 0.0], 'rewardMean': 0.7918211233381597, 'totalEpisodes': 189, 'stepsPerEpisode': 62, 'rewardPerEpisode': 49.67181498297534, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.878749817326434, 'errorList': [], 'lossList': [0.0, -1.3065755331516267, 0.0, 9.198130792379379, 0.0, 0.0, 0.0], 'rewardMean': 0.7988238952283467, 'totalEpisodes': 194, 'stepsPerEpisode': 59, 'rewardPerEpisode': 52.488533827653164
'totalSteps': 19200, 'rewardStep': 0.8942776145747529, 'errorList': [], 'lossList': [0.0, -1.2780962973833083, 0.0, 10.49336924314499, 0.0, 0.0, 0.0], 'rewardMean': 0.825216235862694, 'totalEpisodes': 197, 'stepsPerEpisode': 235, 'rewardPerEpisode': 200.12949384638753
'totalSteps': 20480, 'rewardStep': 0.8183044458465523, 'errorList': [], 'lossList': [0.0, -1.2522411727905274, 0.0, 6.623648307621479, 0.0, 0.0, 0.0], 'rewardMean': 0.8251235007203572, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.304721624526
'totalSteps': 21760, 'rewardStep': 0.9281384120932233, 'errorList': [], 'lossList': [0.0, -1.2202256351709366, 0.0, 6.704067545682192, 0.0, 0.0, 0.0], 'rewardMean': 0.8420059124752134, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.5367602479464
'totalSteps': 23040, 'rewardStep': 0.8459930126068773, 'errorList': [], 'lossList': [0.0, -1.1991836047172546, 0.0, 5.613928981423378, 0.0, 0.0, 0.0], 'rewardMean': 0.8757621028537212, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.4006192443294
'totalSteps': 24320, 'rewardStep': 0.8832554931271746, 'errorList': [], 'lossList': [0.0, -1.1566821801662446, 0.0, 4.0886135543137785, 0.0, 0.0, 0.0], 'rewardMean': 0.8678403960279486, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1191.3132758675315
'totalSteps': 25600, 'rewardStep': 0.9562288525062804, 'errorList': [0.03072094605513777, 0.016217621072333915, 0.01736030458566637, 0.02673553536048345, 0.03765151222443212, 0.03280087268736157, 0.02013805777535241, 0.031797244441557355, 0.016198738572721836, 0.0483359374069197, 0.02069681996221476, 0.06615274567913375, 0.01391867466919578, 0.03232942493256248, 0.015750285182228787, 0.03213743633124023, 0.0207550288798651, 0.028092965526763138, 0.02099504039922373, 0.05213648992595876, 0.01854179886247674, 0.01790861137800206, 0.021797056210624624, 0.04999203019021242, 0.015382439935483537, 0.020392334107307758, 0.02988570452636945, 0.02004451720458566, 0.028175095463377525, 0.016961338554946366, 0.0345889326423857, 0.01865505144763864, 0.015683655455536495, 0.013950781216615991, 0.024087327733243014, 0.052021925260025303, 0.0145624926869843, 0.046770132161882486, 0.022310642379901807, 0.020555333058303874, 0.045799578671522075, 0.018264598841401036, 0.04529709472213482, 0.056501022835689016, 0.06735556260457842, 0.04566652544349493, 0.020087651745714363, 0.013601681518458607, 0.0205191433199637, 0.015503803239444327], 'lossList': [0.0, -1.1266738110780716, 0.0, 2.782379963900894, 0.0, 0.0, 0.0], 'rewardMean': 0.8892553906441256, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1208.7559433496035, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=124.48
