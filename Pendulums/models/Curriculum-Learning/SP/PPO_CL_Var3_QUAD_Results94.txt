#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 94
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8788028971600871, 'errorList': [], 'lossList': [0.0, -1.4243247783184052, 0.0, 33.50672086536884, 0.0, 0.0, 0.0], 'rewardMean': 0.801076910519694, 'totalEpisodes': 13, 'stepsPerEpisode': 313, 'rewardPerEpisode': 259.09562893575065
'totalSteps': 3840, 'rewardStep': 0.7336265300208523, 'errorList': [], 'lossList': [0.0, -1.4265084379911424, 0.0, 32.679054043293, 0.0, 0.0, 0.0], 'rewardMean': 0.7785934503534134, 'totalEpisodes': 16, 'stepsPerEpisode': 180, 'rewardPerEpisode': 129.00323707057674
'totalSteps': 5120, 'rewardStep': 0.6272331099652071, 'errorList': [], 'lossList': [0.0, -1.4319239199161529, 0.0, 15.89360435128212, 0.0, 0.0, 0.0], 'rewardMean': 0.7407533652563618, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 876.2374257643268
'totalSteps': 6400, 'rewardStep': 0.8166636399739774, 'errorList': [], 'lossList': [0.0, -1.4163720679283143, 0.0, 30.760318315029146, 0.0, 0.0, 0.0], 'rewardMean': 0.7559354201998849, 'totalEpisodes': 18, 'stepsPerEpisode': 866, 'rewardPerEpisode': 649.4764318179756
'totalSteps': 7680, 'rewardStep': 0.8469012571930618, 'errorList': [], 'lossList': [0.0, -1.401635285615921, 0.0, 66.57443884849549, 0.0, 0.0, 0.0], 'rewardMean': 0.7710963930320811, 'totalEpisodes': 23, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.537601396843908
'totalSteps': 8960, 'rewardStep': 0.5422171561299891, 'errorList': [], 'lossList': [0.0, -1.3931433409452438, 0.0, 309.4230675506592, 0.0, 0.0, 0.0], 'rewardMean': 0.7383993591889251, 'totalEpisodes': 62, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.293440286404083
'totalSteps': 10240, 'rewardStep': 0.8690716449100554, 'errorList': [], 'lossList': [0.0, -1.3928573805093765, 0.0, 178.79015125274657, 0.0, 0.0, 0.0], 'rewardMean': 0.7547333949040663, 'totalEpisodes': 101, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.936355003281446
'totalSteps': 11520, 'rewardStep': 0.7213704162337294, 'errorList': [], 'lossList': [0.0, -1.3917628848552703, 0.0, 96.77389434814454, 0.0, 0.0, 0.0], 'rewardMean': 0.7510263972740289, 'totalEpisodes': 136, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.2797879253754667
'totalSteps': 12800, 'rewardStep': 0.70011991800775, 'errorList': [], 'lossList': [0.0, -1.3805671900510788, 0.0, 68.1162663269043, 0.0, 0.0, 0.0], 'rewardMean': 0.745935749347401, 'totalEpisodes': 161, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.773863765806378
'totalSteps': 14080, 'rewardStep': 0.1275623358978466, 'errorList': [], 'lossList': [0.0, -1.3671563529968263, 0.0, 49.119088592529295, 0.0, 0.0, 0.0], 'rewardMean': 0.6863568905492556, 'totalEpisodes': 173, 'stepsPerEpisode': 178, 'rewardPerEpisode': 119.09272167350525
'totalSteps': 15360, 'rewardStep': 0.8578897373979569, 'errorList': [], 'lossList': [0.0, -1.3565003454685212, 0.0, 57.519043955802914, 0.0, 0.0, 0.0], 'rewardMean': 0.6842655745730426, 'totalEpisodes': 183, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.799695983243511
'totalSteps': 16640, 'rewardStep': 0.39859832827054337, 'errorList': [], 'lossList': [0.0, -1.3570443922281266, 0.0, 44.986960005760196, 0.0, 0.0, 0.0], 'rewardMean': 0.6507627543980117, 'totalEpisodes': 192, 'stepsPerEpisode': 100, 'rewardPerEpisode': 72.12132208908562
'totalSteps': 17920, 'rewardStep': 0.5937435651728078, 'errorList': [], 'lossList': [0.0, -1.3514400672912599, 0.0, 35.51140457868576, 0.0, 0.0, 0.0], 'rewardMean': 0.6474137999187717, 'totalEpisodes': 199, 'stepsPerEpisode': 352, 'rewardPerEpisode': 273.3298480186446
'totalSteps': 19200, 'rewardStep': 0.7077618683460546, 'errorList': [], 'lossList': [0.0, -1.3345358860492706, 0.0, 8.847138483524322, 0.0, 0.0, 0.0], 'rewardMean': 0.6365236227559794, 'totalEpisodes': 203, 'stepsPerEpisode': 198, 'rewardPerEpisode': 157.79160758403393
'totalSteps': 20480, 'rewardStep': 0.891614596340303, 'errorList': [], 'lossList': [0.0, -1.319089002609253, 0.0, 5.352160125374794, 0.0, 0.0, 0.0], 'rewardMean': 0.6409949566707035, 'totalEpisodes': 207, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.844083634708628
'totalSteps': 21760, 'rewardStep': 0.747651369912889, 'errorList': [], 'lossList': [0.0, -1.3118308287858964, 0.0, 4.412339326143265, 0.0, 0.0, 0.0], 'rewardMean': 0.6615383780489936, 'totalEpisodes': 211, 'stepsPerEpisode': 120, 'rewardPerEpisode': 98.24684890646492
'totalSteps': 23040, 'rewardStep': 0.8072058526886291, 'errorList': [], 'lossList': [0.0, -1.292212136387825, 0.0, 2.99727438300848, 0.0, 0.0, 0.0], 'rewardMean': 0.655351798826851, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 937.3468574952198
'totalSteps': 24320, 'rewardStep': 0.347773217505555, 'errorList': [], 'lossList': [0.0, -1.2658514189720154, 0.0, 3.450347683429718, 0.0, 0.0, 0.0], 'rewardMean': 0.6179920789540334, 'totalEpisodes': 212, 'stepsPerEpisode': 673, 'rewardPerEpisode': 482.1329954519268
'totalSteps': 25600, 'rewardStep': 0.8613915902213798, 'errorList': [], 'lossList': [0.0, -1.2513849526643752, 0.0, 2.207586075365543, 0.0, 0.0, 0.0], 'rewardMean': 0.6341192461753965, 'totalEpisodes': 213, 'stepsPerEpisode': 1257, 'rewardPerEpisode': 1052.9651651137592
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=62.24
