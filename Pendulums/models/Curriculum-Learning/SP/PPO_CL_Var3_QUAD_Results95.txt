#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 95
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.906872887665849, 'errorList': [], 'lossList': [0.0, -1.436858864426613, 0.0, 31.6766219162941, 0.0, 0.0, 0.0], 'rewardMean': 0.9108467591233385, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 381.7383624076052
'totalSteps': 3840, 'rewardStep': 0.7124976265802232, 'errorList': [], 'lossList': [0.0, -1.4218441277742386, 0.0, 29.872747402191163, 0.0, 0.0, 0.0], 'rewardMean': 0.8447303816089667, 'totalEpisodes': 12, 'stepsPerEpisode': 267, 'rewardPerEpisode': 207.2786705000996
'totalSteps': 5120, 'rewardStep': 0.7234412875291126, 'errorList': [], 'lossList': [0.0, -1.420677655339241, 0.0, 23.745978872776032, 0.0, 0.0, 0.0], 'rewardMean': 0.8144081080890032, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 969.8875347486214
'totalSteps': 6400, 'rewardStep': 0.902187948286614, 'errorList': [], 'lossList': [0.0, -1.4151401889324189, 0.0, 20.5246120133996, 0.0, 0.0, 0.0], 'rewardMean': 0.8319640761285253, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1039.161953859216
'totalSteps': 7680, 'rewardStep': 0.8679012942605757, 'errorList': [], 'lossList': [0.0, -1.4022751557826996, 0.0, 75.73765789031982, 0.0, 0.0, 0.0], 'rewardMean': 0.837953612483867, 'totalEpisodes': 17, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.873283549104308
'totalSteps': 8960, 'rewardStep': 0.45495480958753265, 'errorList': [], 'lossList': [0.0, -1.3830880582332612, 0.0, 201.47161987304688, 0.0, 0.0, 0.0], 'rewardMean': 0.7832394977843907, 'totalEpisodes': 36, 'stepsPerEpisode': 75, 'rewardPerEpisode': 55.4809947864094
'totalSteps': 10240, 'rewardStep': 0.4915820710052663, 'errorList': [], 'lossList': [0.0, -1.3778042137622832, 0.0, 254.7901055908203, 0.0, 0.0, 0.0], 'rewardMean': 0.7467823194370002, 'totalEpisodes': 80, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.186058076455167
'totalSteps': 11520, 'rewardStep': 0.5144123152619109, 'errorList': [], 'lossList': [0.0, -1.3743577879667281, 0.0, 127.4450180053711, 0.0, 0.0, 0.0], 'rewardMean': 0.7209634300842125, 'totalEpisodes': 120, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.632858216505287
'totalSteps': 12800, 'rewardStep': 0.8052418415350518, 'errorList': [], 'lossList': [0.0, -1.3629578310251236, 0.0, 74.43108070373535, 0.0, 0.0, 0.0], 'rewardMean': 0.7293912712292965, 'totalEpisodes': 149, 'stepsPerEpisode': 62, 'rewardPerEpisode': 52.494064757570634
'totalSteps': 14080, 'rewardStep': 0.9084192513199941, 'errorList': [], 'lossList': [0.0, -1.356193922162056, 0.0, 56.45492903709412, 0.0, 0.0, 0.0], 'rewardMean': 0.7287511333032131, 'totalEpisodes': 171, 'stepsPerEpisode': 27, 'rewardPerEpisode': 21.227618855950087
'totalSteps': 15360, 'rewardStep': 0.7681617440429443, 'errorList': [], 'lossList': [0.0, -1.3613141059875489, 0.0, 31.061814546585083, 0.0, 0.0, 0.0], 'rewardMean': 0.7148800189409227, 'totalEpisodes': 181, 'stepsPerEpisode': 35, 'rewardPerEpisode': 32.361085451175676
'totalSteps': 16640, 'rewardStep': 0.19425761659773877, 'errorList': [], 'lossList': [0.0, -1.3544855892658234, 0.0, 25.4436012840271, 0.0, 0.0, 0.0], 'rewardMean': 0.6630560179426741, 'totalEpisodes': 189, 'stepsPerEpisode': 104, 'rewardPerEpisode': 64.31913279023709
'totalSteps': 17920, 'rewardStep': 0.13051560697200915, 'errorList': [], 'lossList': [0.0, -1.346006961464882, 0.0, 22.858958797454832, 0.0, 0.0, 0.0], 'rewardMean': 0.6037634498869637, 'totalEpisodes': 195, 'stepsPerEpisode': 225, 'rewardPerEpisode': 149.71475263721612
'totalSteps': 19200, 'rewardStep': 0.7212997533421419, 'errorList': [], 'lossList': [0.0, -1.3497373342514039, 0.0, 36.290216698646546, 0.0, 0.0, 0.0], 'rewardMean': 0.5856746303925167, 'totalEpisodes': 205, 'stepsPerEpisode': 75, 'rewardPerEpisode': 61.65920306138947
'totalSteps': 20480, 'rewardStep': 0.9045151113022079, 'errorList': [], 'lossList': [0.0, -1.3542079281806947, 0.0, 5.752455224990845, 0.0, 0.0, 0.0], 'rewardMean': 0.5893360120966797, 'totalEpisodes': 210, 'stepsPerEpisode': 166, 'rewardPerEpisode': 121.75986010920793
'totalSteps': 21760, 'rewardStep': 0.963900603097752, 'errorList': [0.1234086325688037, 0.10935293863963898, 0.18813571631718579, 0.20512561976002347, 0.15871997150074563, 0.171997800621829, 0.08351221054309486, 0.1213715117129775, 0.09275577005194059, 0.2381719039120494, 0.14266855275524862, 0.08278476810273955, 0.1661292141529582, 0.23010903800623536, 0.13294358305408902, 0.09434739088813837, 0.11091281407934078, 0.1192524358910618, 0.13749411209679957, 0.0853798663025947, 0.09537574044735415, 0.1639172018447213, 0.10799374537309957, 0.08871214518031666, 0.16128977020934082, 0.09957239269626832, 0.1141975454624413, 0.09789508100338463, 0.12912559547920852, 0.17820581391418666, 0.10880214542208078, 0.18573799532345045, 0.10923439784113098, 0.08823688181742646, 0.2840253357549448, 0.3233041047744273, 0.11025601937393614, 0.11385830658146832, 0.24827326820464904, 0.13202221430836011, 0.13401773970423453, 0.09852524798911973, 0.09312948658759988, 0.187098523500325, 0.10079913831522888, 0.21658181761969722, 0.2136596826124925, 0.13960687463153146, 0.17700772329103343, 0.1298850086513595], 'lossList': [0.0, -1.34288991689682, 0.0, 26.64420520067215, 0.0, 0.0, 0.0], 'rewardMean': 0.6402305914477017, 'totalEpisodes': 214, 'stepsPerEpisode': 62, 'rewardPerEpisode': 55.7767308356608, 'successfulTests': 42
'totalSteps': 23040, 'rewardStep': 0.6863166228210283, 'errorList': [], 'lossList': [0.0, -1.3125966149568558, 0.0, 5.9370852579176425, 0.0, 0.0, 0.0], 'rewardMean': 0.6597040466292778, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1079.679814837438
'totalSteps': 24320, 'rewardStep': 0.695172250601284, 'errorList': [], 'lossList': [0.0, -1.2765650469064713, 0.0, 4.745927259474993, 0.0, 0.0, 0.0], 'rewardMean': 0.6777800401632152, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.375288916181
'totalSteps': 25600, 'rewardStep': 0.7866012478007465, 'errorList': [], 'lossList': [0.0, -1.258056783080101, 0.0, 2.3909573209285737, 0.0, 0.0, 0.0], 'rewardMean': 0.6759159807897847, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1031.4369076902599
#maxSuccessfulTests=42, maxSuccessfulTestsAtStep=21760, timeSpent=84.62
