#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 96
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.820952639251212, 'errorList': [], 'lossList': [0.0, -1.450592311024666, 0.0, 38.65628068447113, 0.0, 0.0, 0.0], 'rewardMean': 0.7070652294156856, 'totalEpisodes': 12, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.463763785371725
'totalSteps': 3840, 'rewardStep': 0.9114432569763282, 'errorList': [], 'lossList': [0.0, -1.4681212198734284, 0.0, 31.069992884397507, 0.0, 0.0, 0.0], 'rewardMean': 0.7751912386025666, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.5772151372021
'totalSteps': 5120, 'rewardStep': 0.5640357191610224, 'errorList': [], 'lossList': [0.0, -1.4463179910182953, 0.0, 28.169719552993776, 0.0, 0.0, 0.0], 'rewardMean': 0.7224023587421805, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1022.6058754235723
'totalSteps': 6400, 'rewardStep': 0.9269207658294205, 'errorList': [], 'lossList': [0.0, -1.4466175723075867, 0.0, 20.44680684864521, 0.0, 0.0, 0.0], 'rewardMean': 0.7633060401596286, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.9418990358886
'totalSteps': 7680, 'rewardStep': 0.8820871596071174, 'errorList': [], 'lossList': [0.0, -1.4236830073595046, 0.0, 15.190040002390742, 0.0, 0.0, 0.0], 'rewardMean': 0.7831028934008767, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.040896557364
'totalSteps': 8960, 'rewardStep': 0.6982380356368797, 'errorList': [], 'lossList': [0.0, -1.3995885312557221, 0.0, 189.81341472625732, 0.0, 0.0, 0.0], 'rewardMean': 0.7709793422917343, 'totalEpisodes': 23, 'stepsPerEpisode': 108, 'rewardPerEpisode': 89.08697298404516
'totalSteps': 10240, 'rewardStep': 0.9132369898072963, 'errorList': [], 'lossList': [0.0, -1.39354126393795, 0.0, 312.1029815673828, 0.0, 0.0, 0.0], 'rewardMean': 0.7887615482311794, 'totalEpisodes': 69, 'stepsPerEpisode': 59, 'rewardPerEpisode': 53.967823331260874
'totalSteps': 11520, 'rewardStep': 0.920219911852183, 'errorList': [], 'lossList': [0.0, -1.392156878709793, 0.0, 117.71263261795043, 0.0, 0.0, 0.0], 'rewardMean': 0.8033680330779576, 'totalEpisodes': 115, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.920219911852183
'totalSteps': 12800, 'rewardStep': 0.5736216701009856, 'errorList': [], 'lossList': [0.0, -1.390951882004738, 0.0, 61.00684034347534, 0.0, 0.0, 0.0], 'rewardMean': 0.7803933967802603, 'totalEpisodes': 150, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.207719009383666
'totalSteps': 14080, 'rewardStep': 0.47638594419201163, 'errorList': [], 'lossList': [0.0, -1.3819457274675369, 0.0, 34.24009207725525, 0.0, 0.0, 0.0], 'rewardMean': 0.7687142092414456, 'totalEpisodes': 174, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.636384433979425
'totalSteps': 15360, 'rewardStep': 0.4481138077321837, 'errorList': [], 'lossList': [0.0, -1.362638658285141, 0.0, 15.17102117538452, 0.0, 0.0, 0.0], 'rewardMean': 0.7314303260895428, 'totalEpisodes': 189, 'stepsPerEpisode': 33, 'rewardPerEpisode': 20.47983093146273
'totalSteps': 16640, 'rewardStep': 0.9651466683702922, 'errorList': [189.1857180535921, 66.86757934273709, 217.1442893882828, 150.77861131334544, 88.11131154505492, 104.15140089561692, 245.71192022790424, 199.25116287368218, 133.85235342470463, 211.76276766096214, 171.93850542365496, 213.63201243334856, 88.54990651323564, 234.22027369302123, 189.61325712432395, 227.50964398107672, 157.49723916261368, 180.95044515264286, 30.2094713902941, 170.82608741812786, 230.38691771586616, 217.2284785692467, 176.69666405819888, 232.59385589303432, 169.21970739407325, 124.89935460285763, 198.94361966942756, 237.33457925722232, 199.55011776213507, 215.69263963335226, 253.60544202159963, 177.39450817758384, 206.3939543146438, 114.3662147672314, 144.78261298032325, 222.61374996876913, 175.70586295599972, 209.5324747079692, 240.65595868042033, 233.49009105709138, 243.81805898650626, 214.68209808241647, 164.92113017701786, 173.0894630587916, 234.94902955798975, 166.3644600555663, 156.26731116355734, 221.64972859776833, 49.53205990276178, 227.90677052939694], 'lossList': [0.0, -1.3414224594831468, 0.0, 28.397879238128663, 0.0, 0.0, 0.0], 'rewardMean': 0.7368006672289392, 'totalEpisodes': 203, 'stepsPerEpisode': 51, 'rewardPerEpisode': 47.37348933246998, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7138195393603487, 'errorList': [], 'lossList': [0.0, -1.3338332098722459, 0.0, 22.114435033798216, 0.0, 0.0, 0.0], 'rewardMean': 0.7517790492488718, 'totalEpisodes': 216, 'stepsPerEpisode': 28, 'rewardPerEpisode': 23.99022827622861
'totalSteps': 19200, 'rewardStep': 0.8907296488543068, 'errorList': [], 'lossList': [0.0, -1.3297457247972488, 0.0, 9.183041877746582, 0.0, 0.0, 0.0], 'rewardMean': 0.7481599375513606, 'totalEpisodes': 224, 'stepsPerEpisode': 78, 'rewardPerEpisode': 69.6697107829894
'totalSteps': 20480, 'rewardStep': 0.7705686627475056, 'errorList': [], 'lossList': [0.0, -1.327189905643463, 0.0, 6.487675679922104, 0.0, 0.0, 0.0], 'rewardMean': 0.7370080878653993, 'totalEpisodes': 230, 'stepsPerEpisode': 193, 'rewardPerEpisode': 161.4579160551416
'totalSteps': 21760, 'rewardStep': 0.8275234911717556, 'errorList': [], 'lossList': [0.0, -1.3181242340803145, 0.0, 7.515533263683319, 0.0, 0.0, 0.0], 'rewardMean': 0.7499366334188869, 'totalEpisodes': 236, 'stepsPerEpisode': 29, 'rewardPerEpisode': 22.833867463331188
'totalSteps': 23040, 'rewardStep': 0.7728833704150199, 'errorList': [], 'lossList': [0.0, -1.3108842289447784, 0.0, 3.9970218110084534, 0.0, 0.0, 0.0], 'rewardMean': 0.7359012714796592, 'totalEpisodes': 241, 'stepsPerEpisode': 203, 'rewardPerEpisode': 163.49228030616896
'totalSteps': 24320, 'rewardStep': 0.7037728156217328, 'errorList': [], 'lossList': [0.0, -1.3244795101881026, 0.0, 4.833812720179558, 0.0, 0.0, 0.0], 'rewardMean': 0.7142565618566141, 'totalEpisodes': 244, 'stepsPerEpisode': 353, 'rewardPerEpisode': 294.9047133105019
'totalSteps': 25600, 'rewardStep': 0.7140555071152137, 'errorList': [], 'lossList': [0.0, -1.3137957262992859, 0.0, 4.837026392221451, 0.0, 0.0, 0.0], 'rewardMean': 0.7282999455580371, 'totalEpisodes': 247, 'stepsPerEpisode': 304, 'rewardPerEpisode': 229.76562372947538
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=82.44
