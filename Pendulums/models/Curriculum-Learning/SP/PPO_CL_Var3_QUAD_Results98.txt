#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 98
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.54097636405191, 'errorList': [], 'lossList': [0.0, -1.4222250699996948, 0.0, 32.23712924718857, 0.0, 0.0, 0.0], 'rewardMean': 0.7199533899393251, 'totalEpisodes': 16, 'stepsPerEpisode': 45, 'rewardPerEpisode': 32.11596936575353
'totalSteps': 3840, 'rewardStep': 0.9507175032414743, 'errorList': [], 'lossList': [0.0, -1.414389580488205, 0.0, 33.94896075963974, 0.0, 0.0, 0.0], 'rewardMean': 0.7968747610400415, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 398.78703961116
'totalSteps': 5120, 'rewardStep': 0.8142287877618274, 'errorList': [], 'lossList': [0.0, -1.3999424004554748, 0.0, 29.285108745098114, 0.0, 0.0, 0.0], 'rewardMean': 0.801213267720488, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.6723152979675
'totalSteps': 6400, 'rewardStep': 0.6832846090121437, 'errorList': [], 'lossList': [0.0, -1.387492728829384, 0.0, 18.39789449930191, 0.0, 0.0, 0.0], 'rewardMean': 0.7776275359788192, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.991704926289
'totalSteps': 7680, 'rewardStep': 0.884424318240477, 'errorList': [], 'lossList': [0.0, -1.3629781419038773, 0.0, 27.008746309280397, 0.0, 0.0, 0.0], 'rewardMean': 0.7954269996890955, 'totalEpisodes': 19, 'stepsPerEpisode': 108, 'rewardPerEpisode': 86.37543917296657
'totalSteps': 8960, 'rewardStep': 0.8122827997086036, 'errorList': [], 'lossList': [0.0, -1.3482784676551818, 0.0, 207.9138874053955, 0.0, 0.0, 0.0], 'rewardMean': 0.7978349711204539, 'totalEpisodes': 36, 'stepsPerEpisode': 53, 'rewardPerEpisode': 44.08229774555276
'totalSteps': 10240, 'rewardStep': 0.8224643414831104, 'errorList': [], 'lossList': [0.0, -1.3434381246566773, 0.0, 313.3592003631592, 0.0, 0.0, 0.0], 'rewardMean': 0.8009136424157858, 'totalEpisodes': 75, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.085660022884948
'totalSteps': 11520, 'rewardStep': 0.839268031554725, 'errorList': [], 'lossList': [0.0, -1.3462550735473633, 0.0, 123.49320316314697, 0.0, 0.0, 0.0], 'rewardMean': 0.8051752412090012, 'totalEpisodes': 110, 'stepsPerEpisode': 52, 'rewardPerEpisode': 44.610279509751706
'totalSteps': 12800, 'rewardStep': 0.7372903529871898, 'errorList': [], 'lossList': [0.0, -1.3554707729816438, 0.0, 70.4863418006897, 0.0, 0.0, 0.0], 'rewardMean': 0.79838675238682, 'totalEpisodes': 138, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.037645707666578
'totalSteps': 14080, 'rewardStep': 0.6342047915696001, 'errorList': [], 'lossList': [0.0, -1.3579259657859801, 0.0, 64.13615246772765, 0.0, 0.0, 0.0], 'rewardMean': 0.7719141899611062, 'totalEpisodes': 156, 'stepsPerEpisode': 116, 'rewardPerEpisode': 78.24978669483531
'totalSteps': 15360, 'rewardStep': 0.4767321720085854, 'errorList': [], 'lossList': [0.0, -1.351465094089508, 0.0, 26.941131386756897, 0.0, 0.0, 0.0], 'rewardMean': 0.7654897707567736, 'totalEpisodes': 164, 'stepsPerEpisode': 92, 'rewardPerEpisode': 72.4831212018124
'totalSteps': 16640, 'rewardStep': 0.9284452153601034, 'errorList': [], 'lossList': [0.0, -1.347921869158745, 0.0, 24.0883047580719, 0.0, 0.0, 0.0], 'rewardMean': 0.7632625419686365, 'totalEpisodes': 170, 'stepsPerEpisode': 189, 'rewardPerEpisode': 153.75120343289475
'totalSteps': 17920, 'rewardStep': 0.8056612743508453, 'errorList': [], 'lossList': [0.0, -1.3529945182800294, 0.0, 7.1171266031265255, 0.0, 0.0, 0.0], 'rewardMean': 0.7624057906275382, 'totalEpisodes': 176, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.20431973230515
'totalSteps': 19200, 'rewardStep': 0.5117819906289892, 'errorList': [], 'lossList': [0.0, -1.3456373405456543, 0.0, 8.894423520565033, 0.0, 0.0, 0.0], 'rewardMean': 0.745255528789223, 'totalEpisodes': 181, 'stepsPerEpisode': 153, 'rewardPerEpisode': 94.73015460511965
'totalSteps': 20480, 'rewardStep': 0.7970134025982316, 'errorList': [], 'lossList': [0.0, -1.34323426425457, 0.0, 5.737758146524429, 0.0, 0.0, 0.0], 'rewardMean': 0.7365144372249984, 'totalEpisodes': 185, 'stepsPerEpisode': 267, 'rewardPerEpisode': 233.5927990419434
'totalSteps': 21760, 'rewardStep': 0.786691917501354, 'errorList': [], 'lossList': [0.0, -1.3330412209033966, 0.0, 4.074145343899727, 0.0, 0.0, 0.0], 'rewardMean': 0.7339553490042735, 'totalEpisodes': 187, 'stepsPerEpisode': 619, 'rewardPerEpisode': 525.0177830443815
'totalSteps': 23040, 'rewardStep': 0.7309178934569329, 'errorList': [], 'lossList': [0.0, -1.3133938413858415, 0.0, 3.1327227956056594, 0.0, 0.0, 0.0], 'rewardMean': 0.7248007042016557, 'totalEpisodes': 187, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.880034202192
'totalSteps': 24320, 'rewardStep': 0.7230142024619995, 'errorList': [], 'lossList': [0.0, -1.2823936915397645, 0.0, 2.2796601817011832, 0.0, 0.0, 0.0], 'rewardMean': 0.7131753212923831, 'totalEpisodes': 187, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.9924610065036
'totalSteps': 25600, 'rewardStep': 0.9501562843024658, 'errorList': [0.0805225836389119, 0.06723814134311859, 0.06457202823335718, 0.07223963859449473, 0.07657901512012892, 0.06547977179199826, 0.06512162921944076, 0.06619443459343027, 0.07533022118652913, 0.07045777572321807, 0.07714652297530701, 0.06593710818877396, 0.06867601056715289, 0.08198362743346947, 0.06536541505344533, 0.07146848433826017, 0.07849945993210025, 0.07615402253832933, 0.06486339492364271, 0.07793370982691852, 0.07116791711742938, 0.07753899103085571, 0.06717404667208675, 0.0658460320956463, 0.06805146561653665, 0.0645382676877654, 0.06457885262381152, 0.06512690197895538, 0.06712159293973219, 0.06520620925739584, 0.06518371339335106, 0.0658006968028742, 0.07725006005416948, 0.07607341357089845, 0.07795571896959448, 0.0676090046634283, 0.07549312512317655, 0.07540573816275117, 0.06510216170941308, 0.06532143499419758, 0.07258667005932694, 0.07201160195527086, 0.07275684852661016, 0.07589464437758923, 0.07205448188504827, 0.0732322332229859, 0.06504415061511108, 0.08282156347555969, 0.0830007391423215, 0.07253955208796735], 'lossList': [0.0, -1.25301151573658, 0.0, 1.6941203267872333, 0.0, 0.0, 0.0], 'rewardMean': 0.7344619144239106, 'totalEpisodes': 187, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1157.7338161939745, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=84.42
