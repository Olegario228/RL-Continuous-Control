#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 99
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_QUAD_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'quad', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.82554515950803, 'errorList': [], 'lossList': [0.0, -1.4216401493549347, 0.0, 28.053519744873046, 0.0, 0.0, 0.0], 'rewardMean': 0.7874180119036556, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 772.615112569192
'totalSteps': 3840, 'rewardStep': 0.7577959464956145, 'errorList': [], 'lossList': [0.0, -1.4364871203899383, 0.0, 33.76636894464493, 0.0, 0.0, 0.0], 'rewardMean': 0.7775439901009752, 'totalEpisodes': 14, 'stepsPerEpisode': 160, 'rewardPerEpisode': 123.21633562567631
'totalSteps': 5120, 'rewardStep': 0.6689703119567654, 'errorList': [], 'lossList': [0.0, -1.432207666039467, 0.0, 23.60924898684025, 0.0, 0.0, 0.0], 'rewardMean': 0.7504005705649228, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 977.5604494109731
'totalSteps': 6400, 'rewardStep': 0.8716541984348595, 'errorList': [], 'lossList': [0.0, -1.4185460251569748, 0.0, 17.15198373198509, 0.0, 0.0, 0.0], 'rewardMean': 0.7746512961389102, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.1358711224915
'totalSteps': 7680, 'rewardStep': 0.9132183724014566, 'errorList': [], 'lossList': [0.0, -1.4068711394071578, 0.0, 48.84736850976944, 0.0, 0.0, 0.0], 'rewardMean': 0.7977458088493345, 'totalEpisodes': 18, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.138721405215218
'totalSteps': 8960, 'rewardStep': 0.6530637830839743, 'errorList': [], 'lossList': [0.0, -1.3894227510690689, 0.0, 357.2580381774902, 0.0, 0.0, 0.0], 'rewardMean': 0.7770769480257116, 'totalEpisodes': 57, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.9787940079570197
'totalSteps': 10240, 'rewardStep': 0.8312789124841439, 'errorList': [], 'lossList': [0.0, -1.386326779127121, 0.0, 226.62805335998536, 0.0, 0.0, 0.0], 'rewardMean': 0.7838521935830156, 'totalEpisodes': 108, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.153127805330803
'totalSteps': 11520, 'rewardStep': 0.9701891932648348, 'errorList': [160.17453793016583, 176.5699496957659, 140.24993053082514, 172.0706802217753, 97.5260379324504, 173.42738052468152, 165.6233736969878, 161.97521011403114, 160.61354174140433, 166.0928204331221, 165.34141149730536, 152.58798444201258, 142.72117540001392, 161.37063110455762, 154.59890317157885, 165.03086991838, 170.5852145949521, 154.04161447389833, 172.4284993335602, 165.03668695649068, 177.0702002997571, 171.06575988686913, 129.8710428745331, 168.24580084124497, 163.71345363682246, 167.19838124180924, 152.38717421817805, 161.53976384150494, 155.1716947258606, 170.93941834421744, 177.2540849560471, 139.1254837970433, 179.45449508074444, 125.94249179023295, 172.6159176252794, 138.4593991017204, 159.8882797379948, 178.72439713823644, 157.03655875575842, 167.08750010521072, 144.80557372730908, 156.88236109982304, 163.1717616658758, 167.16995464042895, 166.84864342152676, 137.0165386039096, 161.7738188927205, 155.87657091786213, 174.90138840121293, 173.9760905161958], 'lossList': [0.0, -1.376092774271965, 0.0, 99.3538207244873, 0.0, 0.0, 0.0], 'rewardMean': 0.8045563046587733, 'totalEpisodes': 148, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.7658822704324346, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.517894892027668, 'errorList': [], 'lossList': [0.0, -1.3653609788417815, 0.0, 71.02591243743896, 0.0, 0.0, 0.0], 'rewardMean': 0.7758901633956627, 'totalEpisodes': 182, 'stepsPerEpisode': 44, 'rewardPerEpisode': 25.35744310564942
'totalSteps': 14080, 'rewardStep': 0.5543321628964425, 'errorList': [], 'lossList': [0.0, -1.3584697073698044, 0.0, 49.37565210342407, 0.0, 0.0, 0.0], 'rewardMean': 0.7563942932553789, 'totalEpisodes': 193, 'stepsPerEpisode': 166, 'rewardPerEpisode': 132.31749361527517
'totalSteps': 15360, 'rewardStep': 0.24762448340807902, 'errorList': [], 'lossList': [0.0, -1.3532046192884446, 0.0, 40.01478520393371, 0.0, 0.0, 0.0], 'rewardMean': 0.6986022256453839, 'totalEpisodes': 202, 'stepsPerEpisode': 158, 'rewardPerEpisode': 105.30191832937525
'totalSteps': 16640, 'rewardStep': 0.7058693004764309, 'errorList': [], 'lossList': [0.0, -1.3528916132450104, 0.0, 40.634859828948976, 0.0, 0.0, 0.0], 'rewardMean': 0.6934095610434655, 'totalEpisodes': 213, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.563601614799996
'totalSteps': 17920, 'rewardStep': 0.2837419353637658, 'errorList': [], 'lossList': [0.0, -1.3525495153665543, 0.0, 17.709518461227418, 0.0, 0.0, 0.0], 'rewardMean': 0.6548867233841655, 'totalEpisodes': 217, 'stepsPerEpisode': 578, 'rewardPerEpisode': 458.88326543186196
'totalSteps': 19200, 'rewardStep': 0.8483869118339863, 'errorList': [], 'lossList': [0.0, -1.3386841189861298, 0.0, 23.021651844978333, 0.0, 0.0, 0.0], 'rewardMean': 0.6525599947240782, 'totalEpisodes': 222, 'stepsPerEpisode': 86, 'rewardPerEpisode': 74.66579696270153
'totalSteps': 20480, 'rewardStep': 0.9612778041237595, 'errorList': [4.848450411940065, 0.8986192639783439, 0.9757372602635254, 4.878244386677776, 2.805595266097202, 4.3763424300678455, 0.15994762067769272, 1.106614883518838, 0.2324270523411126, 0.17542095227702437, 0.8077443530800155, 6.343585791044055, 1.5552237542710976, 3.3086020574361417, 1.0438862665474655, 1.2622282899992894, 2.0359041577178143, 1.687453408505256, 2.900913615178997, 1.9395643027326503, 0.43231730892337755, 0.27833854499096317, 0.30758065135166945, 4.7319188782876225, 2.173169554845925, 2.4510333325361486, 2.460362655841893, 2.029416073679455, 0.8855060277330434, 4.482254590933531, 7.5424196951893645, 1.7203844105404669, 0.8378579242874672, 3.364755402767486, 0.17048070431381074, 1.0256959796170824, 3.6819951911689355, 2.4830955880302485, 1.7043247127704053, 1.7153457976154682, 1.9997633310117844, 3.23038423364922, 0.15452857594503747, 1.9053824351254753, 1.057145428317541, 0.5220905237778999, 1.7444617419046973, 4.977880291256725, 0.5374928484354294, 4.091481792488483], 'lossList': [0.0, -1.3356736797094344, 0.0, 12.49591068983078, 0.0, 0.0, 0.0], 'rewardMean': 0.6573659378963086, 'totalEpisodes': 224, 'stepsPerEpisode': 67, 'rewardPerEpisode': 50.16935118805002, 'successfulTests': 4
'totalSteps': 21760, 'rewardStep': 0.6531020390375856, 'errorList': [], 'lossList': [0.0, -1.3552453076839448, 0.0, 10.559063721895217, 0.0, 0.0, 0.0], 'rewardMean': 0.6573697634916696, 'totalEpisodes': 227, 'stepsPerEpisode': 74, 'rewardPerEpisode': 55.9504567852478
'totalSteps': 23040, 'rewardStep': 0.665898694413259, 'errorList': [], 'lossList': [0.0, -1.3670799750089646, 0.0, 6.614010845422745, 0.0, 0.0, 0.0], 'rewardMean': 0.640831741684581, 'totalEpisodes': 230, 'stepsPerEpisode': 240, 'rewardPerEpisode': 185.89699815866828
'totalSteps': 24320, 'rewardStep': 0.7748685369029037, 'errorList': [], 'lossList': [0.0, -1.355175251364708, 0.0, 2.9667126762866975, 0.0, 0.0, 0.0], 'rewardMean': 0.6212996760483881, 'totalEpisodes': 231, 'stepsPerEpisode': 940, 'rewardPerEpisode': 743.135802126804
'totalSteps': 25600, 'rewardStep': 0.8859799185478229, 'errorList': [], 'lossList': [0.0, -1.327956349849701, 0.0, 2.2237023274600505, 0.0, 0.0, 0.0], 'rewardMean': 0.6581081787004035, 'totalEpisodes': 231, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.7681970042236
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=20480, timeSpent=105.19
