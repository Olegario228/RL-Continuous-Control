#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.571403797249991, 'errorList': [], 'lossList': [0.0, -1.4084483307600022, 0.0, 33.72710604667664, 0.0, 0.0, 0.0], 'rewardMean': 0.5078132691127544, 'totalEpisodes': 74, 'stepsPerEpisode': 19, 'rewardPerEpisode': 12.878506200460187
'totalSteps': 3840, 'rewardStep': 0.9256189655801378, 'errorList': [], 'lossList': [0.0, -1.4025061511993409, 0.0, 42.409827365875245, 0.0, 0.0, 0.0], 'rewardMean': 0.6470818346018822, 'totalEpisodes': 118, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.7609358043632994
'totalSteps': 5120, 'rewardStep': 0.8789196657664138, 'errorList': [], 'lossList': [0.0, -1.3934161978960038, 0.0, 61.18658000946045, 0.0, 0.0, 0.0], 'rewardMean': 0.705041292393015, 'totalEpisodes': 163, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.709263465741564
'totalSteps': 6400, 'rewardStep': 0.5963976199925435, 'errorList': [], 'lossList': [0.0, -1.3961154276132584, 0.0, 46.1023870754242, 0.0, 0.0, 0.0], 'rewardMean': 0.6833125579129208, 'totalEpisodes': 180, 'stepsPerEpisode': 26, 'rewardPerEpisode': 16.917502125730763
'totalSteps': 7680, 'rewardStep': 0.7373038693810375, 'errorList': [], 'lossList': [0.0, -1.3874340134859084, 0.0, 45.54347344875336, 0.0, 0.0, 0.0], 'rewardMean': 0.6923111098242735, 'totalEpisodes': 192, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.477961632037827
'totalSteps': 8960, 'rewardStep': 0.7580012886313302, 'errorList': [], 'lossList': [0.0, -1.3661899220943452, 0.0, 58.813814888000486, 0.0, 0.0, 0.0], 'rewardMean': 0.7016954210824246, 'totalEpisodes': 200, 'stepsPerEpisode': 108, 'rewardPerEpisode': 88.5296509534413
'totalSteps': 10240, 'rewardStep': 0.6321432244046089, 'errorList': [], 'lossList': [0.0, -1.3676568281650543, 0.0, 49.21272326469421, 0.0, 0.0, 0.0], 'rewardMean': 0.6930013964976975, 'totalEpisodes': 206, 'stepsPerEpisode': 108, 'rewardPerEpisode': 85.73625152490877
'totalSteps': 11520, 'rewardStep': 0.7689791457996014, 'errorList': [], 'lossList': [0.0, -1.3600001090765, 0.0, 57.79134670257568, 0.0, 0.0, 0.0], 'rewardMean': 0.7014433686423536, 'totalEpisodes': 214, 'stepsPerEpisode': 67, 'rewardPerEpisode': 59.9905671185759
'totalSteps': 12800, 'rewardStep': 0.39950923627395374, 'errorList': [], 'lossList': [0.0, -1.3586121225357055, 0.0, 20.828841230869294, 0.0, 0.0, 0.0], 'rewardMean': 0.6712499554055136, 'totalEpisodes': 217, 'stepsPerEpisode': 471, 'rewardPerEpisode': 300.65005151066316
'totalSteps': 14080, 'rewardStep': 0.5255115276159676, 'errorList': [], 'lossList': [0.0, -1.3567700654268264, 0.0, 29.195798966884613, 0.0, 0.0, 0.0], 'rewardMean': 0.6793788340695586, 'totalEpisodes': 221, 'stepsPerEpisode': 214, 'rewardPerEpisode': 176.3248157363008
'totalSteps': 15360, 'rewardStep': 0.71591796265299, 'errorList': [], 'lossList': [0.0, -1.3423319578170776, 0.0, 20.83034956097603, 0.0, 0.0, 0.0], 'rewardMean': 0.6938302506098585, 'totalEpisodes': 226, 'stepsPerEpisode': 333, 'rewardPerEpisode': 262.790085043903
'totalSteps': 16640, 'rewardStep': 0.9187389057430779, 'errorList': [], 'lossList': [0.0, -1.3468113946914673, 0.0, 13.683962488174439, 0.0, 0.0, 0.0], 'rewardMean': 0.6931422446261526, 'totalEpisodes': 228, 'stepsPerEpisode': 731, 'rewardPerEpisode': 587.8232837885674
'totalSteps': 17920, 'rewardStep': 0.9266428943606073, 'errorList': [], 'lossList': [0.0, -1.3620509976148605, 0.0, 4.4063341069221496, 0.0, 0.0, 0.0], 'rewardMean': 0.6979145674855718, 'totalEpisodes': 230, 'stepsPerEpisode': 447, 'rewardPerEpisode': 369.92345639119446
'totalSteps': 19200, 'rewardStep': 0.7752047951184318, 'errorList': [], 'lossList': [0.0, -1.3769443559646606, 0.0, 52.54782706856727, 0.0, 0.0, 0.0], 'rewardMean': 0.7157952849981607, 'totalEpisodes': 232, 'stepsPerEpisode': 819, 'rewardPerEpisode': 597.8519122824393
'totalSteps': 20480, 'rewardStep': 0.7500959786095022, 'errorList': [], 'lossList': [0.0, -1.3494700002670288, 0.0, 2.4662599875032902, 0.0, 0.0, 0.0], 'rewardMean': 0.7170744959210071, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 989.5008601360167
'totalSteps': 21760, 'rewardStep': 0.866495305873104, 'errorList': [], 'lossList': [0.0, -1.3053789162635803, 0.0, 1.8754236802458764, 0.0, 0.0, 0.0], 'rewardMean': 0.7279238976451845, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.4804550206388
'totalSteps': 23040, 'rewardStep': 0.755074938914282, 'errorList': [], 'lossList': [0.0, -1.2841094028949738, 0.0, 1.4096864028275014, 0.0, 0.0, 0.0], 'rewardMean': 0.7402170690961518, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.8884385550875
'totalSteps': 24320, 'rewardStep': 0.7591831502746094, 'errorList': [], 'lossList': [0.0, -1.2498124170303344, 0.0, 1.2913551686704159, 0.0, 0.0, 0.0], 'rewardMean': 0.7392374695436527, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.3172368014154
'totalSteps': 25600, 'rewardStep': 0.9469467706947355, 'errorList': [0.07889845705412109, 0.03127422185059623, 0.055011437760457964, 0.014101816128776132, 0.03691562173218895, 0.06571713624753886, 0.030698852778094792, 0.04553635481372935, 0.01958309546302979, 0.02398873479140546, 0.05769395815124722, 0.03952774340316429, 0.058807020615121885, 0.017564568174347452, 0.014308978953306338, 0.01916715657818002, 0.07284614705407765, 0.0187065576509262, 0.044322145480871086, 0.037034833012021165, 0.030305575275296516, 0.022782202149032902, 0.026245185996544473, 0.014436881427014258, 0.09129963481136827, 0.04430371294014372, 0.06352119749065241, 0.054913998607515724, 0.044684564535102375, 0.027087213702028764, 0.03197900717947393, 0.03492776256104764, 0.02797241499686649, 0.04455434165163401, 0.04019022184322032, 0.014381211450719217, 0.058518471766436625, 0.03003910365785146, 0.03417084372302026, 0.024600289384693594, 0.018550436440610996, 0.010825039235340249, 0.02840455096603649, 0.019252470538018047, 0.031180370800074964, 0.03230755321294047, 0.02697539768530551, 0.02259736604079613, 0.02832040419368421, 0.0388613297938306], 'lossList': [0.0, -1.2032381635904312, 0.0, 1.200724059715867, 0.0, 0.0, 0.0], 'rewardMean': 0.7939812229857306, 'totalEpisodes': 232, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.7961772836336, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=73.72
