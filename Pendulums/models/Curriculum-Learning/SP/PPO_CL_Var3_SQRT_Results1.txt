#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.769451430023153, 'errorList': [], 'lossList': [0.0, -1.415461305975914, 0.0, 33.761307401657106, 0.0, 0.0, 0.0], 'rewardMean': 0.7922507814322801, 'totalEpisodes': 77, 'stepsPerEpisode': 38, 'rewardPerEpisode': 34.31717122593862
'totalSteps': 3840, 'rewardStep': 0.5250604438123201, 'errorList': [], 'lossList': [0.0, -1.4035189497470855, 0.0, 48.795970878601075, 0.0, 0.0, 0.0], 'rewardMean': 0.7031873355589601, 'totalEpisodes': 116, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5250604438123201
'totalSteps': 5120, 'rewardStep': 0.7420482336123078, 'errorList': [], 'lossList': [0.0, -1.3776871699094773, 0.0, 55.62966449737549, 0.0, 0.0, 0.0], 'rewardMean': 0.712902560072297, 'totalEpisodes': 144, 'stepsPerEpisode': 26, 'rewardPerEpisode': 19.667769052742525
'totalSteps': 6400, 'rewardStep': 0.3326111407874305, 'errorList': [], 'lossList': [0.0, -1.35225272834301, 0.0, 49.613573474884035, 0.0, 0.0, 0.0], 'rewardMean': 0.6368442762153237, 'totalEpisodes': 162, 'stepsPerEpisode': 86, 'rewardPerEpisode': 62.04171360805207
'totalSteps': 7680, 'rewardStep': 0.7378612342634704, 'errorList': [], 'lossList': [0.0, -1.3399030423164369, 0.0, 38.6553076505661, 0.0, 0.0, 0.0], 'rewardMean': 0.6536804358900149, 'totalEpisodes': 170, 'stepsPerEpisode': 164, 'rewardPerEpisode': 127.13414408685175
'totalSteps': 8960, 'rewardStep': 0.7535085361489732, 'errorList': [], 'lossList': [0.0, -1.3264766228199005, 0.0, 35.11795967578888, 0.0, 0.0, 0.0], 'rewardMean': 0.667941593069866, 'totalEpisodes': 180, 'stepsPerEpisode': 109, 'rewardPerEpisode': 89.35261837890967
'totalSteps': 10240, 'rewardStep': 0.7234626255668992, 'errorList': [], 'lossList': [0.0, -1.3046601724624634, 0.0, 31.566629930734635, 0.0, 0.0, 0.0], 'rewardMean': 0.6748817221319952, 'totalEpisodes': 189, 'stepsPerEpisode': 56, 'rewardPerEpisode': 48.82856766113754
'totalSteps': 11520, 'rewardStep': 0.37980470264244753, 'errorList': [], 'lossList': [0.0, -1.297250161767006, 0.0, 9.369552790522576, 0.0, 0.0, 0.0], 'rewardMean': 0.6420953866331565, 'totalEpisodes': 192, 'stepsPerEpisode': 125, 'rewardPerEpisode': 94.51686414961358
'totalSteps': 12800, 'rewardStep': 0.5113924798999926, 'errorList': [], 'lossList': [0.0, -1.2767900800704957, 0.0, 54.01877069950104, 0.0, 0.0, 0.0], 'rewardMean': 0.6290250959598401, 'totalEpisodes': 197, 'stepsPerEpisode': 499, 'rewardPerEpisode': 382.78956951787944
'totalSteps': 14080, 'rewardStep': 0.7772496912794913, 'errorList': [], 'lossList': [0.0, -1.2600365281105042, 0.0, 52.69162044763565, 0.0, 0.0, 0.0], 'rewardMean': 0.6252450518036486, 'totalEpisodes': 201, 'stepsPerEpisode': 51, 'rewardPerEpisode': 45.70413006930006
'totalSteps': 15360, 'rewardStep': 0.6318702829700498, 'errorList': [], 'lossList': [0.0, -1.2623088121414185, 0.0, 34.62399514317512, 0.0, 0.0, 0.0], 'rewardMean': 0.6114869370983383, 'totalEpisodes': 206, 'stepsPerEpisode': 281, 'rewardPerEpisode': 211.4552066587453
'totalSteps': 16640, 'rewardStep': 0.7837710213728873, 'errorList': [], 'lossList': [0.0, -1.263208372592926, 0.0, 20.603843240737916, 0.0, 0.0, 0.0], 'rewardMean': 0.637357994854395, 'totalEpisodes': 213, 'stepsPerEpisode': 25, 'rewardPerEpisode': 22.609335622097287
'totalSteps': 17920, 'rewardStep': 0.43158790431810645, 'errorList': [], 'lossList': [0.0, -1.2636239606142043, 0.0, 9.314055339097976, 0.0, 0.0, 0.0], 'rewardMean': 0.6063119619249748, 'totalEpisodes': 217, 'stepsPerEpisode': 379, 'rewardPerEpisode': 251.28381492911916
'totalSteps': 19200, 'rewardStep': 0.859183101348356, 'errorList': [], 'lossList': [0.0, -1.2509668493270873, 0.0, 4.421551374197006, 0.0, 0.0, 0.0], 'rewardMean': 0.6589691579810674, 'totalEpisodes': 219, 'stepsPerEpisode': 268, 'rewardPerEpisode': 225.73654202210304
'totalSteps': 20480, 'rewardStep': 0.49518769290983067, 'errorList': [], 'lossList': [0.0, -1.226312928199768, 0.0, 5.825161634683609, 0.0, 0.0, 0.0], 'rewardMean': 0.6347018038457034, 'totalEpisodes': 221, 'stepsPerEpisode': 703, 'rewardPerEpisode': 516.2203654302665
'totalSteps': 21760, 'rewardStep': 0.8333503674276116, 'errorList': [], 'lossList': [0.0, -1.1901956403255463, 0.0, 2.992027285695076, 0.0, 0.0, 0.0], 'rewardMean': 0.6426859869735673, 'totalEpisodes': 222, 'stepsPerEpisode': 530, 'rewardPerEpisode': 453.436809362585
'totalSteps': 23040, 'rewardStep': 0.829938537877108, 'errorList': [], 'lossList': [0.0, -1.1350348883867263, 0.0, 1.0372892300784589, 0.0, 0.0, 0.0], 'rewardMean': 0.6533335782045882, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.5127239894737
'totalSteps': 24320, 'rewardStep': 0.7451437294008479, 'errorList': [], 'lossList': [0.0, -1.0897661197185515, 0.0, 0.7705678873509169, 0.0, 0.0, 0.0], 'rewardMean': 0.6898674808804282, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.5132174014448
'totalSteps': 25600, 'rewardStep': 0.9795440926209561, 'errorList': [0.23619405884718303, 0.1580744108906768, 0.2188306905259778, 0.1820959359016248, 0.2198131614051523, 0.1612733973769448, 0.1911874163392718, 0.18064774705160216, 0.19777471848812167, 0.20771939385609547, 0.2832773031745358, 0.14866070795470676, 0.20127291011137452, 0.29125466336362305, 0.2185230476665526, 0.20785585968865375, 0.2144479193527298, 0.19859457829321203, 0.1903892786182841, 0.1518051407014738, 0.1865654285632813, 0.17674228816231075, 0.21881174796607772, 0.20837557689315958, 0.2186101790948311, 0.2658518281605661, 0.24796808160294276, 0.17372959134152732, 0.2286698794023248, 0.20370877496675313, 0.1899231680867456, 0.23043674627840954, 0.2139636049577062, 0.20023631536384004, 0.23141970478522775, 0.19030077873015627, 0.27157853591571257, 0.20705766895494018, 0.1851105853462424, 0.2120098955116725, 0.25711132237028744, 0.2518886351240247, 0.1889659726092035, 0.23202734870326402, 0.2864245060439877, 0.27479591114892193, 0.18936010784155985, 0.18907683935386074, 0.2110120703857828, 0.17797756087829947], 'lossList': [0.0, -1.0423827785253525, 0.0, 1.1211929818615318, 0.0, 0.0, 0.0], 'rewardMean': 0.7366826421525245, 'totalEpisodes': 222, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1191.8385168601333, 'successfulTests': 20
#maxSuccessfulTests=20, maxSuccessfulTestsAtStep=25600, timeSpent=73.26
