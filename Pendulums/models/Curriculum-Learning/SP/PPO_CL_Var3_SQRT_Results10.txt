#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 10
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.7261699002785312, 'errorList': [], 'lossList': [0.0, -1.4048477214574815, 0.0, 26.607114996910095, 0.0, 0.0, 0.0], 'rewardMean': 0.7629844303887472, 'totalEpisodes': 19, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.840639662006444
'totalSteps': 3840, 'rewardStep': 0.9559918750616718, 'errorList': [], 'lossList': [0.0, -1.3938001471757888, 0.0, 45.50059787750244, 0.0, 0.0, 0.0], 'rewardMean': 0.8273202452797221, 'totalEpisodes': 43, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.4599653067994325
'totalSteps': 5120, 'rewardStep': 0.6555207836526086, 'errorList': [], 'lossList': [0.0, -1.373906528353691, 0.0, 63.84759304046631, 0.0, 0.0, 0.0], 'rewardMean': 0.7843703798729438, 'totalEpisodes': 69, 'stepsPerEpisode': 45, 'rewardPerEpisode': 32.55900110174514
'totalSteps': 6400, 'rewardStep': 0.799121322578806, 'errorList': [], 'lossList': [0.0, -1.3535556071996688, 0.0, 91.43143497467041, 0.0, 0.0, 0.0], 'rewardMean': 0.7873205684141162, 'totalEpisodes': 117, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.757241884785358
'totalSteps': 7680, 'rewardStep': 0.8094911817547572, 'errorList': [], 'lossList': [0.0, -1.3482946801185607, 0.0, 38.11366115570068, 0.0, 0.0, 0.0], 'rewardMean': 0.7910156706375564, 'totalEpisodes': 141, 'stepsPerEpisode': 38, 'rewardPerEpisode': 28.315952835896205
'totalSteps': 8960, 'rewardStep': 0.6745890342666645, 'errorList': [], 'lossList': [0.0, -1.3438616794347764, 0.0, 34.781930389404295, 0.0, 0.0, 0.0], 'rewardMean': 0.7743832940131432, 'totalEpisodes': 153, 'stepsPerEpisode': 115, 'rewardPerEpisode': 89.76067710947454
'totalSteps': 10240, 'rewardStep': 0.6427540424551527, 'errorList': [], 'lossList': [0.0, -1.3395436656475068, 0.0, 38.814522457122806, 0.0, 0.0, 0.0], 'rewardMean': 0.7579296375683944, 'totalEpisodes': 166, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.335124999353198
'totalSteps': 11520, 'rewardStep': 0.7086353910589182, 'errorList': [], 'lossList': [0.0, -1.327447006702423, 0.0, 28.233260564804077, 0.0, 0.0, 0.0], 'rewardMean': 0.7524524990673415, 'totalEpisodes': 173, 'stepsPerEpisode': 68, 'rewardPerEpisode': 58.91321039603901
'totalSteps': 12800, 'rewardStep': 0.8809829912725258, 'errorList': [], 'lossList': [0.0, -1.2954608798027039, 0.0, 11.932155196666718, 0.0, 0.0, 0.0], 'rewardMean': 0.7653055482878599, 'totalEpisodes': 178, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.663225389033695
'totalSteps': 14080, 'rewardStep': 0.7290409031290499, 'errorList': [], 'lossList': [0.0, -1.2578106528520585, 0.0, 6.764202319383621, 0.0, 0.0, 0.0], 'rewardMean': 0.7582297425508686, 'totalEpisodes': 181, 'stepsPerEpisode': 284, 'rewardPerEpisode': 229.5760992610571
'totalSteps': 15360, 'rewardStep': 0.5347354423590892, 'errorList': [], 'lossList': [0.0, -1.2401384925842285, 0.0, 25.356122686862946, 0.0, 0.0, 0.0], 'rewardMean': 0.7390862967589245, 'totalEpisodes': 185, 'stepsPerEpisode': 296, 'rewardPerEpisode': 233.04788978363382
'totalSteps': 16640, 'rewardStep': 0.8856750814903307, 'errorList': [], 'lossList': [0.0, -1.2258175832033158, 0.0, 5.146943983435631, 0.0, 0.0, 0.0], 'rewardMean': 0.7320546174017903, 'totalEpisodes': 189, 'stepsPerEpisode': 66, 'rewardPerEpisode': 54.762997413845284
'totalSteps': 17920, 'rewardStep': 0.8206490835405179, 'errorList': [], 'lossList': [0.0, -1.2022411966323852, 0.0, 4.5586925840377805, 0.0, 0.0, 0.0], 'rewardMean': 0.7485674473905812, 'totalEpisodes': 192, 'stepsPerEpisode': 465, 'rewardPerEpisode': 401.5488517157493
'totalSteps': 19200, 'rewardStep': 0.9347359955679446, 'errorList': [0.7094882042186924, 0.22413629233765786, 0.16249612874803798, 0.3127648462404479, 0.18661504330017806, 0.17680119495282218, 0.7494387192833065, 0.4354016254975753, 0.5730749822871798, 0.22117618116706686, 0.6527374531815758, 0.5729530104047216, 0.3869700628975245, 0.4566440484164629, 0.7759443686388714, 0.2755677273090364, 1.061899427604147, 0.21410651289699462, 0.1997220641334576, 0.15919900610059567, 0.17198867724561176, 0.13168724183077918, 0.7713819254046728, 0.09464730018634711, 0.705280137083627, 0.3450690848731968, 0.4828820214142079, 0.4374541115430463, 0.5745527906946493, 0.7040307262212705, 0.35213288365771167, 0.0984819059561405, 0.5388446314608564, 0.11114992741230377, 0.5811860080380463, 0.36957780421999387, 0.15909596565504147, 0.23284142966704366, 0.5150634823652266, 0.4606604393647992, 0.21890746278980447, 0.16592263028974655, 0.11617194574378108, 0.24940631467278085, 0.19130527103651718, 0.16965288257060793, 0.6789923932462049, 0.4253387929248494, 0.3825856474110464, 0.4261409805593046], 'lossList': [0.0, -1.2057294756174088, 0.0, 4.048147739171982, 0.0, 0.0, 0.0], 'rewardMean': 0.7621289146894951, 'totalEpisodes': 195, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.267210306296697, 'successfulTests': 15
'totalSteps': 20480, 'rewardStep': 0.8412267246308429, 'errorList': [], 'lossList': [0.0, -1.1950634425878526, 0.0, 2.9777349781990052, 0.0, 0.0, 0.0], 'rewardMean': 0.7653024689771036, 'totalEpisodes': 196, 'stepsPerEpisode': 194, 'rewardPerEpisode': 159.94003694987805
'totalSteps': 21760, 'rewardStep': 0.9420586996301853, 'errorList': [0.20699805850528813, 0.20736826631345648, 0.302113591277184, 0.20356064398454393, 0.20037450487873387, 0.2598049802017229, 0.19709473031352603, 0.33603028820653835, 0.19731120025584636, 0.2706899169526814, 0.20152338704741293, 0.20210149378569106, 0.20136461954707113, 0.20517716428760893, 0.19830397337516859, 0.20035620677235846, 0.20131628291132517, 0.20335979972021642, 0.26751673809124105, 0.19595710889781756, 0.19681246976272573, 0.20459863734188874, 0.20852579500648338, 0.22243038347813252, 0.19814247839999374, 0.20372878966712776, 0.24224122869595194, 0.20567194799293506, 0.20228500077879072, 0.194539899065137, 0.2161425038445868, 0.20383186506944503, 0.20009799600081657, 0.1945017103905794, 0.20445276446558422, 0.199614633086686, 0.19612035424917879, 0.19563794380415125, 0.237649048749636, 0.20586848441411065, 0.2350987678572331, 0.19716806608933743, 0.1988880812527108, 0.19513470097238728, 0.19502264922820495, 0.27820751882630695, 0.1998601589482926, 0.19942767590617735, 0.22934948404578642, 0.25924384014146823], 'lossList': [0.0, -1.1866647791862488, 0.0, 4.449539155364037, 0.0, 0.0, 0.0], 'rewardMean': 0.7920494355134557, 'totalEpisodes': 197, 'stepsPerEpisode': 113, 'rewardPerEpisode': 95.26090371827907, 'successfulTests': 17
'totalSteps': 23040, 'rewardStep': 0.6840708061839775, 'errorList': [], 'lossList': [0.0, -1.152338371872902, 0.0, 0.7835123996436596, 0.0, 0.0, 0.0], 'rewardMean': 0.7961811118863382, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1071.7820364282786
'totalSteps': 24320, 'rewardStep': 0.7397436116120804, 'errorList': [], 'lossList': [0.0, -1.1189576399326324, 0.0, 0.7472774598002434, 0.0, 0.0, 0.0], 'rewardMean': 0.7992919339416543, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1079.5785969399142
'totalSteps': 25600, 'rewardStep': 0.9198736396304716, 'errorList': [], 'lossList': [0.0, -1.0792842853069304, 0.0, 1.1236031297594309, 0.0, 0.0, 0.0], 'rewardMean': 0.8031809987774491, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.690121180701
#maxSuccessfulTests=17, maxSuccessfulTestsAtStep=21760, timeSpent=96.27
