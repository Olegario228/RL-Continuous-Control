#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 100
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.5945488374039504, 'errorList': [], 'lossList': [0.0, -1.41852057158947, 0.0, 32.79995285987854, 0.0, 0.0, 0.0], 'rewardMean': 0.519385789189734, 'totalEpisodes': 72, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.2536931572095074
'totalSteps': 3840, 'rewardStep': 0.4712828759210935, 'errorList': [], 'lossList': [0.0, -1.412324343919754, 0.0, 45.7119185256958, 0.0, 0.0, 0.0], 'rewardMean': 0.5033514847668538, 'totalEpisodes': 108, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.0176649036741905
'totalSteps': 5120, 'rewardStep': 0.8670581880069964, 'errorList': [], 'lossList': [0.0, -1.3846640729904174, 0.0, 43.256800689697265, 0.0, 0.0, 0.0], 'rewardMean': 0.5942781605768894, 'totalEpisodes': 125, 'stepsPerEpisode': 169, 'rewardPerEpisode': 119.12733889369223
'totalSteps': 6400, 'rewardStep': 0.8558043527543675, 'errorList': [], 'lossList': [0.0, -1.356710093021393, 0.0, 49.849429321289065, 0.0, 0.0, 0.0], 'rewardMean': 0.646583399012385, 'totalEpisodes': 134, 'stepsPerEpisode': 177, 'rewardPerEpisode': 141.960822424227
'totalSteps': 7680, 'rewardStep': 0.6750549898548157, 'errorList': [], 'lossList': [0.0, -1.3431995236873626, 0.0, 59.610502438545225, 0.0, 0.0, 0.0], 'rewardMean': 0.6513286641527901, 'totalEpisodes': 142, 'stepsPerEpisode': 202, 'rewardPerEpisode': 156.08861958068437
'totalSteps': 8960, 'rewardStep': 0.9814737155024895, 'errorList': [], 'lossList': [0.0, -1.3384948074817657, 0.0, 99.95816164016723, 0.0, 0.0, 0.0], 'rewardMean': 0.6984922429170329, 'totalEpisodes': 158, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.503789991182163
'totalSteps': 10240, 'rewardStep': 0.6134220646860304, 'errorList': [], 'lossList': [0.0, -1.3341069746017455, 0.0, 66.3478003501892, 0.0, 0.0, 0.0], 'rewardMean': 0.6878584706381576, 'totalEpisodes': 174, 'stepsPerEpisode': 86, 'rewardPerEpisode': 67.60010952135765
'totalSteps': 11520, 'rewardStep': 0.7613865082287713, 'errorList': [], 'lossList': [0.0, -1.3219970041513442, 0.0, 34.34546626091004, 0.0, 0.0, 0.0], 'rewardMean': 0.6960282525926702, 'totalEpisodes': 183, 'stepsPerEpisode': 68, 'rewardPerEpisode': 59.504782552331044
'totalSteps': 12800, 'rewardStep': 0.7134663297543237, 'errorList': [], 'lossList': [0.0, -1.3037943989038467, 0.0, 12.550145457983017, 0.0, 0.0, 0.0], 'rewardMean': 0.6977720603088355, 'totalEpisodes': 187, 'stepsPerEpisode': 103, 'rewardPerEpisode': 79.4513813071337
'totalSteps': 14080, 'rewardStep': 0.44571600779862425, 'errorList': [], 'lossList': [0.0, -1.298667449951172, 0.0, 17.191653559207918, 0.0, 0.0, 0.0], 'rewardMean': 0.6979213869911463, 'totalEpisodes': 191, 'stepsPerEpisode': 213, 'rewardPerEpisode': 170.1786724411667
'totalSteps': 15360, 'rewardStep': 0.6806627705142091, 'errorList': [], 'lossList': [0.0, -1.2838832092285157, 0.0, 6.745894955098629, 0.0, 0.0, 0.0], 'rewardMean': 0.7065327803021721, 'totalEpisodes': 193, 'stepsPerEpisode': 324, 'rewardPerEpisode': 252.57640992727707
'totalSteps': 16640, 'rewardStep': 0.8020716700509782, 'errorList': [], 'lossList': [0.0, -1.2637267100811005, 0.0, 6.53584244787693, 0.0, 0.0, 0.0], 'rewardMean': 0.7396116597151605, 'totalEpisodes': 194, 'stepsPerEpisode': 613, 'rewardPerEpisode': 503.3517001288737
'totalSteps': 17920, 'rewardStep': 0.9076809594772762, 'errorList': [], 'lossList': [0.0, -1.2419925981760025, 0.0, 4.619260105490684, 0.0, 0.0, 0.0], 'rewardMean': 0.7436739368621886, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1062.320851367401
'totalSteps': 19200, 'rewardStep': 0.9167119993812498, 'errorList': [], 'lossList': [0.0, -1.204774580001831, 0.0, 3.3018559755384924, 0.0, 0.0, 0.0], 'rewardMean': 0.7497647015248768, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1120.9349670804122
'totalSteps': 20480, 'rewardStep': 0.9494954459822448, 'errorList': [0.009817360384437469, 0.018428153037131842, 0.026375053036623256, 0.04540056862902424, 0.041987168089251216, 0.05742728200375774, 0.017161922087136298, 0.02621260792184244, 0.025097467727434673, 0.019321748629555425, 0.030791608674466734, 0.05096979877704845, 0.02728974374340288, 0.020427209787962208, 0.011857039453228616, 0.028625410461644005, 0.04716645753279572, 0.031898695414716474, 0.03813477343699986, 0.021923512577856328, 0.023964637459226768, 0.03703157673741946, 0.015860060079796966, 0.020150490731025095, 0.02873374833295035, 0.01641766332573, 0.042993681529960484, 0.013199693546720076, 0.030886163437563074, 0.01584326012116654, 0.009182387558331208, 0.01824796975137487, 0.04644098818059207, 0.03272107182805607, 0.01704303844380094, 0.027975237278141583, 0.04873566381559956, 0.02322168511742394, 0.02825288639351314, 0.007149281879782798, 0.02284990449990539, 0.02499259579729263, 0.04031444047364012, 0.023635309885400788, 0.014623594003015664, 0.01994736350875085, 0.012012804637120598, 0.04300757878621886, 0.041555389051346404, 0.02124601128298569], 'lossList': [0.0, -1.179314512014389, 0.0, 2.587947778441012, 0.0, 0.0, 0.0], 'rewardMean': 0.7772087471376198, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.8401922546323, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=71.93
