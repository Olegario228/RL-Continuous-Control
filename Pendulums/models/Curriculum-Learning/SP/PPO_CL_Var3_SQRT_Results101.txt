#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 101
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.4961627037443189, 'errorList': [], 'lossList': [0.0, -1.4159039682149888, 0.0, 33.1163254070282, 0.0, 0.0, 0.0], 'rewardMean': 0.6556064182928631, 'totalEpisodes': 65, 'stepsPerEpisode': 39, 'rewardPerEpisode': 29.146048924441214
'totalSteps': 3840, 'rewardStep': 0.692655542415295, 'errorList': [], 'lossList': [0.0, -1.4085001683235168, 0.0, 49.40821491241455, 0.0, 0.0, 0.0], 'rewardMean': 0.6679561263336738, 'totalEpisodes': 98, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.622837167215437
'totalSteps': 5120, 'rewardStep': 0.8234025019329863, 'errorList': [], 'lossList': [0.0, -1.4054061651229859, 0.0, 50.75952304840088, 0.0, 0.0, 0.0], 'rewardMean': 0.706817720233502, 'totalEpisodes': 113, 'stepsPerEpisode': 23, 'rewardPerEpisode': 16.623091311808093
'totalSteps': 6400, 'rewardStep': 0.5569922338532128, 'errorList': [], 'lossList': [0.0, -1.3992846632003784, 0.0, 39.3567658996582, 0.0, 0.0, 0.0], 'rewardMean': 0.6768526229574442, 'totalEpisodes': 118, 'stepsPerEpisode': 83, 'rewardPerEpisode': 65.43659531705666
'totalSteps': 7680, 'rewardStep': 0.6258324647567356, 'errorList': [], 'lossList': [0.0, -1.3923604291677476, 0.0, 19.804051624536513, 0.0, 0.0, 0.0], 'rewardMean': 0.6683492632573261, 'totalEpisodes': 121, 'stepsPerEpisode': 139, 'rewardPerEpisode': 104.8603596142847
'totalSteps': 8960, 'rewardStep': 0.39186323949927804, 'errorList': [], 'lossList': [0.0, -1.376590826511383, 0.0, 86.4774171257019, 0.0, 0.0, 0.0], 'rewardMean': 0.6288512598633192, 'totalEpisodes': 137, 'stepsPerEpisode': 111, 'rewardPerEpisode': 84.9144454197181
'totalSteps': 10240, 'rewardStep': 0.7438679558041181, 'errorList': [], 'lossList': [0.0, -1.3873920512199402, 0.0, 86.79610320091247, 0.0, 0.0, 0.0], 'rewardMean': 0.643228346855919, 'totalEpisodes': 144, 'stepsPerEpisode': 63, 'rewardPerEpisode': 52.23855863014139
'totalSteps': 11520, 'rewardStep': 0.48164722743279476, 'errorList': [], 'lossList': [0.0, -1.3999239468574525, 0.0, 76.60440256118774, 0.0, 0.0, 0.0], 'rewardMean': 0.6252748891422386, 'totalEpisodes': 153, 'stepsPerEpisode': 14, 'rewardPerEpisode': 7.732687105234507
'totalSteps': 12800, 'rewardStep': 0.7112625237760377, 'errorList': [], 'lossList': [0.0, -1.3830543923377991, 0.0, 61.37865330219269, 0.0, 0.0, 0.0], 'rewardMean': 0.6338736526056185, 'totalEpisodes': 161, 'stepsPerEpisode': 70, 'rewardPerEpisode': 58.90967745732244
'totalSteps': 14080, 'rewardStep': 0.8476247445026159, 'errorList': [], 'lossList': [0.0, -1.3614611679315567, 0.0, 11.919125117063523, 0.0, 0.0, 0.0], 'rewardMean': 0.6371311137717394, 'totalEpisodes': 167, 'stepsPerEpisode': 38, 'rewardPerEpisode': 32.86822929566189
'totalSteps': 15360, 'rewardStep': 0.7259570634867216, 'errorList': [], 'lossList': [0.0, -1.3523233807086945, 0.0, 11.137810236811639, 0.0, 0.0, 0.0], 'rewardMean': 0.6601105497459796, 'totalEpisodes': 171, 'stepsPerEpisode': 143, 'rewardPerEpisode': 113.90796922447981
'totalSteps': 16640, 'rewardStep': 0.6977277059527758, 'errorList': [], 'lossList': [0.0, -1.3302145540714263, 0.0, 5.041018227338791, 0.0, 0.0, 0.0], 'rewardMean': 0.6606177660997277, 'totalEpisodes': 175, 'stepsPerEpisode': 141, 'rewardPerEpisode': 114.99725617078494
'totalSteps': 17920, 'rewardStep': 0.7615703119852522, 'errorList': [], 'lossList': [0.0, -1.3204563277959824, 0.0, 3.347264575660229, 0.0, 0.0, 0.0], 'rewardMean': 0.6544345471049542, 'totalEpisodes': 179, 'stepsPerEpisode': 213, 'rewardPerEpisode': 184.9447074618179
'totalSteps': 19200, 'rewardStep': 0.8585628825665236, 'errorList': [], 'lossList': [0.0, -1.303726162314415, 0.0, 3.682964299917221, 0.0, 0.0, 0.0], 'rewardMean': 0.6845916119762855, 'totalEpisodes': 182, 'stepsPerEpisode': 113, 'rewardPerEpisode': 94.04599260490345
'totalSteps': 20480, 'rewardStep': 0.6847250944305077, 'errorList': [], 'lossList': [0.0, -1.2938324880599976, 0.0, 3.7273762649297715, 0.0, 0.0, 0.0], 'rewardMean': 0.6904808749436626, 'totalEpisodes': 186, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.52068194982391
'totalSteps': 21760, 'rewardStep': 0.3862897333751257, 'errorList': [], 'lossList': [0.0, -1.2796743929386138, 0.0, 21.554991480112076, 0.0, 0.0, 0.0], 'rewardMean': 0.6899235243312474, 'totalEpisodes': 187, 'stepsPerEpisode': 821, 'rewardPerEpisode': 528.6176223744347
'totalSteps': 23040, 'rewardStep': 0.8391183150828957, 'errorList': [], 'lossList': [0.0, -1.265071776509285, 0.0, 1.9477509121596812, 0.0, 0.0, 0.0], 'rewardMean': 0.699448560259125, 'totalEpisodes': 187, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.5159216153006
'totalSteps': 24320, 'rewardStep': 0.859011992634042, 'errorList': [], 'lossList': [0.0, -1.2500815349817276, 0.0, 2.108580286800861, 0.0, 0.0, 0.0], 'rewardMean': 0.7371850367792498, 'totalEpisodes': 187, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.3945304749147
'totalSteps': 25600, 'rewardStep': 0.9720788211448094, 'errorList': [0.16938343967832967, 0.17984583043796748, 0.19938779182862035, 0.16699683360421402, 0.18010661501246547, 0.2305138320817439, 0.19233190064999275, 0.2068190262830156, 0.22524850257012702, 0.1947652987427026, 0.179041746961003, 0.19576214614600448, 0.19619197584082235, 0.18979064327974013, 0.1870748244089695, 0.1912394820479685, 0.1857847425174528, 0.17991039847450904, 0.2289194276368298, 0.19220630990046403, 0.21030128345051688, 0.18755752114209723, 0.19349774797333674, 0.23947832470449976, 0.18842829021781976, 0.19731532796750775, 0.23460293322561152, 0.2161497729274597, 0.18759465864879113, 0.1852292149919448, 0.20364737590428555, 0.1660694560923043, 0.1737974700065763, 0.21932438476013028, 0.21724294171244057, 0.19325165896252283, 0.28165488691719637, 0.17019577764873936, 0.2048418933352546, 0.2468015877779052, 0.18936285981886855, 0.18775718544335904, 0.19125859503083242, 0.21805613507579186, 0.19836588491355342, 0.191009301498775, 0.188088263639579, 0.2016324407493053, 0.21709671131157288, 0.1799959095687811], 'lossList': [0.0, -1.2031844061613084, 0.0, 1.0119604105129838, 0.0, 0.0, 0.0], 'rewardMean': 0.7632666665161271, 'totalEpisodes': 187, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.8768594709338, 'successfulTests': 33
#maxSuccessfulTests=33, maxSuccessfulTestsAtStep=25600, timeSpent=86.58
