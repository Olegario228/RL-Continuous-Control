#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 102
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.7625480134212285, 'errorList': [], 'lossList': [0.0, -1.4179403311014176, 0.0, 35.79679041862488, 0.0, 0.0, 0.0], 'rewardMean': 0.7890926776024689, 'totalEpisodes': 81, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.3797216373222985
'totalSteps': 3840, 'rewardStep': 0.7077384452065186, 'errorList': [], 'lossList': [0.0, -1.3949962747097016, 0.0, 46.11077572822571, 0.0, 0.0, 0.0], 'rewardMean': 0.7619746001371522, 'totalEpisodes': 111, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.7338194617562595
'totalSteps': 5120, 'rewardStep': 0.7332736333845977, 'errorList': [], 'lossList': [0.0, -1.3641001838445663, 0.0, 52.21457417488098, 0.0, 0.0, 0.0], 'rewardMean': 0.7547993584490136, 'totalEpisodes': 135, 'stepsPerEpisode': 29, 'rewardPerEpisode': 17.796152721045843
'totalSteps': 6400, 'rewardStep': 0.4534178582004552, 'errorList': [], 'lossList': [0.0, -1.3437431728839875, 0.0, 52.643451080322265, 0.0, 0.0, 0.0], 'rewardMean': 0.6945230583993018, 'totalEpisodes': 150, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.790141503595216
'totalSteps': 7680, 'rewardStep': 0.9837741766097156, 'errorList': [], 'lossList': [0.0, -1.3245530474185943, 0.0, 65.28297601699829, 0.0, 0.0, 0.0], 'rewardMean': 0.7427315781010374, 'totalEpisodes': 164, 'stepsPerEpisode': 55, 'rewardPerEpisode': 42.88008940680996
'totalSteps': 8960, 'rewardStep': 0.8812621493381306, 'errorList': [], 'lossList': [0.0, -1.3321215510368347, 0.0, 74.09868271827698, 0.0, 0.0, 0.0], 'rewardMean': 0.7625216597063365, 'totalEpisodes': 181, 'stepsPerEpisode': 78, 'rewardPerEpisode': 63.944789681874056
'totalSteps': 10240, 'rewardStep': 0.8864564842275586, 'errorList': [], 'lossList': [0.0, -1.3346700209379196, 0.0, 59.33151920318603, 0.0, 0.0, 0.0], 'rewardMean': 0.7780135127714893, 'totalEpisodes': 199, 'stepsPerEpisode': 36, 'rewardPerEpisode': 26.109802239062837
'totalSteps': 11520, 'rewardStep': 0.7982317580905086, 'errorList': [], 'lossList': [0.0, -1.3262250703573226, 0.0, 59.13948094367981, 0.0, 0.0, 0.0], 'rewardMean': 0.7802599844736027, 'totalEpisodes': 211, 'stepsPerEpisode': 36, 'rewardPerEpisode': 28.898309306733037
'totalSteps': 12800, 'rewardStep': 0.8402388057854252, 'errorList': [], 'lossList': [0.0, -1.3241153591871262, 0.0, 75.67043115615844, 0.0, 0.0, 0.0], 'rewardMean': 0.7862578666047849, 'totalEpisodes': 223, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.658543221284685
'totalSteps': 14080, 'rewardStep': 0.8280046081715341, 'errorList': [], 'lossList': [0.0, -1.3254727298021316, 0.0, 38.14493110179901, 0.0, 0.0, 0.0], 'rewardMean': 0.7874945932435674, 'totalEpisodes': 231, 'stepsPerEpisode': 160, 'rewardPerEpisode': 141.16733652828657
'totalSteps': 15360, 'rewardStep': 0.1474535882261358, 'errorList': [], 'lossList': [0.0, -1.3281454038619995, 0.0, 29.711696562767028, 0.0, 0.0, 0.0], 'rewardMean': 0.725985150724058, 'totalEpisodes': 237, 'stepsPerEpisode': 93, 'rewardPerEpisode': 50.70024677261264
'totalSteps': 16640, 'rewardStep': 0.9523216982691171, 'errorList': [3.8245483371505995, 27.4086366809856, 75.12176915437341, 12.424089316044059, 45.91419739338885, 8.726123962311503, 3.2336106490780274, 48.7605677586871, 103.59848603423141, 13.337005502692742, 134.69952339922543, 29.785544006975275, 2.6060913309394302, 66.72134308523526, 2.4748245517904257, 54.07714357852232, 83.86617712786315, 1.383974061365786, 18.83444500947311, 3.0096497815740104, 0.49280911082407425, 35.8556069084571, 51.72760940313692, 3.7794156033455057, 2.095954188159978, 74.12350622317337, 24.94369573109837, 26.543332647512358, 20.540883019068282, 113.2509904005523, 70.80178473388628, 95.96590316200412, 7.078686201876044, 1.0498176331251414, 6.353990877341723, 117.75151417395574, 1.662397378881751, 42.75818872689005, 11.3900915243136, 28.167601085463847, 0.4969344751983044, 21.301371507838564, 25.405648153426696, 50.95526053823988, 0.926880770823793, 89.15470190711945, 3.7176037371895716, 117.39101688209932, 32.923751727694366, 3.6349003567559834], 'lossList': [0.0, -1.340856761932373, 0.0, 10.609447573423386, 0.0, 0.0, 0.0], 'rewardMean': 0.7504434760303178, 'totalEpisodes': 244, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.974615888690147, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.1684455922509389, 'errorList': [], 'lossList': [0.0, -1.342710485458374, 0.0, 4.966460598707199, 0.0, 0.0, 0.0], 'rewardMean': 0.6939606719169519, 'totalEpisodes': 250, 'stepsPerEpisode': 163, 'rewardPerEpisode': 112.32034521902577
'totalSteps': 19200, 'rewardStep': 0.9624319508670223, 'errorList': [20.802910793330373, 2.752475818954523, 1.812205584309967, 6.097830453378615, 8.900054027146487, 0.27744752030665987, 1.5486890198312389, 4.097733469447282, 1.2117915372653254, 2.3605961366196166, 1.8872736552986253, 61.07760177491659, 27.09229957761229, 24.896333099870716, 0.8314564836289623, 0.38236568628022416, 2.7411943069101152, 2.56816892034121, 0.512648386884056, 4.177312577769961, 9.07436432862078, 40.213631015069716, 19.220493186774473, 6.486698961738312, 0.28857850536036106, 1.005466265129112, 2.957785904850555, 3.1897663503043283, 21.88276616732876, 14.530625768385262, 4.08845132625094, 3.560900795336565, 2.0172700421828207, 0.5070253646263764, 1.1118372633939395, 15.999881730229577, 5.30862217758618, 4.983866890131625, 0.766707820882513, 8.823704732041493, 25.345945252858446, 12.198488341498573, 1.8421235577209627, 5.344789984458509, 11.400970628311823, 0.49876009844368535, 20.70688754200253, 2.4384966236827785, 1.6513325151339455, 1.7946751892564494], 'lossList': [0.0, -1.3409816133975982, 0.0, 7.59600949883461, 0.0, 0.0, 0.0], 'rewardMean': 0.7448620811836087, 'totalEpisodes': 256, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.38308595544547, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8142505211645354, 'errorList': [], 'lossList': [0.0, -1.3479347056150437, 0.0, 5.312428494095802, 0.0, 0.0, 0.0], 'rewardMean': 0.7279097156390906, 'totalEpisodes': 259, 'stepsPerEpisode': 98, 'rewardPerEpisode': 79.563265142329
'totalSteps': 21760, 'rewardStep': 0.9075312818491966, 'errorList': [], 'lossList': [0.0, -1.3438427978754044, 0.0, 4.246694731712341, 0.0, 0.0, 0.0], 'rewardMean': 0.7305366288901972, 'totalEpisodes': 261, 'stepsPerEpisode': 60, 'rewardPerEpisode': 50.74826732360605
'totalSteps': 23040, 'rewardStep': 0.9596921744982675, 'errorList': [0.5389728969864915, 0.45849332457003106, 0.4622879104307807, 0.4694600772938043, 0.43020185189662424, 0.44291686348372283, 0.4654384386173611, 0.4518760445495801, 0.43638414326592884, 0.5495381527598879, 0.44479724614100025, 0.45717346348044324, 0.47694585830380415, 0.4753129993879392, 0.4307690691975679, 0.41739636366997296, 0.5125993765052935, 0.39027243758126473, 0.4375809985148503, 0.5224690202912912, 0.4972427879514786, 0.4230624213268865, 0.4684189159025962, 0.43045002736155963, 0.5182565570785476, 0.5105219127125954, 0.47311234076632597, 0.46581071629245635, 0.4894786902906723, 0.460099753897425, 0.48429104397759276, 0.4617720801315173, 0.4613294476371926, 0.4503043903212953, 0.4773596101451607, 0.5166165598983254, 0.40194806061016886, 0.4631182811994596, 0.41722506726008834, 0.4754309141787949, 0.3987759491329531, 0.4698154026951673, 0.4417527992278434, 0.5033825455959895, 0.4749167111573644, 0.5237809049220034, 0.4626400079781366, 0.416782711202616, 0.4491917948700493, 0.5235495795626077], 'lossList': [0.0, -1.3124017387628555, 0.0, 2.989677657261491, 0.0, 0.0, 0.0], 'rewardMean': 0.7378601979172682, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.9034945792082, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.7727032839387162, 'errorList': [], 'lossList': [0.0, -1.2766751158237457, 0.0, 1.6122059318423272, 0.0, 0.0, 0.0], 'rewardMean': 0.735307350502089, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.2818862441422
'totalSteps': 25600, 'rewardStep': 0.778886872785138, 'errorList': [], 'lossList': [0.0, -1.2734837961196899, 0.0, 1.547142415046692, 0.0, 0.0, 0.0], 'rewardMean': 0.7291721572020602, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.9705812678237
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=124.31
