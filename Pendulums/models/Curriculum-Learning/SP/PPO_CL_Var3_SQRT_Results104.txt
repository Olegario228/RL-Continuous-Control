#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 104
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.6945239695357878, 'errorList': [], 'lossList': [0.0, -1.406184024810791, 0.0, 34.048042421340945, 0.0, 0.0, 0.0], 'rewardMean': 0.8078070518435975, 'totalEpisodes': 84, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.0461653197829
'totalSteps': 3840, 'rewardStep': 0.7830484733083282, 'errorList': [], 'lossList': [0.0, -1.3978424459695815, 0.0, 45.89825201034546, 0.0, 0.0, 0.0], 'rewardMean': 0.7995541923318411, 'totalEpisodes': 115, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.157437433500812
'totalSteps': 5120, 'rewardStep': 0.8817076104415853, 'errorList': [], 'lossList': [0.0, -1.3738523888587952, 0.0, 51.15235003471375, 0.0, 0.0, 0.0], 'rewardMean': 0.8200925468592771, 'totalEpisodes': 133, 'stepsPerEpisode': 40, 'rewardPerEpisode': 30.5377481764989
'totalSteps': 6400, 'rewardStep': 0.8036466259858486, 'errorList': [], 'lossList': [0.0, -1.356127471923828, 0.0, 56.5247585773468, 0.0, 0.0, 0.0], 'rewardMean': 0.8168033626845915, 'totalEpisodes': 144, 'stepsPerEpisode': 346, 'rewardPerEpisode': 266.4432439420107
'totalSteps': 7680, 'rewardStep': 0.7473819441263019, 'errorList': [], 'lossList': [0.0, -1.340439696907997, 0.0, 59.40796153068543, 0.0, 0.0, 0.0], 'rewardMean': 0.8052331262582099, 'totalEpisodes': 153, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.954734275917545
'totalSteps': 8960, 'rewardStep': 0.9417351187439754, 'errorList': [], 'lossList': [0.0, -1.325672098994255, 0.0, 77.69345586776734, 0.0, 0.0, 0.0], 'rewardMean': 0.8247334108990334, 'totalEpisodes': 162, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.760355409907037
'totalSteps': 10240, 'rewardStep': 0.5636163137000836, 'errorList': [], 'lossList': [0.0, -1.3287381929159165, 0.0, 62.40750755310059, 0.0, 0.0, 0.0], 'rewardMean': 0.7920937737491647, 'totalEpisodes': 168, 'stepsPerEpisode': 152, 'rewardPerEpisode': 111.05688951299493
'totalSteps': 11520, 'rewardStep': 0.5393670739466665, 'errorList': [], 'lossList': [0.0, -1.3218082517385483, 0.0, 72.41911780357361, 0.0, 0.0, 0.0], 'rewardMean': 0.7640130293266648, 'totalEpisodes': 173, 'stepsPerEpisode': 147, 'rewardPerEpisode': 109.034412508301
'totalSteps': 12800, 'rewardStep': 0.5420526566890431, 'errorList': [], 'lossList': [0.0, -1.30902157664299, 0.0, 107.47149425506592, 0.0, 0.0, 0.0], 'rewardMean': 0.7418169920629026, 'totalEpisodes': 181, 'stepsPerEpisode': 65, 'rewardPerEpisode': 47.28422675332428
'totalSteps': 14080, 'rewardStep': 0.8254125005988558, 'errorList': [], 'lossList': [0.0, -1.3012998527288437, 0.0, 91.8298418045044, 0.0, 0.0, 0.0], 'rewardMean': 0.7322492287076476, 'totalEpisodes': 187, 'stepsPerEpisode': 38, 'rewardPerEpisode': 32.04779117207732
'totalSteps': 15360, 'rewardStep': 0.5923964767859136, 'errorList': [], 'lossList': [0.0, -1.3031985133886337, 0.0, 48.46602823734283, 0.0, 0.0, 0.0], 'rewardMean': 0.7220364794326601, 'totalEpisodes': 190, 'stepsPerEpisode': 196, 'rewardPerEpisode': 142.73799551072932
'totalSteps': 16640, 'rewardStep': 0.564088086152418, 'errorList': [], 'lossList': [0.0, -1.3022341829538346, 0.0, 45.9172785615921, 0.0, 0.0, 0.0], 'rewardMean': 0.700140440717069, 'totalEpisodes': 193, 'stepsPerEpisode': 448, 'rewardPerEpisode': 362.9578887769564
'totalSteps': 17920, 'rewardStep': 0.34479406419727887, 'errorList': [], 'lossList': [0.0, -1.295518360733986, 0.0, 30.53184618473053, 0.0, 0.0, 0.0], 'rewardMean': 0.6464490860926386, 'totalEpisodes': 197, 'stepsPerEpisode': 201, 'rewardPerEpisode': 141.48697778397485
'totalSteps': 19200, 'rewardStep': 0.6624383094869822, 'errorList': [], 'lossList': [0.0, -1.2781616353988647, 0.0, 26.273631660938264, 0.0, 0.0, 0.0], 'rewardMean': 0.632328254442752, 'totalEpisodes': 201, 'stepsPerEpisode': 376, 'rewardPerEpisode': 261.0178104634978
'totalSteps': 20480, 'rewardStep': 0.6425759820447173, 'errorList': [], 'lossList': [0.0, -1.2738800847530365, 0.0, 72.59460687637329, 0.0, 0.0, 0.0], 'rewardMean': 0.6218476582345935, 'totalEpisodes': 205, 'stepsPerEpisode': 213, 'rewardPerEpisode': 164.9902825172748
'totalSteps': 21760, 'rewardStep': 0.934549057208905, 'errorList': [0.627282853313639, 0.5027225419311921, 0.44279767204605786, 0.41803326449976064, 0.4564489980004151, 0.5036561611446218, 0.5141276617153574, 0.5079285794109503, 0.4960100392462479, 0.4541150519171248, 0.5422869937032059, 0.7396176674811272, 0.543285387981977, 0.6131195072651275, 0.4672852492777324, 0.5088804263743776, 0.43525647375385673, 0.47680232500939795, 0.4714579551899781, 0.5024765280775939, 0.5403243104292261, 0.4259927717529071, 0.5018329202986423, 0.5066317967631376, 0.5376951150105999, 0.6194306819211407, 0.4411887371845701, 0.8261621457801875, 0.5282274410158548, 0.44628427235545337, 0.48168694256494754, 0.5720088715745165, 0.5182650277868339, 0.5115473013503818, 0.6389490734935492, 0.43427678659753854, 0.3932297635865855, 0.48007937684045826, 0.6385216888285374, 0.5833455964410489, 0.43244687411906557, 0.4436685865645961, 0.5099505625900029, 0.7161666055263647, 0.4757321713712082, 0.574122188254525, 0.6678231010518517, 0.5037558976958523, 0.5397777783468012, 0.592486753429185], 'lossList': [0.0, -1.281742929816246, 0.0, 91.36113219261169, 0.0, 0.0, 0.0], 'rewardMean': 0.6211290520810864, 'totalEpisodes': 209, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.633164809829578, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.6425765822254388, 'errorList': [], 'lossList': [0.0, -1.278416894674301, 0.0, 28.542373517751695, 0.0, 0.0, 0.0], 'rewardMean': 0.6290250789336219, 'totalEpisodes': 210, 'stepsPerEpisode': 622, 'rewardPerEpisode': 482.066032229967
'totalSteps': 24320, 'rewardStep': 0.48983296551056327, 'errorList': [], 'lossList': [0.0, -1.2804112595319748, 0.0, 8.392407443523407, 0.0, 0.0, 0.0], 'rewardMean': 0.6240716680900116, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 896.8366973375058
'totalSteps': 25600, 'rewardStep': 0.853380305395944, 'errorList': [], 'lossList': [0.0, -1.2931964069604873, 0.0, 31.820969120264053, 0.0, 0.0, 0.0], 'rewardMean': 0.6552044329607016, 'totalEpisodes': 211, 'stepsPerEpisode': 580, 'rewardPerEpisode': 484.4828335878767
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=84.35
