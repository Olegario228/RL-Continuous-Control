#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 106
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6261616738543232, 'errorList': [], 'lossList': [0.0, -1.412555685043335, 0.0, 29.03771513938904, 0.0, 0.0, 0.0], 'rewardMean': 0.7947256128825164, 'totalEpisodes': 27, 'stepsPerEpisode': 50, 'rewardPerEpisode': 33.78341812290397
'totalSteps': 3840, 'rewardStep': 0.7478607175928839, 'errorList': [], 'lossList': [0.0, -1.4022939682006836, 0.0, 43.854224042892454, 0.0, 0.0, 0.0], 'rewardMean': 0.7791039811193056, 'totalEpisodes': 48, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.87903429290586
'totalSteps': 5120, 'rewardStep': 0.5499511669552112, 'errorList': [], 'lossList': [0.0, -1.3958310610055924, 0.0, 29.071379551887514, 0.0, 0.0, 0.0], 'rewardMean': 0.721815777578282, 'totalEpisodes': 59, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.362395673275151
'totalSteps': 6400, 'rewardStep': 0.825038704101612, 'errorList': [], 'lossList': [0.0, -1.378067432641983, 0.0, 54.80867525100708, 0.0, 0.0, 0.0], 'rewardMean': 0.7424603628829479, 'totalEpisodes': 73, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.825038704101612
'totalSteps': 7680, 'rewardStep': 0.6073392216776159, 'errorList': [], 'lossList': [0.0, -1.365144567489624, 0.0, 12.611841011047364, 0.0, 0.0, 0.0], 'rewardMean': 0.7199401726820592, 'totalEpisodes': 76, 'stepsPerEpisode': 84, 'rewardPerEpisode': 59.03094753706117
'totalSteps': 8960, 'rewardStep': 0.49907291038734447, 'errorList': [], 'lossList': [0.0, -1.3545675694942474, 0.0, 113.96771131515503, 0.0, 0.0, 0.0], 'rewardMean': 0.6883877066399571, 'totalEpisodes': 92, 'stepsPerEpisode': 112, 'rewardPerEpisode': 87.19250756744285
'totalSteps': 10240, 'rewardStep': 0.8151158597951798, 'errorList': [], 'lossList': [0.0, -1.3543180829286576, 0.0, 55.179444274902345, 0.0, 0.0, 0.0], 'rewardMean': 0.70422872578436, 'totalEpisodes': 104, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.998070746534085
'totalSteps': 11520, 'rewardStep': 0.8510756668327915, 'errorList': [], 'lossList': [0.0, -1.3582910507917405, 0.0, 13.688025720119477, 0.0, 0.0, 0.0], 'rewardMean': 0.7205450525675191, 'totalEpisodes': 110, 'stepsPerEpisode': 87, 'rewardPerEpisode': 76.08661259351823
'totalSteps': 12800, 'rewardStep': 0.9571255403975288, 'errorList': [1.9115742063420933, 1.003179156747214, 0.5243940718251036, 1.3476831633450124, 0.8339172339701277, 0.5801105049920329, 1.6555083827328705, 0.5246918487646978, 1.4130684553101092, 0.17384669201363787, 1.1895310270242099, 1.6961751058480818, 0.6412262784913672, 0.9159058353449583, 1.5064364785219755, 1.026448324836287, 0.1715294245694959, 1.0626574798061479, 2.0883605921106057, 1.5772904270832169, 1.183616001314241, 0.9058695072209928, 0.9962699580037202, 1.6520197997886492, 1.14047929255684, 0.9298490826687853, 1.5053626524042587, 0.9326298123523343, 1.0500290212077608, 2.2801144505126705, 0.8619824793697909, 1.3267713243537893, 1.3008039407601175, 0.9020811439339799, 0.5851421666206772, 0.5289391299012091, 0.6446219868296184, 1.2031876298704576, 2.3581468300525494, 1.4356601030008105, 0.5352580831918428, 0.05601185170290856, 1.0813240626963156, 1.3925765620140367, 0.3043718058495332, 0.11920103992842367, 0.7610594034087831, 1.7827993298591718, 0.8793566411966822, 1.695662135514526], 'lossList': [0.0, -1.351292495727539, 0.0, 20.35915120601654, 0.0, 0.0, 0.0], 'rewardMean': 0.74420310135052, 'totalEpisodes': 117, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.749882457646326, 'successfulTests': 4
'totalSteps': 14080, 'rewardStep': 0.8738882365060904, 'errorList': [], 'lossList': [0.0, -1.3309851151704788, 0.0, 8.339777784347534, 0.0, 0.0, 0.0], 'rewardMean': 0.7352629698100581, 'totalEpisodes': 119, 'stepsPerEpisode': 112, 'rewardPerEpisode': 97.75428717443201
'totalSteps': 15360, 'rewardStep': 0.8235600428592212, 'errorList': [], 'lossList': [0.0, -1.2913549810647964, 0.0, 5.832138533592224, 0.0, 0.0, 0.0], 'rewardMean': 0.7550028067105479, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.302617868857
'totalSteps': 16640, 'rewardStep': 0.79960890886412, 'errorList': [], 'lossList': [0.0, -1.2396842873096465, 0.0, 3.6290379667282107, 0.0, 0.0, 0.0], 'rewardMean': 0.7601776258376716, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.269086857692
'totalSteps': 17920, 'rewardStep': 0.8798890300286972, 'errorList': [], 'lossList': [0.0, -1.1885165536403657, 0.0, 3.2651048378646372, 0.0, 0.0, 0.0], 'rewardMean': 0.7931714121450202, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.9293947584708
'totalSteps': 19200, 'rewardStep': 0.9630835161487343, 'errorList': [0.01605369302456469, 0.03129706400248286, 0.025518310929517958, 0.016423609287994523, 0.010448029956766318, 0.0177394067419107, 0.024995524460070504, 0.028043380664136124, 0.019923028794684577, 0.013660159648577848, 0.04355110417907081, 0.02921149241098162, 0.010598881872533073, 0.013240910894887619, 0.01411767458702177, 0.028027675265916794, 0.026007193082437877, 0.015780086584479016, 0.01876796120979397, 0.020455264973530327, 0.018171886044279092, 0.01385104551265115, 0.013747943825529452, 0.023930968475842914, 0.03541490360805328, 0.01650964592851049, 0.017371445519002254, 0.024747189014263378, 0.019183632253135924, 0.010133228233744799, 0.019280010397950822, 0.02269395288594104, 0.01814299776627491, 0.012062811717773408, 0.016726995498667248, 0.013532752774262788, 0.026335744988897278, 0.017462373616407952, 0.010868340689104633, 0.0234897763755796, 0.022343418910865255, 0.015349440915762413, 0.026719851556773963, 0.0103785161534742, 0.014105293162063355, 0.012859953093801467, 0.010073459604261849, 0.016727438302554153, 0.016159929485050243, 0.021351707519227117], 'lossList': [0.0, -1.139371697306633, 0.0, 2.8172681940346957, 0.0, 0.0, 0.0], 'rewardMean': 0.8069758933497324, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.9369772397763, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=88.52
