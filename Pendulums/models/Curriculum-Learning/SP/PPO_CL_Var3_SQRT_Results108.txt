#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 108
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.6133627358339729, 'errorList': [], 'lossList': [0.0, -1.4124384748935699, 0.0, 29.839449882507324, 0.0, 0.0, 0.0], 'rewardMean': 0.4788589636897139, 'totalEpisodes': 26, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.478703379542104
'totalSteps': 3840, 'rewardStep': 0.8629943623593628, 'errorList': [], 'lossList': [0.0, -1.3963785201311112, 0.0, 36.19287006378174, 0.0, 0.0, 0.0], 'rewardMean': 0.6069040965795969, 'totalEpisodes': 37, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.00112930555095
'totalSteps': 5120, 'rewardStep': 0.7649631901208342, 'errorList': [], 'lossList': [0.0, -1.3849420142173767, 0.0, 43.12035493850708, 0.0, 0.0, 0.0], 'rewardMean': 0.6464188699649063, 'totalEpisodes': 48, 'stepsPerEpisode': 70, 'rewardPerEpisode': 54.26938688087771
'totalSteps': 6400, 'rewardStep': 0.8755074194719937, 'errorList': [], 'lossList': [0.0, -1.379677450656891, 0.0, 43.48489968299866, 0.0, 0.0, 0.0], 'rewardMean': 0.6922365798663238, 'totalEpisodes': 59, 'stepsPerEpisode': 24, 'rewardPerEpisode': 18.50629634121575
'totalSteps': 7680, 'rewardStep': 0.8644139242220233, 'errorList': [], 'lossList': [0.0, -1.3740119183063506, 0.0, 64.72555525779724, 0.0, 0.0, 0.0], 'rewardMean': 0.720932803925607, 'totalEpisodes': 67, 'stepsPerEpisode': 82, 'rewardPerEpisode': 60.92748607520168
'totalSteps': 8960, 'rewardStep': 0.8301849112973987, 'errorList': [], 'lossList': [0.0, -1.3631037896871567, 0.0, 67.80566536903382, 0.0, 0.0, 0.0], 'rewardMean': 0.7365402478358629, 'totalEpisodes': 75, 'stepsPerEpisode': 119, 'rewardPerEpisode': 92.66502298194777
'totalSteps': 10240, 'rewardStep': 0.682165948401326, 'errorList': [], 'lossList': [0.0, -1.3630922096967697, 0.0, 47.80906706809998, 0.0, 0.0, 0.0], 'rewardMean': 0.7297434604065458, 'totalEpisodes': 83, 'stepsPerEpisode': 83, 'rewardPerEpisode': 70.56043601898895
'totalSteps': 11520, 'rewardStep': 0.5804486337841746, 'errorList': [], 'lossList': [0.0, -1.3713351941108705, 0.0, 21.23881505250931, 0.0, 0.0, 0.0], 'rewardMean': 0.7131551463373934, 'totalEpisodes': 88, 'stepsPerEpisode': 144, 'rewardPerEpisode': 113.1523416015071
'totalSteps': 12800, 'rewardStep': 0.4655837277443801, 'errorList': [], 'lossList': [0.0, -1.3803622126579285, 0.0, 12.17926800251007, 0.0, 0.0, 0.0], 'rewardMean': 0.6883980044780921, 'totalEpisodes': 93, 'stepsPerEpisode': 135, 'rewardPerEpisode': 99.62974276457051
'totalSteps': 14080, 'rewardStep': 0.7538141517095338, 'errorList': [], 'lossList': [0.0, -1.3856427216529845, 0.0, 9.473939344882965, 0.0, 0.0, 0.0], 'rewardMean': 0.7293439004945, 'totalEpisodes': 96, 'stepsPerEpisode': 684, 'rewardPerEpisode': 550.6778676068878
'totalSteps': 15360, 'rewardStep': 0.7042768456998413, 'errorList': [], 'lossList': [0.0, -1.3935428124666214, 0.0, 4.169460523426533, 0.0, 0.0, 0.0], 'rewardMean': 0.7384353114810869, 'totalEpisodes': 97, 'stepsPerEpisode': 535, 'rewardPerEpisode': 384.72466120511166
'totalSteps': 16640, 'rewardStep': 0.8991036924877023, 'errorList': [], 'lossList': [0.0, -1.368273599743843, 0.0, 4.519082717299462, 0.0, 0.0, 0.0], 'rewardMean': 0.7420462444939208, 'totalEpisodes': 97, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1087.1398218822537
'totalSteps': 17920, 'rewardStep': 0.9583842045420436, 'errorList': [0.4046491897334986, 0.405890454591556, 0.3350132983267343, 0.392643445946052, 0.3725733988626456, 0.30422564391341067, 0.370208802225699, 0.516338946469637, 0.32549058932749053, 0.3833856847094162, 0.3698949019315214, 0.3270637159293096, 0.35793868349005137, 0.4189738582266155, 0.3636337947815153, 0.4831630519166955, 0.36588341711510664, 0.32523647319352944, 0.36014941177078874, 0.4533879154155072, 0.4138830568425282, 0.4416969055865034, 0.42865007476469535, 0.4538113806773508, 0.4056225230906957, 0.5020001512079388, 0.3640633554552563, 0.2917261575265887, 0.5254212109605444, 0.3511147078487886, 0.43304181832783367, 0.2827676736107682, 0.5256279856502546, 0.37067152795189207, 0.3932282909407683, 0.38616324743953373, 0.4698638514147423, 0.3166150621706701, 0.4170549394991933, 0.3798947439878162, 0.3377240843842992, 0.3494437724308906, 0.40172494978720574, 0.3670083934801171, 0.42228144447584676, 0.30763334157381506, 0.35278823054428093, 0.3866553446944074, 0.3544776673954931, 0.3934341899086894], 'lossList': [0.0, -1.3316004467010498, 0.0, 2.90738728761673, 0.0, 0.0, 0.0], 'rewardMean': 0.7613883459360418, 'totalEpisodes': 97, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.3795187265953, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.8622892186499075, 'errorList': [], 'lossList': [0.0, -1.3198968118429184, 0.0, 2.1251348704099655, 0.0, 0.0, 0.0], 'rewardMean': 0.760066525853833, 'totalEpisodes': 97, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 955.3229666366188
'totalSteps': 20480, 'rewardStep': 0.8855802315644116, 'errorList': [], 'lossList': [0.0, -1.2789331555366517, 0.0, 1.5282363885641097, 0.0, 0.0, 0.0], 'rewardMean': 0.7621831565880719, 'totalEpisodes': 97, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.1543714648171
'totalSteps': 21760, 'rewardStep': 0.9336480348872105, 'errorList': [0.04608704674820542, 0.048971326231792366, 0.08282174762911566, 0.06503981847860967, 0.0764550929093265, 0.05388746105792787, 0.048697119602117755, 0.10122946910120421, 0.052800677644059914, 0.046583828458591395, 0.06740384428454281, 0.06100046112927758, 0.04783375442141292, 0.08414480889798909, 0.08342980802906544, 0.09895874395859458, 0.06996866635476989, 0.11045267654639439, 0.052290820755966465, 0.05633651748584336, 0.11872691711429569, 0.06453944887078997, 0.0603004978992741, 0.09758965870692811, 0.0381181969427551, 0.06865950671227612, 0.056098742767337006, 0.05802354005393603, 0.08262387235795826, 0.059340167497656346, 0.07100280909802537, 0.06737679949011835, 0.09868105245837937, 0.07009596875645864, 0.04639546620212605, 0.05136261919563152, 0.07157829774014896, 0.09699152545657724, 0.08812168518831587, 0.05786698327848212, 0.048833661481465944, 0.05408514997368214, 0.06427497283444875, 0.047562270042687975, 0.041563867858432996, 0.05474481904342539, 0.06698397090107773, 0.07087468347357102, 0.058514994583415865, 0.05119290096618627], 'lossList': [0.0, -1.2311746340990066, 0.0, 1.2417124500125647, 0.0, 0.0, 0.0], 'rewardMean': 0.7725294689470531, 'totalEpisodes': 97, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.757934433877, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=96.33
