#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 109
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7842078068417688, 'errorList': [], 'lossList': [0.0, -1.4001298248767853, 0.0, 27.399456915855406, 0.0, 0.0, 0.0], 'rewardMean': 0.5741389563086787, 'totalEpisodes': 32, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.512040098907278
'totalSteps': 3840, 'rewardStep': 0.735277506565988, 'errorList': [], 'lossList': [0.0, -1.391819104552269, 0.0, 45.276154022216794, 0.0, 0.0, 0.0], 'rewardMean': 0.6278518063944485, 'totalEpisodes': 58, 'stepsPerEpisode': 57, 'rewardPerEpisode': 39.173566427545104
'totalSteps': 5120, 'rewardStep': 0.8031380695033382, 'errorList': [], 'lossList': [0.0, -1.386797001361847, 0.0, 33.60334210395813, 0.0, 0.0, 0.0], 'rewardMean': 0.6716733721716709, 'totalEpisodes': 69, 'stepsPerEpisode': 44, 'rewardPerEpisode': 37.507717236905144
'totalSteps': 6400, 'rewardStep': 0.7296576821116731, 'errorList': [], 'lossList': [0.0, -1.374382767677307, 0.0, 47.377085304260255, 0.0, 0.0, 0.0], 'rewardMean': 0.6832702341596713, 'totalEpisodes': 79, 'stepsPerEpisode': 274, 'rewardPerEpisode': 195.63982765319102
'totalSteps': 7680, 'rewardStep': 0.4945251413490982, 'errorList': [], 'lossList': [0.0, -1.3515749812126159, 0.0, 43.987316193580625, 0.0, 0.0, 0.0], 'rewardMean': 0.6518127186912425, 'totalEpisodes': 85, 'stepsPerEpisode': 253, 'rewardPerEpisode': 172.24690770570183
'totalSteps': 8960, 'rewardStep': 0.6006301728257877, 'errorList': [], 'lossList': [0.0, -1.342794002890587, 0.0, 112.76805053710937, 0.0, 0.0, 0.0], 'rewardMean': 0.644500926424749, 'totalEpisodes': 110, 'stepsPerEpisode': 63, 'rewardPerEpisode': 46.033895599139974
'totalSteps': 10240, 'rewardStep': 0.7484856793272201, 'errorList': [], 'lossList': [0.0, -1.3474214732646943, 0.0, 69.7946936416626, 0.0, 0.0, 0.0], 'rewardMean': 0.6574990205375579, 'totalEpisodes': 135, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4834019737314885
'totalSteps': 11520, 'rewardStep': 0.7482947724224801, 'errorList': [], 'lossList': [0.0, -1.3435364216566086, 0.0, 26.542344183921813, 0.0, 0.0, 0.0], 'rewardMean': 0.6675874374136604, 'totalEpisodes': 149, 'stepsPerEpisode': 30, 'rewardPerEpisode': 21.0534685832684
'totalSteps': 12800, 'rewardStep': 0.395779661358994, 'errorList': [], 'lossList': [0.0, -1.3300250136852265, 0.0, 13.012053203582763, 0.0, 0.0, 0.0], 'rewardMean': 0.6404066598081938, 'totalEpisodes': 156, 'stepsPerEpisode': 73, 'rewardPerEpisode': 45.04247258459799
'totalSteps': 14080, 'rewardStep': 0.9224115329097463, 'errorList': [], 'lossList': [0.0, -1.309345200061798, 0.0, 11.42657029747963, 0.0, 0.0, 0.0], 'rewardMean': 0.6962408025216094, 'totalEpisodes': 161, 'stepsPerEpisode': 165, 'rewardPerEpisode': 146.6779169425274
'totalSteps': 15360, 'rewardStep': 0.6584792244379686, 'errorList': [], 'lossList': [0.0, -1.2798783195018768, 0.0, 25.82610481739044, 0.0, 0.0, 0.0], 'rewardMean': 0.6836679442812293, 'totalEpisodes': 167, 'stepsPerEpisode': 61, 'rewardPerEpisode': 43.99260066643407
'totalSteps': 16640, 'rewardStep': 0.47712061666538513, 'errorList': [], 'lossList': [0.0, -1.268732513189316, 0.0, 8.716943497657775, 0.0, 0.0, 0.0], 'rewardMean': 0.6578522552911691, 'totalEpisodes': 174, 'stepsPerEpisode': 126, 'rewardPerEpisode': 100.56744188509815
'totalSteps': 17920, 'rewardStep': 0.7484140093824454, 'errorList': [], 'lossList': [0.0, -1.253354315161705, 0.0, 10.86381575345993, 0.0, 0.0, 0.0], 'rewardMean': 0.6523798492790799, 'totalEpisodes': 175, 'stepsPerEpisode': 124, 'rewardPerEpisode': 106.10563096181998
'totalSteps': 19200, 'rewardStep': 0.8380973593037573, 'errorList': [], 'lossList': [0.0, -1.2008997809886932, 0.0, 4.699878581166267, 0.0, 0.0, 0.0], 'rewardMean': 0.6632238169982883, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.9055610578288
'totalSteps': 20480, 'rewardStep': 0.8879115720903185, 'errorList': [], 'lossList': [0.0, -1.1570374870300293, 0.0, 4.052959926724434, 0.0, 0.0, 0.0], 'rewardMean': 0.7025624600724103, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.083327261747
'totalSteps': 21760, 'rewardStep': 0.9038825684031402, 'errorList': [], 'lossList': [0.0, -1.130496392250061, 0.0, 3.051820420399308, 0.0, 0.0, 0.0], 'rewardMean': 0.7328876996301454, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.242093453711
'totalSteps': 23040, 'rewardStep': 0.935543249732769, 'errorList': [0.059027821321323, 0.05878608655542446, 0.05834253184029169, 0.05810139353790608, 0.059264581654680806, 0.06025145979948082, 0.05862626778347086, 0.05940346660100649, 0.10977994198101354, 0.08906790144954296, 0.059509777134389576, 0.0600064314432535, 0.06003824341323208, 0.07337723913698514, 0.06975302883885645, 0.07611915014265527, 0.059512892895909025, 0.0620827579009164, 0.10741735137523954, 0.09886446013843152, 0.05791368619100408, 0.10025515001428606, 0.05984282535118662, 0.058036358065124485, 0.058703524280298916, 0.0792245453813327, 0.05884823644580159, 0.05874177373744193, 0.05967701004866554, 0.05970913864119978, 0.05872968675194905, 0.11256759484362934, 0.10916149489473066, 0.07406705047847392, 0.06031510634794249, 0.059733199049583366, 0.08017836949399292, 0.0682247952438094, 0.05905756533317108, 0.05995262932556231, 0.07083784643848362, 0.05871503786707796, 0.08925496609161228, 0.05976409915720451, 0.05868912863020874, 0.05977271523848988, 0.05986102815421973, 0.11441377850256937, 0.05851822788344529, 0.06004777646392741], 'lossList': [0.0, -1.111987510919571, 0.0, 2.5744518350064753, 0.0, 0.0, 0.0], 'rewardMean': 0.7515934566707003, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.1424850613585, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=77.93
