#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 11
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.6852769437418551, 'errorList': [], 'lossList': [0.0, -1.4064544934034346, 0.0, 32.094004120826725, 0.0, 0.0, 0.0], 'rewardMean': 0.601035422720654, 'totalEpisodes': 24, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.58237424353618
'totalSteps': 3840, 'rewardStep': 0.7625260838356072, 'errorList': [], 'lossList': [0.0, -1.3802711135149002, 0.0, 45.400127744674684, 0.0, 0.0, 0.0], 'rewardMean': 0.6548656430923051, 'totalEpisodes': 44, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.648238406919464
'totalSteps': 5120, 'rewardStep': 0.593600525279696, 'errorList': [], 'lossList': [0.0, -1.3580800753831863, 0.0, 80.45154945373535, 0.0, 0.0, 0.0], 'rewardMean': 0.6395493636391528, 'totalEpisodes': 75, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.852299465660776
'totalSteps': 6400, 'rewardStep': 0.46745818487740176, 'errorList': [], 'lossList': [0.0, -1.355922774076462, 0.0, 83.81501295089721, 0.0, 0.0, 0.0], 'rewardMean': 0.6051311278868026, 'totalEpisodes': 116, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.5423946738302228
'totalSteps': 7680, 'rewardStep': 0.8058674119922271, 'errorList': [], 'lossList': [0.0, -1.3564733475446702, 0.0, 44.40200414180756, 0.0, 0.0, 0.0], 'rewardMean': 0.6385871752377067, 'totalEpisodes': 132, 'stepsPerEpisode': 37, 'rewardPerEpisode': 26.401428367325384
'totalSteps': 8960, 'rewardStep': 0.5062920196786989, 'errorList': [], 'lossList': [0.0, -1.345455870628357, 0.0, 39.96595176696778, 0.0, 0.0, 0.0], 'rewardMean': 0.6196878673007056, 'totalEpisodes': 142, 'stepsPerEpisode': 109, 'rewardPerEpisode': 84.7504009073324
'totalSteps': 10240, 'rewardStep': 0.9133926916342557, 'errorList': [], 'lossList': [0.0, -1.3343253278732299, 0.0, 19.574350357055664, 0.0, 0.0, 0.0], 'rewardMean': 0.6564009703423994, 'totalEpisodes': 149, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.6198526374189504
'totalSteps': 11520, 'rewardStep': 0.558482026156411, 'errorList': [], 'lossList': [0.0, -1.3270255023241042, 0.0, 16.719745533466337, 0.0, 0.0, 0.0], 'rewardMean': 0.6455210876550673, 'totalEpisodes': 154, 'stepsPerEpisode': 139, 'rewardPerEpisode': 106.87164334398776
'totalSteps': 12800, 'rewardStep': 0.6379733308947635, 'errorList': [], 'lossList': [0.0, -1.3072985714673997, 0.0, 9.599840652942657, 0.0, 0.0, 0.0], 'rewardMean': 0.644766311979037, 'totalEpisodes': 159, 'stepsPerEpisode': 122, 'rewardPerEpisode': 87.06551618963384
'totalSteps': 14080, 'rewardStep': 0.4513259320228917, 'errorList': [], 'lossList': [0.0, -1.297153332233429, 0.0, 8.186688892841339, 0.0, 0.0, 0.0], 'rewardMean': 0.6382195150113807, 'totalEpisodes': 166, 'stepsPerEpisode': 106, 'rewardPerEpisode': 63.04889260550337
'totalSteps': 15360, 'rewardStep': 0.7530544934532144, 'errorList': [], 'lossList': [0.0, -1.2909135442972184, 0.0, 6.73597260415554, 0.0, 0.0, 0.0], 'rewardMean': 0.6449972699825167, 'totalEpisodes': 169, 'stepsPerEpisode': 574, 'rewardPerEpisode': 492.63504722123986
'totalSteps': 16640, 'rewardStep': 0.6796199318121157, 'errorList': [], 'lossList': [0.0, -1.2948314529657363, 0.0, 4.196559743881226, 0.0, 0.0, 0.0], 'rewardMean': 0.6367066547801675, 'totalEpisodes': 171, 'stepsPerEpisode': 248, 'rewardPerEpisode': 193.1385777522088
'totalSteps': 17920, 'rewardStep': 0.8752742611726668, 'errorList': [], 'lossList': [0.0, -1.2867127776145935, 0.0, 3.640373288989067, 0.0, 0.0, 0.0], 'rewardMean': 0.6648740283694646, 'totalEpisodes': 174, 'stepsPerEpisode': 114, 'rewardPerEpisode': 95.1801659656146
'totalSteps': 19200, 'rewardStep': 0.9320636407603011, 'errorList': [0.21309933056148206, 0.05298489137035678, 0.06518181322606835, 0.12289574711205142, 0.05431111729870648, 0.14294488198855232, 0.17147977957799534, 0.0765169639614012, 0.1415734560854374, 0.08933616739262104, 0.22887789793998026, 0.056589507719752706, 0.1975665816615095, 0.22284660199970133, 0.054649227166297255, 0.18005699745129225, 0.05328376896135809, 0.05493687991899742, 0.05438271748497099, 0.09288577834166291, 0.08108604295739842, 0.13794668069542876, 0.13178499397823237, 0.056254009752041596, 0.16480876363920885, 0.23166193945189384, 0.08336312385612141, 0.19180716161842817, 0.09337836333627296, 0.1263143741342928, 0.0581165519124168, 0.05593101236024435, 0.05456705726042669, 0.17104351654005956, 0.19121085470821486, 0.0702817412835289, 0.1169056843230794, 0.08860019172491358, 0.057672910241205314, 0.12790838374589816, 0.05515257776189741, 0.24875002423568304, 0.13942643548418082, 0.10693077157346356, 0.05507037724848906, 0.0892130769145754, 0.12919397945176017, 0.17074135772764384, 0.17467770867068003, 0.11486434822226799], 'lossList': [0.0, -1.2665845119953156, 0.0, 5.090353535413742, 0.0, 0.0, 0.0], 'rewardMean': 0.7113345739577546, 'totalEpisodes': 177, 'stepsPerEpisode': 118, 'rewardPerEpisode': 98.24252475950269, 'successfulTests': 45
'totalSteps': 20480, 'rewardStep': 0.6909470405296549, 'errorList': [], 'lossList': [0.0, -1.2329503065347671, 0.0, 2.41932916611433, 0.0, 0.0, 0.0], 'rewardMean': 0.6998425368114973, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.0150101833417
'totalSteps': 21760, 'rewardStep': 0.7650852232714552, 'errorList': [], 'lossList': [0.0, -1.190712279677391, 0.0, 1.373390117585659, 0.0, 0.0, 0.0], 'rewardMean': 0.725721857170773, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.3018949491484
'totalSteps': 23040, 'rewardStep': 0.899181239747532, 'errorList': [], 'lossList': [0.0, -1.1717555737495422, 0.0, 1.4120785640925169, 0.0, 0.0, 0.0], 'rewardMean': 0.7243007119821006, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.2933008592995
'totalSteps': 24320, 'rewardStep': 0.8092661661186532, 'errorList': [], 'lossList': [0.0, -1.1427108055353166, 0.0, 0.8208308492973447, 0.0, 0.0, 0.0], 'rewardMean': 0.7493791259783248, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.1371682336721
'totalSteps': 25600, 'rewardStep': 0.9832331480904782, 'errorList': [0.039043639542042506, 0.04783215014752513, 0.0680119854481978, 0.03954008619656385, 0.049234309643736984, 0.04369646555142743, 0.05506536909114518, 0.07729038279419624, 0.04898853822594662, 0.0648969613555988, 0.04756846626298495, 0.0557048268160488, 0.058060906428424076, 0.05952541474890649, 0.05598164728386467, 0.05463596274771991, 0.04178261782858617, 0.048152765910260464, 0.0968034786960378, 0.04357850312861888, 0.08250204352795314, 0.05669540053748664, 0.0616784795710862, 0.043307928850064674, 0.057670938328165174, 0.06820476962154268, 0.08483528771059894, 0.08687944597776348, 0.04838092569491662, 0.053938856214929415, 0.05328275419833167, 0.03934788190294264, 0.04386961681746923, 0.0441186495162815, 0.08566518015856373, 0.062388651914313316, 0.045619591557776694, 0.03959951100883068, 0.07501641104163313, 0.04426569873622566, 0.05881626298866965, 0.05659976950807093, 0.059696417500543233, 0.08748342222782267, 0.06901575447297507, 0.04256413841785228, 0.05712781000676831, 0.07222404412086655, 0.08650182845282707, 0.04808208671332543], 'lossList': [0.0, -1.0987014722824098, 0.0, 0.7866872889921069, 0.0, 0.0, 0.0], 'rewardMean': 0.7839051076978962, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1211.1934435590595, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=98.87
