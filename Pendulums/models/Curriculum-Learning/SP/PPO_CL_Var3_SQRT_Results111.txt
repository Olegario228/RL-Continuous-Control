#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 111
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.6618955972278966, 'errorList': [], 'lossList': [0.0, -1.4278184390068054, 0.0, 29.28128185749054, 0.0, 0.0, 0.0], 'rewardMean': 0.5893447494636748, 'totalEpisodes': 16, 'stepsPerEpisode': 104, 'rewardPerEpisode': 79.76260667717465
'totalSteps': 3840, 'rewardStep': 0.8399287474718555, 'errorList': [], 'lossList': [0.0, -1.436508339047432, 0.0, 43.52046983718872, 0.0, 0.0, 0.0], 'rewardMean': 0.672872748799735, 'totalEpisodes': 27, 'stepsPerEpisode': 166, 'rewardPerEpisode': 117.18133409130678
'totalSteps': 5120, 'rewardStep': 0.9462319784879193, 'errorList': [], 'lossList': [0.0, -1.4384831178188324, 0.0, 34.6853625535965, 0.0, 0.0, 0.0], 'rewardMean': 0.7412125562217811, 'totalEpisodes': 34, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.545387930515278
'totalSteps': 6400, 'rewardStep': 0.4460463140512118, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6428238088315913, 'totalEpisodes': 43, 'stepsPerEpisode': 83, 'rewardPerEpisode': 67.21630272286271
'totalSteps': 7680, 'rewardStep': 0.5791714997518529, 'errorList': [], 'lossList': [0.0, -1.4339733904600143, 0.0, 42.36799349308014, 0.0, 0.0, 0.0], 'rewardMean': 0.6337306218202, 'totalEpisodes': 48, 'stepsPerEpisode': 574, 'rewardPerEpisode': 353.2871322386088
'totalSteps': 8960, 'rewardStep': 0.7718316347585871, 'errorList': [], 'lossList': [0.0, -1.4128647094964981, 0.0, 109.55832466125489, 0.0, 0.0, 0.0], 'rewardMean': 0.6509932484374985, 'totalEpisodes': 63, 'stepsPerEpisode': 109, 'rewardPerEpisode': 81.30943076531993
'totalSteps': 10240, 'rewardStep': 0.6980484347976313, 'errorList': [], 'lossList': [0.0, -1.3970771515369416, 0.0, 76.0845088005066, 0.0, 0.0, 0.0], 'rewardMean': 0.6562216024775133, 'totalEpisodes': 82, 'stepsPerEpisode': 53, 'rewardPerEpisode': 44.94009367718097
'totalSteps': 11520, 'rewardStep': 0.9219215523919099, 'errorList': [], 'lossList': [0.0, -1.388143863081932, 0.0, 35.18713108062744, 0.0, 0.0, 0.0], 'rewardMean': 0.682791597468953, 'totalEpisodes': 96, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.030497655359575
'totalSteps': 12800, 'rewardStep': 0.7294978406259595, 'errorList': [], 'lossList': [0.0, -1.3727411478757858, 0.0, 29.82092116355896, 0.0, 0.0, 0.0], 'rewardMean': 0.7040619913616035, 'totalEpisodes': 104, 'stepsPerEpisode': 33, 'rewardPerEpisode': 25.433552255598254
'totalSteps': 14080, 'rewardStep': 0.8297651551983878, 'errorList': [], 'lossList': [0.0, -1.3457342690229417, 0.0, 31.45843678474426, 0.0, 0.0, 0.0], 'rewardMean': 0.7208489471586528, 'totalEpisodes': 107, 'stepsPerEpisode': 197, 'rewardPerEpisode': 163.90194274181562
'totalSteps': 15360, 'rewardStep': 0.7255456881702169, 'errorList': [], 'lossList': [0.0, -1.3219310742616655, 0.0, 19.54333237528801, 0.0, 0.0, 0.0], 'rewardMean': 0.7094106412284888, 'totalEpisodes': 115, 'stepsPerEpisode': 121, 'rewardPerEpisode': 92.36997266529639
'totalSteps': 16640, 'rewardStep': 0.9513647955179839, 'errorList': [27.797821276393613, 1.5939058454059618, 29.713920944463535, 17.138174893139418, 2.376830016293005, 55.74275457860594, 28.20297212394476, 36.792666103640926, 61.57384167637617, 76.46850929033941, 7.262967782044095, 13.24297653486376, 2.158353741239402, 40.71325987656442, 35.528877063334086, 65.21575278495392, 3.4008323492300656, 13.453665935662723, 38.794880150688705, 53.61831465544088, 3.500267878326159, 74.40160991001227, 43.813477830966235, 35.25035158726534, 3.9122397561250706, 15.532438415433665, 3.4489848108172643, 42.32705367223578, 43.34146202769766, 53.27287094917587, 16.006208897534258, 24.328600599874562, 64.0515629440501, 8.040717959962526, 18.471898353602985, 33.91720120321095, 35.04065794845675, 0.9666382387852469, 24.848382198618665, 61.06289591064741, 31.036168027498487, 35.77955098269184, 53.94095204000754, 6.703411074051741, 8.641411806283921, 44.391190446278266, 25.769820615137075, 39.8794036718024, 17.250807943151543, 9.267189600397097], 'lossList': [0.0, -1.3059068500995636, 0.0, 33.49207349300384, 0.0, 0.0, 0.0], 'rewardMean': 0.7099239229314953, 'totalEpisodes': 124, 'stepsPerEpisode': 70, 'rewardPerEpisode': 63.768260468961124, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.9336235383721, 'errorList': [0.8468238519141688, 9.820561204268806, 3.203546742584037, 10.675386761507989, 4.187989101210586, 7.988883863094376, 5.225291618222018, 18.918196502413718, 28.34239669282437, 4.558886600975418, 16.35590616240256, 10.054390685287418, 27.819618216003146, 1.3692083240358783, 5.51841946352674, 9.231189515514172, 10.167095927705123, 3.512111993704223, 2.042819719389808, 37.57128803896659, 50.030262458953636, 5.8316342135429675, 1.993369965691096, 2.4521589215128334, 27.27961077195793, 8.37837440463381, 10.108347346381658, 39.461309401304675, 42.29795138089454, 1.9672847979383528, 23.086011991882472, 7.481375122840303, 2.0212615121370554, 44.298856290086924, 8.089993728731445, 3.7878149926418745, 35.19810671200701, 16.74092403898113, 11.312646136551189, 4.241686617437875, 6.751466053971684, 31.536527141700606, 9.224965044143786, 11.891387967302997, 10.764820305080473, 8.515137017163244, 23.322213301163593, 23.653787190722355, 10.039518086530844, 1.3204303104426243], 'lossList': [0.0, -1.2893380975723268, 0.0, 8.537797819375992, 0.0, 0.0, 0.0], 'rewardMean': 0.7586816453635841, 'totalEpisodes': 130, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.442459128801687, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.9644351043162647, 'errorList': [2.911914893136943, 15.039797147596573, 3.4402342907952996, 6.971472607481292, 35.44783606667907, 8.264096387354884, 11.219106198460668, 14.743113057761892, 7.751270890069591, 24.679282342293458, 9.005114132655233, 26.822837905425402, 11.629495021575169, 5.91725991832785, 3.707345218183299, 0.623608777967264, 4.962184785533922, 2.67485371135962, 2.0102497738361897, 2.9820277886019224, 3.627448785312726, 4.680173206149802, 4.984509664904587, 1.562058279554326, 0.9270850019342136, 0.9378095423884929, 11.086484619908402, 3.478152490070455, 34.69977131648592, 7.595358530644458, 16.45739248750242, 17.087735717095573, 0.935024928334807, 4.340414885344156, 4.54238701496911, 11.532338545525585, 0.2822653950478777, 20.796217893830086, 23.83952314574266, 4.298371722277513, 14.984341661088859, 7.998940968734713, 1.4077593050469153, 1.4698356140348987, 3.7977557953584427, 27.22191226927746, 1.155122691112009, 16.642284397841898, 27.689209688569793, 8.365465581188968], 'lossList': [0.0, -1.2530653029680252, 0.0, 5.058948682546616, 0.0, 0.0, 0.0], 'rewardMean': 0.8105205243900894, 'totalEpisodes': 134, 'stepsPerEpisode': 67, 'rewardPerEpisode': 59.621909917094136, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.7497860774670277, 'errorList': [], 'lossList': [0.0, -1.2480720806121826, 0.0, 9.125872315168381, 0.0, 0.0, 0.0], 'rewardMean': 0.8275819821616069, 'totalEpisodes': 139, 'stepsPerEpisode': 55, 'rewardPerEpisode': 47.520996301874575
'totalSteps': 21760, 'rewardStep': 0.19974236565587133, 'errorList': [], 'lossList': [0.0, -1.2733170104026794, 0.0, 9.329183152914048, 0.0, 0.0, 0.0], 'rewardMean': 0.7703730552513354, 'totalEpisodes': 142, 'stepsPerEpisode': 327, 'rewardPerEpisode': 238.85672124954505
'totalSteps': 23040, 'rewardStep': 0.5669924875057648, 'errorList': [], 'lossList': [0.0, -1.2779156577587127, 0.0, 7.83916583776474, 0.0, 0.0, 0.0], 'rewardMean': 0.7572674605221487, 'totalEpisodes': 146, 'stepsPerEpisode': 178, 'rewardPerEpisode': 136.13427791798605
'totalSteps': 24320, 'rewardStep': 0.42397422653797195, 'errorList': [], 'lossList': [0.0, -1.2797384136915206, 0.0, 3.3899808144569397, 0.0, 0.0, 0.0], 'rewardMean': 0.7074727279367549, 'totalEpisodes': 147, 'stepsPerEpisode': 1222, 'rewardPerEpisode': 949.2624607778517
'totalSteps': 25600, 'rewardStep': 0.7755925244548886, 'errorList': [], 'lossList': [0.0, -1.257008507847786, 0.0, 2.112349835932255, 0.0, 0.0, 0.0], 'rewardMean': 0.7120821963196478, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 881.6475699289587
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=125.19
