#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 112
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.5947807314187786, 'errorList': [], 'lossList': [0.0, -1.442324824333191, 0.0, 29.132065868377687, 0.0, 0.0, 0.0], 'rewardMean': 0.5164056618123263, 'totalEpisodes': 21, 'stepsPerEpisode': 135, 'rewardPerEpisode': 81.8116027388282
'totalSteps': 3840, 'rewardStep': 0.7426440410805852, 'errorList': [], 'lossList': [0.0, -1.4360573393106462, 0.0, 38.35361423969269, 0.0, 0.0, 0.0], 'rewardMean': 0.5918184549017459, 'totalEpisodes': 34, 'stepsPerEpisode': 32, 'rewardPerEpisode': 23.22217035739443
'totalSteps': 5120, 'rewardStep': 0.29585754319398255, 'errorList': [], 'lossList': [0.0, -1.4168930923938752, 0.0, 28.36141641616821, 0.0, 0.0, 0.0], 'rewardMean': 0.5178282269748051, 'totalEpisodes': 46, 'stepsPerEpisode': 85, 'rewardPerEpisode': 55.06117149939572
'totalSteps': 6400, 'rewardStep': 0.3581291207918597, 'errorList': [], 'lossList': [0.0, -1.3999458420276643, 0.0, 46.873036003112794, 0.0, 0.0, 0.0], 'rewardMean': 0.485888405738216, 'totalEpisodes': 55, 'stepsPerEpisode': 49, 'rewardPerEpisode': 33.81942111486984
'totalSteps': 7680, 'rewardStep': 0.7251031374694965, 'errorList': [], 'lossList': [0.0, -1.3779725593328476, 0.0, 47.338108968734744, 0.0, 0.0, 0.0], 'rewardMean': 0.5257575276934294, 'totalEpisodes': 67, 'stepsPerEpisode': 58, 'rewardPerEpisode': 43.38512255635878
'totalSteps': 8960, 'rewardStep': 0.7391015292151929, 'errorList': [], 'lossList': [0.0, -1.3546236550807953, 0.0, 79.56649065971375, 0.0, 0.0, 0.0], 'rewardMean': 0.5562352421965385, 'totalEpisodes': 79, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7391015292151929
'totalSteps': 10240, 'rewardStep': 0.8381074369536998, 'errorList': [], 'lossList': [0.0, -1.3288706439733504, 0.0, 61.782310276031495, 0.0, 0.0, 0.0], 'rewardMean': 0.5914692665411837, 'totalEpisodes': 91, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.230225514753108
'totalSteps': 11520, 'rewardStep': 0.7797415769205638, 'errorList': [], 'lossList': [0.0, -1.3265934467315674, 0.0, 76.4318240737915, 0.0, 0.0, 0.0], 'rewardMean': 0.6123884121388925, 'totalEpisodes': 102, 'stepsPerEpisode': 192, 'rewardPerEpisode': 157.08782591739222
'totalSteps': 12800, 'rewardStep': 0.5545200869949347, 'errorList': [], 'lossList': [0.0, -1.324059574007988, 0.0, 35.00973646640777, 0.0, 0.0, 0.0], 'rewardMean': 0.6066015796244968, 'totalEpisodes': 109, 'stepsPerEpisode': 141, 'rewardPerEpisode': 83.9435478783337
'totalSteps': 14080, 'rewardStep': 0.4285905826881544, 'errorList': [], 'lossList': [0.0, -1.3162589114904404, 0.0, 7.347754502296448, 0.0, 0.0, 0.0], 'rewardMean': 0.6056575786727249, 'totalEpisodes': 111, 'stepsPerEpisode': 526, 'rewardPerEpisode': 343.2142762514058
'totalSteps': 15360, 'rewardStep': 0.7070663551892369, 'errorList': [], 'lossList': [0.0, -1.2846068334579468, 0.0, 25.176254118680955, 0.0, 0.0, 0.0], 'rewardMean': 0.6168861410497707, 'totalEpisodes': 115, 'stepsPerEpisode': 315, 'rewardPerEpisode': 258.75770422439405
'totalSteps': 16640, 'rewardStep': 0.8899660555405967, 'errorList': [], 'lossList': [0.0, -1.2667218071222306, 0.0, 6.943268545269966, 0.0, 0.0, 0.0], 'rewardMean': 0.6316183424957716, 'totalEpisodes': 118, 'stepsPerEpisode': 57, 'rewardPerEpisode': 45.958766314824736
'totalSteps': 17920, 'rewardStep': 0.7239694519366391, 'errorList': [], 'lossList': [0.0, -1.2445183634757995, 0.0, 4.068800161778927, 0.0, 0.0, 0.0], 'rewardMean': 0.6744295333700375, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 900.3736216311015
'totalSteps': 19200, 'rewardStep': 0.8405536284699828, 'errorList': [], 'lossList': [0.0, -1.2079205417633057, 0.0, 4.016131924688816, 0.0, 0.0, 0.0], 'rewardMean': 0.7226719841378497, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1038.6908227795936
'totalSteps': 20480, 'rewardStep': 0.9176307773391896, 'errorList': [], 'lossList': [0.0, -1.1759602332115173, 0.0, 3.3268728087842465, 0.0, 0.0, 0.0], 'rewardMean': 0.7419247481248191, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.5324084515232
'totalSteps': 21760, 'rewardStep': 0.9092318203643096, 'errorList': [], 'lossList': [0.0, -1.1294680309295655, 0.0, 2.325268520154059, 0.0, 0.0, 0.0], 'rewardMean': 0.7589377772397308, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1167.2203143977192
'totalSteps': 23040, 'rewardStep': 0.9791946461329926, 'errorList': [0.13113637395845817, 0.13050454648046197, 0.14205947075773828, 0.13491657604118584, 0.2187454788798798, 0.18234123360969118, 0.1582795307784456, 0.12368357834014883, 0.19850492163978212, 0.18886232144326287, 0.12244972667524691, 0.20196696145751428, 0.12208682336143004, 0.3936224635972837, 0.137458952952452, 0.28315184534307913, 0.19605417828384816, 0.362922320611517, 0.14604943855727076, 0.14399881092800643, 0.12123117610125228, 0.12416061409879733, 0.2389677558518518, 0.1674992004522043, 0.29388063862020436, 0.1800131935484467, 0.17079992545779352, 0.28361424323026324, 0.13175829196811692, 0.2018439611413345, 0.1785614717342413, 0.3215378964694929, 0.31012148089874947, 0.14651011590858778, 0.1324687358711207, 0.1525259620149558, 0.15967522461662417, 0.14961524712560578, 0.15539748867814382, 0.13287586695711404, 0.11508545407408702, 0.12814620902276372, 0.11386062057456212, 0.15982633956089679, 0.1231955742400867, 0.17525698050522676, 0.15848109043408365, 0.12518739842631943, 0.11585277994090278, 0.11268449511844329], 'lossList': [0.0, -1.0901229518651963, 0.0, 1.9829036843404173, 0.0, 0.0, 0.0], 'rewardMean': 0.7730464981576601, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1212.9254389893456, 'successfulTests': 39
'totalSteps': 24320, 'rewardStep': 0.8815919361022523, 'errorList': [], 'lossList': [0.0, -1.0733959144353866, 0.0, 1.0978844796400518, 0.0, 0.0, 0.0], 'rewardMean': 0.7832315340758289, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.6452696166384
'totalSteps': 25600, 'rewardStep': 0.8373549566997995, 'errorList': [], 'lossList': [0.0, -1.067118642926216, 0.0, 0.6256041447259486, 0.0, 0.0, 0.0], 'rewardMean': 0.8115150210463152, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.0752658542008
#maxSuccessfulTests=39, maxSuccessfulTestsAtStep=23040, timeSpent=82.4
