#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 113
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.5920328862104888, 'errorList': [], 'lossList': [0.0, -1.4171130120754243, 0.0, 24.003107118606568, 0.0, 0.0, 0.0], 'rewardMean': 0.6786793595787723, 'totalEpisodes': 19, 'stepsPerEpisode': 42, 'rewardPerEpisode': 33.41037240190682
'totalSteps': 3840, 'rewardStep': 0.7692348679936594, 'errorList': [], 'lossList': [0.0, -1.3964508754014968, 0.0, 49.31238533020019, 0.0, 0.0, 0.0], 'rewardMean': 0.7088645290504013, 'totalEpisodes': 34, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.353879817614946
'totalSteps': 5120, 'rewardStep': 0.7553109009033466, 'errorList': [], 'lossList': [0.0, -1.3950349128246307, 0.0, 56.679410161972044, 0.0, 0.0, 0.0], 'rewardMean': 0.7204761220136376, 'totalEpisodes': 46, 'stepsPerEpisode': 74, 'rewardPerEpisode': 64.43558030564931
'totalSteps': 6400, 'rewardStep': 0.74099954779991, 'errorList': [], 'lossList': [0.0, -1.407100578546524, 0.0, 71.7130735206604, 0.0, 0.0, 0.0], 'rewardMean': 0.7245808071708921, 'totalEpisodes': 60, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.97994124727201
'totalSteps': 7680, 'rewardStep': 0.9671855754033104, 'errorList': [], 'lossList': [0.0, -1.4098088908195496, 0.0, 96.7087794303894, 0.0, 0.0, 0.0], 'rewardMean': 0.7650149352096284, 'totalEpisodes': 78, 'stepsPerEpisode': 52, 'rewardPerEpisode': 46.03902706897026
'totalSteps': 8960, 'rewardStep': 0.9786761728078149, 'errorList': [], 'lossList': [0.0, -1.3981436151266098, 0.0, 68.19608674049377, 0.0, 0.0, 0.0], 'rewardMean': 0.7955379691522265, 'totalEpisodes': 88, 'stepsPerEpisode': 72, 'rewardPerEpisode': 62.48535260413276
'totalSteps': 10240, 'rewardStep': 0.9443238184572298, 'errorList': [7.573129533449213, 14.568463640306422, 6.3603438935539005, 43.946662102392516, 23.525269271642376, 11.157751627894234, 21.778651315477326, 5.306042300733074, 1.4742786383020525, 13.055815793232922, 24.965770209430378, 9.647839107685462, 7.306643709943487, 11.580651564775561, 26.538984551643566, 6.603860067299703, 10.607501798338813, 30.44076112510144, 21.23062937504556, 20.994572632071936, 13.092783409249133, 7.05405620492142, 40.63119883981484, 36.4018556674525, 8.559910094053103, 55.500631513502164, 29.988931794375922, 3.8373331017838144, 52.13110091396714, 19.60860680108877, 16.37832759249073, 8.816128986607257, 7.8995722948489755, 51.016110364094544, 2.9296953686560046, 55.66213237603623, 6.736689871360981, 26.41862473825188, 9.213119944231593, 13.689924115319087, 46.542218099039744, 7.380327220082614, 24.465140046238798, 6.984664677839752, 41.696400555261384, 44.73520019361594, 40.97164246657454, 35.624477923586426, 13.714693761360992, 31.24919336402481], 'lossList': [0.0, -1.3831122851371764, 0.0, 88.36550802230835, 0.0, 0.0, 0.0], 'rewardMean': 0.8141362003153519, 'totalEpisodes': 103, 'stepsPerEpisode': 175, 'rewardPerEpisode': 148.679029976788, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9065506661355118, 'errorList': [], 'lossList': [0.0, -1.3940118646621704, 0.0, 31.898915777206422, 0.0, 0.0, 0.0], 'rewardMean': 0.8244044742953697, 'totalEpisodes': 112, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.930357149268209
'totalSteps': 12800, 'rewardStep': 0.668377285937243, 'errorList': [], 'lossList': [0.0, -1.398680915236473, 0.0, 40.684459719657895, 0.0, 0.0, 0.0], 'rewardMean': 0.808801755459557, 'totalEpisodes': 117, 'stepsPerEpisode': 529, 'rewardPerEpisode': 389.78688715663145
'totalSteps': 14080, 'rewardStep': 0.7198226875355266, 'errorList': [], 'lossList': [0.0, -1.3952640479803085, 0.0, 16.64709539294243, 0.0, 0.0, 0.0], 'rewardMean': 0.8042514409184042, 'totalEpisodes': 118, 'stepsPerEpisode': 874, 'rewardPerEpisode': 631.0773751409403
'totalSteps': 15360, 'rewardStep': 0.6813980340531578, 'errorList': [], 'lossList': [0.0, -1.391811637878418, 0.0, 6.886093686819077, 0.0, 0.0, 0.0], 'rewardMean': 0.8131879557026711, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 906.5840030543736
'totalSteps': 16640, 'rewardStep': 0.9179468127211571, 'errorList': [], 'lossList': [0.0, -1.3904899567365647, 0.0, 8.152290108352899, 0.0, 0.0, 0.0], 'rewardMean': 0.8280591501754208, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1095.275347671247
'totalSteps': 17920, 'rewardStep': 0.6562976554544443, 'errorList': [], 'lossList': [0.0, -1.3854668599367141, 0.0, 3.7463582461327314, 0.0, 0.0, 0.0], 'rewardMean': 0.8181578256305306, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1039.7664608819096
'totalSteps': 19200, 'rewardStep': 0.8316562436048409, 'errorList': [], 'lossList': [0.0, -1.3715446293354034, 0.0, 2.2697359009832145, 0.0, 0.0, 0.0], 'rewardMean': 0.8272234952110237, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 972.6981138090024
'totalSteps': 20480, 'rewardStep': 0.90712597430965, 'errorList': [], 'lossList': [0.0, -1.3361344534158706, 0.0, 1.9421760819107294, 0.0, 0.0, 0.0], 'rewardMean': 0.8212175351016576, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.6743927151629
'totalSteps': 21760, 'rewardStep': 0.9009232521578048, 'errorList': [], 'lossList': [0.0, -1.2982658064365387, 0.0, 1.180174569785595, 0.0, 0.0, 0.0], 'rewardMean': 0.8134422430366566, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.8040227170804
'totalSteps': 23040, 'rewardStep': 0.8379488658897021, 'errorList': [], 'lossList': [0.0, -1.268275545835495, 0.0, 1.4633297658152877, 0.0, 0.0, 0.0], 'rewardMean': 0.8028047477799038, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.8347526606133
'totalSteps': 24320, 'rewardStep': 0.8943701511210791, 'errorList': [], 'lossList': [0.0, -1.2180277901887893, 0.0, 1.0405082526057958, 0.0, 0.0, 0.0], 'rewardMean': 0.8015866962784605, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.3806310145624
'totalSteps': 25600, 'rewardStep': 0.9557858227093818, 'errorList': [0.08911368483539078, 0.11684340017604632, 0.07001192267514005, 0.07151757424267201, 0.08276784632341123, 0.12148859960152482, 0.08471200909518449, 0.07359073021907138, 0.06742475265207486, 0.08102013435143236, 0.09866975433215855, 0.10094242790234464, 0.11889639344303721, 0.1306100855443689, 0.10232230184643826, 0.08626342102476801, 0.07144271771850753, 0.08826447078137549, 0.12668054563702202, 0.10788124463967617, 0.11432920346618818, 0.0904675094702254, 0.09876514393536584, 0.06843116053833202, 0.12372392036190041, 0.10704237017415347, 0.1010117214035217, 0.11852272891139176, 0.06602869818694845, 0.08407200697337819, 0.07132615902003062, 0.1128640388657906, 0.09221882706990948, 0.06789500815870564, 0.07413014697287619, 0.08658023080189568, 0.1277233309797258, 0.06674453463117204, 0.09790293843750378, 0.07348329797705072, 0.09790021536423914, 0.09024393335157509, 0.10595316275826189, 0.13498720929488336, 0.11691525361503213, 0.09220344699997148, 0.0899379283751789, 0.06880114792649399, 0.09822263319332755, 0.11437307566730752], 'lossList': [0.0, -1.1882681638002395, 0.0, 0.7711939804628491, 0.0, 0.0, 0.0], 'rewardMean': 0.8303275499556746, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.6677281035413, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=106.06
