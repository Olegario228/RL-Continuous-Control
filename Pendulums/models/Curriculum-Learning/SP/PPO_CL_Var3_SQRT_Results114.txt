#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 114
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8121091954916673, 'errorList': [], 'lossList': [0.0, -1.4160323160886765, 0.0, 27.589103915691375, 0.0, 0.0, 0.0], 'rewardMean': 0.7420123008175052, 'totalEpisodes': 21, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.02237665094989
'totalSteps': 3840, 'rewardStep': 0.7324555522373175, 'errorList': [], 'lossList': [0.0, -1.4131444251537324, 0.0, 44.43071759223938, 0.0, 0.0, 0.0], 'rewardMean': 0.7388267179574427, 'totalEpisodes': 39, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.259973098146325
'totalSteps': 5120, 'rewardStep': 0.8176176895599117, 'errorList': [], 'lossList': [0.0, -1.415564045906067, 0.0, 33.07168099403381, 0.0, 0.0, 0.0], 'rewardMean': 0.7585244608580599, 'totalEpisodes': 52, 'stepsPerEpisode': 43, 'rewardPerEpisode': 36.86440574503343
'totalSteps': 6400, 'rewardStep': 0.6396157888762246, 'errorList': [], 'lossList': [0.0, -1.4093403595685958, 0.0, 32.23470145702362, 0.0, 0.0, 0.0], 'rewardMean': 0.7347427264616929, 'totalEpisodes': 59, 'stepsPerEpisode': 159, 'rewardPerEpisode': 128.69549664081873
'totalSteps': 7680, 'rewardStep': 0.7255996663721164, 'errorList': [], 'lossList': [0.0, -1.3967427128553391, 0.0, 51.38364039897919, 0.0, 0.0, 0.0], 'rewardMean': 0.7332188831134302, 'totalEpisodes': 68, 'stepsPerEpisode': 31, 'rewardPerEpisode': 19.06929244670141
'totalSteps': 8960, 'rewardStep': 0.846611875534506, 'errorList': [], 'lossList': [0.0, -1.394591429233551, 0.0, 130.2348902130127, 0.0, 0.0, 0.0], 'rewardMean': 0.7494178820307267, 'totalEpisodes': 87, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.264437517362893
'totalSteps': 10240, 'rewardStep': 0.8722242622734018, 'errorList': [], 'lossList': [0.0, -1.3889319562911988, 0.0, 119.39980922698975, 0.0, 0.0, 0.0], 'rewardMean': 0.7647686795610611, 'totalEpisodes': 113, 'stepsPerEpisode': 144, 'rewardPerEpisode': 117.14785099250385
'totalSteps': 11520, 'rewardStep': 0.9855368764670844, 'errorList': [88.985211253661, 182.61116715835044, 129.28199125239456, 158.85583227767634, 230.60977845261812, 166.77879352001906, 238.6041956217143, 148.94959766215706, 218.81076780398922, 101.18821721982886, 186.01474971106884, 69.79364665749237, 184.9586584302603, 199.03667691674093, 208.91963784968848, 90.46728747910313, 143.82839680211492, 212.87378985075628, 223.03835246429117, 38.25787098893505, 213.04813514448801, 2.19426656361852, 201.47730696334474, 140.49446475951063, 155.39850322147498, 154.71360263995888, 34.289103245511825, 154.96060630801043, 74.11441712424848, 152.33636017407096, 84.3048013887238, 141.49918499608165, 125.9429489445983, 50.579484818093185, 186.57577579234695, 86.28579471682632, 201.48386019557444, 159.5853146452601, 13.020775070867714, 86.80683491219453, 67.18710476515233, 86.69215027064975, 180.01356586693677, 108.60848494494836, 72.42812986643924, 94.00394303075338, 150.4369062333839, 125.42739278762416, 133.66490142033916, 151.91576367621886], 'lossList': [0.0, -1.3818241155147553, 0.0, 39.597365856170654, 0.0, 0.0, 0.0], 'rewardMean': 0.7892984792172859, 'totalEpisodes': 131, 'stepsPerEpisode': 25, 'rewardPerEpisode': 22.582132051624725, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.7487758767977695, 'errorList': [], 'lossList': [0.0, -1.3777679270505905, 0.0, 19.62851948738098, 0.0, 0.0, 0.0], 'rewardMean': 0.7852462189753343, 'totalEpisodes': 140, 'stepsPerEpisode': 64, 'rewardPerEpisode': 52.61926604205106
'totalSteps': 14080, 'rewardStep': 0.18475369134169606, 'errorList': [], 'lossList': [0.0, -1.3594573509693146, 0.0, 13.8094322514534, 0.0, 0.0, 0.0], 'rewardMean': 0.7365300474951695, 'totalEpisodes': 145, 'stepsPerEpisode': 276, 'rewardPerEpisode': 199.0745134576304
'totalSteps': 15360, 'rewardStep': 0.24043946990876336, 'errorList': [], 'lossList': [0.0, -1.352740821838379, 0.0, 22.267267303466795, 0.0, 0.0, 0.0], 'rewardMean': 0.6793630749368792, 'totalEpisodes': 152, 'stepsPerEpisode': 202, 'rewardPerEpisode': 152.7182762573503
'totalSteps': 16640, 'rewardStep': 0.2771683282951646, 'errorList': [], 'lossList': [0.0, -1.344069800376892, 0.0, 9.104360725879669, 0.0, 0.0, 0.0], 'rewardMean': 0.633834352542664, 'totalEpisodes': 157, 'stepsPerEpisode': 188, 'rewardPerEpisode': 137.18334586534817
'totalSteps': 17920, 'rewardStep': 0.5732068941072557, 'errorList': [], 'lossList': [0.0, -1.3153614783287049, 0.0, 7.18789116024971, 0.0, 0.0, 0.0], 'rewardMean': 0.6093932729973983, 'totalEpisodes': 163, 'stepsPerEpisode': 190, 'rewardPerEpisode': 143.6420100517946
'totalSteps': 19200, 'rewardStep': 0.5508517855810627, 'errorList': [], 'lossList': [0.0, -1.2972674375772477, 0.0, 6.255284692049027, 0.0, 0.0, 0.0], 'rewardMean': 0.600516872667882, 'totalEpisodes': 166, 'stepsPerEpisode': 349, 'rewardPerEpisode': 242.07553785116082
'totalSteps': 20480, 'rewardStep': 0.9047301912111072, 'errorList': [], 'lossList': [0.0, -1.2769231384992599, 0.0, 6.351487865447998, 0.0, 0.0, 0.0], 'rewardMean': 0.6184299251517812, 'totalEpisodes': 171, 'stepsPerEpisode': 184, 'rewardPerEpisode': 160.14964800229905
'totalSteps': 21760, 'rewardStep': 0.6448607386109444, 'errorList': [], 'lossList': [0.0, -1.2624716722965241, 0.0, 4.936525896787644, 0.0, 0.0, 0.0], 'rewardMean': 0.598254811459425, 'totalEpisodes': 173, 'stepsPerEpisode': 245, 'rewardPerEpisode': 210.28741067720995
'totalSteps': 23040, 'rewardStep': 0.8782922849634672, 'errorList': [], 'lossList': [0.0, -1.2577770018577576, 0.0, 5.666210590600968, 0.0, 0.0, 0.0], 'rewardMean': 0.5988616137284315, 'totalEpisodes': 177, 'stepsPerEpisode': 209, 'rewardPerEpisode': 180.99817189787476
'totalSteps': 24320, 'rewardStep': 0.8526585766298446, 'errorList': [], 'lossList': [0.0, -1.2458112967014312, 0.0, 1.301226396560669, 0.0, 0.0, 0.0], 'rewardMean': 0.5855737837447075, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.4942117900437
'totalSteps': 25600, 'rewardStep': 0.9494733724344224, 'errorList': [0.029643613493980727, 0.007184132934867538, 0.010330354003142202, 0.009921052145064474, 0.007896517818152912, 0.008068012496906425, 0.0071364718811769805, 0.010368676080351647, 0.015799612214343402, 0.008694502452187572, 0.008018303918232397, 0.022808117055059716, 0.02840323106909781, 0.010284294346294003, 0.022817135452994942, 0.037638507984055915, 0.02129388227522365, 0.0345654468540235, 0.01901284560704952, 0.020356832608528934, 0.028609741800897256, 0.01710035406320781, 0.024882121820641794, 0.02846108605674659, 0.014032892815343952, 0.00857197135171451, 0.018168966401361987, 0.03727555834133337, 0.008574905996279747, 0.009280873504655536, 0.033676411084878205, 0.013822244065431621, 0.04463915849831234, 0.04035134323659081, 0.030373938086688503, 0.022799200715552855, 0.013897012866551569, 0.03135268364850788, 0.02231135607003565, 0.013625922226535029, 0.03300891377208098, 0.01570416799084073, 0.02155635913704273, 0.009092524645985187, 0.02163645018237642, 0.03855537762772415, 0.020756702917856598, 0.023934063779091724, 0.03825103584177686, 0.047537634807451136], 'lossList': [0.0, -1.201256667971611, 0.0, 1.070989431962371, 0.0, 0.0, 0.0], 'rewardMean': 0.6056435333083727, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.7263308839154, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.73
