#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 116
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.5678965895509172, 'errorList': [], 'lossList': [0.0, -1.4297752892971038, 0.0, 31.199902379512785, 0.0, 0.0, 0.0], 'rewardMean': 0.5632721540136361, 'totalEpisodes': 14, 'stepsPerEpisode': 101, 'rewardPerEpisode': 73.27283722780324
'totalSteps': 3840, 'rewardStep': 0.7739639179060966, 'errorList': [], 'lossList': [0.0, -1.4229376649856567, 0.0, 25.715734654664992, 0.0, 0.0, 0.0], 'rewardMean': 0.6335027419777896, 'totalEpisodes': 18, 'stepsPerEpisode': 545, 'rewardPerEpisode': 370.8672095587754
'totalSteps': 5120, 'rewardStep': 0.9673920947602584, 'errorList': [], 'lossList': [0.0, -1.4131519758701325, 0.0, 32.948062410354616, 0.0, 0.0, 0.0], 'rewardMean': 0.7169750801734068, 'totalEpisodes': 23, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.113466866936491
'totalSteps': 6400, 'rewardStep': 0.45725432583010023, 'errorList': [], 'lossList': [0.0, -1.3968835359811782, 0.0, 50.94289891242981, 0.0, 0.0, 0.0], 'rewardMean': 0.6650309293047455, 'totalEpisodes': 28, 'stepsPerEpisode': 82, 'rewardPerEpisode': 63.29030927599134
'totalSteps': 7680, 'rewardStep': 0.870466128887095, 'errorList': [], 'lossList': [0.0, -1.375069701075554, 0.0, 22.873384355902672, 0.0, 0.0, 0.0], 'rewardMean': 0.6992701292351371, 'totalEpisodes': 29, 'stepsPerEpisode': 980, 'rewardPerEpisode': 790.5431735803313
'totalSteps': 8960, 'rewardStep': 0.6173993691506438, 'errorList': [], 'lossList': [0.0, -1.3532853144407273, 0.0, 103.20207093238831, 0.0, 0.0, 0.0], 'rewardMean': 0.6875743063659238, 'totalEpisodes': 38, 'stepsPerEpisode': 108, 'rewardPerEpisode': 83.00364872743252
'totalSteps': 10240, 'rewardStep': 0.8693546377540384, 'errorList': [], 'lossList': [0.0, -1.3434972310066222, 0.0, 211.5362985610962, 0.0, 0.0, 0.0], 'rewardMean': 0.710296847789438, 'totalEpisodes': 73, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.842067514991403
'totalSteps': 11520, 'rewardStep': 0.970283438195636, 'errorList': [7.378035569087466, 6.10329733929266, 6.094866507608081, 10.249561341661483, 7.106590837988611, 9.01799738389956, 10.348576650397804, 10.356686054825706, 10.340005974294886, 7.650228080063114, 10.405898711201647, 10.288424215034677, 10.334701176560776, 10.309348457416881, 7.456400842678163, 7.050477712965699, 10.391511251956267, 6.867798449863358, 7.72872855026208, 6.956612081018305, 5.697646361432454, 7.948670836509231, 6.646311881597007, 9.602434658847995, 10.512996320116981, 7.5942960058276014, 10.35299102802829, 9.231620319927648, 6.412274193976213, 7.803699714149582, 10.441084921678307, 10.150366594948323, 7.092820990934336, 10.310346031283766, 10.357928276159685, 8.452838860030296, 9.643085054003084, 10.544150634610167, 8.546695534855115, 7.745781177740606, 10.274536561049675, 7.595418312776151, 5.391975529407213, 7.4467086598822405, 10.420596727132496, 10.459453536457412, 10.353720579513947, 10.264988615839815, 9.526914441786127, 8.405128575162918], 'lossList': [0.0, -1.3403617721796035, 0.0, 80.43117504119873, 0.0, 0.0, 0.0], 'rewardMean': 0.7391842467234601, 'totalEpisodes': 98, 'stepsPerEpisode': 52, 'rewardPerEpisode': 42.415309638776456, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.6832938465019726, 'errorList': [], 'lossList': [0.0, -1.339898996949196, 0.0, 36.903984203338624, 0.0, 0.0, 0.0], 'rewardMean': 0.7335952067013113, 'totalEpisodes': 107, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.176778952696163
'totalSteps': 14080, 'rewardStep': 0.6134108244650385, 'errorList': [], 'lossList': [0.0, -1.3262582850456237, 0.0, 10.969491627216339, 0.0, 0.0, 0.0], 'rewardMean': 0.7390715173001797, 'totalEpisodes': 108, 'stepsPerEpisode': 988, 'rewardPerEpisode': 751.0757467212848
'totalSteps': 15360, 'rewardStep': 0.6083931094832933, 'errorList': [], 'lossList': [0.0, -1.297970580458641, 0.0, 8.507257622480392, 0.0, 0.0, 0.0], 'rewardMean': 0.7431211692934172, 'totalEpisodes': 112, 'stepsPerEpisode': 471, 'rewardPerEpisode': 303.4626037694772
'totalSteps': 16640, 'rewardStep': 0.5259729762440385, 'errorList': [], 'lossList': [0.0, -1.283908889889717, 0.0, 6.920794157981873, 0.0, 0.0, 0.0], 'rewardMean': 0.7183220751272115, 'totalEpisodes': 117, 'stepsPerEpisode': 165, 'rewardPerEpisode': 115.35393878830659
'totalSteps': 17920, 'rewardStep': 0.9623298233419568, 'errorList': [0.770693476651509, 0.6290531596076626, 0.6394814026424871, 1.0861706495448453, 0.9529767812652634, 0.6319812660751318, 0.5129185678998552, 0.7494694853290363, 0.9337838236250456, 0.642699138343192, 0.7071117193489269, 0.8206253074144394, 0.5927641625572556, 1.0679048256520562, 0.8701699518880368, 0.8046405212883949, 0.5365456067959584, 0.8408096103862884, 1.0197435285449783, 0.850202742817259, 0.907346168605155, 1.028182572072588, 0.6427620696743769, 0.6729385101994901, 1.000718983419077, 0.5596941117931865, 0.7464067437234891, 1.0396562187781595, 1.0062153249311747, 0.690674078208858, 0.9227640574124424, 0.8820974356092275, 0.5868917033514605, 1.0782012721941356, 1.0882038928924742, 1.152306963549003, 0.7634330157364172, 0.5975048553255252, 0.8451941176008536, 0.664987644273697, 0.8125411798314087, 0.7189778730241488, 0.8554498562935848, 0.8921037169052245, 0.8025207110453486, 0.9774317222502323, 0.79476674768221, 0.9788538486344278, 0.78851922442478, 0.7805478076051662], 'lossList': [0.0, -1.2756619226932526, 0.0, 5.156021659970284, 0.0, 0.0, 0.0], 'rewardMean': 0.7178158479853813, 'totalEpisodes': 118, 'stepsPerEpisode': 1192, 'rewardPerEpisode': 881.9834120463587, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.6078676816728337, 'errorList': [], 'lossList': [0.0, -1.2707312297821045, 0.0, 18.34331394672394, 0.0, 0.0, 0.0], 'rewardMean': 0.7328771835696546, 'totalEpisodes': 120, 'stepsPerEpisode': 669, 'rewardPerEpisode': 452.8570270165716
'totalSteps': 20480, 'rewardStep': 0.6475344250239773, 'errorList': [], 'lossList': [0.0, -1.2553541481494903, 0.0, 3.777527751326561, 0.0, 0.0, 0.0], 'rewardMean': 0.7105840131833429, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 916.6553415746259
'totalSteps': 21760, 'rewardStep': 0.8780246687363223, 'errorList': [], 'lossList': [0.0, -1.2245583701133729, 0.0, 2.932493995279074, 0.0, 0.0, 0.0], 'rewardMean': 0.7366465431419107, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.0017528012409
'totalSteps': 23040, 'rewardStep': 0.9375243290338061, 'errorList': [0.23565147434625963, 0.23047553650362745, 0.2596865798420352, 0.25257669264161475, 0.25337415930806734, 0.2689992456728446, 0.21239000563162228, 0.2555906822927943, 0.24320937198654663, 0.26408927349828143, 0.26097341376692434, 0.31799055626421685, 0.24050724265602588, 0.29839380420275435, 0.3100072836876856, 0.212111443660976, 0.24859063827853592, 0.21989681260732077, 0.23071397327177792, 0.2777695833498373, 0.3414180109061531, 0.25628240595629725, 0.4035090319813246, 0.27482633106702475, 0.2585159460616073, 0.26570756373213766, 0.2682797223080673, 0.21580889113866664, 0.28625563653846614, 0.2760584884473927, 0.28281551857310483, 0.28219224086482014, 0.2522252691229769, 0.3485217028393725, 0.27872386522121534, 0.2854390794349092, 0.23987097420563488, 0.23613064877472326, 0.3317041255100527, 0.2890018724720409, 0.21595499706946242, 0.2529662086263554, 0.3104794103534406, 0.2805996921550016, 0.2549029896263117, 0.23494816809785188, 0.2549913443701301, 0.22848714299898928, 0.2317889766724756, 0.258820162148569], 'lossList': [0.0, -1.1954846769571303, 0.0, 0.961401581838727, 0.0, 0.0, 0.0], 'rewardMean': 0.7434635122698875, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.4210720601332, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.8407899467515942, 'errorList': [], 'lossList': [0.0, -1.1604442977905274, 0.0, 0.8528390654549003, 0.0, 0.0, 0.0], 'rewardMean': 0.7305141631254833, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1086.1540935449614
'totalSteps': 25600, 'rewardStep': 0.9648135124634928, 'errorList': [0.20183661807672099, 0.2980659327939616, 0.3251312205384169, 0.41392775883946337, 0.1541918656158335, 0.2189197879859868, 0.19659255814325804, 0.16149331338500736, 0.08584768094208807, 0.18337950811215958, 0.0777963603998901, 0.15556133614300785, 0.13655365399925787, 0.13505291243936776, 0.27393037039787804, 0.1602257103555112, 0.12260687938879167, 0.25017854935360684, 0.22463050087232775, 0.20879830627424723, 0.25038417293708376, 0.13760773704905654, 0.16417157246140412, 0.2547231433914379, 0.11239930395374094, 0.16998343546019615, 0.36037125730379704, 0.26156081094844436, 0.18393022121476746, 0.1783779479319875, 0.09168290987448618, 0.12851196528358066, 0.2835132027616846, 0.3291351361738438, 0.11245555224474497, 0.17530073859916348, 0.15939948425233208, 0.22352694823155325, 0.110986991811907, 0.33382668157504025, 0.22587863116484502, 0.13177256923197866, 0.21107206543932175, 0.15213163999358068, 0.06855422509208903, 0.08964078104433537, 0.11319878315135062, 0.21762161523291423, 0.14121390838525394, 0.220522654835574], 'lossList': [0.0, -1.1069294899702071, 0.0, 1.4392675255611538, 0.0, 0.0, 0.0], 'rewardMean': 0.7586661297216354, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1203.2110027797594, 'successfulTests': 29
#maxSuccessfulTests=29, maxSuccessfulTestsAtStep=25600, timeSpent=148.5
