#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 117
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.6281152100927028, 'errorList': [], 'lossList': [0.0, -1.441023389697075, 0.0, 29.19389099121094, 0.0, 0.0, 0.0], 'rewardMean': 0.5529504926577504, 'totalEpisodes': 16, 'stepsPerEpisode': 290, 'rewardPerEpisode': 208.69112813786515
'totalSteps': 3840, 'rewardStep': 0.7931913804986837, 'errorList': [], 'lossList': [0.0, -1.4322714281082154, 0.0, 30.17498173236847, 0.0, 0.0, 0.0], 'rewardMean': 0.6330307886047282, 'totalEpisodes': 23, 'stepsPerEpisode': 78, 'rewardPerEpisode': 55.93845293931301
'totalSteps': 5120, 'rewardStep': 0.2552793531204331, 'errorList': [], 'lossList': [0.0, -1.4066946864128114, 0.0, 32.54185935974121, 0.0, 0.0, 0.0], 'rewardMean': 0.5385929297336544, 'totalEpisodes': 32, 'stepsPerEpisode': 66, 'rewardPerEpisode': 36.36181973895222
'totalSteps': 6400, 'rewardStep': 0.8027627553041181, 'errorList': [], 'lossList': [0.0, -1.3873298752307892, 0.0, 62.66731023788452, 0.0, 0.0, 0.0], 'rewardMean': 0.591426894847747, 'totalEpisodes': 41, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.5722913627523638
'totalSteps': 7680, 'rewardStep': 0.9444609228387839, 'errorList': [], 'lossList': [0.0, -1.3828923082351685, 0.0, 72.29459668159485, 0.0, 0.0, 0.0], 'rewardMean': 0.6502658995129199, 'totalEpisodes': 51, 'stepsPerEpisode': 55, 'rewardPerEpisode': 43.66631682633974
'totalSteps': 8960, 'rewardStep': 0.614774080529339, 'errorList': [], 'lossList': [0.0, -1.3731601732969283, 0.0, 65.13933767318726, 0.0, 0.0, 0.0], 'rewardMean': 0.6451956396581225, 'totalEpisodes': 58, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.678809639095679
'totalSteps': 10240, 'rewardStep': 0.8482622042105925, 'errorList': [], 'lossList': [0.0, -1.3578890681266784, 0.0, 105.40315614700317, 0.0, 0.0, 0.0], 'rewardMean': 0.6705789602271814, 'totalEpisodes': 81, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.545143698362484
'totalSteps': 11520, 'rewardStep': 0.3115838478341519, 'errorList': [], 'lossList': [0.0, -1.354063744544983, 0.0, 43.290074806213376, 0.0, 0.0, 0.0], 'rewardMean': 0.6306906144057337, 'totalEpisodes': 97, 'stepsPerEpisode': 90, 'rewardPerEpisode': 58.31873788693461
'totalSteps': 12800, 'rewardStep': 0.674221272796532, 'errorList': [], 'lossList': [0.0, -1.3546728259325027, 0.0, 19.793250007629396, 0.0, 0.0, 0.0], 'rewardMean': 0.6350436802448136, 'totalEpisodes': 108, 'stepsPerEpisode': 107, 'rewardPerEpisode': 82.23388543792541
'totalSteps': 14080, 'rewardStep': 0.8183181808479686, 'errorList': [], 'lossList': [0.0, -1.3507200318574906, 0.0, 12.898544527292252, 0.0, 0.0, 0.0], 'rewardMean': 0.6690969208073305, 'totalEpisodes': 116, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.51196013731704
'totalSteps': 15360, 'rewardStep': 0.6382857510207143, 'errorList': [], 'lossList': [0.0, -1.3596174478530885, 0.0, 9.06306414604187, 0.0, 0.0, 0.0], 'rewardMean': 0.6701139749001317, 'totalEpisodes': 120, 'stepsPerEpisode': 80, 'rewardPerEpisode': 53.86628865800089
'totalSteps': 16640, 'rewardStep': 0.5006444111532329, 'errorList': [], 'lossList': [0.0, -1.3555441415309906, 0.0, 6.323227264881134, 0.0, 0.0, 0.0], 'rewardMean': 0.6408592779655866, 'totalEpisodes': 124, 'stepsPerEpisode': 272, 'rewardPerEpisode': 211.32050045861047
'totalSteps': 17920, 'rewardStep': 0.8105155099975196, 'errorList': [], 'lossList': [0.0, -1.3247573584318162, 0.0, 6.754003632068634, 0.0, 0.0, 0.0], 'rewardMean': 0.6963828936532953, 'totalEpisodes': 126, 'stepsPerEpisode': 260, 'rewardPerEpisode': 229.2889676239916
'totalSteps': 19200, 'rewardStep': 0.7081701518767514, 'errorList': [], 'lossList': [0.0, -1.2942399537563325, 0.0, 4.60555566072464, 0.0, 0.0, 0.0], 'rewardMean': 0.6869236333105586, 'totalEpisodes': 131, 'stepsPerEpisode': 196, 'rewardPerEpisode': 158.32599741573864
'totalSteps': 20480, 'rewardStep': 0.6576077654262402, 'errorList': [], 'lossList': [0.0, -1.2927217394113542, 0.0, 3.2909189203381537, 0.0, 0.0, 0.0], 'rewardMean': 0.6582383175693042, 'totalEpisodes': 134, 'stepsPerEpisode': 234, 'rewardPerEpisode': 187.10273162426597
'totalSteps': 21760, 'rewardStep': 0.7332832740963683, 'errorList': [], 'lossList': [0.0, -1.2846872127056121, 0.0, 2.34164342969656, 0.0, 0.0, 0.0], 'rewardMean': 0.6700892369260071, 'totalEpisodes': 135, 'stepsPerEpisode': 1094, 'rewardPerEpisode': 932.699267254864
'totalSteps': 23040, 'rewardStep': 0.7404010037672237, 'errorList': [], 'lossList': [0.0, -1.2590594971179963, 0.0, 2.40358316719532, 0.0, 0.0, 0.0], 'rewardMean': 0.6593031168816703, 'totalEpisodes': 136, 'stepsPerEpisode': 630, 'rewardPerEpisode': 516.4403178904926
'totalSteps': 24320, 'rewardStep': 0.9607917336000498, 'errorList': [0.22551966591373063, 0.23673790111642554, 0.22542534088630053, 0.22104506927131978, 0.27254137298912584, 0.24483559694482387, 0.20931109952309426, 0.20562747230658715, 0.2539583859036319, 0.2048388907766261, 0.23599869366966386, 0.19799144143592604, 0.24458675612365102, 0.19765780216636947, 0.22237335335593014, 0.21829446072491956, 0.2143854779234544, 0.2408169220219127, 0.24318643966565365, 0.22117179576894944, 0.2305188395645677, 0.2134040217522952, 0.20276634054110426, 0.2312101757498085, 0.21989019428120246, 0.22944546407306823, 0.21581389806914428, 0.23515460503191465, 0.22720508156493657, 0.22452782603337326, 0.24846576734404668, 0.24228061500204068, 0.2212614751961107, 0.2482156490810112, 0.1937571166928774, 0.22405934115207715, 0.21862339079302576, 0.199802228804447, 0.22819954610298362, 0.21901484024839524, 0.2089719668627966, 0.2330521449377856, 0.19589739614017904, 0.2384753890612177, 0.20775550743901366, 0.2326983784099355, 0.2268989535575937, 0.25367886397400474, 0.20120542199009803, 0.24758106723219442], 'lossList': [0.0, -1.2285645806789398, 0.0, 1.8963686683773995, 0.0, 0.0, 0.0], 'rewardMean': 0.72422390545826, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.1930974772442, 'successfulTests': 5
'totalSteps': 25600, 'rewardStep': 0.8860804751345861, 'errorList': [], 'lossList': [0.0, -1.196057710647583, 0.0, 0.6157812527567148, 0.0, 0.0, 0.0], 'rewardMean': 0.7454098256920656, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.5196396374142
#maxSuccessfulTests=5, maxSuccessfulTestsAtStep=24320, timeSpent=85.55
