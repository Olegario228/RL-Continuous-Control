#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 118
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.6013293409193701, 'errorList': [], 'lossList': [0.0, -1.4237133783102036, 0.0, 23.6127030479908, 0.0, 0.0, 0.0], 'rewardMean': 0.7297707247374549, 'totalEpisodes': 17, 'stepsPerEpisode': 41, 'rewardPerEpisode': 28.758605483505505
'totalSteps': 3840, 'rewardStep': 0.906543472442008, 'errorList': [], 'lossList': [0.0, -1.4124500435590743, 0.0, 38.79874831199646, 0.0, 0.0, 0.0], 'rewardMean': 0.7886949739723059, 'totalEpisodes': 24, 'stepsPerEpisode': 378, 'rewardPerEpisode': 254.34189180172788
'totalSteps': 5120, 'rewardStep': 0.8413939119397418, 'errorList': [], 'lossList': [0.0, -1.4034733331203462, 0.0, 42.04787947177887, 0.0, 0.0, 0.0], 'rewardMean': 0.8018697084641648, 'totalEpisodes': 29, 'stepsPerEpisode': 76, 'rewardPerEpisode': 64.2447323474843
'totalSteps': 6400, 'rewardStep': 0.803959007206716, 'errorList': [], 'lossList': [0.0, -1.3917026549577713, 0.0, 47.07695914745331, 0.0, 0.0, 0.0], 'rewardMean': 0.8022875682126751, 'totalEpisodes': 35, 'stepsPerEpisode': 26, 'rewardPerEpisode': 19.4703691108079
'totalSteps': 7680, 'rewardStep': 0.871115731422909, 'errorList': [], 'lossList': [0.0, -1.3800292015075684, 0.0, 62.58167809486389, 0.0, 0.0, 0.0], 'rewardMean': 0.8137589287477139, 'totalEpisodes': 43, 'stepsPerEpisode': 53, 'rewardPerEpisode': 43.06277661452754
'totalSteps': 8960, 'rewardStep': 0.7447588815521206, 'errorList': [], 'lossList': [0.0, -1.3524769657850266, 0.0, 92.92408482551575, 0.0, 0.0, 0.0], 'rewardMean': 0.8039017791483435, 'totalEpisodes': 54, 'stepsPerEpisode': 71, 'rewardPerEpisode': 53.71169834615604
'totalSteps': 10240, 'rewardStep': 0.6914735848585052, 'errorList': [], 'lossList': [0.0, -1.3381400060653688, 0.0, 149.7964878845215, 0.0, 0.0, 0.0], 'rewardMean': 0.7898482548621137, 'totalEpisodes': 82, 'stepsPerEpisode': 35, 'rewardPerEpisode': 30.916137824038223
'totalSteps': 11520, 'rewardStep': 0.8018965933267745, 'errorList': [], 'lossList': [0.0, -1.3508851444721222, 0.0, 67.11902545928955, 0.0, 0.0, 0.0], 'rewardMean': 0.7911869591359649, 'totalEpisodes': 103, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.049502498739873
'totalSteps': 12800, 'rewardStep': 0.5676435670913297, 'errorList': [], 'lossList': [0.0, -1.3546082973480225, 0.0, 32.964904961586, 0.0, 0.0, 0.0], 'rewardMean': 0.7688326199315014, 'totalEpisodes': 115, 'stepsPerEpisode': 188, 'rewardPerEpisode': 150.92112496697536
'totalSteps': 14080, 'rewardStep': 0.7442000610960554, 'errorList': [], 'lossList': [0.0, -1.3412934064865112, 0.0, 26.378941712379454, 0.0, 0.0, 0.0], 'rewardMean': 0.757431415185553, 'totalEpisodes': 122, 'stepsPerEpisode': 236, 'rewardPerEpisode': 171.5848596030548
'totalSteps': 15360, 'rewardStep': 0.43386644850753475, 'errorList': [], 'lossList': [0.0, -1.3437454319000244, 0.0, 58.27648369789124, 0.0, 0.0, 0.0], 'rewardMean': 0.7406851259443694, 'totalEpisodes': 131, 'stepsPerEpisode': 212, 'rewardPerEpisode': 174.23948273396354
'totalSteps': 16640, 'rewardStep': 0.706818020194804, 'errorList': [], 'lossList': [0.0, -1.3510804015398026, 0.0, 7.608095449209213, 0.0, 0.0, 0.0], 'rewardMean': 0.720712580719649, 'totalEpisodes': 137, 'stepsPerEpisode': 59, 'rewardPerEpisode': 43.94072136612625
'totalSteps': 17920, 'rewardStep': 0.9652105881830765, 'errorList': [0.6689153836962787, 0.7574940324143683, 0.7956144663103129, 0.7802223392557008, 0.8221324468634185, 0.6951167112243773, 0.7681235275955518, 0.8055272348327489, 0.7523097529807572, 0.7419406023835124, 0.716492230489572, 0.7415990689070867, 0.763792883369311, 0.6988929927377662, 0.7505125705813571, 0.7713555754474264, 0.7457532885181397, 0.7210463760261147, 0.681371159877107, 0.7165774101076663, 0.8116070799582478, 0.8350538688636632, 0.806534503124735, 0.7505609787213803, 0.7196634295745936, 0.7243502997515265, 0.6845303945182025, 0.7545002324122656, 0.7767660828151303, 0.7843316724906559, 0.7894623975276237, 0.7024463146053732, 0.7262233910720034, 0.7858711363992915, 0.7491844309277115, 0.7933110906619903, 0.6830706347869219, 0.751719519526867, 0.8291437903206152, 0.9275433757388079, 0.711322679405253, 0.7930253852325034, 0.7171476359955928, 0.7416741901235434, 0.6964667084028991, 0.7529356820410494, 0.6709853863874449, 0.7346542254302177, 0.7270880393397264, 0.7828136652122032], 'lossList': [0.0, -1.3370630037784577, 0.0, 5.261616607308388, 0.0, 0.0, 0.0], 'rewardMean': 0.7330942483439825, 'totalEpisodes': 139, 'stepsPerEpisode': 35, 'rewardPerEpisode': 30.935341533749185, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.8463580474013626, 'errorList': [], 'lossList': [0.0, -1.3273016273975373, 0.0, 5.401192283034325, 0.0, 0.0, 0.0], 'rewardMean': 0.7373341523634471, 'totalEpisodes': 141, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.165750662624365
'totalSteps': 20480, 'rewardStep': 0.8336727391208856, 'errorList': [], 'lossList': [0.0, -1.2963845431804657, 0.0, 2.964796778559685, 0.0, 0.0, 0.0], 'rewardMean': 0.7335898531332449, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.0264840589698
'totalSteps': 21760, 'rewardStep': 0.8356809379365281, 'errorList': [], 'lossList': [0.0, -1.2617348617315292, 0.0, 1.6808400737494231, 0.0, 0.0, 0.0], 'rewardMean': 0.7426820587716857, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 978.1841969244321
'totalSteps': 23040, 'rewardStep': 0.6866183463555388, 'errorList': [], 'lossList': [0.0, -1.2519643503427504, 0.0, 1.4561006706953048, 0.0, 0.0, 0.0], 'rewardMean': 0.742196534921389, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.7591592695067
'totalSteps': 24320, 'rewardStep': 0.8356578671539201, 'errorList': [], 'lossList': [0.0, -1.2260397905111313, 0.0, 1.9601308746263384, 0.0, 0.0, 0.0], 'rewardMean': 0.7455726623041036, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.2219538636518
'totalSteps': 25600, 'rewardStep': 0.8829171585242286, 'errorList': [], 'lossList': [0.0, -1.2167057770490646, 0.0, 1.3013104188069702, 0.0, 0.0, 0.0], 'rewardMean': 0.7771000214473933, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.0400608367343
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.55
