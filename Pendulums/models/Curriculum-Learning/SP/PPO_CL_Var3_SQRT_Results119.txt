#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 119
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.9610888913323848, 'errorList': [], 'lossList': [0.0, -1.4205939358472823, 0.0, 29.53516651391983, 0.0, 0.0, 0.0], 'rewardMean': 0.8422199076058429, 'totalEpisodes': 16, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.40803261277932
'totalSteps': 3840, 'rewardStep': 0.671371805131084, 'errorList': [], 'lossList': [0.0, -1.4317890459299087, 0.0, 28.453517532348634, 0.0, 0.0, 0.0], 'rewardMean': 0.7852705401142566, 'totalEpisodes': 20, 'stepsPerEpisode': 195, 'rewardPerEpisode': 131.2775112416129
'totalSteps': 5120, 'rewardStep': 0.540544189555269, 'errorList': [], 'lossList': [0.0, -1.4375884163379669, 0.0, 35.57374997615814, 0.0, 0.0, 0.0], 'rewardMean': 0.7240889524745097, 'totalEpisodes': 24, 'stepsPerEpisode': 236, 'rewardPerEpisode': 187.98776713411394
'totalSteps': 6400, 'rewardStep': 0.5696612837493971, 'errorList': [], 'lossList': [0.0, -1.4292564660310745, 0.0, 46.921070551872255, 0.0, 0.0, 0.0], 'rewardMean': 0.6932034187294871, 'totalEpisodes': 28, 'stepsPerEpisode': 349, 'rewardPerEpisode': 253.6103006308807
'totalSteps': 7680, 'rewardStep': 0.7548338517154992, 'errorList': [], 'lossList': [0.0, -1.4118287342786788, 0.0, 86.86125452041625, 0.0, 0.0, 0.0], 'rewardMean': 0.7034751575604892, 'totalEpisodes': 39, 'stepsPerEpisode': 33, 'rewardPerEpisode': 23.118129538167196
'totalSteps': 8960, 'rewardStep': 0.855306387464784, 'errorList': [], 'lossList': [0.0, -1.4004629188776017, 0.0, 153.48246376037596, 0.0, 0.0, 0.0], 'rewardMean': 0.7251653332611028, 'totalEpisodes': 61, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.8839373571919955
'totalSteps': 10240, 'rewardStep': 0.4371796513648386, 'errorList': [], 'lossList': [0.0, -1.3875496381521224, 0.0, 130.0030824279785, 0.0, 0.0, 0.0], 'rewardMean': 0.6891671230240697, 'totalEpisodes': 88, 'stepsPerEpisode': 80, 'rewardPerEpisode': 60.24663409897808
'totalSteps': 11520, 'rewardStep': 0.8576945648446775, 'errorList': [], 'lossList': [0.0, -1.3721943324804307, 0.0, 69.537933883667, 0.0, 0.0, 0.0], 'rewardMean': 0.7078923943374705, 'totalEpisodes': 105, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.71073021510293
'totalSteps': 12800, 'rewardStep': 0.6734175417200499, 'errorList': [], 'lossList': [0.0, -1.3588456642627715, 0.0, 27.841231713294984, 0.0, 0.0, 0.0], 'rewardMean': 0.7044449090757284, 'totalEpisodes': 116, 'stepsPerEpisode': 58, 'rewardPerEpisode': 42.830730659972
'totalSteps': 14080, 'rewardStep': 0.7990044537156352, 'errorList': [], 'lossList': [0.0, -1.3423481792211533, 0.0, 16.16660621166229, 0.0, 0.0, 0.0], 'rewardMean': 0.7120102620593618, 'totalEpisodes': 122, 'stepsPerEpisode': 91, 'rewardPerEpisode': 74.2018712211393
'totalSteps': 15360, 'rewardStep': 0.9543877854409036, 'errorList': [10.80980920557134, 14.735235277483257, 4.839390757129731, 3.289734630213416, 0.9144141159709217, 7.395427430453022, 1.2235463452794042, 8.76663653306339, 2.133450386714449, 3.6903891773486164, 13.974486683584276, 2.5693655230331665, 6.527066455919006, 9.801376718697446, 6.686744659691724, 7.8684228291276765, 16.486291860590462, 5.648088563324167, 16.815856150510875, 6.18672949828424, 4.158244741249059, 13.277661914950064, 1.287597425191522, 11.527834040563095, 1.5286169201607194, 7.311891094150978, 2.9224318397997466, 5.082956665235313, 0.42154811263584596, 7.877783076994827, 0.1532426302227961, 16.860279773085082, 1.898734768816365, 9.493106434045172, 3.8301710220786123, 5.335355686586041, 7.915629570227024, 5.100745347508712, 5.073064504712869, 5.9961765493110235, 2.659239645253853, 3.1018857575333203, 2.3250499028372613, 10.298082439212573, 17.98284331041787, 4.522554474964736, 20.66749844212452, 6.346842441346439, 1.1462745433644186, 5.136510884747344], 'lossList': [0.0, -1.3220512557029724, 0.0, 20.448145506381987, 0.0, 0.0, 0.0], 'rewardMean': 0.7113401514702138, 'totalEpisodes': 128, 'stepsPerEpisode': 148, 'rewardPerEpisode': 126.80242373326256, 'successfulTests': 1
'totalSteps': 16640, 'rewardStep': 0.5159376894403711, 'errorList': [], 'lossList': [0.0, -1.291362116932869, 0.0, 8.820517807006835, 0.0, 0.0, 0.0], 'rewardMean': 0.6957967399011424, 'totalEpisodes': 131, 'stepsPerEpisode': 208, 'rewardPerEpisode': 167.08328403379116
'totalSteps': 17920, 'rewardStep': 0.8623825926584243, 'errorList': [], 'lossList': [0.0, -1.268650285601616, 0.0, 10.472643003463745, 0.0, 0.0, 0.0], 'rewardMean': 0.7279805802114581, 'totalEpisodes': 136, 'stepsPerEpisode': 66, 'rewardPerEpisode': 52.10391860885626
'totalSteps': 19200, 'rewardStep': 0.7599232609994553, 'errorList': [], 'lossList': [0.0, -1.2956520628929138, 0.0, 6.149116089344025, 0.0, 0.0, 0.0], 'rewardMean': 0.7470067779364639, 'totalEpisodes': 139, 'stepsPerEpisode': 158, 'rewardPerEpisode': 136.27033578557467
'totalSteps': 20480, 'rewardStep': 0.7321285292044225, 'errorList': [], 'lossList': [0.0, -1.2919730460643768, 0.0, 5.576158568859101, 0.0, 0.0, 0.0], 'rewardMean': 0.7447362456853563, 'totalEpisodes': 142, 'stepsPerEpisode': 243, 'rewardPerEpisode': 173.6790573597712
'totalSteps': 21760, 'rewardStep': 0.8268282375189829, 'errorList': [], 'lossList': [0.0, -1.2761089080572128, 0.0, 4.509798954725266, 0.0, 0.0, 0.0], 'rewardMean': 0.741888430690776, 'totalEpisodes': 144, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.251863348401203
'totalSteps': 23040, 'rewardStep': 0.7445049292310886, 'errorList': [], 'lossList': [0.0, -1.271733370423317, 0.0, 3.548234831094742, 0.0, 0.0, 0.0], 'rewardMean': 0.7726209584774011, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 985.7227312811697
'totalSteps': 24320, 'rewardStep': 0.7325644697784205, 'errorList': [], 'lossList': [0.0, -1.240532901287079, 0.0, 1.7965955270826817, 0.0, 0.0, 0.0], 'rewardMean': 0.7601079489707755, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.2557570017477
'totalSteps': 25600, 'rewardStep': 0.9457912104516235, 'errorList': [0.013153175836374544, 0.006725765425085928, 0.007107331088601213, 0.013262340974602313, 0.007555905217113642, 0.009255907989014248, 0.006896460049237915, 0.00735969886267315, 0.012342000828923314, 0.0122677857571022, 0.011087043696980372, 0.01201055494005388, 0.01027015973214128, 0.006672508367498769, 0.008701249553609637, 0.020847709545478498, 0.008357661874273706, 0.011432947719403867, 0.008514128777033982, 0.019360423650686406, 0.008686594322398562, 0.007992621053427195, 0.019438675197448116, 0.010360714828015856, 0.007069726356506681, 0.009016188165517493, 0.018951689783994814, 0.022107175099917203, 0.008956015280003237, 0.010977030579396582, 0.01939220416805386, 0.016815194389432665, 0.025122834403593405, 0.012210464765458076, 0.023244573182798406, 0.009296680816482345, 0.007470402454465289, 0.010664979906080271, 0.009711444582621739, 0.007442202299149996, 0.020010473301120598, 0.019049293580845224, 0.00890973928253858, 0.012193892977270438, 0.008613735770215897, 0.012959181473390263, 0.00894845002769061, 0.009447091401923641, 0.013403966761592424, 0.027865263563268797], 'lossList': [0.0, -1.1881185692548752, 0.0, 1.2116507755219936, 0.0, 0.0, 0.0], 'rewardMean': 0.7873453158439329, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.874406344877, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=104.36
