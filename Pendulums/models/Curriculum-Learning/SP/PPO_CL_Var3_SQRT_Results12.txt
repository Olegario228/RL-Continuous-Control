#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 12
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.5188537477404389, 'errorList': [], 'lossList': [0.0, -1.4416358530521394, 0.0, 34.39481699943543, 0.0, 0.0, 0.0], 'rewardMean': 0.4784421699731565, 'totalEpisodes': 32, 'stepsPerEpisode': 117, 'rewardPerEpisode': 81.50724597156707
'totalSteps': 3840, 'rewardStep': 0.9604742754407886, 'errorList': [], 'lossList': [0.0, -1.41847216963768, 0.0, 45.48514071464538, 0.0, 0.0, 0.0], 'rewardMean': 0.6391195384623671, 'totalEpisodes': 57, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.1996553749772865
'totalSteps': 5120, 'rewardStep': 0.5579854928818975, 'errorList': [], 'lossList': [0.0, -1.37942385494709, 0.0, 65.10534265518189, 0.0, 0.0, 0.0], 'rewardMean': 0.6188360270672497, 'totalEpisodes': 96, 'stepsPerEpisode': 28, 'rewardPerEpisode': 20.225403975204856
'totalSteps': 6400, 'rewardStep': 0.778974118994391, 'errorList': [], 'lossList': [0.0, -1.360808711051941, 0.0, 61.509627189636234, 0.0, 0.0, 0.0], 'rewardMean': 0.6508636454526779, 'totalEpisodes': 130, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.12978449287651
'totalSteps': 7680, 'rewardStep': 0.9592904761416423, 'errorList': [1.676095805222531, 10.029463078568817, 3.9100979117798254, 6.78495676214435, 11.07618107508285, 6.289309819824936, 2.8309767036966105, 10.84100218419479, 7.675996119144254, 8.871190536602029, 7.362172492042968, 2.887548918413275, 6.286181662684124, 4.573132622470642, 9.8356217918578, 3.8243015653904804, 6.281677279399344, 4.587503614265598, 4.382334927892916, 13.724211747281263, 11.02444068379406, 0.9058929237264766, 6.096342986395259, 4.2028628681529145, 4.781185899641334, 3.4126336736868974, 3.4245905048097525, 5.993573979878601, 8.852958297675743, 5.18967549286912, 7.095506801911447, 10.981008298704085, 8.428512864073925, 5.054569147434397, 10.592513879746248, 3.0878495153717216, 6.790118076318701, 5.935850444616568, 4.558680119709013, 5.701741542229322, 4.903650531189289, 3.048784103031466, 3.467226571895741, 7.4274362277949955, 3.9988896764494717, 8.302652331501351, 2.07292397108919, 8.371952757264387, 11.820283837078225, 10.05147360232187], 'lossList': [0.0, -1.3383207130432129, 0.0, 58.222766380310055, 0.0, 0.0, 0.0], 'rewardMean': 0.7022681172341722, 'totalEpisodes': 158, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.374774268644153, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.794369138399601, 'errorList': [], 'lossList': [0.0, -1.3174369525909424, 0.0, 40.53601201057434, 0.0, 0.0, 0.0], 'rewardMean': 0.7154254059720905, 'totalEpisodes': 174, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.192078121055973
'totalSteps': 10240, 'rewardStep': 0.39613173242361643, 'errorList': [], 'lossList': [0.0, -1.2960228472948074, 0.0, 24.52504199028015, 0.0, 0.0, 0.0], 'rewardMean': 0.6755136967785312, 'totalEpisodes': 181, 'stepsPerEpisode': 197, 'rewardPerEpisode': 150.93815627453597
'totalSteps': 11520, 'rewardStep': 0.46625733310315126, 'errorList': [], 'lossList': [0.0, -1.2754214262962342, 0.0, 25.255971944332124, 0.0, 0.0, 0.0], 'rewardMean': 0.652262989703489, 'totalEpisodes': 187, 'stepsPerEpisode': 184, 'rewardPerEpisode': 123.45638909483475
'totalSteps': 12800, 'rewardStep': 0.9106141871627135, 'errorList': [], 'lossList': [0.0, -1.2608139204978943, 0.0, 34.06889214754105, 0.0, 0.0, 0.0], 'rewardMean': 0.6780981094494114, 'totalEpisodes': 191, 'stepsPerEpisode': 376, 'rewardPerEpisode': 306.6269956697388
'totalSteps': 14080, 'rewardStep': 0.7290336832631314, 'errorList': [], 'lossList': [0.0, -1.2376602602005005, 0.0, 7.213295896500349, 0.0, 0.0, 0.0], 'rewardMean': 0.7071984185551372, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.6348782421421
'totalSteps': 15360, 'rewardStep': 0.9165913799576056, 'errorList': [], 'lossList': [0.0, -1.2052642691135407, 0.0, 6.374588051438332, 0.0, 0.0, 0.0], 'rewardMean': 0.7469721817768538, 'totalEpisodes': 194, 'stepsPerEpisode': 100, 'rewardPerEpisode': 80.3114725197022
'totalSteps': 16640, 'rewardStep': 0.8898618621739134, 'errorList': [], 'lossList': [0.0, -1.1778697168827057, 0.0, 5.255200318098068, 0.0, 0.0, 0.0], 'rewardMean': 0.7399109404501664, 'totalEpisodes': 198, 'stepsPerEpisode': 171, 'rewardPerEpisode': 133.65528352610545
'totalSteps': 17920, 'rewardStep': 0.7539229596686506, 'errorList': [], 'lossList': [0.0, -1.1698343336582184, 0.0, 4.121810944974422, 0.0, 0.0, 0.0], 'rewardMean': 0.7595046871288417, 'totalEpisodes': 199, 'stepsPerEpisode': 328, 'rewardPerEpisode': 285.2747697084777
'totalSteps': 19200, 'rewardStep': 0.9315562453133093, 'errorList': [0.2926844311563861, 0.29761307979714835, 0.30894501574763333, 0.248635491698001, 0.2803734011328936, 0.30084649036444666, 0.26130517466500974, 0.29350442849889863, 0.32347142519361066, 0.3213504830886051, 0.285116279278154, 0.2938749476731663, 0.2716065776030438, 0.2806283461024489, 0.29886638513098374, 0.3010014184586258, 0.30332880623456315, 0.30243144857581866, 0.25787221098664936, 0.3082825717386556, 0.32651932524359767, 0.2949604687471063, 0.2682084333887887, 0.29860979861011167, 0.3111193946818814, 0.29242555887956706, 0.29671277241949845, 0.2753580400559759, 0.26785957860821025, 0.32524083344289606, 0.25733668169910623, 0.2869597912755576, 0.2830450473178959, 0.2945032554683388, 0.2756812371390374, 0.3055443047187609, 0.2635534525365736, 0.28850964879294166, 0.2758446232121109, 0.2833690627781353, 0.27137367610101176, 0.2949875924592164, 0.3209222118400852, 0.29466847396786905, 0.2682174277312792, 0.2799124778134329, 0.2874599637185296, 0.2801262935232132, 0.2900802566177132, 0.28710400968874844], 'lossList': [0.0, -1.1714381355047225, 0.0, 8.416379303336143, 0.0, 0.0, 0.0], 'rewardMean': 0.7747628997607334, 'totalEpisodes': 200, 'stepsPerEpisode': 758, 'rewardPerEpisode': 672.166114740173, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8748112672732455, 'errorList': [], 'lossList': [0.0, -1.1488251614570617, 0.0, 1.2771295858174563, 0.0, 0.0, 0.0], 'rewardMean': 0.7663149788738939, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1065.0612495450112
'totalSteps': 21760, 'rewardStep': 0.8690024813250073, 'errorList': [], 'lossList': [0.0, -1.1263817167282104, 0.0, 1.6110188728570938, 0.0, 0.0, 0.0], 'rewardMean': 0.7737783131664344, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.017527547576
'totalSteps': 23040, 'rewardStep': 0.9886331899139342, 'errorList': [0.22972465464734385, 0.19813159081032497, 0.2071182952482422, 0.31598512660118444, 0.19776177191742, 0.19245310117451275, 0.19578153224047504, 0.2599161199856312, 0.18626686734806208, 0.19137105779028296, 0.2503728734462721, 0.22275121055706998, 0.1842344222848891, 0.19763911428751302, 0.2615413129245527, 0.2613558500376339, 0.208832425289635, 0.1836266229285427, 0.3002164541528555, 0.20184379228840943, 0.23263196531913669, 0.1840743657258985, 0.2803090455451191, 0.19285311994510881, 0.23610859626168532, 0.23949903900734007, 0.18313819021930233, 0.183900308836699, 0.21940918005099747, 0.17938275320170993, 0.26921214455591713, 0.18840416181216724, 0.16199548254131887, 0.3351282652560329, 0.17562630565315307, 0.1767572930199222, 0.2598582565780572, 0.18705928766458113, 0.15737181428588218, 0.15576572731224417, 0.22655475202251318, 0.14719374332982918, 0.20638538591700226, 0.28773491093960263, 0.20262965853928633, 0.1532190740337811, 0.18646183338117844, 0.15904020653270218, 0.3227671494919165, 0.21007148526298178], 'lossList': [0.0, -1.0905543547868728, 0.0, 1.3657104546390473, 0.0, 0.0, 0.0], 'rewardMean': 0.8330284589154662, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.7518386321756, 'successfulTests': 25
'totalSteps': 24320, 'rewardStep': 0.8858387875397752, 'errorList': [], 'lossList': [0.0, -1.0605748528242112, 0.0, 0.6256986308656632, 0.0, 0.0, 0.0], 'rewardMean': 0.8749866043591286, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.3649166807966
'totalSteps': 25600, 'rewardStep': 0.8568454255469509, 'errorList': [], 'lossList': [0.0, -1.0417185759544372, 0.0, 0.40375449251383544, 0.0, 0.0, 0.0], 'rewardMean': 0.8696097281975523, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1160.5141359480974
#maxSuccessfulTests=25, maxSuccessfulTestsAtStep=23040, timeSpent=118.42
