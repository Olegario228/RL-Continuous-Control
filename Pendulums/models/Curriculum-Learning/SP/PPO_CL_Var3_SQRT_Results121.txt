#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 121
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.7925178698635806, 'errorList': [], 'lossList': [0.0, -1.4550134706497193, 0.0, 31.636421563625337, 0.0, 0.0, 0.0], 'rewardMean': 0.69284784472187, 'totalEpisodes': 12, 'stepsPerEpisode': 83, 'rewardPerEpisode': 70.73863156414741
'totalSteps': 3840, 'rewardStep': 0.8567821484969604, 'errorList': [], 'lossList': [0.0, -1.4780186063051224, 0.0, 31.53076723575592, 0.0, 0.0, 0.0], 'rewardMean': 0.7474926126469001, 'totalEpisodes': 15, 'stepsPerEpisode': 167, 'rewardPerEpisode': 120.81112603681217
'totalSteps': 5120, 'rewardStep': 0.8188723623752504, 'errorList': [], 'lossList': [0.0, -1.4699656528234482, 0.0, 32.69035983800888, 0.0, 0.0, 0.0], 'rewardMean': 0.7653375500789877, 'totalEpisodes': 19, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.902227259943704
'totalSteps': 6400, 'rewardStep': 0.8546775184432691, 'errorList': [], 'lossList': [0.0, -1.4603348666429519, 0.0, 57.61672218322754, 0.0, 0.0, 0.0], 'rewardMean': 0.7832055437518439, 'totalEpisodes': 25, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.198957180573214
'totalSteps': 7680, 'rewardStep': 0.7843135219177035, 'errorList': [], 'lossList': [0.0, -1.4341968315839768, 0.0, 14.691407088935375, 0.0, 0.0, 0.0], 'rewardMean': 0.7833902067794872, 'totalEpisodes': 25, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.7624235045384
'totalSteps': 8960, 'rewardStep': 0.64307354946949, 'errorList': [], 'lossList': [0.0, -1.3969956523180007, 0.0, 130.61555110931397, 0.0, 0.0, 0.0], 'rewardMean': 0.7633449700209162, 'totalEpisodes': 37, 'stepsPerEpisode': 111, 'rewardPerEpisode': 87.65063065445217
'totalSteps': 10240, 'rewardStep': 0.8302786053969309, 'errorList': [], 'lossList': [0.0, -1.3941716742515564, 0.0, 241.83701713562013, 0.0, 0.0, 0.0], 'rewardMean': 0.771711674442918, 'totalEpisodes': 76, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.174069301564604
'totalSteps': 11520, 'rewardStep': 0.9287807313843335, 'errorList': [], 'lossList': [0.0, -1.391507512331009, 0.0, 112.35575204849243, 0.0, 0.0, 0.0], 'rewardMean': 0.7891637918808531, 'totalEpisodes': 102, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.56374981909113
'totalSteps': 12800, 'rewardStep': 0.847615842467316, 'errorList': [], 'lossList': [0.0, -1.38953049659729, 0.0, 79.79140878677369, 0.0, 0.0, 0.0], 'rewardMean': 0.7950089969394993, 'totalEpisodes': 119, 'stepsPerEpisode': 90, 'rewardPerEpisode': 72.53423816592569
'totalSteps': 14080, 'rewardStep': 0.7106323129643255, 'errorList': [], 'lossList': [0.0, -1.388600686788559, 0.0, 23.467772233486176, 0.0, 0.0, 0.0], 'rewardMean': 0.806754446277916, 'totalEpisodes': 124, 'stepsPerEpisode': 197, 'rewardPerEpisode': 153.549385088518
'totalSteps': 15360, 'rewardStep': 0.7673685252936268, 'errorList': [], 'lossList': [0.0, -1.3748434561491012, 0.0, 11.838088080883026, 0.0, 0.0, 0.0], 'rewardMean': 0.8042395118209205, 'totalEpisodes': 126, 'stepsPerEpisode': 229, 'rewardPerEpisode': 167.9995511491838
'totalSteps': 16640, 'rewardStep': 0.8036787285016566, 'errorList': [], 'lossList': [0.0, -1.3523702478408814, 0.0, 7.046511594057083, 0.0, 0.0, 0.0], 'rewardMean': 0.7989291698213903, 'totalEpisodes': 127, 'stepsPerEpisode': 721, 'rewardPerEpisode': 563.3570402233281
'totalSteps': 17920, 'rewardStep': 0.6394934824284395, 'errorList': [], 'lossList': [0.0, -1.3305857771635055, 0.0, 5.8323058632016185, 0.0, 0.0, 0.0], 'rewardMean': 0.7809912818267091, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 978.0730281294896
'totalSteps': 19200, 'rewardStep': 0.883600000826088, 'errorList': [], 'lossList': [0.0, -1.304733821749687, 0.0, 3.657460456490517, 0.0, 0.0, 0.0], 'rewardMean': 0.783883530064991, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1038.0252147814738
'totalSteps': 20480, 'rewardStep': 0.7332021165638745, 'errorList': [], 'lossList': [0.0, -1.2597478699684144, 0.0, 4.07189058944583, 0.0, 0.0, 0.0], 'rewardMean': 0.778772389529608, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.7329139846513
'totalSteps': 21760, 'rewardStep': 0.833351360995668, 'errorList': [], 'lossList': [0.0, -1.1994341397285462, 0.0, 2.839388111382723, 0.0, 0.0, 0.0], 'rewardMean': 0.797800170682226, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1140.8217435278611
'totalSteps': 23040, 'rewardStep': 0.9438187319004628, 'errorList': [0.1980293946390979, 0.2041031901455057, 0.1843490629026553, 0.18679708940367548, 0.21276301838218475, 0.1899922430980192, 0.19892481483621322, 0.20156031287479997, 0.1684574178706921, 0.19504997448027994, 0.18045221388551114, 0.17790947665603388, 0.18567943270344617, 0.1960442973895744, 0.24041510719417544, 0.1882539239528465, 0.20140558893742808, 0.18033847876656778, 0.18878897152287907, 0.1894080986080988, 0.20766031505998958, 0.2050417365170238, 0.19268362593924632, 0.2052577216950422, 0.18680667547317537, 0.18428191581101336, 0.20165298887399385, 0.20060640267668114, 0.18159926425141862, 0.18545682124942875, 0.17905703389065647, 0.20557784190219516, 0.2043023977496225, 0.1995095798222325, 0.20357229822363077, 0.1864935427511344, 0.2145827500044328, 0.2278095739110714, 0.19570208206741213, 0.19524593639306395, 0.18962163640903854, 0.2007018800529204, 0.2015549703054661, 0.22428942815022948, 0.2202281760797505, 0.19345694909692063, 0.22172115277860419, 0.2442198948062969, 0.20943959830413705, 0.16369281738916505], 'lossList': [0.0, -1.1583609569072724, 0.0, 1.300132884941995, 0.0, 0.0, 0.0], 'rewardMean': 0.809154183332579, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.9220858831309, 'successfulTests': 28
'totalSteps': 24320, 'rewardStep': 0.8955573002909154, 'errorList': [], 'lossList': [0.0, -1.1263351428508759, 0.0, 1.0109646661952139, 0.0, 0.0, 0.0], 'rewardMean': 0.8058318402232374, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.530783868391
'totalSteps': 25600, 'rewardStep': 0.9888437311912457, 'errorList': [0.008053440268045926, 0.0048080744315181796, 0.05008664213605747, 0.011710192893048022, 0.02937587705031386, 0.0526570180851949, 0.036909517340122926, 0.04015914190394559, 0.058449273990143616, 0.027470336626751336, 0.020173990749380796, 0.04041886156309561, 0.04135045828147333, 0.016760397977303533, 0.052210480789338265, 0.0363979615000739, 0.025688978242799622, 0.018237066709944755, 0.10237373945202817, 0.031156175351230275, 0.04102607968854314, 0.029836406562589905, 0.057465812524636985, 0.06124081199203934, 0.033480857168534, 0.025588592196359124, 0.08129181143048661, 0.05567393015452258, 0.033574846638841783, 0.015550228844011568, 0.04779904468036212, 0.011540323733935279, 0.021204944318029717, 0.04458150728793896, 0.08361707177501418, 0.02815029633149567, 0.08530223573695712, 0.013527053939164126, 0.04229993819191187, 0.061176048229630096, 0.016325063784224644, 0.029472687341510793, 0.05360735706883189, 0.07220540652982183, 0.027430863056254185, 0.027343505891673395, 0.035876498025563246, 0.03160010679463358, 0.07461703915082171, 0.015647819254985762], 'lossList': [0.0, -1.0775257933139801, 0.0, 1.177406957037747, 0.0, 0.0, 0.0], 'rewardMean': 0.8199546290956302, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1219.3961822289716, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=104.48
