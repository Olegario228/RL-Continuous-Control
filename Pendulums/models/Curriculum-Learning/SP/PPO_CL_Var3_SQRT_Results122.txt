#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 122
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.6826788460849791, 'errorList': [], 'lossList': [0.0, -1.4508233082294464, 0.0, 30.98265645980835, 0.0, 0.0, 0.0], 'rewardMean': 0.5928898469944954, 'totalEpisodes': 14, 'stepsPerEpisode': 281, 'rewardPerEpisode': 208.73830792625847
'totalSteps': 3840, 'rewardStep': 0.925020430957188, 'errorList': [], 'lossList': [0.0, -1.4535050338506699, 0.0, 28.712057688236236, 0.0, 0.0, 0.0], 'rewardMean': 0.7036000416487264, 'totalEpisodes': 19, 'stepsPerEpisode': 76, 'rewardPerEpisode': 53.79103278562746
'totalSteps': 5120, 'rewardStep': 0.40671492982787205, 'errorList': [], 'lossList': [0.0, -1.416985928416252, 0.0, 35.760268669128415, 0.0, 0.0, 0.0], 'rewardMean': 0.6293787636935128, 'totalEpisodes': 27, 'stepsPerEpisode': 68, 'rewardPerEpisode': 37.46313196220642
'totalSteps': 6400, 'rewardStep': 0.8932776344364317, 'errorList': [], 'lossList': [0.0, -1.4022482007741928, 0.0, 54.932311997413635, 0.0, 0.0, 0.0], 'rewardMean': 0.6821585378420966, 'totalEpisodes': 34, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.680895809964075
'totalSteps': 7680, 'rewardStep': 0.9386487583269075, 'errorList': [], 'lossList': [0.0, -1.38399544775486, 0.0, 115.85051462173462, 0.0, 0.0, 0.0], 'rewardMean': 0.7249069079228984, 'totalEpisodes': 49, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.034805567255715
'totalSteps': 8960, 'rewardStep': 0.684585753454503, 'errorList': [], 'lossList': [0.0, -1.3726760351657867, 0.0, 71.53471439361573, 0.0, 0.0, 0.0], 'rewardMean': 0.719146742998842, 'totalEpisodes': 57, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.960240120781027
'totalSteps': 10240, 'rewardStep': 0.44871302766850435, 'errorList': [], 'lossList': [0.0, -1.3586714166402816, 0.0, 152.91222873687744, 0.0, 0.0, 0.0], 'rewardMean': 0.6853425285825496, 'totalEpisodes': 88, 'stepsPerEpisode': 54, 'rewardPerEpisode': 32.52385744051066
'totalSteps': 11520, 'rewardStep': 0.8389398138003192, 'errorList': [], 'lossList': [0.0, -1.35515445291996, 0.0, 64.11986698150635, 0.0, 0.0, 0.0], 'rewardMean': 0.7024088936067463, 'totalEpisodes': 115, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.128132890721112
'totalSteps': 12800, 'rewardStep': 0.9409689666304725, 'errorList': [59.268771407156514, 31.409213054738974, 23.44564968461679, 64.8200078296585, 32.08746234373222, 105.02716850739583, 80.6144247473833, 82.71692900434961, 91.36078391831336, 35.1577705361623, 45.976978325210986, 101.65707962487912, 69.36032105732781, 20.626909800090274, 50.55883891448495, 117.955557087665, 71.36464051170181, 2.663390735633098, 128.09682831450672, 80.16707927873831, 83.91260924259267, 7.350122184815388, 63.031373114562136, 33.41425169958714, 79.59448537652442, 114.73146783386451, 77.7574622125788, 69.67492995113845, 13.435665691601741, 76.89467370641341, 85.47707798654957, 81.69180200295176, 83.17331773040438, 61.47940998536025, 34.24662938229435, 61.451687853777784, 95.93878668935106, 85.17698212350908, 7.383483607417407, 77.02565721969773, 45.20337939357415, 42.22142534820816, 98.15812111773812, 79.51702101136323, 56.60223685815402, 69.65209144178284, 114.59235663200772, 98.81388716703, 15.52918591083078, 111.75210899356092], 'lossList': [0.0, -1.3622548669576644, 0.0, 35.92147336006165, 0.0, 0.0, 0.0], 'rewardMean': 0.7262649009091189, 'totalEpisodes': 132, 'stepsPerEpisode': 12, 'rewardPerEpisode': 11.150553698901788, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.8538834556974597, 'errorList': [], 'lossList': [0.0, -1.3613905054330826, 0.0, 15.751447587013244, 0.0, 0.0, 0.0], 'rewardMean': 0.7613431616884637, 'totalEpisodes': 138, 'stepsPerEpisode': 66, 'rewardPerEpisode': 55.32077140605193
'totalSteps': 15360, 'rewardStep': 0.7703049122830702, 'errorList': [], 'lossList': [0.0, -1.3586923027038573, 0.0, 38.417172379493714, 0.0, 0.0, 0.0], 'rewardMean': 0.7701057683082728, 'totalEpisodes': 150, 'stepsPerEpisode': 37, 'rewardPerEpisode': 30.478587274375972
'totalSteps': 16640, 'rewardStep': 0.6495352042456308, 'errorList': [], 'lossList': [0.0, -1.3378067886829377, 0.0, 9.153387122154236, 0.0, 0.0, 0.0], 'rewardMean': 0.7425572456371171, 'totalEpisodes': 157, 'stepsPerEpisode': 195, 'rewardPerEpisode': 146.52319849102682
'totalSteps': 17920, 'rewardStep': 0.8018341200094536, 'errorList': [], 'lossList': [0.0, -1.3070158958435059, 0.0, 20.981195291280745, 0.0, 0.0, 0.0], 'rewardMean': 0.7820691646552752, 'totalEpisodes': 163, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.94932631290803
'totalSteps': 19200, 'rewardStep': 0.9384548751119512, 'errorList': [0.4077598047068783, 0.28556227640321036, 0.2911474167659964, 0.09860266734789351, 0.26779331918023813, 0.8058557221019053, 0.21165811153857225, 0.28689055417715675, 0.17997894643421095, 0.2220821988539815, 0.7536359773311626, 0.7574676898244113, 0.5236678706268996, 0.17875971459335027, 0.13060419016682115, 0.4002129372929666, 0.39474862895731916, 0.16807761039541852, 0.3423629439464868, 0.40307676942597476, 0.18369209735445516, 0.4357779115347474, 0.3373010900524654, 0.1988346326963776, 0.11044064463037856, 0.3290296590816044, 0.23883868337547345, 0.13071229131423803, 0.7591264772890494, 0.24987829657854543, 0.37091299374810555, 0.5733512761919057, 0.1034636444991864, 0.19658106911922357, 0.28887306911030225, 0.44574223560047743, 0.2562318479004766, 0.11898996210041025, 0.3730071469604144, 0.6375883762857099, 0.1331166317158764, 0.531269609444504, 0.2642771185557986, 0.10501681712273069, 0.3133313189336478, 0.20404253258273514, 0.15022702741795502, 1.014205426721469, 0.4843128896714621, 0.21866788914582244], 'lossList': [0.0, -1.2880471813678742, 0.0, 8.847778359651565, 0.0, 0.0, 0.0], 'rewardMean': 0.7865868887228272, 'totalEpisodes': 167, 'stepsPerEpisode': 205, 'rewardPerEpisode': 179.7910165166299, 'successfulTests': 15
'totalSteps': 20480, 'rewardStep': 0.853130601763699, 'errorList': [], 'lossList': [0.0, -1.2694393414258958, 0.0, 5.666517649292945, 0.0, 0.0, 0.0], 'rewardMean': 0.7780350730665063, 'totalEpisodes': 169, 'stepsPerEpisode': 113, 'rewardPerEpisode': 98.28460964853825
'totalSteps': 21760, 'rewardStep': 0.7630116198272412, 'errorList': [], 'lossList': [0.0, -1.2593170130252838, 0.0, 5.192619404196739, 0.0, 0.0, 0.0], 'rewardMean': 0.7858776597037801, 'totalEpisodes': 173, 'stepsPerEpisode': 212, 'rewardPerEpisode': 180.49245789714828
'totalSteps': 23040, 'rewardStep': 0.6943862511422383, 'errorList': [], 'lossList': [0.0, -1.2343171322345734, 0.0, 2.9721416887640952, 0.0, 0.0, 0.0], 'rewardMean': 0.8104449820511537, 'totalEpisodes': 174, 'stepsPerEpisode': 735, 'rewardPerEpisode': 588.460054489975
'totalSteps': 24320, 'rewardStep': 0.9009506542098838, 'errorList': [], 'lossList': [0.0, -1.1925058811903, 0.0, 2.9333968499302863, 0.0, 0.0, 0.0], 'rewardMean': 0.81664606609211, 'totalEpisodes': 175, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.999740759870015
'totalSteps': 25600, 'rewardStep': 0.8742527672464097, 'errorList': [], 'lossList': [0.0, -1.1738784289360047, 0.0, 3.6697240167856218, 0.0, 0.0, 0.0], 'rewardMean': 0.8099744461537037, 'totalEpisodes': 179, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.590557318058135
#maxSuccessfulTests=15, maxSuccessfulTestsAtStep=19200, timeSpent=104.84
