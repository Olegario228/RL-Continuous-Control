#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 123
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5599711506588105, 'errorList': [], 'lossList': [0.0, -1.4217401373386382, 0.0, 26.005307273864744, 0.0, 0.0, 0.0], 'rewardMean': 0.7294507832427755, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 29.9630885427462
'totalSteps': 3840, 'rewardStep': 0.9522224220807021, 'errorList': [], 'lossList': [0.0, -1.4138810473680496, 0.0, 23.869919621944426, 0.0, 0.0, 0.0], 'rewardMean': 0.8037079961887509, 'totalEpisodes': 18, 'stepsPerEpisode': 539, 'rewardPerEpisode': 367.02423657200944
'totalSteps': 5120, 'rewardStep': 0.7419930397212791, 'errorList': [], 'lossList': [0.0, -1.397933265566826, 0.0, 48.58922088623047, 0.0, 0.0, 0.0], 'rewardMean': 0.788279257071883, 'totalEpisodes': 23, 'stepsPerEpisode': 78, 'rewardPerEpisode': 69.0983187753742
'totalSteps': 6400, 'rewardStep': 0.8080451743321162, 'errorList': [], 'lossList': [0.0, -1.3927771973609924, 0.0, 66.37881845474243, 0.0, 0.0, 0.0], 'rewardMean': 0.7922324405239296, 'totalEpisodes': 31, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.744447861472002
'totalSteps': 7680, 'rewardStep': 0.8722884347476392, 'errorList': [], 'lossList': [0.0, -1.3884370470046996, 0.0, 71.67144477844238, 0.0, 0.0, 0.0], 'rewardMean': 0.8055751062278812, 'totalEpisodes': 40, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.070241292250984
'totalSteps': 8960, 'rewardStep': 0.816371385819144, 'errorList': [], 'lossList': [0.0, -1.374038587808609, 0.0, 106.75475456237793, 0.0, 0.0, 0.0], 'rewardMean': 0.8071174318837758, 'totalEpisodes': 53, 'stepsPerEpisode': 70, 'rewardPerEpisode': 52.872367079175625
'totalSteps': 10240, 'rewardStep': 0.9317011428823119, 'errorList': [343.23039702559413, 326.10115646701513, 364.4649663456705, 131.80682890055965, 354.15518368706336, 287.712936689993, 277.45952887196404, 316.46233061656073, 361.97919031091374, 227.4078337570961, 351.3223459142916, 206.90935797201016, 333.1469235338813, 324.4863953572746, 314.87741125955296, 304.340261927368, 358.5085111110816, 311.6206853922937, 366.77504451060884, 299.1761562879455, 351.9019911824098, 256.38854216174224, 342.0956457930369, 372.1308377635347, 351.82938689681674, 301.7979506590198, 328.7743915431415, 312.05704972874304, 342.348233376719, 314.9318408548988, 341.54032587272445, 326.7294718128105, 292.6542463045344, 311.5943367005628, 329.37263534231454, 335.1932346769363, 290.12470284867277, 331.03385881404324, 326.73961889424737, 333.4376250777965, 353.5729242902009, 323.00391715177767, 306.45059099556977, 329.26099528687945, 302.57956834293566, 320.48146963971755, 326.14249286179614, 321.29314018604015, 319.68904196790396, 313.0026739169608], 'lossList': [0.0, -1.3587410974502563, 0.0, 164.9478274536133, 0.0, 0.0, 0.0], 'rewardMean': 0.8226903957585929, 'totalEpisodes': 88, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.00570343863852, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.92524240678053, 'errorList': [], 'lossList': [0.0, -1.3501320385932922, 0.0, 52.56638693809509, 0.0, 0.0, 0.0], 'rewardMean': 0.8340850636499193, 'totalEpisodes': 121, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.237189711068972
'totalSteps': 12800, 'rewardStep': 0.7392986370257727, 'errorList': [], 'lossList': [0.0, -1.3417276310920716, 0.0, 52.37162094116211, 0.0, 0.0, 0.0], 'rewardMean': 0.8246064209875046, 'totalEpisodes': 139, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.470623881348095
'totalSteps': 14080, 'rewardStep': 0.9525278165638633, 'errorList': [1.7761023387086623, 1.213602182257162, 1.6659705530752715, 1.2883810678816272, 1.4927891261805823, 1.5045224441163763, 1.6897610709926632, 1.2285581796239218, 1.9485278597403515, 1.4278976025023724, 1.5522685394602687, 1.5802128090691627, 1.1822462362047157, 1.5574909333858875, 1.2232019368866178, 1.467454913827935, 1.6437307310784166, 1.4024606648951, 1.542489907884957, 1.1753068363114052, 1.2431384018595677, 1.4741411736156511, 1.5334487381537825, 1.658618174666631, 1.5366663508249674, 1.8423265337852992, 1.6807404255644591, 1.8853057912251687, 1.6846233178945773, 1.590516797063845, 1.1056480344991377, 1.1869425988210718, 1.447663279158997, 1.2386115012494272, 1.3793329100272855, 1.337816994196623, 1.5349670920717224, 1.5157043068552667, 1.2856818297951311, 1.4603163836782107, 1.155742467761692, 1.3473235017042795, 1.2580061654184105, 1.877971273464384, 1.1664498349095882, 1.5977338524983375, 0.9961450418784723, 1.0933147109953418, 1.783612988646477, 1.2607282412662406], 'lossList': [0.0, -1.337694497704506, 0.0, 20.63214541912079, 0.0, 0.0, 0.0], 'rewardMean': 0.8299661610612169, 'totalEpisodes': 144, 'stepsPerEpisode': 227, 'rewardPerEpisode': 167.77081473961832, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.5219594569788829, 'errorList': [], 'lossList': [0.0, -1.3337992799282075, 0.0, 16.9136847782135, 0.0, 0.0, 0.0], 'rewardMean': 0.8261649916932241, 'totalEpisodes': 148, 'stepsPerEpisode': 258, 'rewardPerEpisode': 201.06154180210115
'totalSteps': 16640, 'rewardStep': 0.8737997498811859, 'errorList': [], 'lossList': [0.0, -1.3274661689996718, 0.0, 8.416756967306137, 0.0, 0.0, 0.0], 'rewardMean': 0.8183227244732725, 'totalEpisodes': 151, 'stepsPerEpisode': 83, 'rewardPerEpisode': 70.65949663529454
'totalSteps': 17920, 'rewardStep': 0.8907512476006928, 'errorList': [], 'lossList': [0.0, -1.3004027092456818, 0.0, 7.647535123825073, 0.0, 0.0, 0.0], 'rewardMean': 0.833198545261214, 'totalEpisodes': 153, 'stepsPerEpisode': 39, 'rewardPerEpisode': 31.66087911629964
'totalSteps': 19200, 'rewardStep': 0.6861083685661125, 'errorList': [], 'lossList': [0.0, -1.2838145411014557, 0.0, 20.760481288433073, 0.0, 0.0, 0.0], 'rewardMean': 0.8210048646846134, 'totalEpisodes': 158, 'stepsPerEpisode': 124, 'rewardPerEpisode': 94.45968852296436
'totalSteps': 20480, 'rewardStep': 0.7743460880480958, 'errorList': [], 'lossList': [0.0, -1.2989994740486146, 0.0, 8.900918439626693, 0.0, 0.0, 0.0], 'rewardMean': 0.8112106300146593, 'totalEpisodes': 162, 'stepsPerEpisode': 401, 'rewardPerEpisode': 309.3018927420469
'totalSteps': 21760, 'rewardStep': 0.7914253439519692, 'errorList': [], 'lossList': [0.0, -1.3106175947189331, 0.0, 8.529658929109573, 0.0, 0.0, 0.0], 'rewardMean': 0.8087160258279417, 'totalEpisodes': 165, 'stepsPerEpisode': 175, 'rewardPerEpisode': 137.22008565075487
'totalSteps': 23040, 'rewardStep': 0.5038956495473996, 'errorList': [], 'lossList': [0.0, -1.3123603373765946, 0.0, 3.8769283521175386, 0.0, 0.0, 0.0], 'rewardMean': 0.7659354764944506, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 853.2379644498119
'totalSteps': 24320, 'rewardStep': 0.6834461522803174, 'errorList': [], 'lossList': [0.0, -1.2807130205631256, 0.0, 1.671997033059597, 0.0, 0.0, 0.0], 'rewardMean': 0.7417558510444292, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 978.0944245719978
'totalSteps': 25600, 'rewardStep': 0.878240674457703, 'errorList': [], 'lossList': [0.0, -1.2395420169830322, 0.0, 1.0740060695260762, 0.0, 0.0, 0.0], 'rewardMean': 0.7556500547876223, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.2665077898314
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=105.16
