#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 125
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.6257288571911466, 'errorList': [], 'lossList': [0.0, -1.4158620619773865, 0.0, 32.89921855926514, 0.0, 0.0, 0.0], 'rewardMean': 0.5349757990833321, 'totalEpisodes': 69, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.00528501155386
'totalSteps': 3840, 'rewardStep': 0.7160079849210751, 'errorList': [], 'lossList': [0.0, -1.4052313590049743, 0.0, 45.66232828140259, 0.0, 0.0, 0.0], 'rewardMean': 0.5953198610292465, 'totalEpisodes': 100, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.731219633673103
'totalSteps': 5120, 'rewardStep': 0.8137535240633211, 'errorList': [], 'lossList': [0.0, -1.3793853121995925, 0.0, 43.94720404624939, 0.0, 0.0, 0.0], 'rewardMean': 0.6499282767877652, 'totalEpisodes': 113, 'stepsPerEpisode': 93, 'rewardPerEpisode': 69.30863547164856
'totalSteps': 6400, 'rewardStep': 0.7850925418370115, 'errorList': [], 'lossList': [0.0, -1.3589782428741455, 0.0, 55.100244674682614, 0.0, 0.0, 0.0], 'rewardMean': 0.6769611297976145, 'totalEpisodes': 122, 'stepsPerEpisode': 276, 'rewardPerEpisode': 216.59439214220862
'totalSteps': 7680, 'rewardStep': 0.7152888859215014, 'errorList': [], 'lossList': [0.0, -1.3570942121744156, 0.0, 54.249041109085084, 0.0, 0.0, 0.0], 'rewardMean': 0.6833490891515956, 'totalEpisodes': 128, 'stepsPerEpisode': 199, 'rewardPerEpisode': 148.76087364381473
'totalSteps': 8960, 'rewardStep': 0.4635837908256982, 'errorList': [], 'lossList': [0.0, -1.3556765311956405, 0.0, 62.54647650718689, 0.0, 0.0, 0.0], 'rewardMean': 0.6519540465336103, 'totalEpisodes': 137, 'stepsPerEpisode': 98, 'rewardPerEpisode': 67.3467360911025
'totalSteps': 10240, 'rewardStep': 0.6045840416081735, 'errorList': [], 'lossList': [0.0, -1.3679323613643646, 0.0, 74.50664109230041, 0.0, 0.0, 0.0], 'rewardMean': 0.6460327959179306, 'totalEpisodes': 149, 'stepsPerEpisode': 110, 'rewardPerEpisode': 82.45699034055706
'totalSteps': 11520, 'rewardStep': 0.49359409519114894, 'errorList': [], 'lossList': [0.0, -1.3745072603225708, 0.0, 60.53712946891785, 0.0, 0.0, 0.0], 'rewardMean': 0.6290951625038438, 'totalEpisodes': 158, 'stepsPerEpisode': 19, 'rewardPerEpisode': 12.18970692805126
'totalSteps': 12800, 'rewardStep': 0.8682923478187609, 'errorList': [], 'lossList': [0.0, -1.3841782408952712, 0.0, 24.200830619335175, 0.0, 0.0, 0.0], 'rewardMean': 0.6530148810353354, 'totalEpisodes': 166, 'stepsPerEpisode': 44, 'rewardPerEpisode': 39.72528879816933
'totalSteps': 14080, 'rewardStep': 0.7561735071414165, 'errorList': [], 'lossList': [0.0, -1.4036978548765182, 0.0, 9.53989504814148, 0.0, 0.0, 0.0], 'rewardMean': 0.6842099576519254, 'totalEpisodes': 169, 'stepsPerEpisode': 517, 'rewardPerEpisode': 354.3729242587027
'totalSteps': 15360, 'rewardStep': 0.8276402411105102, 'errorList': [], 'lossList': [0.0, -1.3811997544765473, 0.0, 8.198743126392365, 0.0, 0.0, 0.0], 'rewardMean': 0.7044010960438617, 'totalEpisodes': 174, 'stepsPerEpisode': 156, 'rewardPerEpisode': 132.92750035362448
'totalSteps': 16640, 'rewardStep': 0.7203922365646349, 'errorList': [], 'lossList': [0.0, -1.36830635368824, 0.0, 6.201900715827942, 0.0, 0.0, 0.0], 'rewardMean': 0.7048395212082177, 'totalEpisodes': 177, 'stepsPerEpisode': 569, 'rewardPerEpisode': 447.38795841582197
'totalSteps': 17920, 'rewardStep': 0.8031050332932154, 'errorList': [], 'lossList': [0.0, -1.3714199882745743, 0.0, 6.275996916890144, 0.0, 0.0, 0.0], 'rewardMean': 0.7037746721312071, 'totalEpisodes': 178, 'stepsPerEpisode': 355, 'rewardPerEpisode': 237.3451375332914
'totalSteps': 19200, 'rewardStep': 0.9086871375200309, 'errorList': [], 'lossList': [0.0, -1.3679069018363952, 0.0, 47.20239327669144, 0.0, 0.0, 0.0], 'rewardMean': 0.7161341316995091, 'totalEpisodes': 180, 'stepsPerEpisode': 818, 'rewardPerEpisode': 652.9979861787663
'totalSteps': 20480, 'rewardStep': 0.8513344430830682, 'errorList': [], 'lossList': [0.0, -1.3557726603746414, 0.0, 3.78157332777977, 0.0, 0.0, 0.0], 'rewardMean': 0.7297386874156657, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.605543385975
'totalSteps': 21760, 'rewardStep': 0.8915920206420164, 'errorList': [], 'lossList': [0.0, -1.3423698604106904, 0.0, 1.7162885850667953, 0.0, 0.0, 0.0], 'rewardMean': 0.7725395103972976, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.452144551368
'totalSteps': 23040, 'rewardStep': 0.8126331292121936, 'errorList': [], 'lossList': [0.0, -1.3193630385398865, 0.0, 1.5398007058352232, 0.0, 0.0, 0.0], 'rewardMean': 0.7933444191576996, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.9719039731044
'totalSteps': 24320, 'rewardStep': 0.8004464174192424, 'errorList': [], 'lossList': [0.0, -1.2745423656702042, 0.0, 1.3387599301710724, 0.0, 0.0, 0.0], 'rewardMean': 0.824029651380509, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.9884287289287
'totalSteps': 25600, 'rewardStep': 0.9520932882069121, 'errorList': [0.05546852809630659, 0.06862169963408718, 0.08860936298852691, 0.046423030700313256, 0.059019668298467894, 0.057800815196189875, 0.0697964056714602, 0.050551619145801543, 0.07935936999461628, 0.04222959370554388, 0.08559118529606793, 0.06683613983508044, 0.051964391831500055, 0.07423323011984158, 0.072858033682909, 0.08518060709757773, 0.10074270379733397, 0.043022685086671064, 0.09615168856083298, 0.06208525854339013, 0.09398683844527647, 0.061540057175722376, 0.11150889947431161, 0.05404805285163583, 0.08629911768241239, 0.10317723695900503, 0.07210427875908838, 0.06228203335965039, 0.12094485634826319, 0.07433366517644233, 0.04993999995093061, 0.051177309930373165, 0.09099576626908702, 0.07804006390038062, 0.06288014679490186, 0.1305847645248382, 0.05523030873328668, 0.07724694857689769, 0.05263205459114673, 0.05045893860525199, 0.0843823999287179, 0.0425205623281493, 0.04050843109668544, 0.09441495232465719, 0.04304541477559416, 0.07854405626776975, 0.0652516400862293, 0.06998033225466839, 0.08319833036203618, 0.04135865950083775], 'lossList': [0.0, -1.226827204823494, 0.0, 1.002497995533049, 0.0, 0.0, 0.0], 'rewardMean': 0.832409745419324, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.463038634122, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=83.6
