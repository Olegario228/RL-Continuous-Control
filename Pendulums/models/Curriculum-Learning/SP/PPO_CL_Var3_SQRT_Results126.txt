#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 126
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.6252940609884929, 'errorList': [], 'lossList': [0.0, -1.4201174807548522, 0.0, 35.02714551925659, 0.0, 0.0, 0.0], 'rewardMean': 0.7201720969149501, 'totalEpisodes': 64, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.70746959291012
'totalSteps': 3840, 'rewardStep': 0.5391507809334588, 'errorList': [], 'lossList': [0.0, -1.404276322722435, 0.0, 49.211414661407474, 0.0, 0.0, 0.0], 'rewardMean': 0.659831658254453, 'totalEpisodes': 101, 'stepsPerEpisode': 16, 'rewardPerEpisode': 8.856354770116159
'totalSteps': 5120, 'rewardStep': 0.8523034132711403, 'errorList': [], 'lossList': [0.0, -1.3930946618318558, 0.0, 48.16872729301453, 0.0, 0.0, 0.0], 'rewardMean': 0.7079495970086248, 'totalEpisodes': 116, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.871240581526515
'totalSteps': 6400, 'rewardStep': 0.5326003699520285, 'errorList': [], 'lossList': [0.0, -1.379226154088974, 0.0, 46.99957187175751, 0.0, 0.0, 0.0], 'rewardMean': 0.6728797515973055, 'totalEpisodes': 124, 'stepsPerEpisode': 75, 'rewardPerEpisode': 60.23872951974412
'totalSteps': 7680, 'rewardStep': 0.7405531675417041, 'errorList': [], 'lossList': [0.0, -1.368331316113472, 0.0, 28.753139481544494, 0.0, 0.0, 0.0], 'rewardMean': 0.6841586542547052, 'totalEpisodes': 126, 'stepsPerEpisode': 493, 'rewardPerEpisode': 404.35920588419833
'totalSteps': 8960, 'rewardStep': 0.8928738295275082, 'errorList': [], 'lossList': [0.0, -1.3714103800058366, 0.0, 64.93736468315124, 0.0, 0.0, 0.0], 'rewardMean': 0.7139751078651057, 'totalEpisodes': 136, 'stepsPerEpisode': 267, 'rewardPerEpisode': 199.44933668288152
'totalSteps': 10240, 'rewardStep': 0.7841747838564452, 'errorList': [], 'lossList': [0.0, -1.3673202085494995, 0.0, 24.598587243556977, 0.0, 0.0, 0.0], 'rewardMean': 0.7227500673640233, 'totalEpisodes': 143, 'stepsPerEpisode': 101, 'rewardPerEpisode': 81.74825519647955
'totalSteps': 11520, 'rewardStep': 0.5289506526945487, 'errorList': [], 'lossList': [0.0, -1.3589736312627791, 0.0, 32.98266894340515, 0.0, 0.0, 0.0], 'rewardMean': 0.701216799067415, 'totalEpisodes': 155, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.765239165469339
'totalSteps': 12800, 'rewardStep': 0.7321110261742644, 'errorList': [], 'lossList': [0.0, -1.3369464486837388, 0.0, 37.685524067878724, 0.0, 0.0, 0.0], 'rewardMean': 0.7043062217781, 'totalEpisodes': 165, 'stepsPerEpisode': 148, 'rewardPerEpisode': 115.28955132351942
'totalSteps': 14080, 'rewardStep': 0.7929097420237858, 'errorList': [], 'lossList': [0.0, -1.298723248243332, 0.0, 16.227665841579437, 0.0, 0.0, 0.0], 'rewardMean': 0.7020921826963378, 'totalEpisodes': 170, 'stepsPerEpisode': 232, 'rewardPerEpisode': 195.92789718103157
'totalSteps': 15360, 'rewardStep': 0.8828285818809557, 'errorList': [], 'lossList': [0.0, -1.276627636551857, 0.0, 6.737460024356842, 0.0, 0.0, 0.0], 'rewardMean': 0.7278456347855841, 'totalEpisodes': 174, 'stepsPerEpisode': 56, 'rewardPerEpisode': 47.12299129937012
'totalSteps': 16640, 'rewardStep': 0.8237435848083312, 'errorList': [], 'lossList': [0.0, -1.2649394327402115, 0.0, 4.483673105835915, 0.0, 0.0, 0.0], 'rewardMean': 0.7563049151730712, 'totalEpisodes': 177, 'stepsPerEpisode': 124, 'rewardPerEpisode': 106.50384538152532
'totalSteps': 17920, 'rewardStep': 0.5029670109009636, 'errorList': [], 'lossList': [0.0, -1.2463575875759125, 0.0, 5.941778712272644, 0.0, 0.0, 0.0], 'rewardMean': 0.7213712749360536, 'totalEpisodes': 180, 'stepsPerEpisode': 360, 'rewardPerEpisode': 297.2317576010456
'totalSteps': 19200, 'rewardStep': 0.9052089746453937, 'errorList': [], 'lossList': [0.0, -1.2302298390865325, 0.0, 3.6036470502614977, 0.0, 0.0, 0.0], 'rewardMean': 0.7586321354053901, 'totalEpisodes': 184, 'stepsPerEpisode': 79, 'rewardPerEpisode': 67.69930189135381
'totalSteps': 20480, 'rewardStep': 0.4653709102176209, 'errorList': [], 'lossList': [0.0, -1.208129951953888, 0.0, 3.6117790693044665, 0.0, 0.0, 0.0], 'rewardMean': 0.7311139096729817, 'totalEpisodes': 186, 'stepsPerEpisode': 443, 'rewardPerEpisode': 371.5955707935686
'totalSteps': 21760, 'rewardStep': 0.7363022730913964, 'errorList': [], 'lossList': [0.0, -1.2017428958415985, 0.0, 4.045922865867615, 0.0, 0.0, 0.0], 'rewardMean': 0.7154567540293705, 'totalEpisodes': 189, 'stepsPerEpisode': 84, 'rewardPerEpisode': 73.03057301580581
'totalSteps': 23040, 'rewardStep': 0.490016032205656, 'errorList': [], 'lossList': [0.0, -1.1981556802988051, 0.0, 2.0797456219792365, 0.0, 0.0, 0.0], 'rewardMean': 0.6860408788642915, 'totalEpisodes': 191, 'stepsPerEpisode': 401, 'rewardPerEpisode': 273.7487848696534
'totalSteps': 24320, 'rewardStep': 0.8975866319628076, 'errorList': [], 'lossList': [0.0, -1.1891977536678313, 0.0, 3.2758451288938524, 0.0, 0.0, 0.0], 'rewardMean': 0.7229044767911175, 'totalEpisodes': 193, 'stepsPerEpisode': 561, 'rewardPerEpisode': 477.18286070614005
'totalSteps': 25600, 'rewardStep': 0.8437519727953863, 'errorList': [], 'lossList': [0.0, -1.165621773004532, 0.0, 2.847853308916092, 0.0, 0.0, 0.0], 'rewardMean': 0.7340685714532298, 'totalEpisodes': 195, 'stepsPerEpisode': 253, 'rewardPerEpisode': 221.68293345928024
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=64.92
