#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 127
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.7921265309814124, 'errorList': [], 'lossList': [0.0, -1.4195195293426515, 0.0, 34.44254495620728, 0.0, 0.0, 0.0], 'rewardMean': 0.8038819363825609, 'totalEpisodes': 79, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.506737453108856
'totalSteps': 3840, 'rewardStep': 0.9170308431198162, 'errorList': [], 'lossList': [0.0, -1.3931816983222962, 0.0, 41.85848208427429, 0.0, 0.0, 0.0], 'rewardMean': 0.8415982386283126, 'totalEpisodes': 105, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.553389014961521
'totalSteps': 5120, 'rewardStep': 0.7955102560568722, 'errorList': [], 'lossList': [0.0, -1.3548889887332916, 0.0, 49.0325679397583, 0.0, 0.0, 0.0], 'rewardMean': 0.8300762429854526, 'totalEpisodes': 123, 'stepsPerEpisode': 48, 'rewardPerEpisode': 39.66331192122029
'totalSteps': 6400, 'rewardStep': 0.6806879412221715, 'errorList': [], 'lossList': [0.0, -1.3336199790239334, 0.0, 47.9946066570282, 0.0, 0.0, 0.0], 'rewardMean': 0.8001985826327964, 'totalEpisodes': 134, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.691649013571496
'totalSteps': 7680, 'rewardStep': 0.9211851897493419, 'errorList': [], 'lossList': [0.0, -1.3134174185991287, 0.0, 75.76934635162354, 0.0, 0.0, 0.0], 'rewardMean': 0.8203630171522206, 'totalEpisodes': 145, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.66005692183977
'totalSteps': 8960, 'rewardStep': 0.6600761620657911, 'errorList': [], 'lossList': [0.0, -1.3138440555334092, 0.0, 69.99620909690857, 0.0, 0.0, 0.0], 'rewardMean': 0.7974648949970164, 'totalEpisodes': 154, 'stepsPerEpisode': 77, 'rewardPerEpisode': 56.899570619823194
'totalSteps': 10240, 'rewardStep': 0.6877152103879456, 'errorList': [], 'lossList': [0.0, -1.3145234447717666, 0.0, 59.773736295700076, 0.0, 0.0, 0.0], 'rewardMean': 0.7837461844208826, 'totalEpisodes': 163, 'stepsPerEpisode': 172, 'rewardPerEpisode': 140.6067671455217
'totalSteps': 11520, 'rewardStep': 0.8004861260260318, 'errorList': [], 'lossList': [0.0, -1.319708393216133, 0.0, 70.29480246543885, 0.0, 0.0, 0.0], 'rewardMean': 0.7856061779325658, 'totalEpisodes': 172, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.27116341574308
'totalSteps': 12800, 'rewardStep': 0.4522792375417895, 'errorList': [], 'lossList': [0.0, -1.3280346131324767, 0.0, 29.626197328567503, 0.0, 0.0, 0.0], 'rewardMean': 0.7522734838934882, 'totalEpisodes': 181, 'stepsPerEpisode': 90, 'rewardPerEpisode': 63.7117081188833
'totalSteps': 14080, 'rewardStep': 0.7714809709300238, 'errorList': [], 'lossList': [0.0, -1.3229802525043488, 0.0, 11.977239873409271, 0.0, 0.0, 0.0], 'rewardMean': 0.7478578468081196, 'totalEpisodes': 188, 'stepsPerEpisode': 95, 'rewardPerEpisode': 77.99139487258958
'totalSteps': 15360, 'rewardStep': 0.8545041295312636, 'errorList': [], 'lossList': [0.0, -1.312007360458374, 0.0, 25.825348472595216, 0.0, 0.0, 0.0], 'rewardMean': 0.7540956066631047, 'totalEpisodes': 196, 'stepsPerEpisode': 91, 'rewardPerEpisode': 76.8810577896889
'totalSteps': 16640, 'rewardStep': 0.8132493213071147, 'errorList': [], 'lossList': [0.0, -1.3026298666000367, 0.0, 7.492577506303787, 0.0, 0.0, 0.0], 'rewardMean': 0.7437174544818346, 'totalEpisodes': 201, 'stepsPerEpisode': 129, 'rewardPerEpisode': 111.64488662508437
'totalSteps': 17920, 'rewardStep': 0.7759791693811237, 'errorList': [], 'lossList': [0.0, -1.3060437846183777, 0.0, 6.702704510688782, 0.0, 0.0, 0.0], 'rewardMean': 0.7417643458142598, 'totalEpisodes': 205, 'stepsPerEpisode': 140, 'rewardPerEpisode': 119.77833523347344
'totalSteps': 19200, 'rewardStep': 0.8568798992332586, 'errorList': [], 'lossList': [0.0, -1.3048485112190247, 0.0, 11.878538650870324, 0.0, 0.0, 0.0], 'rewardMean': 0.7593835416153685, 'totalEpisodes': 206, 'stepsPerEpisode': 759, 'rewardPerEpisode': 647.3070285750681
'totalSteps': 20480, 'rewardStep': 0.6948142586584447, 'errorList': [], 'lossList': [0.0, -1.2744222766160964, 0.0, 12.702645045518874, 0.0, 0.0, 0.0], 'rewardMean': 0.7367464485062787, 'totalEpisodes': 207, 'stepsPerEpisode': 442, 'rewardPerEpisode': 350.60944367654014
'totalSteps': 21760, 'rewardStep': 0.8675957368100284, 'errorList': [], 'lossList': [0.0, -1.2472859519720076, 0.0, 2.7107337594032286, 0.0, 0.0, 0.0], 'rewardMean': 0.7574984059807025, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.7538680149614
'totalSteps': 23040, 'rewardStep': 0.9711999953954981, 'errorList': [0.04520350408270108, 0.05930398389211351, 0.06693970726879783, 0.051896319998281745, 0.06689599784177176, 0.07882632729310941, 0.06438159074043719, 0.09067406863332754, 0.11323625610315702, 0.06072406931813351, 0.12473264925924263, 0.060634564960383976, 0.06951517642153222, 0.10300783375506241, 0.0554976110853399, 0.04865364064257333, 0.059495317437040424, 0.05018252671788538, 0.06089306935155427, 0.06691379789670507, 0.11609305126256007, 0.085327824159436, 0.04634586342344533, 0.06692098737557264, 0.055637042847239054, 0.06627626634131119, 0.061568111863375026, 0.06002979208454107, 0.06143286614143038, 0.05211444896901815, 0.06295698095067805, 0.0916535954392537, 0.04660001736052163, 0.0538346416520737, 0.11283597161083697, 0.05770866175402333, 0.12149274703437304, 0.04450824266079799, 0.050353125953349995, 0.08658851187073814, 0.06200212848995429, 0.06362445543549333, 0.06800994353099934, 0.07592355924641543, 0.07338544824394859, 0.052554927510108214, 0.051462868650329126, 0.05079654925351508, 0.07951107386825276, 0.11112790345658508], 'lossList': [0.0, -1.2017635881900788, 0.0, 1.5859152051061391, 0.0, 0.0, 0.0], 'rewardMean': 0.7858468844814577, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.1129787666612, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=77.91
