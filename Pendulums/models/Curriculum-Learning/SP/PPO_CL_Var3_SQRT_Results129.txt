#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 129
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.6511928255399444, 'errorList': [], 'lossList': [0.0, -1.4017391800880432, 0.0, 33.81525281906128, 0.0, 0.0, 0.0], 'rewardMean': 0.7861414798456758, 'totalEpisodes': 80, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.393665225085524
'totalSteps': 3840, 'rewardStep': 0.7901013856067133, 'errorList': [], 'lossList': [0.0, -1.384725484251976, 0.0, 44.94307264328003, 0.0, 0.0, 0.0], 'rewardMean': 0.7874614484326883, 'totalEpisodes': 108, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.863383751632657
'totalSteps': 5120, 'rewardStep': 0.9040835826287781, 'errorList': [], 'lossList': [0.0, -1.3656670868396759, 0.0, 50.01818347930908, 0.0, 0.0, 0.0], 'rewardMean': 0.8166169819817107, 'totalEpisodes': 123, 'stepsPerEpisode': 45, 'rewardPerEpisode': 38.22407961365878
'totalSteps': 6400, 'rewardStep': 0.7390204979495992, 'errorList': [], 'lossList': [0.0, -1.359545939564705, 0.0, 48.86802339553833, 0.0, 0.0, 0.0], 'rewardMean': 0.8010976851752885, 'totalEpisodes': 131, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.269517557409696
'totalSteps': 7680, 'rewardStep': 0.9123707873912653, 'errorList': [], 'lossList': [0.0, -1.3471442806720733, 0.0, 65.48460439682007, 0.0, 0.0, 0.0], 'rewardMean': 0.8196432022112846, 'totalEpisodes': 141, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.118101243335957
'totalSteps': 8960, 'rewardStep': 0.7585541189919174, 'errorList': [], 'lossList': [0.0, -1.3406735366582871, 0.0, 99.81739049911499, 0.0, 0.0, 0.0], 'rewardMean': 0.8109161903228035, 'totalEpisodes': 154, 'stepsPerEpisode': 215, 'rewardPerEpisode': 176.42782478657222
'totalSteps': 10240, 'rewardStep': 0.593092483296106, 'errorList': [], 'lossList': [0.0, -1.3335542124509812, 0.0, 40.53887157917023, 0.0, 0.0, 0.0], 'rewardMean': 0.7836882269444663, 'totalEpisodes': 158, 'stepsPerEpisode': 399, 'rewardPerEpisode': 278.78222314614004
'totalSteps': 11520, 'rewardStep': 0.27612004849613214, 'errorList': [], 'lossList': [0.0, -1.3222733610868453, 0.0, 64.39313918113709, 0.0, 0.0, 0.0], 'rewardMean': 0.7272917626724291, 'totalEpisodes': 166, 'stepsPerEpisode': 102, 'rewardPerEpisode': 66.67268938173589
'totalSteps': 12800, 'rewardStep': 0.4694235852863439, 'errorList': [], 'lossList': [0.0, -1.297277770638466, 0.0, 110.52067331314088, 0.0, 0.0, 0.0], 'rewardMean': 0.7015049449338207, 'totalEpisodes': 177, 'stepsPerEpisode': 65, 'rewardPerEpisode': 43.859697607713066
'totalSteps': 14080, 'rewardStep': 0.8434098788534877, 'errorList': [], 'lossList': [0.0, -1.2798362505435943, 0.0, 64.66820248603821, 0.0, 0.0, 0.0], 'rewardMean': 0.6937369194040288, 'totalEpisodes': 183, 'stepsPerEpisode': 174, 'rewardPerEpisode': 143.47240748109988
'totalSteps': 15360, 'rewardStep': 0.80397615986058, 'errorList': [], 'lossList': [0.0, -1.2790613657236098, 0.0, 27.63637508392334, 0.0, 0.0, 0.0], 'rewardMean': 0.7090152528360923, 'totalEpisodes': 188, 'stepsPerEpisode': 118, 'rewardPerEpisode': 98.04746143430728
'totalSteps': 16640, 'rewardStep': 0.39049291002806075, 'errorList': [], 'lossList': [0.0, -1.2793210709095002, 0.0, 29.888292889595032, 0.0, 0.0, 0.0], 'rewardMean': 0.6690544052782271, 'totalEpisodes': 191, 'stepsPerEpisode': 445, 'rewardPerEpisode': 366.3739587502999
'totalSteps': 17920, 'rewardStep': 0.8247842951547085, 'errorList': [], 'lossList': [0.0, -1.2707243406772613, 0.0, 6.838331207036972, 0.0, 0.0, 0.0], 'rewardMean': 0.6611244765308201, 'totalEpisodes': 192, 'stepsPerEpisode': 516, 'rewardPerEpisode': 373.5265566749276
'totalSteps': 19200, 'rewardStep': 0.5594049444066901, 'errorList': [], 'lossList': [0.0, -1.2482857155799865, 0.0, 3.1420307701826093, 0.0, 0.0, 0.0], 'rewardMean': 0.6431629211765293, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 890.9374400756255
'totalSteps': 20480, 'rewardStep': 0.8186698198796868, 'errorList': [], 'lossList': [0.0, -1.2240660923719406, 0.0, 3.7811213839054107, 0.0, 0.0, 0.0], 'rewardMean': 0.6337928244253714, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 884.9431317569059
'totalSteps': 21760, 'rewardStep': 0.7100196826233698, 'errorList': [], 'lossList': [0.0, -1.2083462244272232, 0.0, 1.7450227896869182, 0.0, 0.0, 0.0], 'rewardMean': 0.6289393807885165, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.8726994430564
'totalSteps': 23040, 'rewardStep': 0.7485362347628427, 'errorList': [], 'lossList': [0.0, -1.197997567653656, 0.0, 2.30999899238348, 0.0, 0.0, 0.0], 'rewardMean': 0.6444837559351903, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 978.4798885662012
'totalSteps': 24320, 'rewardStep': 0.8956313326875509, 'errorList': [], 'lossList': [0.0, -1.1988798141479493, 0.0, 1.5319730326160788, 0.0, 0.0, 0.0], 'rewardMean': 0.7064348843543321, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.4962191672062
'totalSteps': 25600, 'rewardStep': 0.9638737203242868, 'errorList': [0.02527367248114185, 0.06775852773966506, 0.04373826596399779, 0.07445662966010405, 0.06662458012584772, 0.03214626207829914, 0.05954097949948679, 0.04594969950266933, 0.04474027911679792, 0.04160603466840871, 0.04658757114849477, 0.052845629096141794, 0.05173063160082602, 0.027963193438209282, 0.04887617643836837, 0.04938886540290263, 0.028340588246243772, 0.06341749693813813, 0.04724138350375977, 0.026261286622062393, 0.04592966185911696, 0.04873860139416342, 0.04420967563144113, 0.05806347906013405, 0.034265893138892535, 0.043311844897355734, 0.04166051365392024, 0.054884143699275335, 0.026411426898085077, 0.04305567667807611, 0.05068526545238018, 0.06094272294811841, 0.04415300674596534, 0.04561034657521635, 0.06850318000722225, 0.02792222602406564, 0.054488628935385634, 0.06991215322238628, 0.04695618729752523, 0.04918947183706558, 0.04426529254297762, 0.02533504659246536, 0.028775869313610866, 0.06472266452992627, 0.039711773227241856, 0.053682078063244036, 0.03346598401214913, 0.052775475526223886, 0.04669045182598797, 0.04814585799805858], 'lossList': [0.0, -1.1774711960554123, 0.0, 1.1232567228749395, 0.0, 0.0, 0.0], 'rewardMean': 0.7558798978581265, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.6532092264144, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.3
