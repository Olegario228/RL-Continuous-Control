#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 130
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.7849817880687745, 'errorList': [], 'lossList': [0.0, -1.409587897658348, 0.0, 32.302010498046876, 0.0, 0.0, 0.0], 'rewardMean': 0.6978259435053125, 'totalEpisodes': 28, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.798374377293545
'totalSteps': 3840, 'rewardStep': 0.5429890915297098, 'errorList': [], 'lossList': [0.0, -1.4076368719339372, 0.0, 36.55156212806702, 0.0, 0.0, 0.0], 'rewardMean': 0.6462136595134449, 'totalEpisodes': 44, 'stepsPerEpisode': 170, 'rewardPerEpisode': 114.1368085026069
'totalSteps': 5120, 'rewardStep': 0.8458782660853822, 'errorList': [], 'lossList': [0.0, -1.3990495544672013, 0.0, 35.22925978660584, 0.0, 0.0, 0.0], 'rewardMean': 0.6961298111564292, 'totalEpisodes': 56, 'stepsPerEpisode': 67, 'rewardPerEpisode': 56.8394109862924
'totalSteps': 6400, 'rewardStep': 0.5322160960001021, 'errorList': [], 'lossList': [0.0, -1.3950532335042953, 0.0, 34.48793956279755, 0.0, 0.0, 0.0], 'rewardMean': 0.6633470681251639, 'totalEpisodes': 65, 'stepsPerEpisode': 264, 'rewardPerEpisode': 183.67822025717635
'totalSteps': 7680, 'rewardStep': 0.5611126205208581, 'errorList': [], 'lossList': [0.0, -1.378215698003769, 0.0, 26.652135105133056, 0.0, 0.0, 0.0], 'rewardMean': 0.6463079935244461, 'totalEpisodes': 70, 'stepsPerEpisode': 314, 'rewardPerEpisode': 217.37211312856462
'totalSteps': 8960, 'rewardStep': 0.6656656556354565, 'errorList': [], 'lossList': [0.0, -1.3553645533323289, 0.0, 100.89511016845704, 0.0, 0.0, 0.0], 'rewardMean': 0.6490733738260192, 'totalEpisodes': 83, 'stepsPerEpisode': 249, 'rewardPerEpisode': 199.4122313970833
'totalSteps': 10240, 'rewardStep': 0.9081147363111565, 'errorList': [], 'lossList': [0.0, -1.3620710182189941, 0.0, 137.45868854522706, 0.0, 0.0, 0.0], 'rewardMean': 0.6814535441366614, 'totalEpisodes': 104, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.7088821163292285
'totalSteps': 11520, 'rewardStep': 0.9595794712876216, 'errorList': [28.151872174082882, 23.372852559093854, 18.169305938759187, 36.80578273138956, 18.564763241082048, 23.53134132689418, 23.57117336559672, 23.38137802846185, 18.618329174721787, 23.63341834401406, 23.23695177701033, 23.46073519680553, 23.352724139835168, 23.23810452583582, 34.5070232817014, 18.61010539810323, 23.504549256999468, 23.610157132658983, 23.577009934411855, 18.29198095358538, 29.45226936911302, 23.64162165015843, 23.449620112549045, 23.672649709309525, 18.3327895711362, 23.564335497414714, 12.132072272450106, 23.437472356174744, 23.36160184346004, 23.537562158459167, 23.616042476706912, 33.93426499668758, 34.48059542566284, 23.741891727670485, 38.75989621470233, 23.541032642565433, 23.390599053453805, 36.45722155317783, 18.380896764385987, 30.078001531585976, 23.29173856736695, 20.84502304844106, 23.357411783435623, 23.34246714131203, 23.608325896733973, 41.4164082627084, 25.012995231721227, 23.46124844341676, 23.598449021884516, 18.408645365568702], 'lossList': [0.0, -1.3681280738115311, 0.0, 66.89162849426269, 0.0, 0.0, 0.0], 'rewardMean': 0.7123564249312125, 'totalEpisodes': 127, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9595794712876216, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.817113386392775, 'errorList': [], 'lossList': [0.0, -1.3632047599554062, 0.0, 42.260531406402585, 0.0, 0.0, 0.0], 'rewardMean': 0.7228321210773687, 'totalEpisodes': 139, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.5993083991831
'totalSteps': 14080, 'rewardStep': 0.6243710894957543, 'errorList': [], 'lossList': [0.0, -1.352143030166626, 0.0, 11.0506726872921, 0.0, 0.0, 0.0], 'rewardMean': 0.7242022201327589, 'totalEpisodes': 147, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.91514841027855
'totalSteps': 15360, 'rewardStep': 0.7047301306716205, 'errorList': [], 'lossList': [0.0, -1.3399380493164061, 0.0, 32.87902107000351, 0.0, 0.0, 0.0], 'rewardMean': 0.7161770543930437, 'totalEpisodes': 152, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.993174294025785
'totalSteps': 16640, 'rewardStep': 0.7476913816825835, 'errorList': [], 'lossList': [0.0, -1.3417838573455811, 0.0, 12.307488873004914, 0.0, 0.0, 0.0], 'rewardMean': 0.7366472834083311, 'totalEpisodes': 156, 'stepsPerEpisode': 303, 'rewardPerEpisode': 212.74193375621252
'totalSteps': 17920, 'rewardStep': 0.7065462336437461, 'errorList': [], 'lossList': [0.0, -1.3314848840236664, 0.0, 4.689298395216465, 0.0, 0.0, 0.0], 'rewardMean': 0.7227140801641675, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 879.4101125912829
'totalSteps': 19200, 'rewardStep': 0.8722133418508681, 'errorList': [], 'lossList': [0.0, -1.2949068933725356, 0.0, 4.758498176634312, 0.0, 0.0, 0.0], 'rewardMean': 0.756713804749244, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1062.5190092430332
'totalSteps': 20480, 'rewardStep': 0.9381790026210355, 'errorList': [0.018320378021296005, 0.013703641790697175, 0.018045165550336213, 0.047207082670356566, 0.027491132930783174, 0.045011632220668195, 0.013159474546685935, 0.052021261165106615, 0.015389425021259678, 0.03279825708213424, 0.044476114955405564, 0.03816982498733057, 0.05191231640664052, 0.02235586223557501, 0.03260090116663373, 0.05335444922602438, 0.03346885083766616, 0.05706518409142418, 0.021666433607117775, 0.045801648182500654, 0.04822405253612425, 0.020100064477518224, 0.03915122961642805, 0.02742733353801703, 0.018486463790295893, 0.016860153568675602, 0.028551193069946527, 0.03427583342555148, 0.05597117930996006, 0.03836814714866892, 0.011454651183076163, 0.04149465190074778, 0.038543321655503365, 0.03618687954405405, 0.013181078917667973, 0.040723859328705175, 0.04677772167946311, 0.04641598615893444, 0.052928386890252095, 0.02272969339748898, 0.03339122425284365, 0.049405351356527474, 0.03419523835750404, 0.036190340601320516, 0.03809309040389858, 0.030149126248419332, 0.011647281286024998, 0.057738906341644275, 0.06801465213360888, 0.043063853247620736], 'lossList': [0.0, -1.2624200773239136, 0.0, 4.380532637909055, 0.0, 0.0, 0.0], 'rewardMean': 0.7944204429592617, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.8004209956505, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=91.24
