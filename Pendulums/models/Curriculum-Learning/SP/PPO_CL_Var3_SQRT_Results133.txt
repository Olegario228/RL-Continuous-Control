#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 133
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.6299799948271078, 'errorList': [], 'lossList': [0.0, -1.4124759954214097, 0.0, 29.243755507469178, 0.0, 0.0, 0.0], 'rewardMean': 0.48716759318628133, 'totalEpisodes': 24, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.94139685991246
'totalSteps': 3840, 'rewardStep': 0.891990588138865, 'errorList': [], 'lossList': [0.0, -1.3960252106189728, 0.0, 37.097158532142636, 0.0, 0.0, 0.0], 'rewardMean': 0.6221085915038093, 'totalEpisodes': 35, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.502675198244944
'totalSteps': 5120, 'rewardStep': 0.640997536236457, 'errorList': [], 'lossList': [0.0, -1.3871455526351928, 0.0, 45.71960696220398, 0.0, 0.0, 0.0], 'rewardMean': 0.6268308276869712, 'totalEpisodes': 44, 'stepsPerEpisode': 182, 'rewardPerEpisode': 135.17739874213927
'totalSteps': 6400, 'rewardStep': 0.5702970955092962, 'errorList': [], 'lossList': [0.0, -1.391630094051361, 0.0, 58.32172057151794, 0.0, 0.0, 0.0], 'rewardMean': 0.6155240812514362, 'totalEpisodes': 54, 'stepsPerEpisode': 165, 'rewardPerEpisode': 126.82524816526767
'totalSteps': 7680, 'rewardStep': 0.8838913859097065, 'errorList': [], 'lossList': [0.0, -1.3807669794559478, 0.0, 71.86187258720398, 0.0, 0.0, 0.0], 'rewardMean': 0.6602519653611479, 'totalEpisodes': 66, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.09239680072379
'totalSteps': 8960, 'rewardStep': 0.9303470717638643, 'errorList': [], 'lossList': [0.0, -1.3684236323833465, 0.0, 83.65098028182983, 0.0, 0.0, 0.0], 'rewardMean': 0.698836980561536, 'totalEpisodes': 76, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.85664590862311
'totalSteps': 10240, 'rewardStep': 0.6871861226674305, 'errorList': [], 'lossList': [0.0, -1.3571447765827178, 0.0, 32.86190075874329, 0.0, 0.0, 0.0], 'rewardMean': 0.6973806233247728, 'totalEpisodes': 82, 'stepsPerEpisode': 407, 'rewardPerEpisode': 314.9796912249292
'totalSteps': 11520, 'rewardStep': 0.6126144775154199, 'errorList': [], 'lossList': [0.0, -1.3536261755228043, 0.0, 38.677877292633056, 0.0, 0.0, 0.0], 'rewardMean': 0.6879621626792891, 'totalEpisodes': 87, 'stepsPerEpisode': 165, 'rewardPerEpisode': 142.30635406883673
'totalSteps': 12800, 'rewardStep': 0.669834624209884, 'errorList': [], 'lossList': [0.0, -1.3518925458192825, 0.0, 64.46061256408692, 0.0, 0.0, 0.0], 'rewardMean': 0.6861494088323485, 'totalEpisodes': 93, 'stepsPerEpisode': 203, 'rewardPerEpisode': 137.3066550120873
'totalSteps': 14080, 'rewardStep': 0.803955263945137, 'errorList': [], 'lossList': [0.0, -1.3318393903970718, 0.0, 8.124868798255921, 0.0, 0.0, 0.0], 'rewardMean': 0.7321094160723167, 'totalEpisodes': 96, 'stepsPerEpisode': 287, 'rewardPerEpisode': 217.65264253695162
'totalSteps': 15360, 'rewardStep': 0.537112666953002, 'errorList': [], 'lossList': [0.0, -1.3031401115655898, 0.0, 22.745794166326522, 0.0, 0.0, 0.0], 'rewardMean': 0.7228226832849063, 'totalEpisodes': 100, 'stepsPerEpisode': 539, 'rewardPerEpisode': 423.3319390969777
'totalSteps': 16640, 'rewardStep': 0.8502642017728846, 'errorList': [], 'lossList': [0.0, -1.3099595630168914, 0.0, 19.085270280838014, 0.0, 0.0, 0.0], 'rewardMean': 0.7186500446483082, 'totalEpisodes': 103, 'stepsPerEpisode': 45, 'rewardPerEpisode': 34.633328328535406
'totalSteps': 17920, 'rewardStep': 0.880984165765334, 'errorList': [], 'lossList': [0.0, -1.3175262504816054, 0.0, 4.042454904764891, 0.0, 0.0, 0.0], 'rewardMean': 0.742648707601196, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1047.5380912596786
'totalSteps': 19200, 'rewardStep': 0.5713669116281317, 'errorList': [], 'lossList': [0.0, -1.3062674391269684, 0.0, 3.0700026777386666, 0.0, 0.0, 0.0], 'rewardMean': 0.7427556892130794, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.6660457283889
'totalSteps': 20480, 'rewardStep': 0.9095825159300415, 'errorList': [], 'lossList': [0.0, -1.2833761292696, 0.0, 2.3555737303197386, 0.0, 0.0, 0.0], 'rewardMean': 0.7453248022151129, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.415689091183
'totalSteps': 21760, 'rewardStep': 0.8165605351111949, 'errorList': [], 'lossList': [0.0, -1.245772442817688, 0.0, 1.437095363289118, 0.0, 0.0, 0.0], 'rewardMean': 0.733946148549846, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 981.8041843645001
'totalSteps': 23040, 'rewardStep': 0.7328144885385157, 'errorList': [], 'lossList': [0.0, -1.2244320404529572, 0.0, 0.8948652873188258, 0.0, 0.0, 0.0], 'rewardMean': 0.7385089851369546, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.5918026520733
'totalSteps': 24320, 'rewardStep': 0.8354508814469205, 'errorList': [], 'lossList': [0.0, -1.1767775070667268, 0.0, 1.0627367846667766, 0.0, 0.0, 0.0], 'rewardMean': 0.7607926255301045, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.6042847907886
'totalSteps': 25600, 'rewardStep': 0.9741017566941893, 'errorList': [0.05824321246380973, 0.06141175902372711, 0.025248495599403755, 0.030135915094441943, 0.06390627492221289, 0.03598326040193301, 0.05010044866242812, 0.05584529377624026, 0.05315688005445496, 0.05611107608747727, 0.040284201976387435, 0.06362931029030101, 0.040513452217217416, 0.04007551846865182, 0.027349709255387093, 0.05914178392560885, 0.061893470556545156, 0.05978040427870643, 0.03265926642948691, 0.033985509044285066, 0.04532006111629989, 0.04503830121800114, 0.03344667329263105, 0.025472267341287998, 0.05079743097670105, 0.0366762979356977, 0.05660785168226513, 0.061417185745021606, 0.055235145852317064, 0.028291181918794043, 0.045186789689370836, 0.028173649747685527, 0.05467708082505957, 0.07441995250359958, 0.025253351972182512, 0.0279963821742679, 0.047413766952182475, 0.050641626593174144, 0.04481768922261901, 0.04814848878392992, 0.03058288714846221, 0.048214905815962536, 0.05503849118157156, 0.05548629647744148, 0.02869337843118127, 0.07140303352841183, 0.07857327413773002, 0.03401826159901103, 0.06431157210684461, 0.0407763612816463], 'lossList': [0.0, -1.150721383690834, 0.0, 0.9249603843316436, 0.0, 0.0, 0.0], 'rewardMean': 0.7912193387785351, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.921467817564, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.55
