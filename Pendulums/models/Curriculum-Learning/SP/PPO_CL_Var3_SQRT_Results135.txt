#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 135
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.8388096989783025, 'errorList': [], 'lossList': [0.0, -1.4168127822875975, 0.0, 25.620506722927093, 0.0, 0.0, 0.0], 'rewardMean': 0.8193043297386328, 'totalEpisodes': 13, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.574297750450544
'totalSteps': 3840, 'rewardStep': 0.5691064199167316, 'errorList': [], 'lossList': [0.0, -1.4085798889398575, 0.0, 37.43269364356995, 0.0, 0.0, 0.0], 'rewardMean': 0.7359050264646657, 'totalEpisodes': 24, 'stepsPerEpisode': 167, 'rewardPerEpisode': 125.76991619979756
'totalSteps': 5120, 'rewardStep': 0.5045185204955294, 'errorList': [], 'lossList': [0.0, -1.398322126865387, 0.0, 20.801883454322816, 0.0, 0.0, 0.0], 'rewardMean': 0.6780583999723816, 'totalEpisodes': 28, 'stepsPerEpisode': 191, 'rewardPerEpisode': 126.65846892461364
'totalSteps': 6400, 'rewardStep': 0.44895082860349456, 'errorList': [], 'lossList': [0.0, -1.4032131552696228, 0.0, 43.41699471473694, 0.0, 0.0, 0.0], 'rewardMean': 0.6322368856986043, 'totalEpisodes': 36, 'stepsPerEpisode': 204, 'rewardPerEpisode': 139.86073478343926
'totalSteps': 7680, 'rewardStep': 0.7112571128985303, 'errorList': [], 'lossList': [0.0, -1.4055958849191665, 0.0, 41.55847469806671, 0.0, 0.0, 0.0], 'rewardMean': 0.6454069235652585, 'totalEpisodes': 41, 'stepsPerEpisode': 313, 'rewardPerEpisode': 214.93873055942572
'totalSteps': 8960, 'rewardStep': 0.6804002283785631, 'errorList': [], 'lossList': [0.0, -1.3920527410507202, 0.0, 33.56642389297485, 0.0, 0.0, 0.0], 'rewardMean': 0.6504059671100163, 'totalEpisodes': 46, 'stepsPerEpisode': 247, 'rewardPerEpisode': 201.75560664366594
'totalSteps': 10240, 'rewardStep': 0.5376799691138129, 'errorList': [], 'lossList': [0.0, -1.3699046957492829, 0.0, 171.74135692596437, 0.0, 0.0, 0.0], 'rewardMean': 0.6363152173604909, 'totalEpisodes': 63, 'stepsPerEpisode': 107, 'rewardPerEpisode': 83.88783978456728
'totalSteps': 11520, 'rewardStep': 0.7058429526859304, 'errorList': [], 'lossList': [0.0, -1.3545533871650697, 0.0, 121.52666484832764, 0.0, 0.0, 0.0], 'rewardMean': 0.6440405212855398, 'totalEpisodes': 86, 'stepsPerEpisode': 54, 'rewardPerEpisode': 47.71796645847485
'totalSteps': 12800, 'rewardStep': 0.7853853024811335, 'errorList': [], 'lossList': [0.0, -1.3405977272987366, 0.0, 50.13028882980347, 0.0, 0.0, 0.0], 'rewardMean': 0.6581749994050992, 'totalEpisodes': 97, 'stepsPerEpisode': 122, 'rewardPerEpisode': 105.09802628625678
'totalSteps': 14080, 'rewardStep': 0.5312166248322204, 'errorList': [], 'lossList': [0.0, -1.3157465547323226, 0.0, 15.176772377490998, 0.0, 0.0, 0.0], 'rewardMean': 0.6313167658384249, 'totalEpisodes': 104, 'stepsPerEpisode': 57, 'rewardPerEpisode': 43.28724771268691
'totalSteps': 15360, 'rewardStep': 0.5195637515949492, 'errorList': [], 'lossList': [0.0, -1.2838516664505004, 0.0, 21.598489999771118, 0.0, 0.0, 0.0], 'rewardMean': 0.5993921711000896, 'totalEpisodes': 111, 'stepsPerEpisode': 69, 'rewardPerEpisode': 49.06133864649708
'totalSteps': 16640, 'rewardStep': 0.9008771809234575, 'errorList': [], 'lossList': [0.0, -1.262537743449211, 0.0, 7.660130530595779, 0.0, 0.0, 0.0], 'rewardMean': 0.632569247200762, 'totalEpisodes': 116, 'stepsPerEpisode': 55, 'rewardPerEpisode': 45.42126666449693
'totalSteps': 17920, 'rewardStep': 0.7927260337596852, 'errorList': [], 'lossList': [0.0, -1.2520889413356782, 0.0, 6.509318580627442, 0.0, 0.0, 0.0], 'rewardMean': 0.6613899985271777, 'totalEpisodes': 118, 'stepsPerEpisode': 347, 'rewardPerEpisode': 265.05768846736254
'totalSteps': 19200, 'rewardStep': 0.8395471337595567, 'errorList': [], 'lossList': [0.0, -1.2463333421945573, 0.0, 37.527063331604005, 0.0, 0.0, 0.0], 'rewardMean': 0.7004496290427839, 'totalEpisodes': 121, 'stepsPerEpisode': 781, 'rewardPerEpisode': 664.641836577062
'totalSteps': 20480, 'rewardStep': 0.9420493176897295, 'errorList': [0.07246062494968897, 0.07338475537219323, 0.0818306450616761, 0.13075210581859803, 0.11968835028617984, 0.1410920937983773, 0.07326584585499753, 0.07795150636019092, 0.0733344683292973, 0.07196285447118646, 0.10748215041136612, 0.07241540320904084, 0.0716279871272987, 0.07236015019395516, 0.07336694519051935, 0.07130150442097609, 0.13266319572209326, 0.07116031401963774, 0.10790417159478118, 0.0719622221438411, 0.07179236356724057, 0.08117751202607684, 0.0720452028660798, 0.0725084654263092, 0.07279835422592101, 0.07848974285372223, 0.11382309521740283, 0.07741420041312624, 0.0714532134481055, 0.07233802309363888, 0.07316416484147935, 0.07200747524983357, 0.13332774866992572, 0.07212141248147169, 0.07320872350073557, 0.07179853895887532, 0.07166315701593305, 0.08603176381574737, 0.0714235475599687, 0.07294175033436247, 0.08356854035780259, 0.07174860915471264, 0.07221050003598077, 0.07193290360710575, 0.07346257146720596, 0.09074761652813892, 0.07319608652115761, 0.12070455955726474, 0.07086562134323326, 0.07158078679226203], 'lossList': [0.0, -1.2404796904325486, 0.0, 5.345746691226959, 0.0, 0.0, 0.0], 'rewardMean': 0.7235288495219039, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.689111927053, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=70.44
