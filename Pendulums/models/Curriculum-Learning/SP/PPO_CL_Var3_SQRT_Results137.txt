#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 137
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.6110730009560247, 'errorList': [], 'lossList': [0.0, -1.4434584736824037, 0.0, 29.377801847457885, 0.0, 0.0, 0.0], 'rewardMean': 0.5245517965809493, 'totalEpisodes': 21, 'stepsPerEpisode': 134, 'rewardPerEpisode': 80.626586281757
'totalSteps': 3840, 'rewardStep': 0.7698972221229259, 'errorList': [], 'lossList': [0.0, -1.4423158460855483, 0.0, 39.83457703590393, 0.0, 0.0, 0.0], 'rewardMean': 0.6063336050949415, 'totalEpisodes': 34, 'stepsPerEpisode': 32, 'rewardPerEpisode': 23.706018844310844
'totalSteps': 5120, 'rewardStep': 0.14594517547189995, 'errorList': [], 'lossList': [0.0, -1.4243553739786148, 0.0, 35.056974349021914, 0.0, 0.0, 0.0], 'rewardMean': 0.4912364976891811, 'totalEpisodes': 46, 'stepsPerEpisode': 194, 'rewardPerEpisode': 112.866312866355
'totalSteps': 6400, 'rewardStep': 0.9823065548505355, 'errorList': [], 'lossList': [0.0, -1.4088318663835526, 0.0, 40.53896966934204, 0.0, 0.0, 0.0], 'rewardMean': 0.589450509121452, 'totalEpisodes': 55, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9823065548505355
'totalSteps': 7680, 'rewardStep': 0.8849093379816659, 'errorList': [], 'lossList': [0.0, -1.395483837723732, 0.0, 62.10984812736511, 0.0, 0.0, 0.0], 'rewardMean': 0.6386936472648209, 'totalEpisodes': 65, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.84898590414454
'totalSteps': 8960, 'rewardStep': 0.9512811505170675, 'errorList': [], 'lossList': [0.0, -1.3890146839618682, 0.0, 38.16433726787567, 0.0, 0.0, 0.0], 'rewardMean': 0.6833490048722847, 'totalEpisodes': 71, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.8937769516079
'totalSteps': 10240, 'rewardStep': 0.5103050070795895, 'errorList': [], 'lossList': [0.0, -1.3789916563034057, 0.0, 73.43792304992675, 0.0, 0.0, 0.0], 'rewardMean': 0.6617185051481979, 'totalEpisodes': 78, 'stepsPerEpisode': 112, 'rewardPerEpisode': 94.09459743523364
'totalSteps': 11520, 'rewardStep': 0.77649788525205, 'errorList': [], 'lossList': [0.0, -1.387289257645607, 0.0, 40.18824022769928, 0.0, 0.0, 0.0], 'rewardMean': 0.6744717696041814, 'totalEpisodes': 86, 'stepsPerEpisode': 159, 'rewardPerEpisode': 133.53641290570192
'totalSteps': 12800, 'rewardStep': 0.9963515741648565, 'errorList': [0.9369951249249893, 0.42354954204067613, 0.9672356435841785, 0.40703356977365457, 0.40951372230568267, 0.9378172331001864, 0.504494867327769, 0.08849311574092232, 0.3090892246877909, 1.5195826304690117, 0.2245691113406413, 0.09508931033281916, 0.5921171819340781, 0.4733543022476134, 0.5219140876927513, 0.20336273157416251, 1.0775685180581336, 0.2510173538399228, 0.4042715231591934, 0.09523562891234819, 0.18104743531650203, 0.37805541041119123, 0.8973026940925297, 0.5879963822081604, 0.24386378189297633, 0.8003464667527082, 0.675930535832494, 0.0813674002298459, 0.4410727994230827, 0.5564648303520237, 0.43453852795327796, 0.5911070666184668, 0.4737939080860535, 1.0017696333636588, 0.7050606499850055, 0.5917514690071565, 0.46970630914143263, 0.2728395438102526, 0.4750606331641434, 0.6837218291106281, 0.22919532758755454, 0.4259346742781243, 1.1667419660346938, 0.8414697461396043, 0.767106606667452, 0.08228053717406017, 0.6762873332581912, 0.42007524839515586, 0.8325743468813402, 0.5359174742749052], 'lossList': [0.0, -1.3926362043619156, 0.0, 17.612171691656112, 0.0, 0.0, 0.0], 'rewardMean': 0.706659750060249, 'totalEpisodes': 90, 'stepsPerEpisode': 144, 'rewardPerEpisode': 120.25489081107396, 'successfulTests': 6
'totalSteps': 14080, 'rewardStep': 0.7757087854251058, 'errorList': [], 'lossList': [0.0, -1.3935720962285996, 0.0, 6.587685123085976, 0.0, 0.0, 0.0], 'rewardMean': 0.7404275693821722, 'totalEpisodes': 90, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1028.1180577241646
'totalSteps': 15360, 'rewardStep': 0.7510782078254181, 'errorList': [], 'lossList': [0.0, -1.425877661705017, 0.0, 14.801755261421203, 0.0, 0.0, 0.0], 'rewardMean': 0.7544280900691114, 'totalEpisodes': 96, 'stepsPerEpisode': 51, 'rewardPerEpisode': 38.57998814512329
'totalSteps': 16640, 'rewardStep': 0.8268792545353693, 'errorList': [], 'lossList': [0.0, -1.4410567247867585, 0.0, 10.2486971783638, 0.0, 0.0, 0.0], 'rewardMean': 0.7601262933103559, 'totalEpisodes': 101, 'stepsPerEpisode': 241, 'rewardPerEpisode': 207.34799988114892
'totalSteps': 17920, 'rewardStep': 0.6434055491637687, 'errorList': [], 'lossList': [0.0, -1.43140815615654, 0.0, 4.0140415012836455, 0.0, 0.0, 0.0], 'rewardMean': 0.8098723306795428, 'totalEpisodes': 101, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 964.326704783682
'totalSteps': 19200, 'rewardStep': 0.9626802446517329, 'errorList': [0.038669178120272374, 0.10953521912807535, 0.038544950229621316, 0.10302821270252266, 0.04853711972666853, 0.04638434416023659, 0.08449878728346352, 0.05961015048759712, 0.1205999865396138, 0.08153005049936704, 0.19848346138350656, 0.024325752745870614, 0.09686264032067897, 0.10029860233896776, 0.17018876093781304, 0.04713922423958537, 0.08502928792552462, 0.02967895436814577, 0.08155125708415824, 0.04718584977486796, 0.019704785839098866, 0.03436070538671647, 0.17371706567339606, 0.06517534896128166, 0.05478496372767037, 0.08616160813486612, 0.12544143260750273, 0.10529081224274786, 0.13301695732936916, 0.130366570250218, 0.09860643345693344, 0.17449343148208377, 0.1366335987766398, 0.06611673371707512, 0.0695070760952867, 0.03668144682486787, 0.10218869747674962, 0.028172081201960773, 0.07500900106610471, 0.041633190315691626, 0.0479512152811938, 0.08167566139818061, 0.16956226382250836, 0.06613673280594254, 0.11744841593618846, 0.038085194931166216, 0.07001130984002224, 0.0569524961005706, 0.11691888127783147, 0.1439951288494362], 'lossList': [0.0, -1.403976571559906, 0.0, 10.717840298414231, 0.0, 0.0, 0.0], 'rewardMean': 0.8079096996596624, 'totalEpisodes': 102, 'stepsPerEpisode': 757, 'rewardPerEpisode': 626.0652128896603, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=84.73
