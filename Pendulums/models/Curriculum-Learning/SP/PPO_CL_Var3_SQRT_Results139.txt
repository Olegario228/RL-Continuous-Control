#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 139
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8929956002650765, 'errorList': [], 'lossList': [0.0, -1.4241365104913712, 0.0, 28.15886905670166, 0.0, 0.0, 0.0], 'rewardMean': 0.78245550320421, 'totalEpisodes': 19, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.02475748275037
'totalSteps': 3840, 'rewardStep': 0.7558743391967467, 'errorList': [], 'lossList': [0.0, -1.4266163170337678, 0.0, 38.39810347557068, 0.0, 0.0, 0.0], 'rewardMean': 0.7735951152017222, 'totalEpisodes': 34, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.190479459031735
'totalSteps': 5120, 'rewardStep': 0.8458656144497135, 'errorList': [], 'lossList': [0.0, -1.4159033238887786, 0.0, 34.57073144435883, 0.0, 0.0, 0.0], 'rewardMean': 0.7916627400137201, 'totalEpisodes': 40, 'stepsPerEpisode': 40, 'rewardPerEpisode': 32.56112896184286
'totalSteps': 6400, 'rewardStep': 0.4537121792234198, 'errorList': [], 'lossList': [0.0, -1.4039936149120331, 0.0, 39.25941821575165, 0.0, 0.0, 0.0], 'rewardMean': 0.7240726278556601, 'totalEpisodes': 44, 'stepsPerEpisode': 349, 'rewardPerEpisode': 272.32847546833347
'totalSteps': 7680, 'rewardStep': 0.8334727345410055, 'errorList': [], 'lossList': [0.0, -1.3980595558881759, 0.0, 52.68834567070007, 0.0, 0.0, 0.0], 'rewardMean': 0.7423059789698843, 'totalEpisodes': 51, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.889708942474194
'totalSteps': 8960, 'rewardStep': 0.37411447254726293, 'errorList': [], 'lossList': [0.0, -1.3924235785007477, 0.0, 56.01913685798645, 0.0, 0.0, 0.0], 'rewardMean': 0.6897071923380812, 'totalEpisodes': 58, 'stepsPerEpisode': 331, 'rewardPerEpisode': 214.1262831697956
'totalSteps': 10240, 'rewardStep': 0.7265290236183789, 'errorList': [], 'lossList': [0.0, -1.3759132170677184, 0.0, 162.07657581329346, 0.0, 0.0, 0.0], 'rewardMean': 0.6943099212481184, 'totalEpisodes': 76, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.467318092009291
'totalSteps': 11520, 'rewardStep': 0.7954916294189256, 'errorList': [], 'lossList': [0.0, -1.3640402603149413, 0.0, 78.1448436164856, 0.0, 0.0, 0.0], 'rewardMean': 0.705552333267097, 'totalEpisodes': 103, 'stepsPerEpisode': 34, 'rewardPerEpisode': 30.3947358656559
'totalSteps': 12800, 'rewardStep': 0.8302486270879776, 'errorList': [], 'lossList': [0.0, -1.3487291234731673, 0.0, 47.90633152961731, 0.0, 0.0, 0.0], 'rewardMean': 0.7180219626491849, 'totalEpisodes': 124, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.732284804546042
'totalSteps': 14080, 'rewardStep': 0.5290185823166119, 'errorList': [], 'lossList': [0.0, -1.3411077880859374, 0.0, 14.534415979385376, 0.0, 0.0, 0.0], 'rewardMean': 0.7037322802665118, 'totalEpisodes': 132, 'stepsPerEpisode': 261, 'rewardPerEpisode': 194.13859141737146
'totalSteps': 15360, 'rewardStep': 0.9889743356693486, 'errorList': [2.436524726853355, 36.53427297297069, 37.39322438525495, 4.36493870848361, 23.43423400207344, 12.912559836935397, 31.801474562866847, 5.488223014387094, 60.26372457585369, 9.563899342544033, 76.87078598310362, 70.09150698569043, 39.674714227958944, 12.91578216973233, 3.2790603431282204, 81.43078057628921, 17.181721037688988, 60.55528917685863, 21.79149055832958, 5.0523896307283005, 13.412677537065656, 24.9082047580179, 8.729120484530126, 6.884354778214389, 8.550723804081322, 8.886966936695742, 6.442091989220241, 35.08675762086034, 31.623458481275847, 15.561029032141152, 7.645032732738418, 34.48154780964715, 26.651849204033898, 48.51117721398754, 30.251258378811414, 10.687642020380022, 60.357129065089296, 25.266506800982278, 23.900246138389424, 55.91314886861254, 41.73581377801762, 35.09478848755934, 1.627842734517293, 3.9990343040067575, 47.612516344604956, 59.22659359870171, 12.449714216340704, 30.58740512702836, 54.55927877686037, 4.129446679295437], 'lossList': [0.0, -1.3360853612422943, 0.0, 12.99915337562561, 0.0, 0.0, 0.0], 'rewardMean': 0.7133301538069391, 'totalEpisodes': 137, 'stepsPerEpisode': 352, 'rewardPerEpisode': 320.881055569662, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5017359251012721, 'errorList': [], 'lossList': [0.0, -1.3242361921072006, 0.0, 10.599861751794815, 0.0, 0.0, 0.0], 'rewardMean': 0.6879163123973917, 'totalEpisodes': 140, 'stepsPerEpisode': 153, 'rewardPerEpisode': 123.91570204083476
'totalSteps': 17920, 'rewardStep': 0.7349852754636391, 'errorList': [], 'lossList': [0.0, -1.3107134652137757, 0.0, 8.601263358592988, 0.0, 0.0, 0.0], 'rewardMean': 0.6768282784987842, 'totalEpisodes': 143, 'stepsPerEpisode': 546, 'rewardPerEpisode': 455.25334517174093
'totalSteps': 19200, 'rewardStep': 0.9350216279227679, 'errorList': [11.31716248329267, 12.575407060100886, 0.18360674884423853, 3.703502246423345, 1.8074013524073964, 6.491149686372848, 1.6114401392299516, 2.8690044198373768, 6.265260233202661, 5.929566118253918, 45.277636752417195, 0.2736832353290957, 7.948441856015639, 0.5767450552939466, 24.15555188229256, 2.0662251466323847, 7.709521072359924, 11.520757453832559, 6.438821619862895, 4.817562383122364, 2.149808154940035, 5.392146984745519, 1.9983777117621688, 0.8764299886928175, 10.540317722137637, 34.81465239122639, 11.488587424921622, 1.975860496695273, 7.566878556127312, 3.086139473544434, 1.4121932005662126, 1.2195167436778687, 6.396845708553708, 28.444687332697196, 2.3248438434115775, 9.088128870694096, 4.266721718833864, 0.8686010668895966, 46.91881895684118, 9.96639775761635, 2.976575232230882, 29.15047078734146, 5.102438199276037, 18.135815209858517, 8.821711230743079, 1.0536469297451, 8.436311936369083, 14.010998053331441, 3.375047491407211, 13.886206759102413], 'lossList': [0.0, -1.2711808425188065, 0.0, 5.464220929145813, 0.0, 0.0, 0.0], 'rewardMean': 0.7249592233687191, 'totalEpisodes': 148, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.810191364569892, 'successfulTests': 1
'totalSteps': 20480, 'rewardStep': 0.9797095988726635, 'errorList': [5.309187742507447, 0.85791177684596, 5.79527472469796, 1.2810889553414768, 1.4220606458740268, 4.112752616508241, 0.36737631696958134, 1.8369669581233476, 1.317168855671217, 3.4109162655231837, 1.689269471369669, 0.8051426767447609, 0.19220013569394684, 0.09760574481555707, 3.0946070994679546, 3.63225762617562, 0.6964117736640156, 0.3546909561269673, 3.917169744471982, 1.9019407098843313, 3.5544348838717768, 0.9646928391780797, 1.9847126334021308, 1.4175156659941028, 1.6525547398904865, 0.4170663534742784, 0.8994287793603919, 0.8732912567562775, 0.8638130848308568, 2.8871537777017724, 0.11566741650038184, 0.45752931495774596, 0.676145840193499, 2.210798759208265, 0.5678996366091736, 0.19933684380808378, 0.5581235493248329, 3.6974100714290703, 1.7392395352434733, 2.8876014205832456, 1.7494967803293058, 3.9221276348983483, 1.3040061908410192, 0.45912787211692924, 11.900701516426421, 1.9001734685933385, 0.501591583248673, 1.5462560869841715, 1.0336840655582085, 2.354522492153322], 'lossList': [0.0, -1.2443507874011994, 0.0, 5.221447330713272, 0.0, 0.0, 0.0], 'rewardMean': 0.7395829098018848, 'totalEpisodes': 153, 'stepsPerEpisode': 85, 'rewardPerEpisode': 73.13386626774954, 'successfulTests': 4
'totalSteps': 21760, 'rewardStep': 0.7980424309523401, 'errorList': [], 'lossList': [0.0, -1.2513342916965484, 0.0, 3.3861733400821685, 0.0, 0.0, 0.0], 'rewardMean': 0.7819757056423925, 'totalEpisodes': 154, 'stepsPerEpisode': 270, 'rewardPerEpisode': 226.53701615600454
'totalSteps': 23040, 'rewardStep': 0.8718265507263064, 'errorList': [], 'lossList': [0.0, -1.2602703309059142, 0.0, 3.399445340633392, 0.0, 0.0, 0.0], 'rewardMean': 0.7965054583531853, 'totalEpisodes': 155, 'stepsPerEpisode': 680, 'rewardPerEpisode': 542.1087149865178
'totalSteps': 24320, 'rewardStep': 0.7876229781469574, 'errorList': [], 'lossList': [0.0, -1.2392255383729935, 0.0, 2.0306237149238586, 0.0, 0.0, 0.0], 'rewardMean': 0.7957185932259885, 'totalEpisodes': 155, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1086.7953320246254
'totalSteps': 25600, 'rewardStep': 0.9253149820721303, 'errorList': [], 'lossList': [0.0, -1.2019821906089783, 0.0, 1.2309849299490452, 0.0, 0.0, 0.0], 'rewardMean': 0.8052252287244036, 'totalEpisodes': 155, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.4346316913645
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=20480, timeSpent=99.92
