#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 14
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.6449906508441954, 'errorList': [], 'lossList': [0.0, -1.4157665479183197, 0.0, 27.931963834762573, 0.0, 0.0, 0.0], 'rewardMean': 0.6584530284937693, 'totalEpisodes': 35, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.913742021314047
'totalSteps': 3840, 'rewardStep': 0.7865193662180895, 'errorList': [], 'lossList': [0.0, -1.4174657875299455, 0.0, 41.88292486190796, 0.0, 0.0, 0.0], 'rewardMean': 0.7011418077352093, 'totalEpisodes': 64, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.711632684005195
'totalSteps': 5120, 'rewardStep': 0.8836009591458911, 'errorList': [], 'lossList': [0.0, -1.410020324587822, 0.0, 59.44106252670288, 0.0, 0.0, 0.0], 'rewardMean': 0.7467565955878798, 'totalEpisodes': 92, 'stepsPerEpisode': 45, 'rewardPerEpisode': 37.010665297539546
'totalSteps': 6400, 'rewardStep': 0.8451737722686469, 'errorList': [], 'lossList': [0.0, -1.4096808534860612, 0.0, 65.68790813446044, 0.0, 0.0, 0.0], 'rewardMean': 0.7664400309240331, 'totalEpisodes': 126, 'stepsPerEpisode': 34, 'rewardPerEpisode': 24.59685442635856
'totalSteps': 7680, 'rewardStep': 0.6850309779118396, 'errorList': [], 'lossList': [0.0, -1.4084006088972092, 0.0, 39.82380719184876, 0.0, 0.0, 0.0], 'rewardMean': 0.752871855422001, 'totalEpisodes': 140, 'stepsPerEpisode': 34, 'rewardPerEpisode': 23.00495031039433
'totalSteps': 8960, 'rewardStep': 0.5979655654158956, 'errorList': [], 'lossList': [0.0, -1.3950372087955474, 0.0, 27.884361577033996, 0.0, 0.0, 0.0], 'rewardMean': 0.7307423854211288, 'totalEpisodes': 151, 'stepsPerEpisode': 31, 'rewardPerEpisode': 19.166932975620465
'totalSteps': 10240, 'rewardStep': 0.8603464076886562, 'errorList': [], 'lossList': [0.0, -1.3686424762010574, 0.0, 11.46571454524994, 0.0, 0.0, 0.0], 'rewardMean': 0.7469428882045697, 'totalEpisodes': 158, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.45737819783352
'totalSteps': 11520, 'rewardStep': 0.8329629347587257, 'errorList': [], 'lossList': [0.0, -1.3389847713708878, 0.0, 8.288701095581054, 0.0, 0.0, 0.0], 'rewardMean': 0.7565006711550315, 'totalEpisodes': 163, 'stepsPerEpisode': 82, 'rewardPerEpisode': 69.22842053990635
'totalSteps': 12800, 'rewardStep': 0.9166060787969822, 'errorList': [], 'lossList': [0.0, -1.3315742796659469, 0.0, 21.364781200885773, 0.0, 0.0, 0.0], 'rewardMean': 0.7725112119192266, 'totalEpisodes': 169, 'stepsPerEpisode': 64, 'rewardPerEpisode': 58.47030827245745
'totalSteps': 14080, 'rewardStep': 0.6982818855217714, 'errorList': [], 'lossList': [0.0, -1.3194497674703598, 0.0, 7.353413562178612, 0.0, 0.0, 0.0], 'rewardMean': 0.7751478598570694, 'totalEpisodes': 172, 'stepsPerEpisode': 360, 'rewardPerEpisode': 301.5356182574435
'totalSteps': 15360, 'rewardStep': 0.8602894810389543, 'errorList': [], 'lossList': [0.0, -1.309834291934967, 0.0, 6.871111227869988, 0.0, 0.0, 0.0], 'rewardMean': 0.7966777428765452, 'totalEpisodes': 175, 'stepsPerEpisode': 403, 'rewardPerEpisode': 366.9258248553333
'totalSteps': 16640, 'rewardStep': 0.7363629399738798, 'errorList': [], 'lossList': [0.0, -1.3008886182308197, 0.0, 4.124190984964371, 0.0, 0.0, 0.0], 'rewardMean': 0.7916621002521242, 'totalEpisodes': 178, 'stepsPerEpisode': 170, 'rewardPerEpisode': 152.44649653205153
'totalSteps': 17920, 'rewardStep': 0.8556090210401366, 'errorList': [], 'lossList': [0.0, -1.2849274444580079, 0.0, 3.6152853953838346, 0.0, 0.0, 0.0], 'rewardMean': 0.7888629064415488, 'totalEpisodes': 180, 'stepsPerEpisode': 127, 'rewardPerEpisode': 114.26122028860203
'totalSteps': 19200, 'rewardStep': 0.5666342236019297, 'errorList': [], 'lossList': [0.0, -1.2875467711687087, 0.0, 1.707916000932455, 0.0, 0.0, 0.0], 'rewardMean': 0.7610089515748771, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 914.802514507153
'totalSteps': 20480, 'rewardStep': 0.8984902998210809, 'errorList': [], 'lossList': [0.0, -1.2957546162605285, 0.0, 1.3019081443548202, 0.0, 0.0, 0.0], 'rewardMean': 0.7823548837658012, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.8186666158165
'totalSteps': 21760, 'rewardStep': 0.8897848172676597, 'errorList': [], 'lossList': [0.0, -1.312570042014122, 0.0, 0.9277484046667814, 0.0, 0.0, 0.0], 'rewardMean': 0.8115368089509778, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.9120881239608
'totalSteps': 23040, 'rewardStep': 0.8910084883264655, 'errorList': [], 'lossList': [0.0, -1.3248846358060837, 0.0, 0.8418956058472395, 0.0, 0.0, 0.0], 'rewardMean': 0.8146030170147585, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1136.2724460092352
'totalSteps': 24320, 'rewardStep': 0.9394196326578109, 'errorList': [0.020951346876843396, 0.01942653032876362, 0.039967664096156076, 0.044294118053031004, 0.052307225916291, 0.05620041653154971, 0.03202834462539711, 0.026321205470115593, 0.0283501413270223, 0.04300462510536122, 0.03582326114165491, 0.05409013615281347, 0.04169338057859642, 0.005814703184767821, 0.030954849120690853, 0.05197206732897658, 0.06216898859998938, 0.052386068874898564, 0.0519599626136468, 0.04175098269173853, 0.04121611524279867, 0.043518018318260965, 0.03743297361117926, 0.05273310373751445, 0.0404099501343268, 0.018393164508570724, 0.06052403294656606, 0.07255158105110913, 0.02285268236265316, 0.011042821533040157, 0.04738779579133212, 0.01181657332846912, 0.06519685000207662, 0.023951956935991843, 0.04242633327226865, 0.032826274193760926, 0.049528659537741264, 0.028761471822139166, 0.04995084052678625, 0.05487810225128072, 0.04520397109524808, 0.04917867456105484, 0.018423489088667325, 0.01401786707086934, 0.04040796420205322, 0.032805008258238656, 0.00697816413533517, 0.04468399079425431, 0.01073660553188334, 0.02910729367855943], 'lossList': [0.0, -1.3149164187908173, 0.0, 0.5241841798834502, 0.0, 0.0, 0.0], 'rewardMean': 0.825248686804667, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1193.7081921499744, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=73.01
