#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 140
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9539677475237904, 'errorList': [], 'lossList': [0.0, -1.4422927129268646, 0.0, 26.64136027097702, 0.0, 0.0, 0.0], 'rewardMean': 0.9247746693659787, 'totalEpisodes': 9, 'stepsPerEpisode': 213, 'rewardPerEpisode': 162.3040478095715
'totalSteps': 3840, 'rewardStep': 0.5211050818880907, 'errorList': [], 'lossList': [0.0, -1.4414209294319154, 0.0, 35.01478996276855, 0.0, 0.0, 0.0], 'rewardMean': 0.7902181402066827, 'totalEpisodes': 16, 'stepsPerEpisode': 154, 'rewardPerEpisode': 121.06665336436826
'totalSteps': 5120, 'rewardStep': 0.7403832368585701, 'errorList': [], 'lossList': [0.0, -1.437504562139511, 0.0, 36.79282586574554, 0.0, 0.0, 0.0], 'rewardMean': 0.7777594143696546, 'totalEpisodes': 20, 'stepsPerEpisode': 191, 'rewardPerEpisode': 148.18606025102886
'totalSteps': 6400, 'rewardStep': 0.7038085045691888, 'errorList': [], 'lossList': [0.0, -1.4438922750949859, 0.0, 16.112867367267608, 0.0, 0.0, 0.0], 'rewardMean': 0.7629692324095614, 'totalEpisodes': 21, 'stepsPerEpisode': 594, 'rewardPerEpisode': 385.8013503888135
'totalSteps': 7680, 'rewardStep': 0.9559490825760115, 'errorList': [], 'lossList': [0.0, -1.4385217756032944, 0.0, 39.412103509902956, 0.0, 0.0, 0.0], 'rewardMean': 0.7951325407706364, 'totalEpisodes': 24, 'stepsPerEpisode': 419, 'rewardPerEpisode': 304.64447946549967
'totalSteps': 8960, 'rewardStep': 0.6251722212004357, 'errorList': [], 'lossList': [0.0, -1.411651176214218, 0.0, 36.58792132854462, 0.0, 0.0, 0.0], 'rewardMean': 0.7708524951177507, 'totalEpisodes': 27, 'stepsPerEpisode': 214, 'rewardPerEpisode': 152.95965381993716
'totalSteps': 10240, 'rewardStep': 0.8350156584860923, 'errorList': [], 'lossList': [0.0, -1.4017636227607726, 0.0, 149.00738868713378, 0.0, 0.0, 0.0], 'rewardMean': 0.7788728905387934, 'totalEpisodes': 38, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.2536121605519357
'totalSteps': 11520, 'rewardStep': 0.5023502928089636, 'errorList': [], 'lossList': [0.0, -1.400047317147255, 0.0, 188.71249069213866, 0.0, 0.0, 0.0], 'rewardMean': 0.7481481574577011, 'totalEpisodes': 64, 'stepsPerEpisode': 48, 'rewardPerEpisode': 27.887300117643473
'totalSteps': 12800, 'rewardStep': 0.46590434323887714, 'errorList': [], 'lossList': [0.0, -1.3889688289165496, 0.0, 81.8125418663025, 0.0, 0.0, 0.0], 'rewardMean': 0.7199237760358188, 'totalEpisodes': 82, 'stepsPerEpisode': 146, 'rewardPerEpisode': 108.29612837323938
'totalSteps': 14080, 'rewardStep': 0.5273233954373033, 'errorList': [], 'lossList': [0.0, -1.3718605875968932, 0.0, 22.937954506874085, 0.0, 0.0, 0.0], 'rewardMean': 0.6830979564587324, 'totalEpisodes': 89, 'stepsPerEpisode': 62, 'rewardPerEpisode': 47.4874755942822
'totalSteps': 15360, 'rewardStep': 0.8369648019591907, 'errorList': [], 'lossList': [0.0, -1.3700722467899322, 0.0, 27.778980174064635, 0.0, 0.0, 0.0], 'rewardMean': 0.6713976619022723, 'totalEpisodes': 96, 'stepsPerEpisode': 74, 'rewardPerEpisode': 64.93287138412973
'totalSteps': 16640, 'rewardStep': 0.7957515854816382, 'errorList': [], 'lossList': [0.0, -1.3768699193000793, 0.0, 20.747981659173966, 0.0, 0.0, 0.0], 'rewardMean': 0.6988623122616271, 'totalEpisodes': 101, 'stepsPerEpisode': 169, 'rewardPerEpisode': 148.19483091766537
'totalSteps': 17920, 'rewardStep': 0.5503047993731627, 'errorList': [], 'lossList': [0.0, -1.363807561993599, 0.0, 11.699819626808166, 0.0, 0.0, 0.0], 'rewardMean': 0.6798544685130865, 'totalEpisodes': 103, 'stepsPerEpisode': 448, 'rewardPerEpisode': 272.02846462591174
'totalSteps': 19200, 'rewardStep': 0.7501149690160207, 'errorList': [], 'lossList': [0.0, -1.345500717163086, 0.0, 29.073672547340394, 0.0, 0.0, 0.0], 'rewardMean': 0.6844851149577696, 'totalEpisodes': 107, 'stepsPerEpisode': 559, 'rewardPerEpisode': 473.6557362180608
'totalSteps': 20480, 'rewardStep': 0.9236895001090613, 'errorList': [], 'lossList': [0.0, -1.3496020829677582, 0.0, 4.836855354905128, 0.0, 0.0, 0.0], 'rewardMean': 0.6812591567110745, 'totalEpisodes': 108, 'stepsPerEpisode': 488, 'rewardPerEpisode': 411.90956148114503
'totalSteps': 21760, 'rewardStep': 0.9172581707430736, 'errorList': [], 'lossList': [0.0, -1.3730198127031326, 0.0, 2.538150035738945, 0.0, 0.0, 0.0], 'rewardMean': 0.7104677516653383, 'totalEpisodes': 108, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.3200764859491
'totalSteps': 23040, 'rewardStep': 0.727473244551398, 'errorList': [], 'lossList': [0.0, -1.3755971169471741, 0.0, 1.7844534941017627, 0.0, 0.0, 0.0], 'rewardMean': 0.6997135102718689, 'totalEpisodes': 108, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.2186032682635
'totalSteps': 24320, 'rewardStep': 0.7652117794711865, 'errorList': [], 'lossList': [0.0, -1.3508442425727845, 0.0, 1.5060630517452955, 0.0, 0.0, 0.0], 'rewardMean': 0.7259996589380912, 'totalEpisodes': 108, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.7857647685382
'totalSteps': 25600, 'rewardStep': 0.9328257816147274, 'errorList': [0.04912305675650797, 0.10330640508408813, 0.06614738362313469, 0.0872630497283687, 0.07062999238555498, 0.04299376719060527, 0.06150745072781458, 0.06658847913014779, 0.037034935322060626, 0.06742112202404242, 0.03951149088308137, 0.05287102150922148, 0.07111718008785678, 0.06251578676274051, 0.039458361922242655, 0.05325338845750099, 0.05896130230307157, 0.061524509382593656, 0.10702902733002512, 0.032478224249983555, 0.10892570280327551, 0.07840874993929066, 0.09504464468903333, 0.06539362413051193, 0.039590722569236333, 0.07633242629168403, 0.03661383949028211, 0.0862158473575387, 0.09490210159538082, 0.042371354575023036, 0.04797899743838493, 0.06313870541840011, 0.08973368081522966, 0.09323596802689962, 0.03866720487706821, 0.10650740920542615, 0.034628184417829375, 0.06642317869675965, 0.0783746748037921, 0.07480913537170605, 0.039206657480643746, 0.049620194461811186, 0.052495463052171065, 0.05361434760439056, 0.0712191732773642, 0.07027517268779614, 0.09080813800503593, 0.048766284308544995, 0.07689004216672875, 0.06353402085333117], 'lossList': [0.0, -1.324596335887909, 0.0, 1.0365537693724036, 0.0, 0.0, 0.0], 'rewardMean': 0.7726918027756762, 'totalEpisodes': 108, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.645615723804, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=78.64
