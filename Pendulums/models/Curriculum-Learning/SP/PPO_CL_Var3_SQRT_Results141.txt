#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 141
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7406644251577059, 'errorList': [], 'lossList': [0.0, -1.4345830410718918, 0.0, 32.49093912601471, 0.0, 0.0, 0.0], 'rewardMean': 0.6496560718170306, 'totalEpisodes': 13, 'stepsPerEpisode': 86, 'rewardPerEpisode': 71.92468904089924
'totalSteps': 3840, 'rewardStep': 0.8193582189686134, 'errorList': [], 'lossList': [0.0, -1.4396438366174698, 0.0, 26.7029271030426, 0.0, 0.0, 0.0], 'rewardMean': 0.7062234542008915, 'totalEpisodes': 17, 'stepsPerEpisode': 545, 'rewardPerEpisode': 366.52826563802546
'totalSteps': 5120, 'rewardStep': 0.9101350962810322, 'errorList': [], 'lossList': [0.0, -1.4346835595369338, 0.0, 33.287370662689206, 0.0, 0.0, 0.0], 'rewardMean': 0.7572013647209266, 'totalEpisodes': 22, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.474908117371092
'totalSteps': 6400, 'rewardStep': 0.5416075836222072, 'errorList': [], 'lossList': [0.0, -1.4113790118694305, 0.0, 44.872760825157165, 0.0, 0.0, 0.0], 'rewardMean': 0.7140826085011828, 'totalEpisodes': 26, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.31683098636853
'totalSteps': 7680, 'rewardStep': 0.6286451297654038, 'errorList': [], 'lossList': [0.0, -1.3882158303260803, 0.0, 7.7586379581689835, 0.0, 0.0, 0.0], 'rewardMean': 0.6998430287118862, 'totalEpisodes': 26, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 865.5308698933696
'totalSteps': 8960, 'rewardStep': 0.6247625615280951, 'errorList': [], 'lossList': [0.0, -1.3669137227535249, 0.0, 62.34923955917358, 0.0, 0.0, 0.0], 'rewardMean': 0.6891172476856303, 'totalEpisodes': 31, 'stepsPerEpisode': 444, 'rewardPerEpisode': 318.6974798483418
'totalSteps': 10240, 'rewardStep': 0.8611204481161058, 'errorList': [], 'lossList': [0.0, -1.3578430587053298, 0.0, 123.79406284332275, 0.0, 0.0, 0.0], 'rewardMean': 0.7106176477394398, 'totalEpisodes': 40, 'stepsPerEpisode': 55, 'rewardPerEpisode': 48.885598608079796
'totalSteps': 11520, 'rewardStep': 0.5781326791544574, 'errorList': [], 'lossList': [0.0, -1.3508803862333298, 0.0, 213.45750953674317, 0.0, 0.0, 0.0], 'rewardMean': 0.6958970956744417, 'totalEpisodes': 69, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.378925960663459
'totalSteps': 12800, 'rewardStep': 0.6596368757922321, 'errorList': [], 'lossList': [0.0, -1.3457177066802979, 0.0, 97.84587900161743, 0.0, 0.0, 0.0], 'rewardMean': 0.6922710736862208, 'totalEpisodes': 84, 'stepsPerEpisode': 83, 'rewardPerEpisode': 59.80109677912411
'totalSteps': 14080, 'rewardStep': 0.8771856689765312, 'errorList': [], 'lossList': [0.0, -1.334530740380287, 0.0, 23.717532794475556, 0.0, 0.0, 0.0], 'rewardMean': 0.7241248687362384, 'totalEpisodes': 92, 'stepsPerEpisode': 109, 'rewardPerEpisode': 92.4243109250147
'totalSteps': 15360, 'rewardStep': 0.35241984940211685, 'errorList': [], 'lossList': [0.0, -1.3295227867364883, 0.0, 11.710391726493835, 0.0, 0.0, 0.0], 'rewardMean': 0.6853004111606795, 'totalEpisodes': 96, 'stepsPerEpisode': 173, 'rewardPerEpisode': 111.43503449759196
'totalSteps': 16640, 'rewardStep': 0.8234475343365647, 'errorList': [], 'lossList': [0.0, -1.3320662117004394, 0.0, 7.145411492586136, 0.0, 0.0, 0.0], 'rewardMean': 0.6857093426974746, 'totalEpisodes': 101, 'stepsPerEpisode': 121, 'rewardPerEpisode': 102.58475656731821
'totalSteps': 17920, 'rewardStep': 0.5856347551517331, 'errorList': [], 'lossList': [0.0, -1.3169067960977554, 0.0, 5.908029280006885, 0.0, 0.0, 0.0], 'rewardMean': 0.6532593085845446, 'totalEpisodes': 102, 'stepsPerEpisode': 696, 'rewardPerEpisode': 536.8535741686683
'totalSteps': 19200, 'rewardStep': 0.8690292443517514, 'errorList': [], 'lossList': [0.0, -1.2884245991706849, 0.0, 4.103953977823258, 0.0, 0.0, 0.0], 'rewardMean': 0.6860014746574992, 'totalEpisodes': 102, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 855.0676482803449
'totalSteps': 20480, 'rewardStep': 0.7942800404435848, 'errorList': [], 'lossList': [0.0, -1.2279799228906632, 0.0, 2.99531205072999, 0.0, 0.0, 0.0], 'rewardMean': 0.7025649657253173, 'totalEpisodes': 102, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.7961075343767
'totalSteps': 21760, 'rewardStep': 0.7503109583702098, 'errorList': [], 'lossList': [0.0, -1.1710642129182816, 0.0, 2.538689318522811, 0.0, 0.0, 0.0], 'rewardMean': 0.7151198054095287, 'totalEpisodes': 102, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.4449376902162
'totalSteps': 23040, 'rewardStep': 0.9640100245271912, 'errorList': [0.028549506559601652, 0.031541864004071815, 0.023170203244775967, 0.0383867779219492, 0.023304527889798275, 0.023945170557846424, 0.036696497610969006, 0.027794160367978285, 0.021255767827950783, 0.024763351097424238, 0.034401958971127244, 0.03479509200876196, 0.022673609233555887, 0.07574419253697134, 0.025657688979837213, 0.027545819355986503, 0.056436736107369746, 0.02319032864902659, 0.03305574866644821, 0.024960048445199726, 0.03609577899143894, 0.026085576629255505, 0.03694867033550227, 0.032233429094156746, 0.024766547337322323, 0.0639605246127182, 0.0375989454674539, 0.027065292356343706, 0.039058940837505815, 0.03361366373635524, 0.028102075615516364, 0.028658912942618988, 0.025134746939435366, 0.024040582508229096, 0.025782835332483815, 0.03350604107379614, 0.02502867988611272, 0.023300464326243407, 0.02476225087938264, 0.04892708027564447, 0.0214846070226923, 0.03724856949509397, 0.022911906336967467, 0.022276514131742902, 0.03402259313080752, 0.055745548523034594, 0.026339452413558492, 0.03171392172396, 0.024439489842133087, 0.031220676484771816], 'lossList': [0.0, -1.1477235299348831, 0.0, 1.6626859062165023, 0.0, 0.0, 0.0], 'rewardMean': 0.7254087630506372, 'totalEpisodes': 102, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.3053520899634, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=73.51
