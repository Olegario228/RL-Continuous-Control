#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 142
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.656094654584553, 'errorList': [], 'lossList': [0.0, -1.450130049586296, 0.0, 28.26257312297821, 0.0, 0.0, 0.0], 'rewardMean': 0.5669402149036755, 'totalEpisodes': 15, 'stepsPerEpisode': 291, 'rewardPerEpisode': 210.1441505148699
'totalSteps': 3840, 'rewardStep': 0.8498890337887748, 'errorList': [], 'lossList': [0.0, -1.4597683954238891, 0.0, 38.356641507148744, 0.0, 0.0, 0.0], 'rewardMean': 0.6612564878653753, 'totalEpisodes': 24, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.649197062965765
'totalSteps': 5120, 'rewardStep': 0.42364307934283807, 'errorList': [], 'lossList': [0.0, -1.439216222167015, 0.0, 46.518779287338255, 0.0, 0.0, 0.0], 'rewardMean': 0.601853135734741, 'totalEpisodes': 34, 'stepsPerEpisode': 68, 'rewardPerEpisode': 37.361799404949295
'totalSteps': 6400, 'rewardStep': 0.8385643503542973, 'errorList': [], 'lossList': [0.0, -1.4287865221500398, 0.0, 54.649309940338135, 0.0, 0.0, 0.0], 'rewardMean': 0.6491953786586523, 'totalEpisodes': 41, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.6999454318755003
'totalSteps': 7680, 'rewardStep': 0.9330004448010532, 'errorList': [], 'lossList': [0.0, -1.420007312297821, 0.0, 60.060142517089844, 0.0, 0.0, 0.0], 'rewardMean': 0.6964962230157191, 'totalEpisodes': 49, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.35866150826776
'totalSteps': 8960, 'rewardStep': 0.47944035490317527, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6422322559875832, 'totalEpisodes': 55, 'stepsPerEpisode': 17, 'rewardPerEpisode': 9.516977739026853
'totalSteps': 10240, 'rewardStep': 0.6718485974989626, 'errorList': [], 'lossList': [0.0, -1.4041002756357193, 0.0, 122.41387941360473, 0.0, 0.0, 0.0], 'rewardMean': 0.6455229605999587, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.547719370315612
'totalSteps': 11520, 'rewardStep': 0.7646380325389507, 'errorList': [], 'lossList': [0.0, -1.3974405908584595, 0.0, 109.56250982284546, 0.0, 0.0, 0.0], 'rewardMean': 0.657434467793858, 'totalEpisodes': 88, 'stepsPerEpisode': 90, 'rewardPerEpisode': 69.8119007788548
'totalSteps': 12800, 'rewardStep': 0.3606385360635631, 'errorList': [], 'lossList': [0.0, -1.3949344158172607, 0.0, 57.02407208442688, 0.0, 0.0, 0.0], 'rewardMean': 0.6457197438779343, 'totalEpisodes': 101, 'stepsPerEpisode': 142, 'rewardPerEpisode': 99.18547113379263
'totalSteps': 14080, 'rewardStep': 0.7942974605656237, 'errorList': [], 'lossList': [0.0, -1.3872046303749084, 0.0, 13.569942195415496, 0.0, 0.0, 0.0], 'rewardMean': 0.6595400244760414, 'totalEpisodes': 107, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.252587818928454
'totalSteps': 15360, 'rewardStep': 0.8206940984051077, 'errorList': [], 'lossList': [0.0, -1.3787506186962128, 0.0, 24.179486335515975, 0.0, 0.0, 0.0], 'rewardMean': 0.6566205309376747, 'totalEpisodes': 111, 'stepsPerEpisode': 314, 'rewardPerEpisode': 262.8876951616977
'totalSteps': 16640, 'rewardStep': 0.7723920912650335, 'errorList': [], 'lossList': [0.0, -1.3841310966014861, 0.0, 13.655031839609146, 0.0, 0.0, 0.0], 'rewardMean': 0.6914954321298943, 'totalEpisodes': 116, 'stepsPerEpisode': 162, 'rewardPerEpisode': 140.39664436159435
'totalSteps': 17920, 'rewardStep': 0.7873598594530385, 'errorList': [], 'lossList': [0.0, -1.3940634340047837, 0.0, 6.764051953554153, 0.0, 0.0, 0.0], 'rewardMean': 0.6863749830397684, 'totalEpisodes': 120, 'stepsPerEpisode': 160, 'rewardPerEpisode': 145.137000412113
'totalSteps': 19200, 'rewardStep': 0.8447653789778741, 'errorList': [], 'lossList': [0.0, -1.3918125176429748, 0.0, 5.441727764010429, 0.0, 0.0, 0.0], 'rewardMean': 0.6775514764574504, 'totalEpisodes': 123, 'stepsPerEpisode': 245, 'rewardPerEpisode': 212.06000663126693
'totalSteps': 20480, 'rewardStep': 0.9722909777977742, 'errorList': [0.40531540923228654, 0.33604627258580166, 0.5695931963764864, 0.41299211511929723, 0.3594071845668457, 0.4276075451278713, 0.26696707786974205, 0.4655328005066367, 0.5472868009924436, 0.5946479114544371, 0.5457642691260837, 0.34563255811686944, 0.5461065708654138, 0.3077419848883384, 0.6439637662334146, 0.21625342676545511, 0.5813694419594057, 0.5229473276594988, 0.5416228771369521, 0.6304155748584971, 0.9361987122893113, 0.5169327211523295, 0.4743587467541319, 0.47083918745177317, 0.3659262093207085, 0.2072793051755017, 0.5010568163815798, 0.4680396330146654, 0.3983808231878035, 0.6356145795191755, 0.42801032302209935, 0.4503367014884734, 0.2546164440359051, 0.585998068109708, 0.2141051131140966, 0.5900101685092761, 0.20554774648374657, 0.22318442904542204, 0.7194357445431703, 0.4408276338729303, 0.27358048379481287, 0.4232831110529502, 0.3257160811241645, 0.5234178954714584, 0.3770763813297676, 0.5093389848029896, 0.43372957906194415, 0.24753914184137507, 0.7105553580370498, 0.6093214774635858], 'lossList': [0.0, -1.390911728143692, 0.0, 4.171757027506828, 0.0, 0.0, 0.0], 'rewardMean': 0.7268365387469103, 'totalEpisodes': 126, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.108173286477278, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.5624279137628114, 'errorList': [], 'lossList': [0.0, -1.4000447869300843, 0.0, 3.819592766165733, 0.0, 0.0, 0.0], 'rewardMean': 0.735135294632874, 'totalEpisodes': 127, 'stepsPerEpisode': 339, 'rewardPerEpisode': 254.51544577023427
'totalSteps': 23040, 'rewardStep': 0.8854195761325141, 'errorList': [], 'lossList': [0.0, -1.3685431104898453, 0.0, 2.4898919826745987, 0.0, 0.0, 0.0], 'rewardMean': 0.7564923924962291, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 883.520092994775
'totalSteps': 24320, 'rewardStep': 0.6340199992424888, 'errorList': [], 'lossList': [0.0, -1.3403073728084565, 0.0, 4.224543019533157, 0.0, 0.0, 0.0], 'rewardMean': 0.7434305891665829, 'totalEpisodes': 130, 'stepsPerEpisode': 148, 'rewardPerEpisode': 108.25489790623719
'totalSteps': 25600, 'rewardStep': 0.6962255863905293, 'errorList': [], 'lossList': [0.0, -1.320401647090912, 0.0, 4.564344874620438, 0.0, 0.0, 0.0], 'rewardMean': 0.7769892941992795, 'totalEpisodes': 132, 'stepsPerEpisode': 247, 'rewardPerEpisode': 200.23835482318424
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=76.98
