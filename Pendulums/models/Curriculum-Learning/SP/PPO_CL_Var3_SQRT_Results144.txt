#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 144
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.9218114113526591, 'errorList': [], 'lossList': [0.0, -1.4203245288133621, 0.0, 28.935274198055268, 0.0, 0.0, 0.0], 'rewardMean': 0.82258116761598, 'totalEpisodes': 15, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.349317164886212
'totalSteps': 3840, 'rewardStep': 0.5358578135638747, 'errorList': [], 'lossList': [0.0, -1.42134346306324, 0.0, 23.08260985851288, 0.0, 0.0, 0.0], 'rewardMean': 0.7270067162652781, 'totalEpisodes': 18, 'stepsPerEpisode': 175, 'rewardPerEpisode': 121.46953483894765
'totalSteps': 5120, 'rewardStep': 0.5403975692090296, 'errorList': [], 'lossList': [0.0, -1.4236413365602494, 0.0, 29.268315353393554, 0.0, 0.0, 0.0], 'rewardMean': 0.680354429501216, 'totalEpisodes': 21, 'stepsPerEpisode': 281, 'rewardPerEpisode': 201.05970298792207
'totalSteps': 6400, 'rewardStep': 0.6436412097080544, 'errorList': [], 'lossList': [0.0, -1.4183012825250625, 0.0, 53.57064360618591, 0.0, 0.0, 0.0], 'rewardMean': 0.6730117855425837, 'totalEpisodes': 26, 'stepsPerEpisode': 349, 'rewardPerEpisode': 239.170576847292
'totalSteps': 7680, 'rewardStep': 0.7781961783116448, 'errorList': [], 'lossList': [0.0, -1.4080856847763061, 0.0, 44.78796372890472, 0.0, 0.0, 0.0], 'rewardMean': 0.6905425176707606, 'totalEpisodes': 30, 'stepsPerEpisode': 35, 'rewardPerEpisode': 26.620432301363586
'totalSteps': 8960, 'rewardStep': 0.5923597940069102, 'errorList': [], 'lossList': [0.0, -1.3983138293027877, 0.0, 134.190177154541, 0.0, 0.0, 0.0], 'rewardMean': 0.6765164142902105, 'totalEpisodes': 43, 'stepsPerEpisode': 96, 'rewardPerEpisode': 68.54634862721898
'totalSteps': 10240, 'rewardStep': 0.8097173796310104, 'errorList': [], 'lossList': [0.0, -1.37931516289711, 0.0, 123.77037462234497, 0.0, 0.0, 0.0], 'rewardMean': 0.6931665349578104, 'totalEpisodes': 56, 'stepsPerEpisode': 111, 'rewardPerEpisode': 86.81763118565839
'totalSteps': 11520, 'rewardStep': 0.3377239057872508, 'errorList': [], 'lossList': [0.0, -1.3643549460172653, 0.0, 135.2424834060669, 0.0, 0.0, 0.0], 'rewardMean': 0.6536729094944149, 'totalEpisodes': 76, 'stepsPerEpisode': 87, 'rewardPerEpisode': 61.24272148118475
'totalSteps': 12800, 'rewardStep': 0.6868807848983202, 'errorList': [], 'lossList': [0.0, -1.3581607741117478, 0.0, 81.26183832168579, 0.0, 0.0, 0.0], 'rewardMean': 0.6569936970348055, 'totalEpisodes': 93, 'stepsPerEpisode': 58, 'rewardPerEpisode': 48.80782428373078
'totalSteps': 14080, 'rewardStep': 0.863998034532653, 'errorList': [], 'lossList': [0.0, -1.3516881310939788, 0.0, 62.92616677284241, 0.0, 0.0, 0.0], 'rewardMean': 0.6710584081001406, 'totalEpisodes': 101, 'stepsPerEpisode': 58, 'rewardPerEpisode': 46.98677359823168
'totalSteps': 15360, 'rewardStep': 0.6281847169712558, 'errorList': [], 'lossList': [0.0, -1.3485360211133957, 0.0, 57.64067928314209, 0.0, 0.0, 0.0], 'rewardMean': 0.6416957386620004, 'totalEpisodes': 110, 'stepsPerEpisode': 209, 'rewardPerEpisode': 174.88482772997884
'totalSteps': 16640, 'rewardStep': 0.29117259842525267, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5923047200697604, 'totalEpisodes': 119, 'stepsPerEpisode': 126, 'rewardPerEpisode': 91.70576589231905
'totalSteps': 17920, 'rewardStep': 0.19643486592849235, 'errorList': [], 'lossList': [0.0, -1.348069772720337, 0.0, 22.307576479911805, 0.0, 0.0, 0.0], 'rewardMean': 0.5475840856918043, 'totalEpisodes': 123, 'stepsPerEpisode': 347, 'rewardPerEpisode': 259.15695211607175
'totalSteps': 19200, 'rewardStep': 0.8418563961395917, 'errorList': [], 'lossList': [0.0, -1.347294083237648, 0.0, 9.789419605731965, 0.0, 0.0, 0.0], 'rewardMean': 0.553950107474599, 'totalEpisodes': 128, 'stepsPerEpisode': 50, 'rewardPerEpisode': 35.314488645095054
'totalSteps': 20480, 'rewardStep': 0.9048762114448436, 'errorList': [], 'lossList': [0.0, -1.3366352152824401, 0.0, 9.822744581699371, 0.0, 0.0, 0.0], 'rewardMean': 0.5852017492183924, 'totalEpisodes': 132, 'stepsPerEpisode': 49, 'rewardPerEpisode': 35.90957555722956
'totalSteps': 21760, 'rewardStep': 0.710029969492219, 'errorList': [], 'lossList': [0.0, -1.3309713572263717, 0.0, 8.440842705965043, 0.0, 0.0, 0.0], 'rewardMean': 0.5752330082045132, 'totalEpisodes': 133, 'stepsPerEpisode': 959, 'rewardPerEpisode': 709.6859646807396
'totalSteps': 23040, 'rewardStep': 0.843960303861634, 'errorList': [], 'lossList': [0.0, -1.3222458726167678, 0.0, 5.798649464845657, 0.0, 0.0, 0.0], 'rewardMean': 0.6258566480119514, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 970.2073805189552
'totalSteps': 24320, 'rewardStep': 0.7944104237577464, 'errorList': [], 'lossList': [0.0, -1.2940881979465484, 0.0, 2.773541800528765, 0.0, 0.0, 0.0], 'rewardMean': 0.6366096118978941, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.4905859111313
'totalSteps': 25600, 'rewardStep': 0.9714578855856856, 'errorList': [0.03415211143988389, 0.04144373291903595, 0.05219611942979431, 0.06137058567387402, 0.04929105476135544, 0.03100285505211014, 0.0071869946826044855, 0.0530013761343399, 0.02795933727152288, 0.09305936136305377, 0.0648914930111127, 0.02319883678074383, 0.02672304212706105, 0.07571788778700499, 0.01400562078778673, 0.07715755045901858, 0.017741029547273, 0.09423995267622733, 0.024323419341075708, 0.07201346988439222, 0.015425006213466734, 0.06852002653607057, 0.018359205870573228, 0.048329043182252236, 0.007372695828689218, 0.0070543473310799056, 0.014853499973693944, 0.059339723802605715, 0.007078783424159373, 0.05025293191323887, 0.007307722535688273, 0.04842010450943087, 0.018864976905225032, 0.06207391103855151, 0.007884903430953447, 0.06589520990482581, 0.007808064640806612, 0.007142598112888546, 0.007240750763771395, 0.007193482373767588, 0.0071440278157853445, 0.007432711665718673, 0.009431399371124152, 0.0071146599687614874, 0.07205710686287478, 0.014891088019651232, 0.007187375456904324, 0.030488783063642985, 0.08605979705548723, 0.07365714878361586], 'lossList': [0.0, -1.251221308708191, 0.0, 2.6936766062676907, 0.0, 0.0, 0.0], 'rewardMean': 0.6473555970031974, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.3866450037106, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=73.12
