#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 145
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.8936042246722061, 'errorList': [], 'lossList': [0.0, -1.4400932693481445, 0.0, 26.649887095689774, 0.0, 0.0, 0.0], 'rewardMean': 0.9042124276265171, 'totalEpisodes': 8, 'stepsPerEpisode': 534, 'rewardPerEpisode': 359.5826455708427
'totalSteps': 3840, 'rewardStep': 0.6367383223294267, 'errorList': [], 'lossList': [0.0, -1.4354771095514298, 0.0, 31.564103140830994, 0.0, 0.0, 0.0], 'rewardMean': 0.8150543925274869, 'totalEpisodes': 14, 'stepsPerEpisode': 154, 'rewardPerEpisode': 126.33788877452692
'totalSteps': 5120, 'rewardStep': 0.7481430482680232, 'errorList': [], 'lossList': [0.0, -1.4321384769678116, 0.0, 36.58712565660477, 0.0, 0.0, 0.0], 'rewardMean': 0.798326556462621, 'totalEpisodes': 17, 'stepsPerEpisode': 271, 'rewardPerEpisode': 215.67725585351258
'totalSteps': 6400, 'rewardStep': 0.8554151051860888, 'errorList': [], 'lossList': [0.0, -1.4374441570043563, 0.0, 22.642454097270967, 0.0, 0.0, 0.0], 'rewardMean': 0.8097442662073145, 'totalEpisodes': 18, 'stepsPerEpisode': 592, 'rewardPerEpisode': 375.58860358391223
'totalSteps': 7680, 'rewardStep': 0.6414305004430811, 'errorList': [], 'lossList': [0.0, -1.4359627854824066, 0.0, 32.944743256568906, 0.0, 0.0, 0.0], 'rewardMean': 0.7816919719132757, 'totalEpisodes': 20, 'stepsPerEpisode': 290, 'rewardPerEpisode': 183.2845953500375
'totalSteps': 8960, 'rewardStep': 0.34643206334305254, 'errorList': [], 'lossList': [0.0, -1.4166528016328812, 0.0, 51.380273733139035, 0.0, 0.0, 0.0], 'rewardMean': 0.7195119849746724, 'totalEpisodes': 24, 'stepsPerEpisode': 267, 'rewardPerEpisode': 178.3362169825604
'totalSteps': 10240, 'rewardStep': 0.9616832283117583, 'errorList': [329.87769856059276, 314.76196599704645, 376.2514340519079, 348.4832864798202, 279.33985287413265, 383.64696163166326, 360.82615534515077, 338.44362978988124, 316.36309227533667, 269.52806905585015, 367.40713648372, 343.58778321913144, 352.06866159170085, 370.5633196491085, 326.7162495274902, 351.68641338905087, 360.605569898551, 261.2275221276075, 358.8313027325375, 345.0253026470771, 348.61983187247563, 250.1856968055893, 364.88641293579064, 184.24757018565285, 350.3108008002881, 370.98758991921306, 322.1693881721644, 291.9319002274796, 297.7504028125772, 301.43702337700853, 296.58827949590847, 341.3396268010965, 293.5610480903492, 344.6362623157813, 336.1206106427344, 349.9058914292405, 391.31959656391285, 371.5278926634981, 340.17912123395627, 378.38608474823695, 345.67960111352374, 320.83672111110627, 276.2135207186503, 360.2668436310242, 343.5717382295224, 309.01019874132, 337.34134056356646, 332.45091080283925, 351.08480747137395, 351.56586553107945], 'lossList': [0.0, -1.4024951952695845, 0.0, 151.17069316864013, 0.0, 0.0, 0.0], 'rewardMean': 0.7497833903918081, 'totalEpisodes': 36, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.167443083583077, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9225765855959833, 'errorList': [], 'lossList': [0.0, -1.3899614822864532, 0.0, 240.57140598297119, 0.0, 0.0, 0.0], 'rewardMean': 0.7689826343033831, 'totalEpisodes': 66, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.626809883594316
'totalSteps': 12800, 'rewardStep': 0.7357792201861533, 'errorList': [], 'lossList': [0.0, -1.3833018147945404, 0.0, 139.58854251861572, 0.0, 0.0, 0.0], 'rewardMean': 0.7656622928916601, 'totalEpisodes': 93, 'stepsPerEpisode': 41, 'rewardPerEpisode': 37.29806362036379
'totalSteps': 14080, 'rewardStep': 0.711611136274196, 'errorList': [], 'lossList': [0.0, -1.3825969177484512, 0.0, 106.40690570831299, 0.0, 0.0, 0.0], 'rewardMean': 0.745341343460997, 'totalEpisodes': 117, 'stepsPerEpisode': 47, 'rewardPerEpisode': 31.395450911425023
'totalSteps': 15360, 'rewardStep': 0.6836624898651419, 'errorList': [], 'lossList': [0.0, -1.3946324038505553, 0.0, 42.71040787696838, 0.0, 0.0, 0.0], 'rewardMean': 0.7243471699802906, 'totalEpisodes': 133, 'stepsPerEpisode': 72, 'rewardPerEpisode': 52.36461222824125
'totalSteps': 16640, 'rewardStep': 0.8135422899337617, 'errorList': [], 'lossList': [0.0, -1.406368643641472, 0.0, 37.17914234638214, 0.0, 0.0, 0.0], 'rewardMean': 0.742027566740724, 'totalEpisodes': 141, 'stepsPerEpisode': 20, 'rewardPerEpisode': 13.914545293429594
'totalSteps': 17920, 'rewardStep': 0.7770967624340219, 'errorList': [], 'lossList': [0.0, -1.3999871045351029, 0.0, 12.236861684322356, 0.0, 0.0, 0.0], 'rewardMean': 0.7449229381573239, 'totalEpisodes': 146, 'stepsPerEpisode': 133, 'rewardPerEpisode': 111.20485410080185
'totalSteps': 19200, 'rewardStep': 0.811880466562486, 'errorList': [], 'lossList': [0.0, -1.3871885579824448, 0.0, 13.53852837562561, 0.0, 0.0, 0.0], 'rewardMean': 0.7405694742949636, 'totalEpisodes': 151, 'stepsPerEpisode': 186, 'rewardPerEpisode': 156.94414406515318
'totalSteps': 20480, 'rewardStep': 0.6480387977426285, 'errorList': [], 'lossList': [0.0, -1.3887311202287673, 0.0, 6.348237257003785, 0.0, 0.0, 0.0], 'rewardMean': 0.7412303040249183, 'totalEpisodes': 156, 'stepsPerEpisode': 207, 'rewardPerEpisode': 150.09432573449283
'totalSteps': 21760, 'rewardStep': 0.959120754188416, 'errorList': [0.26370460594397976, 0.23407722450925605, 5.5236056025402585, 1.1013646520348235, 3.418099083435219, 4.08349866282139, 5.409044987792361, 6.672561438608553, 4.5284676789085205, 4.301470184615816, 2.6437064107426202, 1.5502842385810258, 2.367428454410453, 0.9709024127891246, 4.664710468006968, 3.0780000164881627, 2.951428448284489, 1.4807244576837677, 4.139709682281215, 5.656457090288873, 5.5432914719687645, 0.7768526507312937, 1.3229266408170937, 2.074139771569274, 4.085107713458172, 1.433012244765893, 3.4216013236641403, 0.3831213478183925, 2.4288767467831933, 6.289642015570998, 1.235520606868459, 1.678660487744284, 3.1476606828402707, 6.171522715961247, 1.6062733998699414, 3.402108981838903, 5.36165594233958, 5.9013907984724145, 2.4228565535136166, 0.44225839246555454, 2.911094502397867, 4.554261731819809, 3.982983450904916, 5.545299829916293, 5.791239489400847, 4.4313394464637925, 3.703631653466803, 4.1779373369984825, 2.196356515660262, 3.37520498719911], 'lossList': [0.0, -1.377854238152504, 0.0, 6.482105602025985, 0.0, 0.0, 0.0], 'rewardMean': 0.8024991731094546, 'totalEpisodes': 162, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.7751730457498525, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.6489993113441568, 'errorList': [], 'lossList': [0.0, -1.363248279094696, 0.0, 11.871980066299438, 0.0, 0.0, 0.0], 'rewardMean': 0.7712307814126945, 'totalEpisodes': 165, 'stepsPerEpisode': 26, 'rewardPerEpisode': 18.17126618191895
'totalSteps': 24320, 'rewardStep': 0.8113275211162189, 'errorList': [], 'lossList': [0.0, -1.3465630197525025, 0.0, 7.825790667533875, 0.0, 0.0, 0.0], 'rewardMean': 0.760105874964718, 'totalEpisodes': 168, 'stepsPerEpisode': 121, 'rewardPerEpisode': 96.66551580129457
'totalSteps': 25600, 'rewardStep': 0.5340420023565807, 'errorList': [], 'lossList': [0.0, -1.3264045023918152, 0.0, 4.775421606898308, 0.0, 0.0, 0.0], 'rewardMean': 0.7399321531817609, 'totalEpisodes': 169, 'stepsPerEpisode': 344, 'rewardPerEpisode': 278.30147858796624
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=82.74
