#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 146
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8481375127268558, 'errorList': [], 'lossList': [0.0, -1.449788081049919, 0.0, 32.4359645318985, 0.0, 0.0, 0.0], 'rewardMean': 0.7206576661535076, 'totalEpisodes': 12, 'stepsPerEpisode': 82, 'rewardPerEpisode': 71.72486142973216
'totalSteps': 3840, 'rewardStep': 0.853361407501291, 'errorList': [], 'lossList': [0.0, -1.4654608142375947, 0.0, 27.316576873660086, 0.0, 0.0, 0.0], 'rewardMean': 0.7648922466027687, 'totalEpisodes': 13, 'stepsPerEpisode': 1262, 'rewardPerEpisode': 919.3907564438535
'totalSteps': 5120, 'rewardStep': 0.8323578696222399, 'errorList': [], 'lossList': [0.0, -1.4333859848976136, 0.0, 33.65936707496643, 0.0, 0.0, 0.0], 'rewardMean': 0.7817586523576365, 'totalEpisodes': 16, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.82905979896644
'totalSteps': 6400, 'rewardStep': 0.6048749205616883, 'errorList': [], 'lossList': [0.0, -1.423442860841751, 0.0, 42.21498392105102, 0.0, 0.0, 0.0], 'rewardMean': 0.7463819059984469, 'totalEpisodes': 19, 'stepsPerEpisode': 84, 'rewardPerEpisode': 67.95467326626428
'totalSteps': 7680, 'rewardStep': 0.7721497240187429, 'errorList': [], 'lossList': [0.0, -1.4013401776552201, 0.0, 11.813119353353978, 0.0, 0.0, 0.0], 'rewardMean': 0.7506765423351629, 'totalEpisodes': 19, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.1581152677668
'totalSteps': 8960, 'rewardStep': 0.7490901507477516, 'errorList': [], 'lossList': [0.0, -1.3751981830596924, 0.0, 103.6234129524231, 0.0, 0.0, 0.0], 'rewardMean': 0.7504499149655327, 'totalEpisodes': 27, 'stepsPerEpisode': 116, 'rewardPerEpisode': 93.97883645933521
'totalSteps': 10240, 'rewardStep': 0.94068540058138, 'errorList': [340.37991554906455, 329.7555432986016, 387.02106553376046, 378.1347819892947, 389.044797458062, 340.9760294523494, 356.1466432088764, 144.15494957045937, 335.18917127621194, 356.10078828500843, 375.8060912027149, 304.0395938119136, 253.14326379153093, 352.91974540900924, 362.49925093507784, 252.51572323958604, 365.78417694856364, 400.1819604922065, 332.52633504265117, 387.95491468990787, 209.89712341051478, 303.6322706644673, 223.58188340636505, 390.18921896430606, 330.452156820175, 336.5440711672438, 233.05937232249886, 354.5718641813311, 343.04559185824974, 357.0468069711602, 372.9949999538045, 406.56611143088276, 405.8550665808104, 296.86100761739164, 355.01402933592914, 393.5875428130456, 355.6307758934985, 358.4986519361016, 356.27550173054436, 381.46751044094657, 361.7830318397153, 360.30489333987117, 177.93131558610122, 337.7396181775869, 382.93433743703116, 315.7385863164205, 359.83005930578173, 360.4587788795108, 387.22405103568155, 346.3402742441445], 'lossList': [0.0, -1.3707600235939026, 0.0, 96.92622714996338, 0.0, 0.0, 0.0], 'rewardMean': 0.7742293506675136, 'totalEpisodes': 35, 'stepsPerEpisode': 101, 'rewardPerEpisode': 83.87916376903729, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.47287231221860876, 'errorList': [], 'lossList': [0.0, -1.3670508551597595, 0.0, 260.62081596374514, 0.0, 0.0, 0.0], 'rewardMean': 0.740745235284302, 'totalEpisodes': 65, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.519941964968144
'totalSteps': 12800, 'rewardStep': 0.7045108763807366, 'errorList': [], 'lossList': [0.0, -1.36557570874691, 0.0, 75.27418588638305, 0.0, 0.0, 0.0], 'rewardMean': 0.7371217993939455, 'totalEpisodes': 85, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.772265925595338
'totalSteps': 14080, 'rewardStep': 0.6960221996169071, 'errorList': [], 'lossList': [0.0, -1.357251029610634, 0.0, 54.5363245010376, 0.0, 0.0, 0.0], 'rewardMean': 0.7474062373976202, 'totalEpisodes': 102, 'stepsPerEpisode': 59, 'rewardPerEpisode': 45.93823593427094
'totalSteps': 15360, 'rewardStep': 0.5872199318862283, 'errorList': [], 'lossList': [0.0, -1.349392097592354, 0.0, 21.164871051311493, 0.0, 0.0, 0.0], 'rewardMean': 0.7213144793135575, 'totalEpisodes': 108, 'stepsPerEpisode': 173, 'rewardPerEpisode': 135.622829095926
'totalSteps': 16640, 'rewardStep': 0.8438702750820954, 'errorList': [], 'lossList': [0.0, -1.3321806240081786, 0.0, 34.732574481964114, 0.0, 0.0, 0.0], 'rewardMean': 0.7203653660716379, 'totalEpisodes': 112, 'stepsPerEpisode': 69, 'rewardPerEpisode': 61.399504405251086
'totalSteps': 17920, 'rewardStep': 0.7804560397176306, 'errorList': [], 'lossList': [0.0, -1.3037178355455399, 0.0, 5.2430120003223415, 0.0, 0.0, 0.0], 'rewardMean': 0.715175183081177, 'totalEpisodes': 116, 'stepsPerEpisode': 84, 'rewardPerEpisode': 70.91212815357082
'totalSteps': 19200, 'rewardStep': 0.9045733044445392, 'errorList': [], 'lossList': [0.0, -1.275235646367073, 0.0, 3.3558249109983445, 0.0, 0.0, 0.0], 'rewardMean': 0.7451450214694619, 'totalEpisodes': 121, 'stepsPerEpisode': 360, 'rewardPerEpisode': 238.1709516594523
'totalSteps': 20480, 'rewardStep': 0.5870133307513125, 'errorList': [], 'lossList': [0.0, -1.2588054716587067, 0.0, 22.04102578997612, 0.0, 0.0, 0.0], 'rewardMean': 0.7266313821427189, 'totalEpisodes': 124, 'stepsPerEpisode': 119, 'rewardPerEpisode': 95.07217624073778
'totalSteps': 21760, 'rewardStep': 0.6777699542322546, 'errorList': [], 'lossList': [0.0, -1.2590594428777695, 0.0, 6.656134496927262, 0.0, 0.0, 0.0], 'rewardMean': 0.7194993624911693, 'totalEpisodes': 125, 'stepsPerEpisode': 1209, 'rewardPerEpisode': 839.4066150038647
'totalSteps': 23040, 'rewardStep': 0.797004581916197, 'errorList': [], 'lossList': [0.0, -1.2528797775506972, 0.0, 3.724289931282401, 0.0, 0.0, 0.0], 'rewardMean': 0.705131280624651, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.6609009265949
'totalSteps': 24320, 'rewardStep': 0.6226363574474469, 'errorList': [], 'lossList': [0.0, -1.2170825737714768, 0.0, 1.6862486577033997, 0.0, 0.0, 0.0], 'rewardMean': 0.7201076851475349, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.3533084520398
'totalSteps': 25600, 'rewardStep': 0.9827463595212629, 'errorList': [0.13841911691294195, 0.15948292857172597, 0.2065496993775623, 0.13034361140596298, 0.16668999244106605, 0.19719346009029085, 0.1918308660073846, 0.22312666493857022, 0.21758220054543778, 0.19013525055014663, 0.15837702525373346, 0.19273713809460702, 0.20252719439629477, 0.18089138848899539, 0.18515886396186385, 0.19376717717744307, 0.15603919412259729, 0.1575549978978156, 0.27791124897299874, 0.15507373849632222, 0.231993329974069, 0.18367647627239447, 0.19673341683517376, 0.20293319531369985, 0.18628637669906614, 0.19726519593953232, 0.24664256271221682, 0.2459937168801319, 0.18498219087662315, 0.1703444299580943, 0.19311277204393887, 0.12802863066617193, 0.15115707304845896, 0.18844609505288368, 0.2525129328776584, 0.1912497749268095, 0.24599336786728604, 0.15204510144269512, 0.22008267355725725, 0.20354010783016618, 0.18100856498397433, 0.17427250607337427, 0.19370995076053255, 0.25173841042535317, 0.2001943147088383, 0.15831838600957657, 0.1846183231184894, 0.21360300153887182, 0.25174368297967564, 0.15632547814054076], 'lossList': [0.0, -1.173016772866249, 0.0, 2.400387164130807, 0.0, 0.0, 0.0], 'rewardMean': 0.7479312334615875, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.841658504062, 'successfulTests': 33
#maxSuccessfulTests=33, maxSuccessfulTestsAtStep=25600, timeSpent=80.33
