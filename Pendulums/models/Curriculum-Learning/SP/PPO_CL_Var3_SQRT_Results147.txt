#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 147
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.6846759847723713, 'errorList': [], 'lossList': [0.0, -1.4527843517065049, 0.0, 31.161307990550995, 0.0, 0.0, 0.0], 'rewardMean': 0.5938884163381916, 'totalEpisodes': 14, 'stepsPerEpisode': 286, 'rewardPerEpisode': 215.15597260368548
'totalSteps': 3840, 'rewardStep': 0.9225218000423521, 'errorList': [], 'lossList': [0.0, -1.4618944430351257, 0.0, 27.23836005926132, 0.0, 0.0, 0.0], 'rewardMean': 0.7034328775729118, 'totalEpisodes': 17, 'stepsPerEpisode': 485, 'rewardPerEpisode': 374.0158237659728
'totalSteps': 5120, 'rewardStep': 0.6354356036060425, 'errorList': [], 'lossList': [0.0, -1.428086068034172, 0.0, 36.15490975856781, 0.0, 0.0, 0.0], 'rewardMean': 0.6864335590811945, 'totalEpisodes': 22, 'stepsPerEpisode': 68, 'rewardPerEpisode': 42.5308446713733
'totalSteps': 6400, 'rewardStep': 0.6870877573757309, 'errorList': [], 'lossList': [0.0, -1.418465319275856, 0.0, 47.27386765480041, 0.0, 0.0, 0.0], 'rewardMean': 0.6865643987401018, 'totalEpisodes': 27, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6870877573757309
'totalSteps': 7680, 'rewardStep': 0.7774343158639561, 'errorList': [], 'lossList': [0.0, -1.4142415517568587, 0.0, 95.12274787902832, 0.0, 0.0, 0.0], 'rewardMean': 0.7017093849274109, 'totalEpisodes': 36, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.51393694470526
'totalSteps': 8960, 'rewardStep': 0.7195455957613556, 'errorList': [], 'lossList': [0.0, -1.403218111395836, 0.0, 75.2633955669403, 0.0, 0.0, 0.0], 'rewardMean': 0.7042574150465458, 'totalEpisodes': 42, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.93615581523101
'totalSteps': 10240, 'rewardStep': 0.6163254287846655, 'errorList': [], 'lossList': [0.0, -1.3830983382463455, 0.0, 59.006738724708555, 0.0, 0.0, 0.0], 'rewardMean': 0.6932659167638107, 'totalEpisodes': 46, 'stepsPerEpisode': 110, 'rewardPerEpisode': 92.87707515141028
'totalSteps': 11520, 'rewardStep': 0.5852664653790914, 'errorList': [], 'lossList': [0.0, -1.3813187319040299, 0.0, 153.5869859313965, 0.0, 0.0, 0.0], 'rewardMean': 0.6812659777210641, 'totalEpisodes': 66, 'stepsPerEpisode': 20, 'rewardPerEpisode': 12.007695791752683
'totalSteps': 12800, 'rewardStep': 0.7852465892685881, 'errorList': [], 'lossList': [0.0, -1.3798570078611374, 0.0, 40.36131132125855, 0.0, 0.0, 0.0], 'rewardMean': 0.6916640388758165, 'totalEpisodes': 78, 'stepsPerEpisode': 70, 'rewardPerEpisode': 60.24132975286838
'totalSteps': 14080, 'rewardStep': 0.74459392022607, 'errorList': [], 'lossList': [0.0, -1.353939602971077, 0.0, 21.459092955589295, 0.0, 0.0, 0.0], 'rewardMean': 0.7158133461080223, 'totalEpisodes': 83, 'stepsPerEpisode': 87, 'rewardPerEpisode': 71.14121805722802
'totalSteps': 15360, 'rewardStep': 0.8362049267842355, 'errorList': [], 'lossList': [0.0, -1.3330543255805969, 0.0, 33.520202860832214, 0.0, 0.0, 0.0], 'rewardMean': 0.7309662403092088, 'totalEpisodes': 89, 'stepsPerEpisode': 41, 'rewardPerEpisode': 29.6589176955953
'totalSteps': 16640, 'rewardStep': 0.5640267108713788, 'errorList': [], 'lossList': [0.0, -1.3144110429286957, 0.0, 8.277580839395522, 0.0, 0.0, 0.0], 'rewardMean': 0.6951167313921114, 'totalEpisodes': 89, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 877.8682424087857
'totalSteps': 17920, 'rewardStep': 0.8451742457263464, 'errorList': [], 'lossList': [0.0, -1.2674104350805282, 0.0, 5.168210921287536, 0.0, 0.0, 0.0], 'rewardMean': 0.7160905956041418, 'totalEpisodes': 89, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1028.5171695267786
'totalSteps': 19200, 'rewardStep': 0.9045123637702023, 'errorList': [], 'lossList': [0.0, -1.237075127363205, 0.0, 14.237570492625236, 0.0, 0.0, 0.0], 'rewardMean': 0.7378330562435889, 'totalEpisodes': 90, 'stepsPerEpisode': 758, 'rewardPerEpisode': 640.4364726987684
'totalSteps': 20480, 'rewardStep': 0.9466002080649568, 'errorList': [0.05130203479397245, 0.04961212042370334, 0.055803077279733264, 0.09587803925576882, 0.0782904686683768, 0.08161352061953657, 0.055301174049269534, 0.08885119598082768, 0.05478107948533474, 0.08623707075777408, 0.050192572023524125, 0.07980188715086392, 0.05726579536243201, 0.049731922257008294, 0.04851546256286644, 0.05345677159865139, 0.05923784122002725, 0.05273658602528619, 0.05472312481810978, 0.05208776458709747, 0.06908449975811373, 0.06321524010830919, 0.05308294062607103, 0.049961976936836905, 0.0631711193138093, 0.06774973981644165, 0.10421643992480362, 0.06374824685967496, 0.08636835584955176, 0.08233291775655333, 0.0559546579586123, 0.04902314203730642, 0.062178210186150294, 0.05841162894280486, 0.04552940542412797, 0.08340504565521781, 0.05690169847229913, 0.09762983478106635, 0.07338777710725464, 0.04949422819290402, 0.08683559545736177, 0.07043773312144162, 0.04960194711219005, 0.09123759295659088, 0.05838270644987358, 0.05118344930662092, 0.06280127242699797, 0.1044011774247683, 0.06334391300952301, 0.06949634211297866], 'lossList': [0.0, -1.1720206385850906, 0.0, 3.6380787669867276, 0.0, 0.0, 0.0], 'rewardMean': 0.754749645463689, 'totalEpisodes': 90, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.485644768517, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=60.9
