#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 15
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.8940486163371912, 'errorList': [], 'lossList': [0.0, -1.4359237921237946, 0.0, 24.73336625099182, 0.0, 0.0, 0.0], 'rewardMean': 0.8948151037726791, 'totalEpisodes': 15, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.078260581824036
'totalSteps': 3840, 'rewardStep': 0.21456873913420932, 'errorList': [], 'lossList': [0.0, -1.4311394321918487, 0.0, 41.187606725692746, 0.0, 0.0, 0.0], 'rewardMean': 0.668066315559856, 'totalEpisodes': 33, 'stepsPerEpisode': 91, 'rewardPerEpisode': 64.93439276336943
'totalSteps': 5120, 'rewardStep': 0.6960780306730984, 'errorList': [], 'lossList': [0.0, -1.425200753211975, 0.0, 70.40134342193603, 0.0, 0.0, 0.0], 'rewardMean': 0.6750692443381665, 'totalEpisodes': 58, 'stepsPerEpisode': 83, 'rewardPerEpisode': 60.66825971675683
'totalSteps': 6400, 'rewardStep': 0.6136183187418354, 'errorList': [], 'lossList': [0.0, -1.4301867860555648, 0.0, 85.67138908386231, 0.0, 0.0, 0.0], 'rewardMean': 0.6627790592189002, 'totalEpisodes': 97, 'stepsPerEpisode': 21, 'rewardPerEpisode': 14.491546352949172
'totalSteps': 7680, 'rewardStep': 0.6523304954200082, 'errorList': [], 'lossList': [0.0, -1.424657756090164, 0.0, 60.91637186050415, 0.0, 0.0, 0.0], 'rewardMean': 0.6610376319190849, 'totalEpisodes': 125, 'stepsPerEpisode': 42, 'rewardPerEpisode': 30.377659415726846
'totalSteps': 8960, 'rewardStep': 0.8325717628200154, 'errorList': [], 'lossList': [0.0, -1.4077384233474732, 0.0, 46.372404470443726, 0.0, 0.0, 0.0], 'rewardMean': 0.685542507762075, 'totalEpisodes': 140, 'stepsPerEpisode': 101, 'rewardPerEpisode': 81.56314726540216
'totalSteps': 10240, 'rewardStep': 0.4939146488887536, 'errorList': [], 'lossList': [0.0, -1.3871429812908174, 0.0, 42.761863212585446, 0.0, 0.0, 0.0], 'rewardMean': 0.6615890254029098, 'totalEpisodes': 151, 'stepsPerEpisode': 109, 'rewardPerEpisode': 83.63410590968131
'totalSteps': 11520, 'rewardStep': 0.692710160341765, 'errorList': [], 'lossList': [0.0, -1.35790598154068, 0.0, 27.061672530174256, 0.0, 0.0, 0.0], 'rewardMean': 0.6650469292850048, 'totalEpisodes': 158, 'stepsPerEpisode': 68, 'rewardPerEpisode': 58.66671289757644
'totalSteps': 12800, 'rewardStep': 0.5953119162459792, 'errorList': [], 'lossList': [0.0, -1.3442963880300522, 0.0, 11.286406490802765, 0.0, 0.0, 0.0], 'rewardMean': 0.6580734279811022, 'totalEpisodes': 161, 'stepsPerEpisode': 488, 'rewardPerEpisode': 353.9767558259712
'totalSteps': 14080, 'rewardStep': 0.7832320972432296, 'errorList': [], 'lossList': [0.0, -1.3368334150314332, 0.0, 11.238179939985276, 0.0, 0.0, 0.0], 'rewardMean': 0.6468384785846084, 'totalEpisodes': 164, 'stepsPerEpisode': 214, 'rewardPerEpisode': 166.18936202473296
'totalSteps': 15360, 'rewardStep': 0.6163150285562299, 'errorList': [], 'lossList': [0.0, -1.3113586562871933, 0.0, 10.12496277987957, 0.0, 0.0, 0.0], 'rewardMean': 0.6190651198065124, 'totalEpisodes': 166, 'stepsPerEpisode': 253, 'rewardPerEpisode': 203.46465122622934
'totalSteps': 16640, 'rewardStep': 0.9045000262523797, 'errorList': [], 'lossList': [0.0, -1.3000112175941467, 0.0, 6.772384238839149, 0.0, 0.0, 0.0], 'rewardMean': 0.6880582485183294, 'totalEpisodes': 168, 'stepsPerEpisode': 235, 'rewardPerEpisode': 209.77906219360355
'totalSteps': 17920, 'rewardStep': 0.5951138794820034, 'errorList': [], 'lossList': [0.0, -1.3016252875328065, 0.0, 3.9376614594459536, 0.0, 0.0, 0.0], 'rewardMean': 0.6779618333992199, 'totalEpisodes': 169, 'stepsPerEpisode': 687, 'rewardPerEpisode': 474.2926516076
'totalSteps': 19200, 'rewardStep': 0.8715438359358795, 'errorList': [], 'lossList': [0.0, -1.2943848824501039, 0.0, 32.990363801717756, 0.0, 0.0, 0.0], 'rewardMean': 0.7037543851186243, 'totalEpisodes': 171, 'stepsPerEpisode': 818, 'rewardPerEpisode': 654.2727991980788
'totalSteps': 20480, 'rewardStep': 0.8699121315650338, 'errorList': [], 'lossList': [0.0, -1.2977839016914368, 0.0, 6.279014580845833, 0.0, 0.0, 0.0], 'rewardMean': 0.7255125487331269, 'totalEpisodes': 172, 'stepsPerEpisode': 85, 'rewardPerEpisode': 71.9871526755163
'totalSteps': 21760, 'rewardStep': 0.8630311282227207, 'errorList': [], 'lossList': [0.0, -1.2779804968833923, 0.0, 1.7160982575267554, 0.0, 0.0, 0.0], 'rewardMean': 0.7285584852733974, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.0351996065074
'totalSteps': 23040, 'rewardStep': 0.7500952122968307, 'errorList': [], 'lossList': [0.0, -1.2375577259063721, 0.0, 1.0626338943094016, 0.0, 0.0, 0.0], 'rewardMean': 0.7541765416142052, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.1168701464032
'totalSteps': 24320, 'rewardStep': 0.830637319556418, 'errorList': [], 'lossList': [0.0, -1.1940499490499497, 0.0, 0.999454809948802, 0.0, 0.0, 0.0], 'rewardMean': 0.7679692575356705, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.840649415426
'totalSteps': 25600, 'rewardStep': 0.9073067582659976, 'errorList': [], 'lossList': [0.0, -1.1612548172473907, 0.0, 0.6103910525143147, 0.0, 0.0, 0.0], 'rewardMean': 0.7991687417376723, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.0027847256515
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=53.3
