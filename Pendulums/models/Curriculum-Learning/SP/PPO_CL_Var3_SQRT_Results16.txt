#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 16
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.7226036815273913, 'errorList': [], 'lossList': [0.0, -1.432835224866867, 0.0, 29.93111656665802, 0.0, 0.0, 0.0], 'rewardMean': 0.6406257000018731, 'totalEpisodes': 16, 'stepsPerEpisode': 107, 'rewardPerEpisode': 83.48555162675063
'totalSteps': 3840, 'rewardStep': 0.6953263313976806, 'errorList': [], 'lossList': [0.0, -1.4183774811029435, 0.0, 63.42171258926392, 0.0, 0.0, 0.0], 'rewardMean': 0.6588592438004756, 'totalEpisodes': 40, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.179783907650314
'totalSteps': 5120, 'rewardStep': 0.8500714094839684, 'errorList': [], 'lossList': [0.0, -1.398742961883545, 0.0, 76.15904365539551, 0.0, 0.0, 0.0], 'rewardMean': 0.7066622852213488, 'totalEpisodes': 66, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.142845095561835
'totalSteps': 6400, 'rewardStep': 0.6133885178474988, 'errorList': [], 'lossList': [0.0, -1.39678035736084, 0.0, 82.16044115066528, 0.0, 0.0, 0.0], 'rewardMean': 0.6880075317465788, 'totalEpisodes': 113, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.160339397976245
'totalSteps': 7680, 'rewardStep': 0.7211344867424117, 'errorList': [], 'lossList': [0.0, -1.383384364247322, 0.0, 43.043222732543946, 0.0, 0.0, 0.0], 'rewardMean': 0.6935286909125509, 'totalEpisodes': 126, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.959644774588071
'totalSteps': 8960, 'rewardStep': 0.37300705862198547, 'errorList': [], 'lossList': [0.0, -1.3584244394302367, 0.0, 56.46799382209778, 0.0, 0.0, 0.0], 'rewardMean': 0.647739886299613, 'totalEpisodes': 137, 'stepsPerEpisode': 110, 'rewardPerEpisode': 78.15010652968266
'totalSteps': 10240, 'rewardStep': 0.8118472834364334, 'errorList': [], 'lossList': [0.0, -1.3453648281097412, 0.0, 17.84686407327652, 0.0, 0.0, 0.0], 'rewardMean': 0.6682533109417157, 'totalEpisodes': 144, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.0677969266303
'totalSteps': 11520, 'rewardStep': 0.8024971373103008, 'errorList': [], 'lossList': [0.0, -1.3288327986001969, 0.0, 41.705644626617435, 0.0, 0.0, 0.0], 'rewardMean': 0.6831692916493363, 'totalEpisodes': 151, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.00879723622214
'totalSteps': 12800, 'rewardStep': 0.6690518934137226, 'errorList': [], 'lossList': [0.0, -1.3116014260053634, 0.0, 12.812464451789856, 0.0, 0.0, 0.0], 'rewardMean': 0.6817575518257749, 'totalEpisodes': 156, 'stepsPerEpisode': 173, 'rewardPerEpisode': 110.7805250575888
'totalSteps': 14080, 'rewardStep': 0.6957447414882549, 'errorList': [], 'lossList': [0.0, -1.3121567732095718, 0.0, 11.257654964923859, 0.0, 0.0, 0.0], 'rewardMean': 0.6954672541269649, 'totalEpisodes': 160, 'stepsPerEpisode': 76, 'rewardPerEpisode': 53.66050562336571
'totalSteps': 15360, 'rewardStep': 0.8727010336769604, 'errorList': [], 'lossList': [0.0, -1.3232931917905808, 0.0, 6.764170462489128, 0.0, 0.0, 0.0], 'rewardMean': 0.7104769893419217, 'totalEpisodes': 162, 'stepsPerEpisode': 218, 'rewardPerEpisode': 184.67868088618613
'totalSteps': 16640, 'rewardStep': 0.7177273295523845, 'errorList': [], 'lossList': [0.0, -1.3086325031518937, 0.0, 24.73506889939308, 0.0, 0.0, 0.0], 'rewardMean': 0.7127170891573921, 'totalEpisodes': 163, 'stepsPerEpisode': 508, 'rewardPerEpisode': 383.72570129919745
'totalSteps': 17920, 'rewardStep': 0.6733425914680347, 'errorList': [], 'lossList': [0.0, -1.266162821650505, 0.0, 2.997041162252426, 0.0, 0.0, 0.0], 'rewardMean': 0.6950442073557987, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 943.2193935184865
'totalSteps': 19200, 'rewardStep': 0.8026678553531992, 'errorList': [], 'lossList': [0.0, -1.2232759982347488, 0.0, 2.0763263124227525, 0.0, 0.0, 0.0], 'rewardMean': 0.7139721411063689, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.852928751354
'totalSteps': 20480, 'rewardStep': 0.7683856351578104, 'errorList': [], 'lossList': [0.0, -1.164698100090027, 0.0, 2.2887316869199275, 0.0, 0.0, 0.0], 'rewardMean': 0.7186972559479086, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.5490862141003
'totalSteps': 21760, 'rewardStep': 0.8349370939297437, 'errorList': [], 'lossList': [0.0, -1.0910846316814422, 0.0, 1.6216055040434003, 0.0, 0.0, 0.0], 'rewardMean': 0.7648902594786844, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.6007034568977
'totalSteps': 23040, 'rewardStep': 0.969161831730089, 'errorList': [0.06384896428685667, 0.07814399875898871, 0.05259882165158247, 0.0572830335958922, 0.08074107416716798, 0.05530342470995185, 0.07230828973058939, 0.06535299940552149, 0.04214553724318484, 0.06682594780180107, 0.04941661130578002, 0.045037520861025075, 0.056820664050436724, 0.06785048418058089, 0.10735301177014171, 0.0544495035781952, 0.06628969054266733, 0.04629269109615976, 0.054309913053165335, 0.05401605250145548, 0.07138846340409033, 0.07752749134301844, 0.0673221611394283, 0.07360905085216138, 0.0592151389363432, 0.05540971179787928, 0.06722889690558849, 0.07360368327907685, 0.045792178089672286, 0.05267030184509227, 0.04882372298982048, 0.07454362005812995, 0.0736802299518473, 0.06675973015164774, 0.07281981889654472, 0.06006729238213838, 0.08525422700425679, 0.08968299670432543, 0.06427812301911062, 0.07043449936080268, 0.05349726440698143, 0.06979271118552917, 0.06512540475746188, 0.09137123034184456, 0.08988876144677013, 0.05743796431738632, 0.08828264844777114, 0.10264534242882485, 0.07908390721201225, 0.04355071703833223], 'lossList': [0.0, -1.054598284959793, 0.0, 0.858470448218286, 0.0, 0.0, 0.0], 'rewardMean': 0.78062171430805, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.6253867464636, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=69.79
