#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 18
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.7319124085922399, 'errorList': [], 'lossList': [0.0, -1.421384494304657, 0.0, 24.64581847190857, 0.0, 0.0, 0.0], 'rewardMean': 0.7950622585738898, 'totalEpisodes': 19, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.73986150762205
'totalSteps': 3840, 'rewardStep': 0.7999993355076886, 'errorList': [], 'lossList': [0.0, -1.4218002039194106, 0.0, 56.743785572052005, 0.0, 0.0, 0.0], 'rewardMean': 0.796707950885156, 'totalEpisodes': 43, 'stepsPerEpisode': 50, 'rewardPerEpisode': 38.69244200194979
'totalSteps': 5120, 'rewardStep': 0.8739257045127125, 'errorList': [], 'lossList': [0.0, -1.4200090903043747, 0.0, 69.70478147506714, 0.0, 0.0, 0.0], 'rewardMean': 0.8160123892920451, 'totalEpisodes': 70, 'stepsPerEpisode': 75, 'rewardPerEpisode': 59.92363665961079
'totalSteps': 6400, 'rewardStep': 0.9031970248988062, 'errorList': [], 'lossList': [0.0, -1.4011641210317611, 0.0, 68.64974613189698, 0.0, 0.0, 0.0], 'rewardMean': 0.8334493164133974, 'totalEpisodes': 107, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.046709008421878
'totalSteps': 7680, 'rewardStep': 0.7681751582831869, 'errorList': [], 'lossList': [0.0, -1.3835309118032455, 0.0, 67.21545221328735, 0.0, 0.0, 0.0], 'rewardMean': 0.8225702900583624, 'totalEpisodes': 137, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.244926481984571
'totalSteps': 8960, 'rewardStep': 0.6920999709858282, 'errorList': [], 'lossList': [0.0, -1.3741782009601593, 0.0, 45.44427753448486, 0.0, 0.0, 0.0], 'rewardMean': 0.8039316730480003, 'totalEpisodes': 150, 'stepsPerEpisode': 70, 'rewardPerEpisode': 44.60490634536001
'totalSteps': 10240, 'rewardStep': 0.6647842069904739, 'errorList': [], 'lossList': [0.0, -1.3591692793369292, 0.0, 37.23579863548279, 0.0, 0.0, 0.0], 'rewardMean': 0.7865382397908095, 'totalEpisodes': 157, 'stepsPerEpisode': 162, 'rewardPerEpisode': 120.67341318174155
'totalSteps': 11520, 'rewardStep': 0.5730200894882853, 'errorList': [], 'lossList': [0.0, -1.3326294654607773, 0.0, 23.20144179224968, 0.0, 0.0, 0.0], 'rewardMean': 0.7628140008683069, 'totalEpisodes': 160, 'stepsPerEpisode': 336, 'rewardPerEpisode': 279.90435024075134
'totalSteps': 12800, 'rewardStep': 0.6760880620473413, 'errorList': [], 'lossList': [0.0, -1.3280228900909423, 0.0, 13.381781190633774, 0.0, 0.0, 0.0], 'rewardMean': 0.7541414069862103, 'totalEpisodes': 161, 'stepsPerEpisode': 816, 'rewardPerEpisode': 656.3848751156239
'totalSteps': 14080, 'rewardStep': 0.825896957727345, 'errorList': [], 'lossList': [0.0, -1.3083597379922867, 0.0, 7.275680146813393, 0.0, 0.0, 0.0], 'rewardMean': 0.7509098919033909, 'totalEpisodes': 165, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.886313749818324
'totalSteps': 15360, 'rewardStep': 0.5432717844537006, 'errorList': [], 'lossList': [0.0, -1.3134197050333023, 0.0, 4.628579826951027, 0.0, 0.0, 0.0], 'rewardMean': 0.7320458294895368, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 894.133683276197
'totalSteps': 16640, 'rewardStep': 0.7755620270672505, 'errorList': [], 'lossList': [0.0, -1.3011594945192337, 0.0, 5.168712717294693, 0.0, 0.0, 0.0], 'rewardMean': 0.7296020986454931, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.201438867049
'totalSteps': 17920, 'rewardStep': 0.849970241829644, 'errorList': [], 'lossList': [0.0, -1.2975194263458252, 0.0, 3.2132608664035796, 0.0, 0.0, 0.0], 'rewardMean': 0.7272065523771862, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.956065046713
'totalSteps': 19200, 'rewardStep': 0.8289652048368821, 'errorList': [], 'lossList': [0.0, -1.2964191031455994, 0.0, 2.3971073308587076, 0.0, 0.0, 0.0], 'rewardMean': 0.7197833703709938, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.5389369592694
'totalSteps': 20480, 'rewardStep': 0.9639649618604819, 'errorList': [0.10668764079303357, 0.0222668495712808, 0.10217804034423342, 0.08197924671999655, 0.11840872895256437, 0.03362277971996478, 0.09917815344989152, 0.06753022832340366, 0.03923900623594001, 0.09903994227523501, 0.0810904559581836, 0.06179758411269897, 0.028878454349585112, 0.04726567812761374, 0.030579030375296343, 0.038624331780151504, 0.053255418016070435, 0.12110908316917501, 0.059981232985187066, 0.11419005737198265, 0.06870302372103078, 0.0767401531651145, 0.20908911637967123, 0.05529617247968693, 0.025623419561855088, 0.02019680300233138, 0.06301561516226015, 0.12367440647942157, 0.07142559004502835, 0.07068038152093566, 0.03956423354564766, 0.07403316743268219, 0.09797166479517241, 0.04736296641576751, 0.15771836991145433, 0.06830914530202203, 0.08209370770620653, 0.10996004711443609, 0.04967363928586904, 0.05859972789618568, 0.032870578918987446, 0.1358407174155921, 0.06444360335352407, 0.10004118437215676, 0.11040387399457603, 0.13135355452006156, 0.022092894490717684, 0.03250009962175269, 0.09684296067258098, 0.05745585279049207], 'lossList': [0.0, -1.2482154142856599, 0.0, 1.9470835956186057, 0.0, 0.0, 0.0], 'rewardMean': 0.7393623507287232, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.8691806180148, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.9465329273711559, 'errorList': [0.08142786814843506, 0.01628263575039438, 0.022800881881965423, 0.054167558737432446, 0.07278332576676474, 0.04438684921144241, 0.010415629025675487, 0.028870747800755003, 0.03949623715229571, 0.039531442309948206, 0.04448953273030108, 0.11893928791760598, 0.053487134033544666, 0.0667185344326202, 0.0076433191198062855, 0.01008575049382734, 0.06403864146267621, 0.0270630347474296, 0.0436462015684591, 0.013298165268942428, 0.007298652105106716, 0.024030482200720675, 0.033328620823457715, 0.04771779526014535, 0.03686089406596756, 0.04961155969481823, 0.05790605176975965, 0.03991712633511531, 0.042753699999052756, 0.06840838835125873, 0.05053247969984286, 0.034013084035796906, 0.036013939922100176, 0.014753403373226172, 0.08566118207017755, 0.08276646494816103, 0.06475800343629028, 0.05755473915903988, 0.11081937542637052, 0.030219006558886704, 0.04522088764561224, 0.06446272818717692, 0.0355217290677457, 0.019361463652278244, 0.017848178603234222, 0.07604820550808394, 0.038887266272467955, 0.022304990910346083, 0.036496823313971, 0.024532822372051954], 'lossList': [0.0, -1.2040509074926375, 0.0, 1.1040873876214028, 0.0, 0.0, 0.0], 'rewardMean': 0.7648056463672561, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.640249843074, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=89.72
