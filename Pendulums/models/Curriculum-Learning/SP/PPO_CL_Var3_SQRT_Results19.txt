#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 19
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8393892834759831, 'errorList': [], 'lossList': [0.0, -1.4226798194646835, 0.0, 29.656007664203642, 0.0, 0.0, 0.0], 'rewardMean': 0.7813701036776419, 'totalEpisodes': 20, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.257450296500824
'totalSteps': 3840, 'rewardStep': 0.7598893752833858, 'errorList': [], 'lossList': [0.0, -1.4188754004240036, 0.0, 46.83102937698364, 0.0, 0.0, 0.0], 'rewardMean': 0.7742098608795566, 'totalEpisodes': 43, 'stepsPerEpisode': 57, 'rewardPerEpisode': 42.327799349028766
'totalSteps': 5120, 'rewardStep': 0.9869884755865903, 'errorList': [251.13517148912683, 260.9911032890494, 238.5903168722661, 256.55227870284017, 248.13656594016084, 265.07837078130564, 291.6523487628913, 285.8478897777668, 277.56677808650244, 282.9174221927046, 256.1845874797509, 215.49467058103116, 255.31402077825368, 198.51884334319948, 283.0982586056881, 244.26111093614657, 275.1673910332679, 254.2488333938573, 246.76471594518125, 266.45209098099417, 285.41762506405496, 216.17139129027393, 279.1533749659975, 285.90800676266974, 236.70682677550198, 244.73881465048044, 269.82035814001875, 285.8588101847662, 270.9746894231798, 268.34526705397474, 280.3245818667369, 260.910460347219, 230.73687029130235, 265.6323105151447, 292.0077104883043, 269.4115416259335, 272.98890411895155, 288.6273737992875, 276.261942186567, 266.46175357511214, 269.2212463863453, 261.47066529441906, 270.0910155173642, 249.82661922125578, 242.151658926838, 236.11507578154232, 256.82508354529807, 247.33620831399898, 229.49643805357223, 247.68345437157495], 'lossList': [0.0, -1.409032166004181, 0.0, 69.70214601516723, 0.0, 0.0, 0.0], 'rewardMean': 0.827404514556315, 'totalEpisodes': 72, 'stepsPerEpisode': 43, 'rewardPerEpisode': 37.16686464239145, 'successfulTests': 0
'totalSteps': 6400, 'rewardStep': 0.4295557962838218, 'errorList': [], 'lossList': [0.0, -1.4004778915643692, 0.0, 71.37628833770752, 0.0, 0.0, 0.0], 'rewardMean': 0.7478347709018164, 'totalEpisodes': 113, 'stepsPerEpisode': 107, 'rewardPerEpisode': 65.71027848603511
'totalSteps': 7680, 'rewardStep': 0.7530696175524473, 'errorList': [], 'lossList': [0.0, -1.402463372349739, 0.0, 46.11676996231079, 0.0, 0.0, 0.0], 'rewardMean': 0.7487072453435881, 'totalEpisodes': 135, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.399628733563694
'totalSteps': 8960, 'rewardStep': 0.8026694339826129, 'errorList': [], 'lossList': [0.0, -1.3970682644844055, 0.0, 47.31419775962829, 0.0, 0.0, 0.0], 'rewardMean': 0.7564161294348775, 'totalEpisodes': 152, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.367316762269243
'totalSteps': 10240, 'rewardStep': 0.38763375426174673, 'errorList': [], 'lossList': [0.0, -1.373513223528862, 0.0, 30.312885127067567, 0.0, 0.0, 0.0], 'rewardMean': 0.7103183325382361, 'totalEpisodes': 160, 'stepsPerEpisode': 200, 'rewardPerEpisode': 131.4060742309
'totalSteps': 11520, 'rewardStep': 0.8531968416486777, 'errorList': [], 'lossList': [0.0, -1.345282998085022, 0.0, 8.754893395900726, 0.0, 0.0, 0.0], 'rewardMean': 0.7261937224393962, 'totalEpisodes': 165, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.212755960744248
'totalSteps': 12800, 'rewardStep': 0.8511065966204576, 'errorList': [], 'lossList': [0.0, -1.3146502625942231, 0.0, 37.71284921169281, 0.0, 0.0, 0.0], 'rewardMean': 0.7386850098575024, 'totalEpisodes': 171, 'stepsPerEpisode': 64, 'rewardPerEpisode': 56.75143846558913
'totalSteps': 14080, 'rewardStep': 0.7701176986166463, 'errorList': [], 'lossList': [0.0, -1.2889593362808227, 0.0, 11.92789588034153, 0.0, 0.0, 0.0], 'rewardMean': 0.7433616873312369, 'totalEpisodes': 173, 'stepsPerEpisode': 311, 'rewardPerEpisode': 255.01421606650888
'totalSteps': 15360, 'rewardStep': 0.5652362957847441, 'errorList': [], 'lossList': [0.0, -1.266003161072731, 0.0, 22.621867711544038, 0.0, 0.0, 0.0], 'rewardMean': 0.715946388562113, 'totalEpisodes': 176, 'stepsPerEpisode': 702, 'rewardPerEpisode': 463.89653728134834
'totalSteps': 16640, 'rewardStep': 0.5094779562674642, 'errorList': [], 'lossList': [0.0, -1.2403478187322616, 0.0, 7.411372898817063, 0.0, 0.0, 0.0], 'rewardMean': 0.6909052466605208, 'totalEpisodes': 177, 'stepsPerEpisode': 498, 'rewardPerEpisode': 330.8183263520535
'totalSteps': 17920, 'rewardStep': 0.8010490395115843, 'errorList': [], 'lossList': [0.0, -1.2032523000240325, 0.0, 5.116665576100349, 0.0, 0.0, 0.0], 'rewardMean': 0.6723113030530202, 'totalEpisodes': 178, 'stepsPerEpisode': 254, 'rewardPerEpisode': 227.28026545990983
'totalSteps': 19200, 'rewardStep': 0.7241627389237195, 'errorList': [], 'lossList': [0.0, -1.1959200197458266, 0.0, 2.8420247550308706, 0.0, 0.0, 0.0], 'rewardMean': 0.70177199731701, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.5296460572215
'totalSteps': 20480, 'rewardStep': 0.9232870864507431, 'errorList': [], 'lossList': [0.0, -1.1880786395072938, 0.0, 2.4569511203467846, 0.0, 0.0, 0.0], 'rewardMean': 0.7187937442068396, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1109.688295298093
'totalSteps': 21760, 'rewardStep': 0.9446135622077105, 'errorList': [0.04815907027723366, 0.07682350846789794, 0.09495621025755581, 0.07143712883230588, 0.05845982990609921, 0.0898194402757397, 0.06567922825238517, 0.04356832099683025, 0.09258880716720884, 0.03522680092361473, 0.1355663317775842, 0.09537546878970193, 0.06330723603211122, 0.1047592554281226, 0.03583155196697662, 0.06302938745305689, 0.04561243819764192, 0.10407359472566637, 0.09628557337626586, 0.08883185063015936, 0.04396290675152855, 0.051101197222531945, 0.1422910575552018, 0.06407109970453037, 0.14343414718808775, 0.04015109791274101, 0.08419330315437022, 0.0501383261394727, 0.05691900691019697, 0.03836766019195082, 0.05050532722421315, 0.0905258633602421, 0.05736499726730277, 0.08874806126789396, 0.07121717268889664, 0.057901163713140066, 0.08409676680694787, 0.06016054378011492, 0.03944532061397636, 0.08195838987433204, 0.0772760752310307, 0.10952595061402942, 0.04520119666299897, 0.10083733873318325, 0.10171188711296036, 0.09886552581596085, 0.05487787842511628, 0.09191406954170214, 0.11856152253290542, 0.04014030975388696], 'lossList': [0.0, -1.1646732240915298, 0.0, 1.8841714949905872, 0.0, 0.0, 0.0], 'rewardMean': 0.7329881570293495, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.6314234352274, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=91.47
