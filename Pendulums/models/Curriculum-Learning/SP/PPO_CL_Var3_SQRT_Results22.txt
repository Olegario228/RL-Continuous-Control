#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 22
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.6145649443391354, 'errorList': [], 'lossList': [0.0, -1.4429935449361802, 0.0, 29.031121487617494, 0.0, 0.0, 0.0], 'rewardMean': 0.5588328961215736, 'totalEpisodes': 16, 'stepsPerEpisode': 289, 'rewardPerEpisode': 205.45042874160418
'totalSteps': 3840, 'rewardStep': 0.8513989752228421, 'errorList': [], 'lossList': [0.0, -1.4340677732229232, 0.0, 48.806754999160766, 0.0, 0.0, 0.0], 'rewardMean': 0.6563549224886631, 'totalEpisodes': 33, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.013167529358704
'totalSteps': 5120, 'rewardStep': 0.46746755686209523, 'errorList': [], 'lossList': [0.0, -1.4072827452421188, 0.0, 77.63947896957397, 0.0, 0.0, 0.0], 'rewardMean': 0.6091330810820211, 'totalEpisodes': 61, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.3953887896520352
'totalSteps': 6400, 'rewardStep': 0.7083604725957917, 'errorList': [], 'lossList': [0.0, -1.3907246214151383, 0.0, 90.08203315734863, 0.0, 0.0, 0.0], 'rewardMean': 0.6289785593847752, 'totalEpisodes': 123, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4485802694619938
'totalSteps': 7680, 'rewardStep': 0.9171981349206814, 'errorList': [], 'lossList': [0.0, -1.3589894050359725, 0.0, 52.34083261489868, 0.0, 0.0, 0.0], 'rewardMean': 0.6770151553074263, 'totalEpisodes': 161, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.886193942028685
'totalSteps': 8960, 'rewardStep': 0.9385484470818034, 'errorList': [4.643904284787049, 5.965115793333774, 9.597077480480916, 8.301961664347198, 14.804960211873636, 8.80869248273455, 10.850260517910057, 5.356766862558977, 1.2873306280293413, 2.4673170141201948, 1.8313712233620072, 2.016543410668878, 6.619989552846366, 1.6830680914236718, 4.849556183295504, 10.11675040752986, 6.399229458728727, 4.925969469569099, 2.0916183978839715, 5.097508307836282, 7.309986453999662, 9.05391118790329, 8.13946700477709, 2.4360233693088413, 5.328046478824406, 4.791621018877772, 12.160699586841172, 3.972546740332237, 0.8965341111369456, 13.212108283200232, 2.7208367989914835, 2.474076789473019, 1.3497549222350504, 5.825809477541881, 7.7767945978342405, 4.145439735377666, 5.281849832765266, 9.911130760985253, 0.72946699508737, 6.5436828968666765, 4.995615410986148, 8.527171007398557, 2.7434009199273737, 7.7741113967182445, 5.128582457843803, 2.0983189830871614, 6.058405994764779, 11.70425027532045, 4.176330487568516, 0.4459404388849899], 'lossList': [0.0, -1.3402278554439544, 0.0, 38.53681656837463, 0.0, 0.0, 0.0], 'rewardMean': 0.7143770541323373, 'totalEpisodes': 176, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.680191365100107, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.6482190932296035, 'errorList': [], 'lossList': [0.0, -1.3334569746255875, 0.0, 24.143801279067993, 0.0, 0.0, 0.0], 'rewardMean': 0.7061073090194956, 'totalEpisodes': 181, 'stepsPerEpisode': 300, 'rewardPerEpisode': 214.71340923752373
'totalSteps': 11520, 'rewardStep': 0.368400916258565, 'errorList': [], 'lossList': [0.0, -1.3133880639076232, 0.0, 14.771361305713654, 0.0, 0.0, 0.0], 'rewardMean': 0.6685843764905033, 'totalEpisodes': 187, 'stepsPerEpisode': 184, 'rewardPerEpisode': 110.54219608562306
'totalSteps': 12800, 'rewardStep': 0.8870988833523553, 'errorList': [], 'lossList': [0.0, -1.3072801631689073, 0.0, 25.112543964385985, 0.0, 0.0, 0.0], 'rewardMean': 0.6904358271766886, 'totalEpisodes': 193, 'stepsPerEpisode': 142, 'rewardPerEpisode': 118.2439243116632
'totalSteps': 14080, 'rewardStep': 0.7732436448964942, 'errorList': [], 'lossList': [0.0, -1.3080789852142334, 0.0, 10.296832010149956, 0.0, 0.0, 0.0], 'rewardMean': 0.7174501068759368, 'totalEpisodes': 195, 'stepsPerEpisode': 534, 'rewardPerEpisode': 456.5338100185168
'totalSteps': 15360, 'rewardStep': 0.7122877530567671, 'errorList': [], 'lossList': [0.0, -1.3008259624242782, 0.0, 19.252057907581328, 0.0, 0.0, 0.0], 'rewardMean': 0.7272223877476999, 'totalEpisodes': 198, 'stepsPerEpisode': 433, 'rewardPerEpisode': 317.1981386187238
'totalSteps': 16640, 'rewardStep': 0.892604791914664, 'errorList': [], 'lossList': [0.0, -1.277060562968254, 0.0, 5.095967121124268, 0.0, 0.0, 0.0], 'rewardMean': 0.7313429694168821, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1042.527605466755
'totalSteps': 17920, 'rewardStep': 0.5016243891121248, 'errorList': [], 'lossList': [0.0, -1.2575650942325591, 0.0, 3.3933975315093994, 0.0, 0.0, 0.0], 'rewardMean': 0.734758652641885, 'totalEpisodes': 199, 'stepsPerEpisode': 797, 'rewardPerEpisode': 570.6199899526875
'totalSteps': 19200, 'rewardStep': 0.8811601274519032, 'errorList': [], 'lossList': [0.0, -1.2449999397993088, 0.0, 3.6659899251163006, 0.0, 0.0, 0.0], 'rewardMean': 0.7520386181274962, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.8510389920755
'totalSteps': 20480, 'rewardStep': 0.9934368231587741, 'errorList': [0.1107396351223834, 0.20478885299907193, 0.2007999918880597, 0.223361154705665, 0.16751292655527483, 0.18417634867122595, 0.11005894345114285, 0.13253219506968159, 0.08698563965057746, 0.17739103330359113, 0.1700749815951146, 0.12854652029724775, 0.19347474433208053, 0.17414381037925025, 0.2211635967509038, 0.27699014914458514, 0.1335297351503136, 0.26506653057451146, 0.13766852978474456, 0.19886317051854355, 0.15169478562843974, 0.17063694570701665, 0.1679034266653469, 0.21133185567730361, 0.16655386006114656, 0.24792219225013706, 0.24174667793982849, 0.1684474695230055, 0.17426397355378814, 0.1239305152284478, 0.1265090662963558, 0.23951584233939952, 0.2661203609907664, 0.13038804726699996, 0.14711698495662812, 0.18752531075853118, 0.11282489321732242, 0.13877864095654135, 0.12491961690932646, 0.3058039625242772, 0.181509697827858, 0.15144758439404513, 0.2417170100042847, 0.1848392627998252, 0.16190501783970532, 0.16920262801674912, 0.11190209184358905, 0.12823447213452224, 0.1944261548456162, 0.21196566874370845], 'lossList': [0.0, -1.201261315345764, 0.0, 3.0748673034459353, 0.0, 0.0, 0.0], 'rewardMean': 0.7596624869513053, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.0043186972232, 'successfulTests': 36
'totalSteps': 21760, 'rewardStep': 0.9423021592359502, 'errorList': [0.10639063900967839, 0.08314423113396767, 0.10516766757795637, 0.06075043335662536, 0.11456313807089845, 0.12177473664065493, 0.09069717687383842, 0.11140542620870958, 0.057066849379334206, 0.10955399512376182, 0.1097190262691177, 0.04601769435892931, 0.16381271447469548, 0.0690245227381944, 0.1475542671691338, 0.09173132398323862, 0.0617024379454413, 0.10909275164089273, 0.09793027307585483, 0.1281080829739214, 0.1832024275618083, 0.13088816128188077, 0.1040698836184488, 0.11609055350047598, 0.15225036160018718, 0.09427121173912464, 0.05432418521467593, 0.050382023957647515, 0.16477037608624762, 0.07165974886517647, 0.07802664408074717, 0.15158422353433748, 0.10451832704676334, 0.09022855337023752, 0.13134776256219807, 0.10816314721579758, 0.07744491797933625, 0.09986438341694603, 0.10185760085392154, 0.17496444997736751, 0.04968024189567541, 0.16642568098708568, 0.1006025058552677, 0.16500297216889515, 0.09456862723726599, 0.13388893546424457, 0.07937836092103755, 0.0595856818128808, 0.18582005710768673, 0.10770972444035676], 'lossList': [0.0, -1.143681492805481, 0.0, 1.955495641157031, 0.0, 0.0, 0.0], 'rewardMean': 0.7600378581667202, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.5821516290287, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=117.44
