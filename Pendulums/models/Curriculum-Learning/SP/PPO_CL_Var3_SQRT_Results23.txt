#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 23
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.7255742592450147, 'errorList': [], 'lossList': [0.0, -1.424817398786545, 0.0, 25.897383863925935, 0.0, 0.0, 0.0], 'rewardMean': 0.8122523375358776, 'totalEpisodes': 18, 'stepsPerEpisode': 28, 'rewardPerEpisode': 24.370837513789013
'totalSteps': 3840, 'rewardStep': 0.9173561675475181, 'errorList': [], 'lossList': [0.0, -1.4175760763883591, 0.0, 51.9449195766449, 0.0, 0.0, 0.0], 'rewardMean': 0.8472869475397578, 'totalEpisodes': 34, 'stepsPerEpisode': 21, 'rewardPerEpisode': 19.622999281522887
'totalSteps': 5120, 'rewardStep': 0.8296030017937632, 'errorList': [], 'lossList': [0.0, -1.4176175588369369, 0.0, 78.89228044509888, 0.0, 0.0, 0.0], 'rewardMean': 0.8428659611032592, 'totalEpisodes': 59, 'stepsPerEpisode': 72, 'rewardPerEpisode': 50.279734223459776
'totalSteps': 6400, 'rewardStep': 0.9209420015523755, 'errorList': [], 'lossList': [0.0, -1.4185047274827958, 0.0, 97.79839597702026, 0.0, 0.0, 0.0], 'rewardMean': 0.8584811691930824, 'totalEpisodes': 109, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.621071539611467
'totalSteps': 7680, 'rewardStep': 0.6632203776548282, 'errorList': [], 'lossList': [0.0, -1.4105920654535293, 0.0, 70.28435287475585, 0.0, 0.0, 0.0], 'rewardMean': 0.8259377039367067, 'totalEpisodes': 145, 'stepsPerEpisode': 57, 'rewardPerEpisode': 37.48408956075916
'totalSteps': 8960, 'rewardStep': 0.8956358404278668, 'errorList': [], 'lossList': [0.0, -1.3928519982099532, 0.0, 36.00877600669861, 0.0, 0.0, 0.0], 'rewardMean': 0.835894580578301, 'totalEpisodes': 158, 'stepsPerEpisode': 70, 'rewardPerEpisode': 53.95532779852509
'totalSteps': 10240, 'rewardStep': 0.9000789008756659, 'errorList': [], 'lossList': [0.0, -1.3628306406736375, 0.0, 45.80644031047821, 0.0, 0.0, 0.0], 'rewardMean': 0.8439176206154716, 'totalEpisodes': 165, 'stepsPerEpisode': 176, 'rewardPerEpisode': 154.13085949689798
'totalSteps': 11520, 'rewardStep': 0.28424287289239547, 'errorList': [], 'lossList': [0.0, -1.3311618959903717, 0.0, 21.806748321056364, 0.0, 0.0, 0.0], 'rewardMean': 0.7817315375351298, 'totalEpisodes': 169, 'stepsPerEpisode': 318, 'rewardPerEpisode': 204.28746657818687
'totalSteps': 12800, 'rewardStep': 0.3512534792456149, 'errorList': [], 'lossList': [0.0, -1.3095553624629974, 0.0, 19.320389847755433, 0.0, 0.0, 0.0], 'rewardMean': 0.7386837317061783, 'totalEpisodes': 174, 'stepsPerEpisode': 385, 'rewardPerEpisode': 264.3329304289013
'totalSteps': 14080, 'rewardStep': 0.720908580747081, 'errorList': [], 'lossList': [0.0, -1.295947338938713, 0.0, 12.043060995340348, 0.0, 0.0, 0.0], 'rewardMean': 0.7208815481982124, 'totalEpisodes': 178, 'stepsPerEpisode': 360, 'rewardPerEpisode': 268.6339798052865
'totalSteps': 15360, 'rewardStep': 0.712350757500176, 'errorList': [], 'lossList': [0.0, -1.2908423405885696, 0.0, 9.346989969611167, 0.0, 0.0, 0.0], 'rewardMean': 0.7195591980237286, 'totalEpisodes': 182, 'stepsPerEpisode': 158, 'rewardPerEpisode': 131.26522773760055
'totalSteps': 16640, 'rewardStep': 0.6115356343572034, 'errorList': [], 'lossList': [0.0, -1.2903393244743346, 0.0, 7.106587120294571, 0.0, 0.0, 0.0], 'rewardMean': 0.6889771447046971, 'totalEpisodes': 183, 'stepsPerEpisode': 243, 'rewardPerEpisode': 166.01589372978253
'totalSteps': 17920, 'rewardStep': 0.6931011894278504, 'errorList': [], 'lossList': [0.0, -1.2822552913427352, 0.0, 6.716903190016747, 0.0, 0.0, 0.0], 'rewardMean': 0.6753269634681057, 'totalEpisodes': 187, 'stepsPerEpisode': 414, 'rewardPerEpisode': 349.2838705541593
'totalSteps': 19200, 'rewardStep': 0.9407500974963305, 'errorList': [0.4925644325816797, 0.33595535795451065, 0.4513410914884736, 0.4053223646265251, 0.42065546558090694, 0.4052663897704087, 0.519983055930282, 0.3666205116721378, 0.528098417553068, 0.4058497998891217, 0.4248959208171642, 0.4363133257501172, 0.46993384294892737, 0.38081932765005544, 0.40914103334057, 0.4150770408363369, 0.5143246726061802, 0.42858411852161443, 0.4010891791419096, 0.42099501393836763, 0.4246518231710789, 0.5437202116967621, 0.42605266024330274, 0.4658014869075631, 0.4993077346597914, 0.3928572717401821, 0.3933020787650177, 0.5009314377446985, 0.4011515904242697, 0.43887800319240367, 0.393608137582894, 0.3569272951760314, 0.5113988043775083, 0.41267506102537244, 0.4998184579242944, 0.38424882922206327, 0.436968059128357, 0.3457889477132725, 0.5470610764848983, 0.41486749645037274, 0.4029749250526053, 0.3891193272336524, 0.3970684095437796, 0.38058913108300146, 0.4491928398773287, 0.4641953474844427, 0.3391645452012731, 0.43195621167426884, 0.3457107583549075, 0.48509071198951675], 'lossList': [0.0, -1.280140045285225, 0.0, 3.761654553413391, 0.0, 0.0, 0.0], 'rewardMean': 0.6773077730625012, 'totalEpisodes': 188, 'stepsPerEpisode': 257, 'rewardPerEpisode': 185.81090849958102, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.9403008652953301, 'errorList': [0.1580713970057909, 0.1566478990724919, 0.20500522812322822, 0.20874318524748017, 0.17036294299013804, 0.1907960998772247, 0.35196048191068674, 0.13814700492167037, 0.28035553080223424, 0.153954624553571, 0.1583152912214305, 0.20002132931693772, 0.1467689523828306, 0.17368555708581207, 0.12346696714674564, 0.19879165901833074, 0.16037279090960888, 0.1838965882831365, 0.11281624897300595, 0.2983214260902823, 0.1480723382002531, 0.15628325988331293, 0.23277271927173673, 0.1479593250277973, 0.15944375714847278, 0.13938381761287483, 0.15137699966955118, 0.16526607119941158, 0.17472224929570604, 0.14870883166037183, 0.1633707025382207, 0.15425756753649017, 0.15621762432339062, 0.19460979648215124, 0.16705605987503264, 0.18578657095879275, 0.25546972123030187, 0.22534628118125902, 0.17786374780670836, 0.25040263163117493, 0.23932799512841316, 0.24714312508238886, 0.192554046843317, 0.19292231562955003, 0.14916083298709434, 0.18468046307919359, 0.14023798057014872, 0.12685271859333938, 0.12976564184812917, 0.2632309563427136], 'lossList': [0.0, -1.2402013963460923, 0.0, 2.098909411430359, 0.0, 0.0, 0.0], 'rewardMean': 0.7050158218265514, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.6755119935981, 'successfulTests': 37
'totalSteps': 21760, 'rewardStep': 0.8965954651423449, 'errorList': [], 'lossList': [0.0, -1.21600776553154, 0.0, 1.5520802929997444, 0.0, 0.0, 0.0], 'rewardMean': 0.7051117842979993, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.23069817061
'totalSteps': 23040, 'rewardStep': 0.7700257413797964, 'errorList': [], 'lossList': [0.0, -1.1936067003011703, 0.0, 1.231906278245151, 0.0, 0.0, 0.0], 'rewardMean': 0.6921064683484123, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.0536046558436
'totalSteps': 24320, 'rewardStep': 0.8771800063889097, 'errorList': [], 'lossList': [0.0, -1.1428182381391525, 0.0, 1.2276073549687863, 0.0, 0.0, 0.0], 'rewardMean': 0.7514001816980638, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.1179568244834
'totalSteps': 25600, 'rewardStep': 0.9668087387309138, 'errorList': [0.06029862253968454, 0.032948002939759365, 0.03247193214890223, 0.04252449792676136, 0.05380400811994827, 0.03272103168708419, 0.043722521480629155, 0.031317188534806105, 0.049600111523281226, 0.04462927110027171, 0.05922991766430803, 0.031085267464444368, 0.03306918329995306, 0.06902462147791709, 0.03666716030486161, 0.039158764718590526, 0.055169355231047594, 0.05063277703438241, 0.03436492004962894, 0.0586561709782158, 0.04100743652496385, 0.05995327318222616, 0.030753332145595582, 0.03385681615729904, 0.04055449912542482, 0.032968904474461366, 0.03273463580903682, 0.03565785741047316, 0.034164509287650996, 0.035805030189617164, 0.0360199058080189, 0.03121574785203367, 0.05383601241159555, 0.050678936893759574, 0.054373448975133416, 0.03554206380128235, 0.053879035625218424, 0.04867599149562002, 0.032147453730053635, 0.0362557511296719, 0.0411449519546954, 0.04166705647416086, 0.047536495010325995, 0.05841936914773679, 0.04227759165810302, 0.04390872283044104, 0.034870952465193936, 0.06585152714190341, 0.0671123830564215, 0.052918619567761206], 'lossList': [0.0, -1.111617317199707, 0.0, 0.9084079929813743, 0.0, 0.0, 0.0], 'rewardMean': 0.8129557076465936, 'totalEpisodes': 188, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1200.8816440676408, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=126.3
