#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 24
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.9582526605819008, 'errorList': [], 'lossList': [0.0, -1.4217807793617248, 0.0, 29.320527625083923, 0.0, 0.0, 0.0], 'rewardMean': 0.8537717624405909, 'totalEpisodes': 16, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.38163601049723
'totalSteps': 3840, 'rewardStep': 0.6359417985759351, 'errorList': [], 'lossList': [0.0, -1.4221992570161819, 0.0, 54.18967531204223, 0.0, 0.0, 0.0], 'rewardMean': 0.7811617744857057, 'totalEpisodes': 33, 'stepsPerEpisode': 54, 'rewardPerEpisode': 41.79552564609769
'totalSteps': 5120, 'rewardStep': 0.9220228273536873, 'errorList': [], 'lossList': [0.0, -1.4097050791978836, 0.0, 78.67331230163575, 0.0, 0.0, 0.0], 'rewardMean': 0.8163770377027011, 'totalEpisodes': 61, 'stepsPerEpisode': 41, 'rewardPerEpisode': 33.749330485680396
'totalSteps': 6400, 'rewardStep': 0.4568519500133865, 'errorList': [], 'lossList': [0.0, -1.3973484599590302, 0.0, 84.48194131851196, 0.0, 0.0, 0.0], 'rewardMean': 0.7444720201648382, 'totalEpisodes': 112, 'stepsPerEpisode': 32, 'rewardPerEpisode': 23.023886992263517
'totalSteps': 7680, 'rewardStep': 0.3152033893326478, 'errorList': [], 'lossList': [0.0, -1.391219703555107, 0.0, 62.643126029968265, 0.0, 0.0, 0.0], 'rewardMean': 0.672927248359473, 'totalEpisodes': 142, 'stepsPerEpisode': 155, 'rewardPerEpisode': 109.1617551068064
'totalSteps': 8960, 'rewardStep': 0.838492330767277, 'errorList': [], 'lossList': [0.0, -1.382012271285057, 0.0, 47.17411085128784, 0.0, 0.0, 0.0], 'rewardMean': 0.6965794029891593, 'totalEpisodes': 163, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.985553211858756
'totalSteps': 10240, 'rewardStep': 0.6754742666392488, 'errorList': [], 'lossList': [0.0, -1.3654687047004699, 0.0, 42.580176515579225, 0.0, 0.0, 0.0], 'rewardMean': 0.6939412609454205, 'totalEpisodes': 173, 'stepsPerEpisode': 152, 'rewardPerEpisode': 104.68508944544521
'totalSteps': 11520, 'rewardStep': 0.9308178890166351, 'errorList': [3.1061484290806343, 1.4777651333468829, 2.6454111202410915, 3.4328001508464006, 2.845720822495519, 3.024894280468024, 2.5178480447803393, 3.6617497449215572, 1.282214451539651, 3.449421482138643, 1.2310746663060892, 3.494552273894441, 2.1440515899312453, 1.823947512239546, 2.6326307439675793, 1.3899637941669212, 3.1954915699713804, 2.631360429592002, 2.2332910042407685, 2.604949733938629, 2.9495817424960107, 3.4934086519610665, 2.239311173846881, 3.670324222929359, 2.2459613991674257, 3.6824780065460314, 2.720064011965667, 1.8609862068535823, 3.212518462338894, 0.7998291280736334, 3.4702783730466207, 2.121477091784613, 1.264246609147005, 2.6292267671663847, 1.8478590516467233, 3.144981216460419, 1.2739898843897166, 3.0427567403173295, 2.1179698561676665, 2.149679159376008, 2.298000330179346, 3.197933405806262, 2.1225873872809307, 1.7263429700642658, 3.2356482015523946, 3.258660239984322, 3.427223156292085, 2.5387255947658938, 1.2929320842139946, 2.2182725699932284], 'lossList': [0.0, -1.3380210268497468, 0.0, 21.84089970111847, 0.0, 0.0, 0.0], 'rewardMean': 0.7202608862866665, 'totalEpisodes': 178, 'stepsPerEpisode': 146, 'rewardPerEpisode': 118.69302251127102, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.7276937214478256, 'errorList': [], 'lossList': [0.0, -1.3159153032302857, 0.0, 39.74899874210357, 0.0, 0.0, 0.0], 'rewardMean': 0.7210041698027825, 'totalEpisodes': 184, 'stepsPerEpisode': 64, 'rewardPerEpisode': 48.14243017337555
'totalSteps': 14080, 'rewardStep': 0.6168895318679466, 'errorList': [], 'lossList': [0.0, -1.2899382650852202, 0.0, 13.465342236161232, 0.0, 0.0, 0.0], 'rewardMean': 0.707764036559649, 'totalEpisodes': 187, 'stepsPerEpisode': 175, 'rewardPerEpisode': 132.94401300334
'totalSteps': 15360, 'rewardStep': 0.9529108749087474, 'errorList': [4.312074939521975, 3.005966134285723, 6.477774291764867, 7.5467870516848095, 6.828564311129071, 7.078852961206346, 3.399366277388585, 7.306470821681678, 7.660358000724113, 2.6632338409689087, 7.6928087029584935, 1.1237586615947686, 7.235289390360333, 7.29922098174496, 7.671979660867915, 7.650140777531723, 5.56614564263557, 6.8140227439156025, 4.390580959494036, 0.9509699392794281, 7.587169828858631, 7.3684049386291885, 3.3553336682898243, 3.8473385348611635, 7.383147075571101, 1.5734640599692236, 2.731046003512767, 3.7475588653236533, 2.216694106953279, 7.56872918527049, 4.19504449300987, 3.9015931685329774, 7.662051147245184, 5.060511466713172, 4.924627803605504, 1.5180091002625564, 3.380758914629245, 4.352414956190533, 7.464517916352964, 5.8771211287228144, 6.491132375547223, 7.7023746900253975, 6.391853821809138, 5.760570623638403, 6.447066953444969, 3.002377581824843, 7.320488326450219, 0.5253883568232048, 7.696634851331568, 7.054134098420284], 'lossList': [0.0, -1.2622714465856553, 0.0, 12.130604298114777, 0.0, 0.0, 0.0], 'rewardMean': 0.7072298579923337, 'totalEpisodes': 191, 'stepsPerEpisode': 511, 'rewardPerEpisode': 405.8893275674262, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.500508876279394, 'errorList': [], 'lossList': [0.0, -1.2353328853845595, 0.0, 7.654489135742187, 0.0, 0.0, 0.0], 'rewardMean': 0.6936865657626796, 'totalEpisodes': 196, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.5457785261603565
'totalSteps': 17920, 'rewardStep': 0.7454601746747993, 'errorList': [], 'lossList': [0.0, -1.2188645333051682, 0.0, 4.2705329802632335, 0.0, 0.0, 0.0], 'rewardMean': 0.6760303004947908, 'totalEpisodes': 197, 'stepsPerEpisode': 267, 'rewardPerEpisode': 214.16870417564033
'totalSteps': 19200, 'rewardStep': 0.6084410530325637, 'errorList': [], 'lossList': [0.0, -1.226693986058235, 0.0, 5.476786388158798, 0.0, 0.0, 0.0], 'rewardMean': 0.6911892107967086, 'totalEpisodes': 199, 'stepsPerEpisode': 132, 'rewardPerEpisode': 96.59037749397854
'totalSteps': 20480, 'rewardStep': 0.8995904961101342, 'errorList': [], 'lossList': [0.0, -1.2227624785900115, 0.0, 3.0406651124358177, 0.0, 0.0, 0.0], 'rewardMean': 0.7496279214744572, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.6730418287162
'totalSteps': 21760, 'rewardStep': 0.5211666035375861, 'errorList': [], 'lossList': [0.0, -1.2204402524232865, 0.0, 4.740691266655922, 0.0, 0.0, 0.0], 'rewardMean': 0.7178953487514881, 'totalEpisodes': 200, 'stepsPerEpisode': 332, 'rewardPerEpisode': 235.28742905932563
'totalSteps': 23040, 'rewardStep': 0.7721794434499211, 'errorList': [], 'lossList': [0.0, -1.207425622344017, 0.0, 1.9966798198223115, 0.0, 0.0, 0.0], 'rewardMean': 0.7275658664325553, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 981.0227508759637
'totalSteps': 24320, 'rewardStep': 0.8928353496699134, 'errorList': [], 'lossList': [0.0, -1.1821470147371291, 0.0, 1.113837834522128, 0.0, 0.0, 0.0], 'rewardMean': 0.723767612497883, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.2057189161198
'totalSteps': 25600, 'rewardStep': 0.9729801818245233, 'errorList': [0.04874255646150697, 0.05767400762165082, 0.04686557978092048, 0.05336902253864512, 0.03838028351065292, 0.04085214237709538, 0.039685383470852896, 0.035891492955768475, 0.045352800107179195, 0.04888488077269046, 0.03706014334038918, 0.031239090167102725, 0.04169181409759018, 0.05606993498696951, 0.036821109809577396, 0.03062520327936152, 0.049953102232555016, 0.0360668753825797, 0.06577304177227442, 0.057350785206729475, 0.055248883064555686, 0.04082551938354487, 0.03503961846120935, 0.04919343444858029, 0.04467383386101142, 0.03737641292363497, 0.054393742108921804, 0.0344960485215447, 0.04201421425068434, 0.02995297672962431, 0.03750724602403344, 0.05584865139195403, 0.042425576463956095, 0.04483019502113783, 0.055174094484373125, 0.06917143214157682, 0.035583437786146414, 0.0348425435772178, 0.035789936408398594, 0.04113905260821258, 0.05650301114411907, 0.045986693423304865, 0.04853366415127887, 0.04384908933288067, 0.04419769684141616, 0.029796137635947056, 0.02800698514549212, 0.03613026364186978, 0.03949554602944445, 0.04032114200866614], 'lossList': [0.0, -1.167019619345665, 0.0, 0.9825949779525399, 0.0, 0.0, 0.0], 'rewardMean': 0.7482962585355529, 'totalEpisodes': 200, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.9453216010243, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=127.85
