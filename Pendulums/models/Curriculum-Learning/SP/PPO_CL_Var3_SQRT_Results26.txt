#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 26
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.6738779458168573, 'errorList': [], 'lossList': [0.0, -1.4179449373483657, 0.0, 33.997538805007935, 0.0, 0.0, 0.0], 'rewardMean': 0.7444640393291324, 'totalEpisodes': 72, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.001630286984664
'totalSteps': 3840, 'rewardStep': 0.7244310751994605, 'errorList': [], 'lossList': [0.0, -1.4178449988365174, 0.0, 45.91069428443909, 0.0, 0.0, 0.0], 'rewardMean': 0.7377863846192417, 'totalEpisodes': 104, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.43781542102716
'totalSteps': 5120, 'rewardStep': 0.7202002092085108, 'errorList': [], 'lossList': [0.0, -1.4165787619352341, 0.0, 57.88238626480103, 0.0, 0.0, 0.0], 'rewardMean': 0.733389840766559, 'totalEpisodes': 126, 'stepsPerEpisode': 23, 'rewardPerEpisode': 15.746452087887322
'totalSteps': 6400, 'rewardStep': 0.9590077574749113, 'errorList': [8.939562707066477, 7.4455186172145265, 91.2323253513657, 79.94092468840162, 97.4731461364814, 51.01083763492063, 31.809952113348853, 98.78909845491651, 10.404147030854139, 95.34146262599208, 108.5181899458755, 63.478819765362715, 10.682639264675528, 54.22821015675293, 91.97292596730554, 9.976090085891752, 121.05656968222905, 18.010322430631106, 75.09304723274789, 30.52082449267115, 25.58894283257324, 9.72639179128424, 61.434295489342, 66.56120877387885, 96.5780824781375, 46.704419548811856, 62.678725503224264, 18.06943110781729, 46.64799729579489, 10.003371379832156, 65.4462513476627, 47.89453525185761, 44.476029034547224, 9.60896047387252, 30.45876377485581, 83.89339285590039, 81.94499564433454, 28.942144496050258, 10.343818073240067, 10.869699102314812, 7.855069924676074, 91.14586957477107, 58.32486696977474, 119.78836056960462, 103.81715530019859, 73.39273213580384, 47.65154956855173, 78.35311964991949, 8.003562242889021, 25.18871093787075], 'lossList': [0.0, -1.4169079488515854, 0.0, 49.98062337875366, 0.0, 0.0, 0.0], 'rewardMean': 0.7785134241082294, 'totalEpisodes': 138, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.191998397005514, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.744839582940957, 'errorList': [], 'lossList': [0.0, -1.414927258491516, 0.0, 37.344018034935, 0.0, 0.0, 0.0], 'rewardMean': 0.7729011172470174, 'totalEpisodes': 145, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.715442752816518
'totalSteps': 8960, 'rewardStep': 0.8158835182721198, 'errorList': [], 'lossList': [0.0, -1.4060389256477357, 0.0, 45.17776916503906, 0.0, 0.0, 0.0], 'rewardMean': 0.7790414602506034, 'totalEpisodes': 153, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.8500523256011387
'totalSteps': 10240, 'rewardStep': 0.8967689286335965, 'errorList': [], 'lossList': [0.0, -1.3984021711349488, 0.0, 29.63907246828079, 0.0, 0.0, 0.0], 'rewardMean': 0.7937573937984777, 'totalEpisodes': 159, 'stepsPerEpisode': 18, 'rewardPerEpisode': 12.710714186477592
'totalSteps': 11520, 'rewardStep': 0.6320224662923963, 'errorList': [], 'lossList': [0.0, -1.4065572851896286, 0.0, 15.717665838003159, 0.0, 0.0, 0.0], 'rewardMean': 0.7757868462978019, 'totalEpisodes': 163, 'stepsPerEpisode': 262, 'rewardPerEpisode': 233.03354846953795
'totalSteps': 12800, 'rewardStep': 0.42625601494184584, 'errorList': [], 'lossList': [0.0, -1.4164690279960632, 0.0, 23.71589692592621, 0.0, 0.0, 0.0], 'rewardMean': 0.7408337631622063, 'totalEpisodes': 169, 'stepsPerEpisode': 134, 'rewardPerEpisode': 89.84349209920045
'totalSteps': 14080, 'rewardStep': 0.739847290014561, 'errorList': [], 'lossList': [0.0, -1.3863352525234223, 0.0, 7.368694423437119, 0.0, 0.0, 0.0], 'rewardMean': 0.7333134788795216, 'totalEpisodes': 171, 'stepsPerEpisode': 896, 'rewardPerEpisode': 735.5163757585566
'totalSteps': 15360, 'rewardStep': 0.6501293817921369, 'errorList': [], 'lossList': [0.0, -1.3710202127695084, 0.0, 4.211463184952736, 0.0, 0.0, 0.0], 'rewardMean': 0.7309386224770497, 'totalEpisodes': 172, 'stepsPerEpisode': 203, 'rewardPerEpisode': 149.76446630826192
'totalSteps': 16640, 'rewardStep': 0.8347014064147791, 'errorList': [], 'lossList': [0.0, -1.3618391644954682, 0.0, 3.310293670743704, 0.0, 0.0, 0.0], 'rewardMean': 0.7419656555985814, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.6612221944147
'totalSteps': 17920, 'rewardStep': 0.7736689050726016, 'errorList': [], 'lossList': [0.0, -1.323065966963768, 0.0, 2.0731702786684036, 0.0, 0.0, 0.0], 'rewardMean': 0.7473125251849906, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.0601370725908
'totalSteps': 19200, 'rewardStep': 0.8943826468822486, 'errorList': [], 'lossList': [0.0, -1.2807171738147736, 0.0, 1.7535567346960306, 0.0, 0.0, 0.0], 'rewardMean': 0.7408500141257243, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.8210549121443
'totalSteps': 20480, 'rewardStep': 0.7937739092133792, 'errorList': [], 'lossList': [0.0, -1.2330749142169952, 0.0, 1.4071680367738009, 0.0, 0.0, 0.0], 'rewardMean': 0.7457434467529664, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.4557718107337
'totalSteps': 21760, 'rewardStep': 0.8552588655283536, 'errorList': [], 'lossList': [0.0, -1.1660848712921144, 0.0, 1.1578495954349637, 0.0, 0.0, 0.0], 'rewardMean': 0.7496809814785899, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.4879192657727
'totalSteps': 23040, 'rewardStep': 0.9577074374865906, 'errorList': [0.06479127828950251, 0.07038212480907631, 0.059977940007980686, 0.06058695093901927, 0.06973927789146929, 0.062011871217733334, 0.06739169432049898, 0.06670091260033496, 0.060970672720698726, 0.06345266675607159, 0.05907081877374042, 0.05815026778869104, 0.06112711102719235, 0.07006270755367305, 0.09164224475521232, 0.06149141213049438, 0.06625400705405476, 0.0587030172656268, 0.06260216449852152, 0.06177072126108375, 0.06939755914543196, 0.07818412996903829, 0.06644938240071949, 0.07206435757103749, 0.06140057632673027, 0.06941930941555562, 0.06614730236637024, 0.07270576408209833, 0.06070293580199643, 0.06139208135056345, 0.058505776211036, 0.07805754988589735, 0.06648526257254445, 0.06546202028935845, 0.07634525142633307, 0.06691340458561112, 0.08002869962871893, 0.0905926952436783, 0.06423647866873315, 0.06717348312859056, 0.06269762637595888, 0.06583375744806357, 0.06660506951137458, 0.07394378314622999, 0.08233104323923021, 0.06335496185744757, 0.08473818179751993, 0.0993577321070869, 0.07855735704876227, 0.058298611677546096], 'lossList': [0.0, -1.127613006234169, 0.0, 0.4842870689183474, 0.0, 0.0, 0.0], 'rewardMean': 0.7557748323638892, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1167.170073032265, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=98.3
