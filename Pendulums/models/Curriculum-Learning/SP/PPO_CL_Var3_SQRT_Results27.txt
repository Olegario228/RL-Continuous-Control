#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 27
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.6542094753089062, 'errorList': [], 'lossList': [0.0, -1.4206151217222214, 0.0, 35.0069587802887, 0.0, 0.0, 0.0], 'rewardMean': 0.7349234085463079, 'totalEpisodes': 84, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2972407937254122
'totalSteps': 3840, 'rewardStep': 0.9322615450111629, 'errorList': [], 'lossList': [0.0, -1.4135404765605926, 0.0, 51.77372798919678, 0.0, 0.0, 0.0], 'rewardMean': 0.8007027873679262, 'totalEpisodes': 124, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.564888049534714
'totalSteps': 5120, 'rewardStep': 0.47922806087879627, 'errorList': [], 'lossList': [0.0, -1.3900382059812546, 0.0, 54.74216499328613, 0.0, 0.0, 0.0], 'rewardMean': 0.7203341057456437, 'totalEpisodes': 154, 'stepsPerEpisode': 34, 'rewardPerEpisode': 19.24643137698824
'totalSteps': 6400, 'rewardStep': 0.43654362830363874, 'errorList': [], 'lossList': [0.0, -1.3711761403083802, 0.0, 58.704063205718995, 0.0, 0.0, 0.0], 'rewardMean': 0.6635760102572428, 'totalEpisodes': 180, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.232933971248055
'totalSteps': 7680, 'rewardStep': 0.6404810653299382, 'errorList': [], 'lossList': [0.0, -1.3675313067436219, 0.0, 48.19478596687317, 0.0, 0.0, 0.0], 'rewardMean': 0.6597268527693586, 'totalEpisodes': 196, 'stepsPerEpisode': 111, 'rewardPerEpisode': 71.28408111298289
'totalSteps': 8960, 'rewardStep': 0.2630926775105528, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5605683089546571, 'totalEpisodes': 204, 'stepsPerEpisode': 136, 'rewardPerEpisode': 90.59384829871668
'totalSteps': 10240, 'rewardStep': 0.6805820881576989, 'errorList': [], 'lossList': [0.0, -1.3799422472715377, 0.0, 29.5468994307518, 0.0, 0.0, 0.0], 'rewardMean': 0.5739031733105507, 'totalEpisodes': 209, 'stepsPerEpisode': 108, 'rewardPerEpisode': 88.63626970706638
'totalSteps': 11520, 'rewardStep': 0.7340847483152141, 'errorList': [], 'lossList': [0.0, -1.3931693065166473, 0.0, 34.175375695228574, 0.0, 0.0, 0.0], 'rewardMean': 0.589921330811017, 'totalEpisodes': 215, 'stepsPerEpisode': 367, 'rewardPerEpisode': 260.30042414079145
'totalSteps': 12800, 'rewardStep': 0.8973752471311438, 'errorList': [], 'lossList': [0.0, -1.385139375925064, 0.0, 30.280290489196776, 0.0, 0.0, 0.0], 'rewardMean': 0.5980951213457605, 'totalEpisodes': 219, 'stepsPerEpisode': 352, 'rewardPerEpisode': 292.23180239392354
'totalSteps': 14080, 'rewardStep': 0.7566107881408487, 'errorList': [], 'lossList': [0.0, -1.3666972810029983, 0.0, 13.766451163291931, 0.0, 0.0, 0.0], 'rewardMean': 0.6083352526289548, 'totalEpisodes': 221, 'stepsPerEpisode': 207, 'rewardPerEpisode': 179.67887124210026
'totalSteps': 15360, 'rewardStep': 0.7699645244456781, 'errorList': [], 'lossList': [0.0, -1.360487669110298, 0.0, 10.060701072216034, 0.0, 0.0, 0.0], 'rewardMean': 0.5921055505724062, 'totalEpisodes': 224, 'stepsPerEpisode': 697, 'rewardPerEpisode': 550.9938401702908
'totalSteps': 16640, 'rewardStep': 0.45959876457162724, 'errorList': [], 'lossList': [0.0, -1.3757599622011185, 0.0, 5.989049696326256, 0.0, 0.0, 0.0], 'rewardMean': 0.5901426209416893, 'totalEpisodes': 226, 'stepsPerEpisode': 367, 'rewardPerEpisode': 250.74880632895588
'totalSteps': 17920, 'rewardStep': 0.8031003111892766, 'errorList': [], 'lossList': [0.0, -1.3786659342050553, 0.0, 4.025861040353775, 0.0, 0.0, 0.0], 'rewardMean': 0.6267982892302532, 'totalEpisodes': 227, 'stepsPerEpisode': 1273, 'rewardPerEpisode': 1025.9315005412946
'totalSteps': 19200, 'rewardStep': 0.6128788053591694, 'errorList': [], 'lossList': [0.0, -1.3561710608005524, 0.0, 4.440446565747261, 0.0, 0.0, 0.0], 'rewardMean': 0.6240380632331762, 'totalEpisodes': 228, 'stepsPerEpisode': 576, 'rewardPerEpisode': 427.4045751505957
'totalSteps': 20480, 'rewardStep': 0.951585169089542, 'errorList': [0.11637155337738571, 0.08119528291303965, 0.10922293858564869, 0.05801127775501588, 0.04033627321818799, 0.04660438265186861, 0.1687941678316331, 0.11844253883092906, 0.19306116360251563, 0.0748489204252404, 0.06370905321634919, 0.06962563157103992, 0.04200696509467945, 0.07882704446079877, 0.10586796553365165, 0.05576584752584348, 0.14262219496459816, 0.10532081108010254, 0.12040627063588245, 0.048814833596548625, 0.04028471828568091, 0.09758572626554873, 0.1277524365415228, 0.0722475745896005, 0.10712281432469692, 0.07396257666542619, 0.04480767951806159, 0.0420315212088662, 0.07542803285760674, 0.12920417652431943, 0.04745341904186415, 0.15917909277168024, 0.046610108250222895, 0.052920586933312884, 0.20299554565883152, 0.06696036586560461, 0.05134696530230318, 0.1042553779376978, 0.04997559384449225, 0.11578289449177757, 0.08246797712636177, 0.04322512369072621, 0.1383279359384884, 0.07418932851826723, 0.047594527733853816, 0.09666309652952411, 0.04298663940040643, 0.05658868113578915, 0.07603860709476117, 0.1664476108705294], 'lossList': [0.0, -1.349026128053665, 0.0, 2.4542459797859193, 0.0, 0.0, 0.0], 'rewardMean': 0.6928873123910751, 'totalEpisodes': 229, 'stepsPerEpisode': 1187, 'rewardPerEpisode': 1031.4518212221276, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.8718649368654803, 'errorList': [], 'lossList': [0.0, -1.311841706633568, 0.0, 1.4151083034649492, 0.0, 0.0, 0.0], 'rewardMean': 0.753764538326568, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.2024693036042
'totalSteps': 23040, 'rewardStep': 0.9602295209264282, 'errorList': [0.1151248824397919, 0.07260252892955042, 0.09330411164809405, 0.0383401065851202, 0.16632437578228784, 0.13900132257862408, 0.0903883866600828, 0.14184321077285597, 0.06511319005623496, 0.061681097718916236, 0.054271832408734115, 0.11190643528745031, 0.10506023113850926, 0.07699182059628448, 0.046890497800154765, 0.057074300601139945, 0.10455962251174579, 0.04898961195101677, 0.08954622450997295, 0.0560676132942609, 0.08196426090092437, 0.09318829045628342, 0.2438528152489594, 0.10032180150176355, 0.05298606673488233, 0.09292348868435663, 0.18287547878203828, 0.0876153451466966, 0.07906631349029428, 0.1008831348304637, 0.12143530546288568, 0.04583531619674999, 0.07862725234131047, 0.11060003456695201, 0.09194605443437816, 0.07079602587355165, 0.09880939258026103, 0.08178336511034728, 0.09590119994228322, 0.07002774200430545, 0.15808239650778685, 0.07000152245693281, 0.0535509578552065, 0.1175852307825901, 0.1030730241209358, 0.12207965753722728, 0.09995850430556459, 0.07464487291634121, 0.0536720113585897, 0.06903722091523223], 'lossList': [0.0, -1.2700673830509186, 0.0, 1.1546301971562207, 0.0, 0.0, 0.0], 'rewardMean': 0.7817292816034407, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.3485960572928, 'successfulTests': 49
'totalSteps': 24320, 'rewardStep': 0.9185451205503531, 'errorList': [], 'lossList': [0.0, -1.234703432917595, 0.0, 0.95625523975119, 0.0, 0.0, 0.0], 'rewardMean': 0.8001753188269548, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1196.9074059596483
'totalSteps': 25600, 'rewardStep': 0.8856141077042944, 'errorList': [], 'lossList': [0.0, -1.2015231442451477, 0.0, 0.5493577723763883, 0.0, 0.0, 0.0], 'rewardMean': 0.7989992048842698, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1195.3793352422856
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=20480, timeSpent=104.79
