#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 28
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.4319717568889544, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.45629424683943115, 'totalEpisodes': 76, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.067685778882613
'totalSteps': 3840, 'rewardStep': 0.857242943609722, 'errorList': [], 'lossList': [0.0, -1.4304506927728653, 0.0, 32.152692766189574, 0.0, 0.0, 0.0], 'rewardMean': 0.5565314210320038, 'totalEpisodes': 123, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.722484069538263
'totalSteps': 5120, 'rewardStep': 0.7254942927600153, 'errorList': [], 'lossList': [0.0, -1.414154708981514, 0.0, 44.04727401733398, 0.0, 0.0, 0.0], 'rewardMean': 0.590323995377606, 'totalEpisodes': 165, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.68231313197301
'totalSteps': 6400, 'rewardStep': 0.8411895343137302, 'errorList': [], 'lossList': [0.0, -1.3930968779325485, 0.0, 50.30403186798096, 0.0, 0.0, 0.0], 'rewardMean': 0.6321349185336268, 'totalEpisodes': 189, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.627531471952668
'totalSteps': 7680, 'rewardStep': 0.8052619038543685, 'errorList': [], 'lossList': [0.0, -1.3760877275466918, 0.0, 47.00034581184387, 0.0, 0.0, 0.0], 'rewardMean': 0.6568673450080185, 'totalEpisodes': 207, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.30984648342769
'totalSteps': 8960, 'rewardStep': 0.33129095619464777, 'errorList': [], 'lossList': [0.0, -1.3714665240049362, 0.0, 41.25337683677673, 0.0, 0.0, 0.0], 'rewardMean': 0.6161702964063471, 'totalEpisodes': 217, 'stepsPerEpisode': 199, 'rewardPerEpisode': 139.43104293955537
'totalSteps': 10240, 'rewardStep': 0.8846459964169917, 'errorList': [], 'lossList': [0.0, -1.3685510230064393, 0.0, 28.086564106941225, 0.0, 0.0, 0.0], 'rewardMean': 0.6460009297408632, 'totalEpisodes': 223, 'stepsPerEpisode': 177, 'rewardPerEpisode': 152.73732828192985
'totalSteps': 11520, 'rewardStep': 0.5444683142212962, 'errorList': [], 'lossList': [0.0, -1.368691738843918, 0.0, 21.241262621879578, 0.0, 0.0, 0.0], 'rewardMean': 0.6358476681889065, 'totalEpisodes': 230, 'stepsPerEpisode': 59, 'rewardPerEpisode': 43.513669102487846
'totalSteps': 12800, 'rewardStep': 0.37564817927087624, 'errorList': [], 'lossList': [0.0, -1.3639685678482056, 0.0, 16.863585660457613, 0.0, 0.0, 0.0], 'rewardMean': 0.6229185634419557, 'totalEpisodes': 237, 'stepsPerEpisode': 81, 'rewardPerEpisode': 55.63063362471722
'totalSteps': 14080, 'rewardStep': 0.7740078097868223, 'errorList': [], 'lossList': [0.0, -1.3416181129217148, 0.0, 10.05365219950676, 0.0, 0.0, 0.0], 'rewardMean': 0.6571221687317425, 'totalEpisodes': 242, 'stepsPerEpisode': 150, 'rewardPerEpisode': 111.23613296222467
'totalSteps': 15360, 'rewardStep': 0.6846121755247239, 'errorList': [], 'lossList': [0.0, -1.3469230341911316, 0.0, 8.594797979593277, 0.0, 0.0, 0.0], 'rewardMean': 0.6823862105953195, 'totalEpisodes': 248, 'stepsPerEpisode': 129, 'rewardPerEpisode': 98.96479762950585
'totalSteps': 16640, 'rewardStep': 0.8647705535622736, 'errorList': [], 'lossList': [0.0, -1.371828905940056, 0.0, 5.183778681755066, 0.0, 0.0, 0.0], 'rewardMean': 0.6831389715905746, 'totalEpisodes': 252, 'stepsPerEpisode': 234, 'rewardPerEpisode': 202.95715604301168
'totalSteps': 17920, 'rewardStep': 0.7217612733111562, 'errorList': [], 'lossList': [0.0, -1.3890560817718507, 0.0, 5.172430589795113, 0.0, 0.0, 0.0], 'rewardMean': 0.6827656696456886, 'totalEpisodes': 254, 'stepsPerEpisode': 144, 'rewardPerEpisode': 115.27466796137642
'totalSteps': 19200, 'rewardStep': 0.7390169935530073, 'errorList': [], 'lossList': [0.0, -1.3850230103731156, 0.0, 3.7118464702367784, 0.0, 0.0, 0.0], 'rewardMean': 0.6725484155696162, 'totalEpisodes': 258, 'stepsPerEpisode': 256, 'rewardPerEpisode': 194.54959390863252
'totalSteps': 20480, 'rewardStep': 0.5691065149532073, 'errorList': [], 'lossList': [0.0, -1.359610031247139, 0.0, 2.693303076028824, 0.0, 0.0, 0.0], 'rewardMean': 0.6489328766795003, 'totalEpisodes': 260, 'stepsPerEpisode': 405, 'rewardPerEpisode': 294.29119384590234
'totalSteps': 21760, 'rewardStep': 0.8718303137584347, 'errorList': [], 'lossList': [0.0, -1.3201373320817948, 0.0, 2.512255299538374, 0.0, 0.0, 0.0], 'rewardMean': 0.7029868124358789, 'totalEpisodes': 261, 'stepsPerEpisode': 1237, 'rewardPerEpisode': 1061.4070092930838
'totalSteps': 23040, 'rewardStep': 0.8110116298615815, 'errorList': [], 'lossList': [0.0, -1.268754163980484, 0.0, 1.569570872709155, 0.0, 0.0, 0.0], 'rewardMean': 0.6956233757803378, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.495041881298
'totalSteps': 24320, 'rewardStep': 0.887125651636753, 'errorList': [], 'lossList': [0.0, -1.2134459322690965, 0.0, 1.3253179750591517, 0.0, 0.0, 0.0], 'rewardMean': 0.7298891095218836, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.0060479862673
'totalSteps': 25600, 'rewardStep': 0.9551165067403911, 'errorList': [0.0764111214070155, 0.05291011381937846, 0.05919987127128102, 0.057379369695519766, 0.05418809400169451, 0.09426594057672896, 0.05435126734147514, 0.08562491439164947, 0.06203415356963934, 0.06356134647575802, 0.07886296347365464, 0.06670258605816076, 0.08416239877223593, 0.056891775707469755, 0.06321716502678047, 0.08321795798981357, 0.09139827549816937, 0.055103522581350414, 0.08001658340997955, 0.07179392699202915, 0.10610329706010771, 0.08126502333833825, 0.07253942234156441, 0.06798936922875043, 0.05344280117294195, 0.07523882978811956, 0.07254723707924864, 0.053681039600404046, 0.060560335589250634, 0.056417611149826345, 0.06737250052835271, 0.05429389535270255, 0.0598207609876178, 0.06773516665613641, 0.05994210626657478, 0.07165862654582698, 0.05590633440765522, 0.07765272867523887, 0.06631344276589132, 0.058791499062922616, 0.05694132088648045, 0.08841541396983445, 0.06608723575746885, 0.06720504414860304, 0.056919395979356864, 0.09275714712266406, 0.052487152322693895, 0.07899480961552925, 0.08483957412150352, 0.06526929715716603], 'lossList': [0.0, -1.1823613721132278, 0.0, 1.075701259225607, 0.0, 0.0, 0.0], 'rewardMean': 0.787835942268835, 'totalEpisodes': 261, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.338536162521, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.4
