#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 29
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.6921958354759865, 'errorList': [], 'lossList': [0.0, -1.408168624639511, 0.0, 34.205609493255615, 0.0, 0.0, 0.0], 'rewardMean': 0.8066429848136969, 'totalEpisodes': 83, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.57643146302815
'totalSteps': 3840, 'rewardStep': 0.8029215660791744, 'errorList': [], 'lossList': [0.0, -1.4017834150791169, 0.0, 43.039742650985715, 0.0, 0.0, 0.0], 'rewardMean': 0.8054025119021894, 'totalEpisodes': 126, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.196861980920275
'totalSteps': 5120, 'rewardStep': 0.8670004460380148, 'errorList': [], 'lossList': [0.0, -1.393847953081131, 0.0, 46.35474494934082, 0.0, 0.0, 0.0], 'rewardMean': 0.8208019954361457, 'totalEpisodes': 142, 'stepsPerEpisode': 85, 'rewardPerEpisode': 62.7305154339828
'totalSteps': 6400, 'rewardStep': 0.9147863397925691, 'errorList': [], 'lossList': [0.0, -1.3871791112422942, 0.0, 53.43913094520569, 0.0, 0.0, 0.0], 'rewardMean': 0.8395988643074304, 'totalEpisodes': 154, 'stepsPerEpisode': 37, 'rewardPerEpisode': 31.343049569312953
'totalSteps': 7680, 'rewardStep': 0.7348455085127699, 'errorList': [], 'lossList': [0.0, -1.3757333225011825, 0.0, 46.97155161380768, 0.0, 0.0, 0.0], 'rewardMean': 0.822139971674987, 'totalEpisodes': 163, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.619350004079315
'totalSteps': 8960, 'rewardStep': 0.6066176277086006, 'errorList': [], 'lossList': [0.0, -1.366187332868576, 0.0, 36.57288590431213, 0.0, 0.0, 0.0], 'rewardMean': 0.7913510653940746, 'totalEpisodes': 171, 'stepsPerEpisode': 212, 'rewardPerEpisode': 137.87001541075384
'totalSteps': 10240, 'rewardStep': 0.9087774410518048, 'errorList': [], 'lossList': [0.0, -1.3616443836688996, 0.0, 10.679095034599305, 0.0, 0.0, 0.0], 'rewardMean': 0.8060293623512909, 'totalEpisodes': 175, 'stepsPerEpisode': 101, 'rewardPerEpisode': 83.09251391349369
'totalSteps': 11520, 'rewardStep': 0.8518131347421966, 'errorList': [], 'lossList': [0.0, -1.3523785644769668, 0.0, 8.32827556401491, 0.0, 0.0, 0.0], 'rewardMean': 0.8111164481725026, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 972.8875130835827
'totalSteps': 12800, 'rewardStep': 0.8382424287252557, 'errorList': [], 'lossList': [0.0, -1.3339420807361604, 0.0, 33.909348101615905, 0.0, 0.0, 0.0], 'rewardMean': 0.813829046227778, 'totalEpisodes': 178, 'stepsPerEpisode': 63, 'rewardPerEpisode': 54.35172256663361
'totalSteps': 14080, 'rewardStep': 0.9505779277973985, 'errorList': [0.4932006907403964, 0.6081957996504654, 0.5191211938409857, 0.5253452841122821, 0.4153445887219673, 0.4268419637939253, 0.4565158875956764, 0.46875245282380773, 0.5481700945819911, 0.4276009801896099, 0.4643269033474114, 0.5419164080694251, 0.4933413171519045, 0.4842669503588469, 0.4580646876794936, 0.6589228101025644, 0.4318911664485054, 0.4524112216939768, 0.49932619608436773, 0.5480968277465889, 0.4290384306040922, 0.5166540623394525, 0.5249409122185473, 0.5971434996994994, 0.38328471790513724, 0.463705214359887, 0.5057606350483892, 0.49285493039221173, 0.5302266163303361, 0.5481920454140796, 0.4908053779575243, 0.5150028172184058, 0.6293380927721396, 0.5401681368899489, 0.4740732649476669, 0.6111471552667083, 0.4677540916731596, 0.4732660501148267, 0.5649217885354456, 0.510697043378243, 0.45513606844218263, 0.4835644120698272, 0.556960484256961, 0.5649066367750003, 0.49741804886860047, 0.545247818176825, 0.501952306927513, 0.48630450284112925, 0.4895485178955535, 0.4239172930894574], 'lossList': [0.0, -1.3236954301595687, 0.0, 7.817471714317799, 0.0, 0.0, 0.0], 'rewardMean': 0.8167778255923771, 'totalEpisodes': 179, 'stepsPerEpisode': 833, 'rewardPerEpisode': 673.51095484057, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7930172448024264, 'errorList': [], 'lossList': [0.0, -1.328458644747734, 0.0, 4.136943320333958, 0.0, 0.0, 0.0], 'rewardMean': 0.826859966525021, 'totalEpisodes': 179, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 994.1394991528248
'totalSteps': 16640, 'rewardStep': 0.6362296084284621, 'errorList': [], 'lossList': [0.0, -1.317876664996147, 0.0, 23.26263287425041, 0.0, 0.0, 0.0], 'rewardMean': 0.8101907707599498, 'totalEpisodes': 180, 'stepsPerEpisode': 446, 'rewardPerEpisode': 378.8053833749736
'totalSteps': 17920, 'rewardStep': 0.8217943954989816, 'errorList': [], 'lossList': [0.0, -1.3004080581665038, 0.0, 2.4940872198343276, 0.0, 0.0, 0.0], 'rewardMean': 0.8056701657060465, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.530105847908
'totalSteps': 19200, 'rewardStep': 0.9020390910346507, 'errorList': [], 'lossList': [0.0, -1.2687358623743057, 0.0, 1.3205551370978355, 0.0, 0.0, 0.0], 'rewardMean': 0.8043954408302547, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1084.1442036737405
'totalSteps': 20480, 'rewardStep': 0.932503416883269, 'errorList': [0.12076126406652432, 0.06346923001382763, 0.15278083333602444, 0.0990077913938541, 0.1289547995272359, 0.16015669767412588, 0.12022557678705299, 0.10280530204330213, 0.11770441592110956, 0.08523617374805811, 0.18252360979930765, 0.080670581846764, 0.08748577476997929, 0.11290286790210032, 0.1135517267073774, 0.10529822777126957, 0.10722520030709809, 0.08323936547740494, 0.12267530610522161, 0.12446501466933296, 0.129230028169925, 0.14639211330476695, 0.1572854784340022, 0.09606630524768339, 0.12472138987429307, 0.19145040580535122, 0.12708571530025714, 0.1240343384638554, 0.1580792922346705, 0.13324474914468282, 0.08139369792962557, 0.12294313749143469, 0.1437281656227336, 0.1161655495503041, 0.13348572615504628, 0.11056832443772008, 0.168271645404825, 0.10774100254772281, 0.12942493239930955, 0.10317655213502955, 0.15656032724060226, 0.14811664511165334, 0.16914546097197086, 0.16923775141256425, 0.09869867224722385, 0.07599215528555546, 0.11803654307378804, 0.16769362473903782, 0.10733957324065695, 0.10485626985821486], 'lossList': [0.0, -1.234665293097496, 0.0, 1.2644404908269644, 0.0, 0.0, 0.0], 'rewardMean': 0.8241612316673047, 'totalEpisodes': 180, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.3310451030527, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=92.58
