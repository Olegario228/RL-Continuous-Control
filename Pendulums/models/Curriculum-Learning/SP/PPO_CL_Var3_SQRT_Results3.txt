#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.5987247928836995, 'errorList': [], 'lossList': [0.0, -1.4423491144180298, 0.0, 29.158586225509644, 0.0, 0.0, 0.0], 'rewardMean': 0.5518320098120422, 'totalEpisodes': 82, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5987247928836995
'totalSteps': 3840, 'rewardStep': 0.7801029503672158, 'errorList': [], 'lossList': [0.0, -1.4402692371606827, 0.0, 40.7276385974884, 0.0, 0.0, 0.0], 'rewardMean': 0.6279223233304334, 'totalEpisodes': 118, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.356504406728451
'totalSteps': 5120, 'rewardStep': 0.8989131281814023, 'errorList': [], 'lossList': [0.0, -1.4193248265981675, 0.0, 52.212562561035156, 0.0, 0.0, 0.0], 'rewardMean': 0.6956700245431756, 'totalEpisodes': 143, 'stepsPerEpisode': 98, 'rewardPerEpisode': 78.28205444916011
'totalSteps': 6400, 'rewardStep': 0.809924127032613, 'errorList': [], 'lossList': [0.0, -1.395015637278557, 0.0, 67.32270643234253, 0.0, 0.0, 0.0], 'rewardMean': 0.7185208450410631, 'totalEpisodes': 173, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.74764117687523
'totalSteps': 7680, 'rewardStep': 0.6930014740930568, 'errorList': [], 'lossList': [0.0, -1.382955954670906, 0.0, 65.55523681640625, 0.0, 0.0, 0.0], 'rewardMean': 0.7142676165497287, 'totalEpisodes': 197, 'stepsPerEpisode': 82, 'rewardPerEpisode': 63.044227804836396
'totalSteps': 8960, 'rewardStep': 0.5998548116571216, 'errorList': [], 'lossList': [0.0, -1.3836163240671158, 0.0, 32.47575192451477, 0.0, 0.0, 0.0], 'rewardMean': 0.6979229301364992, 'totalEpisodes': 209, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.10709377190486
'totalSteps': 10240, 'rewardStep': 0.6276299846763204, 'errorList': [], 'lossList': [0.0, -1.401155413389206, 0.0, 15.457368913888931, 0.0, 0.0, 0.0], 'rewardMean': 0.6891363119539768, 'totalEpisodes': 215, 'stepsPerEpisode': 71, 'rewardPerEpisode': 55.809079507320675
'totalSteps': 11520, 'rewardStep': 0.8667851240091201, 'errorList': [], 'lossList': [0.0, -1.4066795599460602, 0.0, 14.425444355010987, 0.0, 0.0, 0.0], 'rewardMean': 0.7088750688489928, 'totalEpisodes': 222, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.262539370630627
'totalSteps': 12800, 'rewardStep': 0.7267370363621493, 'errorList': [], 'lossList': [0.0, -1.3988269698619842, 0.0, 19.375801420211793, 0.0, 0.0, 0.0], 'rewardMean': 0.7106612656003085, 'totalEpisodes': 229, 'stepsPerEpisode': 38, 'rewardPerEpisode': 32.325832428837266
'totalSteps': 14080, 'rewardStep': 0.8657070610281012, 'errorList': [], 'lossList': [0.0, -1.4074331879615785, 0.0, 8.594251198768616, 0.0, 0.0, 0.0], 'rewardMean': 0.7467380490290799, 'totalEpisodes': 234, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.894905531299627
'totalSteps': 15360, 'rewardStep': 0.8054114458313667, 'errorList': [], 'lossList': [0.0, -1.4186286801099777, 0.0, 8.360637900233268, 0.0, 0.0, 0.0], 'rewardMean': 0.7674067143238468, 'totalEpisodes': 236, 'stepsPerEpisode': 434, 'rewardPerEpisode': 388.3557784035744
'totalSteps': 16640, 'rewardStep': 0.7904357899785432, 'errorList': [], 'lossList': [0.0, -1.4066372281312942, 0.0, 2.5307542699575425, 0.0, 0.0, 0.0], 'rewardMean': 0.7684399982849796, 'totalEpisodes': 241, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.6189411021693
'totalSteps': 17920, 'rewardStep': 0.893466003167487, 'errorList': [], 'lossList': [0.0, -1.3896220928430558, 0.0, 4.019595886468887, 0.0, 0.0, 0.0], 'rewardMean': 0.7678952857835879, 'totalEpisodes': 244, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.566254266044666
'totalSteps': 19200, 'rewardStep': 0.7788452002295336, 'errorList': [], 'lossList': [0.0, -1.3949183255434037, 0.0, 4.618470765054226, 0.0, 0.0, 0.0], 'rewardMean': 0.76478739310328, 'totalEpisodes': 245, 'stepsPerEpisode': 464, 'rewardPerEpisode': 394.22501997922706
'totalSteps': 20480, 'rewardStep': 0.6423139396966695, 'errorList': [], 'lossList': [0.0, -1.3667191207408904, 0.0, 2.0601645439863203, 0.0, 0.0, 0.0], 'rewardMean': 0.7597186396636413, 'totalEpisodes': 246, 'stepsPerEpisode': 382, 'rewardPerEpisode': 281.3400795321584
'totalSteps': 21760, 'rewardStep': 0.8698529207825402, 'errorList': [], 'lossList': [0.0, -1.3393483144044875, 0.0, 2.5892172837257386, 0.0, 0.0, 0.0], 'rewardMean': 0.7867184505761832, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 990.7427219042597
'totalSteps': 23040, 'rewardStep': 0.8372254247788166, 'errorList': [], 'lossList': [0.0, -1.3015489035844803, 0.0, 1.7490451929345727, 0.0, 0.0, 0.0], 'rewardMean': 0.8076779945864327, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.270392194877
'totalSteps': 24320, 'rewardStep': 0.8863755335961191, 'errorList': [], 'lossList': [0.0, -1.2375653171539307, 0.0, 1.2409936455637216, 0.0, 0.0, 0.0], 'rewardMean': 0.8096370355451328, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.5420531238103
'totalSteps': 25600, 'rewardStep': 0.9530628022753385, 'errorList': [0.03018335809678005, 0.04198202193181145, 0.023560444237638502, 0.0239076724838116, 0.030958366134880667, 0.04957034604798995, 0.03814845864703648, 0.026192415608996062, 0.02405287081257701, 0.026510940179925363, 0.03419201602392883, 0.03487201277964702, 0.046900066722406285, 0.05909709234871759, 0.03572024475963213, 0.029242407876286892, 0.023678625279128236, 0.0361502232736685, 0.04605021524047441, 0.0496933548343343, 0.056153808517236725, 0.03809128869334292, 0.03792438964553716, 0.02262925817862726, 0.043515040262465, 0.03764828257773842, 0.03440202805762173, 0.04929437928453972, 0.024776057337770135, 0.027162839992407442, 0.03153635434857937, 0.051467034868060486, 0.03717272684644717, 0.027612757413557194, 0.02449529551745829, 0.029174040046663158, 0.04551567120093764, 0.023766525206357227, 0.03276953069761888, 0.027541122923549925, 0.03409279666711653, 0.02976616718134393, 0.04057916062586063, 0.05550990382027849, 0.04087327357084901, 0.03686225437576561, 0.030159631855700614, 0.023158024515724452, 0.033746341500422695, 0.04086750765955142], 'lossList': [0.0, -1.2058642649650573, 0.0, 1.017252471614629, 0.0, 0.0, 0.0], 'rewardMean': 0.8322696121364516, 'totalEpisodes': 246, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1202.2706711079586, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=75.52
