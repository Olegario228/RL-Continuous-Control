#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 31
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6316441097508718, 'errorList': [], 'lossList': [0.0, -1.4145234042406083, 0.0, 33.747171001434324, 0.0, 0.0, 0.0], 'rewardMean': 0.7974668308307908, 'totalEpisodes': 32, 'stepsPerEpisode': 39, 'rewardPerEpisode': 34.80031321911144
'totalSteps': 3840, 'rewardStep': 0.7067015792325759, 'errorList': [], 'lossList': [0.0, -1.402945072054863, 0.0, 48.845737800598144, 0.0, 0.0, 0.0], 'rewardMean': 0.7672117469647192, 'totalEpisodes': 56, 'stepsPerEpisode': 27, 'rewardPerEpisode': 18.255054952948896
'totalSteps': 5120, 'rewardStep': 0.7313845430918273, 'errorList': [], 'lossList': [0.0, -1.3720517456531525, 0.0, 47.881717138290405, 0.0, 0.0, 0.0], 'rewardMean': 0.7582549459964962, 'totalEpisodes': 71, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.096016734891098
'totalSteps': 6400, 'rewardStep': 0.9193424533081728, 'errorList': [], 'lossList': [0.0, -1.3540499597787856, 0.0, 83.81717220306396, 0.0, 0.0, 0.0], 'rewardMean': 0.7904724474588315, 'totalEpisodes': 95, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9193424533081728
'totalSteps': 7680, 'rewardStep': 0.2768100020362346, 'errorList': [], 'lossList': [0.0, -1.3405118757486343, 0.0, 23.15433171272278, 0.0, 0.0, 0.0], 'rewardMean': 0.7048620398883987, 'totalEpisodes': 102, 'stepsPerEpisode': 117, 'rewardPerEpisode': 77.49693375718371
'totalSteps': 8960, 'rewardStep': 0.7759926608644487, 'errorList': [], 'lossList': [0.0, -1.3178672498464585, 0.0, 32.688779940605166, 0.0, 0.0, 0.0], 'rewardMean': 0.7150235571706915, 'totalEpisodes': 111, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.74194624631243
'totalSteps': 10240, 'rewardStep': 0.6621212795048237, 'errorList': [], 'lossList': [0.0, -1.3120409631729126, 0.0, 12.838246018886567, 0.0, 0.0, 0.0], 'rewardMean': 0.7084107724624581, 'totalEpisodes': 114, 'stepsPerEpisode': 256, 'rewardPerEpisode': 207.89826741741754
'totalSteps': 11520, 'rewardStep': 0.6495422381138345, 'errorList': [], 'lossList': [0.0, -1.3214868301153182, 0.0, 10.541000382900238, 0.0, 0.0, 0.0], 'rewardMean': 0.7018698242014999, 'totalEpisodes': 117, 'stepsPerEpisode': 166, 'rewardPerEpisode': 146.18849989286974
'totalSteps': 12800, 'rewardStep': 0.627837578726113, 'errorList': [], 'lossList': [0.0, -1.3174278670549393, 0.0, 8.138021849393844, 0.0, 0.0, 0.0], 'rewardMean': 0.6944665996539612, 'totalEpisodes': 118, 'stepsPerEpisode': 915, 'rewardPerEpisode': 653.336858684041
'totalSteps': 14080, 'rewardStep': 0.713959691698739, 'errorList': [], 'lossList': [0.0, -1.297686755657196, 0.0, 6.338151396512985, 0.0, 0.0, 0.0], 'rewardMean': 0.6695336136327642, 'totalEpisodes': 120, 'stepsPerEpisode': 215, 'rewardPerEpisode': 170.1767956262125
'totalSteps': 15360, 'rewardStep': 0.57519505024759, 'errorList': [], 'lossList': [0.0, -1.2781932139396668, 0.0, 4.6588501125574115, 0.0, 0.0, 0.0], 'rewardMean': 0.663888707682436, 'totalEpisodes': 123, 'stepsPerEpisode': 462, 'rewardPerEpisode': 362.22063165284567
'totalSteps': 16640, 'rewardStep': 0.6874685964669964, 'errorList': [], 'lossList': [0.0, -1.2560879957675934, 0.0, 5.3110283803939815, 0.0, 0.0, 0.0], 'rewardMean': 0.661965409405878, 'totalEpisodes': 125, 'stepsPerEpisode': 139, 'rewardPerEpisode': 108.58329153783849
'totalSteps': 17920, 'rewardStep': 0.5650734692412336, 'errorList': [], 'lossList': [0.0, -1.2436884355545044, 0.0, 1.8372246460616588, 0.0, 0.0, 0.0], 'rewardMean': 0.6453343020208187, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 919.6092692132017
'totalSteps': 19200, 'rewardStep': 0.7471699693299511, 'errorList': [], 'lossList': [0.0, -1.2253596991300584, 0.0, 0.950898722037673, 0.0, 0.0, 0.0], 'rewardMean': 0.6281170536229965, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1041.8501715877726
'totalSteps': 20480, 'rewardStep': 0.8395637473244137, 'errorList': [], 'lossList': [0.0, -1.1634310340881349, 0.0, 0.9933310428634285, 0.0, 0.0, 0.0], 'rewardMean': 0.6843924281518144, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.4472677163792
'totalSteps': 21760, 'rewardStep': 0.8342047490412942, 'errorList': [], 'lossList': [0.0, -1.0938432145118713, 0.0, 0.9313558332622052, 0.0, 0.0, 0.0], 'rewardMean': 0.690213636969499, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.7834748647986
'totalSteps': 23040, 'rewardStep': 0.9678105194593536, 'errorList': [0.0737849357276002, 0.11852073285437406, 0.055066778316484606, 0.09990950846000418, 0.06290221194219324, 0.0668744074286981, 0.09516326282915787, 0.07915855755368001, 0.09604018694038798, 0.06982238488057062, 0.06353129780805176, 0.07310535767113566, 0.147400524392527, 0.07379518474464833, 0.10280610466316305, 0.12344647596428414, 0.10417261424246738, 0.11321979762941312, 0.05821731426876115, 0.16407708354405223, 0.08846700575043201, 0.053018179123688704, 0.07140459045354047, 0.09417499536576876, 0.14746082628981713, 0.06823071859660755, 0.13220986131029186, 0.11093767261011195, 0.06614599290884254, 0.07454190213816186, 0.08116766752716599, 0.1351010460688475, 0.09985183506079787, 0.15320133557200905, 0.05782286496703404, 0.09695095866466744, 0.09181105174138392, 0.15381314429781198, 0.09016615832118499, 0.06423841419512819, 0.08872257758454233, 0.07765028014900342, 0.0916100348945651, 0.07912148230998038, 0.10342792575213726, 0.07484608701613475, 0.0705723733370772, 0.14914562780667762, 0.12067598092094046, 0.06720513052362724], 'lossList': [0.0, -1.0541290241479873, 0.0, 0.7603197316266597, 0.0, 0.0, 0.0], 'rewardMean': 0.720782560964952, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.3552499220623, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=78.89
