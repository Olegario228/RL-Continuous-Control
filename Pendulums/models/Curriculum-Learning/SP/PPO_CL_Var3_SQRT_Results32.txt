#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 32
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.5706886503407744, 'errorList': [], 'lossList': [0.0, -1.4169341576099397, 0.0, 35.5687498664856, 0.0, 0.0, 0.0], 'rewardMean': 0.6822293257604628, 'totalEpisodes': 37, 'stepsPerEpisode': 7, 'rewardPerEpisode': 3.777278552709098
'totalSteps': 3840, 'rewardStep': 0.6951810073595845, 'errorList': [], 'lossList': [0.0, -1.3898907500505446, 0.0, 49.346765975952145, 0.0, 0.0, 0.0], 'rewardMean': 0.68654655296017, 'totalEpisodes': 58, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.884240747456907
'totalSteps': 5120, 'rewardStep': 0.6869921978789132, 'errorList': [], 'lossList': [0.0, -1.356753986477852, 0.0, 63.12640821456909, 0.0, 0.0, 0.0], 'rewardMean': 0.6866579641898558, 'totalEpisodes': 81, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.637410351365435
'totalSteps': 6400, 'rewardStep': 0.6924714319106704, 'errorList': [], 'lossList': [0.0, -1.3398136067390443, 0.0, 79.56320404052734, 0.0, 0.0, 0.0], 'rewardMean': 0.6878206577340187, 'totalEpisodes': 107, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.22261910087445
'totalSteps': 7680, 'rewardStep': 0.8708772494737355, 'errorList': [], 'lossList': [0.0, -1.341952812075615, 0.0, 75.65135025024414, 0.0, 0.0, 0.0], 'rewardMean': 0.718330089690638, 'totalEpisodes': 132, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.52879138487609
'totalSteps': 8960, 'rewardStep': 0.888891262779164, 'errorList': [], 'lossList': [0.0, -1.3438458037376404, 0.0, 55.20588934898377, 0.0, 0.0, 0.0], 'rewardMean': 0.7426959715604274, 'totalEpisodes': 149, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.640798552077207
'totalSteps': 10240, 'rewardStep': 0.4320143799174465, 'errorList': [], 'lossList': [0.0, -1.3526840728521348, 0.0, 16.38617064476013, 0.0, 0.0, 0.0], 'rewardMean': 0.703860772605055, 'totalEpisodes': 161, 'stepsPerEpisode': 23, 'rewardPerEpisode': 13.194371532397007
'totalSteps': 11520, 'rewardStep': 0.9341281254675521, 'errorList': [53.16926733377674, 4.7458204673963476, 42.668610040599845, 4.687646875291729, 6.214277155601002, 76.7613063058156, 18.354328042506612, 34.39591161615264, 46.71468802453754, 72.48055422067267, 16.09073406633953, 55.073758663847215, 91.6978962283626, 57.75718667317296, 55.118304647323605, 40.297019887037436, 8.835411180062206, 48.81800370412207, 84.53371289400955, 65.31031473410846, 84.19135286017381, 32.802933788182685, 3.1832325598023576, 23.21523629222727, 101.54555239083284, 53.76186149989303, 21.414597520426607, 65.26844735760602, 53.04170317399559, 84.9185458186156, 11.989976848290185, 88.39065910604366, 1.1804662631189367, 1.9047987471774066, 63.32651415694023, 41.67734453063401, 52.61890752303112, 90.34921355985581, 43.931418800030016, 27.88494094887057, 5.509632337931883, 92.09712111628771, 37.23106410714964, 1.2241659300176913, 78.44918847945009, 59.64867864109066, 105.32069870147315, 84.25207183643934, 58.01582693904426, 66.35405162781588], 'lossList': [0.0, -1.369114779829979, 0.0, 19.834923889636993, 0.0, 0.0, 0.0], 'rewardMean': 0.7294460340342214, 'totalEpisodes': 169, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.51254103887615, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.8882068936072398, 'errorList': [], 'lossList': [0.0, -1.3678040927648545, 0.0, 18.13219196796417, 0.0, 0.0, 0.0], 'rewardMean': 0.7453221199915232, 'totalEpisodes': 177, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.208765090140924
'totalSteps': 14080, 'rewardStep': 0.8068182733739464, 'errorList': [], 'lossList': [0.0, -1.3587843352556228, 0.0, 9.40585188150406, 0.0, 0.0, 0.0], 'rewardMean': 0.7466269472109027, 'totalEpisodes': 181, 'stepsPerEpisode': 85, 'rewardPerEpisode': 67.02393682468667
'totalSteps': 15360, 'rewardStep': 0.9076576584621118, 'errorList': [], 'lossList': [0.0, -1.356850723028183, 0.0, 5.503719898462296, 0.0, 0.0, 0.0], 'rewardMean': 0.7803238480230363, 'totalEpisodes': 187, 'stepsPerEpisode': 220, 'rewardPerEpisode': 186.93907418517568
'totalSteps': 16640, 'rewardStep': 0.8218993154719001, 'errorList': [], 'lossList': [0.0, -1.3679311674833299, 0.0, 3.7653184485435487, 0.0, 0.0, 0.0], 'rewardMean': 0.792995678834268, 'totalEpisodes': 192, 'stepsPerEpisode': 63, 'rewardPerEpisode': 52.160204046453096
'totalSteps': 17920, 'rewardStep': 0.8665665018044768, 'errorList': [], 'lossList': [0.0, -1.383909227848053, 0.0, 4.26801603615284, 0.0, 0.0, 0.0], 'rewardMean': 0.8109531092268243, 'totalEpisodes': 195, 'stepsPerEpisode': 419, 'rewardPerEpisode': 367.50040026607934
'totalSteps': 19200, 'rewardStep': 0.9020805030049845, 'errorList': [], 'lossList': [0.0, -1.3813800936937333, 0.0, 2.979115344583988, 0.0, 0.0, 0.0], 'rewardMean': 0.8319140163362556, 'totalEpisodes': 197, 'stepsPerEpisode': 565, 'rewardPerEpisode': 487.5731819360129
'totalSteps': 20480, 'rewardStep': 0.9476911920107571, 'errorList': [0.4318188701407701, 0.8654363252858024, 0.39465607888526366, 0.26242863674897426, 0.9143542058090365, 0.5202202281633486, 0.5680965119828824, 3.432818959634004, 2.8051715828737396, 0.26532114448070393, 0.18419055554696925, 0.8722584049197442, 0.17967967830344223, 0.18978914128117172, 0.4957218862292656, 0.23226826662580832, 0.1801960780981766, 0.35284412724701986, 0.11787981473467443, 0.3041289173288855, 0.054843868951023625, 0.898696125055757, 0.3460680565322722, 0.43431995954384484, 1.2601755131671644, 2.2566656293178444, 0.25220059536930156, 0.2131347736044374, 1.2036181887757387, 0.7117390333174557, 1.7250517528844136, 2.8335670959619708, 0.5617466054933252, 0.3152028155519485, 0.5612418606997064, 0.6460236135742209, 1.16539580622853, 0.7365618134422448, 0.519813801046823, 1.3640528153764837, 0.07887857837156205, 0.36088442611125937, 0.1798206690806106, 0.41748293777236123, 0.6730115050058153, 0.9451158281753888, 0.09898500238945439, 0.853713677076972, 0.19461861131122224, 0.443934870948876], 'lossList': [0.0, -1.358504474759102, 0.0, 1.8991855838894844, 0.0, 0.0, 0.0], 'rewardMean': 0.8395954105899579, 'totalEpisodes': 201, 'stepsPerEpisode': 61, 'rewardPerEpisode': 54.071395359790046, 'successfulTests': 10
'totalSteps': 21760, 'rewardStep': 0.6923337117711685, 'errorList': [], 'lossList': [0.0, -1.3189515155553817, 0.0, 2.3423398405313494, 0.0, 0.0, 0.0], 'rewardMean': 0.8199396554891584, 'totalEpisodes': 201, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.5353538839852
'totalSteps': 23040, 'rewardStep': 0.901822556020575, 'errorList': [], 'lossList': [0.0, -1.295579795241356, 0.0, 2.1004951187968253, 0.0, 0.0, 0.0], 'rewardMean': 0.8669204730994713, 'totalEpisodes': 203, 'stepsPerEpisode': 167, 'rewardPerEpisode': 152.7115716424988
'totalSteps': 24320, 'rewardStep': 0.8841019811543097, 'errorList': [], 'lossList': [0.0, -1.2751748669147491, 0.0, 1.980314319729805, 0.0, 0.0, 0.0], 'rewardMean': 0.861917858668147, 'totalEpisodes': 204, 'stepsPerEpisode': 337, 'rewardPerEpisode': 277.4091766258812
'totalSteps': 25600, 'rewardStep': 0.8847950581532874, 'errorList': [], 'lossList': [0.0, -1.2370316076278687, 0.0, 1.1442717557400466, 0.0, 0.0, 0.0], 'rewardMean': 0.8615766751227518, 'totalEpisodes': 204, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.4265228608174
#maxSuccessfulTests=10, maxSuccessfulTestsAtStep=20480, timeSpent=106.01
