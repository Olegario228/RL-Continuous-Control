#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 34
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8448231011714354, 'errorList': [], 'lossList': [0.0, -1.4023853302001954, 0.0, 29.48182047843933, 0.0, 0.0, 0.0], 'rewardMean': 0.6044466034735121, 'totalEpisodes': 35, 'stepsPerEpisode': 37, 'rewardPerEpisode': 26.939966004942622
'totalSteps': 3840, 'rewardStep': 0.9623592940402959, 'errorList': [], 'lossList': [0.0, -1.3864101910591125, 0.0, 49.4870036315918, 0.0, 0.0, 0.0], 'rewardMean': 0.72375083366244, 'totalEpisodes': 65, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.52059345981904
'totalSteps': 5120, 'rewardStep': 0.9589022019725499, 'errorList': [], 'lossList': [0.0, -1.3744056576490402, 0.0, 53.814512243270876, 0.0, 0.0, 0.0], 'rewardMean': 0.7825386757399675, 'totalEpisodes': 84, 'stepsPerEpisode': 41, 'rewardPerEpisode': 28.885349868361207
'totalSteps': 6400, 'rewardStep': 0.7583174196103953, 'errorList': [], 'lossList': [0.0, -1.3602199631929397, 0.0, 68.60204210281373, 0.0, 0.0, 0.0], 'rewardMean': 0.777694424514053, 'totalEpisodes': 108, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.788078979453102
'totalSteps': 7680, 'rewardStep': 0.9013771501688891, 'errorList': [], 'lossList': [0.0, -1.3529549092054367, 0.0, 84.98994663238526, 0.0, 0.0, 0.0], 'rewardMean': 0.7983082121231924, 'totalEpisodes': 139, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.78421470948971
'totalSteps': 8960, 'rewardStep': 0.39120819636178367, 'errorList': [], 'lossList': [0.0, -1.3475341737270354, 0.0, 35.32072814464569, 0.0, 0.0, 0.0], 'rewardMean': 0.7401510670144197, 'totalEpisodes': 149, 'stepsPerEpisode': 111, 'rewardPerEpisode': 74.19953520555093
'totalSteps': 10240, 'rewardStep': 0.9630587502297091, 'errorList': [0.22363000756188933, 0.4396832029738144, 0.3943290011300385, 0.3856011180593458, 0.1992526673449161, 0.3794589594755268, 0.5673908992190395, 0.1480766278641423, 0.8137687184217925, 0.09465210044714542, 0.17925143011090958, 0.34320414811704436, 0.3499575419250637, 0.14329309512911645, 0.29949970174778656, 0.4364587084920663, 0.4347654803634805, 0.6115833814516526, 0.39678728700620997, 0.12057155988740716, 0.21856871481651527, 0.2553058623115424, 0.5226421449376466, 0.11715092757676004, 0.5095233746661912, 0.5920138163350747, 0.49455492562636855, 0.1989147481706653, 0.20369346324690363, 0.39100240423724497, 0.38923697550353215, 0.14889154187237078, 0.42821578379355235, 0.22327560603674482, 0.391309213580687, 0.3248597022711353, 0.6790952191126062, 0.23708607277105734, 0.5454509028794693, 0.4461627920039816, 0.2075736148013061, 0.33691446991222745, 0.5611641565894067, 0.15287960806066186, 0.10655847861756707, 0.306825116414039, 0.3767248170827025, 0.20011163121270514, 0.13279322376937105, 0.3008125799268891], 'lossList': [0.0, -1.333923318386078, 0.0, 25.489174942970276, 0.0, 0.0, 0.0], 'rewardMean': 0.7680145274163309, 'totalEpisodes': 157, 'stepsPerEpisode': 44, 'rewardPerEpisode': 37.0881380309778, 'successfulTests': 12
'totalSteps': 11520, 'rewardStep': 0.5441958878013997, 'errorList': [], 'lossList': [0.0, -1.3257195150852203, 0.0, 12.63853669166565, 0.0, 0.0, 0.0], 'rewardMean': 0.7431457896813386, 'totalEpisodes': 158, 'stepsPerEpisode': 380, 'rewardPerEpisode': 287.4598037503057
'totalSteps': 12800, 'rewardStep': 0.8101585520938289, 'errorList': [], 'lossList': [0.0, -1.3144589388370513, 0.0, 14.19687241256237, 0.0, 0.0, 0.0], 'rewardMean': 0.7498470659225875, 'totalEpisodes': 161, 'stepsPerEpisode': 57, 'rewardPerEpisode': 47.70896079120493
'totalSteps': 14080, 'rewardStep': 0.9118688088872092, 'errorList': [], 'lossList': [0.0, -1.298305544257164, 0.0, 8.850688345134259, 0.0, 0.0, 0.0], 'rewardMean': 0.8046269362337496, 'totalEpisodes': 162, 'stepsPerEpisode': 1098, 'rewardPerEpisode': 916.5361365192774
'totalSteps': 15360, 'rewardStep': 0.8580771097400256, 'errorList': [], 'lossList': [0.0, -1.2921093600988387, 0.0, 5.445238323658705, 0.0, 0.0, 0.0], 'rewardMean': 0.8059523370906087, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.0483524912532
'totalSteps': 16640, 'rewardStep': 0.7336274609418815, 'errorList': [], 'lossList': [0.0, -1.2610618430376053, 0.0, 3.7736714099347592, 0.0, 0.0, 0.0], 'rewardMean': 0.7830791537807672, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.683320400226
'totalSteps': 17920, 'rewardStep': 0.8808783087283844, 'errorList': [], 'lossList': [0.0, -1.2238117426633834, 0.0, 2.1810964119434355, 0.0, 0.0, 0.0], 'rewardMean': 0.7752767644563506, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.9701873721729
'totalSteps': 19200, 'rewardStep': 0.8060210192506944, 'errorList': [], 'lossList': [0.0, -1.2048038333654403, 0.0, 2.307387881875038, 0.0, 0.0, 0.0], 'rewardMean': 0.7800471244203806, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.2649111531582
'totalSteps': 20480, 'rewardStep': 0.9739159497454001, 'errorList': [0.15303120529382827, 0.045149940866386754, 0.1697600392687424, 0.06491288911449561, 0.13609250114006893, 0.02609736031121461, 0.13455276534798008, 0.06109944627937662, 0.10656722954670038, 0.05305120140668826, 0.09853696035420514, 0.11788960277890663, 0.04162680803715813, 0.05024950262636438, 0.05486916472720849, 0.044285792742799326, 0.04349741352050895, 0.06476378143364915, 0.03990668856306192, 0.10913148431111025, 0.038619882625179916, 0.051904262123262136, 0.05561841891709682, 0.1762463547868897, 0.027826536652919297, 0.049691507108465816, 0.04943949691141207, 0.05457362860403508, 0.04483810508770259, 0.08197220581500876, 0.04894131977439442, 0.046036097518372246, 0.08507581542424637, 0.06142248910188084, 0.0763234635301145, 0.04030212237148099, 0.11124632562783716, 0.10631914981211268, 0.03533737141257523, 0.04334158180258353, 0.04644120146408997, 0.13150564696973718, 0.10119133253196802, 0.05472980181097951, 0.0395776498260031, 0.04600122810656058, 0.04490035939449885, 0.03269831043120868, 0.036442610737801195, 0.0890646654760066], 'lossList': [0.0, -1.1863103193044662, 0.0, 2.1354932483285665, 0.0, 0.0, 0.0], 'rewardMean': 0.7873010043780317, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.4786969581464, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=92.73
