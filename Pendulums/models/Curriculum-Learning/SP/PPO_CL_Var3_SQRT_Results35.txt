#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 35
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.7919645885139259, 'errorList': [], 'lossList': [0.0, -1.4161930924654007, 0.0, 27.846324572563173, 0.0, 0.0, 0.0], 'rewardMean': 0.7958817745064445, 'totalEpisodes': 17, 'stepsPerEpisode': 20, 'rewardPerEpisode': 17.22986705044205
'totalSteps': 3840, 'rewardStep': 0.04731376319352787, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4215977688499862, 'totalEpisodes': 34, 'stepsPerEpisode': 170, 'rewardPerEpisode': 111.67561013923495
'totalSteps': 5120, 'rewardStep': 0.7180595632533485, 'errorList': [], 'lossList': [0.0, -1.4158363211154938, 0.0, 53.73611886978149, 0.0, 0.0, 0.0], 'rewardMean': 0.48089012773065865, 'totalEpisodes': 60, 'stepsPerEpisode': 65, 'rewardPerEpisode': 50.37280828403798
'totalSteps': 6400, 'rewardStep': 0.8140295486227097, 'errorList': [], 'lossList': [0.0, -1.4122198414802551, 0.0, 69.10160322189331, 0.0, 0.0, 0.0], 'rewardMean': 0.5364133645460004, 'totalEpisodes': 94, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.965295119223546
'totalSteps': 7680, 'rewardStep': 0.8485104409177222, 'errorList': [], 'lossList': [0.0, -1.3997683709859847, 0.0, 78.28288780212402, 0.0, 0.0, 0.0], 'rewardMean': 0.5809986611705321, 'totalEpisodes': 140, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.272410769359002
'totalSteps': 8960, 'rewardStep': 0.6171892059302789, 'errorList': [], 'lossList': [0.0, -1.3798762106895446, 0.0, 56.01210047721863, 0.0, 0.0, 0.0], 'rewardMean': 0.5855224792655005, 'totalEpisodes': 160, 'stepsPerEpisode': 96, 'rewardPerEpisode': 65.93232153275981
'totalSteps': 10240, 'rewardStep': 0.97171601360355, 'errorList': [3.0025236356755096, 4.939343761065584, 4.117684574579897, 4.169925124887919, 1.535423413449295, 2.8880663016318424, 1.8625605385515358, 2.9533382196377147, 1.870231812963647, 2.1086980685732613, 3.1903346158879926, 3.2262331179551764, 2.8712154069513045, 3.0379973252057733, 3.7705950089114117, 4.56085361485228, 3.88590087864904, 4.729330216791084, 2.364960462276358, 2.15767461675036, 2.6306606927327993, 3.2348181685580415, 2.1608221459377845, 1.0857847845155881, 2.532659556949012, 3.358671542694226, 4.405480393369085, 4.640976229164759, 2.498101148288021, 3.404231389968254, 2.228324266843421, 2.7922626798331787, 4.3083978260156615, 4.327223115014655, 3.6722175229524545, 4.075157175939455, 3.5216782003740574, 3.5534133181723733, 2.3338363715289643, 2.453852123102657, 3.9051693870210564, 1.6404835147406653, 2.580041133624808, 2.3204314577828167, 4.38226372166715, 2.763264262161218, 3.92727948632001, 3.4059564749150284, 3.253352663186892, 3.891306329664695], 'lossList': [0.0, -1.3627355474233627, 0.0, 39.374649777412415, 0.0, 0.0, 0.0], 'rewardMean': 0.6284328719697282, 'totalEpisodes': 171, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.179772506274052, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6899535448345281, 'errorList': [], 'lossList': [0.0, -1.3557747834920884, 0.0, 35.33581117630005, 0.0, 0.0, 0.0], 'rewardMean': 0.6345849392562082, 'totalEpisodes': 177, 'stepsPerEpisode': 68, 'rewardPerEpisode': 57.70011746267812
'totalSteps': 12800, 'rewardStep': 0.8136063707413816, 'errorList': [], 'lossList': [0.0, -1.332295570373535, 0.0, 15.762895494699478, 0.0, 0.0, 0.0], 'rewardMean': 0.6359656802804501, 'totalEpisodes': 183, 'stepsPerEpisode': 120, 'rewardPerEpisode': 101.21828752696135
'totalSteps': 14080, 'rewardStep': 0.4625958964320565, 'errorList': [], 'lossList': [0.0, -1.3196843653917312, 0.0, 9.595706858634948, 0.0, 0.0, 0.0], 'rewardMean': 0.6030288110722631, 'totalEpisodes': 187, 'stepsPerEpisode': 73, 'rewardPerEpisode': 48.84773863392218
'totalSteps': 15360, 'rewardStep': 0.6164158733871619, 'errorList': [], 'lossList': [0.0, -1.31599964261055, 0.0, 6.836795205175877, 0.0, 0.0, 0.0], 'rewardMean': 0.6599390220916266, 'totalEpisodes': 189, 'stepsPerEpisode': 275, 'rewardPerEpisode': 232.8648071557794
'totalSteps': 16640, 'rewardStep': 0.8159019103381738, 'errorList': [], 'lossList': [0.0, -1.2897891038656235, 0.0, 7.784721331596375, 0.0, 0.0, 0.0], 'rewardMean': 0.7367978368060911, 'totalEpisodes': 190, 'stepsPerEpisode': 222, 'rewardPerEpisode': 193.96345352677545
'totalSteps': 17920, 'rewardStep': 0.8450949486331176, 'errorList': [], 'lossList': [0.0, -1.2828148519992828, 0.0, 6.054498007893562, 0.0, 0.0, 0.0], 'rewardMean': 0.749501375344068, 'totalEpisodes': 191, 'stepsPerEpisode': 661, 'rewardPerEpisode': 504.8212344746763
'totalSteps': 19200, 'rewardStep': 0.8649176417966652, 'errorList': [], 'lossList': [0.0, -1.2768573755025863, 0.0, 2.1529264218360185, 0.0, 0.0, 0.0], 'rewardMean': 0.7545901846614635, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1029.6497432704557
'totalSteps': 20480, 'rewardStep': 0.8459186643770272, 'errorList': [], 'lossList': [0.0, -1.274188301563263, 0.0, 1.6305148269236087, 0.0, 0.0, 0.0], 'rewardMean': 0.7543310070073941, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.4446798479376
'totalSteps': 21760, 'rewardStep': 0.828739335056535, 'errorList': [], 'lossList': [0.0, -1.272522016763687, 0.0, 1.134342751428485, 0.0, 0.0, 0.0], 'rewardMean': 0.7754860199200198, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.7584295716708
'totalSteps': 23040, 'rewardStep': 0.8787759294089755, 'errorList': [], 'lossList': [0.0, -1.2404377007484435, 0.0, 1.0496246248856187, 0.0, 0.0, 0.0], 'rewardMean': 0.7661920115005623, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.2759507707042
'totalSteps': 24320, 'rewardStep': 0.730532527587812, 'errorList': [], 'lossList': [0.0, -1.1971107286214828, 0.0, 0.7963573109731078, 0.0, 0.0, 0.0], 'rewardMean': 0.7702499097758906, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.9084669510696
'totalSteps': 25600, 'rewardStep': 0.9558251216403069, 'errorList': [0.06256776070189897, 0.028922929768182743, 0.043125610262746854, 0.03693747295094628, 0.02954432093676853, 0.03804058690071932, 0.037666987133565456, 0.033021471812355534, 0.053597412287256056, 0.07024610339723432, 0.049059612842577795, 0.024467663428394928, 0.04570130136498019, 0.05811874220443283, 0.03622916221652171, 0.023338188478983474, 0.022140049609068246, 0.03832431093613384, 0.04413523377193578, 0.05698645308558205, 0.06475987922266299, 0.07298763689947443, 0.028888763564141464, 0.024880385426760996, 0.03270521505110544, 0.05930718506981232, 0.03737215732621576, 0.025082656029730647, 0.05644217904418776, 0.026843515380808897, 0.02575412877762587, 0.07946336389997687, 0.03922701194250257, 0.06203783852693609, 0.07638213792166833, 0.0827039123804966, 0.03857977149261478, 0.02761498717390007, 0.056532187253578815, 0.07491159760343997, 0.06660801773873222, 0.06247611218257065, 0.032699567879501706, 0.0322148341024426, 0.08755275003965002, 0.031445822995514974, 0.06406129653592006, 0.07647893645503773, 0.03155910048449179, 0.04166078715090573], 'lossList': [0.0, -1.160041253566742, 0.0, 0.7229069375060498, 0.0, 0.0, 0.0], 'rewardMean': 0.7844717848657832, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.725572534701, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.03
