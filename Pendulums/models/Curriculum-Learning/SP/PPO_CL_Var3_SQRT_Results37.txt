#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 37
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.5004740168413855, 'errorList': [], 'lossList': [0.0, -1.4405294638872146, 0.0, 29.877795419692994, 0.0, 0.0, 0.0], 'rewardMean': 0.46925230452362976, 'totalEpisodes': 26, 'stepsPerEpisode': 134, 'rewardPerEpisode': 78.32211522709913
'totalSteps': 3840, 'rewardStep': 0.8340932212967878, 'errorList': [], 'lossList': [0.0, -1.430467670559883, 0.0, 47.86144002914429, 0.0, 0.0, 0.0], 'rewardMean': 0.5908659434480158, 'totalEpisodes': 50, 'stepsPerEpisode': 10, 'rewardPerEpisode': 9.313472089187952
'totalSteps': 5120, 'rewardStep': 0.734629892813615, 'errorList': [], 'lossList': [0.0, -1.4019940358400345, 0.0, 54.06874374389648, 0.0, 0.0, 0.0], 'rewardMean': 0.6268069307894155, 'totalEpisodes': 74, 'stepsPerEpisode': 31, 'rewardPerEpisode': 21.31728898043858
'totalSteps': 6400, 'rewardStep': 0.8522748948548173, 'errorList': [], 'lossList': [0.0, -1.3789755308628082, 0.0, 84.30012397766113, 0.0, 0.0, 0.0], 'rewardMean': 0.6719005236024959, 'totalEpisodes': 106, 'stepsPerEpisode': 35, 'rewardPerEpisode': 27.73955315923576
'totalSteps': 7680, 'rewardStep': 0.7639547457875748, 'errorList': [], 'lossList': [0.0, -1.3634173572063446, 0.0, 65.06417366027831, 0.0, 0.0, 0.0], 'rewardMean': 0.6872428939666757, 'totalEpisodes': 138, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.975372494901215
'totalSteps': 8960, 'rewardStep': 0.4858025631361411, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6368828112590421, 'totalEpisodes': 154, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.138354841588157
'totalSteps': 10240, 'rewardStep': 0.6138538900669295, 'errorList': [], 'lossList': [0.0, -1.36370041847229, 0.0, 48.131715812683105, 0.0, 0.0, 0.0], 'rewardMean': 0.6343240422376962, 'totalEpisodes': 176, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.309741074044184
'totalSteps': 11520, 'rewardStep': 0.8188621340938931, 'errorList': [], 'lossList': [0.0, -1.3657215774059295, 0.0, 28.64067950248718, 0.0, 0.0, 0.0], 'rewardMean': 0.6527778514233159, 'totalEpisodes': 189, 'stepsPerEpisode': 70, 'rewardPerEpisode': 57.097946917753625
'totalSteps': 12800, 'rewardStep': 0.8875445489139056, 'errorList': [], 'lossList': [0.0, -1.358052328824997, 0.0, 23.77492825269699, 0.0, 0.0, 0.0], 'rewardMean': 0.6977292470941191, 'totalEpisodes': 196, 'stepsPerEpisode': 141, 'rewardPerEpisode': 119.36913385545384
'totalSteps': 14080, 'rewardStep': 0.23988884373760166, 'errorList': [], 'lossList': [0.0, -1.3442952400445938, 0.0, 8.279452126026154, 0.0, 0.0, 0.0], 'rewardMean': 0.6716707297837408, 'totalEpisodes': 201, 'stepsPerEpisode': 220, 'rewardPerEpisode': 160.31310173967742
'totalSteps': 15360, 'rewardStep': 0.8262237141359552, 'errorList': [], 'lossList': [0.0, -1.3326316201686859, 0.0, 19.55439777135849, 0.0, 0.0, 0.0], 'rewardMean': 0.6708837790676575, 'totalEpisodes': 207, 'stepsPerEpisode': 41, 'rewardPerEpisode': 35.31369338415175
'totalSteps': 16640, 'rewardStep': 0.6692850754305887, 'errorList': [], 'lossList': [0.0, -1.3295192062854766, 0.0, 5.700878189206123, 0.0, 0.0, 0.0], 'rewardMean': 0.6643492973293548, 'totalEpisodes': 211, 'stepsPerEpisode': 236, 'rewardPerEpisode': 182.61755600980575
'totalSteps': 17920, 'rewardStep': 0.7700960302123737, 'errorList': [], 'lossList': [0.0, -1.3223278641700744, 0.0, 5.444269042611122, 0.0, 0.0, 0.0], 'rewardMean': 0.6561314108651104, 'totalEpisodes': 215, 'stepsPerEpisode': 91, 'rewardPerEpisode': 79.2543626804602
'totalSteps': 19200, 'rewardStep': 0.7616070936416351, 'errorList': [], 'lossList': [0.0, -1.294532637000084, 0.0, 5.194932309389114, 0.0, 0.0, 0.0], 'rewardMean': 0.6558966456505165, 'totalEpisodes': 217, 'stepsPerEpisode': 755, 'rewardPerEpisode': 632.6529099050258
'totalSteps': 20480, 'rewardStep': 0.5662172721233287, 'errorList': [], 'lossList': [0.0, -1.269366855621338, 0.0, 2.5951386687159537, 0.0, 0.0, 0.0], 'rewardMean': 0.6639381165492353, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 909.6571695753344
'totalSteps': 21760, 'rewardStep': 0.8374652237917763, 'errorList': [], 'lossList': [0.0, -1.2469999748468399, 0.0, 2.3684899888932707, 0.0, 0.0, 0.0], 'rewardMean': 0.6991043826147987, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.316772809488
'totalSteps': 23040, 'rewardStep': 0.9538912145251163, 'errorList': [0.07037215628670389, 0.02930628190694411, 0.057472091448780974, 0.03508389369157857, 0.06703665991505037, 0.02763266708360172, 0.04730240612898028, 0.08703234019411914, 0.04299251141244724, 0.0348604597462228, 0.060817286234594925, 0.11499654447169581, 0.03922580625583989, 0.0982324405983511, 0.08344138174830729, 0.06442339139484657, 0.06320757581821346, 0.026968507630963284, 0.06161384639625969, 0.038444096380720064, 0.03062383488573406, 0.05706933562842683, 0.05159828922530943, 0.025196978708037176, 0.04057864130265232, 0.03938001929792088, 0.021128317150272573, 0.05227936392164975, 0.04070852580143393, 0.04965936223432684, 0.027055349360922527, 0.025885774559094233, 0.0449670068666783, 0.046296078797809184, 0.0521730747003145, 0.0439826816512978, 0.061281514781043656, 0.06974844020201706, 0.12122460844103825, 0.017097062730958824, 0.048137713030994275, 0.08480823884299589, 0.06487870145085332, 0.0731993275690998, 0.03185351862521804, 0.12973568140194455, 0.09230255811768717, 0.032879819877706065, 0.04310412350494375, 0.07278084133638911], 'lossList': [0.0, -1.2097804456949235, 0.0, 1.6897200046479701, 0.0, 0.0, 0.0], 'rewardMean': 0.7331081150606173, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.4307116914117, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=75.4
