#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 38
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6435090772153219, 'errorList': [], 'lossList': [0.0, -1.4191629683971405, 0.0, 31.696920986175538, 0.0, 0.0, 0.0], 'rewardMean': 0.704417455081189, 'totalEpisodes': 29, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.458371992040252
'totalSteps': 3840, 'rewardStep': 0.9758477398008063, 'errorList': [], 'lossList': [0.0, -1.4096559172868728, 0.0, 42.461842670440674, 0.0, 0.0, 0.0], 'rewardMean': 0.7948942166543947, 'totalEpisodes': 47, 'stepsPerEpisode': 50, 'rewardPerEpisode': 37.540600278303415
'totalSteps': 5120, 'rewardStep': 0.7431387742500006, 'errorList': [], 'lossList': [0.0, -1.4025978708267213, 0.0, 64.9668853378296, 0.0, 0.0, 0.0], 'rewardMean': 0.7819553560532962, 'totalEpisodes': 65, 'stepsPerEpisode': 74, 'rewardPerEpisode': 62.49162146920528
'totalSteps': 6400, 'rewardStep': 0.8535605393966952, 'errorList': [], 'lossList': [0.0, -1.3915988606214524, 0.0, 87.93514112472535, 0.0, 0.0, 0.0], 'rewardMean': 0.796276392721976, 'totalEpisodes': 85, 'stepsPerEpisode': 50, 'rewardPerEpisode': 36.96555582375413
'totalSteps': 7680, 'rewardStep': 0.6613326288248327, 'errorList': [], 'lossList': [0.0, -1.3917026209831238, 0.0, 79.72251792907714, 0.0, 0.0, 0.0], 'rewardMean': 0.7737857654057855, 'totalEpisodes': 109, 'stepsPerEpisode': 85, 'rewardPerEpisode': 61.80939816849647
'totalSteps': 8960, 'rewardStep': 0.9178924056702936, 'errorList': [], 'lossList': [0.0, -1.385414623618126, 0.0, 41.194462547302244, 0.0, 0.0, 0.0], 'rewardMean': 0.7943724283007152, 'totalEpisodes': 122, 'stepsPerEpisode': 70, 'rewardPerEpisode': 60.54470306367876
'totalSteps': 10240, 'rewardStep': 0.6894530831023139, 'errorList': [], 'lossList': [0.0, -1.3739613050222397, 0.0, 34.22204493999481, 0.0, 0.0, 0.0], 'rewardMean': 0.781257510150915, 'totalEpisodes': 129, 'stepsPerEpisode': 137, 'rewardPerEpisode': 107.38959597280714
'totalSteps': 11520, 'rewardStep': 0.634015244598759, 'errorList': [], 'lossList': [0.0, -1.3702808374166489, 0.0, 23.862542862892152, 0.0, 0.0, 0.0], 'rewardMean': 0.7648972584228977, 'totalEpisodes': 132, 'stepsPerEpisode': 366, 'rewardPerEpisode': 272.17510168375276
'totalSteps': 12800, 'rewardStep': 0.3057409323314127, 'errorList': [], 'lossList': [0.0, -1.3658701729774476, 0.0, 11.700061738491058, 0.0, 0.0, 0.0], 'rewardMean': 0.7189816258137492, 'totalEpisodes': 136, 'stepsPerEpisode': 167, 'rewardPerEpisode': 119.41141795907404
'totalSteps': 14080, 'rewardStep': 0.48957762602041827, 'errorList': [], 'lossList': [0.0, -1.336290994286537, 0.0, 8.728619927167893, 0.0, 0.0, 0.0], 'rewardMean': 0.6914068051210855, 'totalEpisodes': 140, 'stepsPerEpisode': 385, 'rewardPerEpisode': 259.96877396711244
'totalSteps': 15360, 'rewardStep': 0.6638356626107954, 'errorList': [], 'lossList': [0.0, -1.3066629296541215, 0.0, 5.8009440213441845, 0.0, 0.0, 0.0], 'rewardMean': 0.6934394636606328, 'totalEpisodes': 143, 'stepsPerEpisode': 205, 'rewardPerEpisode': 171.68679392681767
'totalSteps': 16640, 'rewardStep': 0.828248488682648, 'errorList': [], 'lossList': [0.0, -1.3035599029064178, 0.0, 4.55341345205903, 0.0, 0.0, 0.0], 'rewardMean': 0.6786795385488169, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.7143829051934
'totalSteps': 17920, 'rewardStep': 0.8125797577161493, 'errorList': [], 'lossList': [0.0, -1.2847982466220855, 0.0, 3.098946310132742, 0.0, 0.0, 0.0], 'rewardMean': 0.6856236368954318, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.1719133991785
'totalSteps': 19200, 'rewardStep': 0.8605837244924802, 'errorList': [], 'lossList': [0.0, -1.258287325501442, 0.0, 1.7507181346416474, 0.0, 0.0, 0.0], 'rewardMean': 0.6863259554050103, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.1824520603968
'totalSteps': 20480, 'rewardStep': 0.9770770222470677, 'errorList': [0.0737677480261426, 0.053765793906123335, 0.0748853782375067, 0.07557373081829835, 0.08080952147417222, 0.06382212773158741, 0.08671066699550883, 0.06682650878428668, 0.06528274364161486, 0.0811680963812077, 0.07320495626135999, 0.07124977798183123, 0.0637752435398514, 0.07296783561118839, 0.06499907736907358, 0.05370093930313695, 0.07568612003219001, 0.08756261496020916, 0.07702052895437353, 0.09288241094705288, 0.06335004460389629, 0.07716149557789244, 0.1170611630800557, 0.07785223210003336, 0.05953466367435748, 0.05778289390709255, 0.07628803680219653, 0.07973380196933287, 0.08507098954094933, 0.08281053687172017, 0.058869176916862644, 0.08679895059315798, 0.07567176719360588, 0.07482522945951768, 0.09909947764729614, 0.07682947172173098, 0.0859351125872652, 0.08525380474300762, 0.056441404588614616, 0.06131008062477038, 0.062400307149750936, 0.10135457464343112, 0.07101845480394543, 0.07024678496582804, 0.08001476408252996, 0.0806664074636208, 0.05329297444800072, 0.06482509555224791, 0.08751718800974323, 0.08027132083574721], 'lossList': [0.0, -1.2195884472131728, 0.0, 1.9560117298364639, 0.0, 0.0, 0.0], 'rewardMean': 0.7179003947472339, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.2449185590979, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=74.81
