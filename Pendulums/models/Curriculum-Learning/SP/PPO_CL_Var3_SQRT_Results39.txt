#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 39
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8942386313730034, 'errorList': [], 'lossList': [0.0, -1.4160378152132034, 0.0, 26.42848601818085, 0.0, 0.0, 0.0], 'rewardMean': 0.7830770187581734, 'totalEpisodes': 25, 'stepsPerEpisode': 21, 'rewardPerEpisode': 19.42127887743973
'totalSteps': 3840, 'rewardStep': 0.7795675276290843, 'errorList': [], 'lossList': [0.0, -1.4121493953466415, 0.0, 46.02457088470459, 0.0, 0.0, 0.0], 'rewardMean': 0.7819071883818104, 'totalEpisodes': 51, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.13023821856224
'totalSteps': 5120, 'rewardStep': 0.8602377270920377, 'errorList': [], 'lossList': [0.0, -1.407388561964035, 0.0, 57.8091242980957, 0.0, 0.0, 0.0], 'rewardMean': 0.8014898230593672, 'totalEpisodes': 70, 'stepsPerEpisode': 40, 'rewardPerEpisode': 30.06349113139417
'totalSteps': 6400, 'rewardStep': 0.8505092492382679, 'errorList': [], 'lossList': [0.0, -1.4037441271543503, 0.0, 76.67359706878662, 0.0, 0.0, 0.0], 'rewardMean': 0.8112937082951474, 'totalEpisodes': 94, 'stepsPerEpisode': 38, 'rewardPerEpisode': 33.29983279466488
'totalSteps': 7680, 'rewardStep': 0.6707365457499098, 'errorList': [], 'lossList': [0.0, -1.3986128360033034, 0.0, 85.74338516235352, 0.0, 0.0, 0.0], 'rewardMean': 0.7878675145376078, 'totalEpisodes': 126, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.65656161119019
'totalSteps': 8960, 'rewardStep': 0.6624913714857676, 'errorList': [], 'lossList': [0.0, -1.3960560375452042, 0.0, 49.767982702255246, 0.0, 0.0, 0.0], 'rewardMean': 0.7699566369587735, 'totalEpisodes': 140, 'stepsPerEpisode': 128, 'rewardPerEpisode': 98.24105878975975
'totalSteps': 10240, 'rewardStep': 0.9365203769793237, 'errorList': [92.63459127725828, 16.082876726112797, 55.41105599317688, 39.789892513790065, 21.138975597875273, 66.74685683261421, 77.37432754260438, 86.51451007864321, 77.4405283810979, 68.26927674204656, 67.40296040482362, 135.9086654846973, 23.252655854231758, 6.435519859886024, 122.15727328139258, 48.51481525436699, 40.2475988652319, 9.553862325689423, 43.803896961901735, 61.6941915141045, 6.691710856894209, 0.28820400698091675, 14.596095294943687, 31.917508128264608, 90.04358735838052, 93.30081505220369, 2.135715711146663, 99.90428552036715, 103.65089696778588, 110.11378553972017, 9.64220853117352, 68.48619974541919, 10.903834004638485, 41.40010857320952, 4.655348922575041, 96.1328473510249, 2.5603023861322214, 11.909134262591516, 69.80042132866771, 108.81129248062271, 119.00734009310482, 26.121545712643396, 109.21778471571679, 48.943265508067206, 46.003341775592915, 6.008451965166842, 71.46219018077518, 59.198657620281914, 43.317441446551584, 52.417576318400506], 'lossList': [0.0, -1.3806907665729522, 0.0, 19.58487730741501, 0.0, 0.0, 0.0], 'rewardMean': 0.7907771044613422, 'totalEpisodes': 147, 'stepsPerEpisode': 36, 'rewardPerEpisode': 31.148019598395386, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.3198805561238819, 'errorList': [], 'lossList': [0.0, -1.3521695679426193, 0.0, 15.136148834228516, 0.0, 0.0, 0.0], 'rewardMean': 0.7384552657571799, 'totalEpisodes': 152, 'stepsPerEpisode': 137, 'rewardPerEpisode': 80.91311859493851
'totalSteps': 12800, 'rewardStep': 0.7831893416351557, 'errorList': [], 'lossList': [0.0, -1.340110614299774, 0.0, 24.52042912721634, 0.0, 0.0, 0.0], 'rewardMean': 0.7429286733449775, 'totalEpisodes': 159, 'stepsPerEpisode': 64, 'rewardPerEpisode': 52.295211842373924
'totalSteps': 14080, 'rewardStep': 0.9661507007902043, 'errorList': [3.0483130606348166, 10.567620834795894, 4.076758502647246, 2.564098557827789, 8.274567108106408, 9.290648554337652, 3.1157880909305344, 7.659852858848422, 4.789171095807372, 7.039588398607149, 7.400608906211074, 11.849557523641382, 8.810650114238465, 9.610197673772403, 2.382271792246986, 3.6746635223407362, 1.0577806114416648, 3.289714892069357, 3.9389765763381077, 9.353596960302822, 2.199678359445735, 11.63436372021763, 5.68489152368014, 6.076560767273088, 6.5814629897117385, 4.139072157108997, 5.830284868962646, 4.399483010079961, 3.9127595098882963, 3.6987566952000366, 4.215840295193732, 11.34889228354527, 10.112724064317923, 3.605254927327416, 3.091557051866603, 6.266207424265954, 2.0686674983492757, 4.6431584031099105, 5.124197851146462, 2.3850563484838103, 5.947182860082668, 5.539749624046226, 5.201770835080456, 1.8139750351213533, 6.027707561224646, 9.301348932163755, 10.94893562200414, 3.8298102377570156, 7.794930264513963, 6.93532420618501], 'lossList': [0.0, -1.3371480792760848, 0.0, 10.11732315659523, 0.0, 0.0, 0.0], 'rewardMean': 0.7723522028096637, 'totalEpisodes': 163, 'stepsPerEpisode': 136, 'rewardPerEpisode': 120.80661687038321, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.9388108571191387, 'errorList': [4.876506324834108, 7.258613699461518, 8.464044255993574, 7.774248312114554, 2.3566907751114585, 0.9303460736513604, 3.8122911361739904, 1.759197275064852, 8.63082553905388, 2.982359035430031, 3.5811072784184077, 4.660189755961098, 5.4852058003693225, 1.7669643194385007, 22.78745307679886, 1.9387753000819923, 0.40815554409136234, 3.6826066297998676, 1.574732563042344, 11.897267048579081, 5.315234701813958, 7.795590347507397, 1.449095464221476, 1.1932527852936445, 1.1139632581577212, 3.507772489754327, 9.582050990770716, 2.474946237214734, 7.776829424863043, 9.678927207818514, 5.431594260949859, 18.835550216003575, 9.46002627150336, 5.8409338788174265, 1.8859726020982257, 1.6201137833708021, 2.3556661988589744, 18.642879938459195, 0.05811721474908599, 2.326076232846681, 6.341070118917864, 0.44744560367284036, 4.889709646686551, 32.693263616107465, 17.82660325218599, 19.333503903615817, 7.967192502013775, 1.1225526144399, 1.6359372156572194, 1.6390650912842135], 'lossList': [0.0, -1.3490284311771392, 0.0, 25.889586198329926, 0.0, 0.0, 0.0], 'rewardMean': 0.7768094253842771, 'totalEpisodes': 167, 'stepsPerEpisode': 72, 'rewardPerEpisode': 64.59077653238849, 'successfulTests': 1
'totalSteps': 16640, 'rewardStep': 0.5019744092607948, 'errorList': [], 'lossList': [0.0, -1.3492301440238952, 0.0, 5.9144679081439975, 0.0, 0.0, 0.0], 'rewardMean': 0.7490501135474482, 'totalEpisodes': 170, 'stepsPerEpisode': 218, 'rewardPerEpisode': 173.16499478844597
'totalSteps': 17920, 'rewardStep': 0.7836154157924458, 'errorList': [], 'lossList': [0.0, -1.3546900343894959, 0.0, 5.868687036633491, 0.0, 0.0, 0.0], 'rewardMean': 0.741387882417489, 'totalEpisodes': 173, 'stepsPerEpisode': 347, 'rewardPerEpisode': 312.8513391284011
'totalSteps': 19200, 'rewardStep': 0.9261640041766295, 'errorList': [], 'lossList': [0.0, -1.3416658639907837, 0.0, 2.540650819838047, 0.0, 0.0, 0.0], 'rewardMean': 0.7489533579113252, 'totalEpisodes': 176, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.833361126601282
'totalSteps': 20480, 'rewardStep': 0.4938702681385672, 'errorList': [], 'lossList': [0.0, -1.348390550017357, 0.0, 3.5766773465275765, 0.0, 0.0, 0.0], 'rewardMean': 0.731266730150191, 'totalEpisodes': 177, 'stepsPerEpisode': 664, 'rewardPerEpisode': 471.8965473016689
'totalSteps': 21760, 'rewardStep': 0.7589054965344237, 'errorList': [], 'lossList': [0.0, -1.349166687130928, 0.0, 3.2094602185487746, 0.0, 0.0, 0.0], 'rewardMean': 0.7409081426550566, 'totalEpisodes': 180, 'stepsPerEpisode': 134, 'rewardPerEpisode': 115.14381827548094
'totalSteps': 23040, 'rewardStep': 0.8851868036464184, 'errorList': [], 'lossList': [0.0, -1.3528956413269042, 0.0, 2.4558140084147455, 0.0, 0.0, 0.0], 'rewardMean': 0.735774785321766, 'totalEpisodes': 181, 'stepsPerEpisode': 987, 'rewardPerEpisode': 813.5156408534805
'totalSteps': 24320, 'rewardStep': 0.8305235992345437, 'errorList': [], 'lossList': [0.0, -1.328082424402237, 0.0, 0.6807918432727456, 0.0, 0.0, 0.0], 'rewardMean': 0.7868390896328321, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.4519968814427
'totalSteps': 25600, 'rewardStep': 0.8826934999603091, 'errorList': [], 'lossList': [0.0, -1.3066463315486907, 0.0, 0.526963687762618, 0.0, 0.0, 0.0], 'rewardMean': 0.7967895054653475, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.5917007083556
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=15360, timeSpent=128.61
