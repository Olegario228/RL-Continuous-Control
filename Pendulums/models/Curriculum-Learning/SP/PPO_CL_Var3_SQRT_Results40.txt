#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 40
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.8480350681675812, 'errorList': [], 'lossList': [0.0, -1.4343368643522263, 0.0, 25.35998867034912, 0.0, 0.0, 0.0], 'rewardMean': 0.8718083296878741, 'totalEpisodes': 13, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.721792333973468
'totalSteps': 3840, 'rewardStep': 0.42611173109657885, 'errorList': [], 'lossList': [0.0, -1.417831268310547, 0.0, 45.5817298412323, 0.0, 0.0, 0.0], 'rewardMean': 0.723242796824109, 'totalEpisodes': 29, 'stepsPerEpisode': 153, 'rewardPerEpisode': 115.26951957717647
'totalSteps': 5120, 'rewardStep': 0.6754144852379808, 'errorList': [], 'lossList': [0.0, -1.3985656958818435, 0.0, 60.68596884727478, 0.0, 0.0, 0.0], 'rewardMean': 0.711285718927577, 'totalEpisodes': 45, 'stepsPerEpisode': 103, 'rewardPerEpisode': 76.98380322391606
'totalSteps': 6400, 'rewardStep': 0.8606092493516834, 'errorList': [], 'lossList': [0.0, -1.385289461016655, 0.0, 48.89529272079468, 0.0, 0.0, 0.0], 'rewardMean': 0.7411504250123983, 'totalEpisodes': 54, 'stepsPerEpisode': 130, 'rewardPerEpisode': 105.91134283210283
'totalSteps': 7680, 'rewardStep': 0.9120284564260953, 'errorList': [], 'lossList': [0.0, -1.3828688633441926, 0.0, 137.76344905853273, 0.0, 0.0, 0.0], 'rewardMean': 0.7696300969146811, 'totalEpisodes': 88, 'stepsPerEpisode': 23, 'rewardPerEpisode': 18.983532825047273
'totalSteps': 8960, 'rewardStep': 0.47514949454204375, 'errorList': [], 'lossList': [0.0, -1.3798960679769516, 0.0, 99.07977020263672, 0.0, 0.0, 0.0], 'rewardMean': 0.7275614394328758, 'totalEpisodes': 116, 'stepsPerEpisode': 23, 'rewardPerEpisode': 14.197087481871007
'totalSteps': 10240, 'rewardStep': 0.6496591073178868, 'errorList': [], 'lossList': [0.0, -1.3762963426113128, 0.0, 66.18159173965454, 0.0, 0.0, 0.0], 'rewardMean': 0.7178236479185022, 'totalEpisodes': 128, 'stepsPerEpisode': 109, 'rewardPerEpisode': 81.73271748115897
'totalSteps': 11520, 'rewardStep': 0.6526840890556862, 'errorList': [], 'lossList': [0.0, -1.3707400023937226, 0.0, 55.249778490066525, 0.0, 0.0, 0.0], 'rewardMean': 0.710585919155967, 'totalEpisodes': 135, 'stepsPerEpisode': 67, 'rewardPerEpisode': 58.63250733085695
'totalSteps': 12800, 'rewardStep': 0.7406754907741858, 'errorList': [], 'lossList': [0.0, -1.3642945718765258, 0.0, 53.25380130290985, 0.0, 0.0, 0.0], 'rewardMean': 0.713594876317789, 'totalEpisodes': 140, 'stepsPerEpisode': 134, 'rewardPerEpisode': 109.14096022079003
'totalSteps': 14080, 'rewardStep': 0.39833704796474995, 'errorList': [], 'lossList': [0.0, -1.3668235743045807, 0.0, 40.93766428947449, 0.0, 0.0, 0.0], 'rewardMean': 0.6638704219934473, 'totalEpisodes': 144, 'stepsPerEpisode': 213, 'rewardPerEpisode': 157.2109683257126
'totalSteps': 15360, 'rewardStep': 0.6358479205762964, 'errorList': [], 'lossList': [0.0, -1.3599853736162186, 0.0, 41.5964733839035, 0.0, 0.0, 0.0], 'rewardMean': 0.6426517072343187, 'totalEpisodes': 149, 'stepsPerEpisode': 68, 'rewardPerEpisode': 55.073836342698414
'totalSteps': 16640, 'rewardStep': 0.8716698988203193, 'errorList': [], 'lossList': [0.0, -1.3533004856109618, 0.0, 19.363728997707366, 0.0, 0.0, 0.0], 'rewardMean': 0.6872075240066928, 'totalEpisodes': 154, 'stepsPerEpisode': 97, 'rewardPerEpisode': 87.00413830137131
'totalSteps': 17920, 'rewardStep': 0.7441227566873475, 'errorList': [], 'lossList': [0.0, -1.3319307589530944, 0.0, 5.922520245909691, 0.0, 0.0, 0.0], 'rewardMean': 0.6940783511516294, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 971.6179378024933
'totalSteps': 19200, 'rewardStep': 0.5171518092993528, 'errorList': [], 'lossList': [0.0, -1.3089253413677215, 0.0, 17.385665156841277, 0.0, 0.0, 0.0], 'rewardMean': 0.6597326071463965, 'totalEpisodes': 156, 'stepsPerEpisode': 197, 'rewardPerEpisode': 131.39019506403315
'totalSteps': 20480, 'rewardStep': 0.8086629443432762, 'errorList': [], 'lossList': [0.0, -1.3165691697597504, 0.0, 5.848978947401047, 0.0, 0.0, 0.0], 'rewardMean': 0.6493960559381144, 'totalEpisodes': 157, 'stepsPerEpisode': 1189, 'rewardPerEpisode': 860.9619789639777
'totalSteps': 21760, 'rewardStep': 0.9561851237771309, 'errorList': [0.2480891644503812, 0.11926803994540006, 0.3915137232452546, 0.37882943087578774, 0.37993164579637356, 0.2084657872667213, 0.29813220859763695, 0.08608166807310344, 0.1391373852574877, 0.27246674468369664, 0.45057450943514593, 0.3177658928327368, 0.18200253422222906, 0.5783758287356313, 0.8046904273230698, 0.16897627655446312, 0.0857747152309459, 0.45010552759165556, 0.5300358009845898, 0.6243206403157742, 0.30991055658191663, 0.05996495192772117, 0.43326259243227966, 0.5315158811657165, 0.4507018753924086, 0.32292816406191677, 0.37830499546144675, 0.26450643824079023, 0.44655855386977805, 0.42157560332582916, 0.21764268780697252, 0.17764187085696515, 0.2926622127612307, 0.15967304476528396, 0.1375217166400708, 0.3216536579875685, 0.345323879839253, 0.19406130977805228, 0.23870488128815148, 0.406542248451211, 0.16524416045930157, 0.25857159478779174, 0.2146102843078606, 0.14342787597740667, 0.07522194424232824, 0.1684876206921718, 0.13020336748747077, 0.4545438463794855, 0.5936458018090005, 0.5844265445105026], 'lossList': [0.0, -1.3160548293590546, 0.0, 5.643984927535057, 0.0, 0.0, 0.0], 'rewardMean': 0.6974996188616233, 'totalEpisodes': 159, 'stepsPerEpisode': 66, 'rewardPerEpisode': 62.004081991847244, 'successfulTests': 16
'totalSteps': 23040, 'rewardStep': 0.7259871891512757, 'errorList': [], 'lossList': [0.0, -1.3112819331884384, 0.0, 4.448873929977417, 0.0, 0.0, 0.0], 'rewardMean': 0.7051324270449622, 'totalEpisodes': 160, 'stepsPerEpisode': 324, 'rewardPerEpisode': 260.03532771860375
'totalSteps': 24320, 'rewardStep': 0.9234859568971061, 'errorList': [], 'lossList': [0.0, -1.3004353892803193, 0.0, 5.574514159560204, 0.0, 0.0, 0.0], 'rewardMean': 0.7322126138291041, 'totalEpisodes': 162, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.961271195691804
'totalSteps': 25600, 'rewardStep': 0.7327378609501535, 'errorList': [], 'lossList': [0.0, -1.300530264377594, 0.0, 1.6048021972179414, 0.0, 0.0, 0.0], 'rewardMean': 0.7314188508467008, 'totalEpisodes': 162, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 862.3064041627109
#maxSuccessfulTests=16, maxSuccessfulTestsAtStep=21760, timeSpent=87.32
