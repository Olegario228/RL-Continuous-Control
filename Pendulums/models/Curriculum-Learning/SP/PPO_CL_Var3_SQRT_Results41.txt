#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 41
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.6766382507746302, 'errorList': [], 'lossList': [0.0, -1.4307983314990997, 0.0, 29.455632977485656, 0.0, 0.0, 0.0], 'rewardMean': 0.6176429846254927, 'totalEpisodes': 14, 'stepsPerEpisode': 106, 'rewardPerEpisode': 83.2209602937816
'totalSteps': 3840, 'rewardStep': 0.8503717031918485, 'errorList': [], 'lossList': [0.0, -1.4250933235883714, 0.0, 43.01594857215881, 0.0, 0.0, 0.0], 'rewardMean': 0.6952192241476114, 'totalEpisodes': 26, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.764920601430957
'totalSteps': 5120, 'rewardStep': 0.5282911355119263, 'errorList': [], 'lossList': [0.0, -1.4179530799388886, 0.0, 40.043582887649535, 0.0, 0.0, 0.0], 'rewardMean': 0.6534872019886901, 'totalEpisodes': 38, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.344715425485604
'totalSteps': 6400, 'rewardStep': 0.9851668148941012, 'errorList': [379.3525029780635, 368.79782730001085, 359.30202218498715, 356.342865573744, 241.84058298381535, 329.03041695624, 293.1293148577382, 310.6135362922725, 308.8540218140684, 315.3470789202649, 365.23118247836254, 263.73444394423234, 222.3815799059969, 350.8829637579611, 318.58138217218885, 318.0329823450583, 352.18439265362235, 349.4570605645132, 348.81252578371266, 205.2662612209678, 352.9484626529646, 321.0749208981611, 283.63603817082094, 359.2675653912593, 318.27609363980406, 381.24839180485014, 315.21400819800493, 249.1930883013741, 334.5967119916453, 338.11821627060385, 361.47921371832587, 364.31449800324316, 256.1475630447211, 353.13284088799423, 335.2898396103649, 339.1155734254501, 303.83719385624676, 355.00411407719514, 336.6499878519201, 310.9154271710104, 367.30178300772207, 370.18000076270994, 318.27483749671063, 249.43478285943826, 367.82591226033344, 312.5737948672157, 270.1452280247787, 351.3451017946519, 356.60395635323584, 371.2457146351496], 'lossList': [0.0, -1.4016700083017348, 0.0, 106.75967071533204, 0.0, 0.0, 0.0], 'rewardMean': 0.7198231245697724, 'totalEpisodes': 58, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.8519488167464444, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.5948296420011887, 'errorList': [], 'lossList': [0.0, -1.3897756034135818, 0.0, 98.69076974868774, 0.0, 0.0, 0.0], 'rewardMean': 0.6989908774750084, 'totalEpisodes': 91, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.423768941984884
'totalSteps': 8960, 'rewardStep': 0.3950032567576071, 'errorList': [], 'lossList': [0.0, -1.3802688282728195, 0.0, 59.24334777832031, 0.0, 0.0, 0.0], 'rewardMean': 0.6555640745153797, 'totalEpisodes': 113, 'stepsPerEpisode': 107, 'rewardPerEpisode': 62.1084360331181
'totalSteps': 10240, 'rewardStep': 0.7036251268225128, 'errorList': [], 'lossList': [0.0, -1.3688534891605377, 0.0, 19.67788033723831, 0.0, 0.0, 0.0], 'rewardMean': 0.6615717060537712, 'totalEpisodes': 121, 'stepsPerEpisode': 53, 'rewardPerEpisode': 40.71238624647003
'totalSteps': 11520, 'rewardStep': 0.4042954400215127, 'errorList': [], 'lossList': [0.0, -1.367276137471199, 0.0, 12.731642398834229, 0.0, 0.0, 0.0], 'rewardMean': 0.6329854542724092, 'totalEpisodes': 126, 'stepsPerEpisode': 268, 'rewardPerEpisode': 207.48379401847555
'totalSteps': 12800, 'rewardStep': 0.6534582288654038, 'errorList': [], 'lossList': [0.0, -1.3490655207633973, 0.0, 9.50255971312523, 0.0, 0.0, 0.0], 'rewardMean': 0.6350327317317086, 'totalEpisodes': 132, 'stepsPerEpisode': 53, 'rewardPerEpisode': 41.850852039078966
'totalSteps': 14080, 'rewardStep': 0.7175289196851438, 'errorList': [], 'lossList': [0.0, -1.3212210607528687, 0.0, 8.553259061574936, 0.0, 0.0, 0.0], 'rewardMean': 0.6509208518525875, 'totalEpisodes': 135, 'stepsPerEpisode': 310, 'rewardPerEpisode': 253.02491564332945
'totalSteps': 15360, 'rewardStep': 0.877000622496952, 'errorList': [], 'lossList': [0.0, -1.3148400807380676, 0.0, 6.156677189469337, 0.0, 0.0, 0.0], 'rewardMean': 0.6709570890248197, 'totalEpisodes': 137, 'stepsPerEpisode': 518, 'rewardPerEpisode': 455.30497986228016
'totalSteps': 16640, 'rewardStep': 0.8327678687200695, 'errorList': [], 'lossList': [0.0, -1.301637259721756, 0.0, 3.8479447704553604, 0.0, 0.0, 0.0], 'rewardMean': 0.6691967055776419, 'totalEpisodes': 139, 'stepsPerEpisode': 141, 'rewardPerEpisode': 118.18615879519034
'totalSteps': 17920, 'rewardStep': 0.6432015199422196, 'errorList': [], 'lossList': [0.0, -1.277000628709793, 0.0, 3.736411906480789, 0.0, 0.0, 0.0], 'rewardMean': 0.6806877440206711, 'totalEpisodes': 142, 'stepsPerEpisode': 233, 'rewardPerEpisode': 188.9255373070108
'totalSteps': 19200, 'rewardStep': 0.9287000601013982, 'errorList': [], 'lossList': [0.0, -1.2562371301651, 0.0, 3.3894708301126957, 0.0, 0.0, 0.0], 'rewardMean': 0.6750410685414007, 'totalEpisodes': 143, 'stepsPerEpisode': 1011, 'rewardPerEpisode': 894.3022847114273
'totalSteps': 20480, 'rewardStep': 0.7539664932984617, 'errorList': [], 'lossList': [0.0, -1.2447801560163498, 0.0, 2.3012854060530663, 0.0, 0.0, 0.0], 'rewardMean': 0.6909547536711281, 'totalEpisodes': 144, 'stepsPerEpisode': 113, 'rewardPerEpisode': 98.60307913808823
'totalSteps': 21760, 'rewardStep': 0.7412215078193604, 'errorList': [], 'lossList': [0.0, -1.2287240099906922, 0.0, 2.531345698237419, 0.0, 0.0, 0.0], 'rewardMean': 0.7255765787773034, 'totalEpisodes': 145, 'stepsPerEpisode': 244, 'rewardPerEpisode': 216.5980769136069
'totalSteps': 23040, 'rewardStep': 0.7852225638945449, 'errorList': [], 'lossList': [0.0, -1.2240336209535598, 0.0, 2.7907533293962477, 0.0, 0.0, 0.0], 'rewardMean': 0.7337363224845067, 'totalEpisodes': 146, 'stepsPerEpisode': 371, 'rewardPerEpisode': 310.5082996558481
'totalSteps': 24320, 'rewardStep': 0.8779832690728333, 'errorList': [], 'lossList': [0.0, -1.1720536148548126, 0.0, 0.9174004595354199, 0.0, 0.0, 0.0], 'rewardMean': 0.7811051053896387, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.09303753726
'totalSteps': 25600, 'rewardStep': 0.9716436589298236, 'errorList': [0.17793707325559383, 0.17457974366849005, 0.1528084931572177, 0.17990192317961481, 0.16734150966356964, 0.21359395717257834, 0.1836638565117104, 0.14793536283429726, 0.19761942101752736, 0.17121825858575054, 0.16978562568992517, 0.1842610699687728, 0.18419575014563666, 0.16648370205560753, 0.1610606928798793, 0.18422466882476296, 0.19387609817153098, 0.17868169430448763, 0.1276634624312047, 0.20074226967817035, 0.14620851658389752, 0.16150265632899755, 0.15647431754960794, 0.20834092995474582, 0.16038454852852738, 0.1627833019159362, 0.1797935877495894, 0.13945792868385654, 0.1871719877511463, 0.1665021090285779, 0.18806486309791573, 0.1822859269797126, 0.1727236946567593, 0.21690293327293264, 0.13692115738980123, 0.15886228047264603, 0.24633645506468987, 0.1859400903572622, 0.14885308168210293, 0.2238597822805936, 0.16465136932788962, 0.16350594743782787, 0.15767685712921325, 0.13695282229490846, 0.1576881766486916, 0.19642013639771605, 0.1609017782995056, 0.151610481832054, 0.13724316446057566, 0.17075131716380337], 'lossList': [0.0, -1.1087723028659822, 0.0, 0.6181697612628341, 0.0, 0.0, 0.0], 'rewardMean': 0.8129236483960807, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.3454068495062, 'successfulTests': 44
#maxSuccessfulTests=44, maxSuccessfulTestsAtStep=25600, timeSpent=108.01
