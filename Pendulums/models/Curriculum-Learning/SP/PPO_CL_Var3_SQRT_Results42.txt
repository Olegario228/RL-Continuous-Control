#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 42
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.642345130222761, 'errorList': [], 'lossList': [0.0, -1.43956478536129, 0.0, 27.19620684146881, 0.0, 0.0, 0.0], 'rewardMean': 0.5600654527227795, 'totalEpisodes': 18, 'stepsPerEpisode': 134, 'rewardPerEpisode': 86.70177504961515
'totalSteps': 3840, 'rewardStep': 0.6956056646240423, 'errorList': [], 'lossList': [0.0, -1.4308309441804885, 0.0, 42.04978364944458, 0.0, 0.0, 0.0], 'rewardMean': 0.6052455233565338, 'totalEpisodes': 32, 'stepsPerEpisode': 32, 'rewardPerEpisode': 23.33780717491058
'totalSteps': 5120, 'rewardStep': 0.3622506370474069, 'errorList': [], 'lossList': [0.0, -1.3996224772930146, 0.0, 52.18350234031677, 0.0, 0.0, 0.0], 'rewardMean': 0.5444968017792521, 'totalEpisodes': 52, 'stepsPerEpisode': 50, 'rewardPerEpisode': 31.14551218599674
'totalSteps': 6400, 'rewardStep': 0.765505575933792, 'errorList': [], 'lossList': [0.0, -1.3771320033073424, 0.0, 92.13408710479736, 0.0, 0.0, 0.0], 'rewardMean': 0.58869855661016, 'totalEpisodes': 81, 'stepsPerEpisode': 30, 'rewardPerEpisode': 26.386770009045012
'totalSteps': 7680, 'rewardStep': 0.8694245239021754, 'errorList': [], 'lossList': [0.0, -1.3580156803131103, 0.0, 95.8558243560791, 0.0, 0.0, 0.0], 'rewardMean': 0.635486217825496, 'totalEpisodes': 121, 'stepsPerEpisode': 24, 'rewardPerEpisode': 19.400580716809085
'totalSteps': 8960, 'rewardStep': 0.9368364114520792, 'errorList': [5.85950188432236, 5.970929550622107, 11.579957208310807, 6.808813307836532, 3.3197491727663, 18.262090432775874, 17.086110456385757, 6.302318914185807, 5.877558208245014, 6.487011066851135, 4.506408246149247, 8.464056013164655, 8.807991093885772, 7.738131409904929, 6.653444439136607, 6.666332472287815, 3.8090922025555685, 5.185989206974626, 6.572283710670567, 7.203553322919131, 22.17855986530467, 7.3102703559353595, 6.626719874587696, 7.281866269935328, 7.111828545438637, 6.556020339952986, 7.120855123303784, 6.6387534064655105, 4.5921358399749925, 6.108979717835991, 6.181154848533144, 6.4237446832026075, 24.042882401667747, 6.698141559381436, 17.48040646215217, 7.078276015387777, 14.80900950846056, 6.09867130027571, 16.055561602646662, 6.755891321702799, 7.667043677044486, 6.103517814424695, 5.2340739572777215, 17.82437689205406, 6.297008939974683, 9.814813696008834, 6.443528717566299, 26.79922287328856, 2.55898562928633, 3.8072497542956056], 'lossList': [0.0, -1.3429820668697356, 0.0, 50.73639937400818, 0.0, 0.0, 0.0], 'rewardMean': 0.6785362454864364, 'totalEpisodes': 138, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.636180705420099, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.41147506843021664, 'errorList': [], 'lossList': [0.0, -1.3480812168121339, 0.0, 33.10031124591828, 0.0, 0.0, 0.0], 'rewardMean': 0.645153598354409, 'totalEpisodes': 145, 'stepsPerEpisode': 135, 'rewardPerEpisode': 112.10627999392278
'totalSteps': 11520, 'rewardStep': 0.6294693525672074, 'errorList': [], 'lossList': [0.0, -1.3598968374729157, 0.0, 23.941571600437165, 0.0, 0.0, 0.0], 'rewardMean': 0.6434109043780533, 'totalEpisodes': 152, 'stepsPerEpisode': 213, 'rewardPerEpisode': 151.3181075146092
'totalSteps': 12800, 'rewardStep': 0.6669994325534384, 'errorList': [], 'lossList': [0.0, -1.368863770365715, 0.0, 24.19938381433487, 0.0, 0.0, 0.0], 'rewardMean': 0.6457697571955918, 'totalEpisodes': 157, 'stepsPerEpisode': 142, 'rewardPerEpisode': 103.85164955318814
'totalSteps': 14080, 'rewardStep': 0.7142869543148612, 'errorList': [], 'lossList': [0.0, -1.361884549856186, 0.0, 8.509483643174171, 0.0, 0.0, 0.0], 'rewardMean': 0.6694198751047981, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 925.3360908338415
'totalSteps': 15360, 'rewardStep': 0.5843179960788571, 'errorList': [], 'lossList': [0.0, -1.343307832479477, 0.0, 8.719782398939133, 0.0, 0.0, 0.0], 'rewardMean': 0.6636171616904077, 'totalEpisodes': 158, 'stepsPerEpisode': 1072, 'rewardPerEpisode': 745.5776364884632
'totalSteps': 16640, 'rewardStep': 0.7192898199981281, 'errorList': [], 'lossList': [0.0, -1.3270896065235138, 0.0, 4.3796392405033115, 0.0, 0.0, 0.0], 'rewardMean': 0.6659855772278162, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 985.6986699933885
'totalSteps': 17920, 'rewardStep': 0.8514890541382318, 'errorList': [], 'lossList': [0.0, -1.298643308877945, 0.0, 4.211043278127908, 0.0, 0.0, 0.0], 'rewardMean': 0.7149094189368987, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.7997163046684
'totalSteps': 19200, 'rewardStep': 0.9377831271874608, 'errorList': [0.06129085080862888, 0.03595370099637372, 0.04983588858204905, 0.07695965856829082, 0.051501455824373404, 0.04580418848922292, 0.046893692595977, 0.07725938082202514, 0.07432680532923908, 0.06948686165516262, 0.05915085732380946, 0.04784204954137111, 0.05422306804843278, 0.05337942675015609, 0.04076833925341639, 0.04014156286894028, 0.03846629613975234, 0.07626755154868059, 0.045850876241045176, 0.0456107963931841, 0.035735169826883594, 0.04277091414618372, 0.04369739194866102, 0.04603014630763923, 0.04431450850069442, 0.04227734788567153, 0.06151573376397861, 0.06033731406197098, 0.03460680690426299, 0.04371503099763658, 0.06361183988585623, 0.045500002034208016, 0.06113308901105451, 0.056241303270687944, 0.0406036438186204, 0.03254042021968445, 0.04755973966274009, 0.04126544009895333, 0.06259164560884273, 0.0339696069473905, 0.039717502888415045, 0.033867737018322665, 0.041025127641611085, 0.07384089979977336, 0.037993041057710314, 0.03592934323776065, 0.050603094888741075, 0.03525782558440696, 0.058043456953313206, 0.0575379663645231], 'lossList': [0.0, -1.246119544506073, 0.0, 3.781407184973359, 0.0, 0.0, 0.0], 'rewardMean': 0.7321371740622655, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1140.288753401182, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=93.1
