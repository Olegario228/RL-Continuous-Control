#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 43
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.6114477823479096, 'errorList': [], 'lossList': [0.0, -1.4164487993717194, 0.0, 24.868215618133544, 0.0, 0.0, 0.0], 'rewardMean': 0.7348299454517246, 'totalEpisodes': 18, 'stepsPerEpisode': 39, 'rewardPerEpisode': 31.309919363282688
'totalSteps': 3840, 'rewardStep': 0.7505712030362071, 'errorList': [], 'lossList': [0.0, -1.3949083614349365, 0.0, 50.56406836509704, 0.0, 0.0, 0.0], 'rewardMean': 0.7400770313132187, 'totalEpisodes': 33, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.315819860664185
'totalSteps': 5120, 'rewardStep': 0.8898390437398372, 'errorList': [], 'lossList': [0.0, -1.3899867111444473, 0.0, 64.19084237098694, 0.0, 0.0, 0.0], 'rewardMean': 0.7775175344198734, 'totalEpisodes': 48, 'stepsPerEpisode': 77, 'rewardPerEpisode': 69.18711254001576
'totalSteps': 6400, 'rewardStep': 0.5659328354026889, 'errorList': [], 'lossList': [0.0, -1.3789963960647582, 0.0, 102.85371520996094, 0.0, 0.0, 0.0], 'rewardMean': 0.7352005946164365, 'totalEpisodes': 74, 'stepsPerEpisode': 18, 'rewardPerEpisode': 12.782071760381962
'totalSteps': 7680, 'rewardStep': 0.40307189734829996, 'errorList': [], 'lossList': [0.0, -1.3745057046413423, 0.0, 105.58812118530274, 0.0, 0.0, 0.0], 'rewardMean': 0.6798458117384137, 'totalEpisodes': 106, 'stepsPerEpisode': 35, 'rewardPerEpisode': 23.19990758652983
'totalSteps': 8960, 'rewardStep': 0.6915723073143418, 'errorList': [], 'lossList': [0.0, -1.3670549923181534, 0.0, 55.068696374893186, 0.0, 0.0, 0.0], 'rewardMean': 0.6815210253921178, 'totalEpisodes': 127, 'stepsPerEpisode': 68, 'rewardPerEpisode': 52.31230530287942
'totalSteps': 10240, 'rewardStep': 0.7509221750457065, 'errorList': [], 'lossList': [0.0, -1.346974630355835, 0.0, 42.54129672050476, 0.0, 0.0, 0.0], 'rewardMean': 0.6901961690988163, 'totalEpisodes': 137, 'stepsPerEpisode': 165, 'rewardPerEpisode': 141.4969741677629
'totalSteps': 11520, 'rewardStep': 0.618910948296242, 'errorList': [], 'lossList': [0.0, -1.3243019950389863, 0.0, 15.888920104503631, 0.0, 0.0, 0.0], 'rewardMean': 0.6822755890096414, 'totalEpisodes': 143, 'stepsPerEpisode': 51, 'rewardPerEpisode': 41.53626589073069
'totalSteps': 12800, 'rewardStep': 0.5616055368394046, 'errorList': [], 'lossList': [0.0, -1.2975881803035736, 0.0, 13.988599722385407, 0.0, 0.0, 0.0], 'rewardMean': 0.6702085837926177, 'totalEpisodes': 148, 'stepsPerEpisode': 369, 'rewardPerEpisode': 259.2834323878364
'totalSteps': 14080, 'rewardStep': 0.924766211334028, 'errorList': [], 'lossList': [0.0, -1.2648038583993912, 0.0, 19.088175679445268, 0.0, 0.0, 0.0], 'rewardMean': 0.6768639940704665, 'totalEpisodes': 157, 'stepsPerEpisode': 49, 'rewardPerEpisode': 37.64615115687486
'totalSteps': 15360, 'rewardStep': 0.6430675323058426, 'errorList': [], 'lossList': [0.0, -1.2691661685705184, 0.0, 7.263857107162476, 0.0, 0.0, 0.0], 'rewardMean': 0.6800259690662598, 'totalEpisodes': 165, 'stepsPerEpisode': 84, 'rewardPerEpisode': 70.1868312366495
'totalSteps': 16640, 'rewardStep': 0.6367855208703759, 'errorList': [], 'lossList': [0.0, -1.272183507680893, 0.0, 3.9041803461313247, 0.0, 0.0, 0.0], 'rewardMean': 0.6686474008496768, 'totalEpisodes': 171, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.304121273691063
'totalSteps': 17920, 'rewardStep': 0.8210381538522438, 'errorList': [], 'lossList': [0.0, -1.2647034591436386, 0.0, 4.649508474469185, 0.0, 0.0, 0.0], 'rewardMean': 0.6617673118609173, 'totalEpisodes': 176, 'stepsPerEpisode': 122, 'rewardPerEpisode': 99.93015283764446
'totalSteps': 19200, 'rewardStep': 0.9577945318243599, 'errorList': [0.275340082224828, 0.2295970590263736, 0.23837761603974908, 0.22831701351653444, 0.22688719901072374, 0.3072583312192931, 0.3495236008654781, 0.24365693391378834, 0.23193565870913121, 0.3727403607359915, 0.27295588947417737, 0.22928488842257738, 0.23555529995298988, 0.224602189907025, 0.2197957640635595, 0.22047003122672043, 0.2505562426314006, 0.36953612032150807, 0.22720815190970287, 0.22624634721278677, 0.242131877105323, 0.258097733188566, 0.2339162777045935, 0.22045079651019792, 0.2465633383766981, 0.24017953024371927, 0.26246715875706583, 0.21814421398688555, 0.27417061848419394, 0.23170279043544334, 0.22604329733467982, 0.2388588301116723, 0.312363307697892, 0.2211620620970609, 0.24478491469559635, 0.21387780139775836, 0.24232388602252095, 0.22624758404110257, 0.23175514544361164, 0.22336185108425177, 0.22258422548620443, 0.2882626829414103, 0.2387546424228023, 0.24543506669030588, 0.2304826661960362, 0.2227388743326016, 0.2224053313970776, 0.29329675345709705, 0.2383127373930304, 0.2248502987073039], 'lossList': [0.0, -1.2721630698442459, 0.0, 5.99040129840374, 0.0, 0.0, 0.0], 'rewardMean': 0.7009534815030845, 'totalEpisodes': 178, 'stepsPerEpisode': 203, 'rewardPerEpisode': 165.58573824393451, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.9440332845865687, 'errorList': [0.16869309025955895, 0.1452662827611499, 0.14226885584611615, 0.1560885467321059, 0.14427692489336702, 0.1451476562575902, 0.19064280121848468, 0.15081077764688888, 0.1319019227748474, 0.1395132762344766, 0.13970530743482493, 0.12620919106377815, 0.1634481830264373, 0.15364645189905876, 0.14857278606962868, 0.1503495804745318, 0.16951280086318762, 0.15433545109836566, 0.16318029363131964, 0.13606356730938815, 0.15943846648668333, 0.13197607550005958, 0.16800277164983166, 0.16601896138109662, 0.17063472606692873, 0.16237606579179725, 0.1380306857334636, 0.14974367122785012, 0.13552581928140184, 0.14911076586526792, 0.1751845308044574, 0.1509255784691785, 0.13420587605589704, 0.1636488027609273, 0.13901235315111968, 0.15389971192851956, 0.13910871035286437, 0.16204369518149542, 0.15575242118511612, 0.14793778959728193, 0.14926074281472476, 0.15030369688938708, 0.1503873085518624, 0.17797000886772169, 0.13302887114565534, 0.13928084794904694, 0.1758718074067781, 0.14995955969302416, 0.14894130474662176, 0.13858560051509336], 'lossList': [0.0, -1.2505469363927841, 0.0, 2.31710661187768, 0.0, 0.0, 0.0], 'rewardMean': 0.7550496202269115, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.863002836291, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=95.94
