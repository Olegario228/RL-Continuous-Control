#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 44
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.889625818249738, 'errorList': [], 'lossList': [0.0, -1.4244377672672273, 0.0, 27.37944311618805, 0.0, 0.0, 0.0], 'rewardMean': 0.8064883710645194, 'totalEpisodes': 18, 'stepsPerEpisode': 37, 'rewardPerEpisode': 30.56873662331066
'totalSteps': 3840, 'rewardStep': 0.6297446568568105, 'errorList': [], 'lossList': [0.0, -1.4313225615024567, 0.0, 47.12198865890503, 0.0, 0.0, 0.0], 'rewardMean': 0.7475737996619497, 'totalEpisodes': 37, 'stepsPerEpisode': 55, 'rewardPerEpisode': 40.5157685187798
'totalSteps': 5120, 'rewardStep': 0.8455322067552649, 'errorList': [], 'lossList': [0.0, -1.4398399835824967, 0.0, 49.406488733291624, 0.0, 0.0, 0.0], 'rewardMean': 0.7720634014352785, 'totalEpisodes': 52, 'stepsPerEpisode': 45, 'rewardPerEpisode': 37.9014605323638
'totalSteps': 6400, 'rewardStep': 0.6075104233303734, 'errorList': [], 'lossList': [0.0, -1.4375336968898773, 0.0, 97.35121223449707, 0.0, 0.0, 0.0], 'rewardMean': 0.7391528058142975, 'totalEpisodes': 80, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.124731805976417
'totalSteps': 7680, 'rewardStep': 0.9621157547511515, 'errorList': [263.58616790356933, 207.76629879087128, 235.70083788014236, 201.36695193504147, 105.58193387530385, 262.84110410021003, 103.43248563127946, 181.14708779508533, 244.95349606816964, 165.11012887083456, 105.96198780431678, 98.39799265580032, 240.97808511396482, 267.20303093555225, 212.4452586341375, 262.2433899827709, 250.0370082968969, 192.74144084394783, 199.00807199650757, 264.9050033384263, 124.1030951166217, 86.6245392183812, 175.69035215325363, 238.6224058827761, 223.90969485617953, 108.82866587439769, 119.97199866535516, 226.89907344732248, 233.7772685348015, 1.8944330851280085, 121.0790071202025, 224.2427485468199, 250.6766104686532, 76.64924425580308, 275.0337113831385, 211.21018199992116, 174.43265946778268, 76.00638592385624, 90.29656494017263, 102.74693437106289, 159.70999854752571, 269.1602765614774, 250.2849629121293, 219.31790122930198, 256.0554584864311, 172.60827236349724, 177.1510761756601, 117.65048506210928, 137.02590060804118, 259.47755703254546], 'lossList': [0.0, -1.4415556704998016, 0.0, 88.79098249435425, 0.0, 0.0, 0.0], 'rewardMean': 0.7763132973037732, 'totalEpisodes': 112, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.587006403034082, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.6721227740815766, 'errorList': [], 'lossList': [0.0, -1.4349183440208435, 0.0, 54.62084438323975, 0.0, 0.0, 0.0], 'rewardMean': 0.7614289368434594, 'totalEpisodes': 138, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.491022010997703
'totalSteps': 10240, 'rewardStep': 0.7742514800228454, 'errorList': [], 'lossList': [0.0, -1.418178718686104, 0.0, 42.5467073726654, 0.0, 0.0, 0.0], 'rewardMean': 0.7630317547408827, 'totalEpisodes': 150, 'stepsPerEpisode': 153, 'rewardPerEpisode': 118.25988540612417
'totalSteps': 11520, 'rewardStep': 0.4855405735959398, 'errorList': [], 'lossList': [0.0, -1.396896367073059, 0.0, 15.056275177001954, 0.0, 0.0, 0.0], 'rewardMean': 0.7321994012803335, 'totalEpisodes': 154, 'stepsPerEpisode': 235, 'rewardPerEpisode': 160.70761065103463
'totalSteps': 12800, 'rewardStep': 0.8146953602536844, 'errorList': [], 'lossList': [0.0, -1.353747357726097, 0.0, 39.335853171348575, 0.0, 0.0, 0.0], 'rewardMean': 0.7404489971776685, 'totalEpisodes': 162, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.21405950968549
'totalSteps': 14080, 'rewardStep': 0.9345982801585203, 'errorList': [46.68691036780154, 78.98488460732247, 51.92346841715362, 47.39761903709131, 70.96412131077093, 77.04840004381687, 50.49134543540799, 70.1069888660035, 59.942216962688306, 71.50714378700421, 67.90834404187662, 82.92990366540157, 76.75437206952395, 79.10077011526778, 42.84675823102536, 51.71829287248765, 33.668825905960496, 51.89435511021061, 57.98918521558836, 76.59017204530461, 43.83213944016724, 82.80497535467485, 63.98712627887384, 64.75581579746546, 67.26115086466778, 53.49134231674701, 66.29365520862811, 59.52679313547888, 55.57064487434955, 49.050295892251555, 53.81969822376017, 82.79496182172555, 80.21227311280235, 51.573215362343674, 46.664449446707394, 65.42323046398054, 40.006227452288975, 61.46625105109425, 62.230939824731564, 41.87503619546258, 61.5753379785601, 65.52492221840485, 56.355109055528615, 37.49673579146012, 61.06060109907033, 76.07146658648153, 79.69225042196437, 52.00142231195737, 72.30559278437804, 67.19323148716231], 'lossList': [0.0, -1.3180360901355743, 0.0, 13.627653564214706, 0.0, 0.0, 0.0], 'rewardMean': 0.7615737328055905, 'totalEpisodes': 168, 'stepsPerEpisode': 48, 'rewardPerEpisode': 45.18014308134156, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7523815253259666, 'errorList': [], 'lossList': [0.0, -1.2949512636661529, 0.0, 47.42457120418548, 0.0, 0.0, 0.0], 'rewardMean': 0.7478493035132134, 'totalEpisodes': 177, 'stepsPerEpisode': 70, 'rewardPerEpisode': 56.823686647366635
'totalSteps': 16640, 'rewardStep': 0.15757424229389455, 'errorList': [], 'lossList': [0.0, -1.2757743990421295, 0.0, 7.085936700105667, 0.0, 0.0, 0.0], 'rewardMean': 0.7006322620569218, 'totalEpisodes': 184, 'stepsPerEpisode': 206, 'rewardPerEpisode': 155.20119412000207
'totalSteps': 17920, 'rewardStep': 0.39457328953675186, 'errorList': [], 'lossList': [0.0, -1.2569043534994124, 0.0, 5.200261607766151, 0.0, 0.0, 0.0], 'rewardMean': 0.6555363703350705, 'totalEpisodes': 189, 'stepsPerEpisode': 223, 'rewardPerEpisode': 179.55018238854112
'totalSteps': 19200, 'rewardStep': 0.8044521958228397, 'errorList': [], 'lossList': [0.0, -1.2295501005649567, 0.0, 5.316221716403962, 0.0, 0.0, 0.0], 'rewardMean': 0.6752305475843171, 'totalEpisodes': 192, 'stepsPerEpisode': 350, 'rewardPerEpisode': 289.5673047291396
'totalSteps': 20480, 'rewardStep': 0.4798140911159988, 'errorList': [], 'lossList': [0.0, -1.1987342441082, 0.0, 6.704957968592644, 0.0, 0.0, 0.0], 'rewardMean': 0.6270003812208018, 'totalEpisodes': 196, 'stepsPerEpisode': 227, 'rewardPerEpisode': 154.80154978942275
'totalSteps': 21760, 'rewardStep': 0.5340103067033029, 'errorList': [], 'lossList': [0.0, -1.1791029047966004, 0.0, 3.149147251546383, 0.0, 0.0, 0.0], 'rewardMean': 0.6131891344829744, 'totalEpisodes': 197, 'stepsPerEpisode': 1261, 'rewardPerEpisode': 899.3460028688057
'totalSteps': 23040, 'rewardStep': 0.7658014086356064, 'errorList': [], 'lossList': [0.0, -1.1673724961280822, 0.0, 3.374386838078499, 0.0, 0.0, 0.0], 'rewardMean': 0.6123441273442506, 'totalEpisodes': 200, 'stepsPerEpisode': 662, 'rewardPerEpisode': 568.054305981666
'totalSteps': 24320, 'rewardStep': 0.8324006453376389, 'errorList': [], 'lossList': [0.0, -1.1685440355539323, 0.0, 7.030904262065888, 0.0, 0.0, 0.0], 'rewardMean': 0.6470301345184204, 'totalEpisodes': 202, 'stepsPerEpisode': 173, 'rewardPerEpisode': 152.17323207682006
'totalSteps': 25600, 'rewardStep': 0.9332917555847463, 'errorList': [0.11153977924136135, 0.10451182734166967, 0.10830326375007122, 0.11047197905804512, 0.11366481071791908, 0.17852235451666515, 0.14327544913060292, 0.10424997926473185, 0.11232237788121727, 0.10812874017029825, 0.11409562592345923, 0.13474289602221023, 0.1360909674269466, 0.11008089389260672, 0.11180525909542342, 0.1102242674439774, 0.11567188375586826, 0.11072921883618751, 0.13132940312741131, 0.11986074133526538, 0.122966281930218, 0.13082006103559962, 0.11430000137790236, 0.1088564796027065, 0.10867123969234507, 0.11566167579040143, 0.1051968627982439, 0.10465135690111832, 0.1119264974741188, 0.12174878330504776, 0.1394654880340728, 0.11397176220275376, 0.19679677582982988, 0.11897332263579073, 0.11488419608348997, 0.14648143114388804, 0.11146821321357356, 0.1568202692478686, 0.1290349506980851, 0.10989527759109989, 0.10847643406480297, 0.11295112677109646, 0.10313056667513601, 0.11668898692137696, 0.11770978326020448, 0.10448645500675287, 0.10189944288971375, 0.11255867824903716, 0.11695254768799786, 0.10871966560131575], 'lossList': [0.0, -1.1372341495752334, 0.0, 0.687460260540247, 0.0, 0.0, 0.0], 'rewardMean': 0.6588897740515266, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1087.7787860229087, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=128.76
