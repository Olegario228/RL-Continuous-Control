#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 46
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.7148267905013156, 'errorList': [], 'lossList': [0.0, -1.4548435401916504, 0.0, 30.098162891864778, 0.0, 0.0, 0.0], 'rewardMean': 0.6540023050407375, 'totalEpisodes': 12, 'stepsPerEpisode': 88, 'rewardPerEpisode': 70.17905471433934
'totalSteps': 3840, 'rewardStep': 0.8697002181185166, 'errorList': [], 'lossList': [0.0, -1.4754291254281997, 0.0, 45.068949460983276, 0.0, 0.0, 0.0], 'rewardMean': 0.7259016093999971, 'totalEpisodes': 22, 'stepsPerEpisode': 167, 'rewardPerEpisode': 113.9144919743452
'totalSteps': 5120, 'rewardStep': 0.7186912289252353, 'errorList': [], 'lossList': [0.0, -1.4812707287073135, 0.0, 46.58030154228211, 0.0, 0.0, 0.0], 'rewardMean': 0.7240990142813067, 'totalEpisodes': 32, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.922362262332357
'totalSteps': 6400, 'rewardStep': 0.8625261467526151, 'errorList': [], 'lossList': [0.0, -1.485721006989479, 0.0, 110.84888410568237, 0.0, 0.0, 0.0], 'rewardMean': 0.7517844407755684, 'totalEpisodes': 56, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.7455900557352573
'totalSteps': 7680, 'rewardStep': 0.768323162882562, 'errorList': [], 'lossList': [0.0, -1.481853940486908, 0.0, 109.75281391143798, 0.0, 0.0, 0.0], 'rewardMean': 0.7545408944600673, 'totalEpisodes': 92, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.018313394273584
'totalSteps': 8960, 'rewardStep': 0.8959945665539247, 'errorList': [], 'lossList': [0.0, -1.4708802378177643, 0.0, 61.94381801605225, 0.0, 0.0, 0.0], 'rewardMean': 0.774748561902047, 'totalEpisodes': 120, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.274923408943446
'totalSteps': 10240, 'rewardStep': 0.8287229396430923, 'errorList': [], 'lossList': [0.0, -1.4531586396694183, 0.0, 29.86308858394623, 0.0, 0.0, 0.0], 'rewardMean': 0.7814953591196776, 'totalEpisodes': 130, 'stepsPerEpisode': 55, 'rewardPerEpisode': 46.19031009360019
'totalSteps': 11520, 'rewardStep': 0.48417493423913976, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.72203127414357, 'totalEpisodes': 140, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.27607652507994
'totalSteps': 12800, 'rewardStep': 0.659597833346228, 'errorList': [], 'lossList': [0.0, -1.4291593289375306, 0.0, 47.813590965271, 0.0, 0.0, 0.0], 'rewardMean': 0.7286732755201769, 'totalEpisodes': 152, 'stepsPerEpisode': 173, 'rewardPerEpisode': 127.28717075838509
'totalSteps': 14080, 'rewardStep': 0.7594806088146433, 'errorList': [], 'lossList': [0.0, -1.395686520934105, 0.0, 50.650439491271975, 0.0, 0.0, 0.0], 'rewardMean': 0.7331386573515097, 'totalEpisodes': 160, 'stepsPerEpisode': 163, 'rewardPerEpisode': 139.74763125244323
'totalSteps': 15360, 'rewardStep': 0.8672934760827588, 'errorList': [], 'lossList': [0.0, -1.3863197845220565, 0.0, 11.208660195469855, 0.0, 0.0, 0.0], 'rewardMean': 0.7328979831479339, 'totalEpisodes': 162, 'stepsPerEpisode': 668, 'rewardPerEpisode': 554.0264897334703
'totalSteps': 16640, 'rewardStep': 0.3694527389343129, 'errorList': [], 'lossList': [0.0, -1.3780231845378876, 0.0, 15.749225579500198, 0.0, 0.0, 0.0], 'rewardMean': 0.6979741341488417, 'totalEpisodes': 165, 'stepsPerEpisode': 509, 'rewardPerEpisode': 318.03228860946047
'totalSteps': 17920, 'rewardStep': 0.6397564421877413, 'errorList': [], 'lossList': [0.0, -1.3557861959934234, 0.0, 7.663883115649224, 0.0, 0.0, 0.0], 'rewardMean': 0.6756971636923543, 'totalEpisodes': 167, 'stepsPerEpisode': 774, 'rewardPerEpisode': 611.0204105731242
'totalSteps': 19200, 'rewardStep': 0.929448612058833, 'errorList': [], 'lossList': [0.0, -1.318482553958893, 0.0, 4.2427774211764335, 0.0, 0.0, 0.0], 'rewardMean': 0.6918097086099813, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 918.0628465270587
'totalSteps': 20480, 'rewardStep': 0.712035928819639, 'errorList': [], 'lossList': [0.0, -1.2729636871814727, 0.0, 6.041655995249748, 0.0, 0.0, 0.0], 'rewardMean': 0.6734138448365529, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.2794419746497
'totalSteps': 21760, 'rewardStep': 0.6370685882650723, 'errorList': [], 'lossList': [0.0, -1.2347850739955901, 0.0, 2.475883511453867, 0.0, 0.0, 0.0], 'rewardMean': 0.6542484096987508, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.1647724505804
'totalSteps': 23040, 'rewardStep': 0.9333656369410892, 'errorList': [0.18449445647597218, 0.1753888358865849, 0.16890823736412827, 0.17854406328113082, 0.14131869436857272, 0.17604211324224636, 0.2080664391131231, 0.187751384677605, 0.24598512046902213, 0.16345288398403188, 0.14310874589158035, 0.21136168519230805, 0.19659012389943062, 0.14203481189120723, 0.1593221202690493, 0.22875618681025786, 0.1738499096145009, 0.1659805230423646, 0.1888289060333473, 0.1924685250034688, 0.13968454053113574, 0.2038408442968333, 0.17691705299602606, 0.17768837318867053, 0.20887807299953243, 0.2426465951791217, 0.16285402116821016, 0.20182810049442798, 0.1406309555794677, 0.1527771409231181, 0.14877005036716204, 0.1719323666636702, 0.16813011884011325, 0.17023404947027249, 0.18721711745712752, 0.14434893162608278, 0.20901897558796628, 0.21276514023247975, 0.1485828585887402, 0.18578848938403514, 0.18243773938505561, 0.18290694416786094, 0.18889847356763567, 0.17632826625617865, 0.23307619515636646, 0.18120885115629487, 0.16741561216052034, 0.16783816749296482, 0.1850345000711726, 0.15908887293745266], 'lossList': [0.0, -1.2079424864053727, 0.0, 1.4259602297842502, 0.0, 0.0, 0.0], 'rewardMean': 0.6991674799689458, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.4699664439775, 'successfulTests': 39
'totalSteps': 24320, 'rewardStep': 0.8362637877600481, 'errorList': [], 'lossList': [0.0, -1.1695374089479447, 0.0, 1.4024190489947796, 0.0, 0.0, 0.0], 'rewardMean': 0.7343763653210366, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.0338819371434
'totalSteps': 25600, 'rewardStep': 0.9742634026950984, 'errorList': [0.1089620545352518, 0.08757853029089706, 0.20625211900576965, 0.07740355469433598, 0.08470261937629751, 0.12124982421762999, 0.09239196879812887, 0.13838360576075634, 0.16100986325325412, 0.06770880304337333, 0.08572743114383209, 0.09283648701206845, 0.10890214784649024, 0.12513467875652518, 0.13092364799564468, 0.06934319381599137, 0.15931887703182948, 0.08344440702846012, 0.10986560152374425, 0.11967073255474399, 0.1283890030140447, 0.08886702549684974, 0.12795359228622907, 0.13091449414372697, 0.11389396403840613, 0.16474631350176097, 0.08646410963343767, 0.11322566119460775, 0.1563957579609529, 0.10278094073346865, 0.09450243426658475, 0.07414539305253696, 0.07683094673235198, 0.16363004202455195, 0.09350676135032256, 0.1072234575177559, 0.09478135055445917, 0.1710548052159239, 0.22519764097298536, 0.1622804107757844, 0.0678024535787752, 0.16036351364295168, 0.08046003692728479, 0.10502457508231137, 0.1585767926018704, 0.14193363161491832, 0.15426160190973784, 0.1418859729336119, 0.1637127566859011, 0.08845105425770629], 'lossList': [0.0, -1.1098957306146622, 0.0, 1.8174433244764805, 0.0, 0.0, 0.0], 'rewardMean': 0.7658429222559235, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1193.7092359103037, 'successfulTests': 48
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=25600, timeSpent=104.99
