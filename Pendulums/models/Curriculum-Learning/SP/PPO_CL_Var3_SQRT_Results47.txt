#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 47
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.6493290885893972, 'errorList': [], 'lossList': [0.0, -1.451568723320961, 0.0, 28.91070172548294, 0.0, 0.0, 0.0], 'rewardMean': 0.5762149682467045, 'totalEpisodes': 15, 'stepsPerEpisode': 292, 'rewardPerEpisode': 212.61733113050067
'totalSteps': 3840, 'rewardStep': 0.7964150022812814, 'errorList': [], 'lossList': [0.0, -1.4654852402210237, 0.0, 47.478796253204344, 0.0, 0.0, 0.0], 'rewardMean': 0.6496149795915634, 'totalEpisodes': 29, 'stepsPerEpisode': 32, 'rewardPerEpisode': 25.256735977936785
'totalSteps': 5120, 'rewardStep': 0.5075006282804265, 'errorList': [], 'lossList': [0.0, -1.448465228676796, 0.0, 57.5783843421936, 0.0, 0.0, 0.0], 'rewardMean': 0.6140863917637792, 'totalEpisodes': 46, 'stepsPerEpisode': 47, 'rewardPerEpisode': 32.991180511616804
'totalSteps': 6400, 'rewardStep': 0.7342692324355784, 'errorList': [], 'lossList': [0.0, -1.4293349993228912, 0.0, 93.67603773117065, 0.0, 0.0, 0.0], 'rewardMean': 0.638122959898139, 'totalEpisodes': 71, 'stepsPerEpisode': 28, 'rewardPerEpisode': 22.667400093499925
'totalSteps': 7680, 'rewardStep': 0.6867526508930989, 'errorList': [], 'lossList': [0.0, -1.4161071056127548, 0.0, 96.03128438949585, 0.0, 0.0, 0.0], 'rewardMean': 0.6462279083972989, 'totalEpisodes': 108, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.768405588337625
'totalSteps': 8960, 'rewardStep': 0.8087583002952565, 'errorList': [], 'lossList': [0.0, -1.399168204665184, 0.0, 58.820695056915284, 0.0, 0.0, 0.0], 'rewardMean': 0.6694465358112929, 'totalEpisodes': 135, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.31346828936596
'totalSteps': 10240, 'rewardStep': 0.4815133082393289, 'errorList': [], 'lossList': [0.0, -1.381701837182045, 0.0, 24.737844896316528, 0.0, 0.0, 0.0], 'rewardMean': 0.6459548823647975, 'totalEpisodes': 145, 'stepsPerEpisode': 45, 'rewardPerEpisode': 31.249772747104874
'totalSteps': 11520, 'rewardStep': 0.7639689193029129, 'errorList': [], 'lossList': [0.0, -1.3713547682762146, 0.0, 34.02770712375641, 0.0, 0.0, 0.0], 'rewardMean': 0.6590675531356992, 'totalEpisodes': 153, 'stepsPerEpisode': 64, 'rewardPerEpisode': 44.32531231084722
'totalSteps': 12800, 'rewardStep': 0.9589285925654402, 'errorList': [3.629543565724709, 0.11603060339432486, 6.67738470285523, 4.656847549229498, 1.6060890843945186, 3.9855893802900964, 8.787093055394493, 1.779365548730813, 9.77226877955357, 5.846887224261583, 2.8916934844768827, 3.1997256103188856, 4.116225702074527, 1.2038227979405267, 4.525252550786799, 4.081597814173703, 7.497930655907361, 5.4951042878999745, 2.3479907144954497, 1.6169638218157394, 0.9144170326162493, 5.944846450869821, 1.471107378197501, 1.5621765298686687, 6.034562761147132, 1.0691172053871179, 2.25282784038083, 0.9431734400228867, 2.583897345067316, 3.5975706309794946, 1.227406687311151, 2.121893082705844, 6.14714702436806, 3.9178107355897382, 0.5543820404058112, 2.541661159079398, 0.16911630992658372, 3.3518859626884527, 3.965021463737061, 3.157929663316622, 2.824237458695394, 6.138314297366438, 0.6033445112506314, 5.378138194613229, 1.1942419384648326, 0.5466756338837939, 3.244432429381017, 2.0165967189732306, 0.13603814403497735, 2.731033939908719], 'lossList': [0.0, -1.3635320925712586, 0.0, 31.593231896162035, 0.0, 0.0, 0.0], 'rewardMean': 0.6890536570786734, 'totalEpisodes': 158, 'stepsPerEpisode': 142, 'rewardPerEpisode': 120.96917810552515, 'successfulTests': 3
'totalSteps': 14080, 'rewardStep': 0.854691876089328, 'errorList': [], 'lossList': [0.0, -1.3483419561386107, 0.0, 11.16547870695591, 0.0, 0.0, 0.0], 'rewardMean': 0.7242127598972049, 'totalEpisodes': 161, 'stepsPerEpisode': 30, 'rewardPerEpisode': 27.43325022581035
'totalSteps': 15360, 'rewardStep': 0.8574793664205718, 'errorList': [], 'lossList': [0.0, -1.3256068652868271, 0.0, 19.153155502676963, 0.0, 0.0, 0.0], 'rewardMean': 0.7450277876803224, 'totalEpisodes': 163, 'stepsPerEpisode': 840, 'rewardPerEpisode': 698.8517965402901
'totalSteps': 16640, 'rewardStep': 0.8534098899744993, 'errorList': [], 'lossList': [0.0, -1.3229703772068024, 0.0, 9.486301946640015, 0.0, 0.0, 0.0], 'rewardMean': 0.7507272764496442, 'totalEpisodes': 166, 'stepsPerEpisode': 204, 'rewardPerEpisode': 167.99478257059533
'totalSteps': 17920, 'rewardStep': 0.6798720370740756, 'errorList': [], 'lossList': [0.0, -1.3166237592697143, 0.0, 3.5251101467013357, 0.0, 0.0, 0.0], 'rewardMean': 0.7679644173290091, 'totalEpisodes': 167, 'stepsPerEpisode': 827, 'rewardPerEpisode': 671.9810187683958
'totalSteps': 19200, 'rewardStep': 0.9357750227293695, 'errorList': [0.5559350254104143, 0.5458833986475276, 0.5280247505369586, 0.6078655682792145, 0.5654996976161364, 0.5487567782587848, 0.5882417823538152, 0.5559956612187428, 0.5167587959825855, 0.5265222644123992, 0.5548966388127834, 0.5436751907998209, 0.5775525652880772, 0.5728455451056385, 0.5509483420561946, 0.5403661443689343, 0.5400501882654803, 0.5374518990170921, 0.5911096660693536, 0.5309204342936865, 0.5123499134934207, 0.5456138132372101, 0.5732901889728844, 0.5490184446565496, 0.5305660946443002, 0.5596049954646912, 0.548726053414005, 0.5671672400778849, 0.5785229133942179, 0.5203036957151712, 0.6008200832605078, 0.5553889297047961, 0.5631096107727231, 0.5548109178629711, 0.5736849697523492, 0.5374496195564128, 0.5844095034761758, 0.5499858367912532, 0.5752702564806865, 0.5647381979577887, 0.5826552654345752, 0.5479000207808329, 0.5187860094906527, 0.548217048080272, 0.5772725677549722, 0.5694839114206263, 0.5555085237075557, 0.5675156383755997, 0.5525407852726265, 0.5536349159707611], 'lossList': [0.0, -1.3111621356010437, 0.0, 8.146391617655754, 0.0, 0.0, 0.0], 'rewardMean': 0.7881149963583882, 'totalEpisodes': 171, 'stepsPerEpisode': 111, 'rewardPerEpisode': 99.26001767471759, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.6580675855121966, 'errorList': [], 'lossList': [0.0, -1.2952294856309892, 0.0, 1.5130020144581795, 0.0, 0.0, 0.0], 'rewardMean': 0.7852464898202981, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 826.6943648925086
'totalSteps': 21760, 'rewardStep': 0.760109644094344, 'errorList': [], 'lossList': [0.0, -1.2468356394767761, 0.0, 1.5614300215244292, 0.0, 0.0, 0.0], 'rewardMean': 0.7803816242002066, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1039.2434196035492
'totalSteps': 23040, 'rewardStep': 0.8954868167196338, 'errorList': [], 'lossList': [0.0, -1.1930580115318299, 0.0, 1.2722664669156074, 0.0, 0.0, 0.0], 'rewardMean': 0.8217789750482372, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1107.3382930798073
'totalSteps': 24320, 'rewardStep': 0.8801942993751365, 'errorList': [], 'lossList': [0.0, -1.1582763797044755, 0.0, 1.0655887157283723, 0.0, 0.0, 0.0], 'rewardMean': 0.8334015130554595, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.6121206307403
'totalSteps': 25600, 'rewardStep': 0.8884441129966891, 'errorList': [], 'lossList': [0.0, -1.1378309953212737, 0.0, 0.9097269817814231, 0.0, 0.0, 0.0], 'rewardMean': 0.8263530650985844, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.8657906235123
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=12800, timeSpent=107.52
