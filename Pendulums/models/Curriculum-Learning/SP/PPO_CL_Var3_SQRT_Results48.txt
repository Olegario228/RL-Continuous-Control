#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 48
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.8363151170405569, 'errorList': [], 'lossList': [0.0, -1.427483321428299, 0.0, 27.563102551698684, 0.0, 0.0, 0.0], 'rewardMean': 0.8676227664336487, 'totalEpisodes': 18, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.161715868604823
'totalSteps': 3840, 'rewardStep': 0.8631451196523288, 'errorList': [], 'lossList': [0.0, -1.4177250063419342, 0.0, 38.623039078712466, 0.0, 0.0, 0.0], 'rewardMean': 0.8661302175065421, 'totalEpisodes': 24, 'stepsPerEpisode': 225, 'rewardPerEpisode': 167.4304930240739
'totalSteps': 5120, 'rewardStep': 0.8253463919688119, 'errorList': [], 'lossList': [0.0, -1.4095005828142166, 0.0, 71.9628962135315, 0.0, 0.0, 0.0], 'rewardMean': 0.8559342611221095, 'totalEpisodes': 37, 'stepsPerEpisode': 77, 'rewardPerEpisode': 71.23073147288419
'totalSteps': 6400, 'rewardStep': 0.5244264551921188, 'errorList': [], 'lossList': [0.0, -1.4084226858615876, 0.0, 107.47224493026734, 0.0, 0.0, 0.0], 'rewardMean': 0.7896326999361113, 'totalEpisodes': 59, 'stepsPerEpisode': 17, 'rewardPerEpisode': 9.745099600813962
'totalSteps': 7680, 'rewardStep': 0.49158804127165157, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7044770831748371, 'totalEpisodes': 102, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.0084041671515689
'totalSteps': 8960, 'rewardStep': 0.574736437573127, 'errorList': [], 'lossList': [0.0, -1.4080358582735062, 0.0, 111.04038841247558, 0.0, 0.0, 0.0], 'rewardMean': 0.6882595024746233, 'totalEpisodes': 134, 'stepsPerEpisode': 41, 'rewardPerEpisode': 29.401621543610947
'totalSteps': 10240, 'rewardStep': 0.21122584315675824, 'errorList': [], 'lossList': [0.0, -1.4051784068346023, 0.0, 72.44719402313233, 0.0, 0.0, 0.0], 'rewardMean': 0.635255762550416, 'totalEpisodes': 154, 'stepsPerEpisode': 157, 'rewardPerEpisode': 117.57968174037508
'totalSteps': 11520, 'rewardStep': 0.34569459420393467, 'errorList': [], 'lossList': [0.0, -1.3973438274860381, 0.0, 47.899792451858524, 0.0, 0.0, 0.0], 'rewardMean': 0.606299645715768, 'totalEpisodes': 168, 'stepsPerEpisode': 194, 'rewardPerEpisode': 147.2547185062321
'totalSteps': 12800, 'rewardStep': 0.6488199263431333, 'errorList': [], 'lossList': [0.0, -1.3772739112377166, 0.0, 18.492811501026154, 0.0, 0.0, 0.0], 'rewardMean': 0.5812885967674073, 'totalEpisodes': 179, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.944925767306643
'totalSteps': 14080, 'rewardStep': 0.7652109026271635, 'errorList': [], 'lossList': [0.0, -1.35271204829216, 0.0, 18.33622166633606, 0.0, 0.0, 0.0], 'rewardMean': 0.574178175326068, 'totalEpisodes': 185, 'stepsPerEpisode': 103, 'rewardPerEpisode': 82.08249287562825
'totalSteps': 15360, 'rewardStep': 0.6980179623077585, 'errorList': [], 'lossList': [0.0, -1.3463231766223906, 0.0, 25.122002096176146, 0.0, 0.0, 0.0], 'rewardMean': 0.5576654595916108, 'totalEpisodes': 191, 'stepsPerEpisode': 115, 'rewardPerEpisode': 94.23946716708622
'totalSteps': 16640, 'rewardStep': 0.7035840281323775, 'errorList': [], 'lossList': [0.0, -1.355994234085083, 0.0, 7.763765501976013, 0.0, 0.0, 0.0], 'rewardMean': 0.5454892232079673, 'totalEpisodes': 195, 'stepsPerEpisode': 236, 'rewardPerEpisode': 190.27284296093416
'totalSteps': 17920, 'rewardStep': 0.9052972254663545, 'errorList': [], 'lossList': [0.0, -1.3579345345497131, 0.0, 6.894582998305559, 0.0, 0.0, 0.0], 'rewardMean': 0.5835763002353911, 'totalEpisodes': 197, 'stepsPerEpisode': 377, 'rewardPerEpisode': 329.1717319127755
'totalSteps': 19200, 'rewardStep': 0.8074799549311404, 'errorList': [], 'lossList': [0.0, -1.3487199050188066, 0.0, 5.594344634413719, 0.0, 0.0, 0.0], 'rewardMean': 0.6151654916013398, 'totalEpisodes': 198, 'stepsPerEpisode': 140, 'rewardPerEpisode': 119.48141697155998
'totalSteps': 20480, 'rewardStep': 0.753131483671276, 'errorList': [], 'lossList': [0.0, -1.323804610967636, 0.0, 4.842154112756252, 0.0, 0.0, 0.0], 'rewardMean': 0.6413198358413024, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.730657623801
'totalSteps': 21760, 'rewardStep': 0.8470611381744256, 'errorList': [], 'lossList': [0.0, -1.2926326084136963, 0.0, 3.841001724600792, 0.0, 0.0, 0.0], 'rewardMean': 0.6685523059014322, 'totalEpisodes': 200, 'stepsPerEpisode': 231, 'rewardPerEpisode': 206.57286888707853
'totalSteps': 23040, 'rewardStep': 0.9442472150567963, 'errorList': [0.08701183125270269, 0.18192727300692405, 0.20886547816250536, 0.0942671408028859, 0.1049515304531155, 0.12979426835925914, 0.15200874345448215, 0.15731552209252164, 0.12552011213693512, 0.1382598838134622, 0.09231070371614973, 0.1059719378589292, 0.14299127649111187, 0.21022196727018522, 0.09054761995969325, 0.1483259684062474, 0.1146947167641525, 0.20462988303941598, 0.11361909024467923, 0.08482518549252344, 0.1043122816566922, 0.192134075634509, 0.13599638199600295, 0.10319310207222236, 0.135690653845264, 0.09293667606577775, 0.0930269338482151, 0.08927452399479016, 0.2709391788310715, 0.1238675314484429, 0.15946285273322272, 0.14627660211756827, 0.14892138637558253, 0.11996083664289325, 0.22733416716770685, 0.1561044041063621, 0.1372223370394534, 0.09303781981378255, 0.13527724248278022, 0.12192367732644506, 0.15583595328525773, 0.10504854907194976, 0.12870700489314169, 0.1069182793551031, 0.24005321645773742, 0.12155256642044718, 0.15295738357724714, 0.17867249093052429, 0.23839435479003052, 0.11287653934729676], 'lossList': [0.0, -1.286910393834114, 0.0, 2.4971223163604734, 0.0, 0.0, 0.0], 'rewardMean': 0.741854443091436, 'totalEpisodes': 202, 'stepsPerEpisode': 18, 'rewardPerEpisode': 12.975108264641957, 'successfulTests': 43
'totalSteps': 24320, 'rewardStep': 0.6255503166683841, 'errorList': [], 'lossList': [0.0, -1.2657658016681672, 0.0, 1.7259568163752557, 0.0, 0.0, 0.0], 'rewardMean': 0.7698400153378809, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.227313695163
'totalSteps': 25600, 'rewardStep': 0.9914044571694318, 'errorList': [0.04802954907528844, 0.043271218836483, 0.04332403820886704, 0.04289828375343788, 0.04043340326214667, 0.042917305187610104, 0.05503580511699894, 0.03960021770294384, 0.042884717175962664, 0.03883523775572563, 0.05411449202147713, 0.043430307828687995, 0.050021228716540064, 0.048556861415831896, 0.040883035575664264, 0.039446696053887044, 0.04975637204059, 0.03865564257657065, 0.05000477926670703, 0.04771533006800358, 0.039761657758656345, 0.04107955850909664, 0.04378341507381386, 0.042236065405119295, 0.04334533931194382, 0.04744459759178244, 0.043203582469696446, 0.041370539580804146, 0.04095901176021062, 0.04122489513656561, 0.047636182762351535, 0.04585002422114076, 0.03873773201857834, 0.04359364455274507, 0.04279961205705451, 0.04935536835201716, 0.042778650246994315, 0.042903686432221144, 0.05234634416512999, 0.042380372164506006, 0.04164544427280743, 0.046878414366587066, 0.03927700547732936, 0.040377983320735535, 0.04841984370766448, 0.04358325530874955, 0.05274601359945678, 0.04215741232802337, 0.042303412679895014, 0.04386473023375058], 'lossList': [0.0, -1.246174420118332, 0.0, 1.2580963437259198, 0.0, 0.0, 0.0], 'rewardMean': 0.8040984684205108, 'totalEpisodes': 202, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.407388286165, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.84
