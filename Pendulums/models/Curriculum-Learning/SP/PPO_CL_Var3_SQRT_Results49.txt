#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 49
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.869573686712132, 'errorList': [], 'lossList': [0.0, -1.4269461137056352, 0.0, 27.54433783173561, 0.0, 0.0, 0.0], 'rewardMean': 0.8094322755057065, 'totalEpisodes': 13, 'stepsPerEpisode': 322, 'rewardPerEpisode': 249.35493116361513
'totalSteps': 3840, 'rewardStep': 0.576435592399069, 'errorList': [], 'lossList': [0.0, -1.4295980685949325, 0.0, 49.30650133132934, 0.0, 0.0, 0.0], 'rewardMean': 0.7317667144701607, 'totalEpisodes': 27, 'stepsPerEpisode': 182, 'rewardPerEpisode': 118.33335272190972
'totalSteps': 5120, 'rewardStep': 0.6159712688503994, 'errorList': [], 'lossList': [0.0, -1.4206715553998948, 0.0, 63.151534748077395, 0.0, 0.0, 0.0], 'rewardMean': 0.7028178530652204, 'totalEpisodes': 43, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.76904391591983
'totalSteps': 6400, 'rewardStep': 0.721406835934928, 'errorList': [], 'lossList': [0.0, -1.4171930891275406, 0.0, 84.91528331756592, 0.0, 0.0, 0.0], 'rewardMean': 0.7065356496391619, 'totalEpisodes': 62, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.722427335367843
'totalSteps': 7680, 'rewardStep': 0.917762046109042, 'errorList': [], 'lossList': [0.0, -1.4100882709026337, 0.0, 112.17528991699218, 0.0, 0.0, 0.0], 'rewardMean': 0.7417400490508085, 'totalEpisodes': 105, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.196636803587243
'totalSteps': 8960, 'rewardStep': 0.799254687691038, 'errorList': [], 'lossList': [0.0, -1.3995738357305527, 0.0, 73.19776933670045, 0.0, 0.0, 0.0], 'rewardMean': 0.7499564259994127, 'totalEpisodes': 135, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.923573100750538
'totalSteps': 10240, 'rewardStep': 0.986380899810099, 'errorList': [116.218603646629, 28.103960061406067, 108.3744997463109, 98.64229401536385, 163.65697185208575, 120.26418955435683, 113.35089683409197, 0.7222718350484597, 191.46337150734976, 87.09220387865369, 180.63114543363127, 198.6297310098176, 6.232447475765183, 9.706562420395718, 137.42527980688763, 143.3720902053053, 171.82667608684747, 146.40092216870752, 120.31542003163912, 148.26089732075482, 129.64326884923145, 134.25045911253403, 48.440293542681395, 155.20251487231909, 173.03816532451523, 80.40083655461113, 126.97807210031091, 12.777241865134117, 62.514699930702754, 8.474359595557697, 104.61324442435811, 26.602553933561566, 65.19694493523929, 47.779489759804825, 121.56109741521, 27.848393442008906, 130.14133301170077, 95.78780984110922, 87.43612155426787, 18.295249900864643, 61.431162529092546, 39.849360422213266, 140.3931385459906, 200.58361614132127, 150.43197771179067, 122.28822740964038, 110.17610128914671, 173.63320650233396, 94.44767481627241, 179.9725350525057], 'lossList': [0.0, -1.37549447953701, 0.0, 49.431172389984134, 0.0, 0.0, 0.0], 'rewardMean': 0.7795094852257486, 'totalEpisodes': 146, 'stepsPerEpisode': 130, 'rewardPerEpisode': 108.93170349783654, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.23076502236570667, 'errorList': [], 'lossList': [0.0, -1.3631289565563203, 0.0, 24.67096576690674, 0.0, 0.0, 0.0], 'rewardMean': 0.7185378782412994, 'totalEpisodes': 154, 'stepsPerEpisode': 156, 'rewardPerEpisode': 105.39607133979172
'totalSteps': 12800, 'rewardStep': 0.7504073605529306, 'errorList': [], 'lossList': [0.0, -1.365220084786415, 0.0, 34.604079160690304, 0.0, 0.0, 0.0], 'rewardMean': 0.7217248264724626, 'totalEpisodes': 161, 'stepsPerEpisode': 62, 'rewardPerEpisode': 47.15533069363789
'totalSteps': 14080, 'rewardStep': 0.8287896258694503, 'errorList': [], 'lossList': [0.0, -1.3412446171045302, 0.0, 14.662083373069763, 0.0, 0.0, 0.0], 'rewardMean': 0.7296747026294794, 'totalEpisodes': 165, 'stepsPerEpisode': 66, 'rewardPerEpisode': 48.06005579025196
'totalSteps': 15360, 'rewardStep': 0.4462912301305247, 'errorList': [], 'lossList': [0.0, -1.3052205103635788, 0.0, 44.93806074142456, 0.0, 0.0, 0.0], 'rewardMean': 0.6873464569713188, 'totalEpisodes': 174, 'stepsPerEpisode': 171, 'rewardPerEpisode': 130.1171314930612
'totalSteps': 16640, 'rewardStep': 0.08137890802941994, 'errorList': [], 'lossList': [0.0, -1.2831645023822784, 0.0, 9.590493594408036, 0.0, 0.0, 0.0], 'rewardMean': 0.6378407885343538, 'totalEpisodes': 178, 'stepsPerEpisode': 484, 'rewardPerEpisode': 341.104322411808
'totalSteps': 17920, 'rewardStep': 0.5639522343959547, 'errorList': [], 'lossList': [0.0, -1.264763557910919, 0.0, 24.874243335723875, 0.0, 0.0, 0.0], 'rewardMean': 0.6326388850889093, 'totalEpisodes': 185, 'stepsPerEpisode': 123, 'rewardPerEpisode': 103.27989832415638
'totalSteps': 19200, 'rewardStep': 0.7860602215472512, 'errorList': [], 'lossList': [0.0, -1.245561756491661, 0.0, 19.33634168982506, 0.0, 0.0, 0.0], 'rewardMean': 0.6391042236501416, 'totalEpisodes': 190, 'stepsPerEpisode': 232, 'rewardPerEpisode': 197.10066904230186
'totalSteps': 20480, 'rewardStep': 0.8420227569109271, 'errorList': [], 'lossList': [0.0, -1.2200128012895584, 0.0, 7.982570081949234, 0.0, 0.0, 0.0], 'rewardMean': 0.6315302947303302, 'totalEpisodes': 196, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.536738020618177
'totalSteps': 21760, 'rewardStep': 0.6274087502044413, 'errorList': [], 'lossList': [0.0, -1.2098076349496842, 0.0, 9.463208086490631, 0.0, 0.0, 0.0], 'rewardMean': 0.6143457009816705, 'totalEpisodes': 202, 'stepsPerEpisode': 73, 'rewardPerEpisode': 59.14372262335239
'totalSteps': 23040, 'rewardStep': 0.5609544376027997, 'errorList': [], 'lossList': [0.0, -1.202505219578743, 0.0, 26.866041049957275, 0.0, 0.0, 0.0], 'rewardMean': 0.5718030547609406, 'totalEpisodes': 207, 'stepsPerEpisode': 335, 'rewardPerEpisode': 256.62170983904895
'totalSteps': 24320, 'rewardStep': 0.5192913726657378, 'errorList': [], 'lossList': [0.0, -1.1891677016019822, 0.0, 5.1538445973396305, 0.0, 0.0, 0.0], 'rewardMean': 0.6006556897909437, 'totalEpisodes': 213, 'stepsPerEpisode': 163, 'rewardPerEpisode': 114.79541298541832
'totalSteps': 25600, 'rewardStep': 0.7535496469117108, 'errorList': [], 'lossList': [0.0, -1.1746575295925141, 0.0, 4.437245004177093, 0.0, 0.0, 0.0], 'rewardMean': 0.6009699184268217, 'totalEpisodes': 215, 'stepsPerEpisode': 618, 'rewardPerEpisode': 526.9317792529276
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=85.42
