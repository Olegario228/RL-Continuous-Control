#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 5
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.5521178905708464, 'errorList': [], 'lossList': [0.0, -1.4033599317073822, 0.0, 35.014117317199705, 0.0, 0.0, 0.0], 'rewardMean': 0.5813939947563485, 'totalEpisodes': 39, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.449186583282968
'totalSteps': 3840, 'rewardStep': 0.6331117307505061, 'errorList': [], 'lossList': [0.0, -1.3917430055141449, 0.0, 45.065896339416504, 0.0, 0.0, 0.0], 'rewardMean': 0.5986332400877343, 'totalEpisodes': 71, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.388020384040308
'totalSteps': 5120, 'rewardStep': 0.6747850701591502, 'errorList': [], 'lossList': [0.0, -1.3640105801820754, 0.0, 58.303230133056644, 0.0, 0.0, 0.0], 'rewardMean': 0.6176711976055883, 'totalEpisodes': 106, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.593117183531577
'totalSteps': 6400, 'rewardStep': 0.38558616588849853, 'errorList': [], 'lossList': [0.0, -1.346071066260338, 0.0, 58.81278123855591, 0.0, 0.0, 0.0], 'rewardMean': 0.5712541912621704, 'totalEpisodes': 141, 'stepsPerEpisode': 85, 'rewardPerEpisode': 57.27096273633342
'totalSteps': 7680, 'rewardStep': 0.7019748824108366, 'errorList': [], 'lossList': [0.0, -1.3368516385555267, 0.0, 36.933432521820066, 0.0, 0.0, 0.0], 'rewardMean': 0.5930409731202814, 'totalEpisodes': 157, 'stepsPerEpisode': 46, 'rewardPerEpisode': 32.86324238066616
'totalSteps': 8960, 'rewardStep': 0.4758475434258219, 'errorList': [], 'lossList': [0.0, -1.3300820469856263, 0.0, 16.738576714992522, 0.0, 0.0, 0.0], 'rewardMean': 0.5762990545925015, 'totalEpisodes': 165, 'stepsPerEpisode': 272, 'rewardPerEpisode': 214.65209335375937
'totalSteps': 10240, 'rewardStep': 0.5709435145731766, 'errorList': [], 'lossList': [0.0, -1.3196960228681565, 0.0, 20.97625426530838, 0.0, 0.0, 0.0], 'rewardMean': 0.5756296120900859, 'totalEpisodes': 170, 'stepsPerEpisode': 109, 'rewardPerEpisode': 91.22999708264616
'totalSteps': 11520, 'rewardStep': 0.5486181086271671, 'errorList': [], 'lossList': [0.0, -1.2900839281082153, 0.0, 32.722193405628204, 0.0, 0.0, 0.0], 'rewardMean': 0.5726283339275393, 'totalEpisodes': 176, 'stepsPerEpisode': 68, 'rewardPerEpisode': 51.43948540834201
'totalSteps': 12800, 'rewardStep': 0.7786189085992519, 'errorList': [], 'lossList': [0.0, -1.2585650885105133, 0.0, 11.051354365944862, 0.0, 0.0, 0.0], 'rewardMean': 0.5932273913947106, 'totalEpisodes': 177, 'stepsPerEpisode': 1180, 'rewardPerEpisode': 965.1367247655043
'totalSteps': 14080, 'rewardStep': 0.7906673032729327, 'errorList': [], 'lossList': [0.0, -1.2263995492458344, 0.0, 6.413583687841892, 0.0, 0.0, 0.0], 'rewardMean': 0.6112271118278187, 'totalEpisodes': 180, 'stepsPerEpisode': 75, 'rewardPerEpisode': 61.81824822056356
'totalSteps': 15360, 'rewardStep': 0.6904853408525036, 'errorList': [], 'lossList': [0.0, -1.2053596246242524, 0.0, 16.116845483779908, 0.0, 0.0, 0.0], 'rewardMean': 0.6250638568559845, 'totalEpisodes': 182, 'stepsPerEpisode': 214, 'rewardPerEpisode': 184.92990016538602
'totalSteps': 16640, 'rewardStep': 0.9600470162801201, 'errorList': [0.4024375222121373, 0.38194916017780467, 0.6605001517511492, 0.649053081807207, 0.4111067167810504, 0.6255166679298606, 0.6030358236955973, 0.4901569956787168, 0.29081975304002683, 0.5706947851181299, 0.42306464375825464, 0.4622367370572483, 0.5020757471733426, 0.683920713007855, 0.4203099756937893, 0.47122325380682134, 0.4709838855400191, 0.486118846748769, 0.4624389607597001, 0.4996986834232217, 0.44024556918476826, 0.4140102924375581, 0.4656777048954227, 0.5011668526845373, 0.5095107644378905, 0.5145640168259304, 0.4964647243365571, 0.5415418627767415, 0.48162507936918714, 0.46261604428426273, 0.524274222785391, 0.49532136211811373, 0.44559414101939, 0.4517550199040122, 0.41169304654213684, 0.6080210182490197, 0.6440552696466298, 0.5342647073936749, 0.6350342045760338, 0.6116099337122699, 0.5073950405273139, 0.5248597244115057, 0.47156179121715674, 0.554815158670249, 0.5483782284159104, 0.3566643799455006, 0.36007146387034816, 0.4303275039854515, 0.42136959910328553, 0.6213416453817748], 'lossList': [0.0, -1.2104750716686248, 0.0, 11.118296079933643, 0.0, 0.0, 0.0], 'rewardMean': 0.6577573854089459, 'totalEpisodes': 183, 'stepsPerEpisode': 534, 'rewardPerEpisode': 457.30155465963065, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.6765181736947488, 'errorList': [], 'lossList': [0.0, -1.2178833323717118, 0.0, 2.135431168526411, 0.0, 0.0, 0.0], 'rewardMean': 0.6579306957625057, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.856989235934
'totalSteps': 19200, 'rewardStep': 0.9132912713000022, 'errorList': [], 'lossList': [0.0, -1.2085521936416626, 0.0, 1.4842976442724467, 0.0, 0.0, 0.0], 'rewardMean': 0.7107012063036562, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.2158183316667
'totalSteps': 20480, 'rewardStep': 0.8951587120974425, 'errorList': [], 'lossList': [0.0, -1.1816438406705856, 0.0, 1.3980079805105925, 0.0, 0.0, 0.0], 'rewardMean': 0.7300195892723167, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.238174485287
'totalSteps': 21760, 'rewardStep': 0.9625247892872879, 'errorList': [0.04245461468829155, 0.0949306159180748, 0.04123257885341448, 0.10052492526021087, 0.1510712915216949, 0.13400637647544608, 0.04456903077806456, 0.04100441048014889, 0.047471452083506435, 0.11086893550521017, 0.0478053959662925, 0.048634571832301896, 0.08184814953838988, 0.10766418031947453, 0.04059343751157885, 0.10694459130728329, 0.10267579858725702, 0.04208187486025261, 0.05122643597709736, 0.04228733116747503, 0.17789774725364907, 0.07732640093736048, 0.11342105787285728, 0.04791363864048324, 0.11386763916885315, 0.044096017117742095, 0.04842869685162087, 0.06076471691379927, 0.11442258663722109, 0.13831196528899395, 0.09764935984765857, 0.04570211131096661, 0.059061968142075955, 0.08171265264659558, 0.10465158425095376, 0.07886384753097862, 0.1495238545936136, 0.05121917529076427, 0.03976584904314953, 0.08444878553067899, 0.044702771166482955, 0.06164214070741139, 0.1309334990799061, 0.05636609872443003, 0.18614287115154804, 0.044364306389078956, 0.040064270783562776, 0.08682463182371904, 0.0556387301736355, 0.12240954414513137], 'lossList': [0.0, -1.152062863111496, 0.0, 1.0928774830326438, 0.0, 0.0, 0.0], 'rewardMean': 0.7786873138584633, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.803193680746, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=90.29
