#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 50
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.766347794997562, 'errorList': [], 'lossList': [0.0, -1.4083492052555084, 0.0, 31.69449137687683, 0.0, 0.0, 0.0], 'rewardMean': 0.6052852679865399, 'totalEpisodes': 77, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.398457067212881
'totalSteps': 3840, 'rewardStep': 0.5170543105073754, 'errorList': [], 'lossList': [0.0, -1.3949431657791138, 0.0, 41.16216746330261, 0.0, 0.0, 0.0], 'rewardMean': 0.5758749488268183, 'totalEpisodes': 116, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.6099444054033114
'totalSteps': 5120, 'rewardStep': 0.6335117549698962, 'errorList': [], 'lossList': [0.0, -1.379421234726906, 0.0, 51.93755075454712, 0.0, 0.0, 0.0], 'rewardMean': 0.5902841503625879, 'totalEpisodes': 136, 'stepsPerEpisode': 163, 'rewardPerEpisode': 113.53947887970165
'totalSteps': 6400, 'rewardStep': 0.6645383643814018, 'errorList': [], 'lossList': [0.0, -1.3721139359474182, 0.0, 74.6710329246521, 0.0, 0.0, 0.0], 'rewardMean': 0.6051349931663507, 'totalEpisodes': 157, 'stepsPerEpisode': 27, 'rewardPerEpisode': 16.221912375576835
'totalSteps': 7680, 'rewardStep': 0.5596867634228491, 'errorList': [], 'lossList': [0.0, -1.3751987808942794, 0.0, 78.61158662796021, 0.0, 0.0, 0.0], 'rewardMean': 0.5975602882091003, 'totalEpisodes': 180, 'stepsPerEpisode': 46, 'rewardPerEpisode': 37.11984136498069
'totalSteps': 8960, 'rewardStep': 0.9292520679443854, 'errorList': [], 'lossList': [0.0, -1.360562371611595, 0.0, 69.70813146591186, 0.0, 0.0, 0.0], 'rewardMean': 0.644944828171284, 'totalEpisodes': 200, 'stepsPerEpisode': 51, 'rewardPerEpisode': 45.80349362803568
'totalSteps': 10240, 'rewardStep': 0.7639998643524042, 'errorList': [], 'lossList': [0.0, -1.353767421245575, 0.0, 31.33090391635895, 0.0, 0.0, 0.0], 'rewardMean': 0.6598267076939239, 'totalEpisodes': 213, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.078900647479465
'totalSteps': 11520, 'rewardStep': 0.737234202769677, 'errorList': [], 'lossList': [0.0, -1.342526779770851, 0.0, 29.200338125228882, 0.0, 0.0, 0.0], 'rewardMean': 0.6684275404801188, 'totalEpisodes': 223, 'stepsPerEpisode': 67, 'rewardPerEpisode': 59.370673356262905
'totalSteps': 12800, 'rewardStep': 0.8162587666802501, 'errorList': [], 'lossList': [0.0, -1.316685026884079, 0.0, 11.247340952157975, 0.0, 0.0, 0.0], 'rewardMean': 0.6832106631001319, 'totalEpisodes': 230, 'stepsPerEpisode': 42, 'rewardPerEpisode': 34.17314288080183
'totalSteps': 14080, 'rewardStep': 0.7194249727706644, 'errorList': [], 'lossList': [0.0, -1.2986437410116196, 0.0, 9.127805217504502, 0.0, 0.0, 0.0], 'rewardMean': 0.7107308862796466, 'totalEpisodes': 233, 'stepsPerEpisode': 215, 'rewardPerEpisode': 169.8484034361662
'totalSteps': 15360, 'rewardStep': 0.4322245673575722, 'errorList': [], 'lossList': [0.0, -1.2911966443061829, 0.0, 5.806093369722366, 0.0, 0.0, 0.0], 'rewardMean': 0.6773185635156476, 'totalEpisodes': 236, 'stepsPerEpisode': 257, 'rewardPerEpisode': 189.63530388275225
'totalSteps': 16640, 'rewardStep': 0.8682528185929237, 'errorList': [], 'lossList': [0.0, -1.2900886899232864, 0.0, 4.888106361031532, 0.0, 0.0, 0.0], 'rewardMean': 0.7124384143242024, 'totalEpisodes': 237, 'stepsPerEpisode': 1039, 'rewardPerEpisode': 751.714352764944
'totalSteps': 17920, 'rewardStep': 0.8926696850087604, 'errorList': [], 'lossList': [0.0, -1.2759637814760207, 0.0, 3.449877702891827, 0.0, 0.0, 0.0], 'rewardMean': 0.7383542073280889, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.0511753064724
'totalSteps': 19200, 'rewardStep': 0.8679433929232655, 'errorList': [], 'lossList': [0.0, -1.2442267352342606, 0.0, 3.6605366748571395, 0.0, 0.0, 0.0], 'rewardMean': 0.7586947101822752, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.98696258602
'totalSteps': 20480, 'rewardStep': 0.9485016587343332, 'errorList': [0.1171569662518936, 0.14266243337604148, 0.1441686238682145, 0.14901958920271108, 0.24904425778168823, 0.13222061823301495, 0.16150760417878085, 0.16715894613037405, 0.16280237301857614, 0.11349414015549328, 0.1439385891590196, 0.2245911404442413, 0.14917804826538536, 0.18614479167696596, 0.13728291413760466, 0.09416830832147102, 0.1413362635227381, 0.16788458240052961, 0.1754324188858561, 0.11123058448161653, 0.09919474356369268, 0.12377120205827163, 0.2098282009078942, 0.16077475999639265, 0.15802122976642727, 0.17223432333544397, 0.16296816661992108, 0.1492655449222405, 0.14089433933238302, 0.19387311057068154, 0.18796918736272605, 0.09961287104009536, 0.2560559123362884, 0.09435960585670577, 0.17859596977256897, 0.18035941738388322, 0.17123586081845563, 0.19817357915057726, 0.25750444428216357, 0.1855090642864034, 0.13703155913270618, 0.14194133540272721, 0.1272204440183417, 0.3135450317062786, 0.13297056247762187, 0.17515142137717374, 0.11080390142099017, 0.15920980769024545, 0.10888236947689166, 0.1153047750826485], 'lossList': [0.0, -1.2272662991285324, 0.0, 3.189805489182472, 0.0, 0.0, 0.0], 'rewardMean': 0.7975761997134236, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.6070902969066, 'successfulTests': 44
'totalSteps': 21760, 'rewardStep': 0.9718938479637594, 'errorList': [0.15787673174669778, 0.12339463174473711, 0.15519179867214053, 0.1930399347943378, 0.20934497465442412, 0.21639965735789787, 0.1495670542890619, 0.16775706723941794, 0.14279844165329658, 0.14500404459985475, 0.1476228483555974, 0.1466437523992812, 0.19275448593242606, 0.1714675001913369, 0.1634146156932855, 0.2645684251151756, 0.12245017665512273, 0.16458776614293938, 0.13864980514988443, 0.14938394484901402, 0.27927153847691977, 0.17997778015508623, 0.1976391278923954, 0.1956168180057598, 0.12201008795113166, 0.15840821194698443, 0.15250237309819778, 0.13126173283584786, 0.23273464530330595, 0.21708859403637631, 0.15495929328744487, 0.153199949685212, 0.13397927576372368, 0.13507785650914286, 0.1759835356737473, 0.1958062056922078, 0.20668683693091586, 0.13672266552646709, 0.16671620113733818, 0.18278801055036692, 0.15725729072415917, 0.17546756834958394, 0.20862140065744994, 0.17886216614490325, 0.3454998936167759, 0.1601181357693081, 0.15462809010291698, 0.17660867138732725, 0.13473966657920966, 0.21374584700045704], 'lossList': [0.0, -1.2276329135894775, 0.0, 1.7767678203433752, 0.0, 0.0, 0.0], 'rewardMean': 0.801840377715361, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.544842744119, 'successfulTests': 40
'totalSteps': 23040, 'rewardStep': 0.8769133268961373, 'errorList': [], 'lossList': [0.0, -1.2137878721952438, 0.0, 1.2785917011648416, 0.0, 0.0, 0.0], 'rewardMean': 0.8131317239697344, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.6619140800797
'totalSteps': 24320, 'rewardStep': 0.7495071233209623, 'errorList': [], 'lossList': [0.0, -1.1880487614870072, 0.0, 0.6787272967398167, 0.0, 0.0, 0.0], 'rewardMean': 0.8143590160248628, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.375610796638
'totalSteps': 25600, 'rewardStep': 0.9560789537339363, 'errorList': [0.13679211145803402, 0.05447417072303137, 0.053790412161634306, 0.14435420497692678, 0.0853812613694022, 0.057631023079677685, 0.12385626941969485, 0.0851329096843586, 0.058388347297995906, 0.05984381278133615, 0.05946384988121837, 0.05752267848080065, 0.07018236521362357, 0.10594477316232327, 0.05428129317515117, 0.12686648334166412, 0.08656325143191655, 0.058958450186497306, 0.17076567624211794, 0.06731444669698963, 0.10283982479939834, 0.059100252615257066, 0.2346549396885053, 0.08858641649434641, 0.12457703072025986, 0.2227636515180953, 0.10314086724251026, 0.11056246909090556, 0.08571071195382679, 0.11609553927503964, 0.1381650064827802, 0.10395964809133515, 0.06500152601912325, 0.0641529298876409, 0.1456957678455998, 0.1064342578245291, 0.09088207703893998, 0.11640900637469576, 0.08730348839952284, 0.1395643221164418, 0.07693989501153324, 0.07164969753238273, 0.12376080086358865, 0.1637301815747953, 0.16176873076161727, 0.15965146041734174, 0.08814824440120797, 0.08045551127930148, 0.10462873791251043, 0.09357135803120013], 'lossList': [0.0, -1.1548834174871445, 0.0, 0.8778699356503785, 0.0, 0.0, 0.0], 'rewardMean': 0.8283410347302315, 'totalEpisodes': 237, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1195.3384295819321, 'successfulTests': 48
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=25600, timeSpent=128.33
