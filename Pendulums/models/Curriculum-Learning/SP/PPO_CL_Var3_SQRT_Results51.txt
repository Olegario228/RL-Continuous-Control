#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 51
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5109444451136023, 'errorList': [], 'lossList': [0.0, -1.4151530134677888, 0.0, 33.42661015510559, 0.0, 0.0, 0.0], 'rewardMean': 0.6629972889775049, 'totalEpisodes': 65, 'stepsPerEpisode': 39, 'rewardPerEpisode': 28.72686334843794
'totalSteps': 3840, 'rewardStep': 0.7369817023455003, 'errorList': [], 'lossList': [0.0, -1.4021179044246674, 0.0, 52.06112760543823, 0.0, 0.0, 0.0], 'rewardMean': 0.6876587601001699, 'totalEpisodes': 104, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.98809253477523
'totalSteps': 5120, 'rewardStep': 0.8739831479692851, 'errorList': [], 'lossList': [0.0, -1.3896938705444335, 0.0, 60.73898509979248, 0.0, 0.0, 0.0], 'rewardMean': 0.7342398570674488, 'totalEpisodes': 126, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.6696320118935803
'totalSteps': 6400, 'rewardStep': 0.5029297890805564, 'errorList': [], 'lossList': [0.0, -1.379156438112259, 0.0, 47.547026510238645, 0.0, 0.0, 0.0], 'rewardMean': 0.6879778434700703, 'totalEpisodes': 134, 'stepsPerEpisode': 87, 'rewardPerEpisode': 66.80519848072815
'totalSteps': 7680, 'rewardStep': 0.5603582435302302, 'errorList': [], 'lossList': [0.0, -1.3736045145988465, 0.0, 44.53373354911804, 0.0, 0.0, 0.0], 'rewardMean': 0.6667079101467636, 'totalEpisodes': 145, 'stepsPerEpisode': 55, 'rewardPerEpisode': 40.21109391191981
'totalSteps': 8960, 'rewardStep': 0.6137754877697621, 'errorList': [], 'lossList': [0.0, -1.368675661087036, 0.0, 46.294548683166504, 0.0, 0.0, 0.0], 'rewardMean': 0.6591461355214776, 'totalEpisodes': 156, 'stepsPerEpisode': 110, 'rewardPerEpisode': 83.0093332860828
'totalSteps': 10240, 'rewardStep': 0.8187174736107957, 'errorList': [], 'lossList': [0.0, -1.3738588577508926, 0.0, 17.677671926021574, 0.0, 0.0, 0.0], 'rewardMean': 0.6790925527826424, 'totalEpisodes': 163, 'stepsPerEpisode': 55, 'rewardPerEpisode': 42.87982842467436
'totalSteps': 11520, 'rewardStep': 0.7648190415579176, 'errorList': [], 'lossList': [0.0, -1.3836579179763795, 0.0, 15.86434877872467, 0.0, 0.0, 0.0], 'rewardMean': 0.6886177182021174, 'totalEpisodes': 168, 'stepsPerEpisode': 111, 'rewardPerEpisode': 99.03661639476843
'totalSteps': 12800, 'rewardStep': 0.8601536374474003, 'errorList': [], 'lossList': [0.0, -1.3764241933822632, 0.0, 9.262486420869827, 0.0, 0.0, 0.0], 'rewardMean': 0.7057713101266456, 'totalEpisodes': 174, 'stepsPerEpisode': 42, 'rewardPerEpisode': 38.07632882706328
'totalSteps': 14080, 'rewardStep': 0.7743416162550857, 'errorList': [], 'lossList': [0.0, -1.3659192323684692, 0.0, 5.501924477815628, 0.0, 0.0, 0.0], 'rewardMean': 0.7017004584680137, 'totalEpisodes': 179, 'stepsPerEpisode': 146, 'rewardPerEpisode': 120.54632095500408
'totalSteps': 15360, 'rewardStep': 0.35196177646751475, 'errorList': [], 'lossList': [0.0, -1.3833264118433, 0.0, 6.391569683551788, 0.0, 0.0, 0.0], 'rewardMean': 0.6858021916034047, 'totalEpisodes': 183, 'stepsPerEpisode': 206, 'rewardPerEpisode': 137.2712116528302
'totalSteps': 16640, 'rewardStep': 0.8307495524818819, 'errorList': [], 'lossList': [0.0, -1.3799343556165695, 0.0, 6.938500595092774, 0.0, 0.0, 0.0], 'rewardMean': 0.6951789766170429, 'totalEpisodes': 186, 'stepsPerEpisode': 171, 'rewardPerEpisode': 144.3080971158542
'totalSteps': 17920, 'rewardStep': 0.8055542528036652, 'errorList': [], 'lossList': [0.0, -1.3537144911289216, 0.0, 3.4886913523077965, 0.0, 0.0, 0.0], 'rewardMean': 0.6883360871004809, 'totalEpisodes': 186, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 994.6975926859161
'totalSteps': 19200, 'rewardStep': 0.8053418671072712, 'errorList': [], 'lossList': [0.0, -1.3267702156305312, 0.0, 3.3057949435710907, 0.0, 0.0, 0.0], 'rewardMean': 0.7185772949031526, 'totalEpisodes': 186, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.0135961102228
'totalSteps': 20480, 'rewardStep': 0.7506244535425536, 'errorList': [], 'lossList': [0.0, -1.2803820967674255, 0.0, 2.0372878873348235, 0.0, 0.0, 0.0], 'rewardMean': 0.7376039159043848, 'totalEpisodes': 186, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.696847135561
'totalSteps': 21760, 'rewardStep': 0.7616546961935106, 'errorList': [], 'lossList': [0.0, -1.2203953897953033, 0.0, 1.2301641510426997, 0.0, 0.0, 0.0], 'rewardMean': 0.7523918367467598, 'totalEpisodes': 186, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.0917053103901
'totalSteps': 23040, 'rewardStep': 0.9488415210634159, 'errorList': [0.04914364961959364, 0.042472356687194165, 0.03847743895729352, 0.04120105144149049, 0.06076259080496244, 0.04203396389236072, 0.04116958939476022, 0.05196569143798445, 0.03270915282614412, 0.04731141525988987, 0.03567006551627166, 0.032580374934401635, 0.04030316814314042, 0.03919053849015009, 0.03839730630106116, 0.04078235469126474, 0.05207408113746692, 0.03389210983587915, 0.040345252169280484, 0.04125323658452426, 0.05698898745785886, 0.040663628679741166, 0.0343146114553773, 0.04976540986278301, 0.04165582696280116, 0.03183258705811967, 0.052320793856947864, 0.030707057719915037, 0.032951144504069434, 0.03817587404208393, 0.03521911341069519, 0.03175162904282759, 0.05385174791872428, 0.05040748338708857, 0.03272353142457323, 0.031288929193812834, 0.036048629303702784, 0.035337249864458804, 0.04722937822456429, 0.03459652361580187, 0.040404323566943165, 0.051019318885532655, 0.05201827596873056, 0.07077953292363023, 0.03371710451047677, 0.04421552429283549, 0.03310067766603155, 0.03619415742512936, 0.03188664577060953, 0.03390715676035276], 'lossList': [0.0, -1.181413278579712, 0.0, 1.126392646767199, 0.0, 0.0, 0.0], 'rewardMean': 0.7654042414920217, 'totalEpisodes': 186, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.16200472117, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=78.97
