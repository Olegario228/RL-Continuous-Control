#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 52
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.7454563145651124, 'errorList': [], 'lossList': [0.0, -1.4258107006549836, 0.0, 34.030425243377685, 0.0, 0.0, 0.0], 'rewardMean': 0.780546828174411, 'totalEpisodes': 82, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.433714465540551
'totalSteps': 3840, 'rewardStep': 0.545758073251441, 'errorList': [], 'lossList': [0.0, -1.4112727385759354, 0.0, 52.21052074432373, 0.0, 0.0, 0.0], 'rewardMean': 0.7022839098667544, 'totalEpisodes': 118, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.705999311652763
'totalSteps': 5120, 'rewardStep': 0.6637923929309728, 'errorList': [], 'lossList': [0.0, -1.3801751512289047, 0.0, 61.16286750793457, 0.0, 0.0, 0.0], 'rewardMean': 0.6926610306328089, 'totalEpisodes': 146, 'stepsPerEpisode': 32, 'rewardPerEpisode': 26.90933288223126
'totalSteps': 6400, 'rewardStep': 0.7278203461468433, 'errorList': [], 'lossList': [0.0, -1.3673346561193467, 0.0, 66.76473888397217, 0.0, 0.0, 0.0], 'rewardMean': 0.6996928937356157, 'totalEpisodes': 169, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.003790496512813
'totalSteps': 7680, 'rewardStep': 0.7967094936165132, 'errorList': [], 'lossList': [0.0, -1.3583912539482117, 0.0, 61.73780792236328, 0.0, 0.0, 0.0], 'rewardMean': 0.7158623270490986, 'totalEpisodes': 184, 'stepsPerEpisode': 41, 'rewardPerEpisode': 30.09901201282889
'totalSteps': 8960, 'rewardStep': 0.46005509764318153, 'errorList': [], 'lossList': [0.0, -1.3507541251182555, 0.0, 50.007837562561036, 0.0, 0.0, 0.0], 'rewardMean': 0.6793184371339676, 'totalEpisodes': 193, 'stepsPerEpisode': 79, 'rewardPerEpisode': 51.593607135295315
'totalSteps': 10240, 'rewardStep': 0.940332719035139, 'errorList': [95.57934071316235, 76.71564425018057, 70.13225839519991, 117.54843994094198, 61.1890704426854, 135.5844435687451, 141.88285052000518, 129.8427160579911, 127.94536631068053, 132.55439905743725, 137.268315527427, 95.58056928016154, 129.8584640214193, 157.02800112192492, 152.51040124041558, 153.91574402660837, 16.906832370009, 54.451147197517145, 45.10304462729694, 63.13544951952984, 33.51936065036975, 53.909880546460876, 95.47441062845807, 120.90534612857486, 105.7393493854383, 27.88677039646386, 86.80038610656393, 82.53369363842478, 76.11936107664492, 97.58517620305881, 110.09742855216419, 164.55683213902697, 150.66923780306908, 148.486541563278, 118.07445797381503, 86.3837071466176, 98.16715448954419, 105.6122196196232, 166.69042424034512, 118.05036627242559, 158.11976766709873, 122.11413699528428, 50.84446797412975, 19.78081892744787, 47.267023181321214, 77.71140900156931, 80.65898945837786, 119.23292769324905, 111.919363388737, 136.8987190623879], 'lossList': [0.0, -1.343131538629532, 0.0, 47.22257977485657, 0.0, 0.0, 0.0], 'rewardMean': 0.7119452223716141, 'totalEpisodes': 202, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.503570094544678, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9308242596727231, 'errorList': [22.371510150588612, 69.99773223780437, 91.75352063355773, 124.69214348484849, 134.4626884185309, 66.16552135390671, 89.55780181497404, 21.675302435132117, 122.92234493907357, 53.15435519737302, 51.18315498258672, 90.88128944644868, 59.022817411203185, 115.89549526324319, 70.16387363074907, 144.42057135984015, 71.53214241932466, 145.74462906021003, 91.83096072949108, 36.83823044916081, 33.808358901908946, 98.67518589190598, 30.364264067230504, 133.17911659560045, 43.796885352120995, 95.92692752708791, 36.11319711948951, 99.10225421493648, 111.76685317620581, 80.93692367303935, 7.658639068708961, 114.20941049068297, 53.38128448265965, 44.70570603310509, 94.44872626291233, 148.26599677829486, 168.02935231753887, 48.40125807564864, 80.02323202140828, 129.62292980812825, 117.92908293343214, 79.18301311959678, 79.99191184816898, 80.89796959669044, 87.96763859276173, 40.866166588381304, 113.77847309875656, 107.12656333598817, 135.72504322091604, 113.65360729796139], 'lossList': [0.0, -1.3497440862655639, 0.0, 48.43563964366913, 0.0, 0.0, 0.0], 'rewardMean': 0.7362651154050706, 'totalEpisodes': 208, 'stepsPerEpisode': 110, 'rewardPerEpisode': 94.83055462161886, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.7707971896016494, 'errorList': [], 'lossList': [0.0, -1.3560323530435563, 0.0, 40.538285803794864, 0.0, 0.0, 0.0], 'rewardMean': 0.7397183228247284, 'totalEpisodes': 218, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.55859667562206
'totalSteps': 14080, 'rewardStep': 0.7153737732947721, 'errorList': [], 'lossList': [0.0, -1.3626841616630554, 0.0, 8.5422965747118, 0.0, 0.0, 0.0], 'rewardMean': 0.7296919659758349, 'totalEpisodes': 224, 'stepsPerEpisode': 153, 'rewardPerEpisode': 127.89529226758654
'totalSteps': 15360, 'rewardStep': 0.5816125135728523, 'errorList': [], 'lossList': [0.0, -1.3831778109073638, 0.0, 15.835859208106994, 0.0, 0.0, 0.0], 'rewardMean': 0.7133075858766088, 'totalEpisodes': 228, 'stepsPerEpisode': 791, 'rewardPerEpisode': 527.6362200853899
'totalSteps': 16640, 'rewardStep': 0.8506643354891562, 'errorList': [], 'lossList': [0.0, -1.3717943966388702, 0.0, 6.118307722210884, 0.0, 0.0, 0.0], 'rewardMean': 0.7437982121003804, 'totalEpisodes': 229, 'stepsPerEpisode': 1142, 'rewardPerEpisode': 838.0945221843648
'totalSteps': 17920, 'rewardStep': 0.8206236424235339, 'errorList': [], 'lossList': [0.0, -1.3424398440122605, 0.0, 3.84656331717968, 0.0, 0.0, 0.0], 'rewardMean': 0.7594813370496365, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1034.6474001563456
'totalSteps': 19200, 'rewardStep': 0.8928346917471012, 'errorList': [], 'lossList': [0.0, -1.298271094560623, 0.0, 3.3580314856767655, 0.0, 0.0, 0.0], 'rewardMean': 0.7759827716096622, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1109.617232743861
'totalSteps': 20480, 'rewardStep': 0.9841708511198766, 'errorList': [0.03512207227426754, 0.0726341653814565, 0.0356482089139878, 0.04172706540849003, 0.03308085137456724, 0.08603732073290127, 0.041073375168259874, 0.03787530414977499, 0.04258959079272952, 0.04899815117738748, 0.03595601254620137, 0.07039947246187343, 0.06045538333105654, 0.042534260004848855, 0.09631438713938818, 0.04868056492945118, 0.04721362322221269, 0.0538808389119851, 0.035756174168192635, 0.03894054911704489, 0.07808303136361341, 0.13377826683307065, 0.035418469746296095, 0.0710879163215062, 0.047235194038611476, 0.0607128225686347, 0.04113756469868478, 0.04389135956443586, 0.13757309706210116, 0.045626164768295695, 0.11368244114506536, 0.04451829871570479, 0.045666955203816174, 0.038523830378519315, 0.040037320442254734, 0.0774738010503461, 0.11874075177810166, 0.07024304462988491, 0.04282477215298194, 0.1052768022993818, 0.04248256629323751, 0.04958218488389283, 0.1002732697462765, 0.0421126983075698, 0.04140655312893329, 0.08148053180125048, 0.05210052294306277, 0.13035412374233096, 0.06698431999255247, 0.03549832941068876], 'lossList': [0.0, -1.2490387850999831, 0.0, 3.478018559962511, 0.0, 0.0, 0.0], 'rewardMean': 0.7947289073599986, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1177.0050481351816, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=115.37
