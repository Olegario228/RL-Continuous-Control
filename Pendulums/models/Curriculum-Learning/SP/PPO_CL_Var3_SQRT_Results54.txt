#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 54
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.720926193969028, 'errorList': [], 'lossList': [0.0, -1.4106069046258927, 0.0, 34.65955374717712, 0.0, 0.0, 0.0], 'rewardMean': 0.8210081640602176, 'totalEpisodes': 83, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.72123955624827
'totalSteps': 3840, 'rewardStep': 0.7876103116952426, 'errorList': [], 'lossList': [0.0, -1.4046514803171157, 0.0, 46.5116252708435, 0.0, 0.0, 0.0], 'rewardMean': 0.8098755466052259, 'totalEpisodes': 118, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.272463777514828
'totalSteps': 5120, 'rewardStep': 0.7697774700899942, 'errorList': [], 'lossList': [0.0, -1.386805363893509, 0.0, 53.88383720397949, 0.0, 0.0, 0.0], 'rewardMean': 0.7998510274764179, 'totalEpisodes': 141, 'stepsPerEpisode': 46, 'rewardPerEpisode': 34.42854767906956
'totalSteps': 6400, 'rewardStep': 0.6303548515601848, 'errorList': [], 'lossList': [0.0, -1.3751129627227783, 0.0, 60.927862339019775, 0.0, 0.0, 0.0], 'rewardMean': 0.7659517922931712, 'totalEpisodes': 154, 'stepsPerEpisode': 29, 'rewardPerEpisode': 20.879556091392296
'totalSteps': 7680, 'rewardStep': 0.5837813646383956, 'errorList': [], 'lossList': [0.0, -1.3790139251947402, 0.0, 71.42983715057373, 0.0, 0.0, 0.0], 'rewardMean': 0.7355900543507087, 'totalEpisodes': 167, 'stepsPerEpisode': 31, 'rewardPerEpisode': 21.174149866641365
'totalSteps': 8960, 'rewardStep': 0.3694916236678931, 'errorList': [], 'lossList': [0.0, -1.3759682041406631, 0.0, 86.08903591156006, 0.0, 0.0, 0.0], 'rewardMean': 0.6832902785388779, 'totalEpisodes': 181, 'stepsPerEpisode': 207, 'rewardPerEpisode': 151.17852100385917
'totalSteps': 10240, 'rewardStep': 0.42270861427614376, 'errorList': [], 'lossList': [0.0, -1.3652134203910828, 0.0, 41.73516324520111, 0.0, 0.0, 0.0], 'rewardMean': 0.6507175705060362, 'totalEpisodes': 187, 'stepsPerEpisode': 156, 'rewardPerEpisode': 106.84905282181975
'totalSteps': 11520, 'rewardStep': 0.5803330541591838, 'errorList': [], 'lossList': [0.0, -1.3498392301797866, 0.0, 24.436698884963988, 0.0, 0.0, 0.0], 'rewardMean': 0.6428970686897192, 'totalEpisodes': 191, 'stepsPerEpisode': 380, 'rewardPerEpisode': 263.1849565083378
'totalSteps': 12800, 'rewardStep': 0.9136149840148798, 'errorList': [], 'lossList': [0.0, -1.3333886164426803, 0.0, 30.765665135383607, 0.0, 0.0, 0.0], 'rewardMean': 0.6699688602222353, 'totalEpisodes': 197, 'stepsPerEpisode': 53, 'rewardPerEpisode': 49.36986641355367
'totalSteps': 14080, 'rewardStep': 0.639470520020528, 'errorList': [], 'lossList': [0.0, -1.3082043784856796, 0.0, 13.344583076238631, 0.0, 0.0, 0.0], 'rewardMean': 0.6418068988091474, 'totalEpisodes': 200, 'stepsPerEpisode': 172, 'rewardPerEpisode': 133.34259447161946
'totalSteps': 15360, 'rewardStep': 0.9042587509580653, 'errorList': [], 'lossList': [0.0, -1.293134982585907, 0.0, 10.634699399471282, 0.0, 0.0, 0.0], 'rewardMean': 0.6601401545080512, 'totalEpisodes': 203, 'stepsPerEpisode': 71, 'rewardPerEpisode': 66.35723879963862
'totalSteps': 16640, 'rewardStep': 0.5608679880704441, 'errorList': [], 'lossList': [0.0, -1.287952577471733, 0.0, 5.07915109872818, 0.0, 0.0, 0.0], 'rewardMean': 0.6374659221455713, 'totalEpisodes': 208, 'stepsPerEpisode': 80, 'rewardPerEpisode': 69.30274055446398
'totalSteps': 17920, 'rewardStep': 0.8082894003680011, 'errorList': [], 'lossList': [0.0, -1.2863188344240188, 0.0, 6.808160011768341, 0.0, 0.0, 0.0], 'rewardMean': 0.6413171151733719, 'totalEpisodes': 210, 'stepsPerEpisode': 452, 'rewardPerEpisode': 347.6884283688172
'totalSteps': 19200, 'rewardStep': 0.7708784890043356, 'errorList': [], 'lossList': [0.0, -1.2558946794271468, 0.0, 3.1188193961977957, 0.0, 0.0, 0.0], 'rewardMean': 0.6553694789177871, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 989.8522125676633
'totalSteps': 20480, 'rewardStep': 0.9329131481049909, 'errorList': [0.4367798624164592, 0.41691132537526615, 0.44450573369231383, 0.44071128295777007, 0.49558925844724094, 0.42581037938753014, 0.4323971759929229, 0.4264805521659775, 0.4251452367765658, 0.4260690203848234, 0.437624007259296, 0.4910684312777647, 0.430345785915628, 0.42271562253732964, 0.42990541188738396, 0.41290792430728235, 0.42619530784324616, 0.41863974831201545, 0.4334943248838092, 0.42980531964465857, 0.4171416843274242, 0.4137078183073848, 0.41040960026577283, 0.4473078825696473, 0.43348250872413635, 0.4096717031403536, 0.43177412239814644, 0.40913708757379663, 0.42545477072415716, 0.42865059201593975, 0.4165694012993458, 0.4336495831757836, 0.4544613331061311, 0.43027941117776997, 0.4496963403398899, 0.42334944670434527, 0.44345067258043147, 0.48567653695747937, 0.43012524327035, 0.4146201820810549, 0.4233231717044116, 0.42992714633094553, 0.4393319649443364, 0.4106553678941996, 0.4319335646971943, 0.4195467811524594, 0.43287292864917787, 0.4240504856588841, 0.4224648808911323, 0.42334298964282413], 'lossList': [0.0, -1.2130369877815246, 0.0, 2.785551427304745, 0.0, 0.0, 0.0], 'rewardMean': 0.6902826572644465, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.3965519538704, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.692229967943448, 'errorList': [], 'lossList': [0.0, -1.192941443324089, 0.0, 1.4566318072378635, 0.0, 0.0, 0.0], 'rewardMean': 0.722556491692002, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.7927093353853
'totalSteps': 23040, 'rewardStep': 0.7860715463451011, 'errorList': [], 'lossList': [0.0, -1.189383670091629, 0.0, 0.8177550204843282, 0.0, 0.0, 0.0], 'rewardMean': 0.7588927848988979, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.0353578552388
'totalSteps': 24320, 'rewardStep': 0.8413246685636224, 'errorList': [], 'lossList': [0.0, -1.181425901055336, 0.0, 0.5237274643778801, 0.0, 0.0, 0.0], 'rewardMean': 0.7849919463393417, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.0276241865315
'totalSteps': 25600, 'rewardStep': 0.9527091551465656, 'errorList': [0.04910037404059491, 0.08441583810842018, 0.04902173897340837, 0.04787618911083986, 0.046692268245840704, 0.08913975483837686, 0.04661028931043274, 0.046394008992553526, 0.08588358745896789, 0.0482415740014762, 0.04656184622607704, 0.05152119381497274, 0.10258336645098903, 0.08278885339112334, 0.07762100695375994, 0.07672065676325533, 0.057406563302948244, 0.08754763414015547, 0.10908096825297159, 0.0517951706652446, 0.12994951674631397, 0.04685333934620351, 0.046423242217907286, 0.047540980157450924, 0.049231228385147076, 0.046689243653264365, 0.08896371589521872, 0.08721110763294146, 0.04701784584476251, 0.07212248300620522, 0.04650107547308464, 0.0667158550611192, 0.0470486016925758, 0.047239843902351246, 0.058130802974813496, 0.12520395436915455, 0.046506289373806395, 0.046493655313244495, 0.04940069962492017, 0.046948730753887266, 0.09339148394152294, 0.048981397371255904, 0.04750065788269527, 0.0470989048248506, 0.047405881380132533, 0.045398565500238214, 0.045665269382952615, 0.04652239779472035, 0.04837835816211007, 0.048768822840283464], 'lossList': [0.0, -1.1580773848295212, 0.0, 0.8422643821686506, 0.0, 0.0, 0.0], 'rewardMean': 0.7889013634525102, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.0519212982686, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.84
