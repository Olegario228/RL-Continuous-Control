#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 55
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.7374165748876456, 'errorList': [], 'lossList': [0.0, -1.4118520027399064, 0.0, 33.76445237159729, 0.0, 0.0, 0.0], 'rewardMean': 0.674043336914748, 'totalEpisodes': 32, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.234459535348535
'totalSteps': 3840, 'rewardStep': 0.9805257076073691, 'errorList': [], 'lossList': [0.0, -1.4102527338266373, 0.0, 44.101534595489504, 0.0, 0.0, 0.0], 'rewardMean': 0.7762041271456218, 'totalEpisodes': 60, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.35616608904885
'totalSteps': 5120, 'rewardStep': 0.7460640553194771, 'errorList': [], 'lossList': [0.0, -1.3953636169433594, 0.0, 48.1174462223053, 0.0, 0.0, 0.0], 'rewardMean': 0.7686691091890856, 'totalEpisodes': 76, 'stepsPerEpisode': 165, 'rewardPerEpisode': 122.81688180284101
'totalSteps': 6400, 'rewardStep': 0.811878828905212, 'errorList': [], 'lossList': [0.0, -1.3739888042211532, 0.0, 71.13774293899536, 0.0, 0.0, 0.0], 'rewardMean': 0.7773110531323109, 'totalEpisodes': 91, 'stepsPerEpisode': 140, 'rewardPerEpisode': 110.71144617129688
'totalSteps': 7680, 'rewardStep': 0.5612745073028653, 'errorList': [], 'lossList': [0.0, -1.375752255320549, 0.0, 83.35711166381836, 0.0, 0.0, 0.0], 'rewardMean': 0.7413049621607367, 'totalEpisodes': 110, 'stepsPerEpisode': 45, 'rewardPerEpisode': 28.316473054517033
'totalSteps': 8960, 'rewardStep': 0.5287804778844715, 'errorList': [], 'lossList': [0.0, -1.3915560603141786, 0.0, 49.37825167655945, 0.0, 0.0, 0.0], 'rewardMean': 0.7109443215498417, 'totalEpisodes': 125, 'stepsPerEpisode': 114, 'rewardPerEpisode': 80.02707029960432
'totalSteps': 10240, 'rewardStep': 0.4628796598410714, 'errorList': [], 'lossList': [0.0, -1.3905301183462142, 0.0, 43.26238837718964, 0.0, 0.0, 0.0], 'rewardMean': 0.6799362388362453, 'totalEpisodes': 134, 'stepsPerEpisode': 109, 'rewardPerEpisode': 80.07343044179974
'totalSteps': 11520, 'rewardStep': 0.5421813149986573, 'errorList': [], 'lossList': [0.0, -1.376566174030304, 0.0, 40.3191246843338, 0.0, 0.0, 0.0], 'rewardMean': 0.6646301361876245, 'totalEpisodes': 140, 'stepsPerEpisode': 70, 'rewardPerEpisode': 56.99987374943151
'totalSteps': 12800, 'rewardStep': 0.5918661553137177, 'errorList': [], 'lossList': [0.0, -1.3542193424701692, 0.0, 17.39752243757248, 0.0, 0.0, 0.0], 'rewardMean': 0.6573537381002338, 'totalEpisodes': 141, 'stepsPerEpisode': 818, 'rewardPerEpisode': 645.4853925090318
'totalSteps': 14080, 'rewardStep': 0.649502699639265, 'errorList': [], 'lossList': [0.0, -1.3342220455408096, 0.0, 6.054206628203392, 0.0, 0.0, 0.0], 'rewardMean': 0.6612369981699752, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 928.2175176702369
'totalSteps': 15360, 'rewardStep': 0.6272696360816196, 'errorList': [], 'lossList': [0.0, -1.298409079313278, 0.0, 4.869642668366432, 0.0, 0.0, 0.0], 'rewardMean': 0.6502223042893726, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 906.7062050648725
'totalSteps': 16640, 'rewardStep': 0.9308526776949335, 'errorList': [0.047284844224754666, 0.09073071468403945, 0.07202220863757466, 0.10534422155050709, 0.06040600848432285, 0.09008681885562651, 0.04593885859270943, 0.056972198712177444, 0.06257563244635912, 0.07183715859258252, 0.08200928586784584, 0.09714774411778121, 0.0594346211562452, 0.09071886370914525, 0.0732513430377749, 0.06401997029264778, 0.06870369947063684, 0.08640214446609493, 0.07570255612128375, 0.0750606156507458, 0.051776089355132345, 0.09035210753093512, 0.048016253016007246, 0.05764692119819605, 0.04888107815848002, 0.05639572154468667, 0.08095571404820495, 0.08718447308365894, 0.048878831754129416, 0.07917133069988626, 0.0842615127726524, 0.0798321770392128, 0.08190888070604746, 0.0649924722300883, 0.05642478679740228, 0.07570994449441665, 0.05056849959898984, 0.04678727451328201, 0.08023909349179922, 0.09304743016084964, 0.08823832049644727, 0.057808353433039354, 0.0646343272279275, 0.057429584261209934, 0.05272223365572416, 0.09801817350591234, 0.08526163722346186, 0.07319791628467266, 0.06614878319161341, 0.07596880151412479], 'lossList': [0.0, -1.2649306505918503, 0.0, 4.020542672872543, 0.0, 0.0, 0.0], 'rewardMean': 0.6452550012981291, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.9674546779368, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=16640, timeSpent=63.49
