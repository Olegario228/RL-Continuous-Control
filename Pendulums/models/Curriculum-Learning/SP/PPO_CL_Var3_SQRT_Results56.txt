#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 56
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6864673823331515, 'errorList': [], 'lossList': [0.0, -1.4135106575489045, 0.0, 34.76691756248474, 0.0, 0.0, 0.0], 'rewardMean': 0.8248784671219307, 'totalEpisodes': 31, 'stepsPerEpisode': 39, 'rewardPerEpisode': 35.66206684806129
'totalSteps': 3840, 'rewardStep': 0.6313013240259353, 'errorList': [], 'lossList': [0.0, -1.4132297563552856, 0.0, 43.56348076820373, 0.0, 0.0, 0.0], 'rewardMean': 0.7603527527565989, 'totalEpisodes': 51, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.126725188914786
'totalSteps': 5120, 'rewardStep': 0.6022290162599699, 'errorList': [], 'lossList': [0.0, -1.4122283053398133, 0.0, 50.01889206886292, 0.0, 0.0, 0.0], 'rewardMean': 0.7208218186324415, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.684150438347624
'totalSteps': 6400, 'rewardStep': 0.5051430990564957, 'errorList': [], 'lossList': [0.0, -1.4079164004325866, 0.0, 73.54353002548218, 0.0, 0.0, 0.0], 'rewardMean': 0.6776860747172524, 'totalEpisodes': 80, 'stepsPerEpisode': 85, 'rewardPerEpisode': 64.10040675239618
'totalSteps': 7680, 'rewardStep': 0.6370331403591586, 'errorList': [], 'lossList': [0.0, -1.4053641653060913, 0.0, 57.320384368896484, 0.0, 0.0, 0.0], 'rewardMean': 0.6709105856575701, 'totalEpisodes': 92, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.0610099690682855
'totalSteps': 8960, 'rewardStep': 0.7898821170620398, 'errorList': [], 'lossList': [0.0, -1.4064827746152877, 0.0, 38.800705919265745, 0.0, 0.0, 0.0], 'rewardMean': 0.6879065187153515, 'totalEpisodes': 103, 'stepsPerEpisode': 71, 'rewardPerEpisode': 64.36559460552813
'totalSteps': 10240, 'rewardStep': 0.6444079792295061, 'errorList': [], 'lossList': [0.0, -1.398851990699768, 0.0, 29.68522165298462, 0.0, 0.0, 0.0], 'rewardMean': 0.6824692012796209, 'totalEpisodes': 112, 'stepsPerEpisode': 53, 'rewardPerEpisode': 43.95318655251962
'totalSteps': 11520, 'rewardStep': 0.4603897614526069, 'errorList': [], 'lossList': [0.0, -1.3949343001842498, 0.0, 25.279973492622375, 0.0, 0.0, 0.0], 'rewardMean': 0.6577937079655082, 'totalEpisodes': 121, 'stepsPerEpisode': 16, 'rewardPerEpisode': 9.70213403032314
'totalSteps': 12800, 'rewardStep': 0.3520259484639708, 'errorList': [], 'lossList': [0.0, -1.3801471489667891, 0.0, 24.67008953332901, 0.0, 0.0, 0.0], 'rewardMean': 0.6272169320153544, 'totalEpisodes': 125, 'stepsPerEpisode': 620, 'rewardPerEpisode': 412.6561820408969
'totalSteps': 14080, 'rewardStep': 0.8322423930930201, 'errorList': [], 'lossList': [0.0, -1.3514191234111785, 0.0, 8.67007464170456, 0.0, 0.0, 0.0], 'rewardMean': 0.6141122161335855, 'totalEpisodes': 126, 'stepsPerEpisode': 1276, 'rewardPerEpisode': 909.1984914775593
'totalSteps': 15360, 'rewardStep': 0.9236035710450272, 'errorList': [], 'lossList': [0.0, -1.3428186142444611, 0.0, 5.4022090500593185, 0.0, 0.0, 0.0], 'rewardMean': 0.6378258350047731, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.2960566817039
'totalSteps': 16640, 'rewardStep': 0.8390748714783086, 'errorList': [], 'lossList': [0.0, -1.3335857999324798, 0.0, 4.26971906632185, 0.0, 0.0, 0.0], 'rewardMean': 0.6586031897500103, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1063.01076256037
'totalSteps': 17920, 'rewardStep': 0.8611667705240247, 'errorList': [], 'lossList': [0.0, -1.2876529389619826, 0.0, 4.03625693872571, 0.0, 0.0, 0.0], 'rewardMean': 0.6844969651764159, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.2425148542627
'totalSteps': 19200, 'rewardStep': 0.9094955646919022, 'errorList': [], 'lossList': [0.0, -1.2428288584947587, 0.0, 3.501170153617859, 0.0, 0.0, 0.0], 'rewardMean': 0.7249322117399564, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.591823191614
'totalSteps': 20480, 'rewardStep': 0.8932177401208848, 'errorList': [], 'lossList': [0.0, -1.2001981675624847, 0.0, 2.3722041413933037, 0.0, 0.0, 0.0], 'rewardMean': 0.7505506717161291, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1190.6892759577354
'totalSteps': 21760, 'rewardStep': 0.8899557433450679, 'errorList': [], 'lossList': [0.0, -1.1379001301527023, 0.0, 1.7179884213209151, 0.0, 0.0, 0.0], 'rewardMean': 0.7605580343444319, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1202.3362448741634
'totalSteps': 23040, 'rewardStep': 0.9674951689956279, 'errorList': [0.1401410977325579, 0.14620778223246728, 0.13380048268493613, 0.13493933497565438, 0.14783883933963968, 0.13638714821708822, 0.14238014654677242, 0.14229572510256125, 0.1366990965141627, 0.13950598270204292, 0.13228508143882348, 0.13103101683044163, 0.13546145260120873, 0.144358414053493, 0.19367839885853583, 0.13572823810040882, 0.14187053034510314, 0.13189797477966655, 0.13702227059539432, 0.13599656488904457, 0.1455909015838533, 0.15730450153144385, 0.13983392520779103, 0.14837763625032094, 0.1360156220538536, 0.15221092448515713, 0.14190853346877194, 0.15259909555205303, 0.13323740450449095, 0.13553369025243048, 0.13149220727950975, 0.16957115340431475, 0.14364152446702796, 0.14137035224890857, 0.17022634107582157, 0.14415809382972758, 0.16546883508386948, 0.21153226077846188, 0.13989560318770422, 0.1411703204547696, 0.13684549094850243, 0.1424480712106235, 0.1421015162666689, 0.15332557661513596, 0.17350631504285138, 0.13798879594708408, 0.189066080996742, 0.23334637679244716, 0.17064635653270194, 0.1353142390054356], 'lossList': [0.0, -1.094740447998047, 0.0, 0.7996350788511336, 0.0, 0.0, 0.0], 'rewardMean': 0.7928667533210441, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1183.571529021875, 'successfulTests': 48
'totalSteps': 24320, 'rewardStep': 0.9073232927816287, 'errorList': [], 'lossList': [0.0, -1.0605361133813858, 0.0, 0.48455619005486367, 0.0, 0.0, 0.0], 'rewardMean': 0.8375601064539462, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.1691714236952
'totalSteps': 25600, 'rewardStep': 0.9717917146704718, 'errorList': [0.022727592407480884, 0.0034803735388725656, 0.11231679700972126, 0.019313450729506753, 0.03819295683896715, 0.13681364311882202, 0.05331134546597016, 0.12116246210091382, 0.11866365002283837, 0.015634620071124143, 0.028650513482643285, 0.0603443239783475, 0.05680250045476367, 0.03400344867599095, 0.0770778821561704, 0.05277870423764913, 0.08623354091916585, 0.03059058337515116, 0.2406034714235817, 0.10202929842983177, 0.11408399346341086, 0.0563645951269236, 0.10121303694935252, 0.1437018440248439, 0.06263067885924237, 0.023464035288272896, 0.0851618610708684, 0.16157540451420974, 0.06261253395376204, 0.04191216403458189, 0.08400140449178627, 0.023315227423207418, 0.01447681001006577, 0.12816720563935757, 0.18932115852965364, 0.07383708382460466, 0.18826446407477995, 0.04423801394638802, 0.1171962076101235, 0.1551874892904027, 0.034751570222384384, 0.06792290027523867, 0.0909016974347002, 0.18286977316150013, 0.07473105939696584, 0.09471317824774521, 0.06515873491165516, 0.0947012101396791, 0.18003389788769303, 0.03305346188298217], 'lossList': [0.0, -1.0239600503444672, 0.0, 0.45104020116850735, 0.0, 0.0, 0.0], 'rewardMean': 0.8995366830745963, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1217.4965676865552, 'successfulTests': 49
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=25600, timeSpent=108.72
