#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 57
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.8756890812016177, 'errorList': [], 'lossList': [0.0, -1.413407154083252, 0.0, 34.31850445747376, 0.0, 0.0, 0.0], 'rewardMean': 0.8347295411908844, 'totalEpisodes': 36, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.20273369549541
'totalSteps': 3840, 'rewardStep': 0.8079876395126949, 'errorList': [], 'lossList': [0.0, -1.387614592909813, 0.0, 46.45324732780457, 0.0, 0.0, 0.0], 'rewardMean': 0.8258155739648213, 'totalEpisodes': 59, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.90983566904718
'totalSteps': 5120, 'rewardStep': 0.9354833505635598, 'errorList': [], 'lossList': [0.0, -1.358312996029854, 0.0, 50.725155210495, 0.0, 0.0, 0.0], 'rewardMean': 0.853232518114506, 'totalEpisodes': 77, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.05888490367196
'totalSteps': 6400, 'rewardStep': 0.6901264071898898, 'errorList': [], 'lossList': [0.0, -1.3451287126541138, 0.0, 71.27580352783202, 0.0, 0.0, 0.0], 'rewardMean': 0.8206112959295828, 'totalEpisodes': 97, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.94983906484307
'totalSteps': 7680, 'rewardStep': 0.7798153909585885, 'errorList': [], 'lossList': [0.0, -1.3420494389533997, 0.0, 100.11513744354248, 0.0, 0.0, 0.0], 'rewardMean': 0.813811978434417, 'totalEpisodes': 119, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.355273774592266
'totalSteps': 8960, 'rewardStep': 0.5162673503037375, 'errorList': [], 'lossList': [0.0, -1.3346294581890106, 0.0, 38.11527298927307, 0.0, 0.0, 0.0], 'rewardMean': 0.7713056029871771, 'totalEpisodes': 131, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5162673503037375
'totalSteps': 10240, 'rewardStep': 0.5529772960918756, 'errorList': [], 'lossList': [0.0, -1.3197805953025818, 0.0, 22.906922032833098, 0.0, 0.0, 0.0], 'rewardMean': 0.7440145646252643, 'totalEpisodes': 141, 'stepsPerEpisode': 181, 'rewardPerEpisode': 132.3059528483356
'totalSteps': 11520, 'rewardStep': 0.6847051488846998, 'errorList': [], 'lossList': [0.0, -1.322249429821968, 0.0, 24.26435712337494, 0.0, 0.0, 0.0], 'rewardMean': 0.7374246295429793, 'totalEpisodes': 150, 'stepsPerEpisode': 214, 'rewardPerEpisode': 167.9509845475306
'totalSteps': 12800, 'rewardStep': 0.8820790525754528, 'errorList': [], 'lossList': [0.0, -1.334579656124115, 0.0, 29.84667910337448, 0.0, 0.0, 0.0], 'rewardMean': 0.7518900718462266, 'totalEpisodes': 158, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.376327276762778
'totalSteps': 14080, 'rewardStep': 0.8466827503186551, 'errorList': [], 'lossList': [0.0, -1.3434563452005386, 0.0, 9.096110699176789, 0.0, 0.0, 0.0], 'rewardMean': 0.7571813467600771, 'totalEpisodes': 162, 'stepsPerEpisode': 73, 'rewardPerEpisode': 58.3711262977869
'totalSteps': 15360, 'rewardStep': 0.8501621279924434, 'errorList': [], 'lossList': [0.0, -1.3526303780078888, 0.0, 18.099460262060166, 0.0, 0.0, 0.0], 'rewardMean': 0.7546286514391597, 'totalEpisodes': 165, 'stepsPerEpisode': 212, 'rewardPerEpisode': 168.266444632157
'totalSteps': 16640, 'rewardStep': 0.9285350491227046, 'errorList': [], 'lossList': [0.0, -1.340964607000351, 0.0, 5.832781360149384, 0.0, 0.0, 0.0], 'rewardMean': 0.7666833924001607, 'totalEpisodes': 167, 'stepsPerEpisode': 171, 'rewardPerEpisode': 146.0359563324615
'totalSteps': 17920, 'rewardStep': 0.5608544775074333, 'errorList': [], 'lossList': [0.0, -1.3202586501836777, 0.0, 3.6125220566987992, 0.0, 0.0, 0.0], 'rewardMean': 0.7292205050945479, 'totalEpisodes': 168, 'stepsPerEpisode': 800, 'rewardPerEpisode': 625.4285085478635
'totalSteps': 19200, 'rewardStep': 0.9304041909698688, 'errorList': [0.13460748115899088, 0.14215888291958056, 0.12000978067127682, 0.148919985580617, 0.1641878292454627, 0.17787165837053712, 0.1414201111777306, 0.11128939192690418, 0.16759227199272445, 0.16359236188071594, 0.16328739431895967, 0.1738537504723032, 0.15493513654548396, 0.136695626271047, 0.13782170741294117, 0.13213307267570717, 0.150260954693343, 0.16769745750373552, 0.12218857149104506, 0.1217261905686243, 0.15085103375113906, 0.12925013832740895, 0.15254104057122508, 0.10396152615425576, 0.11785565492380666, 0.18317554573600756, 0.1114943902323344, 0.1702800932186105, 0.16442870325471962, 0.10674525771955969, 0.22002971015488793, 0.14334888756792355, 0.1549801800635568, 0.1383416414882457, 0.1764311982971619, 0.14651554493069174, 0.1034810635840858, 0.09771281423805032, 0.1191701979199911, 0.15953955789723828, 0.16434697168352824, 0.1554430634185177, 0.1496035287361668, 0.12545094715307373, 0.11647058531446669, 0.13012247190230386, 0.18262087105563404, 0.19475788781335193, 0.13856988132585657, 0.1008195577348725], 'lossList': [0.0, -1.302770214676857, 0.0, 8.666657410562038, 0.0, 0.0, 0.0], 'rewardMean': 0.753248283472546, 'totalEpisodes': 170, 'stepsPerEpisode': 759, 'rewardPerEpisode': 666.5798052433842, 'successfulTests': 49
'totalSteps': 20480, 'rewardStep': 0.9238321344350724, 'errorList': [], 'lossList': [0.0, -1.2391692423820495, 0.0, 2.1655029583722354, 0.0, 0.0, 0.0], 'rewardMean': 0.7676499578201943, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.3985917568796
'totalSteps': 21760, 'rewardStep': 0.95919796005208, 'errorList': [0.122149777404805, 0.14478183021406088, 0.1890708919789742, 0.10376853012922935, 0.1889715883732308, 0.14165232804926434, 0.1570789215084774, 0.18468663282871517, 0.11194862054713779, 0.18557839417797048, 0.11523473886235171, 0.07899670918569678, 0.1539522387446862, 0.11625323282116143, 0.24908112130198026, 0.13313740106957422, 0.10470036986879994, 0.11757927484288538, 0.1257014315502495, 0.1247118325331217, 0.2913949864207668, 0.22338748232895267, 0.16940550265511026, 0.18963317668135796, 0.1537838563399348, 0.13593593918099833, 0.0899625540255398, 0.08677280105880243, 0.1631782501465793, 0.12840892722988545, 0.1377959490392997, 0.24580650458021916, 0.18870895344139837, 0.15695904269455313, 0.15922378143250876, 0.18978317608970552, 0.12953672009202638, 0.13132636120063582, 0.12675611530359898, 0.15371421093533566, 0.0943702544881202, 0.2716253841595588, 0.17799689602594582, 0.2615968319779886, 0.1728971102833807, 0.22652841997194334, 0.12860353612542913, 0.11449790660298995, 0.20604496749576146, 0.12538695239675665], 'lossList': [0.0, -1.1514981496334076, 0.0, 1.4903069250285625, 0.0, 0.0, 0.0], 'rewardMean': 0.8119430187950286, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.2180120600565, 'successfulTests': 42
'totalSteps': 23040, 'rewardStep': 0.9759863476334191, 'errorList': [0.2625847695959237, 0.31314745058783505, 0.29251218633183207, 0.33405708834419534, 0.42153005224383316, 0.33315042174447407, 0.41286985487877903, 0.48366576275027434, 0.40847050491126424, 0.43920194741778823, 0.31075010345636217, 0.45982452227373294, 0.25729111101371543, 0.40238359555551095, 0.5822366356137845, 0.3908183792979067, 0.26776576811562014, 0.5009093990797941, 0.27172673746521525, 0.8040082973269608, 0.3823822083989787, 0.6006993201501348, 0.3165842934353581, 0.3224685386392463, 0.4837351058640044, 0.3805432981317102, 0.3406167638277602, 0.5279458192152553, 0.3337003641268827, 0.2601106441042501, 0.316753193699288, 0.44746905839810824, 0.5453114700273658, 0.5329010623315952, 0.30140674125992173, 0.5149049274974814, 0.3797663101931922, 0.3604735612405991, 0.5004126050626313, 0.6052960340141653, 0.4076741763239947, 0.3472510675156832, 0.28039024412519675, 0.30763158162774407, 0.32224617241779446, 0.47878765079005614, 0.45213706405106147, 0.6100401674941617, 0.3269133178267606, 0.29799404461881956], 'lossList': [0.0, -1.1081499195098876, 0.0, 1.5931675577163695, 0.0, 0.0, 0.0], 'rewardMean': 0.854243923949183, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1200.3338684283992, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.8937018354461415, 'errorList': [], 'lossList': [0.0, -1.1011965680122375, 0.0, 0.6868043510988354, 0.0, 0.0, 0.0], 'rewardMean': 0.875143592605327, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.4920140698396
'totalSteps': 25600, 'rewardStep': 0.7445438219016555, 'errorList': [], 'lossList': [0.0, -1.1060554337501527, 0.0, 0.7650961498171092, 0.0, 0.0, 0.0], 'rewardMean': 0.8613900695379474, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.0740239894797
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=19200, timeSpent=128.73
