#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 59
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.8249382584394608, 'errorList': [], 'lossList': [0.0, -1.4125692111253738, 0.0, 30.031024255752563, 0.0, 0.0, 0.0], 'rewardMean': 0.5945041821075248, 'totalEpisodes': 39, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.50439307620286
'totalSteps': 3840, 'rewardStep': 0.8766325356849697, 'errorList': [], 'lossList': [0.0, -1.4184666615724564, 0.0, 43.139267740249636, 0.0, 0.0, 0.0], 'rewardMean': 0.6885469666333397, 'totalEpisodes': 69, 'stepsPerEpisode': 54, 'rewardPerEpisode': 41.70681167759816
'totalSteps': 5120, 'rewardStep': 0.9353821523195186, 'errorList': [], 'lossList': [0.0, -1.4038485944271089, 0.0, 44.891821737289426, 0.0, 0.0, 0.0], 'rewardMean': 0.7502557630548844, 'totalEpisodes': 85, 'stepsPerEpisode': 41, 'rewardPerEpisode': 36.32151171463553
'totalSteps': 6400, 'rewardStep': 0.8020080047188577, 'errorList': [], 'lossList': [0.0, -1.3891250854730606, 0.0, 65.79759370803833, 0.0, 0.0, 0.0], 'rewardMean': 0.760606211387679, 'totalEpisodes': 101, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.966580954072885
'totalSteps': 7680, 'rewardStep': 0.878985474268993, 'errorList': [], 'lossList': [0.0, -1.385804477930069, 0.0, 59.19997275352478, 0.0, 0.0, 0.0], 'rewardMean': 0.7803360885345647, 'totalEpisodes': 118, 'stepsPerEpisode': 41, 'rewardPerEpisode': 31.958274771320106
'totalSteps': 8960, 'rewardStep': 0.8434825965528198, 'errorList': [], 'lossList': [0.0, -1.385809798836708, 0.0, 53.97074065208435, 0.0, 0.0, 0.0], 'rewardMean': 0.7893570182514582, 'totalEpisodes': 134, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.5270505853559704
'totalSteps': 10240, 'rewardStep': 0.7180431397064337, 'errorList': [], 'lossList': [0.0, -1.3821510910987853, 0.0, 20.724430067539217, 0.0, 0.0, 0.0], 'rewardMean': 0.7804427834333303, 'totalEpisodes': 140, 'stepsPerEpisode': 130, 'rewardPerEpisode': 105.35969497646487
'totalSteps': 11520, 'rewardStep': 0.44668378579531415, 'errorList': [], 'lossList': [0.0, -1.371688511967659, 0.0, 16.81583033323288, 0.0, 0.0, 0.0], 'rewardMean': 0.7433584503624396, 'totalEpisodes': 144, 'stepsPerEpisode': 380, 'rewardPerEpisode': 264.003871478392
'totalSteps': 12800, 'rewardStep': 0.7625929971651846, 'errorList': [], 'lossList': [0.0, -1.3595703715085983, 0.0, 38.065080542564395, 0.0, 0.0, 0.0], 'rewardMean': 0.7452819050427142, 'totalEpisodes': 149, 'stepsPerEpisode': 64, 'rewardPerEpisode': 54.982383978945435
'totalSteps': 14080, 'rewardStep': 0.8846398727145518, 'errorList': [], 'lossList': [0.0, -1.3394733273983002, 0.0, 8.429388185739517, 0.0, 0.0, 0.0], 'rewardMean': 0.7973388817366105, 'totalEpisodes': 152, 'stepsPerEpisode': 56, 'rewardPerEpisode': 46.695906593597456
'totalSteps': 15360, 'rewardStep': 0.8551451205943981, 'errorList': [], 'lossList': [0.0, -1.3216629576683045, 0.0, 42.191124055385586, 0.0, 0.0, 0.0], 'rewardMean': 0.8003595679521041, 'totalEpisodes': 158, 'stepsPerEpisode': 32, 'rewardPerEpisode': 25.486619064892388
'totalSteps': 16640, 'rewardStep': 0.5111067127487435, 'errorList': [], 'lossList': [0.0, -1.3116092848777772, 0.0, 31.89277035713196, 0.0, 0.0, 0.0], 'rewardMean': 0.7638069856584815, 'totalEpisodes': 161, 'stepsPerEpisode': 447, 'rewardPerEpisode': 287.90300178486183
'totalSteps': 17920, 'rewardStep': 0.7158355782114199, 'errorList': [], 'lossList': [0.0, -1.2719050770998002, 0.0, 4.687797954976559, 0.0, 0.0, 0.0], 'rewardMean': 0.7418523282476717, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 981.6327285931054
'totalSteps': 19200, 'rewardStep': 0.919321474120714, 'errorList': [], 'lossList': [0.0, -1.2150193977355956, 0.0, 3.1091964012384414, 0.0, 0.0, 0.0], 'rewardMean': 0.7535836751878573, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.7798593373477
'totalSteps': 20480, 'rewardStep': 0.8863742493106226, 'errorList': [], 'lossList': [0.0, -1.16641763150692, 0.0, 2.780284695252776, 0.0, 0.0, 0.0], 'rewardMean': 0.7543225526920202, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.2348672584721
'totalSteps': 21760, 'rewardStep': 0.9569897643678575, 'errorList': [0.0305363532758879, 0.023170464842111525, 0.06982438456695023, 0.022579537743012353, 0.020186368289202745, 0.042808264633274605, 0.04034954590047758, 0.023403500643103457, 0.027057765190569495, 0.03219847378949363, 0.09415696946869698, 0.061820221201053585, 0.03072100101993936, 0.028402373934424595, 0.04265311345466004, 0.04153125833260936, 0.02369106573825015, 0.08439725538860883, 0.027687551712199016, 0.04440826858605024, 0.03610343123249184, 0.0548216202316984, 0.04646798727871544, 0.03616534019383241, 0.056232552243741636, 0.0365228246958632, 0.031978006730659134, 0.036278900938268625, 0.05920118346459552, 0.04886709834295057, 0.04383809984342803, 0.026039391544474166, 0.025469651537853345, 0.026328057560059986, 0.022419632130123875, 0.019597529959144325, 0.03189747335414306, 0.02980778927370425, 0.04975558202789956, 0.041979868147065065, 0.0236718156302184, 0.049407397662623545, 0.06589754535333031, 0.03934450725335824, 0.028415100211944853, 0.028210749297427076, 0.0507656301179157, 0.02676437301472412, 0.06674552291839288, 0.037618817575233894], 'lossList': [0.0, -1.1351796758174897, 0.0, 3.0076040101423858, 0.0, 0.0, 0.0], 'rewardMean': 0.765673269473524, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.9182766876672, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=74.38
