#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 6
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.5857346208642372, 'errorList': [], 'lossList': [0.0, -1.4157803809642793, 0.0, 32.985410623550415, 0.0, 0.0, 0.0], 'rewardMean': 0.7745120863874735, 'totalEpisodes': 35, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.28404518497021
'totalSteps': 3840, 'rewardStep': 0.5391515451036287, 'errorList': [], 'lossList': [0.0, -1.411031454205513, 0.0, 56.206821022033694, 0.0, 0.0, 0.0], 'rewardMean': 0.696058572626192, 'totalEpisodes': 67, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.88582975983873
'totalSteps': 5120, 'rewardStep': 0.6756531523992091, 'errorList': [], 'lossList': [0.0, -1.403919254541397, 0.0, 62.73175550460815, 0.0, 0.0, 0.0], 'rewardMean': 0.6909572175694463, 'totalEpisodes': 93, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.541127922850208
'totalSteps': 6400, 'rewardStep': 0.7563156627108278, 'errorList': [], 'lossList': [0.0, -1.3981710571050643, 0.0, 71.34190221786498, 0.0, 0.0, 0.0], 'rewardMean': 0.7040289065977225, 'totalEpisodes': 134, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.316540036358154
'totalSteps': 7680, 'rewardStep': 0.9419265940925655, 'errorList': [30.5415738685305, 8.312841123294177, 16.34481455838739, 35.52739964395016, 33.789613665817974, 39.513449164588586, 28.635599194337583, 68.55744428175845, 85.07179281336373, 63.78713624532172, 24.47795816128632, 6.657230085453537, 35.06431742908209, 1.6152592602952058, 68.88646463137307, 70.12299103688649, 96.57333683817326, 7.958879065014579, 21.036570886821792, 48.50461908278638, 67.02486112310577, 58.006662766490614, 52.232013370001695, 46.583167257460836, 7.2021683356065225, 9.644121825126872, 19.961882522086537, 51.98799158721683, 7.905620512313598, 52.652166089046915, 25.76829381337274, 7.162293301649589, 15.172774591158293, 18.713212596693943, 76.32736794432914, 7.078192570806484, 18.09055942639267, 15.797407589463884, 70.25986696199305, 40.11162118512926, 46.08973032546373, 56.40397424697447, 43.90875190591957, 77.85755153138489, 28.64403567989383, 73.59268625409973, 83.95315187925621, 73.65005303750223, 24.62189440361244, 67.44662329638479], 'lossList': [0.0, -1.3967115741968155, 0.0, 37.40612410545349, 0.0, 0.0, 0.0], 'rewardMean': 0.7436785211801964, 'totalEpisodes': 152, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.86400973396435, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.3756262850713989, 'errorList': [], 'lossList': [0.0, -1.3975495570898055, 0.0, 33.778300938606264, 0.0, 0.0, 0.0], 'rewardMean': 0.6910996303075111, 'totalEpisodes': 167, 'stepsPerEpisode': 110, 'rewardPerEpisode': 82.14300033544959
'totalSteps': 10240, 'rewardStep': 0.8633728560807405, 'errorList': [], 'lossList': [0.0, -1.3948437410593033, 0.0, 19.39265283346176, 0.0, 0.0, 0.0], 'rewardMean': 0.7126337835291647, 'totalEpisodes': 175, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.503253339526175
'totalSteps': 11520, 'rewardStep': 0.8101148004592152, 'errorList': [], 'lossList': [0.0, -1.3906253159046174, 0.0, 18.967011905908585, 0.0, 0.0, 0.0], 'rewardMean': 0.7234650076325037, 'totalEpisodes': 181, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.55913877260481
'totalSteps': 12800, 'rewardStep': 0.6819900000695123, 'errorList': [], 'lossList': [0.0, -1.387798844575882, 0.0, 10.664769130945206, 0.0, 0.0, 0.0], 'rewardMean': 0.7193175068762045, 'totalEpisodes': 184, 'stepsPerEpisode': 181, 'rewardPerEpisode': 146.64778656692243
'totalSteps': 14080, 'rewardStep': 0.6397778713686597, 'errorList': [], 'lossList': [0.0, -1.3706884396076202, 0.0, 9.296416842341422, 0.0, 0.0, 0.0], 'rewardMean': 0.6869663388219995, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 964.8819955674987
'totalSteps': 15360, 'rewardStep': 0.9790409811947819, 'errorList': [0.014391912535569198, 0.012294278823815038, 0.011655234647295417, 0.01263975713944808, 0.015557895512186407, 0.016958099368237584, 0.030392174410710755, 0.028353134132407625, 0.032354572826483026, 0.012335260928091734, 0.013421403947487478, 0.01738861993860086, 0.013728835666398653, 0.018568922236542554, 0.01357107589530346, 0.014815113522245635, 0.014972046069125822, 0.011694652083384159, 0.04132137861295664, 0.01608615778829851, 0.04636801051012823, 0.01732620307226764, 0.01915680902442738, 0.02670656685717398, 0.014142668790053355, 0.02034483598994965, 0.022138987763821213, 0.01183549493457521, 0.01355536177628002, 0.018651291681672393, 0.03856599467499231, 0.013094181917427245, 0.014797766898151531, 0.02204226168814025, 0.01637253433844642, 0.012034893775096882, 0.04584889841532935, 0.012208475380885384, 0.02012255031052089, 0.018961500928993378, 0.011834372460960332, 0.014545350749068384, 0.012586860627405617, 0.016151688305940656, 0.017201155532817897, 0.016233532452964525, 0.013716152183884995, 0.01932550833995291, 0.018059712211141356, 0.015534792183790811], 'lossList': [0.0, -1.3283426213264464, 0.0, 6.276053574532271, 0.0, 0.0, 0.0], 'rewardMean': 0.7262969748550538, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1099.3513308030363, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=15360, timeSpent=71.99
