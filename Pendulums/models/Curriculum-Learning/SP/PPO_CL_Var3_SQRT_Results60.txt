#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 60
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.8465677068224439, 'errorList': [], 'lossList': [0.0, -1.4017817038297653, 0.0, 24.148598504066467, 0.0, 0.0, 0.0], 'rewardMean': 0.8231833336607035, 'totalEpisodes': 14, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.860784669868899
'totalSteps': 3840, 'rewardStep': 0.3078788811370081, 'errorList': [], 'lossList': [0.0, -1.3821306896209717, 0.0, 40.33597967147827, 0.0, 0.0, 0.0], 'rewardMean': 0.6514151828194717, 'totalEpisodes': 29, 'stepsPerEpisode': 169, 'rewardPerEpisode': 124.81573564993286
'totalSteps': 5120, 'rewardStep': 0.585889565245306, 'errorList': [], 'lossList': [0.0, -1.3718019312620162, 0.0, 31.49939173221588, 0.0, 0.0, 0.0], 'rewardMean': 0.6350337784259303, 'totalEpisodes': 37, 'stepsPerEpisode': 103, 'rewardPerEpisode': 73.89679998157766
'totalSteps': 6400, 'rewardStep': 0.9181509573705218, 'errorList': [], 'lossList': [0.0, -1.3772505646944047, 0.0, 66.6564527797699, 0.0, 0.0, 0.0], 'rewardMean': 0.6916572142148486, 'totalEpisodes': 51, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.970703672600575
'totalSteps': 7680, 'rewardStep': 0.8507361343254564, 'errorList': [], 'lossList': [0.0, -1.370775616168976, 0.0, 110.61918865203857, 0.0, 0.0, 0.0], 'rewardMean': 0.7181703675666166, 'totalEpisodes': 71, 'stepsPerEpisode': 75, 'rewardPerEpisode': 62.15863224548359
'totalSteps': 8960, 'rewardStep': 0.40276670132393777, 'errorList': [], 'lossList': [0.0, -1.3655304563045503, 0.0, 85.38123903274536, 0.0, 0.0, 0.0], 'rewardMean': 0.6731127009605196, 'totalEpisodes': 95, 'stepsPerEpisode': 69, 'rewardPerEpisode': 52.90599697808613
'totalSteps': 10240, 'rewardStep': 0.6839116644158156, 'errorList': [], 'lossList': [0.0, -1.3523836654424668, 0.0, 44.40872602462768, 0.0, 0.0, 0.0], 'rewardMean': 0.6744625713924316, 'totalEpisodes': 110, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.016174487659602
'totalSteps': 11520, 'rewardStep': 0.7020837873970232, 'errorList': [], 'lossList': [0.0, -1.3293289506435395, 0.0, 30.415974445343018, 0.0, 0.0, 0.0], 'rewardMean': 0.6775315953929417, 'totalEpisodes': 116, 'stepsPerEpisode': 68, 'rewardPerEpisode': 61.004350347168305
'totalSteps': 12800, 'rewardStep': 0.7218732842045871, 'errorList': [], 'lossList': [0.0, -1.299708531498909, 0.0, 11.471148788928986, 0.0, 0.0, 0.0], 'rewardMean': 0.6819657642741064, 'totalEpisodes': 117, 'stepsPerEpisode': 819, 'rewardPerEpisode': 591.0357471053163
'totalSteps': 14080, 'rewardStep': 0.6881763019352634, 'errorList': [], 'lossList': [0.0, -1.275863578915596, 0.0, 8.220599665343762, 0.0, 0.0, 0.0], 'rewardMean': 0.6708034984177363, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.2040498060836
'totalSteps': 15360, 'rewardStep': 0.5722080816394848, 'errorList': [], 'lossList': [0.0, -1.2633526504039765, 0.0, 4.828835446238518, 0.0, 0.0, 0.0], 'rewardMean': 0.6433675358994403, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 886.0114749562083
'totalSteps': 16640, 'rewardStep': 0.8492632154898664, 'errorList': [], 'lossList': [0.0, -1.2399301469326018, 0.0, 5.275653814598918, 0.0, 0.0, 0.0], 'rewardMean': 0.6975059693347262, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.659620665988
'totalSteps': 17920, 'rewardStep': 0.9098627732959341, 'errorList': [], 'lossList': [0.0, -1.2035393613576888, 0.0, 4.451452931538224, 0.0, 0.0, 0.0], 'rewardMean': 0.729903290139789, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.4598917199285
'totalSteps': 19200, 'rewardStep': 0.9241089454432244, 'errorList': [], 'lossList': [0.0, -1.157531715631485, 0.0, 3.4788644533604383, 0.0, 0.0, 0.0], 'rewardMean': 0.7304990889470593, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1167.311427996473
'totalSteps': 20480, 'rewardStep': 0.9301611001443074, 'errorList': [0.07929459620456139, 0.0788682713476555, 0.0930344162618728, 0.11876932238612349, 0.08070253387455979, 0.07877243812526567, 0.0804694568355849, 0.10238365356053725, 0.07840567037022873, 0.08083143316547438, 0.13772074931380288, 0.0850581608831054, 0.08004672598844122, 0.21704171243407017, 0.09462708630255201, 0.08133681143456843, 0.07691196771994846, 0.07829727386857445, 0.07963105981892614, 0.13412259914453983, 0.2402649603213974, 0.08115393155299581, 0.0789203662075245, 0.07916202796420056, 0.07767142857379965, 0.1295998818171418, 0.09468984947826496, 0.07855745472927116, 0.07925411028622556, 0.07989106388386155, 0.08110688497976921, 0.08125979672092, 0.09329031009609771, 0.07916215363224387, 0.07822907336448524, 0.0798651099602576, 0.10777636278428393, 0.0797742048923188, 0.11834984014700586, 0.1278037137369, 0.08416449209187778, 0.07808637692008456, 0.1469088345011794, 0.08173499688805096, 0.17359107423109738, 0.08101331146939063, 0.12277703986544575, 0.1254922025213426, 0.08127404631979754, 0.096148583350275], 'lossList': [0.0, -1.1365937519073486, 0.0, 2.8707056969776747, 0.0, 0.0, 0.0], 'rewardMean': 0.7384415855289443, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1185.6110236748655, 'successfulTests': 48
'totalSteps': 21760, 'rewardStep': 0.9613922829859093, 'errorList': [0.13037793148549623, 0.1348867968049441, 0.13963323583339363, 0.13745932612709838, 0.13628931696014607, 0.14101191517527362, 0.13227547244396431, 0.1327083848020694, 0.13195813795040776, 0.13850242813506003, 0.12845602384561475, 0.13372466451991347, 0.1296045083589475, 0.14410517671869483, 0.13951094464253555, 0.13176662990223814, 0.13059132001243307, 0.12950880837130846, 0.13042769209441615, 0.1322048096766345, 0.13456865059905115, 0.12799711249988105, 0.13101078057916823, 0.13166250579153882, 0.13617162049438802, 0.13405967093593033, 0.12940665028755613, 0.13345518743042398, 0.14364350258949324, 0.15416441573134396, 0.13365886960959636, 0.1603656636921921, 0.13447219299894045, 0.1323308003370619, 0.2116826672247788, 0.24832320237856678, 0.13482948464105227, 0.12933480705589884, 0.16008174698064473, 0.12874093685614527, 0.1349467925650732, 0.13254103653473817, 0.1335094101869334, 0.12751384564660243, 0.13128087666529642, 0.14726846980749683, 0.12666159093171295, 0.13509676207439908, 0.137460358370508, 0.12998015348427663], 'lossList': [0.0, -1.1195060098171234, 0.0, 1.5300727787986397, 0.0, 0.0, 0.0], 'rewardMean': 0.7943041436951416, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.7208579304677, 'successfulTests': 48
'totalSteps': 23040, 'rewardStep': 0.8806714793791046, 'errorList': [], 'lossList': [0.0, -1.0983672565221787, 0.0, 0.7380344578903169, 0.0, 0.0, 0.0], 'rewardMean': 0.8139801251914704, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.4682510904188
'totalSteps': 24320, 'rewardStep': 0.7995989156290291, 'errorList': [], 'lossList': [0.0, -1.0656443804502487, 0.0, 0.5436948847863823, 0.0, 0.0, 0.0], 'rewardMean': 0.8237316380146712, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.3984133775566
'totalSteps': 25600, 'rewardStep': 0.9634910302517563, 'errorList': [0.12019812914511831, 0.07095625081904022, 0.20702172249285397, 0.19504006507010146, 0.0612940427483973, 0.09879575646905231, 0.17955775985960312, 0.11455953510850847, 0.09537826745983433, 0.1067747105657933, 0.1044568660687883, 0.08835407465242125, 0.09265593108239173, 0.10481288150845813, 0.06373814371695995, 0.1021576809807725, 0.19655038314486195, 0.08748560113758039, 0.10585914585313572, 0.16413270826531987, 0.09937494688771527, 0.08968677548955586, 0.12031186871464221, 0.12612525063629856, 0.12233372041387976, 0.3486637889746159, 0.14991784174061276, 0.12564882439494612, 0.26228558401960295, 0.06058855341121116, 0.0819131140995763, 0.1457033960248368, 0.08500659851775012, 0.06570925179844214, 0.1622373355404793, 0.058952368311163926, 0.08621570437895605, 0.30501194846060625, 0.0753582211601191, 0.09788229732851841, 0.06874404866926605, 0.0693798403069187, 0.0945404368019103, 0.08131957076560607, 0.11878517347875692, 0.10681367503447149, 0.07204755334437081, 0.06966852937857577, 0.2459283173212489, 0.08747865111252816], 'lossList': [0.0, -1.035519900918007, 0.0, 0.6661969949863851, 0.0, 0.0, 0.0], 'rewardMean': 0.8478934126193881, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1203.7720301926447, 'successfulTests': 45
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=20480, timeSpent=125.61
