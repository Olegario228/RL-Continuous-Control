#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 61
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.5965441286455642, 'errorList': [], 'lossList': [0.0, -1.4232296240329743, 0.0, 29.001981904506682, 0.0, 0.0, 0.0], 'rewardMean': 0.5566690151725084, 'totalEpisodes': 18, 'stepsPerEpisode': 105, 'rewardPerEpisode': 76.55037428920791
'totalSteps': 3840, 'rewardStep': 0.8049745519559881, 'errorList': [], 'lossList': [0.0, -1.4198069632053376, 0.0, 47.439977378845214, 0.0, 0.0, 0.0], 'rewardMean': 0.6394375274336683, 'totalEpisodes': 32, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.424532931244713
'totalSteps': 5120, 'rewardStep': 0.6214734463823249, 'errorList': [], 'lossList': [0.0, -1.4150930148363114, 0.0, 44.44327845573425, 0.0, 0.0, 0.0], 'rewardMean': 0.6349465071708325, 'totalEpisodes': 45, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.341389315884701
'totalSteps': 6400, 'rewardStep': 0.4771290379910997, 'errorList': [], 'lossList': [0.0, -1.403246363401413, 0.0, 70.79788128852844, 0.0, 0.0, 0.0], 'rewardMean': 0.6033830133348859, 'totalEpisodes': 60, 'stepsPerEpisode': 76, 'rewardPerEpisode': 54.292241904316484
'totalSteps': 7680, 'rewardStep': 0.6262577394527726, 'errorList': [], 'lossList': [0.0, -1.3842418575286866, 0.0, 61.75763677597046, 0.0, 0.0, 0.0], 'rewardMean': 0.607195467687867, 'totalEpisodes': 74, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.94119753986022
'totalSteps': 8960, 'rewardStep': 0.2184745108097349, 'errorList': [], 'lossList': [0.0, -1.3751115697622298, 0.0, 88.10085498809815, 0.0, 0.0, 0.0], 'rewardMean': 0.5516639024195624, 'totalEpisodes': 90, 'stepsPerEpisode': 109, 'rewardPerEpisode': 68.12247815062321
'totalSteps': 10240, 'rewardStep': 0.8463192311724899, 'errorList': [], 'lossList': [0.0, -1.3747511112689972, 0.0, 22.905162715911864, 0.0, 0.0, 0.0], 'rewardMean': 0.5884958185136784, 'totalEpisodes': 95, 'stepsPerEpisode': 78, 'rewardPerEpisode': 65.58405989800268
'totalSteps': 11520, 'rewardStep': 0.8910045316823556, 'errorList': [], 'lossList': [0.0, -1.3814439272880554, 0.0, 49.05777509212494, 0.0, 0.0, 0.0], 'rewardMean': 0.6221078977546425, 'totalEpisodes': 100, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.110170479438093
'totalSteps': 12800, 'rewardStep': 0.7677888351745634, 'errorList': [], 'lossList': [0.0, -1.3576404321193696, 0.0, 27.920626192092897, 0.0, 0.0, 0.0], 'rewardMean': 0.6366759914966347, 'totalEpisodes': 103, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.531438867509365
'totalSteps': 14080, 'rewardStep': 0.6037406558150479, 'errorList': [], 'lossList': [0.0, -1.3401627826690674, 0.0, 5.839069842100144, 0.0, 0.0, 0.0], 'rewardMean': 0.6453706669081941, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 849.0875603541275
'totalSteps': 15360, 'rewardStep': 0.8719444669358474, 'errorList': [], 'lossList': [0.0, -1.3413910806179046, 0.0, 5.173915788531303, 0.0, 0.0, 0.0], 'rewardMean': 0.6729107007372225, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 985.5037076226378
'totalSteps': 16640, 'rewardStep': 0.8412287826309364, 'errorList': [], 'lossList': [0.0, -1.3035594785213471, 0.0, 3.93424592345953, 0.0, 0.0, 0.0], 'rewardMean': 0.6765361238047173, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.6931944462497
'totalSteps': 17920, 'rewardStep': 0.8928956507919353, 'errorList': [], 'lossList': [0.0, -1.256919311285019, 0.0, 3.7321752202883363, 0.0, 0.0, 0.0], 'rewardMean': 0.7036783442456783, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.7880250783298
'totalSteps': 19200, 'rewardStep': 0.8188927325834936, 'errorList': [], 'lossList': [0.0, -1.2134738498926163, 0.0, 2.7478890727460383, 0.0, 0.0, 0.0], 'rewardMean': 0.7378547137049176, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.9095499424532
'totalSteps': 20480, 'rewardStep': 0.9149775388176472, 'errorList': [], 'lossList': [0.0, -1.170491578578949, 0.0, 1.731378209348768, 0.0, 0.0, 0.0], 'rewardMean': 0.7667266936414051, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.9623887551581
'totalSteps': 21760, 'rewardStep': 0.8707726575979425, 'errorList': [], 'lossList': [0.0, -1.1142069441080094, 0.0, 1.0496529357135296, 0.0, 0.0, 0.0], 'rewardMean': 0.831956508320226, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.8889767991882
'totalSteps': 23040, 'rewardStep': 0.97430101821095, 'errorList': [0.09777957147483514, 0.18088822109631608, 0.1177357976468607, 0.09669788578009417, 0.07536440878274181, 0.14844899634327302, 0.08278466159006435, 0.12157238970278521, 0.11520347516341438, 0.08458909773993813, 0.13527755334724145, 0.10721550962401989, 0.1469185479203366, 0.08715695911341624, 0.074623417061736, 0.09075545660848716, 0.15578562077449903, 0.14559491795215732, 0.16317526296291401, 0.10510443501073222, 0.06786533273662639, 0.11072907532525526, 0.09318603976579766, 0.1465259235327715, 0.0886197104066203, 0.16397926082222872, 0.11307939018550826, 0.10963794005639382, 0.09823383125572865, 0.08957663844565172, 0.14065147924444354, 0.13006613898395294, 0.18559247382585203, 0.08438323948266124, 0.13746934626574756, 0.0730900257819042, 0.12055726960838647, 0.09908788386314976, 0.1333382257330995, 0.15751966079632954, 0.1261223013549583, 0.12033398477664904, 0.1256059408544102, 0.0938540762637848, 0.13409093467794642, 0.16179598876311593, 0.1578792158960725, 0.09221809340010231, 0.13951092505908202, 0.14198712446377215], 'lossList': [0.0, -1.0800116169452667, 0.0, 0.7952456094138324, 0.0, 0.0, 0.0], 'rewardMean': 0.844754687024072, 'totalEpisodes': 103, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1203.553756434244, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=79.77
