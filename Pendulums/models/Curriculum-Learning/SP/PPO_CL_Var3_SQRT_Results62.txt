#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 62
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.6122470103906809, 'errorList': [], 'lossList': [0.0, -1.4371828830242157, 0.0, 33.930106248855594, 0.0, 0.0, 0.0], 'rewardMean': 0.5251388012982774, 'totalEpisodes': 26, 'stepsPerEpisode': 118, 'rewardPerEpisode': 85.85335962505904
'totalSteps': 3840, 'rewardStep': 0.7407727989631131, 'errorList': [], 'lossList': [0.0, -1.4185161018371581, 0.0, 41.0012508392334, 0.0, 0.0, 0.0], 'rewardMean': 0.5970168005198894, 'totalEpisodes': 46, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.588501830171197
'totalSteps': 5120, 'rewardStep': 0.6661611433571221, 'errorList': [], 'lossList': [0.0, -1.4002399355173112, 0.0, 55.09577304840088, 0.0, 0.0, 0.0], 'rewardMean': 0.6143028862291975, 'totalEpisodes': 67, 'stepsPerEpisode': 45, 'rewardPerEpisode': 36.75328416966718
'totalSteps': 6400, 'rewardStep': 0.8446115800909301, 'errorList': [], 'lossList': [0.0, -1.3861056238412857, 0.0, 82.0963070678711, 0.0, 0.0, 0.0], 'rewardMean': 0.660364625001544, 'totalEpisodes': 90, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8446115800909301
'totalSteps': 7680, 'rewardStep': 0.8672485595999765, 'errorList': [], 'lossList': [0.0, -1.3678508841991424, 0.0, 94.77724529266358, 0.0, 0.0, 0.0], 'rewardMean': 0.6948452807679494, 'totalEpisodes': 119, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.477552898768103
'totalSteps': 8960, 'rewardStep': 0.8098165289655818, 'errorList': [], 'lossList': [0.0, -1.3676017731428147, 0.0, 36.557905216217044, 0.0, 0.0, 0.0], 'rewardMean': 0.7112697447961827, 'totalEpisodes': 134, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.939269693080906
'totalSteps': 10240, 'rewardStep': 0.47942706736440155, 'errorList': [], 'lossList': [0.0, -1.354137722849846, 0.0, 23.911966054439546, 0.0, 0.0, 0.0], 'rewardMean': 0.68228941011721, 'totalEpisodes': 143, 'stepsPerEpisode': 252, 'rewardPerEpisode': 173.89290897316553
'totalSteps': 11520, 'rewardStep': 0.6410615087928737, 'errorList': [], 'lossList': [0.0, -1.340950489640236, 0.0, 13.778898190259934, 0.0, 0.0, 0.0], 'rewardMean': 0.6777085321922838, 'totalEpisodes': 149, 'stepsPerEpisode': 131, 'rewardPerEpisode': 90.9715560880059
'totalSteps': 12800, 'rewardStep': 0.8576752116477272, 'errorList': [], 'lossList': [0.0, -1.342921218276024, 0.0, 20.30626781463623, 0.0, 0.0, 0.0], 'rewardMean': 0.6957052001378281, 'totalEpisodes': 156, 'stepsPerEpisode': 240, 'rewardPerEpisode': 195.49136128028306
'totalSteps': 14080, 'rewardStep': 0.4601545203537895, 'errorList': [], 'lossList': [0.0, -1.3683703207969666, 0.0, 5.375535356998443, 0.0, 0.0, 0.0], 'rewardMean': 0.6979175929526196, 'totalEpisodes': 160, 'stepsPerEpisode': 338, 'rewardPerEpisode': 266.1936482736939
'totalSteps': 15360, 'rewardStep': 0.7364698281449207, 'errorList': [], 'lossList': [0.0, -1.386384950876236, 0.0, 25.371064997911454, 0.0, 0.0, 0.0], 'rewardMean': 0.7103398747280437, 'totalEpisodes': 164, 'stepsPerEpisode': 742, 'rewardPerEpisode': 616.3511130425139
'totalSteps': 16640, 'rewardStep': 0.8550392218668726, 'errorList': [], 'lossList': [0.0, -1.375611207485199, 0.0, 6.25208929002285, 0.0, 0.0, 0.0], 'rewardMean': 0.7217665170184195, 'totalEpisodes': 166, 'stepsPerEpisode': 479, 'rewardPerEpisode': 372.3745476154151
'totalSteps': 17920, 'rewardStep': 0.5622075431181441, 'errorList': [], 'lossList': [0.0, -1.3659104377031326, 0.0, 3.304363534152508, 0.0, 0.0, 0.0], 'rewardMean': 0.7113711569945218, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 969.8787100427253
'totalSteps': 19200, 'rewardStep': 0.8488849944173273, 'errorList': [], 'lossList': [0.0, -1.3382200056314468, 0.0, 2.315876244753599, 0.0, 0.0, 0.0], 'rewardMean': 0.7117984984271615, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.0014144855538
'totalSteps': 20480, 'rewardStep': 0.9350853898082292, 'errorList': [0.06105719595956699, 0.04196711024033681, 0.10277742386564008, 0.04475211152764512, 0.09873365129864775, 0.08711665638259342, 0.08027354418740047, 0.08672032653130313, 0.05165329471439227, 0.05991824951478936, 0.04843632425665849, 0.0639241125011718, 0.05848663842983446, 0.05919343928926566, 0.08885311782167682, 0.038606221468201696, 0.08485856759222052, 0.04231813108579561, 0.0465068318699261, 0.05085598494559785, 0.040836726890313715, 0.07431693284750338, 0.07715546893369876, 0.05756208349488857, 0.041129459867880136, 0.05970781891671592, 0.08502059671470427, 0.07052248203514026, 0.06410932521824557, 0.08347918956689984, 0.05203627727256875, 0.05676158069655965, 0.061530127345677914, 0.056079968750524914, 0.05505165294269103, 0.05982190425473275, 0.07970241052888329, 0.06167384295251071, 0.06431724945487662, 0.061521909705389004, 0.04650356031876761, 0.07122841462923069, 0.07608619296363259, 0.097459328889499, 0.04525455804177664, 0.08510131079407182, 0.03885129867346598, 0.08791936294084934, 0.04892877119339846, 0.04393449671835413], 'lossList': [0.0, -1.2857815057039261, 0.0, 1.964900923371315, 0.0, 0.0, 0.0], 'rewardMean': 0.7185821814479867, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.2719156084872, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=71.99
