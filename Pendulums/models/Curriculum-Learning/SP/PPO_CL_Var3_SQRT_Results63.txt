#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 63
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6433325997444477, 'errorList': [], 'lossList': [0.0, -1.4111486846208572, 0.0, 30.186520018577575, 0.0, 0.0, 0.0], 'rewardMean': 0.7043292163457519, 'totalEpisodes': 27, 'stepsPerEpisode': 35, 'rewardPerEpisode': 27.522314155903803
'totalSteps': 3840, 'rewardStep': 0.906166051115722, 'errorList': [], 'lossList': [0.0, -1.392542375922203, 0.0, 39.73864461898804, 0.0, 0.0, 0.0], 'rewardMean': 0.7716081612690752, 'totalEpisodes': 43, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.75908938685611
'totalSteps': 5120, 'rewardStep': 0.9026668213478123, 'errorList': [], 'lossList': [0.0, -1.390822873711586, 0.0, 51.03405547142029, 0.0, 0.0, 0.0], 'rewardMean': 0.8043728262887595, 'totalEpisodes': 56, 'stepsPerEpisode': 74, 'rewardPerEpisode': 64.8922881878197
'totalSteps': 6400, 'rewardStep': 0.9530930095540863, 'errorList': [], 'lossList': [0.0, -1.3895406770706176, 0.0, 90.28441873550415, 0.0, 0.0, 0.0], 'rewardMean': 0.8341168629418249, 'totalEpisodes': 75, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.98978959660491
'totalSteps': 7680, 'rewardStep': 0.8351097382176229, 'errorList': [], 'lossList': [0.0, -1.3718613195419311, 0.0, 118.4375006866455, 0.0, 0.0, 0.0], 'rewardMean': 0.834282342154458, 'totalEpisodes': 103, 'stepsPerEpisode': 53, 'rewardPerEpisode': 45.138140367854525
'totalSteps': 8960, 'rewardStep': 0.7055566050508862, 'errorList': [], 'lossList': [0.0, -1.3528552430868148, 0.0, 65.61143948554992, 0.0, 0.0, 0.0], 'rewardMean': 0.815892951139662, 'totalEpisodes': 121, 'stepsPerEpisode': 70, 'rewardPerEpisode': 56.11741189174742
'totalSteps': 10240, 'rewardStep': 0.6437262177630929, 'errorList': [], 'lossList': [0.0, -1.3376885414123536, 0.0, 27.319080691337586, 0.0, 0.0, 0.0], 'rewardMean': 0.7943721094675908, 'totalEpisodes': 130, 'stepsPerEpisode': 163, 'rewardPerEpisode': 135.80072148205798
'totalSteps': 11520, 'rewardStep': 0.9241237384682536, 'errorList': [], 'lossList': [0.0, -1.3174775952100755, 0.0, 13.615765138864518, 0.0, 0.0, 0.0], 'rewardMean': 0.8087889571343312, 'totalEpisodes': 135, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.662098009075743
'totalSteps': 12800, 'rewardStep': 0.7643001774178926, 'errorList': [], 'lossList': [0.0, -1.2916346454620362, 0.0, 16.01226126194, 0.0, 0.0, 0.0], 'rewardMean': 0.8043400791626872, 'totalEpisodes': 136, 'stepsPerEpisode': 816, 'rewardPerEpisode': 664.0170306843379
'totalSteps': 14080, 'rewardStep': 0.7955100370295395, 'errorList': [], 'lossList': [0.0, -1.2305308562517165, 0.0, 9.168027637004853, 0.0, 0.0, 0.0], 'rewardMean': 0.8073584995709355, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 886.2263528736897
'totalSteps': 15360, 'rewardStep': 0.7442559880788405, 'errorList': [], 'lossList': [0.0, -1.1847140675783157, 0.0, 7.134776241183281, 0.0, 0.0, 0.0], 'rewardMean': 0.8174508384043749, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.173472995689
'totalSteps': 16640, 'rewardStep': 0.6866129458814068, 'errorList': [], 'lossList': [0.0, -1.1688216906785964, 0.0, 5.587188234329224, 0.0, 0.0, 0.0], 'rewardMean': 0.7954955278809435, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.954796821885
'totalSteps': 17920, 'rewardStep': 0.9116917365589764, 'errorList': [], 'lossList': [0.0, -1.158153978586197, 0.0, 4.688477826490998, 0.0, 0.0, 0.0], 'rewardMean': 0.7963980194020598, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.1470432271224
'totalSteps': 19200, 'rewardStep': 0.8651957800104675, 'errorList': [], 'lossList': [0.0, -1.122297413945198, 0.0, 3.275153125077486, 0.0, 0.0, 0.0], 'rewardMean': 0.7876082964476979, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.3424801766791
'totalSteps': 20480, 'rewardStep': 0.9451644970765164, 'errorList': [0.0297085358832161, 0.05689589524257396, 0.07914089024833945, 0.08325449764267488, 0.013664740013805961, 0.04218534234593012, 0.08029975081862693, 0.040713792250183935, 0.03683009554932176, 0.04988097790460523, 0.03878642931334886, 0.023546649435421434, 0.07034142724350084, 0.02905116317163379, 0.027909576186996517, 0.07096749033012542, 0.059724146116568184, 0.06177892998129233, 0.027041752599024336, 0.06781329251788372, 0.03515324679179993, 0.04679894049028059, 0.05650830362576463, 0.08445912411704802, 0.07150128231888646, 0.11588844415424616, 0.08533503264179233, 0.0937634535085616, 0.10279337872436678, 0.049425319510762206, 0.05042480646027638, 0.038502847246483396, 0.034712364919602424, 0.05356857866832768, 0.014598709785010024, 0.12049296106079314, 0.049294914636710055, 0.07987622122269702, 0.04158221177509861, 0.03137382048952763, 0.013638837967384486, 0.06179159143640231, 0.09346765674893243, 0.026150017049530145, 0.08448254373247245, 0.02772733428322422, 0.03627042352663432, 0.03000325170381904, 0.009927520793132218, 0.10390723223024258], 'lossList': [0.0, -1.0994735807180405, 0.0, 2.4628279107436537, 0.0, 0.0, 0.0], 'rewardMean': 0.7986137723335872, 'totalEpisodes': 136, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.2267623675514, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=70.29
