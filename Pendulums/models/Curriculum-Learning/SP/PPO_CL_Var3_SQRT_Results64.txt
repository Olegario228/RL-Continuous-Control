#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 64
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8598101524680305, 'errorList': [], 'lossList': [0.0, -1.4216133147478103, 0.0, 26.219116859436035, 0.0, 0.0, 0.0], 'rewardMean': 0.7658627793056869, 'totalEpisodes': 22, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.726881656426954
'totalSteps': 3840, 'rewardStep': 0.778154162468652, 'errorList': [], 'lossList': [0.0, -1.4268783885240555, 0.0, 43.86540143013001, 0.0, 0.0, 0.0], 'rewardMean': 0.7699599070266753, 'totalEpisodes': 47, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.564602519379314
'totalSteps': 5120, 'rewardStep': 0.9032243907224109, 'errorList': [], 'lossList': [0.0, -1.4269529163837433, 0.0, 38.23504683494568, 0.0, 0.0, 0.0], 'rewardMean': 0.8032760279506093, 'totalEpisodes': 62, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.755816949198746
'totalSteps': 6400, 'rewardStep': 0.8687874020544837, 'errorList': [], 'lossList': [0.0, -1.4216295140981674, 0.0, 45.472615509033204, 0.0, 0.0, 0.0], 'rewardMean': 0.816378302771384, 'totalEpisodes': 76, 'stepsPerEpisode': 36, 'rewardPerEpisode': 31.491910276720255
'totalSteps': 7680, 'rewardStep': 0.6725433484873894, 'errorList': [], 'lossList': [0.0, -1.4096379315853118, 0.0, 101.3907808303833, 0.0, 0.0, 0.0], 'rewardMean': 0.7924058103907182, 'totalEpisodes': 100, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.3511448412863
'totalSteps': 8960, 'rewardStep': 0.6694310729825074, 'errorList': [], 'lossList': [0.0, -1.4010282570123673, 0.0, 74.39177446365356, 0.0, 0.0, 0.0], 'rewardMean': 0.7748379907609738, 'totalEpisodes': 132, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.746184468895482
'totalSteps': 10240, 'rewardStep': 0.865145894347109, 'errorList': [], 'lossList': [0.0, -1.388198893070221, 0.0, 41.102059803009034, 0.0, 0.0, 0.0], 'rewardMean': 0.7861264787092408, 'totalEpisodes': 145, 'stepsPerEpisode': 112, 'rewardPerEpisode': 93.92998564387575
'totalSteps': 11520, 'rewardStep': 0.34254329134768585, 'errorList': [], 'lossList': [0.0, -1.3756027019023895, 0.0, 14.912587742805481, 0.0, 0.0, 0.0], 'rewardMean': 0.7368394578912902, 'totalEpisodes': 148, 'stepsPerEpisode': 537, 'rewardPerEpisode': 404.41266189021906
'totalSteps': 12800, 'rewardStep': 0.8349725453994626, 'errorList': [], 'lossList': [0.0, -1.3530803912878036, 0.0, 26.033063435554503, 0.0, 0.0, 0.0], 'rewardMean': 0.7466527666421074, 'totalEpisodes': 155, 'stepsPerEpisode': 63, 'rewardPerEpisode': 54.73061325801821
'totalSteps': 14080, 'rewardStep': 0.8782158737571735, 'errorList': [], 'lossList': [0.0, -1.3345608830451965, 0.0, 19.55503840923309, 0.0, 0.0, 0.0], 'rewardMean': 0.7672828134034905, 'totalEpisodes': 158, 'stepsPerEpisode': 54, 'rewardPerEpisode': 46.94736467374158
'totalSteps': 15360, 'rewardStep': 0.5350770700944142, 'errorList': [], 'lossList': [0.0, -1.319732694029808, 0.0, 27.626948018074035, 0.0, 0.0, 0.0], 'rewardMean': 0.7348095051661288, 'totalEpisodes': 163, 'stepsPerEpisode': 291, 'rewardPerEpisode': 206.55238951046272
'totalSteps': 16640, 'rewardStep': 0.6267240549798845, 'errorList': [], 'lossList': [0.0, -1.2940919649600984, 0.0, 11.873945524692536, 0.0, 0.0, 0.0], 'rewardMean': 0.7196664944172522, 'totalEpisodes': 165, 'stepsPerEpisode': 134, 'rewardPerEpisode': 108.98279934601071
'totalSteps': 17920, 'rewardStep': 0.6824574171279439, 'errorList': [], 'lossList': [0.0, -1.2609827280044557, 0.0, 7.817771774530411, 0.0, 0.0, 0.0], 'rewardMean': 0.6975897970578055, 'totalEpisodes': 166, 'stepsPerEpisode': 348, 'rewardPerEpisode': 298.88901456638393
'totalSteps': 19200, 'rewardStep': 0.6420145195632868, 'errorList': [], 'lossList': [0.0, -1.2437814044952393, 0.0, 3.8079891812801363, 0.0, 0.0, 0.0], 'rewardMean': 0.6749125088086857, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 924.6141209154853
'totalSteps': 20480, 'rewardStep': 0.8751647273117177, 'errorList': [], 'lossList': [0.0, -1.2179164046049118, 0.0, 3.4724887265264988, 0.0, 0.0, 0.0], 'rewardMean': 0.6951746466911185, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.6944933911002
'totalSteps': 21760, 'rewardStep': 0.9175043839976965, 'errorList': [], 'lossList': [0.0, -1.1705781900882721, 0.0, 2.2899046893417836, 0.0, 0.0, 0.0], 'rewardMean': 0.7199819777926375, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.6427783589795
'totalSteps': 23040, 'rewardStep': 0.8720645739998109, 'errorList': [], 'lossList': [0.0, -1.1630642092227936, 0.0, 1.6430027978122235, 0.0, 0.0, 0.0], 'rewardMean': 0.7206738457579077, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.5520191034893
'totalSteps': 24320, 'rewardStep': 0.922023742367448, 'errorList': [], 'lossList': [0.0, -1.14831776201725, 0.0, 1.5502960524521767, 0.0, 0.0, 0.0], 'rewardMean': 0.7786218908598839, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1203.310856333642
'totalSteps': 25600, 'rewardStep': 0.9914560825654277, 'errorList': [0.05612172602330365, 0.05721297213224146, 0.057005062048115965, 0.05463000214167159, 0.054512399612259406, 0.05452057180165871, 0.10819582881668321, 0.06606474686034443, 0.08533028915253127, 0.0810061638942512, 0.07943526039982225, 0.0843305031350908, 0.09018626846322592, 0.05756034077185339, 0.094251223409429, 0.08281207244409955, 0.05911014886884189, 0.12318479199774349, 0.05300013445410637, 0.05656339731696182, 0.08139282679051724, 0.0555392690265222, 0.07074181768904018, 0.09579945645038447, 0.05628366387098621, 0.055503297525909386, 0.055191653042624716, 0.05409160860821683, 0.05516299814762113, 0.053906491156502694, 0.09538695748715664, 0.056269309320617265, 0.054916005266005236, 0.05504472209426875, 0.05538527009522123, 0.057712432589797816, 0.10528413226358344, 0.05645107568001498, 0.057522122428310195, 0.05528747690989426, 0.056668107347412094, 0.055343448503481295, 0.05850823103473282, 0.057791632638067275, 0.053793412184670396, 0.11075012768193865, 0.05788929772855345, 0.08710382958025165, 0.09780977693614461, 0.07286154883879573], 'lossList': [0.0, -1.1301724714040757, 0.0, 1.1491060373745858, 0.0, 0.0, 0.0], 'rewardMean': 0.7942702445764803, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1217.4632395981987, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=83.96
