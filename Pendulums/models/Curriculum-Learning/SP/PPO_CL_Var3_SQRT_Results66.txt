#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 66
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.6352343784452039, 'errorList': [], 'lossList': [0.0, -1.4266061341762544, 0.0, 29.395146288871764, 0.0, 0.0, 0.0], 'rewardMean': 0.5969410484607796, 'totalEpisodes': 14, 'stepsPerEpisode': 105, 'rewardPerEpisode': 81.15837846330852
'totalSteps': 3840, 'rewardStep': 0.6726935119901307, 'errorList': [], 'lossList': [0.0, -1.4176122862100602, 0.0, 35.25902097225189, 0.0, 0.0, 0.0], 'rewardMean': 0.6221918696372299, 'totalEpisodes': 22, 'stepsPerEpisode': 380, 'rewardPerEpisode': 238.19402546544265
'totalSteps': 5120, 'rewardStep': 0.8189604317230068, 'errorList': [], 'lossList': [0.0, -1.4096490651369096, 0.0, 39.33245418071747, 0.0, 0.0, 0.0], 'rewardMean': 0.6713840101586741, 'totalEpisodes': 30, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.534409979587958
'totalSteps': 6400, 'rewardStep': 0.7576150317656243, 'errorList': [], 'lossList': [0.0, -1.4012514680624009, 0.0, 74.77538139343261, 0.0, 0.0, 0.0], 'rewardMean': 0.6886302144800641, 'totalEpisodes': 43, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.406804198797036
'totalSteps': 7680, 'rewardStep': 0.9744246040321531, 'errorList': [278.5268948613822, 269.3345523803766, 234.7715198966395, 283.12565360451777, 246.87365023024452, 287.0646953311716, 279.8684452123519, 204.87785042365184, 250.3521254666881, 287.5870606675309, 188.58891535429578, 250.63353442551636, 245.40289849442945, 152.2554813079175, 259.23563319349523, 276.27866744988495, 193.92622507584173, 165.1599444907467, 249.67846175408195, 240.4992701070307, 216.19172137221133, 222.68923368080206, 229.34663636250866, 234.3495843275102, 270.23648800552996, 274.1330276805769, 217.09324899676955, 261.7404160487067, 268.3765834250717, 153.3723187716163, 216.71993137396834, 74.77160277766055, 238.43413536879538, 264.26886606642756, 283.7334139715089, 95.94605440422998, 112.94101399700399, 152.09876090117692, 267.73469793884726, 255.57948836291948, 287.6129858957935, 128.04395070131025, 246.1615167478392, 307.4271935741599, 221.13115061553844, 230.24803387527004, 197.4241726827786, 224.61716867805717, 276.18514457899965, 266.0946103134289], 'lossList': [0.0, -1.3913462311029434, 0.0, 67.30046443939209, 0.0, 0.0, 0.0], 'rewardMean': 0.7362626127387456, 'totalEpisodes': 55, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.170459870313172, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.46422369818294285, 'errorList': [], 'lossList': [0.0, -1.3896177929639817, 0.0, 81.69592470169067, 0.0, 0.0, 0.0], 'rewardMean': 0.6973999106593453, 'totalEpisodes': 79, 'stepsPerEpisode': 40, 'rewardPerEpisode': 30.674672655760325
'totalSteps': 10240, 'rewardStep': 0.9779481441202909, 'errorList': [7.6995990190453725, 27.061316771190832, 8.841924935850251, 32.52645333318473, 13.765567736629402, 15.113980431263029, 23.47950870212995, 13.562120082905967, 11.332957771549992, 30.99057441848315, 7.857931774444244, 15.18335488900107, 21.5199040283863, 19.39030129270349, 12.977784245625372, 25.09499481573159, 39.26329874920126, 20.617551250741705, 18.73691960071142, 17.575332659470334, 25.890984223948358, 29.753725570398263, 24.931356466359127, 29.826230826017817, 11.24680977034955, 38.94431571248454, 21.473464664743997, 18.0161387459515, 20.899054894717132, 29.120803841434352, 2.2472164687507217, 21.405541181067743, 7.049872376585518, 2.5614581117025867, 48.90277141079626, 11.895195969733031, 24.276972995465346, 21.40022941255278, 13.113122205070713, 19.123330037020015, 2.6595766205839144, 29.83877141162075, 13.662427977614703, 22.42355186636722, 7.977050423537911, 10.212297149805835, 6.363438627572036, 20.1796180971205, 1.6010270530333126, 13.550323095338966], 'lossList': [0.0, -1.3892678231000901, 0.0, 19.390756096839905, 0.0, 0.0, 0.0], 'rewardMean': 0.7324684398419634, 'totalEpisodes': 87, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.002434039731495, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6786889720211464, 'errorList': [], 'lossList': [0.0, -1.3757794564962387, 0.0, 23.818691487312318, 0.0, 0.0, 0.0], 'rewardMean': 0.7264929434174282, 'totalEpisodes': 98, 'stepsPerEpisode': 78, 'rewardPerEpisode': 60.131283713251115
'totalSteps': 12800, 'rewardStep': 0.7461684490323941, 'errorList': [], 'lossList': [0.0, -1.346136248111725, 0.0, 10.386349308490754, 0.0, 0.0, 0.0], 'rewardMean': 0.7284604939789248, 'totalEpisodes': 104, 'stepsPerEpisode': 151, 'rewardPerEpisode': 106.82655917126944
'totalSteps': 14080, 'rewardStep': 0.7026373324932745, 'errorList': [], 'lossList': [0.0, -1.3306040900945664, 0.0, 7.414971605539322, 0.0, 0.0, 0.0], 'rewardMean': 0.7428594553806167, 'totalEpisodes': 107, 'stepsPerEpisode': 172, 'rewardPerEpisode': 145.49642046471521
'totalSteps': 15360, 'rewardStep': 0.4199274863608119, 'errorList': [], 'lossList': [0.0, -1.3506625139713286, 0.0, 5.486323707103729, 0.0, 0.0, 0.0], 'rewardMean': 0.7213287661721776, 'totalEpisodes': 110, 'stepsPerEpisode': 527, 'rewardPerEpisode': 382.2817041338293
'totalSteps': 16640, 'rewardStep': 0.6196155926302485, 'errorList': [], 'lossList': [0.0, -1.3330230259895324, 0.0, 5.442379167675972, 0.0, 0.0, 0.0], 'rewardMean': 0.7160209742361893, 'totalEpisodes': 111, 'stepsPerEpisode': 1273, 'rewardPerEpisode': 898.0761736355529
'totalSteps': 17920, 'rewardStep': 0.6957778225998742, 'errorList': [], 'lossList': [0.0, -1.3145541775226592, 0.0, 4.636457155942917, 0.0, 0.0, 0.0], 'rewardMean': 0.703702713323876, 'totalEpisodes': 115, 'stepsPerEpisode': 187, 'rewardPerEpisode': 155.63872515181308
'totalSteps': 19200, 'rewardStep': 0.7143039849868649, 'errorList': [], 'lossList': [0.0, -1.3164365595579148, 0.0, 4.611927362680436, 0.0, 0.0, 0.0], 'rewardMean': 0.6993716086460001, 'totalEpisodes': 117, 'stepsPerEpisode': 333, 'rewardPerEpisode': 229.8335771224312
'totalSteps': 20480, 'rewardStep': 0.7128585888628791, 'errorList': [], 'lossList': [0.0, -1.2754520791769028, 0.0, 3.4217617267370226, 0.0, 0.0, 0.0], 'rewardMean': 0.6732150071290727, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.0169830949978
'totalSteps': 21760, 'rewardStep': 0.6432867987030273, 'errorList': [], 'lossList': [0.0, -1.24153313934803, 0.0, 2.2775040912628173, 0.0, 0.0, 0.0], 'rewardMean': 0.6911213171810812, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1034.0376038822837
'totalSteps': 23040, 'rewardStep': 0.9296165417165383, 'errorList': [], 'lossList': [0.0, -1.2200889420509338, 0.0, 1.1819821760058402, 0.0, 0.0, 0.0], 'rewardMean': 0.6862881569407059, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.0987987724625
'totalSteps': 24320, 'rewardStep': 0.8018757606679713, 'errorList': [], 'lossList': [0.0, -1.163176376223564, 0.0, 0.9063250327482819, 0.0, 0.0, 0.0], 'rewardMean': 0.6986068358053885, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1144.953554714209
'totalSteps': 25600, 'rewardStep': 0.9601272111984718, 'errorList': [0.11133529671665467, 0.06246579740498562, 0.10503170912225537, 0.07863960086472327, 0.09855328208114193, 0.06612625741106887, 0.08502118549723775, 0.07734237297516892, 0.0898829021782467, 0.09742238950887042, 0.08125375781633502, 0.06082126968819049, 0.09271395817107476, 0.12165788128685412, 0.10518949917194274, 0.09727449323992929, 0.09841976382061098, 0.09052761173582734, 0.08365081495108537, 0.06031829549397372, 0.06820438903304371, 0.07443325346938434, 0.10559389922943296, 0.09794728789415774, 0.10462914280612157, 0.13189687054673596, 0.11963172475779828, 0.06586317850264589, 0.10173814298203586, 0.0943326574588349, 0.0700027189434653, 0.08794831076020618, 0.10079795620194415, 0.09173114599519913, 0.11046924404068885, 0.08405303361843637, 0.07812973633191878, 0.09621935596635772, 0.08019461210843212, 0.06946251398480925, 0.07644251301690798, 0.11563014354701465, 0.08326663908969617, 0.11138259379435858, 0.14724443755092176, 0.08047021282136037, 0.08270142614323424, 0.06948461503748032, 0.09966054945425971, 0.0770358340695144], 'lossList': [0.0, -1.102642341852188, 0.0, 0.7571067658811808, 0.0, 0.0, 0.0], 'rewardMean': 0.7200027120219962, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.4773512123063, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=124.16
