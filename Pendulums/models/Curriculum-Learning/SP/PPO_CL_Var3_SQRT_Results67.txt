#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 67
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.6910206233244295, 'errorList': [], 'lossList': [0.0, -1.4332380884885787, 0.0, 25.991313605308534, 0.0, 0.0, 0.0], 'rewardMean': 0.5844031992736138, 'totalEpisodes': 16, 'stepsPerEpisode': 134, 'rewardPerEpisode': 86.98488527433217
'totalSteps': 3840, 'rewardStep': 0.7468946261373005, 'errorList': [], 'lossList': [0.0, -1.4198949527740479, 0.0, 41.069943552017214, 0.0, 0.0, 0.0], 'rewardMean': 0.638567008228176, 'totalEpisodes': 27, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.991368085282623
'totalSteps': 5120, 'rewardStep': 0.17721943069362556, 'errorList': [], 'lossList': [0.0, -1.401133241057396, 0.0, 41.17620694160461, 0.0, 0.0, 0.0], 'rewardMean': 0.5232301138445384, 'totalEpisodes': 40, 'stepsPerEpisode': 66, 'rewardPerEpisode': 31.937024761089443
'totalSteps': 6400, 'rewardStep': 0.6599848749883191, 'errorList': [], 'lossList': [0.0, -1.40183134496212, 0.0, 56.93144762992859, 0.0, 0.0, 0.0], 'rewardMean': 0.5505810660732946, 'totalEpisodes': 51, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.956878458398226
'totalSteps': 7680, 'rewardStep': 0.942443617703175, 'errorList': [104.43656011440684, 200.6287373652809, 134.92005899388852, 296.76479073310907, 50.857665763253436, 167.33878001019153, 173.1081526808503, 180.57122684441225, 161.67801109640354, 119.22937012197026, 87.31865706236704, 140.84731632144658, 297.29716264578695, 138.18695421853772, 92.10514628435118, 30.881519233003125, 149.4874784665714, 128.7238281938539, 155.73930039256857, 142.2998176278149, 80.5092298998054, 172.28464083864478, 65.72983782837662, 127.72902407871666, 192.52306542010177, 258.2553492783087, 155.00735646810028, 94.17881996441189, 180.6012929172267, 75.66091796982197, 54.69964932892616, 89.35146746571554, 153.409555716646, 164.19917838641632, 271.8309573380176, 68.07282520629809, 68.41398880846194, 143.22825834417566, 194.6218964210865, 28.784141307569584, 137.44066685918156, 148.4050061546919, 68.0704383245496, 142.1604783742149, 152.08360326168008, 140.26745660526493, 172.1402055292298, 66.50435520397546, 95.06621371155225, 149.89544841492219], 'lossList': [0.0, -1.3966145026683807, 0.0, 110.6291138458252, 0.0, 0.0, 0.0], 'rewardMean': 0.6158914913449413, 'totalEpisodes': 74, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.139762011446875, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.5989478081423376, 'errorList': [], 'lossList': [0.0, -1.3902286088466644, 0.0, 64.07557447433472, 0.0, 0.0, 0.0], 'rewardMean': 0.6134709651731408, 'totalEpisodes': 90, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.009953919116935
'totalSteps': 10240, 'rewardStep': 0.685032614551377, 'errorList': [], 'lossList': [0.0, -1.3863221353292465, 0.0, 50.67070322036743, 0.0, 0.0, 0.0], 'rewardMean': 0.6224161713454204, 'totalEpisodes': 102, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.373726088833274
'totalSteps': 11520, 'rewardStep': 0.7293931509960189, 'errorList': [], 'lossList': [0.0, -1.390161292552948, 0.0, 26.149849462509156, 0.0, 0.0, 0.0], 'rewardMean': 0.6343025024177091, 'totalEpisodes': 110, 'stepsPerEpisode': 55, 'rewardPerEpisode': 43.88171540116951
'totalSteps': 12800, 'rewardStep': 0.8943135536199164, 'errorList': [], 'lossList': [0.0, -1.3962914669513702, 0.0, 41.71357053518295, 0.0, 0.0, 0.0], 'rewardMean': 0.6603036075379298, 'totalEpisodes': 117, 'stepsPerEpisode': 143, 'rewardPerEpisode': 120.55148987017827
'totalSteps': 14080, 'rewardStep': 0.7969222930082167, 'errorList': [], 'lossList': [0.0, -1.4143441557884215, 0.0, 9.026593594551086, 0.0, 0.0, 0.0], 'rewardMean': 0.6922172593164717, 'totalEpisodes': 121, 'stepsPerEpisode': 86, 'rewardPerEpisode': 68.23679839734828
'totalSteps': 15360, 'rewardStep': 0.872613273758687, 'errorList': [], 'lossList': [0.0, -1.4289660012722016, 0.0, 6.258206515312195, 0.0, 0.0, 0.0], 'rewardMean': 0.7103765243598974, 'totalEpisodes': 125, 'stepsPerEpisode': 352, 'rewardPerEpisode': 238.26559308913073
'totalSteps': 16640, 'rewardStep': 0.5150510848893267, 'errorList': [], 'lossList': [0.0, -1.4262217837572098, 0.0, 5.881898255348205, 0.0, 0.0, 0.0], 'rewardMean': 0.6871921702350999, 'totalEpisodes': 127, 'stepsPerEpisode': 306, 'rewardPerEpisode': 226.76120838478818
'totalSteps': 17920, 'rewardStep': 0.6592182983045226, 'errorList': [], 'lossList': [0.0, -1.420485904812813, 0.0, 4.671011860370636, 0.0, 0.0, 0.0], 'rewardMean': 0.7353920569961897, 'totalEpisodes': 128, 'stepsPerEpisode': 1261, 'rewardPerEpisode': 853.4195814072684
'totalSteps': 19200, 'rewardStep': 0.9443421119674167, 'errorList': [0.14502549962481381, 0.2161028802592486, 0.1573053154531636, 0.17022938960482908, 0.20803451328899877, 0.2005324235101033, 0.15649008408629275, 0.19122694573458615, 0.18917557195418563, 0.19966208545479425, 0.24178303093095857, 0.20276999216402763, 0.14412557234852189, 0.19446531345545273, 0.18764104541505716, 0.1560576900363017, 0.128432813596465, 0.16115604705293168, 0.1557016186699409, 0.19412162743477007, 0.19278422287808342, 0.17387082405671486, 0.13605744793520763, 0.17546230953538525, 0.1841587891941012, 0.2179304286077571, 0.16970448010899736, 0.1670055940899968, 0.24419031111692527, 0.17334114549666801, 0.22391589620960872, 0.20172779325598786, 0.17040975425849827, 0.15676698560131092, 0.2118451975252863, 0.13083775348652898, 0.18241124619759452, 0.1844788725000679, 0.18894512106231717, 0.18758938861550709, 0.17900279886818518, 0.14525550372251195, 0.14566116640201981, 0.18084451841775986, 0.1662117212861177, 0.15776851090351846, 0.18360266534495076, 0.2501888861828453, 0.16735494987632824, 0.17970223925371578], 'lossList': [0.0, -1.4009401202201843, 0.0, 3.6817620438337326, 0.0, 0.0, 0.0], 'rewardMean': 0.7638277806940994, 'totalEpisodes': 129, 'stepsPerEpisode': 772, 'rewardPerEpisode': 628.8179862008125, 'successfulTests': 39
'totalSteps': 20480, 'rewardStep': 0.8368129860165432, 'errorList': [], 'lossList': [0.0, -1.3786437815427781, 0.0, 2.1279597342014314, 0.0, 0.0, 0.0], 'rewardMean': 0.7532647175254363, 'totalEpisodes': 129, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1041.2238166870288
'totalSteps': 21760, 'rewardStep': 0.7793651515543933, 'errorList': [], 'lossList': [0.0, -1.3381124633550643, 0.0, 2.002239573299885, 0.0, 0.0, 0.0], 'rewardMean': 0.7713064518666418, 'totalEpisodes': 129, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1114.7536000830016
'totalSteps': 23040, 'rewardStep': 0.9629863909693681, 'errorList': [0.34033599020411687, 0.19770329732828978, 0.15161727500653685, 0.3212299681904876, 0.26772584910345165, 0.3068490234119197, 0.23953940956802627, 0.29796814169900837, 0.25759639395611317, 0.2024984767651669, 0.20160627365765751, 0.18545334129291213, 0.23601702629616458, 0.2560068509104278, 0.21731725722432743, 0.1786223451996466, 0.27138474999934603, 0.19549266448420546, 0.2718474712548949, 0.20450472348568333, 0.24823686122725674, 0.27917962053580614, 0.22205859361856667, 0.31179365968187556, 0.2167064704129152, 0.26165861514488686, 0.2198052783196314, 0.20864199748313964, 0.2309031416774476, 0.2655195728378104, 0.21556533096303057, 0.20091437809316276, 0.22220934179571844, 0.19842178740661143, 0.20626975792455024, 0.25031489065181073, 0.3099207483770761, 0.24991420673078038, 0.2185088523437011, 0.2959515299534598, 0.191277881650392, 0.22423614568141317, 0.2534998779358527, 0.19985875084468924, 0.23471110005954324, 0.2903041339305948, 0.14895222436415767, 0.19505970206541715, 0.1430472545761696, 0.24915742054862003], 'lossList': [0.0, -1.3007586586475373, 0.0, 1.7868093487247825, 0.0, 0.0, 0.0], 'rewardMean': 0.799101829508441, 'totalEpisodes': 129, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.109247608875, 'successfulTests': 11
'totalSteps': 24320, 'rewardStep': 0.8395905701181234, 'errorList': [], 'lossList': [0.0, -1.2744769567251206, 0.0, 0.9522336140275002, 0.0, 0.0, 0.0], 'rewardMean': 0.8101215714206514, 'totalEpisodes': 129, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.587215917806
'totalSteps': 25600, 'rewardStep': 0.8571809773337624, 'errorList': [], 'lossList': [0.0, -1.2533636480569839, 0.0, 0.7553657495975494, 0.0, 0.0, 0.0], 'rewardMean': 0.8064083137920361, 'totalEpisodes': 129, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.7009155236062
#maxSuccessfulTests=39, maxSuccessfulTestsAtStep=19200, timeSpent=126.29
