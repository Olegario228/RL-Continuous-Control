#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 68
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5789189912537807, 'errorList': [], 'lossList': [0.0, -1.4244653338193893, 0.0, 22.46688851594925, 0.0, 0.0, 0.0], 'rewardMean': 0.7185655499046602, 'totalEpisodes': 17, 'stepsPerEpisode': 40, 'rewardPerEpisode': 27.112578391192706
'totalSteps': 3840, 'rewardStep': 0.8669919691796841, 'errorList': [], 'lossList': [0.0, -1.417253879904747, 0.0, 44.698237085342406, 0.0, 0.0, 0.0], 'rewardMean': 0.7680410229963348, 'totalEpisodes': 28, 'stepsPerEpisode': 226, 'rewardPerEpisode': 162.5863519941345
'totalSteps': 5120, 'rewardStep': 0.7795547538477989, 'errorList': [], 'lossList': [0.0, -1.4143684101104737, 0.0, 55.0213340473175, 0.0, 0.0, 0.0], 'rewardMean': 0.7709194557092008, 'totalEpisodes': 37, 'stepsPerEpisode': 77, 'rewardPerEpisode': 67.05945167330978
'totalSteps': 6400, 'rewardStep': 0.73012683040956, 'errorList': [], 'lossList': [0.0, -1.4178485107421874, 0.0, 95.61254940032958, 0.0, 0.0, 0.0], 'rewardMean': 0.7627609306492726, 'totalEpisodes': 58, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.229209232951684
'totalSteps': 7680, 'rewardStep': 0.7430168837575111, 'errorList': [], 'lossList': [0.0, -1.401424230337143, 0.0, 118.3511287689209, 0.0, 0.0, 0.0], 'rewardMean': 0.7594702561673125, 'totalEpisodes': 90, 'stepsPerEpisode': 52, 'rewardPerEpisode': 43.64124577081322
'totalSteps': 8960, 'rewardStep': 0.6244719930018762, 'errorList': [], 'lossList': [0.0, -1.391961289048195, 0.0, 101.92709980010986, 0.0, 0.0, 0.0], 'rewardMean': 0.7401847900008215, 'totalEpisodes': 123, 'stepsPerEpisode': 40, 'rewardPerEpisode': 27.408216613994156
'totalSteps': 10240, 'rewardStep': 0.9275475607292853, 'errorList': [], 'lossList': [0.0, -1.3918241411447525, 0.0, 57.32731483459472, 0.0, 0.0, 0.0], 'rewardMean': 0.7636051363418794, 'totalEpisodes': 135, 'stepsPerEpisode': 138, 'rewardPerEpisode': 128.24344422958112
'totalSteps': 11520, 'rewardStep': 0.8230483350057636, 'errorList': [], 'lossList': [0.0, -1.3863213557004928, 0.0, 45.86715807914734, 0.0, 0.0, 0.0], 'rewardMean': 0.7702099361934222, 'totalEpisodes': 144, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.0510339415921885
'totalSteps': 12800, 'rewardStep': 0.25518524015802224, 'errorList': [], 'lossList': [0.0, -1.3900599247217178, 0.0, 27.750174102783202, 0.0, 0.0, 0.0], 'rewardMean': 0.7187074665898822, 'totalEpisodes': 149, 'stepsPerEpisode': 121, 'rewardPerEpisode': 81.90358907124028
'totalSteps': 14080, 'rewardStep': 0.809105279139397, 'errorList': [], 'lossList': [0.0, -1.3713093501329423, 0.0, 31.72470287322998, 0.0, 0.0, 0.0], 'rewardMean': 0.7137967836482678, 'totalEpisodes': 158, 'stepsPerEpisode': 105, 'rewardPerEpisode': 80.02836242636783
'totalSteps': 15360, 'rewardStep': 0.5678284081402849, 'errorList': [], 'lossList': [0.0, -1.3471989065408707, 0.0, 11.50115509033203, 0.0, 0.0, 0.0], 'rewardMean': 0.7126877253369183, 'totalEpisodes': 165, 'stepsPerEpisode': 106, 'rewardPerEpisode': 81.59730330779331
'totalSteps': 16640, 'rewardStep': 0.9122621999465165, 'errorList': [], 'lossList': [0.0, -1.3328551793098449, 0.0, 6.365422010421753, 0.0, 0.0, 0.0], 'rewardMean': 0.7172147484136016, 'totalEpisodes': 170, 'stepsPerEpisode': 99, 'rewardPerEpisode': 86.96002834779817
'totalSteps': 17920, 'rewardStep': 0.8353735194304179, 'errorList': [], 'lossList': [0.0, -1.3134449857473374, 0.0, 6.645075773000717, 0.0, 0.0, 0.0], 'rewardMean': 0.7227966249718634, 'totalEpisodes': 175, 'stepsPerEpisode': 96, 'rewardPerEpisode': 81.9518935601581
'totalSteps': 19200, 'rewardStep': 0.8088787200220642, 'errorList': [], 'lossList': [0.0, -1.3131668657064437, 0.0, 6.816608006954193, 0.0, 0.0, 0.0], 'rewardMean': 0.730671813933114, 'totalEpisodes': 180, 'stepsPerEpisode': 184, 'rewardPerEpisode': 154.8933937494194
'totalSteps': 20480, 'rewardStep': 0.9769397297024208, 'errorList': [0.3822209791997087, 0.38382009758509594, 0.3723739111672727, 0.3838073670613878, 0.3730943444989551, 0.38745371729143246, 0.5301685399804129, 0.3802430223481595, 0.46995455639959105, 0.37732850491782244, 0.38254737090084784, 0.3899265833349795, 0.3780988041916695, 0.3747507691213504, 0.37722084853420473, 0.37310827774858335, 0.3825511543457255, 0.4100275893287829, 0.3795203458943708, 0.43675010917983903, 0.3802477791269012, 0.37459538282809035, 0.4239667727679599, 0.3768708175028528, 0.37451869366562845, 0.37769920498131954, 0.3761660041702663, 0.38138236415946314, 0.37507685197112106, 0.37762709488345103, 0.38197823025148153, 0.3904284704797035, 0.38275688525402224, 0.3721424815103819, 0.3822585438587116, 0.3717059384627558, 0.45564058953103864, 0.3846209981746881, 0.3725417863784408, 0.4604541797505619, 0.39683619474629805, 0.36879423548182333, 0.3730497110920335, 0.38428423396133027, 0.3841206641290524, 0.37305484679120765, 0.3750945307331041, 0.38283042172697057, 0.3814377100591553, 0.4270554003444783], 'lossList': [0.0, -1.3082983082532882, 0.0, 5.589770202636719, 0.0, 0.0, 0.0], 'rewardMean': 0.7540640985276048, 'totalEpisodes': 183, 'stepsPerEpisode': 130, 'rewardPerEpisode': 104.94859871388896, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8345276988412499, 'errorList': [], 'lossList': [0.0, -1.3104041761159897, 0.0, 4.516177214384079, 0.0, 0.0, 0.0], 'rewardMean': 0.7750696691115423, 'totalEpisodes': 184, 'stepsPerEpisode': 831, 'rewardPerEpisode': 630.0990647944882
'totalSteps': 23040, 'rewardStep': 0.687577100905567, 'errorList': [], 'lossList': [0.0, -1.3005962771177293, 0.0, 1.5368572917580605, 0.0, 0.0, 0.0], 'rewardMean': 0.7510726231291704, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.4415126111082
'totalSteps': 24320, 'rewardStep': 0.8053488663966938, 'errorList': [], 'lossList': [0.0, -1.2645393282175064, 0.0, 1.6797006975114346, 0.0, 0.0, 0.0], 'rewardMean': 0.7493026762682634, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.318682614728
'totalSteps': 25600, 'rewardStep': 0.9637860338718677, 'errorList': [0.043161625016376166, 0.028068868929930543, 0.02762872816956085, 0.0330006127755482, 0.039357477804012594, 0.027618763676235707, 0.0496091723506264, 0.027376475414879167, 0.037169284236814815, 0.03375859865296094, 0.041253781001073325, 0.027214881660409588, 0.029020932626457645, 0.04586180120086155, 0.03778168719671938, 0.03158280035953285, 0.04071829923558623, 0.03748029019924267, 0.036846538819235886, 0.04159130084544474, 0.03206257905733924, 0.042007363399447474, 0.0271065592321528, 0.03251049139506645, 0.030983828799269256, 0.027617083359382993, 0.028524286906403212, 0.038790554409703436, 0.028722800320039397, 0.042430025642691944, 0.03974367350811105, 0.027242605834177605, 0.03958518398021896, 0.03723142527117019, 0.03982340415751028, 0.029360400728408752, 0.03888622175711494, 0.03658865847700124, 0.032826820715255736, 0.033176444862994724, 0.03318201272718957, 0.033140145024094174, 0.03558777337797524, 0.046992535904029714, 0.03269970770936745, 0.033887514754868904, 0.04019746564158701, 0.04588203274416217, 0.04675115398020126, 0.047925817632452544], 'lossList': [0.0, -1.242277393937111, 0.0, 1.5366870289668442, 0.0, 0.0, 0.0], 'rewardMean': 0.8201627556396479, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.9058508835083, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.76
