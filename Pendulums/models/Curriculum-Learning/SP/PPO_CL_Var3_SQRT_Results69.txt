#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 69
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8747083553239553, 'errorList': [], 'lossList': [0.0, -1.4243912357091904, 0.0, 28.95387493610382, 0.0, 0.0, 0.0], 'rewardMean': 0.7990296396016281, 'totalEpisodes': 16, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.54355150548645
'totalSteps': 3840, 'rewardStep': 0.728165044295928, 'errorList': [], 'lossList': [0.0, -1.4305305474996566, 0.0, 46.51994418144226, 0.0, 0.0, 0.0], 'rewardMean': 0.7754081078330614, 'totalEpisodes': 29, 'stepsPerEpisode': 183, 'rewardPerEpisode': 122.36109239389715
'totalSteps': 5120, 'rewardStep': 0.782827563280278, 'errorList': [], 'lossList': [0.0, -1.4203640973567964, 0.0, 49.29129451751709, 0.0, 0.0, 0.0], 'rewardMean': 0.7772629716948656, 'totalEpisodes': 42, 'stepsPerEpisode': 43, 'rewardPerEpisode': 37.875366360679735
'totalSteps': 6400, 'rewardStep': 0.8127032149586583, 'errorList': [], 'lossList': [0.0, -1.4050466418266296, 0.0, 63.243489294052125, 0.0, 0.0, 0.0], 'rewardMean': 0.7843510203476242, 'totalEpisodes': 53, 'stepsPerEpisode': 274, 'rewardPerEpisode': 209.50486196717745
'totalSteps': 7680, 'rewardStep': 0.5490656587782065, 'errorList': [], 'lossList': [0.0, -1.386047551035881, 0.0, 116.26950244903564, 0.0, 0.0, 0.0], 'rewardMean': 0.7451367934193879, 'totalEpisodes': 75, 'stepsPerEpisode': 38, 'rewardPerEpisode': 29.558610181444877
'totalSteps': 8960, 'rewardStep': 0.7314503753859067, 'errorList': [], 'lossList': [0.0, -1.3826203656196594, 0.0, 102.2290965461731, 0.0, 0.0, 0.0], 'rewardMean': 0.7431815908431763, 'totalEpisodes': 107, 'stepsPerEpisode': 28, 'rewardPerEpisode': 20.014214425167136
'totalSteps': 10240, 'rewardStep': 0.795876883762768, 'errorList': [], 'lossList': [0.0, -1.3724659758806228, 0.0, 46.19353067398071, 0.0, 0.0, 0.0], 'rewardMean': 0.7497685024581252, 'totalEpisodes': 120, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.804989874949056
'totalSteps': 11520, 'rewardStep': 0.5517951385004078, 'errorList': [], 'lossList': [0.0, -1.3660621100664139, 0.0, 12.144508627653122, 0.0, 0.0, 0.0], 'rewardMean': 0.7277714620183788, 'totalEpisodes': 124, 'stepsPerEpisode': 381, 'rewardPerEpisode': 260.6447990498917
'totalSteps': 12800, 'rewardStep': 0.7094303253085629, 'errorList': [], 'lossList': [0.0, -1.3579284262657165, 0.0, 29.92926808357239, 0.0, 0.0, 0.0], 'rewardMean': 0.7259373483473972, 'totalEpisodes': 132, 'stepsPerEpisode': 64, 'rewardPerEpisode': 53.963407416102584
'totalSteps': 14080, 'rewardStep': 0.5956389270887175, 'errorList': [], 'lossList': [0.0, -1.3451057559251784, 0.0, 9.232967339158058, 0.0, 0.0, 0.0], 'rewardMean': 0.7131661486683388, 'totalEpisodes': 135, 'stepsPerEpisode': 513, 'rewardPerEpisode': 399.8144725643458
'totalSteps': 15360, 'rewardStep': 0.9889080495126032, 'errorList': [0.7560488451517434, 0.2406455067072394, 1.178373379742525, 0.6557030596025638, 0.12028910646112836, 2.169456486355498, 0.03407327376329806, 0.7552317634782286, 2.0882504014846015, 1.796865129138055, 1.8507967241452283, 2.434292161110839, 1.6768380632069715, 1.8458266416054658, 0.526243387601741, 0.6456423024451741, 1.3178086208820505, 0.11494548855857076, 1.52813322403486, 1.088448811039944, 0.7500466234344018, 1.278662579040004, 0.8480803103474261, 0.3746760005188886, 2.1586330021155753, 0.3832271384641038, 3.0390223663892093, 0.021294464176593544, 1.6573287808773416, 0.4953801098487682, 1.3868353864289653, 1.1534075419017618, 1.2851353853674066, 1.3081580031031825, 1.0060668597514715, 0.061332567488349324, 0.6170087528147612, 0.22869596704887787, 1.4175715993063775, 3.4273735784845076, 1.252280781695431, 3.0307983213672323, 0.6512007509394733, 0.6048270483208807, 1.202108587979997, 0.2572554283843791, 1.8904195706608413, 1.3771658129977096, 0.8666872324028944, 1.15377798079547], 'lossList': [0.0, -1.3445989364385604, 0.0, 16.555557287931443, 0.0, 0.0, 0.0], 'rewardMean': 0.7245861180872037, 'totalEpisodes': 138, 'stepsPerEpisode': 610, 'rewardPerEpisode': 539.5622846062961, 'successfulTests': 5
'totalSteps': 16640, 'rewardStep': 0.6380974448132405, 'errorList': [], 'lossList': [0.0, -1.3244500714540481, 0.0, 22.84360744714737, 0.0, 0.0, 0.0], 'rewardMean': 0.7155793581389349, 'totalEpisodes': 142, 'stepsPerEpisode': 139, 'rewardPerEpisode': 121.83018316961946
'totalSteps': 17920, 'rewardStep': 0.707591720355212, 'errorList': [], 'lossList': [0.0, -1.317091161608696, 0.0, 5.522794125676155, 0.0, 0.0, 0.0], 'rewardMean': 0.7080557738464284, 'totalEpisodes': 145, 'stepsPerEpisode': 146, 'rewardPerEpisode': 117.32660838089403
'totalSteps': 19200, 'rewardStep': 0.8624110029235008, 'errorList': [], 'lossList': [0.0, -1.3236529833078385, 0.0, 3.8394828218221666, 0.0, 0.0, 0.0], 'rewardMean': 0.7130265526429126, 'totalEpisodes': 146, 'stepsPerEpisode': 547, 'rewardPerEpisode': 388.0097225298244
'totalSteps': 20480, 'rewardStep': 0.8992623726830773, 'errorList': [], 'lossList': [0.0, -1.3231398683786393, 0.0, 3.0062235575914382, 0.0, 0.0, 0.0], 'rewardMean': 0.7480462240333997, 'totalEpisodes': 147, 'stepsPerEpisode': 203, 'rewardPerEpisode': 177.941277293017
'totalSteps': 21760, 'rewardStep': 0.7527759480971834, 'errorList': [], 'lossList': [0.0, -1.3088160455226898, 0.0, 1.854011721611023, 0.0, 0.0, 0.0], 'rewardMean': 0.7501787813045275, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.7083857530108
'totalSteps': 23040, 'rewardStep': 0.9199248465783707, 'errorList': [], 'lossList': [0.0, -1.2912377429008484, 0.0, 1.1672347406297923, 0.0, 0.0, 0.0], 'rewardMean': 0.7625835775860876, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.9792708593875
'totalSteps': 24320, 'rewardStep': 0.8759669300578793, 'errorList': [], 'lossList': [0.0, -1.2682013988494873, 0.0, 0.964188028909266, 0.0, 0.0, 0.0], 'rewardMean': 0.7950007567418348, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.0232297981772
'totalSteps': 25600, 'rewardStep': 0.948170344498282, 'errorList': [0.008179108338558196, 0.008118302375300113, 0.009716092553183557, 0.01121585625783292, 0.009362219066857152, 0.0074970573616498365, 0.007271895740553779, 0.006026683743494607, 0.007194437894323527, 0.011137616538519676, 0.010324556888208896, 0.006373783003383603, 0.011186890756677266, 0.01838791633209921, 0.00654818021441968, 0.01295200324274646, 0.006556723704071881, 0.01261023692830026, 0.015940460573528374, 0.012733600285228236, 0.025303220027684602, 0.018447244801399903, 0.01154013932779558, 0.006399842599786147, 0.022672553540238154, 0.008449232982730405, 0.014387489253702286, 0.01181196340421321, 0.011185626595107892, 0.012376669455212753, 0.007565689762370764, 0.013215189496430996, 0.020034372814121862, 0.025075278502775564, 0.018502345226614442, 0.0158057921879492, 0.01610901742511558, 0.02053183299131397, 0.006639856808352541, 0.018778188131588287, 0.01207666398067794, 0.013787074555281408, 0.016439267845343598, 0.01192748596387607, 0.019599216171690872, 0.03491175217827735, 0.008852388794105076, 0.01168558781107, 0.031232605389717786, 0.02372767363617097], 'lossList': [0.0, -1.236251128911972, 0.0, 0.8807740541547537, 0.0, 0.0, 0.0], 'rewardMean': 0.8188747586608066, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.5673510863608, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=105.63
