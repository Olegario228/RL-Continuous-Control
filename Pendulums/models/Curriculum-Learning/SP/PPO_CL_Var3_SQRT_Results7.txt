#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 7
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.8645005935362142, 'errorList': [], 'lossList': [0.0, -1.4144874328374863, 0.0, 35.6691119480133, 0.0, 0.0, 0.0], 'rewardMean': 0.8291352973581827, 'totalEpisodes': 41, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.384140893912807
'totalSteps': 3840, 'rewardStep': 0.853804479320304, 'errorList': [], 'lossList': [0.0, -1.3778650134801864, 0.0, 53.355420637130734, 0.0, 0.0, 0.0], 'rewardMean': 0.8373583580122231, 'totalEpisodes': 69, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.365177893871274
'totalSteps': 5120, 'rewardStep': 0.8703470083724211, 'errorList': [], 'lossList': [0.0, -1.3454862958192826, 0.0, 63.40362020492554, 0.0, 0.0, 0.0], 'rewardMean': 0.8456055206022727, 'totalEpisodes': 103, 'stepsPerEpisode': 28, 'rewardPerEpisode': 24.617318377105978
'totalSteps': 6400, 'rewardStep': 0.7037453477518859, 'errorList': [], 'lossList': [0.0, -1.3358053362369537, 0.0, 69.48702724456787, 0.0, 0.0, 0.0], 'rewardMean': 0.8172334860321954, 'totalEpisodes': 131, 'stepsPerEpisode': 36, 'rewardPerEpisode': 30.807400819827073
'totalSteps': 7680, 'rewardStep': 0.7291813606909036, 'errorList': [], 'lossList': [0.0, -1.3308041787147522, 0.0, 50.02214873313904, 0.0, 0.0, 0.0], 'rewardMean': 0.8025581318086467, 'totalEpisodes': 154, 'stepsPerEpisode': 60, 'rewardPerEpisode': 42.54467648067305
'totalSteps': 8960, 'rewardStep': 0.8171729687392127, 'errorList': [], 'lossList': [0.0, -1.317668840289116, 0.0, 28.506713805198668, 0.0, 0.0, 0.0], 'rewardMean': 0.8046459656558704, 'totalEpisodes': 164, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.014336674962605
'totalSteps': 10240, 'rewardStep': 0.6855891224249672, 'errorList': [], 'lossList': [0.0, -1.2871482276916504, 0.0, 23.601587035655974, 0.0, 0.0, 0.0], 'rewardMean': 0.7897638602520074, 'totalEpisodes': 169, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.857688815405613
'totalSteps': 11520, 'rewardStep': 0.9834185694641764, 'errorList': [0.12856068055279365, 0.3066247710956386, 0.23338709915849454, 0.517512647125479, 0.2852451551994534, 0.3236889895627905, 0.24097411447989195, 0.3305504297795141, 0.15285498061394678, 0.251820956270863, 0.8162477106520601, 0.25343306994549236, 0.3296355172005258, 1.2061673644392696, 0.1683611095447774, 0.13768029979547064, 0.1794185759650446, 0.7177756408586949, 0.3934430888078065, 0.8240127198697034, 0.7260046863799747, 0.6279654642017265, 0.7314914765331586, 1.026083794510858, 0.18967251041896943, 0.30536370943037594, 0.25302791872311087, 0.42683937865825616, 0.5328744858284348, 0.5507503719458171, 0.4190214510863615, 0.05756429165934907, 0.20955500502295699, 0.7883577894887062, 0.7266017333590709, 0.19036307333028035, 0.16417121598414763, 0.1797464712723571, 0.5011958038718017, 0.9802593629166729, 0.45863923917838206, 0.30488654594762665, 0.7574292719342751, 0.4028166477258597, 0.17502557797809176, 0.48882621804407955, 0.10265307968455299, 0.8721993631938134, 0.5440635439000224, 0.22223737765533313], 'lossList': [0.0, -1.2595831650495528, 0.0, 7.426095694899559, 0.0, 0.0, 0.0], 'rewardMean': 0.8112810501644706, 'totalEpisodes': 174, 'stepsPerEpisode': 113, 'rewardPerEpisode': 99.33241298476823, 'successfulTests': 12
'totalSteps': 12800, 'rewardStep': 0.839019529931784, 'errorList': [], 'lossList': [0.0, -1.241255670785904, 0.0, 14.576955981850624, 0.0, 0.0, 0.0], 'rewardMean': 0.8140548981412019, 'totalEpisodes': 177, 'stepsPerEpisode': 367, 'rewardPerEpisode': 249.66548900917124
'totalSteps': 14080, 'rewardStep': 0.7553299105802992, 'errorList': [], 'lossList': [0.0, -1.2225871926546097, 0.0, 9.029688957929611, 0.0, 0.0, 0.0], 'rewardMean': 0.8102108890812169, 'totalEpisodes': 178, 'stepsPerEpisode': 821, 'rewardPerEpisode': 558.1955725043354
'totalSteps': 15360, 'rewardStep': 0.5589185618573569, 'errorList': [], 'lossList': [0.0, -1.2073824995756148, 0.0, 11.648974967002868, 0.0, 0.0, 0.0], 'rewardMean': 0.779652685913331, 'totalEpisodes': 181, 'stepsPerEpisode': 340, 'rewardPerEpisode': 204.2222505616472
'totalSteps': 16640, 'rewardStep': 0.62506218559529, 'errorList': [], 'lossList': [0.0, -1.1944300049543382, 0.0, 5.02560884475708, 0.0, 0.0, 0.0], 'rewardMean': 0.7567784565408298, 'totalEpisodes': 182, 'stepsPerEpisode': 1229, 'rewardPerEpisode': 948.3812947772967
'totalSteps': 17920, 'rewardStep': 0.8365162732673812, 'errorList': [], 'lossList': [0.0, -1.1791092598438262, 0.0, 4.1248956161737445, 0.0, 0.0, 0.0], 'rewardMean': 0.7533953830303257, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.0453536708853
'totalSteps': 19200, 'rewardStep': 0.8490408527943393, 'errorList': [], 'lossList': [0.0, -1.154715002179146, 0.0, 1.665898217856884, 0.0, 0.0, 0.0], 'rewardMean': 0.7679249335345711, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.4360475902275
'totalSteps': 20480, 'rewardStep': 0.9498069257878392, 'errorList': [0.10444417247164313, 0.1996793468319854, 0.14157586630188965, 0.10727021176948402, 0.10389472403504064, 0.0976060756130368, 0.09128804388681849, 0.10139299484590579, 0.131010427020764, 0.10267272187914878, 0.13993169096477492, 0.09990944319990351, 0.11853819072168578, 0.1551148361989018, 0.13017780552691946, 0.1697892247303821, 0.08797183083548064, 0.15109790289897615, 0.07315993800964825, 0.14748980799941197, 0.08072193712209726, 0.0745428280837074, 0.09125635476232041, 0.21642021410108306, 0.09672196622509716, 0.17603920907340886, 0.13455320152881783, 0.06751939716489073, 0.0715712424210302, 0.07715949116107182, 0.13008232528765667, 0.13490984293735475, 0.15665896238757543, 0.08912843906768758, 0.09884506221172544, 0.16864438269253418, 0.10060882997348478, 0.06632211282549691, 0.0998687777946957, 0.16493774196604263, 0.07091164878045275, 0.1157473972693721, 0.16382248114885042, 0.07784914540158165, 0.11884584458557705, 0.09525574549093543, 0.1008452196303072, 0.106756179905483, 0.12389528311353952, 0.13364548967697132], 'lossList': [0.0, -1.1421971505880355, 0.0, 1.86196928486228, 0.0, 0.0, 0.0], 'rewardMean': 0.7899874900442646, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.3064583966961, 'successfulTests': 49
'totalSteps': 21760, 'rewardStep': 0.8948386638227912, 'errorList': [], 'lossList': [0.0, -1.1075186395645142, 0.0, 1.5637316117808222, 0.0, 0.0, 0.0], 'rewardMean': 0.7977540595526225, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.3020357153341
'totalSteps': 23040, 'rewardStep': 0.9905829828181418, 'errorList': [0.3099041627474147, 0.1811099252221527, 0.08550438125313253, 0.2991082555746184, 0.2694831940034946, 0.33303365665122703, 0.1777501046068457, 0.2403998341487663, 0.2174546294492991, 0.16105353640015774, 0.1358311054142992, 0.11812084504229747, 0.17346449730969804, 0.20643316247295057, 0.17056653406619204, 0.1484676358658615, 0.2777827960826444, 0.1307596494721318, 0.21874362489597401, 0.1407421062521619, 0.22254956890122057, 0.2231904349195594, 0.15984786964965905, 0.2578618184422604, 0.15575187440293162, 0.24098584429039632, 0.15779631736990787, 0.15681699460455384, 0.1905757885660343, 0.23291840332075758, 0.22134183024655305, 0.17746997438449985, 0.20148707286297593, 0.13611246975663976, 0.1720130290376215, 0.18914336139263097, 0.27115483604815954, 0.20142339020592076, 0.17912271253887854, 0.24340052002501855, 0.15979793369064998, 0.19583659511923493, 0.19263817038840905, 0.14704308764443838, 0.17835533786045366, 0.24959272499115576, 0.1116207958998299, 0.13892894654647187, 0.08285134286882755, 0.20294362701126933], 'lossList': [0.0, -1.0714791464805602, 0.0, 1.4756026657298207, 0.0, 0.0, 0.0], 'rewardMean': 0.82825344559194, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.3436364014526, 'successfulTests': 29
'totalSteps': 24320, 'rewardStep': 0.8749424829148816, 'errorList': [], 'lossList': [0.0, -1.0474526911973954, 0.0, 0.8813332515209913, 0.0, 0.0, 0.0], 'rewardMean': 0.8174058369370105, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1193.5794936616855
'totalSteps': 25600, 'rewardStep': 0.8850496578416802, 'errorList': [], 'lossList': [0.0, -1.0222653347253798, 0.0, 0.4244565223157406, 0.0, 0.0, 0.0], 'rewardMean': 0.8220088497279999, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.5580742498096
#maxSuccessfulTests=49, maxSuccessfulTestsAtStep=20480, timeSpent=117.2
