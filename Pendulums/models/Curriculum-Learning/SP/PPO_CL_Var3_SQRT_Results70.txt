#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 70
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9201394251512911, 'errorList': [], 'lossList': [0.0, -1.438127514719963, 0.0, 24.94007591843605, 0.0, 0.0, 0.0], 'rewardMean': 0.9174800278660595, 'totalEpisodes': 8, 'stepsPerEpisode': 534, 'rewardPerEpisode': 350.2785424853099
'totalSteps': 3840, 'rewardStep': 0.48625972530429074, 'errorList': [], 'lossList': [0.0, -1.4290182876586914, 0.0, 38.625619559288026, 0.0, 0.0, 0.0], 'rewardMean': 0.7737399270121367, 'totalEpisodes': 19, 'stepsPerEpisode': 155, 'rewardPerEpisode': 120.8086569261362
'totalSteps': 5120, 'rewardStep': 0.6602856765015068, 'errorList': [], 'lossList': [0.0, -1.4263893586397172, 0.0, 48.98388883590698, 0.0, 0.0, 0.0], 'rewardMean': 0.7453763643844792, 'totalEpisodes': 27, 'stepsPerEpisode': 194, 'rewardPerEpisode': 140.33851333212635
'totalSteps': 6400, 'rewardStep': 0.6422769840177878, 'errorList': [], 'lossList': [0.0, -1.428264935016632, 0.0, 68.81219854354859, 0.0, 0.0, 0.0], 'rewardMean': 0.7247564883111408, 'totalEpisodes': 38, 'stepsPerEpisode': 103, 'rewardPerEpisode': 83.80493145109173
'totalSteps': 7680, 'rewardStep': 0.8379895310512502, 'errorList': [], 'lossList': [0.0, -1.4308113521337509, 0.0, 116.59733968734741, 0.0, 0.0, 0.0], 'rewardMean': 0.7436286621011591, 'totalEpisodes': 57, 'stepsPerEpisode': 57, 'rewardPerEpisode': 46.7095118124693
'totalSteps': 8960, 'rewardStep': 0.7990988746453787, 'errorList': [], 'lossList': [0.0, -1.427168139219284, 0.0, 133.60260749816894, 0.0, 0.0, 0.0], 'rewardMean': 0.7515529781789047, 'totalEpisodes': 85, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.11928652126603
'totalSteps': 10240, 'rewardStep': 0.7647636873263757, 'errorList': [], 'lossList': [0.0, -1.4187879431247712, 0.0, 99.9510743713379, 0.0, 0.0, 0.0], 'rewardMean': 0.7532043168223386, 'totalEpisodes': 112, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.722792252097722
'totalSteps': 11520, 'rewardStep': 0.9114236823976756, 'errorList': [], 'lossList': [0.0, -1.4002696925401688, 0.0, 43.69419159889221, 0.0, 0.0, 0.0], 'rewardMean': 0.7707842463307093, 'totalEpisodes': 121, 'stepsPerEpisode': 71, 'rewardPerEpisode': 50.24526529130997
'totalSteps': 12800, 'rewardStep': 0.8175660822860082, 'errorList': [], 'lossList': [0.0, -1.3729307460784912, 0.0, 50.84169378757477, 0.0, 0.0, 0.0], 'rewardMean': 0.7754624299262393, 'totalEpisodes': 128, 'stepsPerEpisode': 89, 'rewardPerEpisode': 78.49593958251764
'totalSteps': 14080, 'rewardStep': 0.3462604772121949, 'errorList': [], 'lossList': [0.0, -1.3400301712751388, 0.0, 13.669922635555267, 0.0, 0.0, 0.0], 'rewardMean': 0.718606414589376, 'totalEpisodes': 129, 'stepsPerEpisode': 588, 'rewardPerEpisode': 357.2416470237867
'totalSteps': 15360, 'rewardStep': 0.48395298857447333, 'errorList': [], 'lossList': [0.0, -1.3051429241895676, 0.0, 27.133500447273253, 0.0, 0.0, 0.0], 'rewardMean': 0.6749877709316943, 'totalEpisodes': 134, 'stepsPerEpisode': 232, 'rewardPerEpisode': 173.59899141900726
'totalSteps': 16640, 'rewardStep': 0.894213653263417, 'errorList': [], 'lossList': [0.0, -1.2884773707389832, 0.0, 10.378370325565339, 0.0, 0.0, 0.0], 'rewardMean': 0.7157831637276069, 'totalEpisodes': 140, 'stepsPerEpisode': 99, 'rewardPerEpisode': 89.68808108867948
'totalSteps': 17920, 'rewardStep': 0.8111701307206797, 'errorList': [], 'lossList': [0.0, -1.2778526854515075, 0.0, 7.341981645822525, 0.0, 0.0, 0.0], 'rewardMean': 0.7308716091495241, 'totalEpisodes': 144, 'stepsPerEpisode': 357, 'rewardPerEpisode': 284.1256665850437
'totalSteps': 19200, 'rewardStep': 0.8235787522495367, 'errorList': [], 'lossList': [0.0, -1.2729621708393097, 0.0, 11.233422651290894, 0.0, 0.0, 0.0], 'rewardMean': 0.749001785972699, 'totalEpisodes': 149, 'stepsPerEpisode': 102, 'rewardPerEpisode': 87.49877777729297
'totalSteps': 20480, 'rewardStep': 0.8251306247648885, 'errorList': [], 'lossList': [0.0, -1.2742038583755493, 0.0, 5.483200787901878, 0.0, 0.0, 0.0], 'rewardMean': 0.7477158953440629, 'totalEpisodes': 152, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.965468973283167
'totalSteps': 21760, 'rewardStep': 0.9610819125479011, 'errorList': [1.684079694517117, 2.1905053478761376, 0.6029078308495806, 4.740273358248022, 4.169627563222348, 0.7849177412216273, 2.905533366221503, 1.8537383900576079, 2.5801510425239407, 5.23131414962109, 1.2425847714603255, 3.039413528579344, 0.8851172074350377, 0.30316317080167665, 1.4833076777491887, 2.5839775377401564, 2.0514445045085776, 1.7883646882769244, 1.4206807494605476, 2.8604093492600082, 3.3140543229961748, 0.8564509355185964, 2.1215755622416115, 2.745048065322458, 0.9894114654957131, 2.485041298059161, 1.91807946756203, 3.256962743654394, 1.6531144829770394, 0.7142527912002259, 2.1328687463042897, 0.549241497830166, 3.492869714927383, 2.7877553725577426, 0.25201996218942896, 0.3613488877443087, 3.4744073523948633, 1.9314095912266374, 0.1860687053695571, 1.47702774473596, 3.8166848754315383, 2.4764499423462207, 2.6881439028543763, 0.5470869530796645, 2.313351874338257, 0.34682162342002726, 0.3737604723091522, 3.8932374615489227, 0.6937594931649722, 1.5603724503357188], 'lossList': [0.0, -1.259166001677513, 0.0, 5.483509653806687, 0.0, 0.0, 0.0], 'rewardMean': 0.7639141991343151, 'totalEpisodes': 155, 'stepsPerEpisode': 62, 'rewardPerEpisode': 57.028859273448575, 'successfulTests': 1
'totalSteps': 23040, 'rewardStep': 0.6069301239410841, 'errorList': [], 'lossList': [0.0, -1.2659603130817414, 0.0, 6.257815091013908, 0.0, 0.0, 0.0], 'rewardMean': 0.748130842795786, 'totalEpisodes': 158, 'stepsPerEpisode': 81, 'rewardPerEpisode': 67.94523554430012
'totalSteps': 24320, 'rewardStep': 0.8042905101067797, 'errorList': [], 'lossList': [0.0, -1.2471194165945052, 0.0, 2.6453239941596984, 0.0, 0.0, 0.0], 'rewardMean': 0.7374175255666964, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.233899565891
'totalSteps': 25600, 'rewardStep': 0.43923783327250104, 'errorList': [], 'lossList': [0.0, -1.2290464645624162, 0.0, 6.262689774036407, 0.0, 0.0, 0.0], 'rewardMean': 0.6995847006653457, 'totalEpisodes': 160, 'stepsPerEpisode': 349, 'rewardPerEpisode': 265.0033121123861
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=21760, timeSpent=85.28
