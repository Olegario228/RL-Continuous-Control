#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 71
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.7861137363941268, 'errorList': [], 'lossList': [0.0, -1.4539394974708557, 0.0, 31.013346841335295, 0.0, 0.0, 0.0], 'rewardMean': 0.6896457779871431, 'totalEpisodes': 12, 'stepsPerEpisode': 85, 'rewardPerEpisode': 71.63079334099432
'totalSteps': 3840, 'rewardStep': 0.8774129928047771, 'errorList': [], 'lossList': [0.0, -1.4825279659032822, 0.0, 41.922069306373594, 0.0, 0.0, 0.0], 'rewardMean': 0.752234849593021, 'totalEpisodes': 20, 'stepsPerEpisode': 167, 'rewardPerEpisode': 119.01097561879014
'totalSteps': 5120, 'rewardStep': 0.8119739599639169, 'errorList': [], 'lossList': [0.0, -1.4910261678695678, 0.0, 45.62154725074768, 0.0, 0.0, 0.0], 'rewardMean': 0.767169627185745, 'totalEpisodes': 29, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.976357589631517
'totalSteps': 6400, 'rewardStep': 0.9072596952128817, 'errorList': [], 'lossList': [0.0, -1.4808418476581573, 0.0, 61.39586007118225, 0.0, 0.0, 0.0], 'rewardMean': 0.7951876407911723, 'totalEpisodes': 41, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.340951181385856
'totalSteps': 7680, 'rewardStep': 0.8544452367527645, 'errorList': [], 'lossList': [0.0, -1.4618527972698212, 0.0, 131.24759384155274, 0.0, 0.0, 0.0], 'rewardMean': 0.805063906784771, 'totalEpisodes': 66, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.404809323982243
'totalSteps': 8960, 'rewardStep': 0.6413368997855062, 'errorList': [], 'lossList': [0.0, -1.4563841122388839, 0.0, 104.47321117401123, 0.0, 0.0, 0.0], 'rewardMean': 0.7816743343563047, 'totalEpisodes': 105, 'stepsPerEpisode': 13, 'rewardPerEpisode': 7.517638640522927
'totalSteps': 10240, 'rewardStep': 0.9548642935481773, 'errorList': [12.278188721372251, 13.124159825386768, 14.35272037789205, 12.959832408641844, 16.158033454325913, 6.155371481479739, 9.909787503891913, 14.99713742033196, 13.004525787356513, 2.538150251101327, 12.568701412142655, 13.25037681153702, 9.530632360338336, 14.071425734270028, 10.896002830110236, 12.051432068733526, 7.9213294158522185, 10.019670667746851, 12.384264776106445, 4.971517364795483, 13.756502828013865, 11.785278186986163, 11.835274689778759, 9.55608173652251, 2.196337820955576, 14.575000877012906, 5.330026636033195, 12.277539523608581, 9.993158031620695, 9.259591556767857, 14.31661611409908, 13.15068892073435, 15.503156151531558, 9.25948136041589, 17.560118488439187, 7.68400741879826, 14.920446404592294, 18.324140594293063, 7.292288697993238, 13.528605975399179, 5.865566829851651, 0.8539427366455848, 12.808485490006598, 11.958246961630081, 7.652532469119993, 13.238852015765772, 11.414626437211982, 9.99442050516421, 13.596458780533723, 10.396088833019956], 'lossList': [0.0, -1.450652201771736, 0.0, 54.41584175109863, 0.0, 0.0, 0.0], 'rewardMean': 0.8033230792552888, 'totalEpisodes': 129, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.064481246914916, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6423217600210466, 'errorList': [], 'lossList': [0.0, -1.4364572286605835, 0.0, 28.809258737564086, 0.0, 0.0, 0.0], 'rewardMean': 0.7854340437848175, 'totalEpisodes': 139, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.352215775814283
'totalSteps': 12800, 'rewardStep': 0.7975923494225665, 'errorList': [], 'lossList': [0.0, -1.4071698862314224, 0.0, 38.63999114990234, 0.0, 0.0, 0.0], 'rewardMean': 0.7866498743485923, 'totalEpisodes': 149, 'stepsPerEpisode': 86, 'rewardPerEpisode': 57.370113979270336
'totalSteps': 14080, 'rewardStep': 0.7252478961319768, 'errorList': [], 'lossList': [0.0, -1.388013215661049, 0.0, 14.992526829242706, 0.0, 0.0, 0.0], 'rewardMean': 0.799856882003774, 'totalEpisodes': 152, 'stepsPerEpisode': 80, 'rewardPerEpisode': 63.914534404363664
'totalSteps': 15360, 'rewardStep': 0.9028586687038549, 'errorList': [], 'lossList': [0.0, -1.3754797339439393, 0.0, 9.619092413783074, 0.0, 0.0, 0.0], 'rewardMean': 0.8115313752347468, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 941.5700664694131
'totalSteps': 16640, 'rewardStep': 0.7300251533027599, 'errorList': [], 'lossList': [0.0, -1.328116945028305, 0.0, 20.843884833455085, 0.0, 0.0, 0.0], 'rewardMean': 0.7967925912845452, 'totalEpisodes': 153, 'stepsPerEpisode': 509, 'rewardPerEpisode': 375.3321194134426
'totalSteps': 17920, 'rewardStep': 0.7440649658090963, 'errorList': [], 'lossList': [0.0, -1.2832731980085372, 0.0, 5.992760231494904, 0.0, 0.0, 0.0], 'rewardMean': 0.7900016918690632, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.2375769742866
'totalSteps': 19200, 'rewardStep': 0.8910748087917171, 'errorList': [], 'lossList': [0.0, -1.2420901942253113, 0.0, 4.422224437594414, 0.0, 0.0, 0.0], 'rewardMean': 0.7883832032269467, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1107.6184401322207
'totalSteps': 20480, 'rewardStep': 0.7496151949956265, 'errorList': [], 'lossList': [0.0, -1.192303967475891, 0.0, 2.470964156165719, 0.0, 0.0, 0.0], 'rewardMean': 0.7779001990512328, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.241979340425
'totalSteps': 21760, 'rewardStep': 0.8443262296440082, 'errorList': [], 'lossList': [0.0, -1.133227489590645, 0.0, 1.9201344330608845, 0.0, 0.0, 0.0], 'rewardMean': 0.798199132037083, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.9816141291865
'totalSteps': 23040, 'rewardStep': 0.9750153859981617, 'errorList': [0.13069782218674075, 0.1161053897492605, 0.07905434944328422, 0.10975325987737243, 0.12416768154078495, 0.09991031502950466, 0.059120976851287275, 0.0785972853182646, 0.11164449905215175, 0.050700660204326224, 0.052609125287502494, 0.07999029152241444, 0.07319598320416519, 0.07250482044870718, 0.0709572208507797, 0.08873476700996645, 0.08172854877521032, 0.05172386459589272, 0.07089730572305943, 0.08666059070642392, 0.0766192101117494, 0.11204066945509336, 0.06154210308385587, 0.07952982549632685, 0.0669795300206758, 0.0561724761431056, 0.07158667122226353, 0.071242849792112, 0.08443370036446057, 0.05726556991191398, 0.08704272545831236, 0.10419692689209438, 0.05273483603964605, 0.09091631027899047, 0.07886509382412826, 0.09251715278244162, 0.10458000930112374, 0.09437025181076758, 0.0691768564873713, 0.1262138142682151, 0.09011213532481731, 0.052795364451003704, 0.08447791083564986, 0.05271919718697155, 0.07222654069607852, 0.11668096028353829, 0.11786240671555966, 0.08685808007550759, 0.15932946623911018, 0.10948181341886609], 'lossList': [0.0, -1.1012239599227904, 0.0, 2.0467037413455547, 0.0, 0.0, 0.0], 'rewardMean': 0.8002142412820815, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.6564352497348, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=99.84
