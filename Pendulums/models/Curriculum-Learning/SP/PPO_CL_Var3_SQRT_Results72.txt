#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 72
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.6657929186452562, 'errorList': [], 'lossList': [0.0, -1.4466296976804733, 0.0, 29.548242342472076, 0.0, 0.0, 0.0], 'rewardMean': 0.584446883274634, 'totalEpisodes': 15, 'stepsPerEpisode': 292, 'rewardPerEpisode': 215.69065244536242
'totalSteps': 3840, 'rewardStep': 0.8620324840402587, 'errorList': [], 'lossList': [0.0, -1.452019277215004, 0.0, 36.484979190826415, 0.0, 0.0, 0.0], 'rewardMean': 0.6769754168631756, 'totalEpisodes': 23, 'stepsPerEpisode': 30, 'rewardPerEpisode': 21.07147184492402
'totalSteps': 5120, 'rewardStep': 0.19743841709129267, 'errorList': [], 'lossList': [0.0, -1.4346551179885865, 0.0, 49.09053480148315, 0.0, 0.0, 0.0], 'rewardMean': 0.5570911669202049, 'totalEpisodes': 36, 'stepsPerEpisode': 68, 'rewardPerEpisode': 30.681472562199517
'totalSteps': 6400, 'rewardStep': 0.6599278524060436, 'errorList': [], 'lossList': [0.0, -1.4260981428623198, 0.0, 77.79963748931885, 0.0, 0.0, 0.0], 'rewardMean': 0.5776585040173726, 'totalEpisodes': 53, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.348248826353874
'totalSteps': 7680, 'rewardStep': 0.7785488147822062, 'errorList': [], 'lossList': [0.0, -1.4165689116716385, 0.0, 126.43243377685548, 0.0, 0.0, 0.0], 'rewardMean': 0.6111402224781782, 'totalEpisodes': 78, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.203400683549226
'totalSteps': 8960, 'rewardStep': 0.5437664609079331, 'errorList': [], 'lossList': [0.0, -1.4046841365098954, 0.0, 121.20738195419311, 0.0, 0.0, 0.0], 'rewardMean': 0.6015153993967146, 'totalEpisodes': 115, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.622840827967387
'totalSteps': 10240, 'rewardStep': 0.8852598154949612, 'errorList': [], 'lossList': [0.0, -1.400024294257164, 0.0, 59.21624778747559, 0.0, 0.0, 0.0], 'rewardMean': 0.6369834514089954, 'totalEpisodes': 134, 'stepsPerEpisode': 34, 'rewardPerEpisode': 24.709774629196698
'totalSteps': 11520, 'rewardStep': 0.8115168373086205, 'errorList': [], 'lossList': [0.0, -1.4054488825798035, 0.0, 42.28209753513336, 0.0, 0.0, 0.0], 'rewardMean': 0.656376049842287, 'totalEpisodes': 149, 'stepsPerEpisode': 71, 'rewardPerEpisode': 61.66186103112776
'totalSteps': 12800, 'rewardStep': 0.8626382712452464, 'errorList': [], 'lossList': [0.0, -1.4113531237840653, 0.0, 27.749882097244264, 0.0, 0.0, 0.0], 'rewardMean': 0.677002271982583, 'totalEpisodes': 159, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.49178765732657
'totalSteps': 14080, 'rewardStep': 0.6849378416855207, 'errorList': [], 'lossList': [0.0, -1.4191554981470107, 0.0, 7.431507569551468, 0.0, 0.0, 0.0], 'rewardMean': 0.695185971360734, 'totalEpisodes': 166, 'stepsPerEpisode': 143, 'rewardPerEpisode': 95.61575803157278
'totalSteps': 15360, 'rewardStep': 0.3658402499200831, 'errorList': [], 'lossList': [0.0, -1.421975743174553, 0.0, 18.398381073474884, 0.0, 0.0, 0.0], 'rewardMean': 0.6651907044882166, 'totalEpisodes': 172, 'stepsPerEpisode': 316, 'rewardPerEpisode': 235.66952434425275
'totalSteps': 16640, 'rewardStep': 0.9341156182140841, 'errorList': [0.9343440462089827, 12.90586090478649, 2.7450580259194513, 0.683114282272515, 0.30904831957262296, 5.476575775890221, 3.5046546507952474, 0.9795088673486975, 3.5487975046960876, 5.740148777355175, 4.0327528544876055, 2.00079667278205, 2.547503831645828, 19.793971240855274, 4.111864500372494, 9.210524487489176, 11.08776545863241, 0.22950418217201374, 0.5894397383122788, 5.467993907347457, 1.2250693606826242, 2.597428341530304, 1.6553308780306246, 11.121045746505617, 3.803425041004056, 4.621871791394803, 15.348150755188737, 7.426032765717054, 4.111786474815569, 1.517075111625523, 1.2630342039336393, 2.251121178032632, 2.423172651478985, 5.042085868639003, 2.1938908176047054, 3.1184806880446816, 3.4631461149938483, 9.901046892313968, 8.982069563007247, 1.2521676956825598, 2.1520882316209717, 14.118628680578968, 7.3704861265513495, 2.951608770544741, 8.682752715140715, 2.788618955372587, 0.21508449360687495, 4.253828642266862, 8.024561247172763, 4.550635182696807], 'lossList': [0.0, -1.421685627102852, 0.0, 7.5224683213233945, 0.0, 0.0, 0.0], 'rewardMean': 0.6723990179055991, 'totalEpisodes': 176, 'stepsPerEpisode': 351, 'rewardPerEpisode': 294.84380335680163, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.8099084212330853, 'errorList': [], 'lossList': [0.0, -1.4099179512262345, 0.0, 5.927626323699951, 0.0, 0.0, 0.0], 'rewardMean': 0.7336460183197783, 'totalEpisodes': 179, 'stepsPerEpisode': 240, 'rewardPerEpisode': 207.8125964418105
'totalSteps': 19200, 'rewardStep': 0.922177986111031, 'errorList': [], 'lossList': [0.0, -1.37341452896595, 0.0, 5.318862465918064, 0.0, 0.0, 0.0], 'rewardMean': 0.759871031690277, 'totalEpisodes': 180, 'stepsPerEpisode': 666, 'rewardPerEpisode': 588.1833804052683
'totalSteps': 20480, 'rewardStep': 0.7559666480339237, 'errorList': [], 'lossList': [0.0, -1.3371152245998383, 0.0, 3.673438602089882, 0.0, 0.0, 0.0], 'rewardMean': 0.7576128150154487, 'totalEpisodes': 184, 'stepsPerEpisode': 60, 'rewardPerEpisode': 44.45316161244874
'totalSteps': 21760, 'rewardStep': 0.9042973332920424, 'errorList': [], 'lossList': [0.0, -1.346993852853775, 0.0, 1.7669395181536673, 0.0, 0.0, 0.0], 'rewardMean': 0.7936659022538598, 'totalEpisodes': 187, 'stepsPerEpisode': 137, 'rewardPerEpisode': 117.46295955874594
'totalSteps': 23040, 'rewardStep': 0.8677070442270804, 'errorList': [], 'lossList': [0.0, -1.3405459356307983, 0.0, 1.531769939661026, 0.0, 0.0, 0.0], 'rewardMean': 0.7919106251270718, 'totalEpisodes': 190, 'stepsPerEpisode': 102, 'rewardPerEpisode': 90.06831301649348
'totalSteps': 24320, 'rewardStep': 0.941825858255162, 'errorList': [1.4814464947652295, 0.7375102951837618, 1.7948566617243478, 0.8154644078651825, 0.993260196069857, 1.6050695792471024, 0.9486702675427009, 0.7058566447115331, 0.9693186145035632, 0.6150988158844433, 1.311243015921175, 1.6110640212461038, 0.8271264125446556, 1.7867567020240882, 0.5203749573762846, 1.1911420834305042, 1.9695429475848785, 2.3192968815725616, 0.6015911541395362, 1.6397814206249182, 0.8633624482821747, 1.4839207338574556, 0.9068183845578822, 2.116662659692151, 2.0228058291094193, 2.294141810525409, 2.413617079108936, 0.9120509275812518, 0.6243459762304188, 1.6260457140509705, 0.7048950996923283, 0.9830883597333269, 1.3917006765052145, 1.5989213445314687, 1.1926980852850473, 1.0122737823024965, 1.5156536235032079, 0.7257849800534165, 2.4846576232924487, 1.3568337619819095, 1.687167170947195, 1.2906814509575129, 1.644821746637571, 1.0635064762442852, 2.215417982342055, 0.6083967730130587, 0.9170510276070378, 1.7279399595127678, 1.571353578673568, 2.2072401671234103], 'lossList': [0.0, -1.3010963362455368, 0.0, 1.9204088920354843, 0.0, 0.0, 0.0], 'rewardMean': 0.804941527221726, 'totalEpisodes': 192, 'stepsPerEpisode': 99, 'rewardPerEpisode': 89.74724939576608, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.8901732018401018, 'errorList': [], 'lossList': [0.0, -1.2739690178632737, 0.0, 5.833809682428837, 0.0, 0.0, 0.0], 'rewardMean': 0.8076950202812114, 'totalEpisodes': 194, 'stepsPerEpisode': 115, 'rewardPerEpisode': 105.61740610295153
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=107.16
