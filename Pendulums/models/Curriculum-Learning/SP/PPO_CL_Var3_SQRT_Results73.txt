#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 73
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.7312590381629107, 'errorList': [], 'lossList': [0.0, -1.4288949131965638, 0.0, 29.511996659040452, 0.0, 0.0, 0.0], 'rewardMean': 0.8150947269948255, 'totalEpisodes': 18, 'stepsPerEpisode': 28, 'rewardPerEpisode': 24.737583378983675
'totalSteps': 3840, 'rewardStep': 0.8542213367396314, 'errorList': [], 'lossList': [0.0, -1.4301256895065309, 0.0, 33.76066139698028, 0.0, 0.0, 0.0], 'rewardMean': 0.8281369302430942, 'totalEpisodes': 24, 'stepsPerEpisode': 379, 'rewardPerEpisode': 255.666261616941
'totalSteps': 5120, 'rewardStep': 0.7579091183121291, 'errorList': [], 'lossList': [0.0, -1.4233402824401855, 0.0, 48.989687428474426, 0.0, 0.0, 0.0], 'rewardMean': 0.8105799772603529, 'totalEpisodes': 31, 'stepsPerEpisode': 78, 'rewardPerEpisode': 69.92118910429107
'totalSteps': 6400, 'rewardStep': 0.7248930166509353, 'errorList': [], 'lossList': [0.0, -1.4202000665664674, 0.0, 82.35618814468384, 0.0, 0.0, 0.0], 'rewardMean': 0.7934425851384693, 'totalEpisodes': 50, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.34259991292389
'totalSteps': 7680, 'rewardStep': 0.6924055158575342, 'errorList': [], 'lossList': [0.0, -1.4101288491487503, 0.0, 136.91723426818848, 0.0, 0.0, 0.0], 'rewardMean': 0.7766030735916468, 'totalEpisodes': 79, 'stepsPerEpisode': 54, 'rewardPerEpisode': 41.45147031515637
'totalSteps': 8960, 'rewardStep': 0.7890002589435686, 'errorList': [], 'lossList': [0.0, -1.4002392691373826, 0.0, 78.10740243911744, 0.0, 0.0, 0.0], 'rewardMean': 0.7783741000704928, 'totalEpisodes': 115, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.627343619179195
'totalSteps': 10240, 'rewardStep': 0.6297898484475095, 'errorList': [], 'lossList': [0.0, -1.3831353509426116, 0.0, 47.64458062171936, 0.0, 0.0, 0.0], 'rewardMean': 0.75980106861762, 'totalEpisodes': 135, 'stepsPerEpisode': 33, 'rewardPerEpisode': 17.916855365492424
'totalSteps': 11520, 'rewardStep': 0.8672119209322247, 'errorList': [], 'lossList': [0.0, -1.3820802128314973, 0.0, 19.9968874502182, 0.0, 0.0, 0.0], 'rewardMean': 0.7717356077636872, 'totalEpisodes': 143, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.929854579488658
'totalSteps': 12800, 'rewardStep': 0.6183930653949619, 'errorList': [], 'lossList': [0.0, -1.3698590666055679, 0.0, 14.301234550476075, 0.0, 0.0, 0.0], 'rewardMean': 0.7564013535268146, 'totalEpisodes': 146, 'stepsPerEpisode': 26, 'rewardPerEpisode': 18.034232170391626
'totalSteps': 14080, 'rewardStep': 0.9051162847156926, 'errorList': [], 'lossList': [0.0, -1.32151981651783, 0.0, 8.605833628177642, 0.0, 0.0, 0.0], 'rewardMean': 0.7570199404157097, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 919.3823622193111
'totalSteps': 15360, 'rewardStep': 0.7645426667684304, 'errorList': [], 'lossList': [0.0, -1.3112774085998535, 0.0, 12.492896618843078, 0.0, 0.0, 0.0], 'rewardMean': 0.7603483032762617, 'totalEpisodes': 147, 'stepsPerEpisode': 525, 'rewardPerEpisode': 430.6931929167044
'totalSteps': 16640, 'rewardStep': 0.4961132481636729, 'errorList': [], 'lossList': [0.0, -1.3151239013671876, 0.0, 5.1280978626012805, 0.0, 0.0, 0.0], 'rewardMean': 0.724537494418666, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 867.9354830078368
'totalSteps': 17920, 'rewardStep': 0.8864915178642188, 'errorList': [], 'lossList': [0.0, -1.318837732076645, 0.0, 3.9633053788542747, 0.0, 0.0, 0.0], 'rewardMean': 0.737395734373875, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1034.2779746602582
'totalSteps': 19200, 'rewardStep': 0.9091974633024456, 'errorList': [], 'lossList': [0.0, -1.2957917612791061, 0.0, 3.8346168226003647, 0.0, 0.0, 0.0], 'rewardMean': 0.7558261790390259, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.288288222629
'totalSteps': 20480, 'rewardStep': 0.9740131776795147, 'errorList': [0.04693406222862652, 0.03171756482592569, 0.03923505307896072, 0.06474895621545548, 0.039723892021374385, 0.022106710596869455, 0.08709468405861433, 0.045738390497829795, 0.046506970087186, 0.07382406994206762, 0.028116711177078178, 0.03162135362374744, 0.02682617501104622, 0.05366179645485014, 0.030991368821585284, 0.031036980554158152, 0.06051652594921107, 0.09148257935414073, 0.06457249844092859, 0.08158968209902699, 0.0334152212099804, 0.03651498124846605, 0.08353245760936501, 0.06077008759914997, 0.021772918574369332, 0.017834469353474484, 0.06543162862749322, 0.08057657276566838, 0.08023281843298251, 0.0843668579604175, 0.022283983181306154, 0.08510243140826769, 0.0337741163066838, 0.05596737314636275, 0.12255463323164814, 0.0635753080925715, 0.08435920167193288, 0.05500597033810188, 0.033518413358943296, 0.03039472210097784, 0.03396799822315644, 0.09333232616953764, 0.048248607825175024, 0.06710650191877242, 0.07534905599632803, 0.05900407921786008, 0.025292633967923486, 0.0312755394099687, 0.06919043266879958, 0.06842815408258657], 'lossList': [0.0, -1.268721910715103, 0.0, 2.982465591169894, 0.0, 0.0, 0.0], 'rewardMean': 0.7839869452212241, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.3251584576199, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=72.65
