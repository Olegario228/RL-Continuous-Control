#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 74
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8624471825282862, 'errorList': [], 'lossList': [0.0, -1.4296625500917435, 0.0, 28.737666558027268, 0.0, 0.0, 0.0], 'rewardMean': 0.8058690234137836, 'totalEpisodes': 13, 'stepsPerEpisode': 321, 'rewardPerEpisode': 251.632520166278
'totalSteps': 3840, 'rewardStep': 0.611250939138649, 'errorList': [], 'lossList': [0.0, -1.4399333882331848, 0.0, 28.586548328399658, 0.0, 0.0, 0.0], 'rewardMean': 0.7409963286554054, 'totalEpisodes': 17, 'stepsPerEpisode': 183, 'rewardPerEpisode': 125.6722542215036
'totalSteps': 5120, 'rewardStep': 0.5302268558448323, 'errorList': [], 'lossList': [0.0, -1.4400426936149597, 0.0, 50.09752662658691, 0.0, 0.0, 0.0], 'rewardMean': 0.6883039604527621, 'totalEpisodes': 25, 'stepsPerEpisode': 237, 'rewardPerEpisode': 181.08634605283825
'totalSteps': 6400, 'rewardStep': 0.8593121031456465, 'errorList': [], 'lossList': [0.0, -1.4171307575702667, 0.0, 89.15305276870727, 0.0, 0.0, 0.0], 'rewardMean': 0.722505588991339, 'totalEpisodes': 37, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.19190374297655
'totalSteps': 7680, 'rewardStep': 0.6815507856299902, 'errorList': [], 'lossList': [0.0, -1.3927707409858703, 0.0, 123.88786407470702, 0.0, 0.0, 0.0], 'rewardMean': 0.7156797884311142, 'totalEpisodes': 58, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.9375809565362
'totalSteps': 8960, 'rewardStep': 0.8666138706630362, 'errorList': [], 'lossList': [0.0, -1.385794997215271, 0.0, 124.66603393554688, 0.0, 0.0, 0.0], 'rewardMean': 0.7372418001785316, 'totalEpisodes': 90, 'stepsPerEpisode': 32, 'rewardPerEpisode': 23.527187659588634
'totalSteps': 10240, 'rewardStep': 0.6369262191734326, 'errorList': [], 'lossList': [0.0, -1.380736704468727, 0.0, 84.40088317871094, 0.0, 0.0, 0.0], 'rewardMean': 0.7247023525528943, 'totalEpisodes': 106, 'stepsPerEpisode': 82, 'rewardPerEpisode': 66.55591805211893
'totalSteps': 11520, 'rewardStep': 0.8146304143658696, 'errorList': [], 'lossList': [0.0, -1.3748767518997191, 0.0, 57.79830741882324, 0.0, 0.0, 0.0], 'rewardMean': 0.7346943594210027, 'totalEpisodes': 114, 'stepsPerEpisode': 165, 'rewardPerEpisode': 115.47142954668746
'totalSteps': 12800, 'rewardStep': 0.654861738964301, 'errorList': [], 'lossList': [0.0, -1.356288416981697, 0.0, 51.841000146865845, 0.0, 0.0, 0.0], 'rewardMean': 0.7267110973753325, 'totalEpisodes': 122, 'stepsPerEpisode': 62, 'rewardPerEpisode': 49.300613543321134
'totalSteps': 14080, 'rewardStep': 0.9737856454864539, 'errorList': [17.05688186486182, 2.5033237898607896, 19.8052258176225, 17.640226553288606, 16.006082337884173, 1.269320562633055, 7.072990703042184, 3.825349919402655, 2.70834122469461, 16.430559629419527, 9.2444478983955, 2.1796527683298748, 12.972108403368948, 2.295651867306327, 7.799329369359507, 9.57599403245698, 19.434205536187196, 9.938883793917391, 14.731151498359496, 13.017654544517594, 10.766460640224631, 8.447251972643457, 16.497444802693085, 19.47114124502526, 9.755846673766897, 9.729770288533102, 8.119685359477725, 11.795713328286322, 2.146632472263612, 5.281457814132729, 8.705892567999859, 19.323882872977943, 24.383246155937353, 19.652986367367884, 4.398785194778495, 17.042598157166825, 2.748143545421016, 11.744879573020775, 6.094139505082729, 9.05307205595164, 21.81496271830479, 13.031772429396211, 8.401886228276725, 6.3319188463406055, 0.8860467006784261, 18.197946512906956, 2.498213203512959, 12.989677685280888, 7.946167164513813, 15.795232797786971], 'lossList': [0.0, -1.3407499504089355, 0.0, 27.663550367355345, 0.0, 0.0, 0.0], 'rewardMean': 0.7491605754940498, 'totalEpisodes': 128, 'stepsPerEpisode': 42, 'rewardPerEpisode': 37.58049971741063, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.5582156002007651, 'errorList': [], 'lossList': [0.0, -1.3442468601465225, 0.0, 26.566957025527955, 0.0, 0.0, 0.0], 'rewardMean': 0.7187374172612977, 'totalEpisodes': 134, 'stepsPerEpisode': 75, 'rewardPerEpisode': 43.75678337579411
'totalSteps': 16640, 'rewardStep': 0.7759532106982754, 'errorList': [], 'lossList': [0.0, -1.3354119706153869, 0.0, 26.810940449237822, 0.0, 0.0, 0.0], 'rewardMean': 0.7352076444172603, 'totalEpisodes': 143, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7759532106982754
'totalSteps': 17920, 'rewardStep': 0.7543628632194859, 'errorList': [], 'lossList': [0.0, -1.334856162071228, 0.0, 23.510430532693864, 0.0, 0.0, 0.0], 'rewardMean': 0.7576212451547256, 'totalEpisodes': 149, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.32889337991405
'totalSteps': 19200, 'rewardStep': 0.8368047480062686, 'errorList': [], 'lossList': [0.0, -1.3316929316520691, 0.0, 5.4149759298563005, 0.0, 0.0, 0.0], 'rewardMean': 0.7553705096407878, 'totalEpisodes': 153, 'stepsPerEpisode': 224, 'rewardPerEpisode': 191.65453185924272
'totalSteps': 20480, 'rewardStep': 0.8170684353048189, 'errorList': [], 'lossList': [0.0, -1.317899955511093, 0.0, 6.470861833691597, 0.0, 0.0, 0.0], 'rewardMean': 0.7689222746082707, 'totalEpisodes': 157, 'stepsPerEpisode': 210, 'rewardPerEpisode': 174.47221614279965
'totalSteps': 21760, 'rewardStep': 0.6956640288660413, 'errorList': [], 'lossList': [0.0, -1.3103855526447297, 0.0, 4.338358797430992, 0.0, 0.0, 0.0], 'rewardMean': 0.7518272904285711, 'totalEpisodes': 159, 'stepsPerEpisode': 274, 'rewardPerEpisode': 210.77554440521115
'totalSteps': 23040, 'rewardStep': 0.8282504697847417, 'errorList': [], 'lossList': [0.0, -1.2963979297876358, 0.0, 4.645489849448204, 0.0, 0.0, 0.0], 'rewardMean': 0.7709597154897021, 'totalEpisodes': 162, 'stepsPerEpisode': 106, 'rewardPerEpisode': 92.59469404210257
'totalSteps': 24320, 'rewardStep': 0.8121875446736261, 'errorList': [], 'lossList': [0.0, -1.3021127945184707, 0.0, 5.0766468942165375, 0.0, 0.0, 0.0], 'rewardMean': 0.7707154285204777, 'totalEpisodes': 166, 'stepsPerEpisode': 49, 'rewardPerEpisode': 43.01780446401852
'totalSteps': 25600, 'rewardStep': 0.881610654469335, 'errorList': [], 'lossList': [0.0, -1.2918660390377044, 0.0, 2.620392059087753, 0.0, 0.0, 0.0], 'rewardMean': 0.7933903200709811, 'totalEpisodes': 167, 'stepsPerEpisode': 162, 'rewardPerEpisode': 134.5134664419509
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=87.23
