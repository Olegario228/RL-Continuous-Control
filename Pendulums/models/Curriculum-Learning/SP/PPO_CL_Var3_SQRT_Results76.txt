#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 76
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5010902444859473, 'errorList': [], 'lossList': [0.0, -1.4158290261030197, 0.0, 34.109994192123416, 0.0, 0.0, 0.0], 'rewardMean': 0.6580701886636773, 'totalEpisodes': 65, 'stepsPerEpisode': 39, 'rewardPerEpisode': 28.962695469954046
'totalSteps': 3840, 'rewardStep': 0.7223149393044694, 'errorList': [], 'lossList': [0.0, -1.4054242885112762, 0.0, 50.07603960037231, 0.0, 0.0, 0.0], 'rewardMean': 0.6794851055439413, 'totalEpisodes': 100, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.238213386235355
'totalSteps': 5120, 'rewardStep': 0.8464921368816107, 'errorList': [], 'lossList': [0.0, -1.3919441199302673, 0.0, 49.955907611846925, 0.0, 0.0, 0.0], 'rewardMean': 0.7212368633783586, 'totalEpisodes': 115, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.519042482215374
'totalSteps': 6400, 'rewardStep': 0.6352151821705455, 'errorList': [], 'lossList': [0.0, -1.391530094742775, 0.0, 58.36461696624756, 0.0, 0.0, 0.0], 'rewardMean': 0.704032527136796, 'totalEpisodes': 124, 'stepsPerEpisode': 83, 'rewardPerEpisode': 70.86336975076759
'totalSteps': 7680, 'rewardStep': 0.8458688600913221, 'errorList': [], 'lossList': [0.0, -1.4008015203475952, 0.0, 62.47854077339172, 0.0, 0.0, 0.0], 'rewardMean': 0.7276719159625503, 'totalEpisodes': 131, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.976520381640746
'totalSteps': 8960, 'rewardStep': 0.7943419257967628, 'errorList': [], 'lossList': [0.0, -1.3927994614839554, 0.0, 41.976649322509765, 0.0, 0.0, 0.0], 'rewardMean': 0.7371962030817236, 'totalEpisodes': 138, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.230596886608907
'totalSteps': 10240, 'rewardStep': 0.7832291438467109, 'errorList': [], 'lossList': [0.0, -1.3847854512929916, 0.0, 45.35508793354035, 0.0, 0.0, 0.0], 'rewardMean': 0.742950320677347, 'totalEpisodes': 144, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.7809533172726
'totalSteps': 11520, 'rewardStep': 0.6559271817278538, 'errorList': [], 'lossList': [0.0, -1.3807965755462646, 0.0, 62.55073299407959, 0.0, 0.0, 0.0], 'rewardMean': 0.7332810830162921, 'totalEpisodes': 153, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.026292037551514
'totalSteps': 12800, 'rewardStep': 0.666450496003551, 'errorList': [], 'lossList': [0.0, -1.3624468010663986, 0.0, 49.885779304504396, 0.0, 0.0, 0.0], 'rewardMean': 0.7265980243150181, 'totalEpisodes': 162, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.3399563019906817
'totalSteps': 14080, 'rewardStep': 0.6312651625108812, 'errorList': [], 'lossList': [0.0, -1.3348221558332443, 0.0, 14.449051195383072, 0.0, 0.0, 0.0], 'rewardMean': 0.7082195272819656, 'totalEpisodes': 164, 'stepsPerEpisode': 334, 'rewardPerEpisode': 245.8379926722403
'totalSteps': 15360, 'rewardStep': 0.9461251327022824, 'errorList': [11.93866746979474, 10.407097718208874, 14.326870967651972, 11.831881452234652, 15.721157349274863, 13.983024294512562, 8.506805399878832, 7.553534235994535, 12.320157675702646, 11.618789503600011, 10.586042501885562, 12.747324425474952, 16.36416547656149, 14.413876271086226, 1.226223171004427, 8.482249582383709, 15.542934357707443, 14.904823560577835, 16.91479491827105, 5.6823358592431, 5.796484369362401, 11.660918112767348, 7.525968408039509, 9.492657193986389, 14.845688381402582, 9.876218345818646, 2.2031396664186675, 7.326309019307743, 12.753023865792414, 12.286581883411596, 12.022430203807394, 15.941906208497032, 13.001074407637997, 8.59504031282556, 11.82897012753774, 16.510115072520307, 7.0028902909765005, 5.372773201558287, 2.0779673374497416, 2.5444281342462602, 15.695035651443643, 5.378699405771224, 18.851298402386032, 12.708355172441998, 10.265624562075278, 17.209658681698823, 9.946151358391088, 10.789177319037652, 18.016669798252064, 10.195283909416181], 'lossList': [0.0, -1.331073110103607, 0.0, 16.16299025416374, 0.0, 0.0, 0.0], 'rewardMean': 0.752723016103599, 'totalEpisodes': 167, 'stepsPerEpisode': 540, 'rewardPerEpisode': 450.49396704257623, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.5788881390303526, 'errorList': [], 'lossList': [0.0, -1.3142522811889648, 0.0, 8.339616297483444, 0.0, 0.0, 0.0], 'rewardMean': 0.7383803360761874, 'totalEpisodes': 170, 'stepsPerEpisode': 145, 'rewardPerEpisode': 108.3708764574054
'totalSteps': 17920, 'rewardStep': 0.8417628509873104, 'errorList': [], 'lossList': [0.0, -1.2843790221214295, 0.0, 4.128781709671021, 0.0, 0.0, 0.0], 'rewardMean': 0.7379074074867573, 'totalEpisodes': 176, 'stepsPerEpisode': 71, 'rewardPerEpisode': 59.95250556238155
'totalSteps': 19200, 'rewardStep': 0.8556472756769519, 'errorList': [], 'lossList': [0.0, -1.2560049217939377, 0.0, 3.4230406314134596, 0.0, 0.0, 0.0], 'rewardMean': 0.7599506168373978, 'totalEpisodes': 179, 'stepsPerEpisode': 354, 'rewardPerEpisode': 264.65005205997886
'totalSteps': 20480, 'rewardStep': 0.7137808269377927, 'errorList': [], 'lossList': [0.0, -1.231418394446373, 0.0, 15.99632968902588, 0.0, 0.0, 0.0], 'rewardMean': 0.7467418135220449, 'totalEpisodes': 183, 'stepsPerEpisode': 32, 'rewardPerEpisode': 28.49141257883909
'totalSteps': 21760, 'rewardStep': 0.6020478660546756, 'errorList': [], 'lossList': [0.0, -1.2187708562612534, 0.0, 5.074803802669049, 0.0, 0.0, 0.0], 'rewardMean': 0.7275124075478363, 'totalEpisodes': 184, 'stepsPerEpisode': 305, 'rewardPerEpisode': 238.3671777869986
'totalSteps': 23040, 'rewardStep': 0.9276489506918183, 'errorList': [], 'lossList': [0.0, -1.2139787220954894, 0.0, 1.405031181871891, 0.0, 0.0, 0.0], 'rewardMean': 0.741954388232347, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1095.2779810976274
'totalSteps': 24320, 'rewardStep': 0.8251944908181829, 'errorList': [], 'lossList': [0.0, -1.1816021603345872, 0.0, 1.3562193575501442, 0.0, 0.0, 0.0], 'rewardMean': 0.7588811191413799, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.8640370854698
'totalSteps': 25600, 'rewardStep': 0.9069451379368645, 'errorList': [], 'lossList': [0.0, -1.12882026553154, 0.0, 0.5763019569590688, 0.0, 0.0, 0.0], 'rewardMean': 0.7829305833347113, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.8547356766242
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=85.04
