#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 78
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.39961536922127716, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.43472332172764644, 'totalEpisodes': 72, 'stepsPerEpisode': 37, 'rewardPerEpisode': 25.77372538667661
'totalSteps': 3840, 'rewardStep': 0.7741563852910484, 'errorList': [], 'lossList': [0.0, -1.4358250749111177, 0.0, 30.707802486419677, 0.0, 0.0, 0.0], 'rewardMean': 0.5195815876184969, 'totalEpisodes': 120, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.000434322166944
'totalSteps': 5120, 'rewardStep': 0.7412331052131339, 'errorList': [], 'lossList': [0.0, -1.4300383299589157, 0.0, 43.340853128433224, 0.0, 0.0, 0.0], 'rewardMean': 0.5639118911374243, 'totalEpisodes': 147, 'stepsPerEpisode': 76, 'rewardPerEpisode': 62.15552771958467
'totalSteps': 6400, 'rewardStep': 0.9020388634278037, 'errorList': [], 'lossList': [0.0, -1.4127223151922226, 0.0, 59.893150119781495, 0.0, 0.0, 0.0], 'rewardMean': 0.6202663865191542, 'totalEpisodes': 168, 'stepsPerEpisode': 110, 'rewardPerEpisode': 78.29785339512199
'totalSteps': 7680, 'rewardStep': 0.886892371740753, 'errorList': [], 'lossList': [0.0, -1.4027819591760635, 0.0, 67.45616668701172, 0.0, 0.0, 0.0], 'rewardMean': 0.6583558129793826, 'totalEpisodes': 188, 'stepsPerEpisode': 83, 'rewardPerEpisode': 60.65368329866627
'totalSteps': 8960, 'rewardStep': 0.9102367897982002, 'errorList': [], 'lossList': [0.0, -1.3951277261972428, 0.0, 52.56659665107727, 0.0, 0.0, 0.0], 'rewardMean': 0.6898409350817347, 'totalEpisodes': 202, 'stepsPerEpisode': 70, 'rewardPerEpisode': 54.6939474692943
'totalSteps': 10240, 'rewardStep': 0.745974988385648, 'errorList': [], 'lossList': [0.0, -1.3838142997026444, 0.0, 42.48167971611023, 0.0, 0.0, 0.0], 'rewardMean': 0.6960780521155029, 'totalEpisodes': 209, 'stepsPerEpisode': 162, 'rewardPerEpisode': 137.89198968232364
'totalSteps': 11520, 'rewardStep': 0.8434478255398962, 'errorList': [], 'lossList': [0.0, -1.3702551168203354, 0.0, 26.405604541301727, 0.0, 0.0, 0.0], 'rewardMean': 0.7108150294579423, 'totalEpisodes': 212, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.585392236658471
'totalSteps': 12800, 'rewardStep': 0.7415570546532156, 'errorList': [], 'lossList': [0.0, -1.3532952612638474, 0.0, 12.894431065320969, 0.0, 0.0, 0.0], 'rewardMean': 0.7344768122492253, 'totalEpisodes': 213, 'stepsPerEpisode': 737, 'rewardPerEpisode': 580.9716663126875
'totalSteps': 14080, 'rewardStep': 0.8479012237025875, 'errorList': [], 'lossList': [0.0, -1.352127497792244, 0.0, 41.489247059822084, 0.0, 0.0, 0.0], 'rewardMean': 0.7793053976973564, 'totalEpisodes': 215, 'stepsPerEpisode': 364, 'rewardPerEpisode': 307.45832031490374
'totalSteps': 15360, 'rewardStep': 0.7433822392826885, 'errorList': [], 'lossList': [0.0, -1.3586839598417282, 0.0, 73.66488807678223, 0.0, 0.0, 0.0], 'rewardMean': 0.8136820847034976, 'totalEpisodes': 220, 'stepsPerEpisode': 113, 'rewardPerEpisode': 93.46405441509407
'totalSteps': 16640, 'rewardStep': 0.30401864315542726, 'errorList': [], 'lossList': [0.0, -1.363531853556633, 0.0, 94.10662413597107, 0.0, 0.0, 0.0], 'rewardMean': 0.7666683104899354, 'totalEpisodes': 225, 'stepsPerEpisode': 221, 'rewardPerEpisode': 163.09275116972978
'totalSteps': 17920, 'rewardStep': 0.8966897214122964, 'errorList': [], 'lossList': [0.0, -1.377011644244194, 0.0, 91.68277805805207, 0.0, 0.0, 0.0], 'rewardMean': 0.7822139721098517, 'totalEpisodes': 230, 'stepsPerEpisode': 132, 'rewardPerEpisode': 112.02736984435407
'totalSteps': 19200, 'rewardStep': 0.6197956611150092, 'errorList': [], 'lossList': [0.0, -1.3904897803068161, 0.0, 82.71665192604065, 0.0, 0.0, 0.0], 'rewardMean': 0.7539896518785721, 'totalEpisodes': 234, 'stepsPerEpisode': 184, 'rewardPerEpisode': 150.85466395312267
'totalSteps': 20480, 'rewardStep': 0.7437981624656353, 'errorList': [], 'lossList': [0.0, -1.394833248257637, 0.0, 88.0815718460083, 0.0, 0.0, 0.0], 'rewardMean': 0.7396802309510604, 'totalEpisodes': 239, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.0298976556570905
'totalSteps': 21760, 'rewardStep': 0.9500044338141042, 'errorList': [0.38235441168683115, 0.09535825467099979, 0.34787612700276316, 0.3484952436671449, 0.08920294719932811, 0.15365189407578234, 0.5904466749744766, 0.1997537384862985, 0.503421550938768, 0.1523728979260475, 0.2203284614105065, 0.1887603007053594, 0.07811073216635005, 0.1964997494036114, 0.13201183212970796, 0.2165788689772503, 0.08066583698997527, 0.2833426077153901, 0.25601469296104673, 0.39364193807359615, 0.10251456580660401, 0.14364553278375647, 0.31329215825500356, 0.0855522651257291, 0.1154652124444994, 0.11884136188137984, 0.11577010659586169, 0.22896186569617982, 0.2179192535492998, 0.10574418821026373, 0.22269079002184117, 0.1933198230027017, 0.24217853971414344, 0.21620241359976897, 0.2517419265705833, 0.2288430797660536, 0.41763138451329423, 0.3577064602691203, 0.2143947694920073, 0.44026008389444504, 0.29747922869451976, 0.46567738083543714, 0.21665298175157735, 0.2503675022938767, 0.2741223111380943, 0.16953876013439179, 0.09717281097666997, 0.17414583348174098, 0.12276013912518703, 0.3931329249349865], 'lossList': [0.0, -1.3849226647615434, 0.0, 45.32330657243729, 0.0, 0.0, 0.0], 'rewardMean': 0.7436569953526508, 'totalEpisodes': 243, 'stepsPerEpisode': 513, 'rewardPerEpisode': 428.07386792379873, 'successfulTests': 22
'totalSteps': 23040, 'rewardStep': 0.6613712798027644, 'errorList': [], 'lossList': [0.0, -1.3847853511571884, 0.0, 5.51578896433115, 0.0, 0.0, 0.0], 'rewardMean': 0.7351966244943625, 'totalEpisodes': 244, 'stepsPerEpisode': 855, 'rewardPerEpisode': 690.8946675399983
'totalSteps': 24320, 'rewardStep': 0.5052571274589218, 'errorList': [], 'lossList': [0.0, -1.3868770217895507, 0.0, 51.16695317149162, 0.0, 0.0, 0.0], 'rewardMean': 0.7013775546862651, 'totalEpisodes': 246, 'stepsPerEpisode': 333, 'rewardPerEpisode': 274.7847568769919
'totalSteps': 25600, 'rewardStep': 0.8238110270580979, 'errorList': [], 'lossList': [0.0, -1.3950346118211747, 0.0, 35.74956250190735, 0.0, 0.0, 0.0], 'rewardMean': 0.7096029519267533, 'totalEpisodes': 247, 'stepsPerEpisode': 891, 'rewardPerEpisode': 749.2387696467382
#maxSuccessfulTests=22, maxSuccessfulTestsAtStep=21760, timeSpent=84.09
