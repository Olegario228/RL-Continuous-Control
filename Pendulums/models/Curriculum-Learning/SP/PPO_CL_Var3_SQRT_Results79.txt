#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 79
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7332841251799272, 'errorList': [], 'lossList': [0.0, -1.4096651488542558, 0.0, 33.09793794631958, 0.0, 0.0, 0.0], 'rewardMean': 0.8271871296656672, 'totalEpisodes': 83, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.84593674539649
'totalSteps': 3840, 'rewardStep': 0.8272955375493076, 'errorList': [], 'lossList': [0.0, -1.4072631818056107, 0.0, 45.818852977752684, 0.0, 0.0, 0.0], 'rewardMean': 0.8272232656268806, 'totalEpisodes': 117, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.922035454179095
'totalSteps': 5120, 'rewardStep': 0.7806034082402558, 'errorList': [], 'lossList': [0.0, -1.3972946035861968, 0.0, 57.13352668762207, 0.0, 0.0, 0.0], 'rewardMean': 0.8155683012802244, 'totalEpisodes': 143, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.793183521160278
'totalSteps': 6400, 'rewardStep': 0.7534482348734173, 'errorList': [], 'lossList': [0.0, -1.390251130461693, 0.0, 46.70641925811768, 0.0, 0.0, 0.0], 'rewardMean': 0.803144287998863, 'totalEpisodes': 151, 'stepsPerEpisode': 68, 'rewardPerEpisode': 46.96388022915076
'totalSteps': 7680, 'rewardStep': 0.8872116751523494, 'errorList': [], 'lossList': [0.0, -1.3777429521083833, 0.0, 54.93926076889038, 0.0, 0.0, 0.0], 'rewardMean': 0.8171555191911107, 'totalEpisodes': 163, 'stepsPerEpisode': 44, 'rewardPerEpisode': 36.03575497407693
'totalSteps': 8960, 'rewardStep': 0.6704585221522964, 'errorList': [], 'lossList': [0.0, -1.3750877279043197, 0.0, 62.44325724601745, 0.0, 0.0, 0.0], 'rewardMean': 0.796198805328423, 'totalEpisodes': 177, 'stepsPerEpisode': 112, 'rewardPerEpisode': 91.39477245337802
'totalSteps': 10240, 'rewardStep': 0.8583996416983967, 'errorList': [], 'lossList': [0.0, -1.3668834400177001, 0.0, 36.36017777919769, 0.0, 0.0, 0.0], 'rewardMean': 0.8039739098746697, 'totalEpisodes': 183, 'stepsPerEpisode': 130, 'rewardPerEpisode': 104.16306934566504
'totalSteps': 11520, 'rewardStep': 0.613693593950573, 'errorList': [], 'lossList': [0.0, -1.3647298967838288, 0.0, 18.233972817659378, 0.0, 0.0, 0.0], 'rewardMean': 0.78283165254977, 'totalEpisodes': 188, 'stepsPerEpisode': 206, 'rewardPerEpisode': 172.27104514063623
'totalSteps': 12800, 'rewardStep': 0.5942056639575588, 'errorList': [], 'lossList': [0.0, -1.376285316348076, 0.0, 32.05603631734848, 0.0, 0.0, 0.0], 'rewardMean': 0.7639690536905489, 'totalEpisodes': 195, 'stepsPerEpisode': 64, 'rewardPerEpisode': 46.927763110998285
'totalSteps': 14080, 'rewardStep': 0.8581730690739501, 'errorList': [], 'lossList': [0.0, -1.3731249743700027, 0.0, 11.215752130746841, 0.0, 0.0, 0.0], 'rewardMean': 0.7576773471828032, 'totalEpisodes': 201, 'stepsPerEpisode': 105, 'rewardPerEpisode': 85.66330179389023
'totalSteps': 15360, 'rewardStep': 0.481612845484516, 'errorList': [], 'lossList': [0.0, -1.3817656594514847, 0.0, 20.530486508607865, 0.0, 0.0, 0.0], 'rewardMean': 0.7325102192132621, 'totalEpisodes': 205, 'stepsPerEpisode': 505, 'rewardPerEpisode': 388.13755543642844
'totalSteps': 16640, 'rewardStep': 0.3485831797333547, 'errorList': [], 'lossList': [0.0, -1.3618227142095565, 0.0, 6.770262242555618, 0.0, 0.0, 0.0], 'rewardMean': 0.6846389834316668, 'totalEpisodes': 207, 'stepsPerEpisode': 446, 'rewardPerEpisode': 318.43040361900347
'totalSteps': 17920, 'rewardStep': 0.5854919452505547, 'errorList': [], 'lossList': [0.0, -1.3355104565620421, 0.0, 9.052079709172249, 0.0, 0.0, 0.0], 'rewardMean': 0.6651278371326967, 'totalEpisodes': 209, 'stepsPerEpisode': 261, 'rewardPerEpisode': 221.87074112398668
'totalSteps': 19200, 'rewardStep': 0.5198677746268254, 'errorList': [], 'lossList': [0.0, -1.3155249756574632, 0.0, 2.1032225739955903, 0.0, 0.0, 0.0], 'rewardMean': 0.6417697911080376, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 907.1268155719539
'totalSteps': 20480, 'rewardStep': 0.9303890941699527, 'errorList': [0.2018209849613225, 0.032526358113959784, 0.23651529608624283, 0.18349651734931424, 0.2926891871636731, 0.0379291888846864, 0.17194121724133488, 0.05215570648364083, 0.11831034718245537, 0.0452624040030282, 0.12322728445827084, 0.27928828740196077, 0.10690006675778298, 0.04083870983828301, 0.05234875218831245, 0.042418524478548325, 0.04079014663541022, 0.1362365597983653, 0.10567756610517279, 0.13309921184294388, 0.027969209208836433, 0.08869220789074896, 0.02828930460630326, 0.2484478178472034, 0.05371201438695251, 0.050551387122316556, 0.05552661099199954, 0.025563578511286728, 0.10586088271129783, 0.08631803054263196, 0.09201904960624176, 0.13049126583894446, 0.20342318615087782, 0.15829599216270504, 0.21052656762120692, 0.08633924060064979, 0.14065376657297968, 0.26802175308712867, 0.04686359522968476, 0.054727873942171906, 0.10318031694259946, 0.15737220547680314, 0.12865935455114974, 0.03010164561735222, 0.051594207763083856, 0.09039479420297604, 0.054673353570578016, 0.03473273223388292, 0.062206521637013755, 0.09367796350958901], 'lossList': [0.0, -1.2774439650774, 0.0, 3.3520263853669165, 0.0, 0.0, 0.0], 'rewardMean': 0.6460875330097979, 'totalEpisodes': 210, 'stepsPerEpisode': 1271, 'rewardPerEpisode': 1077.8120692571722, 'successfulTests': 42
'totalSteps': 21760, 'rewardStep': 0.8857363750125301, 'errorList': [], 'lossList': [0.0, -1.2507443189620973, 0.0, 1.699974180907011, 0.0, 0.0, 0.0], 'rewardMean': 0.6676153182958211, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1086.1448307656226
'totalSteps': 23040, 'rewardStep': 0.8675398328132906, 'errorList': [], 'lossList': [0.0, -1.2431562292575835, 0.0, 1.2406703393161296, 0.0, 0.0, 0.0], 'rewardMean': 0.6685293374073107, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.5297230724614
'totalSteps': 24320, 'rewardStep': 0.9173090560391172, 'errorList': [], 'lossList': [0.0, -1.2231348752975464, 0.0, 1.001204609759152, 0.0, 0.0, 0.0], 'rewardMean': 0.6988908836161649, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1167.1898253203105
'totalSteps': 25600, 'rewardStep': 0.9869680485483001, 'errorList': [0.048810230186272056, 0.12647135966689507, 0.049457165168660826, 0.08848291978021766, 0.04699954145181, 0.13310126357195817, 0.07542855435198102, 0.041181036266933695, 0.12758660707622652, 0.04593791226039188, 0.05949387240117237, 0.05534482603502757, 0.15800272713466465, 0.12283116612602758, 0.10766419526353722, 0.10449481087953241, 0.07442720694195515, 0.1273805840441127, 0.17556779292439495, 0.15273599186367492, 0.21372085738863614, 0.05596096089471571, 0.03526253065431278, 0.10525557006099044, 0.05128966729353206, 0.05076890623387098, 0.13338118547906908, 0.12779503481706997, 0.06344428045251743, 0.09527233364450309, 0.051490486994917345, 0.18113234172794368, 0.038328204833063685, 0.054205030895089, 0.16526542762885077, 0.2075745206029159, 0.04020314758627162, 0.04123700616961139, 0.052396566479803376, 0.05553041731903497, 0.14629408034649935, 0.051653252145156175, 0.11320854154193183, 0.08997031655663129, 0.04332653024105795, 0.03672109607057753, 0.03939144853549468, 0.03924222133542569, 0.04693738464595464, 0.04800647691465804], 'lossList': [0.0, -1.1997698903083802, 0.0, 1.1017333270423115, 0.0, 0.0, 0.0], 'rewardMean': 0.7381671220752392, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1214.700943466581, 'successfulTests': 48
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=25600, timeSpent=103.28
