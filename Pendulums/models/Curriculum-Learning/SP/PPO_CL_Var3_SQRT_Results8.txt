#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 8
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.5195315126549785, 'errorList': [], 'lossList': [0.0, -1.4204127329587937, 0.0, 30.98557370185852, 0.0, 0.0, 0.0], 'rewardMean': 0.4319433521002167, 'totalEpisodes': 35, 'stepsPerEpisode': 34, 'rewardPerEpisode': 27.55217090860717
'totalSteps': 3840, 'rewardStep': 0.9721317269056265, 'errorList': [], 'lossList': [0.0, -1.4111692368984223, 0.0, 52.09386888504028, 0.0, 0.0, 0.0], 'rewardMean': 0.61200614370202, 'totalEpisodes': 67, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.91033963569332
'totalSteps': 5120, 'rewardStep': 0.8330622839220801, 'errorList': [], 'lossList': [0.0, -1.399263955950737, 0.0, 63.10806936264038, 0.0, 0.0, 0.0], 'rewardMean': 0.667270178757035, 'totalEpisodes': 91, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.179037396040483
'totalSteps': 6400, 'rewardStep': 0.8799281221393191, 'errorList': [], 'lossList': [0.0, -1.3880573695898055, 0.0, 77.76521188735961, 0.0, 0.0, 0.0], 'rewardMean': 0.7098017674334918, 'totalEpisodes': 116, 'stepsPerEpisode': 47, 'rewardPerEpisode': 36.7947843237555
'totalSteps': 7680, 'rewardStep': 0.3249252414489219, 'errorList': [], 'lossList': [0.0, -1.3926865357160567, 0.0, 56.962350587844846, 0.0, 0.0, 0.0], 'rewardMean': 0.6456556797693969, 'totalEpisodes': 134, 'stepsPerEpisode': 86, 'rewardPerEpisode': 50.34843281755657
'totalSteps': 8960, 'rewardStep': 0.9576301817473232, 'errorList': [5.043108145738803, 3.0264776003833993, 8.275544039661671, 6.087442339748355, 4.682777435865104, 11.058135515482622, 8.846570275592367, 3.412668739144678, 9.762063631811783, 9.746966503645936, 8.842907761182783, 7.1381909605696245, 6.29040874225516, 10.983121991634729, 5.226021459336685, 6.080462450646175, 4.034008108056796, 2.8652639492826153, 12.900135368912755, 6.601430628056098, 7.477160694701583, 4.306863238776972, 4.464198760790429, 5.5880698940469005, 2.6156636207874806, 10.375117062280086, 6.388340681727858, 7.28208049732484, 6.7565735028185285, 9.456165609479449, 11.4152432134641, 7.973326888174173, 11.878808151193637, 12.602132906009258, 6.18742843263482, 5.43287658801121, 8.16304570541353, 8.507607320766152, 8.722373638383804, 8.621214355100697, 9.582171706109818, 11.458787205751088, 4.135277545786707, 8.98722514378667, 7.132947251215982, 11.81871506787885, 10.845933896092319, 9.970880982934778, 13.26788319990048, 6.312734526750916], 'lossList': [0.0, -1.3936424005031585, 0.0, 49.047096166610714, 0.0, 0.0, 0.0], 'rewardMean': 0.6902234657662435, 'totalEpisodes': 146, 'stepsPerEpisode': 105, 'rewardPerEpisode': 80.92687988609441, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.5837861781895022, 'errorList': [], 'lossList': [0.0, -1.3936592280864715, 0.0, 41.42793875694275, 0.0, 0.0, 0.0], 'rewardMean': 0.6769188048191508, 'totalEpisodes': 152, 'stepsPerEpisode': 160, 'rewardPerEpisode': 127.49222959163589
'totalSteps': 11520, 'rewardStep': 0.7810028119215835, 'errorList': [], 'lossList': [0.0, -1.3858235198259354, 0.0, 18.594944367408754, 0.0, 0.0, 0.0], 'rewardMean': 0.6884836944971988, 'totalEpisodes': 157, 'stepsPerEpisode': 61, 'rewardPerEpisode': 51.49351490326982
'totalSteps': 12800, 'rewardStep': 0.6704694283723196, 'errorList': [], 'lossList': [0.0, -1.38660426735878, 0.0, 34.40719558238983, 0.0, 0.0, 0.0], 'rewardMean': 0.6866822678847109, 'totalEpisodes': 162, 'stepsPerEpisode': 227, 'rewardPerEpisode': 188.6011295144314
'totalSteps': 14080, 'rewardStep': 0.823305929181848, 'errorList': [], 'lossList': [0.0, -1.3922611892223358, 0.0, 9.354489273428918, 0.0, 0.0, 0.0], 'rewardMean': 0.7345773416483503, 'totalEpisodes': 165, 'stepsPerEpisode': 41, 'rewardPerEpisode': 33.722557805831485
'totalSteps': 15360, 'rewardStep': 0.8047772508870707, 'errorList': [], 'lossList': [0.0, -1.39168330848217, 0.0, 9.59934268772602, 0.0, 0.0, 0.0], 'rewardMean': 0.7631019154715595, 'totalEpisodes': 169, 'stepsPerEpisode': 128, 'rewardPerEpisode': 111.26719460808516
'totalSteps': 16640, 'rewardStep': 0.686913139368052, 'errorList': [], 'lossList': [0.0, -1.3907828319072724, 0.0, 3.6264650088548662, 0.0, 0.0, 0.0], 'rewardMean': 0.734580056717802, 'totalEpisodes': 173, 'stepsPerEpisode': 146, 'rewardPerEpisode': 117.80225093604743
'totalSteps': 17920, 'rewardStep': 0.9848553409685616, 'errorList': [0.624087967514645, 0.20722867982465015, 0.19127407940045962, 0.2149505669290373, 0.7477347280295302, 0.3016349522270005, 0.5917270757920811, 0.7297859306691789, 0.8603444102296645, 1.0604530471497158, 0.43735556033035583, 0.34516387372054325, 1.2016890757143344, 0.9282016381586631, 0.5024171170950607, 0.07134725813367335, 0.2047046345802308, 0.5587542007019954, 0.07512185886350053, 0.50800903442847, 0.1451187595523334, 0.7059703922184287, 0.1620043299443576, 0.10958142306222159, 0.07805777711621714, 0.24992411815537405, 0.6502122776329576, 0.24533048581731226, 0.07670530135749569, 0.41419965617036086, 0.1568403530129389, 0.07988868825451768, 0.698372935702106, 0.9190685701505482, 0.36272303561955155, 0.21026705277292426, 0.9097047266974638, 0.6371352657832584, 0.40298667857830944, 0.23636283771240038, 0.1807132541579139, 0.07245640690914532, 0.46211695243001655, 0.590559442621914, 0.48342538673292235, 0.0677700052333409, 0.6908457157074769, 0.4930127666098787, 0.2806520516381877, 0.3196708087490629], 'lossList': [0.0, -1.3944705265760422, 0.0, 7.8016950023174285, 0.0, 0.0, 0.0], 'rewardMean': 0.7497593624224501, 'totalEpisodes': 177, 'stepsPerEpisode': 28, 'rewardPerEpisode': 25.508377033382814, 'successfulTests': 13
'totalSteps': 19200, 'rewardStep': 0.4948568878154993, 'errorList': [], 'lossList': [0.0, -1.405029637813568, 0.0, 5.091244083046913, 0.0, 0.0, 0.0], 'rewardMean': 0.7112522389900683, 'totalEpisodes': 178, 'stepsPerEpisode': 730, 'rewardPerEpisode': 548.7441996422718
'totalSteps': 20480, 'rewardStep': 0.8913000739619586, 'errorList': [], 'lossList': [0.0, -1.386775113940239, 0.0, 2.1850586552917957, 0.0, 0.0, 0.0], 'rewardMean': 0.767889722241372, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1011.5323064851987
'totalSteps': 21760, 'rewardStep': 0.7767796119952972, 'errorList': [], 'lossList': [0.0, -1.3371147006750106, 0.0, 1.7236974829435348, 0.0, 0.0, 0.0], 'rewardMean': 0.7498046652661693, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 878.2820347427819
'totalSteps': 23040, 'rewardStep': 0.741539094000304, 'errorList': [], 'lossList': [0.0, -1.2818568551540375, 0.0, 0.6630688381195068, 0.0, 0.0, 0.0], 'rewardMean': 0.7655799568472494, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.0998404437075
'totalSteps': 24320, 'rewardStep': 0.8317122073482135, 'errorList': [], 'lossList': [0.0, -1.2269325369596482, 0.0, 0.8004570158943534, 0.0, 0.0, 0.0], 'rewardMean': 0.7706508963899125, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1119.9523800585937
'totalSteps': 25600, 'rewardStep': 0.9192189664309296, 'errorList': [], 'lossList': [0.0, -1.1979088413715362, 0.0, 0.44665614733472464, 0.0, 0.0, 0.0], 'rewardMean': 0.7955258501957735, 'totalEpisodes': 178, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.5486952870292
#maxSuccessfulTests=13, maxSuccessfulTestsAtStep=17920, timeSpent=96.93
