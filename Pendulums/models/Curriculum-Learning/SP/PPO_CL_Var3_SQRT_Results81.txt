#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 81
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6893403398049232, 'errorList': [], 'lossList': [0.0, -1.4055571365356445, 0.0, 35.11375904083252, 0.0, 0.0, 0.0], 'rewardMean': 0.8263149458578165, 'totalEpisodes': 31, 'stepsPerEpisode': 39, 'rewardPerEpisode': 35.74973862084732
'totalSteps': 3840, 'rewardStep': 0.7430058180096185, 'errorList': [], 'lossList': [0.0, -1.39185972571373, 0.0, 42.48447986602783, 0.0, 0.0, 0.0], 'rewardMean': 0.7985452365750838, 'totalEpisodes': 49, 'stepsPerEpisode': 21, 'rewardPerEpisode': 13.842146260703158
'totalSteps': 5120, 'rewardStep': 0.838866134877635, 'errorList': [], 'lossList': [0.0, -1.3787720376253128, 0.0, 39.4315088224411, 0.0, 0.0, 0.0], 'rewardMean': 0.8086254611507215, 'totalEpisodes': 62, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.41907758304859
'totalSteps': 6400, 'rewardStep': 0.532151409980439, 'errorList': [], 'lossList': [0.0, -1.3667329227924347, 0.0, 68.37908707618713, 0.0, 0.0, 0.0], 'rewardMean': 0.753330650916665, 'totalEpisodes': 75, 'stepsPerEpisode': 82, 'rewardPerEpisode': 60.553808158262406
'totalSteps': 7680, 'rewardStep': 0.631071470938005, 'errorList': [], 'lossList': [0.0, -1.3584012490510942, 0.0, 22.032506753206253, 0.0, 0.0, 0.0], 'rewardMean': 0.7329541209202217, 'totalEpisodes': 79, 'stepsPerEpisode': 279, 'rewardPerEpisode': 234.91467384015684
'totalSteps': 8960, 'rewardStep': 0.4407859962554276, 'errorList': [], 'lossList': [0.0, -1.3548185074329375, 0.0, 143.1548664855957, 0.0, 0.0, 0.0], 'rewardMean': 0.6912158173966797, 'totalEpisodes': 104, 'stepsPerEpisode': 41, 'rewardPerEpisode': 28.20935019056282
'totalSteps': 10240, 'rewardStep': 0.8303135282970971, 'errorList': [], 'lossList': [0.0, -1.360103525519371, 0.0, 28.13749640226364, 0.0, 0.0, 0.0], 'rewardMean': 0.708603031259232, 'totalEpisodes': 112, 'stepsPerEpisode': 156, 'rewardPerEpisode': 131.188356435896
'totalSteps': 11520, 'rewardStep': 0.7382292493074497, 'errorList': [], 'lossList': [0.0, -1.3544190603494644, 0.0, 42.88214626312256, 0.0, 0.0, 0.0], 'rewardMean': 0.7118948332645895, 'totalEpisodes': 121, 'stepsPerEpisode': 75, 'rewardPerEpisode': 59.808018724410594
'totalSteps': 12800, 'rewardStep': 0.6579563208399243, 'errorList': [], 'lossList': [0.0, -1.3151421111822128, 0.0, 9.84542467713356, 0.0, 0.0, 0.0], 'rewardMean': 0.706500982022123, 'totalEpisodes': 126, 'stepsPerEpisode': 153, 'rewardPerEpisode': 107.34696969524087
'totalSteps': 14080, 'rewardStep': 0.8887533465915846, 'errorList': [], 'lossList': [0.0, -1.3017186111211776, 0.0, 7.415321068763733, 0.0, 0.0, 0.0], 'rewardMean': 0.6990473614902103, 'totalEpisodes': 131, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.298731595289695
'totalSteps': 15360, 'rewardStep': 0.4423481177830077, 'errorList': [], 'lossList': [0.0, -1.315137676000595, 0.0, 4.793049000501632, 0.0, 0.0, 0.0], 'rewardMean': 0.6743481392880187, 'totalEpisodes': 135, 'stepsPerEpisode': 298, 'rewardPerEpisode': 225.48905869296175
'totalSteps': 16640, 'rewardStep': 0.797313703526775, 'errorList': [], 'lossList': [0.0, -1.3189729970693589, 0.0, 4.816821997761727, 0.0, 0.0, 0.0], 'rewardMean': 0.6797789278397344, 'totalEpisodes': 139, 'stepsPerEpisode': 381, 'rewardPerEpisode': 325.9614513100912
'totalSteps': 17920, 'rewardStep': 0.749526568126947, 'errorList': [], 'lossList': [0.0, -1.327481460571289, 0.0, 4.313014631271362, 0.0, 0.0, 0.0], 'rewardMean': 0.6708449711646657, 'totalEpisodes': 141, 'stepsPerEpisode': 140, 'rewardPerEpisode': 112.78649074038715
'totalSteps': 19200, 'rewardStep': 0.9202491041445666, 'errorList': [], 'lossList': [0.0, -1.299163453578949, 0.0, 2.90174830108881, 0.0, 0.0, 0.0], 'rewardMean': 0.7096547405810785, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.9173052088813
'totalSteps': 20480, 'rewardStep': 0.7676333229993032, 'errorList': [], 'lossList': [0.0, -1.2212396657466889, 0.0, 2.134152073636651, 0.0, 0.0, 0.0], 'rewardMean': 0.7233109257872082, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.268752676575
'totalSteps': 21760, 'rewardStep': 0.7729345892887964, 'errorList': [], 'lossList': [0.0, -1.1571018022298813, 0.0, 1.499464310631156, 0.0, 0.0, 0.0], 'rewardMean': 0.7565257850905451, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.3953617095524
'totalSteps': 23040, 'rewardStep': 0.9662385650891223, 'errorList': [0.010373413829536875, 0.013658201847759809, 0.015712336828182526, 0.019390164650944352, 0.011322136466795843, 0.00983429484846836, 0.013036614800751857, 0.010529187045210504, 0.027147363712631253, 0.023595349084117652, 0.017372434408048136, 0.015964433359473788, 0.0210704438028044, 0.012843892451999777, 0.02097425930442153, 0.011527258118938243, 0.010567930935511253, 0.014108512526232745, 0.011202971783868722, 0.009740811033123431, 0.010970002044107746, 0.014169729055911788, 0.015046069860361505, 0.011402144362849661, 0.024038484686224684, 0.023953727313883073, 0.010607390789670944, 0.018830143528885693, 0.0117924456540373, 0.014661496940413516, 0.01846152718940148, 0.022711912769880334, 0.016471883466358545, 0.010517248336092747, 0.02619250171388066, 0.02190448701714808, 0.017603604615683104, 0.03433126756375269, 0.014551533242391114, 0.015567481392297218, 0.00976558058637316, 0.016188204995620317, 0.010518236593728972, 0.012878048426975675, 0.01973323387140139, 0.009990383679676591, 0.02685298536635441, 0.03678940372135251, 0.022960488277145603, 0.030955846293328232], 'lossList': [0.0, -1.1173598551750183, 0.0, 1.12276966907084, 0.0, 0.0, 0.0], 'rewardMean': 0.7701182887697477, 'totalEpisodes': 141, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.9204940662114, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=79.45
