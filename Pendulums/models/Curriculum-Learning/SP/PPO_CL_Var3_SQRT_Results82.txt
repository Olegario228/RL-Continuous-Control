#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 82
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.9395987807032167, 'errorList': [], 'lossList': [0.0, -1.4186035758256912, 0.0, 32.36305640220642, 0.0, 0.0, 0.0], 'rewardMean': 0.8666843909416839, 'totalEpisodes': 34, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.3999312888610245
'totalSteps': 3840, 'rewardStep': 0.7079097510914988, 'errorList': [], 'lossList': [0.0, -1.4002557182312012, 0.0, 44.191247644424436, 0.0, 0.0, 0.0], 'rewardMean': 0.8137595109916221, 'totalEpisodes': 53, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.928009445723796
'totalSteps': 5120, 'rewardStep': 0.32719901109826327, 'errorList': [], 'lossList': [0.0, -1.375246022939682, 0.0, 41.431970291137695, 0.0, 0.0, 0.0], 'rewardMean': 0.6921193860182825, 'totalEpisodes': 66, 'stepsPerEpisode': 86, 'rewardPerEpisode': 57.441726616820254
'totalSteps': 6400, 'rewardStep': 0.6522535584652204, 'errorList': [], 'lossList': [0.0, -1.3572557681798936, 0.0, 61.92197045326233, 0.0, 0.0, 0.0], 'rewardMean': 0.68414622050767, 'totalEpisodes': 81, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.49223550526952
'totalSteps': 7680, 'rewardStep': 0.8212935390262701, 'errorList': [], 'lossList': [0.0, -1.3472851860523223, 0.0, 80.93167644500733, 0.0, 0.0, 0.0], 'rewardMean': 0.7070041069274368, 'totalEpisodes': 94, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.35643914942665
'totalSteps': 8960, 'rewardStep': 0.46326619991378293, 'errorList': [], 'lossList': [0.0, -1.3524254608154296, 0.0, 81.75706876754761, 0.0, 0.0, 0.0], 'rewardMean': 0.6721844059254863, 'totalEpisodes': 116, 'stepsPerEpisode': 80, 'rewardPerEpisode': 64.81721190725972
'totalSteps': 10240, 'rewardStep': 0.6131267761030592, 'errorList': [], 'lossList': [0.0, -1.3671806395053863, 0.0, 40.97322614192963, 0.0, 0.0, 0.0], 'rewardMean': 0.6648022021976828, 'totalEpisodes': 129, 'stepsPerEpisode': 65, 'rewardPerEpisode': 55.54816146376365
'totalSteps': 11520, 'rewardStep': 0.5982726570274923, 'errorList': [], 'lossList': [0.0, -1.3706229197978974, 0.0, 23.01663489818573, 0.0, 0.0, 0.0], 'rewardMean': 0.6574100305121061, 'totalEpisodes': 138, 'stepsPerEpisode': 227, 'rewardPerEpisode': 175.2066124317554
'totalSteps': 12800, 'rewardStep': 0.9619460030857269, 'errorList': [0.6842857027053905, 0.5499748017652328, 1.1698979198463735, 0.7645043364535978, 0.41156224978747735, 1.0373116507765843, 1.3665408923683433, 0.47892037488371253, 1.1352921945906884, 0.9461045143494117, 1.0137324906262057, 0.5204867088671069, 1.1244650132193375, 0.5676790777551828, 1.2246893495851146, 1.3438484399523243, 1.0200481780474824, 0.687653153124923, 0.46330225005751186, 0.6181360519551174, 0.3128814681501193, 0.7334308003519866, 0.46980775720474166, 0.2924217482764028, 1.1046924517834384, 0.6780686558664818, 0.2776480565034692, 0.5934797817192058, 0.9066546324654592, 0.5827172029586493, 0.4794658611632155, 0.45099773662258946, 1.4790531076241311, 0.9115326898632911, 0.6483116783713434, 0.29035444655500614, 0.3099533245643638, 0.5110383691791608, 0.5952178890072404, 0.5816322444186729, 0.3722041327816235, 1.1661061411464235, 0.3921034688381705, 0.9198357397665391, 0.48311700750098907, 0.3885545952979609, 0.8333438952855453, 0.5809449575932008, 0.36473272893661174, 0.515612054622018], 'lossList': [0.0, -1.3550784128904343, 0.0, 18.389659309387206, 0.0, 0.0, 0.0], 'rewardMean': 0.6878636277694682, 'totalEpisodes': 144, 'stepsPerEpisode': 41, 'rewardPerEpisode': 36.65080084748947, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.45851859297532543, 'errorList': [], 'lossList': [0.0, -1.3436285138130188, 0.0, 10.107514710426331, 0.0, 0.0, 0.0], 'rewardMean': 0.6543384869489856, 'totalEpisodes': 146, 'stepsPerEpisode': 531, 'rewardPerEpisode': 353.8545434271453
'totalSteps': 15360, 'rewardStep': 0.6766310442809769, 'errorList': [], 'lossList': [0.0, -1.3301847976446153, 0.0, 12.13054975271225, 0.0, 0.0, 0.0], 'rewardMean': 0.6280417133067616, 'totalEpisodes': 147, 'stepsPerEpisode': 1043, 'rewardPerEpisode': 798.8302271429018
'totalSteps': 16640, 'rewardStep': 0.8903388705252888, 'errorList': [], 'lossList': [0.0, -1.2856577217578888, 0.0, 5.366065437793732, 0.0, 0.0, 0.0], 'rewardMean': 0.6462846252501406, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.6795935595409
'totalSteps': 17920, 'rewardStep': 0.8631227066663165, 'errorList': [], 'lossList': [0.0, -1.2229850935935973, 0.0, 3.888559879511595, 0.0, 0.0, 0.0], 'rewardMean': 0.699876994806946, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.5228206208747
'totalSteps': 19200, 'rewardStep': 0.9223408688650968, 'errorList': [], 'lossList': [0.0, -1.1685759705305099, 0.0, 3.7741426273435352, 0.0, 0.0, 0.0], 'rewardMean': 0.7268857258469336, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.3931666081583
'totalSteps': 20480, 'rewardStep': 0.9510222509105916, 'errorList': [0.10294263805735696, 0.10352075614331326, 0.10377956132613, 0.10562434976813528, 0.10452881028475362, 0.10411807735188998, 0.10306260665632952, 0.10320637441274466, 0.10449651850709907, 0.10426221413124497, 0.10343934299422584, 0.10315210784955339, 0.1037988903314977, 0.10340572806991272, 0.11879851675591245, 0.1448237993813569, 0.10327334774690305, 0.13255206191438132, 0.10334626695769855, 0.1295214040174314, 0.10344297962881024, 0.1036937551788701, 0.10401739267767672, 0.10350551990972316, 0.10409677409020979, 0.10416324268854098, 0.12052958214948672, 0.10370129065929413, 0.10373476480463768, 0.10320420764677012, 0.1029847853013023, 0.1208602839725918, 0.1361943925260718, 0.10320138445461828, 0.1034901437485258, 0.10349290165197744, 0.10312503146826812, 0.10336503233445415, 0.10375562166676831, 0.1398991025600819, 0.1038565375595153, 0.10801386021257686, 0.14057145876427266, 0.10383454371536432, 0.10344959815698597, 0.10361828433563487, 0.10298698034013967, 0.10392447394638328, 0.10378377888071066, 0.12100332961620032], 'lossList': [0.0, -1.1252489441633224, 0.0, 3.0138217493891717, 0.0, 0.0, 0.0], 'rewardMean': 0.7398585970353657, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1192.1866343494594, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=91.85
