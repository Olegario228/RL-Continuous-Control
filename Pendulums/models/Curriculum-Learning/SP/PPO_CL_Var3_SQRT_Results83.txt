#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 83
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.5966739625694394, 'errorList': [], 'lossList': [0.0, -1.417743558883667, 0.0, 30.66874897003174, 0.0, 0.0, 0.0], 'rewardMean': 0.47051457705744715, 'totalEpisodes': 27, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.26716063459525
'totalSteps': 3840, 'rewardStep': 0.8860850296300434, 'errorList': [], 'lossList': [0.0, -1.4010719060897827, 0.0, 45.904995861053465, 0.0, 0.0, 0.0], 'rewardMean': 0.6090380612483126, 'totalEpisodes': 42, 'stepsPerEpisode': 50, 'rewardPerEpisode': 39.00037176612139
'totalSteps': 5120, 'rewardStep': 0.8558909947219633, 'errorList': [], 'lossList': [0.0, -1.3905327069759368, 0.0, 50.09384905815124, 0.0, 0.0, 0.0], 'rewardMean': 0.6707512946167253, 'totalEpisodes': 52, 'stepsPerEpisode': 71, 'rewardPerEpisode': 57.83043077321613
'totalSteps': 6400, 'rewardStep': 0.9033963277181244, 'errorList': [], 'lossList': [0.0, -1.3876538509130478, 0.0, 56.762812576293946, 0.0, 0.0, 0.0], 'rewardMean': 0.7172803012370051, 'totalEpisodes': 61, 'stepsPerEpisode': 25, 'rewardPerEpisode': 16.421778297819973
'totalSteps': 7680, 'rewardStep': 0.9289515520694038, 'errorList': [], 'lossList': [0.0, -1.3825401216745377, 0.0, 84.4099045753479, 0.0, 0.0, 0.0], 'rewardMean': 0.752558843042405, 'totalEpisodes': 73, 'stepsPerEpisode': 85, 'rewardPerEpisode': 68.68096533481766
'totalSteps': 8960, 'rewardStep': 0.7682625380019303, 'errorList': [], 'lossList': [0.0, -1.3799061125516892, 0.0, 118.17706979751587, 0.0, 0.0, 0.0], 'rewardMean': 0.7548022280366229, 'totalEpisodes': 91, 'stepsPerEpisode': 72, 'rewardPerEpisode': 60.63121995476239
'totalSteps': 10240, 'rewardStep': 0.9046776949739014, 'errorList': [], 'lossList': [0.0, -1.3690501254796983, 0.0, 58.69386650085449, 0.0, 0.0, 0.0], 'rewardMean': 0.7735366614037826, 'totalEpisodes': 100, 'stepsPerEpisode': 42, 'rewardPerEpisode': 36.80803387435143
'totalSteps': 11520, 'rewardStep': 0.673060231223184, 'errorList': [], 'lossList': [0.0, -1.3580239534378051, 0.0, 14.355812408924102, 0.0, 0.0, 0.0], 'rewardMean': 0.7623726136059383, 'totalEpisodes': 104, 'stepsPerEpisode': 69, 'rewardPerEpisode': 56.15861767000247
'totalSteps': 12800, 'rewardStep': 0.9023728361457611, 'errorList': [], 'lossList': [0.0, -1.351701791882515, 0.0, 16.883277823925017, 0.0, 0.0, 0.0], 'rewardMean': 0.7763726358599207, 'totalEpisodes': 108, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.562409995584475
'totalSteps': 14080, 'rewardStep': 0.3949212187686483, 'errorList': [], 'lossList': [0.0, -1.3657875955104828, 0.0, 7.708324976563453, 0.0, 0.0, 0.0], 'rewardMean': 0.78142923858224, 'totalEpisodes': 110, 'stepsPerEpisode': 534, 'rewardPerEpisode': 374.35090818490477
'totalSteps': 15360, 'rewardStep': 0.692908046619333, 'errorList': [], 'lossList': [0.0, -1.3984159314632416, 0.0, 7.1168945324420925, 0.0, 0.0, 0.0], 'rewardMean': 0.7910526469872294, 'totalEpisodes': 111, 'stepsPerEpisode': 1258, 'rewardPerEpisode': 971.3152172200556
'totalSteps': 16640, 'rewardStep': 0.8826398682877257, 'errorList': [], 'lossList': [0.0, -1.3885403645038605, 0.0, 4.559233963936567, 0.0, 0.0, 0.0], 'rewardMean': 0.7907081308529975, 'totalEpisodes': 111, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.5305376178926
'totalSteps': 17920, 'rewardStep': 0.8587828401928976, 'errorList': [], 'lossList': [0.0, -1.35121710896492, 0.0, 3.683748723939061, 0.0, 0.0, 0.0], 'rewardMean': 0.7909973154000909, 'totalEpisodes': 111, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.24601556829
'totalSteps': 19200, 'rewardStep': 0.8808622578052105, 'errorList': [], 'lossList': [0.0, -1.332623171210289, 0.0, 2.9737625148892404, 0.0, 0.0, 0.0], 'rewardMean': 0.7887439084087996, 'totalEpisodes': 111, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.167664401254
'totalSteps': 20480, 'rewardStep': 0.9504229300205119, 'errorList': [0.1287537429827323, 0.09623415371539608, 0.09330858580995878, 0.05317366075984895, 0.04903790490820628, 0.10348700223863332, 0.08765563135851323, 0.077950060638724, 0.09628651069577686, 0.10229257668976932, 0.071683597952993, 0.10333598953283447, 0.09929885283504915, 0.043936802821947105, 0.14952973590008936, 0.04649773429736316, 0.06182096069783363, 0.09635779669340724, 0.06353504140292619, 0.05246554946333704, 0.05004291142311095, 0.14219269217770683, 0.10762543937608243, 0.06580658902607417, 0.04578748057350394, 0.11374724773432739, 0.08689421584610833, 0.08328273587273125, 0.04308885531040091, 0.04371000437787788, 0.051892409655749316, 0.10278813984880067, 0.07395311928057831, 0.04188225908650365, 0.10601393680506051, 0.11021736705112446, 0.04930735424690749, 0.06654966905240171, 0.06977490292807434, 0.08282449531096323, 0.04275181159902631, 0.06697051713466748, 0.1202879656997772, 0.10513875546167636, 0.1654041156228561, 0.11132061214287045, 0.08449464434578484, 0.17000356034207226, 0.07758072263821096, 0.06129525172218805], 'lossList': [0.0, -1.2940888434648514, 0.0, 2.5137147998437284, 0.0, 0.0, 0.0], 'rewardMean': 0.7908910462039104, 'totalEpisodes': 111, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1165.8020353931668, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=69.94
