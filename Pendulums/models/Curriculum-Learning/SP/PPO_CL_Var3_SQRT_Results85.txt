#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 85
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.8584613665903847, 'errorList': [], 'lossList': [0.0, -1.4033936154842377, 0.0, 24.75596191883087, 0.0, 0.0, 0.0], 'rewardMean': 0.829130163544674, 'totalEpisodes': 14, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.954351225724983
'totalSteps': 3840, 'rewardStep': 0.43573125414659286, 'errorList': [], 'lossList': [0.0, -1.3897707909345627, 0.0, 35.83540291786194, 0.0, 0.0, 0.0], 'rewardMean': 0.6979971937453135, 'totalEpisodes': 26, 'stepsPerEpisode': 167, 'rewardPerEpisode': 116.52611888055564
'totalSteps': 5120, 'rewardStep': 0.6128773906001965, 'errorList': [], 'lossList': [0.0, -1.3821584153175355, 0.0, 21.768125230073927, 0.0, 0.0, 0.0], 'rewardMean': 0.6767172429590342, 'totalEpisodes': 31, 'stepsPerEpisode': 191, 'rewardPerEpisode': 133.9183396679578
'totalSteps': 6400, 'rewardStep': 0.5143860782468934, 'errorList': [], 'lossList': [0.0, -1.3770710813999176, 0.0, 53.61363261222839, 0.0, 0.0, 0.0], 'rewardMean': 0.6442510100166061, 'totalEpisodes': 42, 'stepsPerEpisode': 204, 'rewardPerEpisode': 135.46143967213297
'totalSteps': 7680, 'rewardStep': 0.5445063171560818, 'errorList': [], 'lossList': [0.0, -1.366431792974472, 0.0, 61.55970441818237, 0.0, 0.0, 0.0], 'rewardMean': 0.6276268945398521, 'totalEpisodes': 51, 'stepsPerEpisode': 193, 'rewardPerEpisode': 138.86941907416383
'totalSteps': 8960, 'rewardStep': 0.9439566546811892, 'errorList': [249.68721046651706, 169.08325358809654, 375.59875188505595, 346.2211119200366, 332.4868426424612, 267.9979384119778, 341.9996597998188, 361.68441857211707, 129.26233189662582, 337.50187707321345, 370.85518907660895, 297.4494360620154, 366.55439750681853, 253.59540079092838, 227.47630435865042, 366.4355394702356, 236.89167460420964, 348.272290091786, 388.17762997365725, 259.4669976958444, 327.84568519073144, 327.67590678867856, 318.01546969894173, 296.11166560550186, 275.8963937545494, 333.37655494378583, 266.5635805041517, 293.034306212934, 257.683269890607, 351.40097813269006, 373.5312795829468, 330.77727134911646, 315.3440917287083, 275.68546326251953, 327.13035220636726, 200.10582800039217, 340.90110812411683, 296.632516076383, 383.1956468756548, 322.3000538313132, 279.9832735324331, 208.92962268922838, 324.85376385496613, 337.7640614084297, 281.72551341097306, 377.22967523386114, 333.1718078896833, 337.68980302883887, 315.8980547864537, 226.5448270635442], 'lossList': [0.0, -1.353998589515686, 0.0, 149.30951190948485, 0.0, 0.0, 0.0], 'rewardMean': 0.6728168602743289, 'totalEpisodes': 81, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.982808041635346, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.5536771968033785, 'errorList': [], 'lossList': [0.0, -1.353402453660965, 0.0, 82.58261564254761, 0.0, 0.0, 0.0], 'rewardMean': 0.65792440234046, 'totalEpisodes': 108, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.0788928005630383
'totalSteps': 11520, 'rewardStep': 0.5979942389499745, 'errorList': [], 'lossList': [0.0, -1.3447222512960435, 0.0, 34.372899165153505, 0.0, 0.0, 0.0], 'rewardMean': 0.6512654952970727, 'totalEpisodes': 118, 'stepsPerEpisode': 47, 'rewardPerEpisode': 37.77907203710455
'totalSteps': 12800, 'rewardStep': 0.7391729525909567, 'errorList': [], 'lossList': [0.0, -1.3179246664047242, 0.0, 25.163909757137297, 0.0, 0.0, 0.0], 'rewardMean': 0.6600562410264612, 'totalEpisodes': 128, 'stepsPerEpisode': 123, 'rewardPerEpisode': 94.33225240054665
'totalSteps': 14080, 'rewardStep': 0.6425523290274551, 'errorList': [], 'lossList': [0.0, -1.2975661224126815, 0.0, 21.589512369632722, 0.0, 0.0, 0.0], 'rewardMean': 0.6443315778793104, 'totalEpisodes': 132, 'stepsPerEpisode': 57, 'rewardPerEpisode': 48.74679523716282
'totalSteps': 15360, 'rewardStep': 0.860551644108502, 'errorList': [], 'lossList': [0.0, -1.29996890604496, 0.0, 10.505196475982666, 0.0, 0.0, 0.0], 'rewardMean': 0.644540605631122, 'totalEpisodes': 137, 'stepsPerEpisode': 40, 'rewardPerEpisode': 37.222586532977616
'totalSteps': 16640, 'rewardStep': 0.7746922214694452, 'errorList': [], 'lossList': [0.0, -1.302588340640068, 0.0, 7.745563446283341, 0.0, 0.0, 0.0], 'rewardMean': 0.6784367023634073, 'totalEpisodes': 138, 'stepsPerEpisode': 320, 'rewardPerEpisode': 282.96632058716744
'totalSteps': 17920, 'rewardStep': 0.7628809810776356, 'errorList': [], 'lossList': [0.0, -1.2948825472593308, 0.0, 4.82591270327568, 0.0, 0.0, 0.0], 'rewardMean': 0.6934370614111514, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 866.0927378111696
'totalSteps': 19200, 'rewardStep': 0.7099310688341028, 'errorList': [], 'lossList': [0.0, -1.2795158904790878, 0.0, 3.275728297829628, 0.0, 0.0, 0.0], 'rewardMean': 0.7129915604698721, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 879.9962247866835
'totalSteps': 20480, 'rewardStep': 0.7474129720669447, 'errorList': [], 'lossList': [0.0, -1.2481618416309357, 0.0, 1.545908046066761, 0.0, 0.0, 0.0], 'rewardMean': 0.7332822259609585, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 920.0046377081383
'totalSteps': 21760, 'rewardStep': 0.8831974870703019, 'errorList': [], 'lossList': [0.0, -1.2242039078474045, 0.0, 2.2240636163949965, 0.0, 0.0, 0.0], 'rewardMean': 0.7272063091998696, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.983195306539
'totalSteps': 23040, 'rewardStep': 0.838997426170638, 'errorList': [], 'lossList': [0.0, -1.2072709465026856, 0.0, 2.342521238550544, 0.0, 0.0, 0.0], 'rewardMean': 0.7557383321365957, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.0556152533552
'totalSteps': 24320, 'rewardStep': 0.7545462989812536, 'errorList': [], 'lossList': [0.0, -1.1699253851175309, 0.0, 1.6826465497538448, 0.0, 0.0, 0.0], 'rewardMean': 0.7713935381397236, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.3150749371907
'totalSteps': 25600, 'rewardStep': 0.9437983889244972, 'errorList': [0.08400524026726744, 0.04406487525315154, 0.0504252810341509, 0.03144278009003684, 0.043415069984151816, 0.07796718220020907, 0.0632643084039854, 0.06455922910768713, 0.03833652814366406, 0.04441282178282041, 0.05396210456817251, 0.06932706734867412, 0.053317804663511326, 0.036178832687715816, 0.0302740921171109, 0.02829642543575412, 0.10327119677353096, 0.021941741191347328, 0.0572727491777892, 0.029257996101077523, 0.027177946404512567, 0.012891791559289252, 0.01983764781952959, 0.03233543559220995, 0.09783665893851069, 0.0671208378960179, 0.09201211412647448, 0.051949281412941035, 0.06643253838706782, 0.01873826196806123, 0.051634726543582726, 0.026410072150968692, 0.03026173889934262, 0.0418423741112941, 0.048742033839101466, 0.03037527601760177, 0.07492029677501227, 0.05004925594102911, 0.025087579333020967, 0.02659916734775664, 0.041713456869626145, 0.017127200747946804, 0.049361537538321516, 0.036879570926841916, 0.030228489284162732, 0.02300739711661124, 0.019439743226342815, 0.04284478527182244, 0.04966687495681091, 0.03481182490603238], 'lossList': [0.0, -1.1236706191301347, 0.0, 1.3844280363991857, 0.0, 0.0, 0.0], 'rewardMean': 0.7918560817730775, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.238866471837, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=106.7
