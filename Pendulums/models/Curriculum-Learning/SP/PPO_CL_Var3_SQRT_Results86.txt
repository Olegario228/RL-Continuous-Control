#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 86
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.5207819328957195, 'errorList': [], 'lossList': [0.0, -1.421821967959404, 0.0, 29.291517839431762, 0.0, 0.0, 0.0], 'rewardMean': 0.5187879172975862, 'totalEpisodes': 18, 'stepsPerEpisode': 101, 'rewardPerEpisode': 67.91003960075288
'totalSteps': 3840, 'rewardStep': 0.8215496371051139, 'errorList': [], 'lossList': [0.0, -1.4165815258026122, 0.0, 41.665086760520936, 0.0, 0.0, 0.0], 'rewardMean': 0.6197084905667621, 'totalEpisodes': 29, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.13291710669998
'totalSteps': 5120, 'rewardStep': 0.7551946019142364, 'errorList': [], 'lossList': [0.0, -1.41416956782341, 0.0, 31.49073893070221, 0.0, 0.0, 0.0], 'rewardMean': 0.6535800184036307, 'totalEpisodes': 37, 'stepsPerEpisode': 23, 'rewardPerEpisode': 15.631312821429352
'totalSteps': 6400, 'rewardStep': 0.48678999589033106, 'errorList': [], 'lossList': [0.0, -1.4007092106342316, 0.0, 85.3661642074585, 0.0, 0.0, 0.0], 'rewardMean': 0.6202220139009708, 'totalEpisodes': 51, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.7496992269345393
'totalSteps': 7680, 'rewardStep': 0.5529184947948842, 'errorList': [], 'lossList': [0.0, -1.3864012587070464, 0.0, 32.977111093997955, 0.0, 0.0, 0.0], 'rewardMean': 0.609004760716623, 'totalEpisodes': 54, 'stepsPerEpisode': 805, 'rewardPerEpisode': 578.1410600371288
'totalSteps': 8960, 'rewardStep': 0.7985695201876202, 'errorList': [], 'lossList': [0.0, -1.3775649571418762, 0.0, 154.43623958587648, 0.0, 0.0, 0.0], 'rewardMean': 0.6360854406410511, 'totalEpisodes': 81, 'stepsPerEpisode': 116, 'rewardPerEpisode': 90.77744985475685
'totalSteps': 10240, 'rewardStep': 0.5330778956289361, 'errorList': [], 'lossList': [0.0, -1.3778106862306594, 0.0, 44.83193308830261, 0.0, 0.0, 0.0], 'rewardMean': 0.6232094975145368, 'totalEpisodes': 94, 'stepsPerEpisode': 55, 'rewardPerEpisode': 42.077141472338596
'totalSteps': 11520, 'rewardStep': 0.6727041698335215, 'errorList': [], 'lossList': [0.0, -1.3744680947065353, 0.0, 25.65764633178711, 0.0, 0.0, 0.0], 'rewardMean': 0.6287089055499796, 'totalEpisodes': 105, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.595201715610797
'totalSteps': 12800, 'rewardStep': 0.7586956816342545, 'errorList': [], 'lossList': [0.0, -1.353449546098709, 0.0, 35.45454431772232, 0.0, 0.0, 0.0], 'rewardMean': 0.641707583158407, 'totalEpisodes': 114, 'stepsPerEpisode': 128, 'rewardPerEpisode': 105.35494491114478
'totalSteps': 14080, 'rewardStep': 0.8900875507941645, 'errorList': [], 'lossList': [0.0, -1.3429714721441268, 0.0, 8.39738471865654, 0.0, 0.0, 0.0], 'rewardMean': 0.6790369480678782, 'totalEpisodes': 119, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.349316427363405
'totalSteps': 15360, 'rewardStep': 0.42386118341379536, 'errorList': [], 'lossList': [0.0, -1.3470920062065124, 0.0, 8.024147737622261, 0.0, 0.0, 0.0], 'rewardMean': 0.6693448731196858, 'totalEpisodes': 121, 'stepsPerEpisode': 388, 'rewardPerEpisode': 285.2882701531036
'totalSteps': 16640, 'rewardStep': 0.7430805035133556, 'errorList': [], 'lossList': [0.0, -1.334889521598816, 0.0, 20.391028735637665, 0.0, 0.0, 0.0], 'rewardMean': 0.66149795976051, 'totalEpisodes': 123, 'stepsPerEpisode': 508, 'rewardPerEpisode': 403.97318606967536
'totalSteps': 17920, 'rewardStep': 0.6966017176783594, 'errorList': [], 'lossList': [0.0, -1.2968670010566712, 0.0, 4.272101259827614, 0.0, 0.0, 0.0], 'rewardMean': 0.6556386713369223, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.1618884829434
'totalSteps': 19200, 'rewardStep': 0.8582605843481645, 'errorList': [], 'lossList': [0.0, -1.2517256450653076, 0.0, 3.0416535344719886, 0.0, 0.0, 0.0], 'rewardMean': 0.6927857301827057, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.5881029182165
'totalSteps': 20480, 'rewardStep': 0.835485147640751, 'errorList': [], 'lossList': [0.0, -1.204762535095215, 0.0, 2.3213488548249006, 0.0, 0.0, 0.0], 'rewardMean': 0.7210423954672924, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.2522186250883
'totalSteps': 21760, 'rewardStep': 0.8060783417002926, 'errorList': [], 'lossList': [0.0, -1.1408430635929108, 0.0, 1.577168688699603, 0.0, 0.0, 0.0], 'rewardMean': 0.7217932776185595, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.0754683953523
'totalSteps': 23040, 'rewardStep': 0.9669478070252018, 'errorList': [0.03534716601402137, 0.05691120897406623, 0.027512725324281836, 0.05007137789594982, 0.02739190837031377, 0.03215179388526403, 0.04667558139782055, 0.03629870400973247, 0.04893799125071702, 0.03397336451330491, 0.03115365448708182, 0.03373539122830857, 0.07606380201510514, 0.034007197163852095, 0.049038391520386915, 0.06308354408927527, 0.05159038833381238, 0.058808652696805276, 0.02942463668283614, 0.08663055573232786, 0.04239849555788966, 0.027035420311931416, 0.032490644697831784, 0.04689235908048193, 0.07704256163240371, 0.033069066523300814, 0.0668986044408396, 0.0556065887732753, 0.029681223843876666, 0.0354743090083704, 0.0382728773378465, 0.0710856472240085, 0.049884215848258355, 0.08172608886768865, 0.029783746375434277, 0.045635465178231416, 0.04587862155686247, 0.08029930281003955, 0.04474960289453593, 0.03220950844123222, 0.03946479796120905, 0.03631860569759798, 0.04516926738215107, 0.037597986067448234, 0.05068582282476836, 0.03512353610633272, 0.033244127705392645, 0.07592751815483946, 0.06127821387636911, 0.03324827265568279], 'lossList': [0.0, -1.1070673143863679, 0.0, 1.2154727343842386, 0.0, 0.0, 0.0], 'rewardMean': 0.7651802687581861, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.6081829981026, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=77.85
