#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 87
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.560539087202914, 'errorList': [], 'lossList': [0.0, -1.44181725025177, 0.0, 29.424953079223634, 0.0, 0.0, 0.0], 'rewardMean': 0.49928483970439397, 'totalEpisodes': 22, 'stepsPerEpisode': 136, 'rewardPerEpisode': 91.16482987728624
'totalSteps': 3840, 'rewardStep': 0.7231840509244988, 'errorList': [], 'lossList': [0.0, -1.4383346593379975, 0.0, 45.410275506973264, 0.0, 0.0, 0.0], 'rewardMean': 0.5739179101110956, 'totalEpisodes': 39, 'stepsPerEpisode': 32, 'rewardPerEpisode': 23.026944268682204
'totalSteps': 5120, 'rewardStep': 0.5654917784522087, 'errorList': [], 'lossList': [0.0, -1.4213207751512527, 0.0, 56.05125936508179, 0.0, 0.0, 0.0], 'rewardMean': 0.5718113771963739, 'totalEpisodes': 56, 'stepsPerEpisode': 49, 'rewardPerEpisode': 36.87272320724968
'totalSteps': 6400, 'rewardStep': 0.5749335311551332, 'errorList': [], 'lossList': [0.0, -1.4051789766550065, 0.0, 47.54431957244873, 0.0, 0.0, 0.0], 'rewardMean': 0.5724358079881258, 'totalEpisodes': 66, 'stepsPerEpisode': 21, 'rewardPerEpisode': 12.86476784682082
'totalSteps': 7680, 'rewardStep': 0.8864178414109382, 'errorList': [], 'lossList': [0.0, -1.3808141285181046, 0.0, 78.81636312484741, 0.0, 0.0, 0.0], 'rewardMean': 0.6247661468919278, 'totalEpisodes': 82, 'stepsPerEpisode': 47, 'rewardPerEpisode': 35.65105329536147
'totalSteps': 8960, 'rewardStep': 0.803494660883112, 'errorList': [], 'lossList': [0.0, -1.369306302666664, 0.0, 77.37633165359497, 0.0, 0.0, 0.0], 'rewardMean': 0.6502987917478114, 'totalEpisodes': 100, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.283307502942776
'totalSteps': 10240, 'rewardStep': 0.449431304314251, 'errorList': [], 'lossList': [0.0, -1.3695759451389313, 0.0, 45.607909288406375, 0.0, 0.0, 0.0], 'rewardMean': 0.6251903558186162, 'totalEpisodes': 118, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.56859659990468
'totalSteps': 11520, 'rewardStep': 0.4953948159978762, 'errorList': [], 'lossList': [0.0, -1.3775611215829848, 0.0, 39.7719442653656, 0.0, 0.0, 0.0], 'rewardMean': 0.6107686291718672, 'totalEpisodes': 131, 'stepsPerEpisode': 68, 'rewardPerEpisode': 48.52372337593964
'totalSteps': 12800, 'rewardStep': 0.7975899061721792, 'errorList': [], 'lossList': [0.0, -1.3706080490350723, 0.0, 35.67165347099304, 0.0, 0.0, 0.0], 'rewardMean': 0.6294507568718986, 'totalEpisodes': 140, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.359867810505515
'totalSteps': 14080, 'rewardStep': 0.7921193325318647, 'errorList': [], 'lossList': [0.0, -1.3671654123067856, 0.0, 11.241842283010483, 0.0, 0.0, 0.0], 'rewardMean': 0.6648596309044976, 'totalEpisodes': 145, 'stepsPerEpisode': 154, 'rewardPerEpisode': 129.77385990540859
'totalSteps': 15360, 'rewardStep': 0.9636610837106669, 'errorList': [2.707629524256331, 0.340170338513675, 8.505426856652758, 1.9586248458897486, 0.46774838400551655, 5.069443906222053, 18.412998104072575, 6.200481566718347, 2.3348412542769506, 20.134982997400257, 19.058984767205004, 7.213488158984441, 2.2805085140207138, 9.433763651728349, 4.868232701816362, 10.2244293683943, 1.3603339610521246, 17.583413163265195, 5.717113864850344, 3.4818214202049664, 0.2149418030964028, 0.7443556003690345, 1.1899750423396507, 10.799864456139343, 0.4396153656617803, 3.9265502993475123, 25.699227484885466, 0.46805316528789387, 8.748988198513658, 35.71713516112077, 7.463507582238574, 6.919554390272858, 0.17594762284950075, 24.247462909217944, 8.037264699178063, 1.482015249095936, 3.7479050937571428, 10.51218258234513, 3.9550316700067714, 15.997898296470497, 6.823378336785443, 0.42553242831501437, 5.451324610064816, 9.724844602168506, 0.5830114274590351, 8.277313885771681, 2.986838295244353, 7.392146256516825, 4.695578157437587, 3.0898673751087458], 'lossList': [0.0, -1.3619800382852554, 0.0, 22.441599729061128, 0.0, 0.0, 0.0], 'rewardMean': 0.7051718305552729, 'totalEpisodes': 152, 'stepsPerEpisode': 96, 'rewardPerEpisode': 86.81566520605095, 'successfulTests': 1
'totalSteps': 16640, 'rewardStep': 0.9453898632743948, 'errorList': [1.747343165113988, 0.4072453145952005, 0.4910354362712259, 0.5834121208343053, 0.4521889569561732, 1.4007308585620477, 0.2086238852421969, 0.5605948581096443, 0.11493548182321896, 0.08232551657419655, 0.2765508869806907, 0.9262258806061501, 0.9633666384063381, 1.8609629170810889, 0.7321293166242625, 2.2244576847500097, 0.7381729347416915, 0.44260799575564297, 0.26916623825927427, 0.40901926647424425, 1.2283046687714563, 1.0010083805074914, 0.27128192582609095, 0.6865699952870232, 0.029346208772793557, 0.08307818631337044, 0.39620753299282635, 0.03369534614420413, 0.9430184669081408, 0.601369776812755, 1.7793096073955037, 1.0111883579555343, 0.01524164310854031, 0.7001828683434974, 0.09546756368650339, 0.8620291145983716, 1.1367952516032793, 0.08688879061404818, 1.1799962802576351, 1.2599460068704165, 0.7156142068247553, 0.9323794573023544, 1.67527244985763, 0.23286891732883, 0.39795643804546255, 1.5953203624446954, 1.4218959988905142, 0.0052931022424561125, 0.16070507914369864, 0.3647183958374113], 'lossList': [0.0, -1.3323262244462968, 0.0, 7.357400724887848, 0.0, 0.0, 0.0], 'rewardMean': 0.7273924117902626, 'totalEpisodes': 156, 'stepsPerEpisode': 95, 'rewardPerEpisode': 85.69554490354798, 'successfulTests': 10
'totalSteps': 17920, 'rewardStep': 0.721328946195855, 'errorList': [], 'lossList': [0.0, -1.299492593407631, 0.0, 5.458678522109985, 0.0, 0.0, 0.0], 'rewardMean': 0.7429761285646271, 'totalEpisodes': 158, 'stepsPerEpisode': 127, 'rewardPerEpisode': 101.90936486843134
'totalSteps': 19200, 'rewardStep': 0.7473778820573794, 'errorList': [], 'lossList': [0.0, -1.2720167231559754, 0.0, 4.060073476135731, 0.0, 0.0, 0.0], 'rewardMean': 0.7602205636548517, 'totalEpisodes': 160, 'stepsPerEpisode': 209, 'rewardPerEpisode': 174.35630140451056
'totalSteps': 20480, 'rewardStep': 0.48258211652196764, 'errorList': [], 'lossList': [0.0, -1.2568633967638017, 0.0, 3.3818806010484694, 0.0, 0.0, 0.0], 'rewardMean': 0.7198369911659547, 'totalEpisodes': 164, 'stepsPerEpisode': 268, 'rewardPerEpisode': 189.95431382224888
'totalSteps': 21760, 'rewardStep': 0.44470847281665155, 'errorList': [], 'lossList': [0.0, -1.2426112192869185, 0.0, 2.3120492577552794, 0.0, 0.0, 0.0], 'rewardMean': 0.6839583723593087, 'totalEpisodes': 165, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 973.8857086108964
'totalSteps': 23040, 'rewardStep': 0.969241879517248, 'errorList': [0.3995400701332186, 0.3671724417613636, 0.3561719048706816, 0.4159962798203025, 0.36596725525799473, 0.37134226478959287, 0.3724687832414812, 0.35745720505748424, 0.35214897459531136, 0.35301386725063955, 0.4082896621803372, 0.3632577507016067, 0.36631738481351545, 0.3569679421836163, 0.3884242776645413, 0.41317530887140047, 0.38571395483987236, 0.3438763078896718, 0.39073467340342516, 0.3812201328288373, 0.39300375994629533, 0.32929855397313884, 0.37162053087308866, 0.3643747363316944, 0.4077593631022442, 0.3630013188039994, 0.359910712346584, 0.36443554351079227, 0.34669404232719697, 0.3467226044202071, 0.36435042038677257, 0.35321310165412334, 0.34365682936565295, 0.42366948530852155, 0.349462007991329, 0.3566150585770694, 0.3793127503181, 0.36810106052623226, 0.34052159972723456, 0.33081955467343416, 0.39121077965202733, 0.3151361516848737, 0.3489675154817345, 0.40107839181048766, 0.3820073420238018, 0.3353129926851828, 0.3634151911652314, 0.3393598306089813, 0.41186415923401754, 0.39033977570078204], 'lossList': [0.0, -1.229247755408287, 0.0, 1.7470525546371938, 0.0, 0.0, 0.0], 'rewardMean': 0.7359394298796084, 'totalEpisodes': 166, 'stepsPerEpisode': 1271, 'rewardPerEpisode': 1161.657822941155, 'successfulTests': 0
'totalSteps': 24320, 'rewardStep': 0.7968903005561923, 'errorList': [], 'lossList': [0.0, -1.2096208035945892, 0.0, 0.8464156424999237, 0.0, 0.0, 0.0], 'rewardMean': 0.7660889783354399, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.4029788535508
'totalSteps': 25600, 'rewardStep': 0.8411085241617996, 'errorList': [], 'lossList': [0.0, -1.2020487868785859, 0.0, 0.7412293893098831, 0.0, 0.0, 0.0], 'rewardMean': 0.7704408401344021, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.3556298986912
#maxSuccessfulTests=10, maxSuccessfulTestsAtStep=16640, timeSpent=127.15
