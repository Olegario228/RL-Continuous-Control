#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 88
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.5352985832260151, 'errorList': [], 'lossList': [0.0, -1.4167521464824677, 0.0, 23.303662810325623, 0.0, 0.0, 0.0], 'rewardMean': 0.6503122080865356, 'totalEpisodes': 19, 'stepsPerEpisode': 43, 'rewardPerEpisode': 32.64436285429389
'totalSteps': 3840, 'rewardStep': 0.9421391145984562, 'errorList': [], 'lossList': [0.0, -1.3990043491125106, 0.0, 50.25917410850525, 0.0, 0.0, 0.0], 'rewardMean': 0.7475878435905091, 'totalEpisodes': 37, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.316055578556373
'totalSteps': 5120, 'rewardStep': 0.895968816172562, 'errorList': [], 'lossList': [0.0, -1.4032571524381638, 0.0, 47.489776859283445, 0.0, 0.0, 0.0], 'rewardMean': 0.7846830867360223, 'totalEpisodes': 49, 'stepsPerEpisode': 74, 'rewardPerEpisode': 65.46646170016702
'totalSteps': 6400, 'rewardStep': 0.8742010967190985, 'errorList': [], 'lossList': [0.0, -1.4066503405570985, 0.0, 79.10870096206665, 0.0, 0.0, 0.0], 'rewardMean': 0.8025866887326375, 'totalEpisodes': 68, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.431348749126835
'totalSteps': 7680, 'rewardStep': 0.7765801073935908, 'errorList': [], 'lossList': [0.0, -1.390134992003441, 0.0, 81.72675449371337, 0.0, 0.0, 0.0], 'rewardMean': 0.798252258509463, 'totalEpisodes': 88, 'stepsPerEpisode': 81, 'rewardPerEpisode': 56.24569150793291
'totalSteps': 8960, 'rewardStep': 0.6095847990921349, 'errorList': [], 'lossList': [0.0, -1.36995971262455, 0.0, 88.35335376739502, 0.0, 0.0, 0.0], 'rewardMean': 0.7712997643069875, 'totalEpisodes': 114, 'stepsPerEpisode': 71, 'rewardPerEpisode': 52.776050884928786
'totalSteps': 10240, 'rewardStep': 0.7358739361304154, 'errorList': [], 'lossList': [0.0, -1.3525186449289321, 0.0, 50.67136576652527, 0.0, 0.0, 0.0], 'rewardMean': 0.7668715357849161, 'totalEpisodes': 125, 'stepsPerEpisode': 164, 'rewardPerEpisode': 123.97502120987023
'totalSteps': 11520, 'rewardStep': 0.2538342154185912, 'errorList': [], 'lossList': [0.0, -1.337691136598587, 0.0, 17.68531168937683, 0.0, 0.0, 0.0], 'rewardMean': 0.7098673890775467, 'totalEpisodes': 128, 'stepsPerEpisode': 374, 'rewardPerEpisode': 253.95669717788235
'totalSteps': 12800, 'rewardStep': 0.44903431895336937, 'errorList': [], 'lossList': [0.0, -1.3150500178337097, 0.0, 13.142985721826554, 0.0, 0.0, 0.0], 'rewardMean': 0.6837840820651289, 'totalEpisodes': 129, 'stepsPerEpisode': 817, 'rewardPerEpisode': 555.6208234229592
'totalSteps': 14080, 'rewardStep': 0.9022245338420927, 'errorList': [], 'lossList': [0.0, -1.2618468797206879, 0.0, 9.079949276149273, 0.0, 0.0, 0.0], 'rewardMean': 0.6974739521546327, 'totalEpisodes': 130, 'stepsPerEpisode': 1261, 'rewardPerEpisode': 907.9789345350409
'totalSteps': 15360, 'rewardStep': 0.6986133917752244, 'errorList': [], 'lossList': [0.0, -1.226589726805687, 0.0, 4.960852783024311, 0.0, 0.0, 0.0], 'rewardMean': 0.7138054330095536, 'totalEpisodes': 130, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 971.6356496060199
'totalSteps': 16640, 'rewardStep': 0.8369402859956784, 'errorList': [], 'lossList': [0.0, -1.212249023914337, 0.0, 5.707664848789573, 0.0, 0.0, 0.0], 'rewardMean': 0.7032855501492759, 'totalEpisodes': 130, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.8778017769848
'totalSteps': 17920, 'rewardStep': 0.922170886752809, 'errorList': [], 'lossList': [0.0, -1.1786858892440797, 0.0, 8.91939763367176, 0.0, 0.0, 0.0], 'rewardMean': 0.7059057572073004, 'totalEpisodes': 132, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.802883170405934
'totalSteps': 19200, 'rewardStep': 0.7451220958749182, 'errorList': [], 'lossList': [0.0, -1.1732703351974487, 0.0, 4.449882800579071, 0.0, 0.0, 0.0], 'rewardMean': 0.6929978571228825, 'totalEpisodes': 133, 'stepsPerEpisode': 189, 'rewardPerEpisode': 159.4431817465168
'totalSteps': 20480, 'rewardStep': 0.9428859426114687, 'errorList': [0.051760688638704345, 0.0544461346396758, 0.19042642195277226, 0.13967006781571126, 0.07644793929647585, 0.05020462356142047, 0.1401027777803861, 0.09665419221974135, 0.09539161074778965, 0.05384924911870575, 0.04207078064252392, 0.06888191995242024, 0.10381386266307546, 0.08686309676317149, 0.060519522733787696, 0.12736735751999845, 0.12398292523460652, 0.08078566469733527, 0.09158184453929932, 0.09047437899615632, 0.08971978751777704, 0.04497340586670003, 0.09018897852400488, 0.14433608448198348, 0.13329707174498817, 0.22038587732234732, 0.14386575079465613, 0.12654513699057982, 0.21680321289378615, 0.10956736051416537, 0.07784259613448556, 0.05886277780106841, 0.09607769951903042, 0.09748260216577911, 0.07751486615874657, 0.2054993469938278, 0.09985873304056558, 0.057688239266496494, 0.07204565134619711, 0.08542672173968244, 0.06915416252109352, 0.12935678931894512, 0.11034105575868651, 0.07472308381782937, 0.10898091574981776, 0.07634743306554108, 0.09579832762853359, 0.09231900338681456, 0.07330530462517315, 0.1553863353242625], 'lossList': [0.0, -1.1530569952726364, 0.0, 1.7876290632784366, 0.0, 0.0, 0.0], 'rewardMean': 0.7096284406446702, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1056.3430426547882, 'successfulTests': 47
'totalSteps': 21760, 'rewardStep': 0.9215038294748823, 'errorList': [], 'lossList': [0.0, -1.1077893549203872, 0.0, 1.0271553117781878, 0.0, 0.0, 0.0], 'rewardMean': 0.7408203436829449, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.8438444759963
'totalSteps': 23040, 'rewardStep': 0.829877487204527, 'errorList': [], 'lossList': [0.0, -1.072752069234848, 0.0, 1.0078177361004055, 0.0, 0.0, 0.0], 'rewardMean': 0.7502206987903561, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1177.2501744202614
'totalSteps': 24320, 'rewardStep': 0.919793956556978, 'errorList': [], 'lossList': [0.0, -1.0258896362781524, 0.0, 0.9612740977481008, 0.0, 0.0, 0.0], 'rewardMean': 0.8168166729041948, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.1363239980692
'totalSteps': 25600, 'rewardStep': 0.9602725274357674, 'errorList': [0.011735329089042726, 0.026788735216857786, 0.035310350940402524, 0.03660429663170865, 0.054550240297014704, 0.03973998093356435, 0.006387168322856115, 0.046883661603864896, 0.012346290860306393, 0.03196116181918634, 0.011798102273537541, 0.03648478775775456, 0.017480531308087386, 0.013076929064898551, 0.026601256162579655, 0.027186661640289947, 0.033634297572923584, 0.024404115609525674, 0.019764130162867995, 0.010519766030590766, 0.04054949230510558, 0.03691206013473199, 0.02751593052205509, 0.013997061247968389, 0.022376408839419954, 0.03337641030337574, 0.019367200781878143, 0.036276412194717486, 0.02203143761190121, 0.01875131526156662, 0.020235414437644313, 0.0036719494999989206, 0.009215623932394557, 0.026803990582634978, 0.017120887178322355, 0.00680841718896778, 0.017876084720731763, 0.021835744677816498, 0.03472868743680364, 0.0041478356653351, 0.05778881370038562, 0.013808614227059696, 0.03954433862829246, 0.012129549160539044, 0.02292752007650166, 0.032220500574707565, 0.014666266231199918, 0.022819288886126193, 0.03665293150738452, 0.03738520170884377], 'lossList': [0.0, -0.9889161044359207, 0.0, 0.7279791921190918, 0.0, 0.0, 0.0], 'rewardMean': 0.8679404937524346, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1216.349042378057, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=104.62
