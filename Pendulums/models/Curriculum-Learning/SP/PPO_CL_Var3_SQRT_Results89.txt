#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 89
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8701470191931291, 'errorList': [], 'lossList': [0.0, -1.4204826205968857, 0.0, 26.234544701576233, 0.0, 0.0, 0.0], 'rewardMean': 0.7710312126682362, 'totalEpisodes': 22, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.85909553367856
'totalSteps': 3840, 'rewardStep': 0.683963104954663, 'errorList': [], 'lossList': [0.0, -1.4188335007429123, 0.0, 36.30248338699341, 0.0, 0.0, 0.0], 'rewardMean': 0.7420085100970452, 'totalEpisodes': 41, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.557479530228015
'totalSteps': 5120, 'rewardStep': 0.8387675813855839, 'errorList': [], 'lossList': [0.0, -1.4079299724102021, 0.0, 37.06052281856537, 0.0, 0.0, 0.0], 'rewardMean': 0.7661982779191798, 'totalEpisodes': 51, 'stepsPerEpisode': 43, 'rewardPerEpisode': 38.48011996350042
'totalSteps': 6400, 'rewardStep': 0.7550954117485398, 'errorList': [], 'lossList': [0.0, -1.396817073225975, 0.0, 53.31639543533325, 0.0, 0.0, 0.0], 'rewardMean': 0.7639777046850519, 'totalEpisodes': 64, 'stepsPerEpisode': 33, 'rewardPerEpisode': 25.944578242932554
'totalSteps': 7680, 'rewardStep': 0.6564073829185948, 'errorList': [], 'lossList': [0.0, -1.3904786831140519, 0.0, 71.49404922485351, 0.0, 0.0, 0.0], 'rewardMean': 0.7460493177239756, 'totalEpisodes': 74, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.699131468644847
'totalSteps': 8960, 'rewardStep': 0.8454275588236081, 'errorList': [], 'lossList': [0.0, -1.3866262382268906, 0.0, 172.81361976623535, 0.0, 0.0, 0.0], 'rewardMean': 0.7602462093096374, 'totalEpisodes': 106, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.155910387667195
'totalSteps': 10240, 'rewardStep': 0.9830013654029547, 'errorList': [170.47931358599027, 84.41403730696875, 211.02028200908555, 249.92791077599603, 139.057306108587, 22.05871374798726, 210.0938490358331, 144.19965630904605, 173.87089898462742, 184.3770728635184, 109.36488815328947, 140.00193957608144, 143.97773029509534, 253.14016273907333, 214.74158172447238, 169.3825494862395, 249.49666607937175, 163.70668139549954, 117.2119134492205, 231.8901998004383, 140.79600477644865, 155.79446667409687, 229.13724059681022, 223.2697992638257, 243.28848292512288, 137.33788364850156, 211.09447671465537, 152.48037632851077, 236.20438353065592, 201.21641216150914, 183.77260120913158, 217.0973894898051, 197.13630123946672, 50.85624045241718, 266.6525762832554, 165.41341911507442, 247.53946906643867, 143.61663288645045, 224.764931501546, 255.75756301517188, 153.19881499523083, 217.5072394331387, 232.04651442538983, 252.56350294415196, 196.58829173906042, 127.330546176056, 163.2703280129231, 254.41861579576704, 83.62448185320952, 129.41480730896927], 'lossList': [0.0, -1.3773587709665298, 0.0, 79.2116481590271, 0.0, 0.0, 0.0], 'rewardMean': 0.788090603821302, 'totalEpisodes': 127, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.289706092626284, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.325224941333595, 'errorList': [], 'lossList': [0.0, -1.3606370437145232, 0.0, 31.85428417682648, 0.0, 0.0, 0.0], 'rewardMean': 0.7366610857671123, 'totalEpisodes': 142, 'stepsPerEpisode': 78, 'rewardPerEpisode': 49.641881265856725
'totalSteps': 12800, 'rewardStep': 0.7455428550341755, 'errorList': [], 'lossList': [0.0, -1.3332690161466598, 0.0, 27.943364129066467, 0.0, 0.0, 0.0], 'rewardMean': 0.7375492626938186, 'totalEpisodes': 152, 'stepsPerEpisode': 57, 'rewardPerEpisode': 40.77673058583473
'totalSteps': 14080, 'rewardStep': 0.5447206185880785, 'errorList': [], 'lossList': [0.0, -1.3249403059482574, 0.0, 10.781266431808472, 0.0, 0.0, 0.0], 'rewardMean': 0.7248297839382922, 'totalEpisodes': 157, 'stepsPerEpisode': 92, 'rewardPerEpisode': 67.80582567141477
'totalSteps': 15360, 'rewardStep': 0.8157480568396541, 'errorList': [], 'lossList': [0.0, -1.3269242960214616, 0.0, 9.335913152694703, 0.0, 0.0, 0.0], 'rewardMean': 0.7193898877029448, 'totalEpisodes': 166, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.207668629612723
'totalSteps': 16640, 'rewardStep': 0.6173619772774592, 'errorList': [], 'lossList': [0.0, -1.3369470143318176, 0.0, 5.569948111772537, 0.0, 0.0, 0.0], 'rewardMean': 0.7127297749352245, 'totalEpisodes': 171, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.080598365582077
'totalSteps': 17920, 'rewardStep': 0.5960106799556856, 'errorList': [], 'lossList': [0.0, -1.335511969923973, 0.0, 6.700015759468078, 0.0, 0.0, 0.0], 'rewardMean': 0.6884540847922346, 'totalEpisodes': 177, 'stepsPerEpisode': 116, 'rewardPerEpisode': 96.9210443108665
'totalSteps': 19200, 'rewardStep': 0.935841175549424, 'errorList': [1.0032363346692816, 0.19649649333386013, 0.08521563241553197, 1.1645511438799088, 3.011374154278493, 1.3827918215675317, 0.10466121880025732, 2.0520240435262664, 1.174120472767026, 0.5743320781762262, 0.6593375098139397, 0.474388846928395, 0.12591444978279506, 1.7947232243286562, 15.476764528332744, 0.7189044369296647, 0.6895749650936617, 1.2079834588581946, 0.2694046753139141, 0.6507691415883914, 0.221981917870316, 3.692676385340851, 2.0298345570780887, 0.35104870630430945, 6.465796972859879, 0.0689698426709158, 1.4539818374678812, 0.5592024640031132, 0.29990860004040365, 6.140127471630966, 0.613934973473044, 0.9954520534691583, 0.5756093249942186, 2.9760597767787513, 0.8601166644745851, 0.4210163851876551, 1.5062891671777952, 2.1635393729260435, 2.726341526508704, 29.429553630359663, 0.4156243738495344, 0.0667439404534909, 4.3523072238044875, 25.063485291149906, 3.4746977283813734, 0.19045916130321264, 0.3000433876533618, 34.82187861465376, 0.0973842863288176, 1.279561627775305], 'lossList': [0.0, -1.3320234876871109, 0.0, 4.554620434045791, 0.0, 0.0, 0.0], 'rewardMean': 0.7065286611723229, 'totalEpisodes': 182, 'stepsPerEpisode': 44, 'rewardPerEpisode': 40.78134734042972, 'successfulTests': 8
'totalSteps': 20480, 'rewardStep': 0.7728222675357164, 'errorList': [], 'lossList': [0.0, -1.3147169345617293, 0.0, 5.38107275724411, 0.0, 0.0, 0.0], 'rewardMean': 0.718170149634035, 'totalEpisodes': 185, 'stepsPerEpisode': 369, 'rewardPerEpisode': 289.66620989356346
'totalSteps': 21760, 'rewardStep': 0.6559999864679873, 'errorList': [], 'lossList': [0.0, -1.2925932323932647, 0.0, 5.5836860710382465, 0.0, 0.0, 0.0], 'rewardMean': 0.699227392398473, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 975.1822257211187
'totalSteps': 23040, 'rewardStep': 0.8419444980293238, 'errorList': [], 'lossList': [0.0, -1.2548819160461426, 0.0, 2.146098800599575, 0.0, 0.0, 0.0], 'rewardMean': 0.68512170566111, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.4774999452636
'totalSteps': 24320, 'rewardStep': 0.8407269256207386, 'errorList': [], 'lossList': [0.0, -1.2207890141010285, 0.0, 1.2070787778869272, 0.0, 0.0, 0.0], 'rewardMean': 0.7366719040898244, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1123.4511204719213
'totalSteps': 25600, 'rewardStep': 0.9679360069188275, 'errorList': [0.09551963441760293, 0.09017908386438853, 0.08859575302441158, 0.09728495502407072, 0.10791454209806753, 0.08910280908381504, 0.09317843816272878, 0.08754650506706592, 0.08408450020036272, 0.0865225492793764, 0.08565509762769559, 0.08582617189347785, 0.11216862876433324, 0.12874741298388714, 0.09047676541866055, 0.08728136869954022, 0.08691710064332198, 0.09200125747302788, 0.11547612553544566, 0.08686228972226008, 0.0915355739448091, 0.0996072234316803, 0.09188872971495297, 0.0853060698392315, 0.09574362706090872, 0.0880333335017132, 0.08744998930401056, 0.08754118362269345, 0.09724257987335751, 0.1036079618800987, 0.08482272804889551, 0.08638653566918562, 0.08795780366825565, 0.1154934994273779, 0.08927726901981607, 0.09255889380737864, 0.09465170454842224, 0.08694510921941974, 0.08712275902785309, 0.11709473039176169, 0.08751223414532522, 0.09152954364022495, 0.0857605222863813, 0.08836372717752898, 0.08491283450967303, 0.0877056988245083, 0.1225718209707683, 0.08589824986674491, 0.08806586824239801, 0.09397868712882833], 'lossList': [0.0, -1.195062628388405, 0.0, 1.60639361910522, 0.0, 0.0, 0.0], 'rewardMean': 0.7589112192782894, 'totalEpisodes': 185, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.107399736194, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=127.07
