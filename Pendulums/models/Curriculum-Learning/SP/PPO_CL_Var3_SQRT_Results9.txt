#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 9
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.952529088633024, 'errorList': [], 'lossList': [0.0, -1.408070645928383, 0.0, 33.35446970939636, 0.0, 0.0, 0.0], 'rewardMean': 0.6582995972043063, 'totalEpisodes': 40, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.50004648163206
'totalSteps': 3840, 'rewardStep': 0.9040844719338504, 'errorList': [], 'lossList': [0.0, -1.4020617932081223, 0.0, 43.46443332672119, 0.0, 0.0, 0.0], 'rewardMean': 0.740227888780821, 'totalEpisodes': 67, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.21559837791194
'totalSteps': 5120, 'rewardStep': 0.7458656103157807, 'errorList': [], 'lossList': [0.0, -1.37885347366333, 0.0, 67.88120292663574, 0.0, 0.0, 0.0], 'rewardMean': 0.741637319164561, 'totalEpisodes': 95, 'stepsPerEpisode': 42, 'rewardPerEpisode': 33.273285728156615
'totalSteps': 6400, 'rewardStep': 0.6252201542123018, 'errorList': [], 'lossList': [0.0, -1.3615438771247863, 0.0, 79.57842605590821, 0.0, 0.0, 0.0], 'rewardMean': 0.7183538861741091, 'totalEpisodes': 129, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.7936418388281328
'totalSteps': 7680, 'rewardStep': 0.537207476728165, 'errorList': [], 'lossList': [0.0, -1.3530777168273926, 0.0, 51.506584873199465, 0.0, 0.0, 0.0], 'rewardMean': 0.6881628179331184, 'totalEpisodes': 146, 'stepsPerEpisode': 35, 'rewardPerEpisode': 25.669502695555664
'totalSteps': 8960, 'rewardStep': 0.6694395190627698, 'errorList': [], 'lossList': [0.0, -1.3432580506801606, 0.0, 65.60003704071045, 0.0, 0.0, 0.0], 'rewardMean': 0.6854880609516402, 'totalEpisodes': 163, 'stepsPerEpisode': 12, 'rewardPerEpisode': 7.7526252866007255
'totalSteps': 10240, 'rewardStep': 0.7690922715041715, 'errorList': [], 'lossList': [0.0, -1.3261816412210465, 0.0, 17.192692365646362, 0.0, 0.0, 0.0], 'rewardMean': 0.6959385872707065, 'totalEpisodes': 169, 'stepsPerEpisode': 122, 'rewardPerEpisode': 90.92019215996119
'totalSteps': 11520, 'rewardStep': 0.5542949082340272, 'errorList': [], 'lossList': [0.0, -1.2997726136446, 0.0, 11.169300611019134, 0.0, 0.0, 0.0], 'rewardMean': 0.6802004007110755, 'totalEpisodes': 170, 'stepsPerEpisode': 643, 'rewardPerEpisode': 441.55342694977685
'totalSteps': 12800, 'rewardStep': 0.8996212907657506, 'errorList': [], 'lossList': [0.0, -1.2830031543970108, 0.0, 44.33497716903687, 0.0, 0.0, 0.0], 'rewardMean': 0.7021424897165429, 'totalEpisodes': 175, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.08493195264055
'totalSteps': 14080, 'rewardStep': 0.9413052085970579, 'errorList': [0.34691958899968656, 0.3508198995692537, 0.3521698714459708, 0.4289842478958394, 0.31722873865063145, 0.44260925886658464, 0.43135008699661814, 0.3821846118770818, 0.45305971210594886, 0.3206098656086587, 0.3538252678849153, 0.4039958194626186, 0.4513380716847643, 0.5092489827969087, 0.3758566101393637, 0.37333791451726817, 0.4915797300316525, 0.3903227282322784, 0.5470366807202921, 0.4691990218919261, 0.3072173784668434, 0.41682781740708774, 0.4122152905179089, 0.3739414237466371, 0.5458562494657946, 0.47042092920032597, 0.3721780131828062, 0.38186404094828996, 0.43940920621717244, 0.5074774056769441, 0.41121316132678476, 0.49832896146793654, 0.39538551071949163, 0.38426528645434227, 0.39553054112608765, 0.30614895775437967, 0.5063306329095676, 0.36566917211867955, 0.5842654643465488, 0.42295607744051994, 0.5889610285935143, 0.5823614029558414, 0.3814018381917788, 0.3749027272776179, 0.37376099842357025, 0.3816911727264427, 0.3104897875936614, 0.3977111710125344, 0.28773530101268585, 0.34639453913505247], 'lossList': [0.0, -1.2733803474903107, 0.0, 9.086598343253137, 0.0, 0.0, 0.0], 'rewardMean': 0.7598659999986899, 'totalEpisodes': 176, 'stepsPerEpisode': 792, 'rewardPerEpisode': 596.5538075973174, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.6414720762846904, 'errorList': [], 'lossList': [0.0, -1.2761004650592804, 0.0, 5.163104257881641, 0.0, 0.0, 0.0], 'rewardMean': 0.7287602987638564, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 886.7910660510329
'totalSteps': 16640, 'rewardStep': 0.6197420210156459, 'errorList': [], 'lossList': [0.0, -1.2606540948152543, 0.0, 4.578129767775535, 0.0, 0.0, 0.0], 'rewardMean': 0.700326053672036, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 994.7538551803395
'totalSteps': 17920, 'rewardStep': 0.8572600815974273, 'errorList': [], 'lossList': [0.0, -1.2398300880193711, 0.0, 3.1245915335416794, 0.0, 0.0, 0.0], 'rewardMean': 0.7114655008002008, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.6909148335271
'totalSteps': 19200, 'rewardStep': 0.9171890106269205, 'errorList': [], 'lossList': [0.0, -1.2246012270450592, 0.0, 1.8019847679138183, 0.0, 0.0, 0.0], 'rewardMean': 0.7406623864416626, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.324024825704
'totalSteps': 20480, 'rewardStep': 0.9640707759296209, 'errorList': [0.13962308768666676, 0.12240283363857711, 0.14440320774620535, 0.1907977060141694, 0.2635967960002081, 0.1328586379863019, 0.13458164791563834, 0.12879348436581167, 0.1308126894469974, 0.13000006941028808, 0.13651197394519501, 0.2735800020719525, 0.13512366546624607, 0.1266628687594066, 0.13217828461803524, 0.12040335013604636, 0.1314480763271418, 0.16991624800535352, 0.14124614692829124, 0.13175068435150378, 0.12274245035812813, 0.13988576496458374, 0.12258939331957468, 0.14715912580094132, 0.13545055967600758, 0.11938879364121481, 0.1327504374963369, 0.12075293583164834, 0.14034256865623565, 0.13151506784798092, 0.1199251956458012, 0.14279999118584452, 0.21785228964715975, 0.19691249753117085, 0.2133156775059579, 0.1255055572090426, 0.14390413179545908, 0.2698314983370652, 0.13238065759594222, 0.12305105363612136, 0.15311393208208407, 0.13672314830531845, 0.1385491592731967, 0.1196420256382391, 0.133350576405399, 0.1404879697457842, 0.13403586894802144, 0.12798268237823723, 0.13229827838061822, 0.12775869332200387], 'lossList': [0.0, -1.2085156345367432, 0.0, 2.2765953316539527, 0.0, 0.0, 0.0], 'rewardMean': 0.7833487163618081, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.20280662001, 'successfulTests': 45
'totalSteps': 21760, 'rewardStep': 0.8745868970655513, 'errorList': [], 'lossList': [0.0, -1.2062047415971755, 0.0, 1.1216639341786503, 0.0, 0.0, 0.0], 'rewardMean': 0.8038634541620864, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.4983994318654
'totalSteps': 23040, 'rewardStep': 0.8895325378440938, 'errorList': [], 'lossList': [0.0, -1.2025891375541686, 0.0, 0.7201027106121183, 0.0, 0.0, 0.0], 'rewardMean': 0.8159074807960787, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.9786899229357
'totalSteps': 24320, 'rewardStep': 0.9216632629961815, 'errorList': [], 'lossList': [0.0, -1.1763151568174361, 0.0, 0.6145205061137676, 0.0, 0.0, 0.0], 'rewardMean': 0.852644316272294, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.3642591604303
'totalSteps': 25600, 'rewardStep': 0.9862670027009155, 'errorList': [0.09979700242757145, 0.14308923149934671, 0.09803243537078057, 0.09335404612979399, 0.08262830868509596, 0.12950697993678717, 0.0843073546941531, 0.08072742879948312, 0.13090442998071605, 0.09361124946934142, 0.0843742218933235, 0.07799668103104802, 0.1489768937169637, 0.13742232770372464, 0.12310181840132203, 0.10938213895886223, 0.09950144095250918, 0.12306757781830478, 0.18266631054111235, 0.09753836094741948, 0.20004378931112968, 0.0840428123618378, 0.08072530817360454, 0.09099736797782029, 0.09283495603972325, 0.08414843326144404, 0.14533654091381729, 0.11881831664082408, 0.08592557714699643, 0.09909094429151558, 0.08167333165885851, 0.11010958933677951, 0.08566834888774871, 0.08745888017725528, 0.0971448018123089, 0.2068325111683611, 0.0803976864795415, 0.08083290476066245, 0.08333810622508453, 0.08512531954695123, 0.15142837784475058, 0.08872216398317664, 0.09107336252227778, 0.08736020427564876, 0.08850383717947341, 0.07397888098606158, 0.0732365569387097, 0.08049378379311528, 0.09073559528138653, 0.09307419101342211], 'lossList': [0.0, -1.1484261655807495, 0.0, 0.6452591251209379, 0.0, 0.0, 0.0], 'rewardMean': 0.8613088874658106, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1211.9734444660085, 'successfulTests': 48
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=25600, timeSpent=116.96
