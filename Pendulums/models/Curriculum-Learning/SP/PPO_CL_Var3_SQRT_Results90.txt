#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 90
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.7673054975317173, 'errorList': [], 'lossList': [0.0, -1.4415857195854187, 0.0, 25.351859731674196, 0.0, 0.0, 0.0], 'rewardMean': 0.8314435443699422, 'totalEpisodes': 11, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.956902628671843
'totalSteps': 3840, 'rewardStep': 0.6809960582059985, 'errorList': [], 'lossList': [0.0, -1.4353615409135818, 0.0, 40.731958541870114, 0.0, 0.0, 0.0], 'rewardMean': 0.7812943823152944, 'totalEpisodes': 24, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.361305511675113
'totalSteps': 5120, 'rewardStep': 0.7716533297682017, 'errorList': [], 'lossList': [0.0, -1.4391990470886231, 0.0, 54.15734040737152, 0.0, 0.0, 0.0], 'rewardMean': 0.7788841191785212, 'totalEpisodes': 34, 'stepsPerEpisode': 192, 'rewardPerEpisode': 150.84143657263124
'totalSteps': 6400, 'rewardStep': 0.6841458494900818, 'errorList': [], 'lossList': [0.0, -1.4421698051691054, 0.0, 32.944702889919284, 0.0, 0.0, 0.0], 'rewardMean': 0.7599364652408334, 'totalEpisodes': 40, 'stepsPerEpisode': 295, 'rewardPerEpisode': 208.76076747927712
'totalSteps': 7680, 'rewardStep': 0.5651421682521021, 'errorList': [], 'lossList': [0.0, -1.412581633925438, 0.0, 89.47452981948852, 0.0, 0.0, 0.0], 'rewardMean': 0.7274707490760447, 'totalEpisodes': 50, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.041650887422679
'totalSteps': 8960, 'rewardStep': 0.8952103132035526, 'errorList': [], 'lossList': [0.0, -1.3940411311388017, 0.0, 179.01822780609132, 0.0, 0.0, 0.0], 'rewardMean': 0.7514335439514029, 'totalEpisodes': 80, 'stepsPerEpisode': 23, 'rewardPerEpisode': 18.733134140698436
'totalSteps': 10240, 'rewardStep': 0.7942467335886636, 'errorList': [], 'lossList': [0.0, -1.3805946236848832, 0.0, 120.02804649353027, 0.0, 0.0, 0.0], 'rewardMean': 0.7567851926560606, 'totalEpisodes': 111, 'stepsPerEpisode': 18, 'rewardPerEpisode': 12.61636199713817
'totalSteps': 11520, 'rewardStep': 0.8421559252189053, 'errorList': [], 'lossList': [0.0, -1.369525475502014, 0.0, 77.33232200622558, 0.0, 0.0, 0.0], 'rewardMean': 0.7662708296074877, 'totalEpisodes': 127, 'stepsPerEpisode': 65, 'rewardPerEpisode': 47.075178483817396
'totalSteps': 12800, 'rewardStep': 0.46294708652663596, 'errorList': [], 'lossList': [0.0, -1.3550647395849227, 0.0, 58.74804481506348, 0.0, 0.0, 0.0], 'rewardMean': 0.7359384552994026, 'totalEpisodes': 138, 'stepsPerEpisode': 142, 'rewardPerEpisode': 97.81349579220579
'totalSteps': 14080, 'rewardStep': 0.5618615291800024, 'errorList': [], 'lossList': [0.0, -1.3409933310747146, 0.0, 22.314748927354813, 0.0, 0.0, 0.0], 'rewardMean': 0.7025664490965862, 'totalEpisodes': 141, 'stepsPerEpisode': 58, 'rewardPerEpisode': 47.560349044429174
'totalSteps': 15360, 'rewardStep': 0.5299321534722845, 'errorList': [], 'lossList': [0.0, -1.326458375453949, 0.0, 18.87108584046364, 0.0, 0.0, 0.0], 'rewardMean': 0.6788291146906429, 'totalEpisodes': 143, 'stepsPerEpisode': 264, 'rewardPerEpisode': 221.7566548910969
'totalSteps': 16640, 'rewardStep': 0.8799485882158705, 'errorList': [], 'lossList': [0.0, -1.3112347251176835, 0.0, 13.774183322191238, 0.0, 0.0, 0.0], 'rewardMean': 0.69872436769163, 'totalEpisodes': 147, 'stepsPerEpisode': 178, 'rewardPerEpisode': 156.75385369713814
'totalSteps': 17920, 'rewardStep': 0.9435938752961948, 'errorList': [2.0220122822512576, 3.8696051696859217, 2.7306351973213374, 2.776102639060798, 2.69372902417066, 2.783600540538361, 1.6887376646685814, 2.547797929094559, 2.9056759965230765, 1.8821842817624608, 2.176183464204041, 1.6491829634894724, 1.8965293387510445, 2.7523657918667346, 2.734235930298147, 1.346207444812723, 3.115908970341442, 2.0984997175914657, 2.7384206674959035, 3.390662521681437, 2.824013316814542, 1.8519976104697533, 3.3253822822300294, 2.576278925986661, 2.2881999087927287, 2.1185941658878273, 2.50950059520309, 2.437610574487562, 1.7977093389665881, 1.7836208416988846, 3.351927400068358, 3.1225401173679517, 2.9172320538865657, 2.027054044037634, 2.228099680702511, 2.9183489488712198, 2.807520400563986, 1.9056081152524718, 3.4680181703390205, 3.232242482794033, 1.5837435496217667, 1.5148211981397612, 2.5298421013942685, 1.7224335701489728, 1.931294699346651, 1.507814063383869, 3.139471165964739, 3.3871808601663966, 2.604144009764773, 3.249159361403308], 'lossList': [0.0, -1.3192400950193406, 0.0, 11.204173755347728, 0.0, 0.0, 0.0], 'rewardMean': 0.7159184222444293, 'totalEpisodes': 148, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.512814479814836, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.8850795934493748, 'errorList': [], 'lossList': [0.0, -1.3216112315654756, 0.0, 9.663897140026092, 0.0, 0.0, 0.0], 'rewardMean': 0.7360117966403587, 'totalEpisodes': 152, 'stepsPerEpisode': 63, 'rewardPerEpisode': 55.00298172906999
'totalSteps': 20480, 'rewardStep': 0.8479127467186499, 'errorList': [], 'lossList': [0.0, -1.3239222621917726, 0.0, 9.033908474445344, 0.0, 0.0, 0.0], 'rewardMean': 0.7642888544870134, 'totalEpisodes': 156, 'stepsPerEpisode': 250, 'rewardPerEpisode': 208.83890319688464
'totalSteps': 21760, 'rewardStep': 0.745313580207185, 'errorList': [], 'lossList': [0.0, -1.3346690380573272, 0.0, 6.392944911718368, 0.0, 0.0, 0.0], 'rewardMean': 0.7492991811873766, 'totalEpisodes': 159, 'stepsPerEpisode': 129, 'rewardPerEpisode': 100.99503033506096
'totalSteps': 23040, 'rewardStep': 0.8110681595440087, 'errorList': [], 'lossList': [0.0, -1.3510358923673629, 0.0, 4.023246603608132, 0.0, 0.0, 0.0], 'rewardMean': 0.7509813237829113, 'totalEpisodes': 165, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.535990185319992
'totalSteps': 24320, 'rewardStep': 0.8819617707453822, 'errorList': [], 'lossList': [0.0, -1.3603486907482147, 0.0, 3.465085172653198, 0.0, 0.0, 0.0], 'rewardMean': 0.7549619083355589, 'totalEpisodes': 168, 'stepsPerEpisode': 52, 'rewardPerEpisode': 43.33027179537557
'totalSteps': 25600, 'rewardStep': 0.6616465178559676, 'errorList': [], 'lossList': [0.0, -1.3687282282114028, 0.0, 3.0096548050642014, 0.0, 0.0, 0.0], 'rewardMean': 0.774831851468492, 'totalEpisodes': 169, 'stepsPerEpisode': 304, 'rewardPerEpisode': 254.0152356737344
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.07
