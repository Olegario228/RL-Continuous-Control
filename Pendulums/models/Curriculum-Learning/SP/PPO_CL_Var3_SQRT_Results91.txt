#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 91
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.6170676822579599, 'errorList': [], 'lossList': [0.0, -1.4304148191213608, 0.0, 30.682620911598207, 0.0, 0.0, 0.0], 'rewardMean': 0.5878577003671575, 'totalEpisodes': 14, 'stepsPerEpisode': 103, 'rewardPerEpisode': 78.94677874662905
'totalSteps': 3840, 'rewardStep': 0.7781064146409005, 'errorList': [], 'lossList': [0.0, -1.4265531063079835, 0.0, 34.06171078681946, 0.0, 0.0, 0.0], 'rewardMean': 0.6512739384584051, 'totalEpisodes': 20, 'stepsPerEpisode': 380, 'rewardPerEpisode': 256.4652929261715
'totalSteps': 5120, 'rewardStep': 0.8045383686455868, 'errorList': [], 'lossList': [0.0, -1.42105559527874, 0.0, 30.652189710140227, 0.0, 0.0, 0.0], 'rewardMean': 0.6895900460052006, 'totalEpisodes': 25, 'stepsPerEpisode': 48, 'rewardPerEpisode': 31.919085314849518
'totalSteps': 6400, 'rewardStep': 0.8350258220690527, 'errorList': [], 'lossList': [0.0, -1.4073819953203202, 0.0, 56.91529367446899, 0.0, 0.0, 0.0], 'rewardMean': 0.718677201217971, 'totalEpisodes': 33, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.611153252153571
'totalSteps': 7680, 'rewardStep': 0.727005684299287, 'errorList': [], 'lossList': [0.0, -1.3978251862525939, 0.0, 31.61222876906395, 0.0, 0.0, 0.0], 'rewardMean': 0.7200652817315237, 'totalEpisodes': 35, 'stepsPerEpisode': 981, 'rewardPerEpisode': 739.9819455405985
'totalSteps': 8960, 'rewardStep': 0.42686844961356346, 'errorList': [], 'lossList': [0.0, -1.3986924088001251, 0.0, 132.91542190551758, 0.0, 0.0, 0.0], 'rewardMean': 0.6781800200003865, 'totalEpisodes': 52, 'stepsPerEpisode': 62, 'rewardPerEpisode': 42.15233403403176
'totalSteps': 10240, 'rewardStep': 0.6176533825557736, 'errorList': [], 'lossList': [0.0, -1.3992940658330917, 0.0, 48.72628080844879, 0.0, 0.0, 0.0], 'rewardMean': 0.67061419031981, 'totalEpisodes': 63, 'stepsPerEpisode': 156, 'rewardPerEpisode': 120.40908740480509
'totalSteps': 11520, 'rewardStep': 0.40824581702296747, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6181405156604415, 'totalEpisodes': 71, 'stepsPerEpisode': 14, 'rewardPerEpisode': 7.607394937529649
'totalSteps': 12800, 'rewardStep': 0.5680709306083335, 'errorList': [], 'lossList': [0.0, -1.374658963084221, 0.0, 62.42989861011505, 0.0, 0.0, 0.0], 'rewardMean': 0.6190828368736392, 'totalEpisodes': 80, 'stepsPerEpisode': 100, 'rewardPerEpisode': 65.76830598472182
'totalSteps': 14080, 'rewardStep': 0.8783178453688844, 'errorList': [], 'lossList': [0.0, -1.3365377062559127, 0.0, 24.32395142555237, 0.0, 0.0, 0.0], 'rewardMean': 0.6452078531847316, 'totalEpisodes': 85, 'stepsPerEpisode': 231, 'rewardPerEpisode': 194.8420170008483
'totalSteps': 15360, 'rewardStep': 0.9204900548273054, 'errorList': [], 'lossList': [0.0, -1.3281988197565078, 0.0, 6.698907514810562, 0.0, 0.0, 0.0], 'rewardMean': 0.6594462172033723, 'totalEpisodes': 88, 'stepsPerEpisode': 242, 'rewardPerEpisode': 195.82248274477482
'totalSteps': 16640, 'rewardStep': 0.9161606209230982, 'errorList': [], 'lossList': [0.0, -1.3348401349782943, 0.0, 10.78179013490677, 0.0, 0.0, 0.0], 'rewardMean': 0.6706084424311234, 'totalEpisodes': 91, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.697572856162676
'totalSteps': 17920, 'rewardStep': 0.7489600861131359, 'errorList': [], 'lossList': [0.0, -1.3170529979467391, 0.0, 12.890552322864533, 0.0, 0.0, 0.0], 'rewardMean': 0.6620018688355316, 'totalEpisodes': 93, 'stepsPerEpisode': 155, 'rewardPerEpisode': 112.08190330274377
'totalSteps': 19200, 'rewardStep': 0.916538904917194, 'errorList': [], 'lossList': [0.0, -1.2980071860551834, 0.0, 3.914433844089508, 0.0, 0.0, 0.0], 'rewardMean': 0.6809551908973223, 'totalEpisodes': 94, 'stepsPerEpisode': 253, 'rewardPerEpisode': 210.91923943910132
'totalSteps': 20480, 'rewardStep': 0.5720145459269226, 'errorList': [], 'lossList': [0.0, -1.283786654472351, 0.0, 5.474795458316803, 0.0, 0.0, 0.0], 'rewardMean': 0.6954698005286581, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 959.042349170121
'totalSteps': 21760, 'rewardStep': 0.6444021085134125, 'errorList': [], 'lossList': [0.0, -1.2628984653949737, 0.0, 2.5391205602884295, 0.0, 0.0, 0.0], 'rewardMean': 0.6981446731244223, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1045.969273614148
'totalSteps': 23040, 'rewardStep': 0.9097529458437418, 'errorList': [], 'lossList': [0.0, -1.23380424618721, 0.0, 1.3767664217948914, 0.0, 0.0, 0.0], 'rewardMean': 0.7482953860064997, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.0643905326228
'totalSteps': 24320, 'rewardStep': 0.7655702577957391, 'errorList': [], 'lossList': [0.0, -1.1830863124132156, 0.0, 0.94262411583215, 0.0, 0.0, 0.0], 'rewardMean': 0.7840278300837766, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.1352121774391
'totalSteps': 25600, 'rewardStep': 0.9643480269894334, 'errorList': [0.03360220327998872, 0.0527846927831977, 0.0564899151278195, 0.06614721455933817, 0.05503249841189572, 0.046594235197649034, 0.06438086993586549, 0.07729946040815612, 0.061625988950015816, 0.08229246749942723, 0.059307277904401434, 0.05717138585110602, 0.06716188549878914, 0.06426469665228161, 0.08683152988319952, 0.05089208450603172, 0.0638136924851045, 0.061797983796765606, 0.06028852670844893, 0.060029343591451345, 0.080715546770868, 0.05103839573267504, 0.04275433449202078, 0.044473353304950315, 0.04839494360333154, 0.032872679552972434, 0.05930462805326602, 0.04234891942675285, 0.04000203050183254, 0.04673431914202272, 0.043332379920640665, 0.07189189267453801, 0.06753802582334877, 0.06451028981206591, 0.038890010425505266, 0.03752440403705555, 0.06287099018877879, 0.03962653382299173, 0.03466364432649701, 0.052621933406548936, 0.0686970198233841, 0.04383741615797193, 0.04321684824824174, 0.04501222515916228, 0.033172975393091046, 0.06238649295699729, 0.05878614662944955, 0.0807698515516572, 0.07375249026314101, 0.04260582574329148], 'lossList': [0.0, -1.1307662695646286, 0.0, 0.7829912121593953, 0.0, 0.0, 0.0], 'rewardMean': 0.8236555397218867, 'totalEpisodes': 94, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.2815090456838, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.21
