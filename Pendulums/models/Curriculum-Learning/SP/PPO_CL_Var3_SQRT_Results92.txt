#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 92
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.7147105268442937, 'errorList': [], 'lossList': [0.0, -1.4341765123605728, 0.0, 26.44806610584259, 0.0, 0.0, 0.0], 'rewardMean': 0.596248151033546, 'totalEpisodes': 16, 'stepsPerEpisode': 134, 'rewardPerEpisode': 88.14885749286687
'totalSteps': 3840, 'rewardStep': 0.7483760363947256, 'errorList': [], 'lossList': [0.0, -1.424269174337387, 0.0, 32.367110118865966, 0.0, 0.0, 0.0], 'rewardMean': 0.6469574461539392, 'totalEpisodes': 23, 'stepsPerEpisode': 30, 'rewardPerEpisode': 21.30159730933789
'totalSteps': 5120, 'rewardStep': 0.22878605718474687, 'errorList': [], 'lossList': [0.0, -1.407533238530159, 0.0, 43.13949376583099, 0.0, 0.0, 0.0], 'rewardMean': 0.5424145989116411, 'totalEpisodes': 35, 'stepsPerEpisode': 66, 'rewardPerEpisode': 34.91325480476871
'totalSteps': 6400, 'rewardStep': 0.8877420491582442, 'errorList': [], 'lossList': [0.0, -1.4053295862674713, 0.0, 42.285950412750246, 0.0, 0.0, 0.0], 'rewardMean': 0.6114800889609617, 'totalEpisodes': 42, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.6727665503053393
'totalSteps': 7680, 'rewardStep': 0.8883804874646382, 'errorList': [], 'lossList': [0.0, -1.4035596162080766, 0.0, 102.73132223129272, 0.0, 0.0, 0.0], 'rewardMean': 0.6576301553782411, 'totalEpisodes': 55, 'stepsPerEpisode': 54, 'rewardPerEpisode': 43.90254726383799
'totalSteps': 8960, 'rewardStep': 0.8340253199594064, 'errorList': [], 'lossList': [0.0, -1.4082534885406495, 0.0, 122.82444278717041, 0.0, 0.0, 0.0], 'rewardMean': 0.6828294646041219, 'totalEpisodes': 75, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.5603343936294087
'totalSteps': 10240, 'rewardStep': 0.944023124266994, 'errorList': [95.01534551535894, 159.32227546749635, 202.39946762220714, 220.0503363629623, 155.00713924712622, 160.09349884551003, 109.64377283630894, 203.45504066660803, 187.60963358039007, 124.41195923426042, 74.00246401596675, 219.42941495813687, 169.25489481056846, 144.50158565675923, 136.1056029234221, 233.6722234297461, 207.63820873110552, 87.31721986396539, 131.9843903209408, 136.14347853984583, 205.00841770149665, 225.91307783909852, 65.50572164651368, 138.99573534693513, 164.96263335039774, 108.14029363815496, 149.86859859234642, 37.28581963164701, 142.90759694855447, 161.18967813355192, 84.19436871855534, 135.19780667100395, 191.93720690797556, 191.18071626936438, 108.30695772629288, 148.98452067571841, 141.92610291183215, 156.89133959037093, 117.88556068763273, 107.11026883907384, 140.52613115791002, 184.53522627735043, 200.80818280016106, 230.35139384957063, 204.4650618980659, 140.21403727219078, 112.73565543909403, 181.23147340028262, 220.4444221862529, 143.71211426809128], 'lossList': [0.0, -1.4053654539585114, 0.0, 73.82348378181457, 0.0, 0.0, 0.0], 'rewardMean': 0.7154786720619809, 'totalEpisodes': 88, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.767677584797325, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5391267491430777, 'errorList': [], 'lossList': [0.0, -1.4059577745199203, 0.0, 52.2429642868042, 0.0, 0.0, 0.0], 'rewardMean': 0.6958840139598805, 'totalEpisodes': 95, 'stepsPerEpisode': 215, 'rewardPerEpisode': 171.67682733474405
'totalSteps': 12800, 'rewardStep': 0.9546186859002597, 'errorList': [34.962756583894496, 35.28571546763886, 23.151662980964417, 16.233077727715912, 9.087438307335475, 17.144114588806843, 12.472280984104106, 12.916539439316809, 11.423170284558333, 4.6984830191677815, 16.817090263748305, 55.628798972537865, 13.304796945960996, 27.736779874922156, 6.49578503275468, 2.479351353200811, 17.507845339373414, 43.82151114102245, 0.8798847255447396, 42.78799453612769, 1.33027070427072, 11.77028569795687, 21.179666104669593, 14.943265945755954, 10.12121626180815, 20.91660638869587, 34.21350684147228, 7.6282085792095815, 9.596443065950346, 0.5366973125023599, 18.480689625199272, 11.910915637285775, 31.88038261841934, 7.49519563087845, 7.611437321595269, 36.07516881582696, 3.031840292824809, 21.963755284008176, 26.86566393653532, 17.4299677836418, 14.425506609576205, 11.24621762575594, 7.991716391207719, 17.041749042467224, 3.5833101312343123, 3.057628517471799, 12.069105393141482, 13.12249001478083, 33.4663580580386, 4.604663269361912], 'lossList': [0.0, -1.412794479727745, 0.0, 32.16486263751984, 0.0, 0.0, 0.0], 'rewardMean': 0.7217574811539185, 'totalEpisodes': 103, 'stepsPerEpisode': 102, 'rewardPerEpisode': 92.02158989717952, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.5861854236851494, 'errorList': [], 'lossList': [0.0, -1.4020087254047393, 0.0, 12.80447647690773, 0.0, 0.0, 0.0], 'rewardMean': 0.7325974460001536, 'totalEpisodes': 106, 'stepsPerEpisode': 214, 'rewardPerEpisode': 146.9906236844534
'totalSteps': 15360, 'rewardStep': 0.8915027836338789, 'errorList': [], 'lossList': [0.0, -1.3841988414525985, 0.0, 27.14846023797989, 0.0, 0.0, 0.0], 'rewardMean': 0.750276671679112, 'totalEpisodes': 112, 'stepsPerEpisode': 121, 'rewardPerEpisode': 94.68551845375612
'totalSteps': 16640, 'rewardStep': 0.8211139055200551, 'errorList': [], 'lossList': [0.0, -1.3695993894338607, 0.0, 9.136988990306854, 0.0, 0.0, 0.0], 'rewardMean': 0.757550458591645, 'totalEpisodes': 116, 'stepsPerEpisode': 123, 'rewardPerEpisode': 105.58478099991665
'totalSteps': 17920, 'rewardStep': 0.6508242562907325, 'errorList': [], 'lossList': [0.0, -1.370151447057724, 0.0, 5.154817190170288, 0.0, 0.0, 0.0], 'rewardMean': 0.7997542785022437, 'totalEpisodes': 120, 'stepsPerEpisode': 254, 'rewardPerEpisode': 218.9402184198123
'totalSteps': 19200, 'rewardStep': 0.8754157642229051, 'errorList': [], 'lossList': [0.0, -1.3623487210273744, 0.0, 4.871017273664474, 0.0, 0.0, 0.0], 'rewardMean': 0.7985216500087098, 'totalEpisodes': 124, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.268494918576874
'totalSteps': 20480, 'rewardStep': 0.7967707975787928, 'errorList': [], 'lossList': [0.0, -1.3448764646053315, 0.0, 4.654965650141239, 0.0, 0.0, 0.0], 'rewardMean': 0.7893606810201251, 'totalEpisodes': 125, 'stepsPerEpisode': 262, 'rewardPerEpisode': 223.94006851761048
'totalSteps': 21760, 'rewardStep': 0.8373279726885183, 'errorList': [], 'lossList': [0.0, -1.3269028055667877, 0.0, 2.3567559266090394, 0.0, 0.0, 0.0], 'rewardMean': 0.7896909462930364, 'totalEpisodes': 126, 'stepsPerEpisode': 800, 'rewardPerEpisode': 669.9186379964202
'totalSteps': 23040, 'rewardStep': 0.7660335565931359, 'errorList': [], 'lossList': [0.0, -1.3087603390216827, 0.0, 2.48692955493927, 0.0, 0.0, 0.0], 'rewardMean': 0.7718919895256506, 'totalEpisodes': 127, 'stepsPerEpisode': 626, 'rewardPerEpisode': 531.9306969649105
'totalSteps': 24320, 'rewardStep': 0.770420279856956, 'errorList': [], 'lossList': [0.0, -1.2893094658851623, 0.0, 1.5915363043546678, 0.0, 0.0, 0.0], 'rewardMean': 0.7950213425970384, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.2972499134703
'totalSteps': 25600, 'rewardStep': 0.7688007917796815, 'errorList': [], 'lossList': [0.0, -1.2558549600839615, 0.0, 1.0416843800246716, 0.0, 0.0, 0.0], 'rewardMean': 0.7764395531849806, 'totalEpisodes': 127, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.3177839889213
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=106.53
