#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 93
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5941053261893655, 'errorList': [], 'lossList': [0.0, -1.4249071699380875, 0.0, 23.09064333319664, 0.0, 0.0, 0.0], 'rewardMean': 0.7261587173724526, 'totalEpisodes': 17, 'stepsPerEpisode': 40, 'rewardPerEpisode': 27.393362303887805
'totalSteps': 3840, 'rewardStep': 0.8977095819209983, 'errorList': [], 'lossList': [0.0, -1.4183790647983552, 0.0, 39.348436598777774, 0.0, 0.0, 0.0], 'rewardMean': 0.7833423388886346, 'totalEpisodes': 24, 'stepsPerEpisode': 379, 'rewardPerEpisode': 249.07988470263135
'totalSteps': 5120, 'rewardStep': 0.8032008477089421, 'errorList': [], 'lossList': [0.0, -1.4140242171287536, 0.0, 60.01337692260742, 0.0, 0.0, 0.0], 'rewardMean': 0.7883069660937114, 'totalEpisodes': 33, 'stepsPerEpisode': 77, 'rewardPerEpisode': 67.89882382499361
'totalSteps': 6400, 'rewardStep': 0.5862416819187505, 'errorList': [], 'lossList': [0.0, -1.4209148341417313, 0.0, 78.17442687988282, 0.0, 0.0, 0.0], 'rewardMean': 0.7478939092587192, 'totalEpisodes': 46, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.031515880057436
'totalSteps': 7680, 'rewardStep': 0.6939973056521613, 'errorList': [], 'lossList': [0.0, -1.4094947385787964, 0.0, 114.23750562667847, 0.0, 0.0, 0.0], 'rewardMean': 0.7389111419909596, 'totalEpisodes': 66, 'stepsPerEpisode': 81, 'rewardPerEpisode': 54.66513940794737
'totalSteps': 8960, 'rewardStep': 0.7906142134146739, 'errorList': [], 'lossList': [0.0, -1.3998881894350053, 0.0, 105.19657175064087, 0.0, 0.0, 0.0], 'rewardMean': 0.7462972950514902, 'totalEpisodes': 88, 'stepsPerEpisode': 86, 'rewardPerEpisode': 68.36319745680612
'totalSteps': 10240, 'rewardStep': 0.9508076776557695, 'errorList': [17.89523668741441, 5.640889176195307, 22.45837841119716, 5.753197063393408, 6.582348787229106, 18.82047228049069, 19.01288139428333, 21.619512888285044, 7.7449772616372545, 0.41263991907312086, 4.26437459813586, 5.258606886884736, 2.0537894165858885, 17.822552653505188, 5.955758915393595, 8.519182628168126, 3.1850486680649306, 28.10273992795987, 4.807431418960677, 34.513408696369524, 5.056775946019173, 1.8361207577280705, 13.889282424518937, 12.094304250687157, 18.88569705241563, 24.423801796065515, 3.396494857427965, 13.008940417010148, 5.486437027765983, 13.254583049274181, 5.457244031297597, 1.7665017696074476, 21.705922656185205, 28.257622221043995, 12.123860482405272, 17.861825537719902, 4.086450277000775, 2.6221069952955602, 1.8779745460445352, 19.04513161949347, 18.95659210744627, 14.846174275490537, 4.279155068970297, 14.026657551689688, 9.456621104195946, 27.698666773216974, 15.331157694011782, 30.37222103506878, 15.221971838178048, 5.735313207158537], 'lossList': [0.0, -1.3895190525054932, 0.0, 97.34005062103272, 0.0, 0.0, 0.0], 'rewardMean': 0.7718610928770251, 'totalEpisodes': 116, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.8566959771575196, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5162417750035968, 'errorList': [], 'lossList': [0.0, -1.3849562841653824, 0.0, 41.51469290733338, 0.0, 0.0, 0.0], 'rewardMean': 0.7434589464466441, 'totalEpisodes': 126, 'stepsPerEpisode': 40, 'rewardPerEpisode': 25.908380980003855
'totalSteps': 12800, 'rewardStep': 0.2806098755310348, 'errorList': [], 'lossList': [0.0, -1.3815990895032884, 0.0, 19.2861457324028, 0.0, 0.0, 0.0], 'rewardMean': 0.6971740393550832, 'totalEpisodes': 130, 'stepsPerEpisode': 294, 'rewardPerEpisode': 208.2187780505842
'totalSteps': 14080, 'rewardStep': 0.8552531038448731, 'errorList': [], 'lossList': [0.0, -1.3571461164951324, 0.0, 30.182131052017212, 0.0, 0.0, 0.0], 'rewardMean': 0.6968781388840165, 'totalEpisodes': 140, 'stepsPerEpisode': 28, 'rewardPerEpisode': 20.22598884647039
'totalSteps': 15360, 'rewardStep': 0.4127732304078795, 'errorList': [], 'lossList': [0.0, -1.3378466552495956, 0.0, 20.166180040836334, 0.0, 0.0, 0.0], 'rewardMean': 0.678744929305868, 'totalEpisodes': 147, 'stepsPerEpisode': 149, 'rewardPerEpisode': 112.59309606306216
'totalSteps': 16640, 'rewardStep': 0.5196387214605938, 'errorList': [], 'lossList': [0.0, -1.3192350178956986, 0.0, 10.992118870019914, 0.0, 0.0, 0.0], 'rewardMean': 0.6409378432598275, 'totalEpisodes': 150, 'stepsPerEpisode': 576, 'rewardPerEpisode': 432.8926197391751
'totalSteps': 17920, 'rewardStep': 0.8754557886500809, 'errorList': [], 'lossList': [0.0, -1.3041888016462326, 0.0, 6.492924144268036, 0.0, 0.0, 0.0], 'rewardMean': 0.6481633373539414, 'totalEpisodes': 156, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.64922533697417
'totalSteps': 19200, 'rewardStep': 0.662339757644328, 'errorList': [], 'lossList': [0.0, -1.2990010339021683, 0.0, 6.387932839393616, 0.0, 0.0, 0.0], 'rewardMean': 0.6557731449264991, 'totalEpisodes': 159, 'stepsPerEpisode': 107, 'rewardPerEpisode': 74.88320652387773
'totalSteps': 20480, 'rewardStep': 0.975915964110988, 'errorList': [0.11070770981686037, 0.7123712382551506, 2.6544189071215185, 0.6453594206420009, 2.1558060920559643, 0.4754792004177098, 0.9022122088237864, 0.8201244484185668, 0.7309220602063339, 1.6873435198211426, 0.5666595019663107, 0.3552222792127167, 1.416074427097092, 2.071775270267583, 1.0254629032607587, 3.0503333975575906, 1.02089099710773, 0.523007447176385, 0.37249524826993846, 0.12935501935818886, 1.166366603245409, 2.0422561608528826, 0.288674269643609, 1.5788390989084158, 1.7204550157104688, 1.1191116010935092, 1.7658266653462544, 0.9131234211130663, 2.0601059209910217, 1.4570058562685748, 0.7645190655877521, 0.23395244592688802, 0.4084108268875417, 2.716375798401302, 0.6467585397298686, 1.8179896188394011, 0.6748313049359527, 0.7112769487901855, 1.6727363138381623, 0.8017110416095292, 0.29992152343273687, 4.019085665588387, 2.7306560523461427, 0.6412180850732389, 0.14794292171631634, 2.5319373702306525, 1.3915137608478183, 0.185916325558449, 0.6633056970134459, 0.18636749228205016], 'lossList': [0.0, -1.2776164746284484, 0.0, 5.720188565254212, 0.0, 0.0, 0.0], 'rewardMean': 0.6839650107723818, 'totalEpisodes': 162, 'stepsPerEpisode': 331, 'rewardPerEpisode': 278.24564549675836, 'successfulTests': 5
'totalSteps': 21760, 'rewardStep': 0.6880952638874639, 'errorList': [], 'lossList': [0.0, -1.2428787314891816, 0.0, 5.153233323693275, 0.0, 0.0, 0.0], 'rewardMean': 0.6737131158196608, 'totalEpisodes': 164, 'stepsPerEpisode': 204, 'rewardPerEpisode': 159.99258387060667
'totalSteps': 23040, 'rewardStep': 0.8303020827603013, 'errorList': [], 'lossList': [0.0, -1.2124776810407638, 0.0, 4.678911924362183, 0.0, 0.0, 0.0], 'rewardMean': 0.661662556330114, 'totalEpisodes': 166, 'stepsPerEpisode': 512, 'rewardPerEpisode': 432.5829451220598
'totalSteps': 24320, 'rewardStep': 0.6756275004272742, 'errorList': [], 'lossList': [0.0, -1.1827972143888474, 0.0, 2.7704360538721087, 0.0, 0.0, 0.0], 'rewardMean': 0.6776011288724818, 'totalEpisodes': 169, 'stepsPerEpisode': 144, 'rewardPerEpisode': 124.76947160042393
'totalSteps': 25600, 'rewardStep': 0.8876207377233225, 'errorList': [], 'lossList': [0.0, -1.1721411061286926, 0.0, 2.993827914595604, 0.0, 0.0, 0.0], 'rewardMean': 0.7383022150917105, 'totalEpisodes': 171, 'stepsPerEpisode': 139, 'rewardPerEpisode': 117.46275762191124
#maxSuccessfulTests=5, maxSuccessfulTestsAtStep=20480, timeSpent=102.44
