#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 94
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8214462704842911, 'errorList': [], 'lossList': [0.0, -1.4235526484251022, 0.0, 28.68785089969635, 0.0, 0.0, 0.0], 'rewardMean': 0.7723985971817959, 'totalEpisodes': 16, 'stepsPerEpisode': 36, 'rewardPerEpisode': 26.74053388903964
'totalSteps': 3840, 'rewardStep': 0.7856019396042101, 'errorList': [], 'lossList': [0.0, -1.4371772235631943, 0.0, 28.930344350337982, 0.0, 0.0, 0.0], 'rewardMean': 0.7767997113226007, 'totalEpisodes': 22, 'stepsPerEpisode': 180, 'rewardPerEpisode': 118.38069705307052
'totalSteps': 5120, 'rewardStep': 0.49089938639653485, 'errorList': [], 'lossList': [0.0, -1.4497636687755584, 0.0, 44.9390545463562, 0.0, 0.0, 0.0], 'rewardMean': 0.7053246300910843, 'totalEpisodes': 30, 'stepsPerEpisode': 239, 'rewardPerEpisode': 174.62161859826892
'totalSteps': 6400, 'rewardStep': 0.5880115939032307, 'errorList': [], 'lossList': [0.0, -1.4385305577516556, 0.0, 47.36367946147919, 0.0, 0.0, 0.0], 'rewardMean': 0.6818620228535135, 'totalEpisodes': 38, 'stepsPerEpisode': 350, 'rewardPerEpisode': 250.94497493625875
'totalSteps': 7680, 'rewardStep': 0.7233044885177021, 'errorList': [], 'lossList': [0.0, -1.4243283951282502, 0.0, 63.67171201705933, 0.0, 0.0, 0.0], 'rewardMean': 0.6887691004642117, 'totalEpisodes': 47, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.2409342894798
'totalSteps': 8960, 'rewardStep': 0.5875900214404737, 'errorList': [], 'lossList': [0.0, -1.422294084429741, 0.0, 160.71443161010743, 0.0, 0.0, 0.0], 'rewardMean': 0.6743149463179634, 'totalEpisodes': 76, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.149746450644627
'totalSteps': 10240, 'rewardStep': 0.473803375196569, 'errorList': [], 'lossList': [0.0, -1.416313197016716, 0.0, 86.33704059600831, 0.0, 0.0, 0.0], 'rewardMean': 0.6492509999277891, 'totalEpisodes': 107, 'stepsPerEpisode': 15, 'rewardPerEpisode': 7.0918471324039
'totalSteps': 11520, 'rewardStep': 0.944631737676654, 'errorList': [3.8909124082528743, 53.21656818227407, 28.50795175562976, 31.80348012637561, 55.4268812906295, 23.604080196495264, 66.29241706874625, 27.157126077418802, 41.420133390592305, 26.863393067199585, 48.219901605739956, 33.87552998387637, 22.597682362369643, 33.94844335487329, 49.641530734848914, 11.679148672840395, 16.56968060285101, 39.15321772720288, 37.74536421500715, 13.625781993240473, 50.33724362936409, 7.552466356511973, 38.920983549046944, 26.193736102191835, 17.420756209606562, 42.48711098270547, 3.6051988784424114, 34.80422695375284, 11.106156267537232, 41.965712310947296, 15.379493306865507, 38.06556281173589, 0.014167858946295266, 12.42399539230743, 49.74873612519234, 3.704407409064118, 49.27048161004656, 5.181496883313683, 17.623019495855168, 3.1287050556199825, 32.544904577967756, 28.64606505729647, 31.266125455537384, 10.72351711239538, 16.098504838057963, 21.149670858479098, 30.230358008367386, 41.94742571968644, 30.331775107066242, 1.8585934876084171], 'lossList': [0.0, -1.4271256083250046, 0.0, 12.302306458950042, 0.0, 0.0, 0.0], 'rewardMean': 0.6820710818998852, 'totalEpisodes': 116, 'stepsPerEpisode': 150, 'rewardPerEpisode': 121.90896738681545, 'successfulTests': 1
'totalSteps': 12800, 'rewardStep': 0.7357226050378137, 'errorList': [], 'lossList': [0.0, -1.4207451593875886, 0.0, 26.342318387031554, 0.0, 0.0, 0.0], 'rewardMean': 0.687436234213678, 'totalEpisodes': 126, 'stepsPerEpisode': 62, 'rewardPerEpisode': 54.2984461527597
'totalSteps': 14080, 'rewardStep': 0.5809780091651615, 'errorList': [], 'lossList': [0.0, -1.395238648056984, 0.0, 14.992223575115204, 0.0, 0.0, 0.0], 'rewardMean': 0.673198942742264, 'totalEpisodes': 130, 'stepsPerEpisode': 389, 'rewardPerEpisode': 262.7980195466614
'totalSteps': 15360, 'rewardStep': 0.6725993082742747, 'errorList': [], 'lossList': [0.0, -1.3818226385116577, 0.0, 24.72170704126358, 0.0, 0.0, 0.0], 'rewardMean': 0.6583142465212624, 'totalEpisodes': 138, 'stepsPerEpisode': 246, 'rewardPerEpisode': 204.60677046387426
'totalSteps': 16640, 'rewardStep': 0.5557993953026027, 'errorList': [], 'lossList': [0.0, -1.3885084480047225, 0.0, 9.805767328739167, 0.0, 0.0, 0.0], 'rewardMean': 0.6353339920911016, 'totalEpisodes': 141, 'stepsPerEpisode': 207, 'rewardPerEpisode': 173.27724809674558
'totalSteps': 17920, 'rewardStep': 0.7694499286806074, 'errorList': [], 'lossList': [0.0, -1.3905324923992157, 0.0, 6.8607129848003385, 0.0, 0.0, 0.0], 'rewardMean': 0.6631890463195089, 'totalEpisodes': 145, 'stepsPerEpisode': 44, 'rewardPerEpisode': 39.08192218448887
'totalSteps': 19200, 'rewardStep': 0.8639485511065779, 'errorList': [], 'lossList': [0.0, -1.3753187990188598, 0.0, 5.7538925570249555, 0.0, 0.0, 0.0], 'rewardMean': 0.6907827420398437, 'totalEpisodes': 148, 'stepsPerEpisode': 103, 'rewardPerEpisode': 92.35211635230938
'totalSteps': 20480, 'rewardStep': 0.9600503508179186, 'errorList': [0.4171305827045954, 1.1810375393566528, 0.6146295626595641, 1.1658987632375968, 0.2648252330058858, 0.2252496770732037, 0.8322816742726106, 1.3835247426018906, 0.28765554614137884, 0.1625567060707695, 0.09337156384011717, 0.27887433101296727, 0.6516262673345709, 0.7540908645710006, 1.8460360809840708, 0.18454971518956095, 1.0261664240520658, 0.5577686264001241, 0.28015428196934977, 0.09363828471461581, 1.8069711011332974, 1.0937602475041803, 0.09994642719728566, 0.17217299656208565, 0.21365922267770157, 0.7410721372899057, 1.1525843650785355, 0.37362708406812983, 0.3742605301227339, 0.16235827364580527, 0.58370225960999, 0.46183784844673437, 0.4767605332409538, 1.3518749121078946, 0.42052029427690724, 0.7087781511355615, 0.9769072528476556, 0.21523890698022838, 1.5617613145804916, 0.18174044993913716, 0.12875102208410677, 2.0174950838295853, 1.329703658697386, 0.5346077296005498, 1.6366256386302906, 1.6276006714880333, 0.8517168019242606, 1.3058980287268191, 0.3158306258432062, 0.09698116412474782], 'lossList': [0.0, -1.3746012955904008, 0.0, 4.336017516851425, 0.0, 0.0, 0.0], 'rewardMean': 0.7144573282698652, 'totalEpisodes': 151, 'stepsPerEpisode': 176, 'rewardPerEpisode': 155.18342734561423, 'successfulTests': 10
'totalSteps': 21760, 'rewardStep': 0.8491693760224089, 'errorList': [], 'lossList': [0.0, -1.3720560336112977, 0.0, 5.0979188430309295, 0.0, 0.0, 0.0], 'rewardMean': 0.7406152637280587, 'totalEpisodes': 153, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.86044781969017
'totalSteps': 23040, 'rewardStep': 0.8316170766141836, 'errorList': [], 'lossList': [0.0, -1.3667546021938324, 0.0, 2.134236445426941, 0.0, 0.0, 0.0], 'rewardMean': 0.7763966338698204, 'totalEpisodes': 154, 'stepsPerEpisode': 709, 'rewardPerEpisode': 588.9610641645787
'totalSteps': 24320, 'rewardStep': 0.8430158674483474, 'errorList': [], 'lossList': [0.0, -1.3401866334676742, 0.0, 1.2931884063780308, 0.0, 0.0, 0.0], 'rewardMean': 0.7662350468469896, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.5233362900613
'totalSteps': 25600, 'rewardStep': 0.976102438516905, 'errorList': [0.02587073163941125, 0.016763538373626548, 0.016437065803036256, 0.02638139970539611, 0.0291826723495627, 0.018483496365860812, 0.023396154428961397, 0.017917804805070273, 0.026405981998826725, 0.026909158918843597, 0.02674293632620673, 0.018594798935079093, 0.032927748615117186, 0.03813298580147321, 0.016938460389785626, 0.018148353623549544, 0.025742159583451496, 0.03022908120653156, 0.03739136763715386, 0.021892485509368776, 0.0198539982640488, 0.027141329316928496, 0.021130303360460137, 0.0174282681697205, 0.02069537248104339, 0.016384565854683408, 0.016303177510484698, 0.02480835706515535, 0.01965271678507272, 0.023481734942829374, 0.02015890118542734, 0.018456649669823712, 0.02341233591177055, 0.03480501711047026, 0.016638351893486922, 0.02051466308252712, 0.020915783004117297, 0.025666810368676168, 0.01643621970537175, 0.0338319438003739, 0.021885688077674405, 0.017161677899491082, 0.02595740093795177, 0.017226582900243766, 0.019653843810540274, 0.024882539063444706, 0.033851875404091354, 0.01855441563755811, 0.02361920411819554, 0.01763142219719916], 'lossList': [0.0, -1.3028369176387786, 0.0, 1.0551811115071177, 0.0, 0.0, 0.0], 'rewardMean': 0.7902730301948987, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.9503708830268, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=127.03
