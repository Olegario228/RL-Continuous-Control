#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 95
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9075052556320018, 'errorList': [], 'lossList': [0.0, -1.4384597128629684, 0.0, 25.65667756676674, 0.0, 0.0, 0.0], 'rewardMean': 0.911162943106415, 'totalEpisodes': 8, 'stepsPerEpisode': 534, 'rewardPerEpisode': 354.17290581342706
'totalSteps': 3840, 'rewardStep': 0.5647617165712449, 'errorList': [], 'lossList': [0.0, -1.4286074316501618, 0.0, 40.81076984405517, 0.0, 0.0, 0.0], 'rewardMean': 0.7956958675946916, 'totalEpisodes': 19, 'stepsPerEpisode': 154, 'rewardPerEpisode': 121.66439554965912
'totalSteps': 5120, 'rewardStep': 0.6823022821253294, 'errorList': [], 'lossList': [0.0, -1.4227336764335632, 0.0, 45.66926565170288, 0.0, 0.0, 0.0], 'rewardMean': 0.767347471227351, 'totalEpisodes': 26, 'stepsPerEpisode': 193, 'rewardPerEpisode': 140.9766124091325
'totalSteps': 6400, 'rewardStep': 0.7447146021617363, 'errorList': [], 'lossList': [0.0, -1.419700891971588, 0.0, 41.73347237586975, 0.0, 0.0, 0.0], 'rewardMean': 0.7628208974142281, 'totalEpisodes': 30, 'stepsPerEpisode': 206, 'rewardPerEpisode': 160.69234144731766
'totalSteps': 7680, 'rewardStep': 0.8475244266883173, 'errorList': [], 'lossList': [0.0, -1.397648024559021, 0.0, 113.21014051437378, 0.0, 0.0, 0.0], 'rewardMean': 0.7769381522932429, 'totalEpisodes': 45, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.05342688467888
'totalSteps': 8960, 'rewardStep': 0.7607550676514923, 'errorList': [], 'lossList': [0.0, -1.3851896542310715, 0.0, 168.48642513275146, 0.0, 0.0, 0.0], 'rewardMean': 0.7746262830587071, 'totalEpisodes': 74, 'stepsPerEpisode': 52, 'rewardPerEpisode': 46.21740872328428
'totalSteps': 10240, 'rewardStep': 0.57668416805722, 'errorList': [], 'lossList': [0.0, -1.376894770860672, 0.0, 97.8665265083313, 0.0, 0.0, 0.0], 'rewardMean': 0.7498835186835213, 'totalEpisodes': 97, 'stepsPerEpisode': 38, 'rewardPerEpisode': 28.01701750821659
'totalSteps': 11520, 'rewardStep': 0.48889553098660987, 'errorList': [], 'lossList': [0.0, -1.364558680653572, 0.0, 59.274162435531615, 0.0, 0.0, 0.0], 'rewardMean': 0.7208848533838644, 'totalEpisodes': 112, 'stepsPerEpisode': 70, 'rewardPerEpisode': 54.90164972666236
'totalSteps': 12800, 'rewardStep': 0.9765656162505624, 'errorList': [16.700955137576777, 17.42789361041788, 13.940664919380739, 11.8099433212454, 7.686093578919867, 17.74188324123384, 13.332155710692302, 19.92295126587852, 18.23122089697173, 14.145264850862882, 13.781284558434537, 19.567256108117604, 13.285580556972983, 19.606976335382424, 18.49068260932366, 11.585921647543246, 12.089933961957463, 15.418289960685234, 4.493806798809531, 19.0409035376473, 10.019386064151094, 12.036376620775993, 11.906147404225706, 15.309366452625039, 11.12706905302458, 7.036739901146768, 15.159567185522578, 13.748820171727255, 18.056658448250978, 8.56485729211482, 1.499657357219664, 7.239544048917697, 1.2048151012318469, 12.78430792150032, 14.55774705445534, 7.844541291276146, 3.076536682491856, 14.211960732524396, 21.520447370666194, 1.110350016252649, 15.098392447012632, 12.206529481674151, 17.77790578853178, 14.72591122534823, 12.513385647513317, 12.622161944060151, 3.8146938928627736, 20.48440009711853, 16.91003366903786, 7.302823549028741], 'lossList': [0.0, -1.3423595958948136, 0.0, 47.73792520523071, 0.0, 0.0, 0.0], 'rewardMean': 0.7464529296705342, 'totalEpisodes': 120, 'stepsPerEpisode': 120, 'rewardPerEpisode': 100.44525824036498, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.5753747427565035, 'errorList': [], 'lossList': [0.0, -1.31850856423378, 0.0, 32.336824893951416, 0.0, 0.0, 0.0], 'rewardMean': 0.7125083408881018, 'totalEpisodes': 125, 'stepsPerEpisode': 216, 'rewardPerEpisode': 165.4321932322305
'totalSteps': 15360, 'rewardStep': 0.606489881994429, 'errorList': [], 'lossList': [0.0, -1.304398348927498, 0.0, 29.176245884895323, 0.0, 0.0, 0.0], 'rewardMean': 0.6824068035243445, 'totalEpisodes': 129, 'stepsPerEpisode': 68, 'rewardPerEpisode': 52.003341519564025
'totalSteps': 16640, 'rewardStep': 0.3985496767486888, 'errorList': [], 'lossList': [0.0, -1.304137215614319, 0.0, 9.95724760890007, 0.0, 0.0, 0.0], 'rewardMean': 0.6657855995420889, 'totalEpisodes': 134, 'stepsPerEpisode': 181, 'rewardPerEpisode': 129.15929860952286
'totalSteps': 17920, 'rewardStep': 0.9120869347132954, 'errorList': [], 'lossList': [0.0, -1.296103828549385, 0.0, 5.056894038915634, 0.0, 0.0, 0.0], 'rewardMean': 0.6887640648008856, 'totalEpisodes': 140, 'stepsPerEpisode': 46, 'rewardPerEpisode': 39.62467047434697
'totalSteps': 19200, 'rewardStep': 0.8229346723378431, 'errorList': [], 'lossList': [0.0, -1.2784333419799805, 0.0, 30.678645071983336, 0.0, 0.0, 0.0], 'rewardMean': 0.6965860718184962, 'totalEpisodes': 144, 'stepsPerEpisode': 305, 'rewardPerEpisode': 261.5457968855677
'totalSteps': 20480, 'rewardStep': 0.9026817501140834, 'errorList': [], 'lossList': [0.0, -1.2755710697174072, 0.0, 7.1461390936374665, 0.0, 0.0, 0.0], 'rewardMean': 0.7021018041610727, 'totalEpisodes': 146, 'stepsPerEpisode': 46, 'rewardPerEpisode': 38.28738802590137
'totalSteps': 21760, 'rewardStep': 0.5740065631276048, 'errorList': [], 'lossList': [0.0, -1.2758116680383682, 0.0, 4.910797500610352, 0.0, 0.0, 0.0], 'rewardMean': 0.683426953708684, 'totalEpisodes': 147, 'stepsPerEpisode': 556, 'rewardPerEpisode': 386.655200384709
'totalSteps': 23040, 'rewardStep': 0.724996394737499, 'errorList': [], 'lossList': [0.0, -1.2596186500787736, 0.0, 1.7990524570643902, 0.0, 0.0, 0.0], 'rewardMean': 0.6982581763767118, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.905063584201
'totalSteps': 24320, 'rewardStep': 0.6152702300649272, 'errorList': [], 'lossList': [0.0, -1.2440779167413711, 0.0, 1.6586388421058655, 0.0, 0.0, 0.0], 'rewardMean': 0.7108956462845436, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.9884772164105
'totalSteps': 25600, 'rewardStep': 0.904816240014621, 'errorList': [], 'lossList': [0.0, -1.2095444363355636, 0.0, 1.4774374759197235, 0.0, 0.0, 0.0], 'rewardMean': 0.7037207086609495, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1084.7441197049907
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.94
