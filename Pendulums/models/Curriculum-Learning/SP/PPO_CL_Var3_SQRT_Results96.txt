#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 96
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.7250844217236958, 'errorList': [], 'lossList': [0.0, -1.4549581265449525, 0.0, 30.853931126594542, 0.0, 0.0, 0.0], 'rewardMean': 0.6591311206519276, 'totalEpisodes': 12, 'stepsPerEpisode': 84, 'rewardPerEpisode': 67.49267424734074
'totalSteps': 3840, 'rewardStep': 0.865853469029665, 'errorList': [], 'lossList': [0.0, -1.4781619107723236, 0.0, 32.66956331372261, 0.0, 0.0, 0.0], 'rewardMean': 0.7280385701111735, 'totalEpisodes': 16, 'stepsPerEpisode': 167, 'rewardPerEpisode': 121.10017093325722
'totalSteps': 5120, 'rewardStep': 0.8544742963221574, 'errorList': [], 'lossList': [0.0, -1.4692045897245407, 0.0, 29.771716051101684, 0.0, 0.0, 0.0], 'rewardMean': 0.7596475016639195, 'totalEpisodes': 19, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.850746935161533
'totalSteps': 6400, 'rewardStep': 0.9327761598384474, 'errorList': [], 'lossList': [0.0, -1.446493654847145, 0.0, 68.31945702552795, 0.0, 0.0, 0.0], 'rewardMean': 0.794273233298825, 'totalEpisodes': 27, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.351434696223075
'totalSteps': 7680, 'rewardStep': 0.7870742192845479, 'errorList': [], 'lossList': [0.0, -1.4172255170345307, 0.0, 42.31043630838394, 0.0, 0.0, 0.0], 'rewardMean': 0.7930733976297789, 'totalEpisodes': 30, 'stepsPerEpisode': 805, 'rewardPerEpisode': 594.6398768843835
'totalSteps': 8960, 'rewardStep': 0.8042463888673419, 'errorList': [], 'lossList': [0.0, -1.4121383106708527, 0.0, 247.6809445953369, 0.0, 0.0, 0.0], 'rewardMean': 0.794669539235145, 'totalEpisodes': 63, 'stepsPerEpisode': 23, 'rewardPerEpisode': 17.143577189915387
'totalSteps': 10240, 'rewardStep': 0.748082048145872, 'errorList': [], 'lossList': [0.0, -1.4052449458837508, 0.0, 136.26152462005615, 0.0, 0.0, 0.0], 'rewardMean': 0.7888461028489859, 'totalEpisodes': 92, 'stepsPerEpisode': 57, 'rewardPerEpisode': 50.845376659086014
'totalSteps': 11520, 'rewardStep': 0.7280822008935347, 'errorList': [], 'lossList': [0.0, -1.4007222962379455, 0.0, 65.04928610801697, 0.0, 0.0, 0.0], 'rewardMean': 0.7820945581872691, 'totalEpisodes': 123, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.916582486719395
'totalSteps': 12800, 'rewardStep': 0.9080620338818841, 'errorList': [], 'lossList': [0.0, -1.4023023629188538, 0.0, 42.65385600090027, 0.0, 0.0, 0.0], 'rewardMean': 0.7946913057567306, 'totalEpisodes': 143, 'stepsPerEpisode': 34, 'rewardPerEpisode': 29.746602426074205
'totalSteps': 14080, 'rewardStep': 0.9262493670366835, 'errorList': [], 'lossList': [0.0, -1.387651693224907, 0.0, 43.81314826011658, 0.0, 0.0, 0.0], 'rewardMean': 0.8279984605023829, 'totalEpisodes': 151, 'stepsPerEpisode': 195, 'rewardPerEpisode': 153.39212263892074
'totalSteps': 15360, 'rewardStep': 0.774443104020548, 'errorList': [], 'lossList': [0.0, -1.3593249064683914, 0.0, 13.467124004364013, 0.0, 0.0, 0.0], 'rewardMean': 0.8329343287320681, 'totalEpisodes': 155, 'stepsPerEpisode': 369, 'rewardPerEpisode': 272.61249739945714
'totalSteps': 16640, 'rewardStep': 0.7098343015733892, 'errorList': [], 'lossList': [0.0, -1.3196646517515183, 0.0, 11.123116805553437, 0.0, 0.0, 0.0], 'rewardMean': 0.8173324119864407, 'totalEpisodes': 160, 'stepsPerEpisode': 121, 'rewardPerEpisode': 94.68179689972011
'totalSteps': 17920, 'rewardStep': 0.6624441903186071, 'errorList': [], 'lossList': [0.0, -1.309675327539444, 0.0, 7.394359339475631, 0.0, 0.0, 0.0], 'rewardMean': 0.7981294013860856, 'totalEpisodes': 161, 'stepsPerEpisode': 175, 'rewardPerEpisode': 127.84142763860702
'totalSteps': 19200, 'rewardStep': 0.8807122201344624, 'errorList': [], 'lossList': [0.0, -1.2844534665346146, 0.0, 3.8240656834840774, 0.0, 0.0, 0.0], 'rewardMean': 0.792923007415687, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 950.6327163095917
'totalSteps': 20480, 'rewardStep': 0.7274739764784779, 'errorList': [], 'lossList': [0.0, -1.2380567264556885, 0.0, 5.195452674776316, 0.0, 0.0, 0.0], 'rewardMean': 0.7869629831350802, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.7558217999544
'totalSteps': 21760, 'rewardStep': 0.7761737578149706, 'errorList': [], 'lossList': [0.0, -1.1696172952651978, 0.0, 3.3050101562589407, 0.0, 0.0, 0.0], 'rewardMean': 0.7841557200298429, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.7699629305157
'totalSteps': 23040, 'rewardStep': 0.9648793704576567, 'errorList': [0.13943121568365074, 0.18160577735434433, 0.11830634234712828, 0.1369942304759356, 0.18369242288892496, 0.11817640428898347, 0.16563408883258016, 0.12858853720501054, 0.12777616655650473, 0.16225491793692268, 0.10689360257267734, 0.09264979025828914, 0.12643447102713912, 0.13273332132592097, 0.2701172035763987, 0.11719409236748626, 0.14009739380424144, 0.09711022302222701, 0.10565002323835093, 0.11709883445064079, 0.13784169477799976, 0.14526459397717292, 0.16535384951741175, 0.1361388225776176, 0.13578307693956138, 0.14071177771787247, 0.1455158955797065, 0.18896721911983397, 0.09131099492979787, 0.1050041295319597, 0.10786276186781228, 0.19119360873296212, 0.1756033262771682, 0.14741876624772052, 0.20725905112640555, 0.16377206823667437, 0.1992656767345574, 0.26936980640382224, 0.1428103098574908, 0.17537262254266947, 0.10556163917535491, 0.1587850012470807, 0.12846202958860908, 0.20414507417330532, 0.22477283210736415, 0.12022272138189892, 0.2477276191139623, 0.3056330823372049, 0.2126550139109813, 0.13190089532944047], 'lossList': [0.0, -1.140122610926628, 0.0, 2.040978891141713, 0.0, 0.0, 0.0], 'rewardMean': 0.8058354522610214, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1116.9750814045926, 'successfulTests': 42
'totalSteps': 24320, 'rewardStep': 0.8457194308770675, 'errorList': [], 'lossList': [0.0, -1.1108808404207229, 0.0, 1.6280539951473474, 0.0, 0.0, 0.0], 'rewardMean': 0.8175991752593748, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.366038359355
'totalSteps': 25600, 'rewardStep': 0.956586213595893, 'errorList': [0.06625432983669431, 0.027824659059717014, 0.13721775366767272, 0.07339672954920712, 0.11148660995733073, 0.16176965661935486, 0.08930800065264423, 0.10843776648358115, 0.14931305774749706, 0.05732516904425138, 0.07820683428156572, 0.09286210289967949, 0.0993522049689678, 0.023368858918488538, 0.16147512291749613, 0.09240950444913533, 0.1151023624858702, 0.04868032983724089, 0.24815935640092468, 0.1414127950597884, 0.10818945793325316, 0.10799269164876778, 0.1659498432489199, 0.16022472063366458, 0.11611712238342596, 0.039353787035774246, 0.17107924080585046, 0.14750430583260435, 0.09618370527346826, 0.055310761061198296, 0.10629805507841192, 0.07185900407326046, 0.0946021039320709, 0.16183787182412618, 0.21390335241318997, 0.08943502643186028, 0.21047675937648716, 0.0809479015420116, 0.11767092016072474, 0.1736765269591735, 0.03705962195440216, 0.07956083307111944, 0.16413730207077828, 0.18286876382128497, 0.053780515905596044, 0.12737682868907663, 0.11971780565916325, 0.0983347029406644, 0.19127420731041306, 0.04738581163824914], 'lossList': [0.0, -1.0653516030311585, 0.0, 1.4123250087723136, 0.0, 0.0, 0.0], 'rewardMean': 0.8224515932307754, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.732517505799, 'successfulTests': 47
#maxSuccessfulTests=47, maxSuccessfulTestsAtStep=25600, timeSpent=106.2
