#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 97
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_SQRT_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.6764665890017514, 'errorList': [], 'lossList': [0.0, -1.4488234883546829, 0.0, 30.25238534450531, 0.0, 0.0, 0.0], 'rewardMean': 0.5897837184528816, 'totalEpisodes': 15, 'stepsPerEpisode': 292, 'rewardPerEpisode': 217.9920735071993
'totalSteps': 3840, 'rewardStep': 0.9259516048016303, 'errorList': [], 'lossList': [0.0, -1.454305535554886, 0.0, 32.59769304990768, 0.0, 0.0, 0.0], 'rewardMean': 0.701839680569131, 'totalEpisodes': 21, 'stepsPerEpisode': 77, 'rewardPerEpisode': 58.962806220914196
'totalSteps': 5120, 'rewardStep': 0.6628657021980934, 'errorList': [], 'lossList': [0.0, -1.4325499963760375, 0.0, 45.653255004882816, 0.0, 0.0, 0.0], 'rewardMean': 0.6920961859763717, 'totalEpisodes': 32, 'stepsPerEpisode': 53, 'rewardPerEpisode': 37.079900310469036
'totalSteps': 6400, 'rewardStep': 0.8768504219837157, 'errorList': [], 'lossList': [0.0, -1.4267508566379548, 0.0, 69.92671709060669, 0.0, 0.0, 0.0], 'rewardMean': 0.7290470331778405, 'totalEpisodes': 43, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.570732030305652
'totalSteps': 7680, 'rewardStep': 0.6122651599123337, 'errorList': [], 'lossList': [0.0, -1.4194344669580459, 0.0, 89.73854362487793, 0.0, 0.0, 0.0], 'rewardMean': 0.7095833876335894, 'totalEpisodes': 58, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.186120375895233
'totalSteps': 8960, 'rewardStep': 0.6934717644022532, 'errorList': [], 'lossList': [0.0, -1.4161624318361283, 0.0, 127.85147338867188, 0.0, 0.0, 0.0], 'rewardMean': 0.7072817271719699, 'totalEpisodes': 91, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4149408634770266
'totalSteps': 10240, 'rewardStep': 0.7438766346095327, 'errorList': [], 'lossList': [0.0, -1.4186503636837005, 0.0, 62.781928596496584, 0.0, 0.0, 0.0], 'rewardMean': 0.7118560906016652, 'totalEpisodes': 111, 'stepsPerEpisode': 55, 'rewardPerEpisode': 37.859358455274226
'totalSteps': 11520, 'rewardStep': 0.3009293160797345, 'errorList': [], 'lossList': [0.0, -1.4214902061223984, 0.0, 15.276726756095886, 0.0, 0.0, 0.0], 'rewardMean': 0.6661975600992285, 'totalEpisodes': 121, 'stepsPerEpisode': 110, 'rewardPerEpisode': 68.18054917596028
'totalSteps': 12800, 'rewardStep': 0.6006612828090461, 'errorList': [], 'lossList': [0.0, -1.4050366216897965, 0.0, 26.276969337463377, 0.0, 0.0, 0.0], 'rewardMean': 0.6596439323702102, 'totalEpisodes': 131, 'stepsPerEpisode': 106, 'rewardPerEpisode': 83.78440936984786
'totalSteps': 14080, 'rewardStep': 0.7502071191841369, 'errorList': [], 'lossList': [0.0, -1.3762687331438064, 0.0, 15.26164383172989, 0.0, 0.0, 0.0], 'rewardMean': 0.6843545594982228, 'totalEpisodes': 139, 'stepsPerEpisode': 30, 'rewardPerEpisode': 22.92655233707585
'totalSteps': 15360, 'rewardStep': 0.9047352924149228, 'errorList': [], 'lossList': [0.0, -1.3441764837503434, 0.0, 30.691917147636413, 0.0, 0.0, 0.0], 'rewardMean': 0.7071814298395399, 'totalEpisodes': 147, 'stepsPerEpisode': 92, 'rewardPerEpisode': 78.60241900465857
'totalSteps': 16640, 'rewardStep': 0.9456662080192857, 'errorList': [7.29272397874659, 12.961716467719029, 18.289146626562655, 18.997179929664313, 1.4593954646048142, 1.254219786709571, 8.900809697893811, 27.11449803188961, 0.5309014223419373, 1.4833737635379076, 7.947706539403012, 9.562802897280804, 4.277158215954733, 8.530566300127374, 6.877823907872293, 1.280117082231056, 3.3836739645687635, 6.585371848200761, 16.819798998936676, 13.700106735356032, 0.8362468403050985, 1.1842435761526906, 2.8622801788892467, 0.23944263289541437, 0.4595058180789368, 1.9187021632581736, 18.025931358100205, 5.9883852673580416, 3.545695108456639, 11.376582015226655, 6.38044116600673, 33.91209571686114, 6.142799577137654, 4.387117289664664, 6.404552552625715, 10.1858037639098, 3.1396728113560024, 6.327020620843053, 8.53201762576506, 13.670727247194419, 13.259280105920586, 0.7329012427509131, 2.2133432570971525, 12.476694698655677, 5.818107977170257, 35.27638733022036, 7.691363413138042, 12.733039715850396, 11.771828961211193, 12.777926977873491], 'lossList': [0.0, -1.3269051069021225, 0.0, 7.075318170785904, 0.0, 0.0, 0.0], 'rewardMean': 0.7091528901613054, 'totalEpisodes': 152, 'stepsPerEpisode': 77, 'rewardPerEpisode': 69.65241664293444, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.8217977923098116, 'errorList': [], 'lossList': [0.0, -1.3169034266471862, 0.0, 6.502425311803818, 0.0, 0.0, 0.0], 'rewardMean': 0.7250460991724772, 'totalEpisodes': 156, 'stepsPerEpisode': 152, 'rewardPerEpisode': 136.13955170558745
'totalSteps': 19200, 'rewardStep': 0.7592626385497887, 'errorList': [], 'lossList': [0.0, -1.3010159623622894, 0.0, 9.696836043000221, 0.0, 0.0, 0.0], 'rewardMean': 0.7132873208290846, 'totalEpisodes': 158, 'stepsPerEpisode': 282, 'rewardPerEpisode': 240.01250458493786
'totalSteps': 20480, 'rewardStep': 0.8009829938769917, 'errorList': [], 'lossList': [0.0, -1.2944984668493271, 0.0, 3.7555114364624025, 0.0, 0.0, 0.0], 'rewardMean': 0.7321591042255504, 'totalEpisodes': 162, 'stepsPerEpisode': 65, 'rewardPerEpisode': 48.85699876550254
'totalSteps': 21760, 'rewardStep': 0.7503582576438068, 'errorList': [], 'lossList': [0.0, -1.3122683155536652, 0.0, 3.866975599527359, 0.0, 0.0, 0.0], 'rewardMean': 0.7378477535497058, 'totalEpisodes': 165, 'stepsPerEpisode': 250, 'rewardPerEpisode': 211.28663511472055
'totalSteps': 23040, 'rewardStep': 0.676170549566106, 'errorList': [], 'lossList': [0.0, -1.3045207679271698, 0.0, 2.5876753133535386, 0.0, 0.0, 0.0], 'rewardMean': 0.7310771450453631, 'totalEpisodes': 166, 'stepsPerEpisode': 1057, 'rewardPerEpisode': 840.0301188975506
'totalSteps': 24320, 'rewardStep': 0.8427117312623048, 'errorList': [], 'lossList': [0.0, -1.2665458506345748, 0.0, 1.892552400380373, 0.0, 0.0, 0.0], 'rewardMean': 0.7852553865636202, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1032.8838751663295
'totalSteps': 25600, 'rewardStep': 0.8127680423982577, 'errorList': [], 'lossList': [0.0, -1.2374091398715974, 0.0, 0.972517631649971, 0.0, 0.0, 0.0], 'rewardMean': 0.8064660625225413, 'totalEpisodes': 166, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.1291674945764
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=83.73
