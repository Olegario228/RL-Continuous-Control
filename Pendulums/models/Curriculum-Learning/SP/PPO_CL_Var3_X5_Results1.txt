#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980188642849941, 'errorList': [], 'lossList': [0.0, -1.4172632962465286, 0.0, 30.401201176643372, 0.0, 0.0, 0.0], 'rewardMean': 0.7065344985632007, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.34134190724609
'totalSteps': 3840, 'rewardStep': 0.7170219922501487, 'errorList': [], 'lossList': [0.0, -1.4030259454250336, 0.0, 35.851880435943606, 0.0, 0.0, 0.0], 'rewardMean': 0.7100303297921834, 'totalEpisodes': 66, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.18420190106966
'totalSteps': 5120, 'rewardStep': 0.6314149533208492, 'errorList': [], 'lossList': [0.0, -1.3931202650070191, 0.0, 37.899265899658204, 0.0, 0.0, 0.0], 'rewardMean': 0.6903764856743498, 'totalEpisodes': 77, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.0834175097368925
'totalSteps': 6400, 'rewardStep': 0.5765108742750198, 'errorList': [], 'lossList': [0.0, -1.3877398985624314, 0.0, 111.59674125671387, 0.0, 0.0, 0.0], 'rewardMean': 0.6676033633944838, 'totalEpisodes': 118, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.5742849431442723
'totalSteps': 7680, 'rewardStep': 0.37405209683886065, 'errorList': [], 'lossList': [0.0, -1.377381575703621, 0.0, 51.74402933120727, 0.0, 0.0, 0.0], 'rewardMean': 0.61867815230188, 'totalEpisodes': 136, 'stepsPerEpisode': 216, 'rewardPerEpisode': 160.1279594300134
'totalSteps': 8960, 'rewardStep': 0.5126996423961945, 'errorList': [], 'lossList': [0.0, -1.3606773954629898, 0.0, 59.87819816589356, 0.0, 0.0, 0.0], 'rewardMean': 0.6035383651724963, 'totalEpisodes': 154, 'stepsPerEpisode': 111, 'rewardPerEpisode': 80.32614378517437
'totalSteps': 10240, 'rewardStep': 0.7232493964090465, 'errorList': [], 'lossList': [0.0, -1.3518838059902192, 0.0, 37.537451906204225, 0.0, 0.0, 0.0], 'rewardMean': 0.6185022440770651, 'totalEpisodes': 162, 'stepsPerEpisode': 56, 'rewardPerEpisode': 46.935472616173506
'totalSteps': 11520, 'rewardStep': 0.46949389957756327, 'errorList': [], 'lossList': [0.0, -1.3286521172523498, 0.0, 17.58671612739563, 0.0, 0.0, 0.0], 'rewardMean': 0.6019457613548983, 'totalEpisodes': 167, 'stepsPerEpisode': 95, 'rewardPerEpisode': 72.160172185689
'totalSteps': 12800, 'rewardStep': 0.6992813512620181, 'errorList': [], 'lossList': [0.0, -1.288252649307251, 0.0, 15.092597470283508, 0.0, 0.0, 0.0], 'rewardMean': 0.6116793203456102, 'totalEpisodes': 171, 'stepsPerEpisode': 188, 'rewardPerEpisode': 145.79635762207806
'totalSteps': 14080, 'rewardStep': 0.7325921025124238, 'errorList': [], 'lossList': [0.0, -1.2704246455430985, 0.0, 7.096869357228279, 0.0, 0.0, 0.0], 'rewardMean': 0.6034335173127119, 'totalEpisodes': 172, 'stepsPerEpisode': 445, 'rewardPerEpisode': 344.76437434512604
'totalSteps': 15360, 'rewardStep': 0.7460585101997572, 'errorList': [], 'lossList': [0.0, -1.2854057848453522, 0.0, 7.522313171029091, 0.0, 0.0, 0.0], 'rewardMean': 0.6182374819041883, 'totalEpisodes': 177, 'stepsPerEpisode': 110, 'rewardPerEpisode': 91.72664831086006
'totalSteps': 16640, 'rewardStep': 0.8296284972016923, 'errorList': [], 'lossList': [0.0, -1.2828874897956848, 0.0, 4.335119047164917, 0.0, 0.0, 0.0], 'rewardMean': 0.6294981323993426, 'totalEpisodes': 183, 'stepsPerEpisode': 78, 'rewardPerEpisode': 66.07596242242298
'totalSteps': 17920, 'rewardStep': 0.8180089061120559, 'errorList': [], 'lossList': [0.0, -1.262903133034706, 0.0, 3.786373054981232, 0.0, 0.0, 0.0], 'rewardMean': 0.6481575276784632, 'totalEpisodes': 187, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.299481682235154
'totalSteps': 19200, 'rewardStep': 0.4985397776956513, 'errorList': [], 'lossList': [0.0, -1.2301054525375366, 0.0, 3.3980913829803465, 0.0, 0.0, 0.0], 'rewardMean': 0.6403604180205263, 'totalEpisodes': 192, 'stepsPerEpisode': 198, 'rewardPerEpisode': 149.80207629174603
'totalSteps': 20480, 'rewardStep': 0.729158129007075, 'errorList': [], 'lossList': [0.0, -1.2100286376476288, 0.0, 2.8187038415670393, 0.0, 0.0, 0.0], 'rewardMean': 0.6758710212373478, 'totalEpisodes': 197, 'stepsPerEpisode': 148, 'rewardPerEpisode': 128.94870538657426
'totalSteps': 21760, 'rewardStep': 0.7433708404060813, 'errorList': [], 'lossList': [0.0, -1.1877998250722885, 0.0, 3.343558420538902, 0.0, 0.0, 0.0], 'rewardMean': 0.6989381410383365, 'totalEpisodes': 200, 'stepsPerEpisode': 159, 'rewardPerEpisode': 143.2849917161803
'totalSteps': 23040, 'rewardStep': 0.9373261070867963, 'errorList': [1.1027529413817676, 0.4359693652780101, 0.8459241287283247, 0.9408721555646467, 1.3479213053851355, 0.910225097348462, 0.4554362420709052, 0.8911928792986636, 0.13474486958925594, 1.008545675553419, 0.7116988190905976, 0.6182722768961376, 0.7654487657883083, 0.5956269596576311, 0.19453112461085836, 0.8702176122576043, 1.1292059353636512, 0.6866378237789966, 0.6634594860865053, 0.9477336335175487, 1.0274392135127357, 0.6440987255826793, 0.3412670684094707, 0.833837385127368, 0.7824766269685773, 0.18747193789147612, 1.1743374274511849, 0.22375061884980507, 0.6998998860170658, 0.6571337959939083, 0.7134711417895431, 0.16342902428355474, 1.2414228895895683, 1.0972859955942948, 0.169520336414906, 0.130646601416524, 0.35460946111031216, 0.288213352162735, 0.988840256124929, 0.3015927122902355, 0.7845187946482355, 1.076491305455079, 0.8820338188539402, 1.6070401205411986, 0.2298714175628225, 0.6492283159248771, 0.18824229610722085, 0.3692217114240382, 0.11668446802208249, 0.20067444541471252], 'lossList': [0.0, -1.174007259607315, 0.0, 1.8029044315218925, 0.0, 0.0, 0.0], 'rewardMean': 0.7203458121061114, 'totalEpisodes': 203, 'stepsPerEpisode': 41, 'rewardPerEpisode': 37.874086419462216, 'successfulTests': 8
'totalSteps': 24320, 'rewardStep': 0.7481489061516278, 'errorList': [], 'lossList': [0.0, -1.1623441618680954, 0.0, 2.0216700598597526, 0.0, 0.0, 0.0], 'rewardMean': 0.7482113127635179, 'totalEpisodes': 205, 'stepsPerEpisode': 211, 'rewardPerEpisode': 185.64954362603504
'totalSteps': 25600, 'rewardStep': 0.6195201439006719, 'errorList': [], 'lossList': [0.0, -1.1338423031568527, 0.0, 1.918316459953785, 0.0, 0.0, 0.0], 'rewardMean': 0.7402351920273833, 'totalEpisodes': 206, 'stepsPerEpisode': 699, 'rewardPerEpisode': 523.7760037066691
#maxSuccessfulTests=8, maxSuccessfulTestsAtStep=23040, timeSpent=71.49
