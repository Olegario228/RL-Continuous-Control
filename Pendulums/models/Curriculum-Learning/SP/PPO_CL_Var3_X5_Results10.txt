#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 10
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9248175885116525, 'errorList': [], 'lossList': [0.0, -1.4073614650964736, 0.0, 27.188909937143325, 0.0, 0.0, 0.0], 'rewardMean': 0.8623082745053078, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.0353607535033
'totalSteps': 3840, 'rewardStep': 0.6273120935411091, 'errorList': [], 'lossList': [0.0, -1.4012649846076966, 0.0, 37.42641267299652, 0.0, 0.0, 0.0], 'rewardMean': 0.7839762141839083, 'totalEpisodes': 12, 'stepsPerEpisode': 265, 'rewardPerEpisode': 201.46162364824528
'totalSteps': 5120, 'rewardStep': 0.7150818324390735, 'errorList': [], 'lossList': [0.0, -1.4069947344064713, 0.0, 24.90335988998413, 0.0, 0.0, 0.0], 'rewardMean': 0.7667526187476996, 'totalEpisodes': 14, 'stepsPerEpisode': 473, 'rewardPerEpisode': 342.15395622607383
'totalSteps': 6400, 'rewardStep': 0.8846238740022383, 'errorList': [], 'lossList': [0.0, -1.3994487231969834, 0.0, 257.0531830596924, 0.0, 0.0, 0.0], 'rewardMean': 0.7903268697986073, 'totalEpisodes': 82, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.485019881683428
'totalSteps': 7680, 'rewardStep': 0.7334097341303903, 'errorList': [], 'lossList': [0.0, -1.3937745845317842, 0.0, 96.59675773620606, 0.0, 0.0, 0.0], 'rewardMean': 0.7808406805205711, 'totalEpisodes': 133, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4595226185370658
'totalSteps': 8960, 'rewardStep': 0.5980604150789852, 'errorList': [], 'lossList': [0.0, -1.3841727995872497, 0.0, 52.92534087181091, 0.0, 0.0, 0.0], 'rewardMean': 0.754729214028916, 'totalEpisodes': 163, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.222806790041382
'totalSteps': 10240, 'rewardStep': 0.8131120694704823, 'errorList': [], 'lossList': [0.0, -1.3628003454208375, 0.0, 62.51141347885132, 0.0, 0.0, 0.0], 'rewardMean': 0.7620270709591118, 'totalEpisodes': 190, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.609169155754698
'totalSteps': 11520, 'rewardStep': 0.8758878169145577, 'errorList': [], 'lossList': [0.0, -1.3317233699560165, 0.0, 47.330795192718504, 0.0, 0.0, 0.0], 'rewardMean': 0.7746782649541614, 'totalEpisodes': 202, 'stepsPerEpisode': 52, 'rewardPerEpisode': 45.000263667332476
'totalSteps': 12800, 'rewardStep': 0.7968670118539178, 'errorList': [], 'lossList': [0.0, -1.3042055642604828, 0.0, 34.75219336986542, 0.0, 0.0, 0.0], 'rewardMean': 0.776897139644137, 'totalEpisodes': 207, 'stepsPerEpisode': 89, 'rewardPerEpisode': 74.23573359895555
'totalSteps': 14080, 'rewardStep': 0.7732086828148905, 'errorList': [], 'lossList': [0.0, -1.293150936961174, 0.0, 31.086108849048614, 0.0, 0.0, 0.0], 'rewardMean': 0.7742381118757297, 'totalEpisodes': 210, 'stepsPerEpisode': 194, 'rewardPerEpisode': 157.95164293538076
'totalSteps': 15360, 'rewardStep': 0.6911944950123355, 'errorList': [], 'lossList': [0.0, -1.2977518165111541, 0.0, 16.40243638753891, 0.0, 0.0, 0.0], 'rewardMean': 0.7508758025257981, 'totalEpisodes': 213, 'stepsPerEpisode': 573, 'rewardPerEpisode': 445.17930022018743
'totalSteps': 16640, 'rewardStep': 0.7976095897203653, 'errorList': [], 'lossList': [0.0, -1.3189572149515152, 0.0, 16.331073464155196, 0.0, 0.0, 0.0], 'rewardMean': 0.7679055521437237, 'totalEpisodes': 216, 'stepsPerEpisode': 189, 'rewardPerEpisode': 162.38993705647434
'totalSteps': 17920, 'rewardStep': 0.7127725884565418, 'errorList': [], 'lossList': [0.0, -1.3105029082298278, 0.0, 7.167771745920181, 0.0, 0.0, 0.0], 'rewardMean': 0.7676746277454705, 'totalEpisodes': 222, 'stepsPerEpisode': 97, 'rewardPerEpisode': 72.81431535754213
'totalSteps': 19200, 'rewardStep': 0.5702105039776737, 'errorList': [], 'lossList': [0.0, -1.2991927230358125, 0.0, 26.119563157558442, 0.0, 0.0, 0.0], 'rewardMean': 0.736233290743014, 'totalEpisodes': 228, 'stepsPerEpisode': 158, 'rewardPerEpisode': 118.62159486476477
'totalSteps': 20480, 'rewardStep': 0.7507573849477709, 'errorList': [], 'lossList': [0.0, -1.2792698156833648, 0.0, 9.383508521318436, 0.0, 0.0, 0.0], 'rewardMean': 0.7379680558247521, 'totalEpisodes': 232, 'stepsPerEpisode': 91, 'rewardPerEpisode': 65.62253513180376
'totalSteps': 21760, 'rewardStep': 0.949855846615274, 'errorList': [2.791294993305086, 1.144066160002062, 0.13356886828773948, 1.773486568049988, 0.1284281756636059, 1.9555189729931703, 1.9889432510642908, 0.8650486979653749, 1.0624689354041579, 0.20988304130003493, 0.7020109061787254, 0.8578240439205834, 0.9042508254915521, 0.9208540014692732, 0.6712526123705792, 0.1976339598009701, 2.057863696994302, 1.3250570768560321, 0.2510830821766909, 0.983050336894988, 0.20363790621977493, 1.6623386189467175, 2.0151508709866905, 1.1649855208017905, 0.8845309925121082, 1.1767581530618922, 2.043007650989852, 1.9040439908730409, 0.7184325280354406, 0.2788393732173132, 2.190110971145463, 0.26008363951677277, 1.6465417910091869, 0.7956265189329343, 1.5798167843116482, 0.3874432468034648, 2.102332017778999, 0.6369855394677582, 0.3457316389607485, 0.8766713126267895, 1.345043267342335, 0.2122343186793932, 0.9130416013272422, 1.0526043812145551, 0.4866082593191429, 1.4131497964082074, 0.6872452579912067, 1.938276652323538, 1.3188081446760962, 2.111873511016113], 'lossList': [0.0, -1.2551689320802688, 0.0, 24.570661363601683, 0.0, 0.0, 0.0], 'rewardMean': 0.773147598978381, 'totalEpisodes': 236, 'stepsPerEpisode': 60, 'rewardPerEpisode': 55.10176378747139, 'successfulTests': 3
'totalSteps': 23040, 'rewardStep': 0.6190394321512651, 'errorList': [], 'lossList': [0.0, -1.2350439995527267, 0.0, 4.269749191403389, 0.0, 0.0, 0.0], 'rewardMean': 0.7537403352464593, 'totalEpisodes': 238, 'stepsPerEpisode': 99, 'rewardPerEpisode': 83.26909309949471
'totalSteps': 24320, 'rewardStep': 0.9558250164788801, 'errorList': [0.9703794722074409, 0.6234175385716891, 0.5673034955440522, 1.0007026335969527, 0.7444171181321382, 0.6795510519150932, 0.7315033481099128, 0.6971924229197257, 0.6091647621055847, 0.9951726822480067, 0.7832284506726933, 0.6734019839455865, 0.7855060715227504, 0.7393014284915795, 0.7443479270334435, 0.5700770229566379, 0.6885650989486105, 0.8032044383145034, 0.8644384767132951, 1.0072065379789634, 1.1901483235074062, 0.7275157748366672, 0.45717063083630227, 0.6313071116821924, 0.8818664542275902, 0.724295446321397, 0.6262176773583654, 1.04968177655701, 0.5522863457000144, 0.8740735838850043, 0.5520675457877517, 0.8133256503928732, 1.0112902636795518, 0.730171764289226, 0.880171211032175, 0.7377369151583183, 1.032981444043334, 0.7356479287205245, 0.7871617904651457, 0.7499220855865726, 0.9194895515141535, 0.5661867374570809, 0.896515408965469, 0.7391340932073397, 0.790176046740038, 0.6089792947235556, 0.517324894049302, 0.7753277140436119, 0.9935246939254304, 0.5804618867889083], 'lossList': [0.0, -1.2212168180942535, 0.0, 6.056985850334168, 0.0, 0.0, 0.0], 'rewardMean': 0.7617340552028915, 'totalEpisodes': 241, 'stepsPerEpisode': 76, 'rewardPerEpisode': 67.57287567152304, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.6600128340770626, 'errorList': [], 'lossList': [0.0, -1.2131314951181411, 0.0, 4.560206061005593, 0.0, 0.0, 0.0], 'rewardMean': 0.748048637425206, 'totalEpisodes': 242, 'stepsPerEpisode': 755, 'rewardPerEpisode': 614.0398635977969
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=21760, timeSpent=89.95
