#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 100
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7614810527792156, 'errorList': [], 'lossList': [0.0, -1.4163161396980286, 0.0, 34.041192936897275, 0.0, 0.0, 0.0], 'rewardMean': 0.6028518968773666, 'totalEpisodes': 54, 'stepsPerEpisode': 22, 'rewardPerEpisode': 14.045500787071914
'totalSteps': 3840, 'rewardStep': 0.24356484543378487, 'errorList': [], 'lossList': [0.0, -1.415860481262207, 0.0, 39.943852787017825, 0.0, 0.0, 0.0], 'rewardMean': 0.4830895463961727, 'totalEpisodes': 69, 'stepsPerEpisode': 95, 'rewardPerEpisode': 70.65840282816846
'totalSteps': 5120, 'rewardStep': 0.9072913863543064, 'errorList': [], 'lossList': [0.0, -1.4083302295207978, 0.0, 33.26576286315918, 0.0, 0.0, 0.0], 'rewardMean': 0.5891400063857062, 'totalEpisodes': 80, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.96015289930873
'totalSteps': 6400, 'rewardStep': 0.793186112380066, 'errorList': [], 'lossList': [0.0, -1.403409669995308, 0.0, 23.68816910982132, 0.0, 0.0, 0.0], 'rewardMean': 0.6299492275845782, 'totalEpisodes': 88, 'stepsPerEpisode': 205, 'rewardPerEpisode': 171.67943518341394
'totalSteps': 7680, 'rewardStep': 0.7952834547528785, 'errorList': [], 'lossList': [0.0, -1.395907492041588, 0.0, 18.818603127002717, 0.0, 0.0, 0.0], 'rewardMean': 0.6575049321126282, 'totalEpisodes': 93, 'stepsPerEpisode': 98, 'rewardPerEpisode': 69.83462475625392
'totalSteps': 8960, 'rewardStep': 0.8498687490239869, 'errorList': [], 'lossList': [0.0, -1.3897769594192504, 0.0, 25.52795403242111, 0.0, 0.0, 0.0], 'rewardMean': 0.6849854773856795, 'totalEpisodes': 96, 'stepsPerEpisode': 224, 'rewardPerEpisode': 182.52230143401576
'totalSteps': 10240, 'rewardStep': 0.6383907757637343, 'errorList': [], 'lossList': [0.0, -1.3988610625267028, 0.0, 248.6909481048584, 0.0, 0.0, 0.0], 'rewardMean': 0.6791611396829362, 'totalEpisodes': 126, 'stepsPerEpisode': 38, 'rewardPerEpisode': 23.764629025958097
'totalSteps': 11520, 'rewardStep': 0.85047918884773, 'errorList': [], 'lossList': [0.0, -1.3931381410360337, 0.0, 93.40025672912597, 0.0, 0.0, 0.0], 'rewardMean': 0.6981964784790244, 'totalEpisodes': 151, 'stepsPerEpisode': 71, 'rewardPerEpisode': 60.95676272313169
'totalSteps': 12800, 'rewardStep': 0.3597287058445499, 'errorList': [], 'lossList': [0.0, -1.3851854068040848, 0.0, 43.9524622631073, 0.0, 0.0, 0.0], 'rewardMean': 0.664349701215577, 'totalEpisodes': 166, 'stepsPerEpisode': 122, 'rewardPerEpisode': 84.56126965365355
'totalSteps': 14080, 'rewardStep': 0.600188366950453, 'errorList': [], 'lossList': [0.0, -1.3930822360515593, 0.0, 16.15274706363678, 0.0, 0.0, 0.0], 'rewardMean': 0.6799462638130706, 'totalEpisodes': 176, 'stepsPerEpisode': 213, 'rewardPerEpisode': 176.20492139236285
'totalSteps': 15360, 'rewardStep': 0.7727517143415799, 'errorList': [], 'lossList': [0.0, -1.3853766179084779, 0.0, 27.517107317447664, 0.0, 0.0, 0.0], 'rewardMean': 0.681073329969307, 'totalEpisodes': 182, 'stepsPerEpisode': 68, 'rewardPerEpisode': 62.621555925615404
'totalSteps': 16640, 'rewardStep': 0.6491647775120266, 'errorList': [], 'lossList': [0.0, -1.3862361073493958, 0.0, 14.08227998971939, 0.0, 0.0, 0.0], 'rewardMean': 0.7216333231771312, 'totalEpisodes': 185, 'stepsPerEpisode': 321, 'rewardPerEpisode': 238.9405374407457
'totalSteps': 17920, 'rewardStep': 0.8615473955677538, 'errorList': [], 'lossList': [0.0, -1.3955477839708328, 0.0, 7.63482967376709, 0.0, 0.0, 0.0], 'rewardMean': 0.7170589240984758, 'totalEpisodes': 188, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.13484051727594
'totalSteps': 19200, 'rewardStep': 0.9044359643327387, 'errorList': [], 'lossList': [0.0, -1.392979701757431, 0.0, 18.71314513206482, 0.0, 0.0, 0.0], 'rewardMean': 0.7281839092937431, 'totalEpisodes': 190, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.023732070788874
'totalSteps': 20480, 'rewardStep': 0.7354497003605464, 'errorList': [], 'lossList': [0.0, -1.3838011777400971, 0.0, 5.327028093338012, 0.0, 0.0, 0.0], 'rewardMean': 0.7222005338545099, 'totalEpisodes': 191, 'stepsPerEpisode': 462, 'rewardPerEpisode': 351.12633981589306
'totalSteps': 21760, 'rewardStep': 0.9358599191563184, 'errorList': [0.08876774801106116, 0.04964680146463783, 0.0663300873696423, 0.09686388580124125, 0.06804164495947854, 0.04624356662298988, 0.0751469184244615, 0.06294808916016216, 0.08712577357754911, 0.06350095440339364, 0.06359751432513244, 0.12151268117556004, 0.12630688974688145, 0.05395974984667131, 0.10035709418897297, 0.06555734992952571, 0.09379542350222198, 0.17815134088854703, 0.041968082425526354, 0.04358937601498287, 0.14740116011686907, 0.04573146966726598, 0.056836711539082097, 0.05518744903106986, 0.06447500035472928, 0.04351461443213112, 0.042289481770813556, 0.05635646944616534, 0.09086720262526722, 0.04243607971720565, 0.06365952961036753, 0.07330092477313659, 0.06696005015777205, 0.16128052718636662, 0.056957691864164606, 0.06319272876769824, 0.057255936481245044, 0.051824230718918625, 0.044876419929609626, 0.06205470977490999, 0.09185980175762463, 0.045842341126555504, 0.03956461916873019, 0.09538547873491918, 0.05370886467349775, 0.08714883379989577, 0.06648949663869749, 0.09008004527485085, 0.07823067317882088, 0.06947193654841033], 'lossList': [0.0, -1.355791481733322, 0.0, 2.748660875558853, 0.0, 0.0, 0.0], 'rewardMean': 0.7307996508677432, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1006.6831416590402, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=69.58
