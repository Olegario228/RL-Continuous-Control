#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 101
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.598075412276579, 'errorList': [], 'lossList': [0.0, -1.4175023859739304, 0.0, 30.411532626152038, 0.0, 0.0, 0.0], 'rewardMean': 0.7065627725589931, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.34344707830752
'totalSteps': 3840, 'rewardStep': 0.7333532821388857, 'errorList': [], 'lossList': [0.0, -1.4058793711662292, 0.0, 37.43542018413544, 0.0, 0.0, 0.0], 'rewardMean': 0.7154929424189573, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.244759399580825
'totalSteps': 5120, 'rewardStep': 0.7127671765657714, 'errorList': [], 'lossList': [0.0, -1.403301100730896, 0.0, 25.30396100282669, 0.0, 0.0, 0.0], 'rewardMean': 0.7148115009556608, 'totalEpisodes': 75, 'stepsPerEpisode': 36, 'rewardPerEpisode': 25.141514078795858
'totalSteps': 6400, 'rewardStep': 0.5384328808496843, 'errorList': [], 'lossList': [0.0, -1.3871454226970672, 0.0, 25.06265156507492, 0.0, 0.0, 0.0], 'rewardMean': 0.6795357769344654, 'totalEpisodes': 79, 'stepsPerEpisode': 82, 'rewardPerEpisode': 65.52214738380664
'totalSteps': 7680, 'rewardStep': 0.6396075669456545, 'errorList': [], 'lossList': [0.0, -1.3627315032482148, 0.0, 8.507291611731052, 0.0, 0.0, 0.0], 'rewardMean': 0.6728810752696637, 'totalEpisodes': 79, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 880.8373378013288
'totalSteps': 8960, 'rewardStep': 0.7434083957516187, 'errorList': [], 'lossList': [0.0, -1.344964611530304, 0.0, 18.034898616075516, 0.0, 0.0, 0.0], 'rewardMean': 0.6829564067670858, 'totalEpisodes': 80, 'stepsPerEpisode': 448, 'rewardPerEpisode': 351.85124119579916
'totalSteps': 10240, 'rewardStep': 0.7561719840891034, 'errorList': [], 'lossList': [0.0, -1.3311323010921479, 0.0, 107.97396346092223, 0.0, 0.0, 0.0], 'rewardMean': 0.692108353932338, 'totalEpisodes': 95, 'stepsPerEpisode': 157, 'rewardPerEpisode': 129.1999031625581
'totalSteps': 11520, 'rewardStep': 0.443533123558889, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6423933078576483, 'totalEpisodes': 109, 'stepsPerEpisode': 15, 'rewardPerEpisode': 9.129122709091394
'totalSteps': 12800, 'rewardStep': 0.3623670866507307, 'errorList': [], 'lossList': [0.0, -1.3313305526971817, 0.0, 82.96156993865966, 0.0, 0.0, 0.0], 'rewardMean': 0.5971250032385805, 'totalEpisodes': 126, 'stepsPerEpisode': 86, 'rewardPerEpisode': 66.03031518752252
'totalSteps': 14080, 'rewardStep': 0.8611339093639787, 'errorList': [], 'lossList': [0.0, -1.3343677479028702, 0.0, 22.698731706142425, 0.0, 0.0, 0.0], 'rewardMean': 0.6234308529473205, 'totalEpisodes': 135, 'stepsPerEpisode': 44, 'rewardPerEpisode': 36.9397084166605
'totalSteps': 15360, 'rewardStep': 0.8314816543973738, 'errorList': [], 'lossList': [0.0, -1.3379413920640946, 0.0, 16.36999683380127, 0.0, 0.0, 0.0], 'rewardMean': 0.6332436901731693, 'totalEpisodes': 138, 'stepsPerEpisode': 174, 'rewardPerEpisode': 147.39046171758727
'totalSteps': 16640, 'rewardStep': 0.6646545737461753, 'errorList': [], 'lossList': [0.0, -1.3193171316385268, 0.0, 11.659567725658416, 0.0, 0.0, 0.0], 'rewardMean': 0.6284324298912097, 'totalEpisodes': 142, 'stepsPerEpisode': 140, 'rewardPerEpisode': 103.44459393777353
'totalSteps': 17920, 'rewardStep': 0.45323525891299277, 'errorList': [], 'lossList': [0.0, -1.283157839179039, 0.0, 9.988314155340195, 0.0, 0.0, 0.0], 'rewardMean': 0.6199126676975405, 'totalEpisodes': 143, 'stepsPerEpisode': 927, 'rewardPerEpisode': 690.773579833228
'totalSteps': 19200, 'rewardStep': 0.7074039481500581, 'errorList': [], 'lossList': [0.0, -1.2469821435213089, 0.0, 7.70467229783535, 0.0, 0.0, 0.0], 'rewardMean': 0.626692305817981, 'totalEpisodes': 145, 'stepsPerEpisode': 630, 'rewardPerEpisode': 496.7795797995168
'totalSteps': 20480, 'rewardStep': 0.4911626944493718, 'errorList': [], 'lossList': [0.0, -1.2270152115821837, 0.0, 7.520648518800735, 0.0, 0.0, 0.0], 'rewardMean': 0.6014677356877562, 'totalEpisodes': 146, 'stepsPerEpisode': 379, 'rewardPerEpisode': 319.0423569449558
'totalSteps': 21760, 'rewardStep': 0.7306525834723422, 'errorList': [], 'lossList': [0.0, -1.2071505427360534, 0.0, 2.304758442044258, 0.0, 0.0, 0.0], 'rewardMean': 0.5989157956260802, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 980.9780207898481
'totalSteps': 23040, 'rewardStep': 0.8859878905349087, 'errorList': [], 'lossList': [0.0, -1.183627125620842, 0.0, 1.3727843677997589, 0.0, 0.0, 0.0], 'rewardMean': 0.6431612723236821, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.2313592878713
'totalSteps': 24320, 'rewardStep': 0.8151346556838734, 'errorList': [], 'lossList': [0.0, -1.1348128145933152, 0.0, 1.215639730542898, 0.0, 0.0, 0.0], 'rewardMean': 0.6803214255361805, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.5145100706093
'totalSteps': 25600, 'rewardStep': 0.9591281522726648, 'errorList': [0.07401997839860662, 0.0938810342938789, 0.11274368782611052, 0.12299798029428799, 0.0760087431677832, 0.1232609244549895, 0.12518942692267648, 0.08727171402587731, 0.10757685660871438, 0.11358476577496165, 0.09687609723793701, 0.0727622251674203, 0.07270187769358556, 0.10676731553744084, 0.08849674217099944, 0.10114580250805824, 0.13605999171567534, 0.09489952315386942, 0.12352163597091803, 0.10876958908914429, 0.10929088833065283, 0.09473462023144771, 0.0876555148318582, 0.11316656163098206, 0.0911858532504175, 0.11322182607569857, 0.09131579485940981, 0.0933108739015202, 0.13149786922540543, 0.0897920402013657, 0.10080350932072135, 0.11978946985371242, 0.08260931696183844, 0.07158744947124662, 0.07028553158590443, 0.09731300898214286, 0.11946411498534501, 0.10585095317528655, 0.10418108439014122, 0.10535685429021627, 0.1120233123536089, 0.09996635235085805, 0.08917487302284567, 0.11977251192143741, 0.09825133421894536, 0.11633249338583777, 0.08859078076907637, 0.10583953916812584, 0.09150563827795606, 0.0945247240848665], 'lossList': [0.0, -1.0586960428953172, 0.0, 1.1834526915103196, 0.0, 0.0, 0.0], 'rewardMean': 0.7399975320983739, 'totalEpisodes': 146, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1164.7258305905013, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=79.98
