#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 103
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.43587957502119606, 'errorList': [], 'lossList': [0.0, -1.434119564294815, 0.0, 31.66639961242676, 0.0, 0.0, 0.0], 'rewardMean': 0.4704094008807904, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.313282544931464
'totalSteps': 3840, 'rewardStep': 0.9561758358201343, 'errorList': [], 'lossList': [0.0, -1.426741201877594, 0.0, 33.82632946491241, 0.0, 0.0, 0.0], 'rewardMean': 0.6323315458605717, 'totalEpisodes': 72, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.208039708372695
'totalSteps': 5120, 'rewardStep': 0.7322379944428848, 'errorList': [], 'lossList': [0.0, -1.4205656784772873, 0.0, 41.522786378860474, 0.0, 0.0, 0.0], 'rewardMean': 0.6573081580061501, 'totalEpisodes': 79, 'stepsPerEpisode': 73, 'rewardPerEpisode': 56.752849680090826
'totalSteps': 6400, 'rewardStep': 0.6562317513693985, 'errorList': [], 'lossList': [0.0, -1.4083254438638688, 0.0, 47.78221901893616, 0.0, 0.0, 0.0], 'rewardMean': 0.6570928766787997, 'totalEpisodes': 87, 'stepsPerEpisode': 163, 'rewardPerEpisode': 128.67540522422152
'totalSteps': 7680, 'rewardStep': 0.6996084714077181, 'errorList': [], 'lossList': [0.0, -1.3962922340631485, 0.0, 54.8498184299469, 0.0, 0.0, 0.0], 'rewardMean': 0.6641788091336195, 'totalEpisodes': 94, 'stepsPerEpisode': 82, 'rewardPerEpisode': 62.07770627771748
'totalSteps': 8960, 'rewardStep': 0.9385762754895663, 'errorList': [], 'lossList': [0.0, -1.395541981458664, 0.0, 17.098985342979432, 0.0, 0.0, 0.0], 'rewardMean': 0.703378447184469, 'totalEpisodes': 100, 'stepsPerEpisode': 97, 'rewardPerEpisode': 80.45488167993027
'totalSteps': 10240, 'rewardStep': 0.439852908892174, 'errorList': [], 'lossList': [0.0, -1.3959727430343627, 0.0, 142.46797660827636, 0.0, 0.0, 0.0], 'rewardMean': 0.6704377548979321, 'totalEpisodes': 118, 'stepsPerEpisode': 61, 'rewardPerEpisode': 42.424031929972976
'totalSteps': 11520, 'rewardStep': 0.6892296518543636, 'errorList': [], 'lossList': [0.0, -1.4005263966321946, 0.0, 94.9840047454834, 0.0, 0.0, 0.0], 'rewardMean': 0.6725257434486467, 'totalEpisodes': 129, 'stepsPerEpisode': 37, 'rewardPerEpisode': 23.96459378606132
'totalSteps': 12800, 'rewardStep': 0.3893032359172861, 'errorList': [], 'lossList': [0.0, -1.4011669534444808, 0.0, 100.97118871688843, 0.0, 0.0, 0.0], 'rewardMean': 0.6442034926955106, 'totalEpisodes': 138, 'stepsPerEpisode': 176, 'rewardPerEpisode': 108.02140543759026
'totalSteps': 14080, 'rewardStep': 0.7120167665412378, 'errorList': [], 'lossList': [0.0, -1.3996799808740616, 0.0, 31.298714084625246, 0.0, 0.0, 0.0], 'rewardMean': 0.6649112466755959, 'totalEpisodes': 140, 'stepsPerEpisode': 200, 'rewardPerEpisode': 130.27956237085112
'totalSteps': 15360, 'rewardStep': 0.5272574064453174, 'errorList': [], 'lossList': [0.0, -1.389886365532875, 0.0, 12.967323951721191, 0.0, 0.0, 0.0], 'rewardMean': 0.674049029818008, 'totalEpisodes': 142, 'stepsPerEpisode': 515, 'rewardPerEpisode': 373.67358166728195
'totalSteps': 16640, 'rewardStep': 0.8559303461278317, 'errorList': [], 'lossList': [0.0, -1.3891040015220641, 0.0, 9.60653125166893, 0.0, 0.0, 0.0], 'rewardMean': 0.6640244808487779, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1004.6335643966456
'totalSteps': 17920, 'rewardStep': 0.9036560692577894, 'errorList': [], 'lossList': [0.0, -1.3700927299261094, 0.0, 5.74089589715004, 0.0, 0.0, 0.0], 'rewardMean': 0.6811662883302683, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 954.9984426763668
'totalSteps': 19200, 'rewardStep': 0.6164731160274436, 'errorList': [], 'lossList': [0.0, -1.3552844434976579, 0.0, 94.88398593425751, 0.0, 0.0, 0.0], 'rewardMean': 0.6771904247960728, 'totalEpisodes': 146, 'stepsPerEpisode': 431, 'rewardPerEpisode': 291.93397374260377
'totalSteps': 20480, 'rewardStep': 0.9563252522959904, 'errorList': [8.351883034295486, 1.8860447094154942, 14.234392979859129, 18.055509165571127, 4.995504351750784, 11.417347461337984, 15.986047062402971, 4.3449365621763, 9.883500206689087, 12.104828224517806, 15.419116636029937, 9.846364455879623, 0.6101143742980988, 23.087204868366513, 14.931634175892574, 20.75075069631122, 12.359506392374902, 1.195760615579473, 12.196292437743232, 7.416756881798034, 11.780059893162257, 9.138888410907633, 6.002138264980368, 17.755785821712514, 9.002912840620496, 18.614803371797784, 22.538265135135077, 3.4734891801904353, 20.876615197895084, 3.6738949665529805, 22.523605018801845, 30.188086287923497, 14.26536060138734, 26.731176245275773, 5.6873698388447576, 1.1957994208781693, 3.4493206634021476, 7.280262868378488, 17.48726346981739, 4.476880446193614, 6.2009204059465866, 14.215274563485808, 1.036145277127206, 2.564165547318014, 10.198740038719722, 12.971013272789767, 23.32196560433647, 2.8169638053596198, 8.824976383354016, 2.232106240622233], 'lossList': [0.0, -1.3536580735445023, 0.0, 78.60905336856842, 0.0, 0.0, 0.0], 'rewardMean': 0.7028621028849, 'totalEpisodes': 150, 'stepsPerEpisode': 377, 'rewardPerEpisode': 323.34953931266955, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8373717852192452, 'errorList': [], 'lossList': [0.0, -1.3718592822551727, 0.0, 81.90561227798462, 0.0, 0.0, 0.0], 'rewardMean': 0.6927416538578679, 'totalEpisodes': 153, 'stepsPerEpisode': 635, 'rewardPerEpisode': 527.5878433783528
'totalSteps': 23040, 'rewardStep': 0.9020970471305387, 'errorList': [], 'lossList': [0.0, -1.3952918148040772, 0.0, 104.73086585044861, 0.0, 0.0, 0.0], 'rewardMean': 0.7389660676817045, 'totalEpisodes': 157, 'stepsPerEpisode': 33, 'rewardPerEpisode': 28.20838702206329
'totalSteps': 24320, 'rewardStep': 0.5457199625557906, 'errorList': [], 'lossList': [0.0, -1.3992773485183716, 0.0, 84.68986009597778, 0.0, 0.0, 0.0], 'rewardMean': 0.7246150987518472, 'totalEpisodes': 161, 'stepsPerEpisode': 241, 'rewardPerEpisode': 202.2357415773954
'totalSteps': 25600, 'rewardStep': 0.9370959450251721, 'errorList': [0.5978684344973054, 2.5055234117276153, 2.363394491311141, 2.4833380814547663, 3.133564641504345, 4.3352789350231316, 1.1894164439292896, 3.5079937038501927, 2.3317245697486455, 1.4759914226285042, 1.5513999468684312, 2.331425199838865, 2.188425155490337, 2.628189058580869, 1.637808172572441, 0.4487510897997053, 2.7467600782361448, 1.931110575732413, 0.982129289161448, 1.8564466536708193, 1.828694528930215, 4.1204749327835835, 2.33614049488804, 1.8564817053808316, 1.669927407823489, 2.297236214119978, 2.0775004187081647, 2.15294310489409, 0.6421670488834635, 4.726259900291413, 2.3994960953576863, 1.2770446350248776, 0.46089069174386194, 1.6079766556443875, 1.9171028606286111, 2.536072584033855, 4.051095871502534, 1.9443718913565329, 3.387507906541747, 1.7253702402370161, 3.5489848034675835, 2.4927625374971485, 1.6917472837745982, 1.9129141464361534, 1.208099140562296, 2.5949826654821435, 3.1079467844843065, 4.096230060279858, 1.0546725609246799, 2.546742876630778], 'lossList': [0.0, -1.3934168010950088, 0.0, 103.98310169219971, 0.0, 0.0, 0.0], 'rewardMean': 0.7793943696626358, 'totalEpisodes': 166, 'stepsPerEpisode': 100, 'rewardPerEpisode': 89.53646995259848, 'successfulTests': 0
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=103.68
