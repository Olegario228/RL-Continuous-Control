#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 104
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764192896745447, 'errorList': [], 'lossList': [0.0, -1.4117753690481185, 0.0, 34.56666161537171, 0.0, 0.0, 0.0], 'rewardMean': 0.8487547119129759, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.540062801646165
'totalSteps': 3840, 'rewardStep': 0.8317068251637675, 'errorList': [], 'lossList': [0.0, -1.4151463109254836, 0.0, 39.13495526313782, 0.0, 0.0, 0.0], 'rewardMean': 0.8430720829965731, 'totalEpisodes': 84, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.84984210249653
'totalSteps': 5120, 'rewardStep': 0.7636500102047102, 'errorList': [], 'lossList': [0.0, -1.4112633401155472, 0.0, 26.801249465942384, 0.0, 0.0, 0.0], 'rewardMean': 0.8232165647986074, 'totalEpisodes': 89, 'stepsPerEpisode': 144, 'rewardPerEpisode': 114.34275258328417
'totalSteps': 6400, 'rewardStep': 0.6967816828410464, 'errorList': [], 'lossList': [0.0, -1.4141244727373123, 0.0, 26.594142162799834, 0.0, 0.0, 0.0], 'rewardMean': 0.7979295884070952, 'totalEpisodes': 93, 'stepsPerEpisode': 450, 'rewardPerEpisode': 319.25690171860384
'totalSteps': 7680, 'rewardStep': 0.14294782419090862, 'errorList': [], 'lossList': [0.0, -1.4028120815753937, 0.0, 19.871378676891325, 0.0, 0.0, 0.0], 'rewardMean': 0.6887659610377308, 'totalEpisodes': 94, 'stepsPerEpisode': 428, 'rewardPerEpisode': 268.0468679875096
'totalSteps': 8960, 'rewardStep': 0.8393433858495692, 'errorList': [], 'lossList': [0.0, -1.3849330401420594, 0.0, 62.35872266292572, 0.0, 0.0, 0.0], 'rewardMean': 0.7102770217251363, 'totalEpisodes': 100, 'stepsPerEpisode': 62, 'rewardPerEpisode': 48.717708359499
'totalSteps': 10240, 'rewardStep': 0.6246934078889396, 'errorList': [], 'lossList': [0.0, -1.388113888502121, 0.0, 184.93066703796387, 0.0, 0.0, 0.0], 'rewardMean': 0.6995790699956117, 'totalEpisodes': 125, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.567472632520037
'totalSteps': 11520, 'rewardStep': 0.9832383977152666, 'errorList': [168.25214849517903, 151.5072573861856, 215.95910543018968, 223.7136769922751, 117.14829403848724, 138.15641221919554, 159.3172687825662, 215.53429566499008, 201.62578238096955, 200.48827952178567, 82.58828811290397, 31.31803785963369, 74.84523820208783, 161.89388236633403, 162.6547069553039, 183.3916464213372, 39.510152308659926, 238.78265536723117, 196.65822484109393, 204.67528145344895, 225.25025311516848, 121.10180283265993, 189.82137928006884, 53.02628213638939, 191.87318114034974, 227.94855646653932, 40.43609676526977, 197.03218298577434, 141.11649116528665, 133.18385102767724, 157.17379711145907, 123.55223439911913, 149.78088970289156, 209.51836210131745, 95.55635216107972, 205.52812530111925, 228.82226896703955, 126.89801684332139, 173.42769518002166, 181.6062929021218, 174.5207968417127, 132.05798366029555, 131.70373495247796, 41.88501155663813, 71.30797806496783, 41.54695094476402, 173.0456792345516, 136.44860360910147, 158.57615550870256, 149.5855074442414], 'lossList': [0.0, -1.3901206064224243, 0.0, 115.04220861434936, 0.0, 0.0, 0.0], 'rewardMean': 0.7310967730755734, 'totalEpisodes': 146, 'stepsPerEpisode': 29, 'rewardPerEpisode': 26.539855728077022, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.4091406661498617, 'errorList': [], 'lossList': [0.0, -1.3782925575971603, 0.0, 66.7167600440979, 0.0, 0.0, 0.0], 'rewardMean': 0.6989011623830022, 'totalEpisodes': 159, 'stepsPerEpisode': 63, 'rewardPerEpisode': 41.0250084896884
'totalSteps': 14080, 'rewardStep': 0.7223485028002283, 'errorList': [], 'lossList': [0.0, -1.35765749335289, 0.0, 29.047606182098388, 0.0, 0.0, 0.0], 'rewardMean': 0.6790269992478842, 'totalEpisodes': 168, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.933241467684006
'totalSteps': 15360, 'rewardStep': 0.3870232848313337, 'errorList': [], 'lossList': [0.0, -1.3476410347223282, 0.0, 11.424374144077301, 0.0, 0.0, 0.0], 'rewardMean': 0.6400873987635631, 'totalEpisodes': 175, 'stepsPerEpisode': 123, 'rewardPerEpisode': 87.46372407704557
'totalSteps': 16640, 'rewardStep': 0.6576102386865648, 'errorList': [], 'lossList': [0.0, -1.3563017708063125, 0.0, 9.648214802742004, 0.0, 0.0, 0.0], 'rewardMean': 0.622677740115843, 'totalEpisodes': 181, 'stepsPerEpisode': 61, 'rewardPerEpisode': 49.76933271807969
'totalSteps': 17920, 'rewardStep': 0.7497727910066251, 'errorList': [], 'lossList': [0.0, -1.3905222302675246, 0.0, 7.228789380788803, 0.0, 0.0, 0.0], 'rewardMean': 0.6212900181960344, 'totalEpisodes': 185, 'stepsPerEpisode': 362, 'rewardPerEpisode': 285.74346900104103
'totalSteps': 19200, 'rewardStep': 0.4257799591921493, 'errorList': [], 'lossList': [0.0, -1.3888502269983292, 0.0, 12.94142204284668, 0.0, 0.0, 0.0], 'rewardMean': 0.5941898458311446, 'totalEpisodes': 189, 'stepsPerEpisode': 448, 'rewardPerEpisode': 345.6938479393722
'totalSteps': 20480, 'rewardStep': 0.5481397747884853, 'errorList': [], 'lossList': [0.0, -1.3909820765256882, 0.0, 5.006006801724434, 0.0, 0.0, 0.0], 'rewardMean': 0.6347090408909024, 'totalEpisodes': 192, 'stepsPerEpisode': 478, 'rewardPerEpisode': 363.2544862226548
'totalSteps': 21760, 'rewardStep': 0.8289813379595313, 'errorList': [], 'lossList': [0.0, -1.4005515086650848, 0.0, 4.56027436375618, 0.0, 0.0, 0.0], 'rewardMean': 0.6336728361018986, 'totalEpisodes': 193, 'stepsPerEpisode': 1039, 'rewardPerEpisode': 789.6339943663927
'totalSteps': 23040, 'rewardStep': 0.7915545616477216, 'errorList': [], 'lossList': [0.0, -1.391344639658928, 0.0, 2.9595523178577423, 0.0, 0.0, 0.0], 'rewardMean': 0.6503589514777768, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.047483320951
'totalSteps': 24320, 'rewardStep': 0.7668962644036179, 'errorList': [], 'lossList': [0.0, -1.369769313931465, 0.0, 1.400769546702504, 0.0, 0.0, 0.0], 'rewardMean': 0.628724738146612, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.7712331682155
'totalSteps': 25600, 'rewardStep': 0.9791503794262565, 'errorList': [0.05667664066278224, 0.08191062119540128, 0.05650248441999615, 0.05780821461875328, 0.056120836699540516, 0.08534710201295924, 0.056112245929606175, 0.05577517153162935, 0.08319942065547015, 0.0572618653408945, 0.05559278394657026, 0.05724187313707274, 0.08985898599886333, 0.08302688556970003, 0.06621901891174965, 0.0699593483534253, 0.07057641486973898, 0.08188489824791384, 0.09634406619837611, 0.05822994470108412, 0.10561099125847367, 0.05648511307842064, 0.05563266530877164, 0.0573293991862469, 0.056847545738491975, 0.0557558602403031, 0.08308574465154067, 0.08523135588464209, 0.05646582521695247, 0.07003065698664382, 0.056042488349853044, 0.05785963336813078, 0.056543196431374586, 0.05680246407231792, 0.057888473334929726, 0.1069547793087345, 0.05585104630353667, 0.05575374223128663, 0.05613007498593989, 0.05638138089476173, 0.09173425679454353, 0.060770217597075335, 0.057197108860588014, 0.05667993694009518, 0.05666971954347314, 0.05462447140397768, 0.05464241449357194, 0.05600434431974951, 0.05576939684801775, 0.05582697802823826], 'lossList': [0.0, -1.326812312602997, 0.0, 1.3709516922384501, 0.0, 0.0, 0.0], 'rewardMean': 0.6857257094742514, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.4706780512545, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=100.69
