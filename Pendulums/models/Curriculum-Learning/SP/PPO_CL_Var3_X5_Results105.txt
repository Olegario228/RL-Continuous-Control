#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 105
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.875520819905225, 'errorList': [], 'lossList': [0.0, -1.398583208322525, 0.0, 28.81631910562515, 0.0, 0.0, 0.0], 'rewardMean': 0.7430954594235377, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.951802155377276
'totalSteps': 3840, 'rewardStep': 0.7237240385658629, 'errorList': [], 'lossList': [0.0, -1.3883249735832215, 0.0, 30.585794742107392, 0.0, 0.0, 0.0], 'rewardMean': 0.7366383191376461, 'totalEpisodes': 26, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.3816257868936
'totalSteps': 5120, 'rewardStep': 0.7037518713918485, 'errorList': [], 'lossList': [0.0, -1.3992202383279801, 0.0, 16.336507117748262, 0.0, 0.0, 0.0], 'rewardMean': 0.7284167072011967, 'totalEpisodes': 27, 'stepsPerEpisode': 124, 'rewardPerEpisode': 103.80075390419094
'totalSteps': 6400, 'rewardStep': 0.9493639285804231, 'errorList': [], 'lossList': [0.0, -1.409991284608841, 0.0, 15.682801906466484, 0.0, 0.0, 0.0], 'rewardMean': 0.7726061514770419, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 945.8405222698544
'totalSteps': 7680, 'rewardStep': 0.7369351730398171, 'errorList': [], 'lossList': [0.0, -1.395590009689331, 0.0, 24.357751816511154, 0.0, 0.0, 0.0], 'rewardMean': 0.7666609884041712, 'totalEpisodes': 28, 'stepsPerEpisode': 418, 'rewardPerEpisode': 286.2647003252295
'totalSteps': 8960, 'rewardStep': 0.7352134510848137, 'errorList': [], 'lossList': [0.0, -1.3697216683626174, 0.0, 43.19227549076081, 0.0, 0.0, 0.0], 'rewardMean': 0.7621684830728344, 'totalEpisodes': 31, 'stepsPerEpisode': 484, 'rewardPerEpisode': 334.54669231248636
'totalSteps': 10240, 'rewardStep': 0.9181006798523603, 'errorList': [], 'lossList': [0.0, -1.3701350861787795, 0.0, 388.1716426849365, 0.0, 0.0, 0.0], 'rewardMean': 0.7816600076702751, 'totalEpisodes': 69, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.768157727263115
'totalSteps': 11520, 'rewardStep': 0.6985909431082158, 'errorList': [], 'lossList': [0.0, -1.3673734378814697, 0.0, 257.61958381652835, 0.0, 0.0, 0.0], 'rewardMean': 0.772430111607824, 'totalEpisodes': 109, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4278822143608707
'totalSteps': 12800, 'rewardStep': 0.6461524656744835, 'errorList': [], 'lossList': [0.0, -1.3679215490818024, 0.0, 132.7123296356201, 0.0, 0.0, 0.0], 'rewardMean': 0.75980234701449, 'totalEpisodes': 139, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2736235239021916
'totalSteps': 14080, 'rewardStep': 0.699424473507146, 'errorList': [], 'lossList': [0.0, -1.3697962820529939, 0.0, 78.23535392761231, 0.0, 0.0, 0.0], 'rewardMean': 0.7686777844710196, 'totalEpisodes': 162, 'stepsPerEpisode': 62, 'rewardPerEpisode': 53.26328254732249
'totalSteps': 15360, 'rewardStep': 0.763893042354518, 'errorList': [], 'lossList': [0.0, -1.362713131904602, 0.0, 47.05168755531311, 0.0, 0.0, 0.0], 'rewardMean': 0.757515006715949, 'totalEpisodes': 176, 'stepsPerEpisode': 67, 'rewardPerEpisode': 44.42828242013658
'totalSteps': 16640, 'rewardStep': 0.8169872150863847, 'errorList': [], 'lossList': [0.0, -1.3613550007343291, 0.0, 51.937497234344484, 0.0, 0.0, 0.0], 'rewardMean': 0.766841324368001, 'totalEpisodes': 189, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.1064445020944365
'totalSteps': 17920, 'rewardStep': 0.9451977079911414, 'errorList': [41.689005196734406, 56.27505311474766, 77.33160226206583, 57.633150091779356, 32.91908074673053, 68.77275353495365, 93.70368247601051, 30.016980768605528, 26.209926468371723, 16.051450638729268, 9.671899657043046, 87.51749711765886, 70.9195755385848, 44.94727546668788, 88.34003082934015, 53.923595066201386, 103.47184724952314, 60.15125560432698, 23.684819559672878, 84.08270280614154, 17.179734813518216, 22.140722339091507, 97.75606617481938, 86.44690289404312, 90.49323905369268, 20.799559977812912, 14.052739941612133, 17.611713893968034, 8.752756752533939, 95.51821861370038, 43.38470106205978, 82.29546204712767, 74.55247086134193, 88.3151739735864, 35.601019505611106, 95.79492730325728, 58.14843033332951, 74.75860290814384, 48.469933222824494, 21.435257864939192, 57.50051950097276, 132.74940927805628, 51.4579826592499, 44.490540764319064, 94.50721582942585, 87.65102253707019, 54.74815849706603, 32.012099208905965, 74.0411169414831, 43.79991924096718], 'lossList': [0.0, -1.3614561796188354, 0.0, 35.14198092460632, 0.0, 0.0, 0.0], 'rewardMean': 0.7909859080279303, 'totalEpisodes': 197, 'stepsPerEpisode': 156, 'rewardPerEpisode': 132.67049590064212, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.7598204991361919, 'errorList': [], 'lossList': [0.0, -1.357945475578308, 0.0, 33.99017956256866, 0.0, 0.0, 0.0], 'rewardMean': 0.7720315650835072, 'totalEpisodes': 205, 'stepsPerEpisode': 49, 'rewardPerEpisode': 36.291150019689745
'totalSteps': 20480, 'rewardStep': 0.4705264820051137, 'errorList': [], 'lossList': [0.0, -1.3652500867843629, 0.0, 11.744562441110611, 0.0, 0.0, 0.0], 'rewardMean': 0.7453906959800369, 'totalEpisodes': 210, 'stepsPerEpisode': 372, 'rewardPerEpisode': 299.90654443092706
'totalSteps': 21760, 'rewardStep': 0.9622354853744631, 'errorList': [7.146448540924665, 12.967138731211284, 11.1315815479051, 9.11648127941742, 14.772072263693735, 21.16936148581403, 5.613756299792491, 6.155565229690259, 16.793604272772242, 21.205424487239714, 13.662528376427906, 10.881242421349581, 27.765555978529882, 0.8484748684492986, 16.568959842812635, 15.72041115359748, 23.26979697945292, 9.395085560305201, 16.20335757175765, 6.956841294941991, 4.278182759767713, 1.2458997372008196, 19.511734334405947, 5.821734973169133, 11.411356273750759, 29.20265509649243, 1.1064463562280131, 3.0638565596245315, 7.486289022505612, 18.633337271173698, 10.733625807312478, 19.73806224080076, 7.297133985156471, 4.13756392727427, 9.117628509546226, 21.22080077630455, 8.416749447964811, 16.84116853929289, 15.636131022055906, 19.026098150202223, 11.751263341367565, 9.829279781023079, 23.15739070212792, 19.6626981834057, 17.96614389159312, 6.560078667249425, 7.554214820517723, 16.109767366710305, 24.055050265913856, 16.98069429166499], 'lossList': [0.0, -1.3715717005729675, 0.0, 21.944137086868285, 0.0, 0.0, 0.0], 'rewardMean': 0.7680928994090019, 'totalEpisodes': 217, 'stepsPerEpisode': 62, 'rewardPerEpisode': 53.97594650620086, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.6111806198066669, 'errorList': [], 'lossList': [0.0, -1.3633211010694504, 0.0, 7.393504360318184, 0.0, 0.0, 0.0], 'rewardMean': 0.7374008934044325, 'totalEpisodes': 220, 'stepsPerEpisode': 236, 'rewardPerEpisode': 198.06501956279897
'totalSteps': 24320, 'rewardStep': 0.9196336572818703, 'errorList': [], 'lossList': [0.0, -1.3519996815919877, 0.0, 5.186213403940201, 0.0, 0.0, 0.0], 'rewardMean': 0.759505164821798, 'totalEpisodes': 225, 'stepsPerEpisode': 60, 'rewardPerEpisode': 46.6514172374593
'totalSteps': 25600, 'rewardStep': 0.6477501537517196, 'errorList': [], 'lossList': [0.0, -1.3445593470335007, 0.0, 4.842148498296738, 0.0, 0.0, 0.0], 'rewardMean': 0.7596649336295215, 'totalEpisodes': 228, 'stepsPerEpisode': 204, 'rewardPerEpisode': 160.97286600763042
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=103.93
