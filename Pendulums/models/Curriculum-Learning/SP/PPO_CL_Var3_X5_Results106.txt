#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 106
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6323988190547443, 'errorList': [], 'lossList': [0.0, -1.422237776517868, 0.0, 26.87902349948883, 0.0, 0.0, 0.0], 'rewardMean': 0.797844185482727, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.80808263927176
'totalSteps': 3840, 'rewardStep': 0.840065708029194, 'errorList': [], 'lossList': [0.0, -1.4243863356113433, 0.0, 22.452271625995635, 0.0, 0.0, 0.0], 'rewardMean': 0.8119180263315493, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 255.32057663252021
'totalSteps': 5120, 'rewardStep': 0.6863747154750369, 'errorList': [], 'lossList': [0.0, -1.4260586059093476, 0.0, 15.658135797977447, 0.0, 0.0, 0.0], 'rewardMean': 0.7805321986174212, 'totalEpisodes': 23, 'stepsPerEpisode': 72, 'rewardPerEpisode': 50.34466536293242
'totalSteps': 6400, 'rewardStep': 0.654470664220495, 'errorList': [], 'lossList': [0.0, -1.4211655533313752, 0.0, 24.404052567481994, 0.0, 0.0, 0.0], 'rewardMean': 0.7553198917380359, 'totalEpisodes': 25, 'stepsPerEpisode': 303, 'rewardPerEpisode': 183.36080646051315
'totalSteps': 7680, 'rewardStep': 0.5651337951159152, 'errorList': [], 'lossList': [0.0, -1.403168374300003, 0.0, 9.015460076332092, 0.0, 0.0, 0.0], 'rewardMean': 0.7236222089676825, 'totalEpisodes': 25, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 929.1098474054697
'totalSteps': 8960, 'rewardStep': 0.9562865770045927, 'errorList': [], 'lossList': [0.0, -1.3856587308645247, 0.0, 24.442399940490724, 0.0, 0.0, 0.0], 'rewardMean': 0.7568599758300982, 'totalEpisodes': 26, 'stepsPerEpisode': 447, 'rewardPerEpisode': 372.74593492804115
'totalSteps': 10240, 'rewardStep': 0.8207226926938357, 'errorList': [], 'lossList': [0.0, -1.3748965859413147, 0.0, 313.3003338623047, 0.0, 0.0, 0.0], 'rewardMean': 0.7648428154380654, 'totalEpisodes': 56, 'stepsPerEpisode': 100, 'rewardPerEpisode': 84.65929980268552
'totalSteps': 11520, 'rewardStep': 0.6985803827588138, 'errorList': [], 'lossList': [0.0, -1.373176752924919, 0.0, 115.09990509033203, 0.0, 0.0, 0.0], 'rewardMean': 0.7574803229181486, 'totalEpisodes': 82, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.3685678627001492
'totalSteps': 12800, 'rewardStep': 0.5926782974364238, 'errorList': [], 'lossList': [0.0, -1.3716241371631623, 0.0, 50.85946746826172, 0.0, 0.0, 0.0], 'rewardMean': 0.7410001203699761, 'totalEpisodes': 100, 'stepsPerEpisode': 74, 'rewardPerEpisode': 57.788550536755054
'totalSteps': 14080, 'rewardStep': 0.18451552046218478, 'errorList': [], 'lossList': [0.0, -1.3676520669460297, 0.0, 35.00296977043152, 0.0, 0.0, 0.0], 'rewardMean': 0.6631227172251236, 'totalEpisodes': 114, 'stepsPerEpisode': 76, 'rewardPerEpisode': 45.27030052957489
'totalSteps': 15360, 'rewardStep': 0.46622323337100646, 'errorList': [], 'lossList': [0.0, -1.356974738240242, 0.0, 27.76491011619568, 0.0, 0.0, 0.0], 'rewardMean': 0.6465051586567498, 'totalEpisodes': 121, 'stepsPerEpisode': 173, 'rewardPerEpisode': 126.18582387646771
'totalSteps': 16640, 'rewardStep': 0.9308588037517578, 'errorList': [5.373665898882497, 6.304904941761533, 6.532104532930118, 6.496963247161558, 3.617505947638602, 5.066833103745933, 6.523218669969086, 6.20650227315848, 5.765709087705272, 5.982598160951543, 6.460933757775225, 6.331067077280253, 4.814579536901142, 5.73644952705666, 6.24965777477424, 4.382066365726036, 5.235648115903617, 3.974210286228257, 4.41968410848265, 3.069933297166468, 6.150113098749242, 3.6833894545232626, 5.579332404140873, 4.09827775487375, 4.894364193019443, 4.84237851967453, 6.4455786487015825, 3.945019000191666, 4.03499373571792, 5.335998707762438, 4.8518880280441135, 4.035645370348329, 6.333492819993381, 3.9566414478224976, 5.224477415311243, 5.817389937103153, 5.2180703332256, 5.680764799185596, 4.681194605434834, 5.307195040389557, 4.7379616218204434, 5.800001242887207, 6.102112570093763, 3.6693896832136303, 4.904861609125911, 6.2522994952281445, 6.443100677256454, 5.658345515448639, 4.991086652296978, 6.573066369833652], 'lossList': [0.0, -1.353236802816391, 0.0, 24.45894295454025, 0.0, 0.0, 0.0], 'rewardMean': 0.6555844682290063, 'totalEpisodes': 127, 'stepsPerEpisode': 70, 'rewardPerEpisode': 64.33753052624198, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7499240413355795, 'errorList': [], 'lossList': [0.0, -1.35327390730381, 0.0, 7.16481991648674, 0.0, 0.0, 0.0], 'rewardMean': 0.6619394008150604, 'totalEpisodes': 129, 'stepsPerEpisode': 117, 'rewardPerEpisode': 97.80171493089503
'totalSteps': 19200, 'rewardStep': 0.6484339476161132, 'errorList': [], 'lossList': [0.0, -1.334183485507965, 0.0, 16.072464458942413, 0.0, 0.0, 0.0], 'rewardMean': 0.6613357291546222, 'totalEpisodes': 130, 'stepsPerEpisode': 190, 'rewardPerEpisode': 126.26659788415407
'totalSteps': 20480, 'rewardStep': 0.6743326492911614, 'errorList': [], 'lossList': [0.0, -1.3079445564746857, 0.0, 5.950677196979523, 0.0, 0.0, 0.0], 'rewardMean': 0.672255614572147, 'totalEpisodes': 133, 'stepsPerEpisode': 63, 'rewardPerEpisode': 54.07325800632187
'totalSteps': 21760, 'rewardStep': 0.6924437788918683, 'errorList': [], 'lossList': [0.0, -1.2680058598518371, 0.0, 20.10426176071167, 0.0, 0.0, 0.0], 'rewardMean': 0.6458713347608744, 'totalEpisodes': 134, 'stepsPerEpisode': 820, 'rewardPerEpisode': 556.0239422056039
'totalSteps': 23040, 'rewardStep': 0.8137683099399162, 'errorList': [], 'lossList': [0.0, -1.2384815800189972, 0.0, 4.097780393958092, 0.0, 0.0, 0.0], 'rewardMean': 0.6451758964854826, 'totalEpisodes': 134, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.8004456501492
'totalSteps': 24320, 'rewardStep': 0.6414883359146976, 'errorList': [], 'lossList': [0.0, -1.2212670528888703, 0.0, 1.773331478536129, 0.0, 0.0, 0.0], 'rewardMean': 0.639466691801071, 'totalEpisodes': 134, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.4148515235314
'totalSteps': 25600, 'rewardStep': 0.9552779132559285, 'errorList': [0.19157911951982956, 0.1943773736913972, 0.29816801554276356, 0.32948027816225783, 0.16820578204665598, 0.24736982301856045, 0.1895880606025407, 0.2034662182128889, 0.3213875320705217, 0.207534548881945, 0.26351226978800313, 0.1789659251138048, 0.2127108442629913, 0.25058294508002715, 0.18833626066413353, 0.18768882098077905, 0.21531491429870553, 0.16459262593416485, 0.27565142743958143, 0.29005256665805207, 0.24329343773718268, 0.1664005550880245, 0.2460892658067949, 0.17807055543872866, 0.17055100067167864, 0.3236178577525409, 0.23170494686015303, 0.2011896855924929, 0.18775221062028538, 0.1824088870867626, 0.19375198323472906, 0.19743199329469371, 0.1998505415658894, 0.19718197803608717, 0.18647614966880474, 0.18631849713892293, 0.33176832145563107, 0.18056522081223692, 0.19802572317347836, 0.16375493891712867, 0.22114506381714788, 0.18615022643826237, 0.19187856087482782, 0.16695544058349016, 0.17679892823943358, 0.24232366140641895, 0.19248657261677138, 0.18181945080595088, 0.31660639105404514, 0.19506574153721506], 'lossList': [0.0, -1.172635805606842, 0.0, 2.3153472467511893, 0.0, 0.0, 0.0], 'rewardMean': 0.6757266533830213, 'totalEpisodes': 134, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.3704215401247, 'successfulTests': 29
#maxSuccessfulTests=29, maxSuccessfulTestsAtStep=25600, timeSpent=102.59
