#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 107
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.60242322831525, 'errorList': [], 'lossList': [0.0, -1.4165353220701218, 0.0, 23.93462346315384, 0.0, 0.0, 0.0], 'rewardMean': 0.6980966147477006, 'totalEpisodes': 20, 'stepsPerEpisode': 134, 'rewardPerEpisode': 81.2898275658927
'totalSteps': 3840, 'rewardStep': 0.8525249573596347, 'errorList': [], 'lossList': [0.0, -1.4049951869249344, 0.0, 27.289225111007692, 0.0, 0.0, 0.0], 'rewardMean': 0.7495727289516786, 'totalEpisodes': 24, 'stepsPerEpisode': 75, 'rewardPerEpisode': 59.10431066216104
'totalSteps': 5120, 'rewardStep': 0.9156940693554986, 'errorList': [], 'lossList': [0.0, -1.3851358890533447, 0.0, 16.895231511592865, 0.0, 0.0, 0.0], 'rewardMean': 0.7911030640526335, 'totalEpisodes': 29, 'stepsPerEpisode': 27, 'rewardPerEpisode': 24.392462231769034
'totalSteps': 6400, 'rewardStep': 0.14650677889494773, 'errorList': [], 'lossList': [0.0, -1.3776165944337846, 0.0, 24.726573836803436, 0.0, 0.0, 0.0], 'rewardMean': 0.6621838070210964, 'totalEpisodes': 30, 'stepsPerEpisode': 573, 'rewardPerEpisode': 391.6532406341893
'totalSteps': 7680, 'rewardStep': 0.6299187487616622, 'errorList': [], 'lossList': [0.0, -1.3658322900533677, 0.0, 15.043934671878814, 0.0, 0.0, 0.0], 'rewardMean': 0.6568062973111907, 'totalEpisodes': 31, 'stepsPerEpisode': 816, 'rewardPerEpisode': 640.1481010668284
'totalSteps': 8960, 'rewardStep': 0.7455566565085694, 'errorList': [], 'lossList': [0.0, -1.3481132471561432, 0.0, 20.629133480787278, 0.0, 0.0, 0.0], 'rewardMean': 0.6694849200536732, 'totalEpisodes': 32, 'stepsPerEpisode': 359, 'rewardPerEpisode': 238.35749168813084
'totalSteps': 10240, 'rewardStep': 0.7937154646776464, 'errorList': [], 'lossList': [0.0, -1.3448009473085403, 0.0, 283.27086673736574, 0.0, 0.0, 0.0], 'rewardMean': 0.6850137381316701, 'totalEpisodes': 56, 'stepsPerEpisode': 80, 'rewardPerEpisode': 67.96097940314708
'totalSteps': 11520, 'rewardStep': 0.7664954554804636, 'errorList': [], 'lossList': [0.0, -1.3435272198915482, 0.0, 188.53982864379884, 0.0, 0.0, 0.0], 'rewardMean': 0.694067262281536, 'totalEpisodes': 89, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.94873065597545
'totalSteps': 12800, 'rewardStep': 0.8814573469702308, 'errorList': [], 'lossList': [0.0, -1.3421739906072616, 0.0, 57.33383111000061, 0.0, 0.0, 0.0], 'rewardMean': 0.7128062707504055, 'totalEpisodes': 111, 'stepsPerEpisode': 46, 'rewardPerEpisode': 43.016959425082554
'totalSteps': 14080, 'rewardStep': 0.5689796170467601, 'errorList': [], 'lossList': [0.0, -1.3401496011018752, 0.0, 52.19845404624939, 0.0, 0.0, 0.0], 'rewardMean': 0.6903272323370663, 'totalEpisodes': 144, 'stepsPerEpisode': 70, 'rewardPerEpisode': 49.540082715096815
'totalSteps': 15360, 'rewardStep': 0.9482143976020458, 'errorList': [116.77244747087279, 187.62491510894807, 221.07362942248244, 273.0173198728391, 226.4345117051656, 108.09444348424536, 288.72846761541973, 71.47633941318348, 191.28611878142905, 291.5946778435848, 276.42940622756606, 286.2772483977148, 275.27047347494045, 280.305724265345, 214.2627433349062, 241.6364961257115, 234.24617679545892, 190.0008945384515, 231.35281028044596, 260.06477520181147, 172.92889744503339, 304.82628543493587, 151.8968198383666, 269.08104565057744, 220.30179309086097, 185.5960535950584, 221.51230535709706, 285.8499213297724, 248.53283546282645, 193.26636511689756, 179.33475926721184, 279.9809386545278, 194.51440789309024, 266.28885134947495, 185.41085949517083, 221.41497509955943, 272.6427602627299, 282.92460295118076, 290.0323812691071, 263.7269995660826, 70.04431280538051, 283.5104658598319, 270.98359481288776, 289.68586944881685, 290.6653726062957, 225.42969822874997, 140.66592721045555, 242.36296217912064, 213.00893951508522, 278.3294597882809], 'lossList': [0.0, -1.3419707131385803, 0.0, 34.141954164505, 0.0, 0.0, 0.0], 'rewardMean': 0.7249063492657458, 'totalEpisodes': 166, 'stepsPerEpisode': 39, 'rewardPerEpisode': 36.235560199182984, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7354579664913969, 'errorList': [], 'lossList': [0.0, -1.3355144035816193, 0.0, 18.433889894485475, 0.0, 0.0, 0.0], 'rewardMean': 0.7131996501789221, 'totalEpisodes': 179, 'stepsPerEpisode': 55, 'rewardPerEpisode': 40.15056629408319
'totalSteps': 17920, 'rewardStep': 0.8787362426149172, 'errorList': [], 'lossList': [0.0, -1.3334952366352082, 0.0, 10.688999202251434, 0.0, 0.0, 0.0], 'rewardMean': 0.709503867504864, 'totalEpisodes': 189, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.59615758218299
'totalSteps': 19200, 'rewardStep': 0.888382508622948, 'errorList': [], 'lossList': [0.0, -1.3239894753694534, 0.0, 10.96807225227356, 0.0, 0.0, 0.0], 'rewardMean': 0.783691440477664, 'totalEpisodes': 193, 'stepsPerEpisode': 93, 'rewardPerEpisode': 83.40827207775618
'totalSteps': 20480, 'rewardStep': 0.8500414288009112, 'errorList': [], 'lossList': [0.0, -1.3219636422395706, 0.0, 11.988277488350867, 0.0, 0.0, 0.0], 'rewardMean': 0.8057037084815889, 'totalEpisodes': 199, 'stepsPerEpisode': 31, 'rewardPerEpisode': 25.752793567663904
'totalSteps': 21760, 'rewardStep': 0.7894648149156899, 'errorList': [], 'lossList': [0.0, -1.3166606241464616, 0.0, 8.3108518576622, 0.0, 0.0, 0.0], 'rewardMean': 0.8100945243223009, 'totalEpisodes': 202, 'stepsPerEpisode': 67, 'rewardPerEpisode': 52.58002356571117
'totalSteps': 23040, 'rewardStep': 0.8932127164639336, 'errorList': [], 'lossList': [0.0, -1.3108704257011414, 0.0, 4.839082443118095, 0.0, 0.0, 0.0], 'rewardMean': 0.8200442495009298, 'totalEpisodes': 206, 'stepsPerEpisode': 59, 'rewardPerEpisode': 50.306116120020484
'totalSteps': 24320, 'rewardStep': 0.870788339212141, 'errorList': [], 'lossList': [0.0, -1.3118858444690704, 0.0, 4.728505921363831, 0.0, 0.0, 0.0], 'rewardMean': 0.8304735378740974, 'totalEpisodes': 207, 'stepsPerEpisode': 746, 'rewardPerEpisode': 617.4893734388779
'totalSteps': 25600, 'rewardStep': 0.5172941376269191, 'errorList': [], 'lossList': [0.0, -1.293285801410675, 0.0, 3.068282461166382, 0.0, 0.0, 0.0], 'rewardMean': 0.7940572169397664, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 984.292953156244
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.87
