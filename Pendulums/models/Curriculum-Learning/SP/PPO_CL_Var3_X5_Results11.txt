#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 11
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7406241708983295, 'errorList': [], 'lossList': [0.0, -1.4381530529260635, 0.0, 33.294612147808074, 0.0, 0.0, 0.0], 'rewardMean': 0.6287090362988912, 'totalEpisodes': 12, 'stepsPerEpisode': 75, 'rewardPerEpisode': 63.57640562340732
'totalSteps': 3840, 'rewardStep': 0.8639467461250414, 'errorList': [], 'lossList': [0.0, -1.445116308927536, 0.0, 27.14651432991028, 0.0, 0.0, 0.0], 'rewardMean': 0.7071216062409412, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 975.1189563629035
'totalSteps': 5120, 'rewardStep': 0.5913682806262659, 'errorList': [], 'lossList': [0.0, -1.4191674709320068, 0.0, 21.802579880356788, 0.0, 0.0, 0.0], 'rewardMean': 0.6781832748372724, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 967.1239111780103
'totalSteps': 6400, 'rewardStep': 0.6769748088812879, 'errorList': [], 'lossList': [0.0, -1.415907184481621, 0.0, 261.6445718383789, 0.0, 0.0, 0.0], 'rewardMean': 0.6779415816460755, 'totalEpisodes': 66, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.4974360284039046
'totalSteps': 7680, 'rewardStep': 0.5389152626978594, 'errorList': [], 'lossList': [0.0, -1.4112034392356874, 0.0, 98.27201263427735, 0.0, 0.0, 0.0], 'rewardMean': 0.6547705284880395, 'totalEpisodes': 112, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.678437321097963
'totalSteps': 8960, 'rewardStep': 0.767162682367589, 'errorList': [], 'lossList': [0.0, -1.4076762968301773, 0.0, 60.546281032562256, 0.0, 0.0, 0.0], 'rewardMean': 0.6708265504708323, 'totalEpisodes': 152, 'stepsPerEpisode': 59, 'rewardPerEpisode': 49.18959570553464
'totalSteps': 10240, 'rewardStep': 0.6460846411282855, 'errorList': [], 'lossList': [0.0, -1.4018916201591491, 0.0, 31.36280619621277, 0.0, 0.0, 0.0], 'rewardMean': 0.6677338118030139, 'totalEpisodes': 172, 'stepsPerEpisode': 115, 'rewardPerEpisode': 80.86962968032437
'totalSteps': 11520, 'rewardStep': 0.8089248485125378, 'errorList': [], 'lossList': [0.0, -1.385628890991211, 0.0, 30.69129149436951, 0.0, 0.0, 0.0], 'rewardMean': 0.6834217047707388, 'totalEpisodes': 187, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.715637506801176
'totalSteps': 12800, 'rewardStep': 0.9123024761405135, 'errorList': [], 'lossList': [0.0, -1.3620347434282303, 0.0, 34.08373426914215, 0.0, 0.0, 0.0], 'rewardMean': 0.7063097819077162, 'totalEpisodes': 195, 'stepsPerEpisode': 35, 'rewardPerEpisode': 26.371946986747272
'totalSteps': 14080, 'rewardStep': 0.6459344027185778, 'errorList': [], 'lossList': [0.0, -1.3317956632375718, 0.0, 23.018198909759523, 0.0, 0.0, 0.0], 'rewardMean': 0.7192238320096288, 'totalEpisodes': 198, 'stepsPerEpisode': 213, 'rewardPerEpisode': 149.19030540753448
'totalSteps': 15360, 'rewardStep': 0.8728459307880421, 'errorList': [], 'lossList': [0.0, -1.2899870204925536, 0.0, 10.552066693305969, 0.0, 0.0, 0.0], 'rewardMean': 0.7324460079986, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 971.0416663431272
'totalSteps': 16640, 'rewardStep': 0.7960069056686011, 'errorList': [], 'lossList': [0.0, -1.2543100726604461, 0.0, 9.674913059175015, 0.0, 0.0, 0.0], 'rewardMean': 0.725652023952956, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.3333327874257
'totalSteps': 17920, 'rewardStep': 0.8678435578974827, 'errorList': [], 'lossList': [0.0, -1.2310427278280258, 0.0, 8.399150095954537, 0.0, 0.0, 0.0], 'rewardMean': 0.7532995516800776, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.882533145289
'totalSteps': 19200, 'rewardStep': 0.9230063359143748, 'errorList': [], 'lossList': [0.0, -1.1895796048641205, 0.0, 6.346510192230344, 0.0, 0.0, 0.0], 'rewardMean': 0.7779027043833863, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.9509006071012
'totalSteps': 20480, 'rewardStep': 0.8884920375269485, 'errorList': [], 'lossList': [0.0, -1.1537525814771652, 0.0, 4.269573749117553, 0.0, 0.0, 0.0], 'rewardMean': 0.8128603818662953, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1177.092090205229
'totalSteps': 21760, 'rewardStep': 0.8235576981486655, 'errorList': [], 'lossList': [0.0, -1.0954896533489227, 0.0, 2.1774746508151295, 0.0, 0.0, 0.0], 'rewardMean': 0.8184998834444028, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.0480619064754
'totalSteps': 23040, 'rewardStep': 0.8990395363922493, 'errorList': [], 'lossList': [0.0, -1.0618349826335907, 0.0, 1.0069852122291922, 0.0, 0.0, 0.0], 'rewardMean': 0.8437953729707992, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.013176669169
'totalSteps': 24320, 'rewardStep': 0.9085146976500459, 'errorList': [], 'lossList': [0.0, -1.0296131122112273, 0.0, 0.8147442461177706, 0.0, 0.0, 0.0], 'rewardMean': 0.8537543578845501, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.8758050287245
'totalSteps': 25600, 'rewardStep': 0.9859131454039417, 'errorList': [0.38065471106571264, 0.17159558833915467, 0.30335339401484485, 0.23747809951734578, 0.29059973873528994, 0.19106016950930121, 0.24369157785498907, 0.22353119847237243, 0.25709859721383493, 0.27761396666212124, 0.36717929617318057, 0.15717531215331057, 0.26445051664810754, 0.4078388162413539, 0.30214763341924095, 0.27888650998020503, 0.31539211144717066, 0.25942051547363176, 0.25966918837982017, 0.16219284892220756, 0.21019965237824254, 0.21382197687227744, 0.30161002642157636, 0.27905972375288257, 0.3013686189985543, 0.43138119158451044, 0.40396842078023826, 0.19154324127118033, 0.3101010037778633, 0.26925805083633747, 0.22560549063004967, 0.2955266308644619, 0.28743256117095917, 0.2622782706984557, 0.3482937257532836, 0.2437462697112249, 0.34914503634191546, 0.278425992904525, 0.2325537785180531, 0.25230828319569865, 0.32537424777094637, 0.3506080516452638, 0.23930688104314807, 0.35351773141522247, 0.4763309880866278, 0.35532527773618766, 0.27070624136812865, 0.22230003170975796, 0.28593605370388314, 0.2365802851872789], 'lossList': [0.0, -0.9973767650127411, 0.0, 1.0288450730592011, 0.0, 0.0, 0.0], 'rewardMean': 0.861115424810893, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1213.5950913546167, 'successfulTests': 5
#maxSuccessfulTests=5, maxSuccessfulTestsAtStep=25600, timeSpent=71.93
