#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 110
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9248721390269645, 'errorList': [], 'lossList': [0.0, -1.4073566979169845, 0.0, 27.20355219721794, 0.0, 0.0, 0.0], 'rewardMean': 0.8623355497629638, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.1000537018294
'totalSteps': 3840, 'rewardStep': 0.6391255824986546, 'errorList': [], 'lossList': [0.0, -1.399945724606514, 0.0, 33.89789511442184, 0.0, 0.0, 0.0], 'rewardMean': 0.7879322273415275, 'totalEpisodes': 11, 'stepsPerEpisode': 263, 'rewardPerEpisode': 201.53298938203199
'totalSteps': 5120, 'rewardStep': 0.7085123455416708, 'errorList': [], 'lossList': [0.0, -1.4014861875772475, 0.0, 25.172038042545317, 0.0, 0.0, 0.0], 'rewardMean': 0.7680772568915633, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 963.1981523712897
'totalSteps': 6400, 'rewardStep': 0.8583168302516224, 'errorList': [], 'lossList': [0.0, -1.3969596654176712, 0.0, 20.515854534208774, 0.0, 0.0, 0.0], 'rewardMean': 0.7861251715635751, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1030.7866206852066
'totalSteps': 7680, 'rewardStep': 0.8227187224368412, 'errorList': [], 'lossList': [0.0, -1.3843033331632615, 0.0, 15.726444241404533, 0.0, 0.0, 0.0], 'rewardMean': 0.7922240967091194, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.9645914626312
'totalSteps': 8960, 'rewardStep': 0.8385358892420042, 'errorList': [], 'lossList': [0.0, -1.365729575753212, 0.0, 7.96523865044117, 0.0, 0.0, 0.0], 'rewardMean': 0.79884006707096, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 978.6249016859124
'totalSteps': 10240, 'rewardStep': 0.9100985005231438, 'errorList': [], 'lossList': [0.0, -1.3774792975187302, 0.0, 500.31591201782226, 0.0, 0.0, 0.0], 'rewardMean': 0.8127473712524831, 'totalEpisodes': 50, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.740926006935316
'totalSteps': 11520, 'rewardStep': 0.923083144689452, 'errorList': [], 'lossList': [0.0, -1.3760909432172774, 0.0, 281.83439056396486, 0.0, 0.0, 0.0], 'rewardMean': 0.8250069016343685, 'totalEpisodes': 100, 'stepsPerEpisode': 29, 'rewardPerEpisode': 27.21183795413285
'totalSteps': 12800, 'rewardStep': 0.6561623722116687, 'errorList': [], 'lossList': [0.0, -1.3748623156547546, 0.0, 111.04168125152587, 0.0, 0.0, 0.0], 'rewardMean': 0.8081224486920984, 'totalEpisodes': 134, 'stepsPerEpisode': 17, 'rewardPerEpisode': 9.699984759021465
'totalSteps': 14080, 'rewardStep': 0.9494274461720256, 'errorList': [266.7685949806031, 281.8547623771605, 210.6709918966145, 249.75185683495474, 242.74395534345416, 287.4228719396847, 217.0913780525075, 263.76732909995894, 287.46683308694435, 223.11730702306753, 232.75464214907947, 268.9502663668025, 282.0197244242744, 299.1887599460653, 296.5490223989076, 259.1008978842923, 177.01498288154576, 142.26788869319583, 276.08979484980034, 273.9918777458359, 286.46773217102464, 291.70840021809533, 259.37682390617334, 229.12275342379314, 276.371560418564, 266.80089428997246, 209.69289184543305, 282.6869845650757, 116.72099477504464, 300.267401757691, 212.62317777053758, 148.0865414917656, 164.41392351610705, 289.7606273502064, 200.34096910062834, 118.62777079299002, 296.2127723416672, 256.54675489363007, 183.36090810714344, 265.17634469084567, 274.3344289307346, 276.0230304872603, 266.41505966423756, 205.21013782328995, 207.81433039490398, 279.6418415580053, 294.86521120757385, 260.5728133061217, 277.37869415884273, 276.76097870380386], 'lossList': [0.0, -1.3766405385732652, 0.0, 72.06062662124634, 0.0, 0.0, 0.0], 'rewardMean': 0.8230852972594047, 'totalEpisodes': 171, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.598670048097333, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.8542617349192755, 'errorList': [], 'lossList': [0.0, -1.3784423631429672, 0.0, 54.0845373916626, 0.0, 0.0, 0.0], 'rewardMean': 0.8160242568486359, 'totalEpisodes': 192, 'stepsPerEpisode': 71, 'rewardPerEpisode': 63.644062292287934
'totalSteps': 16640, 'rewardStep': 0.9460415832374718, 'errorList': [72.17717699056669, 72.20792602594994, 39.94431210848612, 60.4502363617659, 75.9087630537055, 143.41194340025527, 69.43366939537863, 69.88487711018696, 78.74077662451501, 72.0108313591945, 147.18548669915458, 163.6018984541907, 28.94221218658452, 94.75029481450149, 31.02657119218661, 127.53276030288713, 115.37152822998672, 120.03208373797251, 159.55814140208128, 22.319394297077704, 118.07551297548216, 119.87348694308837, 143.2421162401575, 143.03905320458688, 70.37747755460417, 129.62447240226706, 24.920123666231113, 109.9263116055163, 130.07224657566596, 163.58300287736557, 172.44604390316815, 3.5392379025781744, 80.24410828909166, 154.07859265386728, 91.80564293863287, 67.3148439350026, 55.4983083699512, 130.29998681588867, 141.3556129601351, 111.48797068506842, 176.89086055711624, 152.82470213516902, 169.36668578671265, 152.38142280184465, 137.22322623893749, 59.7967520846113, 142.05570368113894, 119.51742600023766, 44.36825769587512, 15.194354994356166], 'lossList': [0.0, -1.3758106994628907, 0.0, 34.147845454216004, 0.0, 0.0, 0.0], 'rewardMean': 0.8467158569225177, 'totalEpisodes': 202, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.416392669811666, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.8757560571981772, 'errorList': [], 'lossList': [0.0, -1.3748720771074294, 0.0, 21.681228044033052, 0.0, 0.0, 0.0], 'rewardMean': 0.8634402280881682, 'totalEpisodes': 208, 'stepsPerEpisode': 75, 'rewardPerEpisode': 55.05540891857592
'totalSteps': 19200, 'rewardStep': 0.4877323618026031, 'errorList': [], 'lossList': [0.0, -1.3780060642957688, 0.0, 43.00169727325439, 0.0, 0.0, 0.0], 'rewardMean': 0.8263817812432661, 'totalEpisodes': 214, 'stepsPerEpisode': 123, 'rewardPerEpisode': 88.64514549317754
'totalSteps': 20480, 'rewardStep': 0.9306774190759001, 'errorList': [1.6002924489345511, 39.65677491191464, 19.25762228455842, 0.5642415511304195, 5.274884790214987, 26.898021312313556, 8.886427602846833, 1.5140659679907695, 8.16648721305789, 0.630036676301167, 10.784089473118634, 15.468213837195119, 14.070438969055136, 9.537663018569898, 12.723613499667765, 0.4609137023025115, 30.74917049032117, 6.649422127183817, 0.20765810642344676, 0.39555312154335676, 2.121398063750011, 1.7682711784016247, 0.9904867873400568, 1.6478556342361534, 3.292911915011147, 3.818041899813714, 2.516352556530033, 1.1621419025971953, 3.4953818230007196, 8.7342482523423, 25.37988335992014, 1.6738044741247535, 6.3615345415605224, 1.8514345190520196, 3.6707068958448708, 6.336232339880611, 0.17438387629984803, 1.0808552504312752, 0.5753170876688206, 0.0986336732412564, 0.06211460140329292, 0.8779264060509431, 6.598870015269375, 17.44915719382144, 3.074779917276941, 6.489112650835915, 5.57963635759754, 7.86636591787686, 1.1220949951438572, 9.189041682799038], 'lossList': [0.0, -1.379620730280876, 0.0, 35.428936347961425, 0.0, 0.0, 0.0], 'rewardMean': 0.8371776509071722, 'totalEpisodes': 219, 'stepsPerEpisode': 61, 'rewardPerEpisode': 53.035885452651996, 'successfulTests': 3
'totalSteps': 21760, 'rewardStep': 0.6742567266134267, 'errorList': [], 'lossList': [0.0, -1.3754337936639787, 0.0, 22.1664759850502, 0.0, 0.0, 0.0], 'rewardMean': 0.8207497346443144, 'totalEpisodes': 224, 'stepsPerEpisode': 140, 'rewardPerEpisode': 97.72816066848712
'totalSteps': 23040, 'rewardStep': 0.36176693844955066, 'errorList': [], 'lossList': [0.0, -1.3701117312908173, 0.0, 30.603930780887605, 0.0, 0.0, 0.0], 'rewardMean': 0.7659165784369552, 'totalEpisodes': 226, 'stepsPerEpisode': 982, 'rewardPerEpisode': 734.193595359235
'totalSteps': 24320, 'rewardStep': 0.8413418768124332, 'errorList': [], 'lossList': [0.0, -1.359389396905899, 0.0, 22.45431939482689, 0.0, 0.0, 0.0], 'rewardMean': 0.7577424516492532, 'totalEpisodes': 228, 'stepsPerEpisode': 83, 'rewardPerEpisode': 61.528477848257786
'totalSteps': 25600, 'rewardStep': 0.7448074514859621, 'errorList': [], 'lossList': [0.0, -1.3468701809644699, 0.0, 5.129964093267918, 0.0, 0.0, 0.0], 'rewardMean': 0.7666069595766827, 'totalEpisodes': 228, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.2230722667169
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=20480, timeSpent=124.43
