#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 112
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.8175897020487661, 'errorList': [], 'lossList': [0.0, -1.4439541923999786, 0.0, 25.71401263475418, 0.0, 0.0, 0.0], 'rewardMean': 0.6278101471273201, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.8139127141892
'totalSteps': 3840, 'rewardStep': 0.8822275387210969, 'errorList': [], 'lossList': [0.0, -1.4527422696352006, 0.0, 40.4448198223114, 0.0, 0.0, 0.0], 'rewardMean': 0.7126159443252457, 'totalEpisodes': 14, 'stepsPerEpisode': 487, 'rewardPerEpisode': 393.0310622837745
'totalSteps': 5120, 'rewardStep': 0.6879284014776464, 'errorList': [], 'lossList': [0.0, -1.4401733964681624, 0.0, 19.281333545446397, 0.0, 0.0, 0.0], 'rewardMean': 0.7064440586133459, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 913.0018756987989
'totalSteps': 6400, 'rewardStep': 0.5193467872321591, 'errorList': [], 'lossList': [0.0, -1.4252857822179794, 0.0, 29.676124614477157, 0.0, 0.0, 0.0], 'rewardMean': 0.6690246043371085, 'totalEpisodes': 15, 'stepsPerEpisode': 574, 'rewardPerEpisode': 442.9723682060141
'totalSteps': 7680, 'rewardStep': 0.6700419873466991, 'errorList': [], 'lossList': [0.0, -1.4150288951396943, 0.0, 26.323916951417925, 0.0, 0.0, 0.0], 'rewardMean': 0.6691941681720404, 'totalEpisodes': 16, 'stepsPerEpisode': 824, 'rewardPerEpisode': 648.5097132042604
'totalSteps': 8960, 'rewardStep': 0.8446644383802361, 'errorList': [], 'lossList': [0.0, -1.4232137072086335, 0.0, 25.27906715929508, 0.0, 0.0, 0.0], 'rewardMean': 0.694261349630354, 'totalEpisodes': 17, 'stepsPerEpisode': 361, 'rewardPerEpisode': 275.0752053791496
'totalSteps': 10240, 'rewardStep': 0.7036795265217342, 'errorList': [], 'lossList': [0.0, -1.400567011833191, 0.0, 468.85671798706056, 0.0, 0.0, 0.0], 'rewardMean': 0.6954386217417765, 'totalEpisodes': 58, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.624016986587076
'totalSteps': 11520, 'rewardStep': 0.7171777969387917, 'errorList': [], 'lossList': [0.0, -1.3993173408508301, 0.0, 151.03103435516357, 0.0, 0.0, 0.0], 'rewardMean': 0.697854085652556, 'totalEpisodes': 91, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.39141221593685
'totalSteps': 12800, 'rewardStep': 0.8365168400235875, 'errorList': [], 'lossList': [0.0, -1.3948358517885209, 0.0, 65.79418443679809, 0.0, 0.0, 0.0], 'rewardMean': 0.7117203610896591, 'totalEpisodes': 126, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.076010411450007
'totalSteps': 14080, 'rewardStep': 0.5878345740441797, 'errorList': [], 'lossList': [0.0, -1.3880001413822174, 0.0, 37.20702912330628, 0.0, 0.0, 0.0], 'rewardMean': 0.7267007592734895, 'totalEpisodes': 158, 'stepsPerEpisode': 28, 'rewardPerEpisode': 23.800080848634355
'totalSteps': 15360, 'rewardStep': 0.858780685424493, 'errorList': [], 'lossList': [0.0, -1.381465464234352, 0.0, 38.28664359092713, 0.0, 0.0, 0.0], 'rewardMean': 0.7308198576110625, 'totalEpisodes': 181, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.306414947128847
'totalSteps': 16640, 'rewardStep': 0.9388538926231149, 'errorList': [20.858738696074898, 80.27087756142161, 8.845196928192268, 27.86522586579281, 27.99702369047179, 26.402718653830576, 2.836213934975858, 15.193061655928812, 6.493795441356921, 38.47655755036833, 8.063824170685677, 12.568869911577693, 0.11368439336272015, 95.35719729718247, 10.856509249683988, 64.37358015150512, 69.28545584540063, 31.220332069953525, 26.347172519778937, 28.588816450865657, 25.735204540031678, 14.39216616881245, 16.758820169279364, 64.82479883227765, 6.306160406535093, 17.61642943257651, 85.23604971635422, 47.648176097055206, 7.798930606533011, 20.501741044079726, 56.59689318437484, 64.3542942918687, 1.3261490119078214, 15.232742013053084, 0.7240931491248448, 0.9146029561178506, 10.269400821046005, 51.7179924489699, 49.270193130843225, 20.39938470485813, 11.37009580970078, 74.33942492139163, 38.708209302408065, 1.8123918374808252, 61.644226103766314, 5.31078189836679, 37.283079866297, 9.88425592176462, 43.07781702808739, 14.547973107974952], 'lossList': [0.0, -1.3760637891292573, 0.0, 17.500424170494078, 0.0, 0.0, 0.0], 'rewardMean': 0.7364824930012641, 'totalEpisodes': 190, 'stepsPerEpisode': 46, 'rewardPerEpisode': 39.500340235149906, 'successfulTests': 1
'totalSteps': 17920, 'rewardStep': 0.024948217264556982, 'errorList': [], 'lossList': [0.0, -1.3568606209754943, 0.0, 20.008512125015258, 0.0, 0.0, 0.0], 'rewardMean': 0.6701844745799551, 'totalEpisodes': 197, 'stepsPerEpisode': 203, 'rewardPerEpisode': 143.7234103082793
'totalSteps': 19200, 'rewardStep': 0.8382624774092233, 'errorList': [], 'lossList': [0.0, -1.331241992712021, 0.0, 13.191856424808503, 0.0, 0.0, 0.0], 'rewardMean': 0.7020760435976616, 'totalEpisodes': 205, 'stepsPerEpisode': 156, 'rewardPerEpisode': 134.6341316960464
'totalSteps': 20480, 'rewardStep': 0.647480091316746, 'errorList': [], 'lossList': [0.0, -1.3295763641595841, 0.0, 10.590600370168685, 0.0, 0.0, 0.0], 'rewardMean': 0.6998198539946664, 'totalEpisodes': 209, 'stepsPerEpisode': 163, 'rewardPerEpisode': 133.92116252960912
'totalSteps': 21760, 'rewardStep': 0.7273312830179575, 'errorList': [], 'lossList': [0.0, -1.3308061838150025, 0.0, 6.600191193819046, 0.0, 0.0, 0.0], 'rewardMean': 0.6880865384584384, 'totalEpisodes': 212, 'stepsPerEpisode': 72, 'rewardPerEpisode': 59.44980636580955
'totalSteps': 23040, 'rewardStep': 0.7068231607710325, 'errorList': [], 'lossList': [0.0, -1.3167391735315324, 0.0, 4.364379125237465, 0.0, 0.0, 0.0], 'rewardMean': 0.6884009018833682, 'totalEpisodes': 215, 'stepsPerEpisode': 241, 'rewardPerEpisode': 193.50223715859872
'totalSteps': 24320, 'rewardStep': 0.43998733260510503, 'errorList': [], 'lossList': [0.0, -1.2932426017522811, 0.0, 3.199767817854881, 0.0, 0.0, 0.0], 'rewardMean': 0.6606818554499997, 'totalEpisodes': 217, 'stepsPerEpisode': 285, 'rewardPerEpisode': 204.41359222424052
'totalSteps': 25600, 'rewardStep': 0.5736931179292437, 'errorList': [], 'lossList': [0.0, -1.278035698533058, 0.0, 3.8009437996149065, 0.0, 0.0, 0.0], 'rewardMean': 0.6343994832405653, 'totalEpisodes': 218, 'stepsPerEpisode': 1273, 'rewardPerEpisode': 913.1117608915134
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=16640, timeSpent=81.46
