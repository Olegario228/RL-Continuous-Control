#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 113
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6571790238637092, 'errorList': [], 'lossList': [0.0, -1.4219415307044982, 0.0, 27.967324876785277, 0.0, 0.0, 0.0], 'rewardMean': 0.7112524284053826, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.155318086592914
'totalSteps': 3840, 'rewardStep': 0.9610644552179114, 'errorList': [], 'lossList': [0.0, -1.4156790775060655, 0.0, 27.10434205055237, 0.0, 0.0, 0.0], 'rewardMean': 0.794523104009559, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 374.0707982232474
'totalSteps': 5120, 'rewardStep': 0.8078394364665894, 'errorList': [], 'lossList': [0.0, -1.4097591370344162, 0.0, 27.572469246685504, 0.0, 0.0, 0.0], 'rewardMean': 0.7978521871238166, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.6372820391823
'totalSteps': 6400, 'rewardStep': 0.6825150761809219, 'errorList': [], 'lossList': [0.0, -1.3902441936731338, 0.0, 19.68911181330681, 0.0, 0.0, 0.0], 'rewardMean': 0.7747847649352376, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.0554769182022
'totalSteps': 7680, 'rewardStep': 0.9700832556059505, 'errorList': [], 'lossList': [0.0, -1.370688983798027, 0.0, 16.65751660645008, 0.0, 0.0, 0.0], 'rewardMean': 0.8073345133803564, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.2971084484632
'totalSteps': 8960, 'rewardStep': 0.7785309365550336, 'errorList': [], 'lossList': [0.0, -1.3546600979566574, 0.0, 8.918784222006797, 0.0, 0.0, 0.0], 'rewardMean': 0.8032197166910245, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.2454794568926
'totalSteps': 10240, 'rewardStep': 0.7413971365642257, 'errorList': [], 'lossList': [0.0, -1.3434048861265182, 0.0, 513.8228287506104, 0.0, 0.0, 0.0], 'rewardMean': 0.7954918941751747, 'totalEpisodes': 49, 'stepsPerEpisode': 126, 'rewardPerEpisode': 115.79340621859883
'totalSteps': 11520, 'rewardStep': 0.9454030002604478, 'errorList': [125.19250354156084, 136.04706666283465, 136.78824411757745, 130.40071615459865, 129.12310008778402, 135.73975096153296, 136.30191796977084, 134.96858001175568, 132.1600360747276, 131.1545891306543, 140.63902661318798, 123.80202038479594, 137.1785475718036, 131.96272588481193, 137.01719720191596, 127.16486658365827, 140.64195746480254, 139.51469907876836, 133.20850465646606, 120.1560612313794, 140.98837480095466, 99.36317543241867, 140.96751540256736, 120.85415641845192, 133.7871026827463, 123.07723645771311, 138.38480580616888, 131.2591259787084, 133.99874396445014, 122.7278122003956, 91.88129505030626, 129.86794864405957, 132.51081128415314, 123.60371872787749, 113.59493637894373, 125.49574230865439, 135.72002380749296, 134.17941622389245, 139.5789026739725, 123.92178536984558, 113.83325445162801, 82.30130674376532, 130.16792760204711, 117.8867081752331, 127.99157555690685, 132.40474976263468, 107.53866649989992, 131.01931309184198, 124.42358476420009, 128.33938287967936], 'lossList': [0.0, -1.342885717153549, 0.0, 347.0372780609131, 0.0, 0.0, 0.0], 'rewardMean': 0.812148683740205, 'totalEpisodes': 87, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.10736742153827, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.7256972741673434, 'errorList': [], 'lossList': [0.0, -1.3431352412700652, 0.0, 194.50361923217773, 0.0, 0.0, 0.0], 'rewardMean': 0.8035035427829189, 'totalEpisodes': 125, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.647629326882121
'totalSteps': 14080, 'rewardStep': 0.7029589781798471, 'errorList': [], 'lossList': [0.0, -1.337778754234314, 0.0, 61.51799704551697, 0.0, 0.0, 0.0], 'rewardMean': 0.7972668573061981, 'totalEpisodes': 159, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.107850328215848
'totalSteps': 15360, 'rewardStep': 0.776294538765628, 'errorList': [], 'lossList': [0.0, -1.333190417289734, 0.0, 33.41740238189697, 0.0, 0.0, 0.0], 'rewardMean': 0.8091784087963898, 'totalEpisodes': 192, 'stepsPerEpisode': 46, 'rewardPerEpisode': 40.84862217145845
'totalSteps': 16640, 'rewardStep': 0.8803592717518819, 'errorList': [], 'lossList': [0.0, -1.3263199871778488, 0.0, 19.873732256889344, 0.0, 0.0, 0.0], 'rewardMean': 0.8011078904497868, 'totalEpisodes': 214, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.997409542114949
'totalSteps': 17920, 'rewardStep': 0.49049083021734197, 'errorList': [], 'lossList': [0.0, -1.3242428797483443, 0.0, 16.415991015434265, 0.0, 0.0, 0.0], 'rewardMean': 0.7693730298248622, 'totalEpisodes': 225, 'stepsPerEpisode': 65, 'rewardPerEpisode': 43.5518811021782
'totalSteps': 19200, 'rewardStep': 0.3985584457316505, 'errorList': [], 'lossList': [0.0, -1.318728437423706, 0.0, 16.630187227725983, 0.0, 0.0, 0.0], 'rewardMean': 0.7409773667799351, 'totalEpisodes': 232, 'stepsPerEpisode': 197, 'rewardPerEpisode': 131.72835694384585
'totalSteps': 20480, 'rewardStep': 0.9061026826292391, 'errorList': [], 'lossList': [0.0, -1.309608074426651, 0.0, 12.605655190944672, 0.0, 0.0, 0.0], 'rewardMean': 0.7345793094822639, 'totalEpisodes': 236, 'stepsPerEpisode': 182, 'rewardPerEpisode': 150.597015121286
'totalSteps': 21760, 'rewardStep': 0.882943018032044, 'errorList': [], 'lossList': [0.0, -1.2936736339330672, 0.0, 9.765109612941743, 0.0, 0.0, 0.0], 'rewardMean': 0.7450205176299649, 'totalEpisodes': 242, 'stepsPerEpisode': 101, 'rewardPerEpisode': 91.94131344618697
'totalSteps': 23040, 'rewardStep': 0.38534700167490005, 'errorList': [], 'lossList': [0.0, -1.2667652678489685, 0.0, 8.583248535394668, 0.0, 0.0, 0.0], 'rewardMean': 0.7094155041410324, 'totalEpisodes': 244, 'stepsPerEpisode': 755, 'rewardPerEpisode': 601.4080614341593
'totalSteps': 24320, 'rewardStep': 0.9169506991179283, 'errorList': [], 'lossList': [0.0, -1.2453532660007476, 0.0, 11.469060163497925, 0.0, 0.0, 0.0], 'rewardMean': 0.7065702740267803, 'totalEpisodes': 248, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.7603287018937657
'totalSteps': 25600, 'rewardStep': 0.9708203376317383, 'errorList': [0.010472550216478995, 0.006545100027916756, 0.009382968284254629, 0.005528714347271032, 0.015180898131617434, 0.007720594837675403, 0.002616552568954634, 0.01021818345442198, 0.01807111646779018, 0.006587692937512003, 0.006740247830478577, 0.0068963146926092735, 0.00936567578655723, 0.004096743830918787, 0.005151009679968029, 0.006233523214678569, 0.004154473230377955, 0.011631045533448304, 0.0032400163877302395, 0.015434994623759659, 0.009835107552666903, 0.005237308597936672, 0.0025519504737187043, 0.004084954033674416, 0.0039447485170577045, 0.002727440611639941, 0.006237755260020225, 0.004434174966425707, 0.006709138567709112, 0.005971481261154403, 0.005124342185445797, 0.005782029799089691, 0.0023732651443972718, 0.005113939358379354, 0.0029222963347444223, 0.015302819580127797, 0.003985267563454483, 0.0025759032617342827, 0.0069481220435859735, 0.004833672795900965, 0.011628195405323605, 0.00838378299981488, 0.003566229572220827, 0.004399999185944367, 0.002530700606903925, 0.005758836551393569, 0.008398771846249063, 0.008617064366406907, 0.012127978738409796, 0.0033211053050052645], 'lossList': [0.0, -1.228295915722847, 0.0, 3.80976538002491, 0.0, 0.0, 0.0], 'rewardMean': 0.7310825803732198, 'totalEpisodes': 248, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.0565595390874, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=102.37
