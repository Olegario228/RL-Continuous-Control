#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 114
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8691085372648566, 'errorList': [], 'lossList': [0.0, -1.4240563344955444, 0.0, 30.64494592487812, 0.0, 0.0, 0.0], 'rewardMean': 0.7705119717041, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 252.45126305546043
'totalSteps': 3840, 'rewardStep': 0.7729674656942122, 'errorList': [], 'lossList': [0.0, -1.4417815965414047, 0.0, 30.43477287054062, 0.0, 0.0, 0.0], 'rewardMean': 0.771330469700804, 'totalEpisodes': 15, 'stepsPerEpisode': 168, 'rewardPerEpisode': 126.68083565058349
'totalSteps': 5120, 'rewardStep': 0.6079649061862789, 'errorList': [], 'lossList': [0.0, -1.445258199572563, 0.0, 22.841494723558426, 0.0, 0.0, 0.0], 'rewardMean': 0.7304890788221727, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 954.8449473550374
'totalSteps': 6400, 'rewardStep': 0.8450174386193572, 'errorList': [], 'lossList': [0.0, -1.4332854336500167, 0.0, 18.690112679600716, 0.0, 0.0, 0.0], 'rewardMean': 0.7533947507816097, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 996.6077625444125
'totalSteps': 7680, 'rewardStep': 0.9377183980084481, 'errorList': [], 'lossList': [0.0, -1.4316302615404128, 0.0, 11.582921728491783, 0.0, 0.0, 0.0], 'rewardMean': 0.7841153586527493, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 966.3637174716822
'totalSteps': 8960, 'rewardStep': 0.6482717404849998, 'errorList': [], 'lossList': [0.0, -1.4218854022026062, 0.0, 67.00395256519317, 0.0, 0.0, 0.0], 'rewardMean': 0.764709127485928, 'totalEpisodes': 20, 'stepsPerEpisode': 214, 'rewardPerEpisode': 161.26680857618837
'totalSteps': 10240, 'rewardStep': 0.9316833508787059, 'errorList': [92.63643258836287, 79.20487268377838, 93.4962177699778, 78.33258693351992, 97.71461216598105, 95.30354909543482, 89.14172878170291, 91.28968816932635, 91.29426040296957, 92.50987459734513, 94.62504572608972, 69.58872245261279, 91.51271279121156, 88.53067996988344, 90.93048349094288, 90.28630716922747, 96.82334985277802, 92.3082749452019, 94.49337989403232, 90.74042065990673, 93.47231082450561, 95.35213815675898, 92.3017396878634, 92.91023911910226, 94.40530256640507, 98.08566684619929, 81.73941454534955, 88.66769687144892, 91.36187817297449, 95.32647566517373, 93.74968985428298, 95.40913991954683, 95.84532409228704, 81.77534341155904, 95.03282988318864, 92.59274067418593, 87.43379530720395, 77.78987264807704, 89.26874723550075, 81.48536498296085, 95.50616894571004, 95.57175806537357, 83.56478980194161, 96.10493596132525, 95.9559468950929, 92.27894351657568, 84.66468499549093, 82.80919484756834, 93.71728924291938, 98.79476367252248], 'lossList': [0.0, -1.4216225349903107, 0.0, 373.7788269805908, 0.0, 0.0, 0.0], 'rewardMean': 0.7855809054100252, 'totalEpisodes': 64, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.768681374070608, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5872787158043663, 'errorList': [], 'lossList': [0.0, -1.4203953289985656, 0.0, 146.0926598739624, 0.0, 0.0, 0.0], 'rewardMean': 0.7635473287871742, 'totalEpisodes': 100, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.693157329881249
'totalSteps': 12800, 'rewardStep': 0.7425854331663025, 'errorList': [], 'lossList': [0.0, -1.413888258934021, 0.0, 89.36689783096314, 0.0, 0.0, 0.0], 'rewardMean': 0.7614511392250871, 'totalEpisodes': 140, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.2296794530252937
'totalSteps': 14080, 'rewardStep': 0.7442345005053458, 'errorList': [], 'lossList': [0.0, -1.4081476163864135, 0.0, 75.93607189178466, 0.0, 0.0, 0.0], 'rewardMean': 0.7686830486612874, 'totalEpisodes': 166, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.368867161298524
'totalSteps': 15360, 'rewardStep': 0.3670292365743626, 'errorList': [], 'lossList': [0.0, -1.3991041630506516, 0.0, 35.2854634141922, 0.0, 0.0, 0.0], 'rewardMean': 0.7184751185922379, 'totalEpisodes': 183, 'stepsPerEpisode': 76, 'rewardPerEpisode': 45.13555558928776
'totalSteps': 16640, 'rewardStep': 0.4644493260150263, 'errorList': [], 'lossList': [0.0, -1.3827820324897766, 0.0, 46.18211051940918, 0.0, 0.0, 0.0], 'rewardMean': 0.6876233046243193, 'totalEpisodes': 200, 'stepsPerEpisode': 44, 'rewardPerEpisode': 28.132011222278233
'totalSteps': 17920, 'rewardStep': 0.6949657499888628, 'errorList': [], 'lossList': [0.0, -1.3632352066040039, 0.0, 21.633327734470367, 0.0, 0.0, 0.0], 'rewardMean': 0.6963233890045777, 'totalEpisodes': 211, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.67214652564837
'totalSteps': 19200, 'rewardStep': 0.7022873057077448, 'errorList': [], 'lossList': [0.0, -1.3332807207107544, 0.0, 20.347762734889983, 0.0, 0.0, 0.0], 'rewardMean': 0.6820503757134164, 'totalEpisodes': 216, 'stepsPerEpisode': 320, 'rewardPerEpisode': 266.0710193731005
'totalSteps': 20480, 'rewardStep': 0.86818104639559, 'errorList': [], 'lossList': [0.0, -1.310800593495369, 0.0, 8.928374141454697, 0.0, 0.0, 0.0], 'rewardMean': 0.6750966405521307, 'totalEpisodes': 221, 'stepsPerEpisode': 48, 'rewardPerEpisode': 40.34960586125758
'totalSteps': 21760, 'rewardStep': 0.7896064457988947, 'errorList': [], 'lossList': [0.0, -1.3094278341531753, 0.0, 6.509320008158684, 0.0, 0.0, 0.0], 'rewardMean': 0.6892301110835202, 'totalEpisodes': 225, 'stepsPerEpisode': 274, 'rewardPerEpisode': 238.6348329854354
'totalSteps': 23040, 'rewardStep': 0.7915674116646081, 'errorList': [], 'lossList': [0.0, -1.3187523025274277, 0.0, 9.438827322721481, 0.0, 0.0, 0.0], 'rewardMean': 0.6752185171621103, 'totalEpisodes': 228, 'stepsPerEpisode': 90, 'rewardPerEpisode': 74.03306860793761
'totalSteps': 24320, 'rewardStep': 0.6953778754931135, 'errorList': [], 'lossList': [0.0, -1.3046479618549347, 0.0, 4.408724203705788, 0.0, 0.0, 0.0], 'rewardMean': 0.686028433130985, 'totalEpisodes': 230, 'stepsPerEpisode': 350, 'rewardPerEpisode': 290.88924797102413
'totalSteps': 25600, 'rewardStep': 0.546547469473666, 'errorList': [], 'lossList': [0.0, -1.2911507630348205, 0.0, 3.6789325293898583, 0.0, 0.0, 0.0], 'rewardMean': 0.6664246367617214, 'totalEpisodes': 230, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.407757324511
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.06
