#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 115
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9360051883016813, 'errorList': [], 'lossList': [0.0, -1.4352489554882049, 0.0, 30.506278136372565, 0.0, 0.0, 0.0], 'rewardMean': 0.9157933897549242, 'totalEpisodes': 8, 'stepsPerEpisode': 529, 'rewardPerEpisode': 377.1040632025541
'totalSteps': 3840, 'rewardStep': 0.7555668198112835, 'errorList': [], 'lossList': [0.0, -1.4207206022739411, 0.0, 31.991608583927153, 0.0, 0.0, 0.0], 'rewardMean': 0.862384533107044, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 203.90321499120026
'totalSteps': 5120, 'rewardStep': 0.7623952554225158, 'errorList': [], 'lossList': [0.0, -1.4165082687139512, 0.0, 27.513575037717818, 0.0, 0.0, 0.0], 'rewardMean': 0.837387213685912, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1012.5961592356146
'totalSteps': 6400, 'rewardStep': 0.9699806566011066, 'errorList': [], 'lossList': [0.0, -1.4062653356790542, 0.0, 23.19974252641201, 0.0, 0.0, 0.0], 'rewardMean': 0.8639059022689508, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.892036443494
'totalSteps': 7680, 'rewardStep': 0.6928482861952932, 'errorList': [], 'lossList': [0.0, -1.3945427274703979, 0.0, 15.808900180757046, 0.0, 0.0, 0.0], 'rewardMean': 0.8353962995900078, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.8936697105862
'totalSteps': 8960, 'rewardStep': 0.8264788875670764, 'errorList': [], 'lossList': [0.0, -1.3531471520662308, 0.0, 8.908495738208295, 0.0, 0.0, 0.0], 'rewardMean': 0.834122383586732, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.7868777125752
'totalSteps': 10240, 'rewardStep': 0.5545365622378353, 'errorList': [], 'lossList': [0.0, -1.348666433095932, 0.0, 490.39205741882324, 0.0, 0.0, 0.0], 'rewardMean': 0.7991741559181199, 'totalEpisodes': 43, 'stepsPerEpisode': 83, 'rewardPerEpisode': 63.98772345775703
'totalSteps': 11520, 'rewardStep': 0.4528894646388014, 'errorList': [], 'lossList': [0.0, -1.3465721267461777, 0.0, 386.4975405883789, 0.0, 0.0, 0.0], 'rewardMean': 0.7606980791093068, 'totalEpisodes': 93, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.224102552012837
'totalSteps': 12800, 'rewardStep': 0.7186106207751749, 'errorList': [], 'lossList': [0.0, -1.346232078075409, 0.0, 117.1403309249878, 0.0, 0.0, 0.0], 'rewardMean': 0.7564893332758935, 'totalEpisodes': 137, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.06913164434355
'totalSteps': 14080, 'rewardStep': 0.48915846705291044, 'errorList': [], 'lossList': [0.0, -1.3483375579118728, 0.0, 64.00823524475098, 0.0, 0.0, 0.0], 'rewardMean': 0.715847020860368, 'totalEpisodes': 175, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.48915846705291044
'totalSteps': 15360, 'rewardStep': 0.8708061230507813, 'errorList': [], 'lossList': [0.0, -1.3455849975347518, 0.0, 50.370812492370604, 0.0, 0.0, 0.0], 'rewardMean': 0.709327114335278, 'totalEpisodes': 209, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.762761134274562
'totalSteps': 16640, 'rewardStep': 0.8306033507802675, 'errorList': [], 'lossList': [0.0, -1.3448193657398224, 0.0, 29.04932024002075, 0.0, 0.0, 0.0], 'rewardMean': 0.7168307674321761, 'totalEpisodes': 225, 'stepsPerEpisode': 46, 'rewardPerEpisode': 38.88600100856519
'totalSteps': 17920, 'rewardStep': 0.9342963415301198, 'errorList': [3.3929514693304985, 14.313494425274344, 12.60184748181505, 20.90899100512244, 7.2068251065186875, 14.675575589127275, 1.0816048868163932, 16.373031852345687, 15.214933018257085, 14.568581317747777, 20.592701740121026, 7.304940926871896, 12.721224430901376, 23.103454960421207, 0.5481080139890144, 20.26466490856252, 16.694844478173525, 13.18755458864833, 16.01785426371882, 10.48764110862374, 11.021174703945983, 1.2345514946050244, 22.323272736044146, 5.885238580978103, 2.6368254174036494, 15.596394180971199, 19.548829958876947, 22.33723253437189, 14.620693059273671, 0.1521014876495433, 0.8806828825458926, 1.0788254460673086, 18.544717215133808, 12.770334387680128, 3.1383730660705664, 7.556852094242616, 6.312388089311611, 14.338156248410462, 1.4287033828683826, 18.67324452491052, 0.4628906095965719, 23.135183741539993, 6.588259483884298, 12.90272111235454, 12.806290825835092, 18.020804940406794, 9.547180949404028, 22.02223859963519, 15.928329439156592, 8.862301903472504], 'lossList': [0.0, -1.3426059317588805, 0.0, 25.98086506843567, 0.0, 0.0, 0.0], 'rewardMean': 0.7340208760429366, 'totalEpisodes': 233, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.59895493268979, 'successfulTests': 1
'totalSteps': 19200, 'rewardStep': 0.7551362541754991, 'errorList': [], 'lossList': [0.0, -1.3322979074716568, 0.0, 20.80522380590439, 0.0, 0.0, 0.0], 'rewardMean': 0.712536435800376, 'totalEpisodes': 238, 'stepsPerEpisode': 333, 'rewardPerEpisode': 275.5164215767836
'totalSteps': 20480, 'rewardStep': 0.8740716180670444, 'errorList': [], 'lossList': [0.0, -1.324136011004448, 0.0, 12.231013190746307, 0.0, 0.0, 0.0], 'rewardMean': 0.7306587689875511, 'totalEpisodes': 243, 'stepsPerEpisode': 99, 'rewardPerEpisode': 75.26112181521933
'totalSteps': 21760, 'rewardStep': 0.8483413233253224, 'errorList': [], 'lossList': [0.0, -1.315874578356743, 0.0, 22.176584191322327, 0.0, 0.0, 0.0], 'rewardMean': 0.7328450125633756, 'totalEpisodes': 249, 'stepsPerEpisode': 60, 'rewardPerEpisode': 49.09942765953191
'totalSteps': 23040, 'rewardStep': 0.2180965894870272, 'errorList': [], 'lossList': [0.0, -1.3079688745737075, 0.0, 7.000577282905579, 0.0, 0.0, 0.0], 'rewardMean': 0.6992010152882948, 'totalEpisodes': 250, 'stepsPerEpisode': 685, 'rewardPerEpisode': 459.7630364769892
'totalSteps': 24320, 'rewardStep': 0.6811016973386916, 'errorList': [], 'lossList': [0.0, -1.306370331645012, 0.0, 5.513321679234505, 0.0, 0.0, 0.0], 'rewardMean': 0.7220222385582838, 'totalEpisodes': 250, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 963.0375933644266
'totalSteps': 25600, 'rewardStep': 0.8201917757053842, 'errorList': [], 'lossList': [0.0, -1.290127741098404, 0.0, 3.0632783043384553, 0.0, 0.0, 0.0], 'rewardMean': 0.7321803540513048, 'totalEpisodes': 250, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.6714926298077
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=17920, timeSpent=81.35
