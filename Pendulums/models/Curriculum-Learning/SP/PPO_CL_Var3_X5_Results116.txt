#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 116
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8584773762797421, 'errorList': [], 'lossList': [0.0, -1.4446191197633744, 0.0, 36.9141774892807, 0.0, 0.0, 0.0], 'rewardMean': 0.7085625473780486, 'totalEpisodes': 12, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.02811234838807
'totalSteps': 3840, 'rewardStep': 0.8944009800004282, 'errorList': [], 'lossList': [0.0, -1.4568239593505858, 0.0, 30.44013352572918, 0.0, 0.0, 0.0], 'rewardMean': 0.7705086915855084, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.8041395198932
'totalSteps': 5120, 'rewardStep': 0.6498535335801406, 'errorList': [], 'lossList': [0.0, -1.4395900559425354, 0.0, 29.96006373167038, 0.0, 0.0, 0.0], 'rewardMean': 0.7403449020841665, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.28239467875
'totalSteps': 6400, 'rewardStep': 0.9407084082310461, 'errorList': [], 'lossList': [0.0, -1.4377742767333985, 0.0, 23.449915381371977, 0.0, 0.0, 0.0], 'rewardMean': 0.7804176033135424, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.540427735877
'totalSteps': 7680, 'rewardStep': 0.8529723389589741, 'errorList': [], 'lossList': [0.0, -1.417629005908966, 0.0, 18.594840948879718, 0.0, 0.0, 0.0], 'rewardMean': 0.7925100592544476, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1148.3926584133887
'totalSteps': 8960, 'rewardStep': 0.9540137533866181, 'errorList': [], 'lossList': [0.0, -1.3968455255031587, 0.0, 9.234339894503355, 0.0, 0.0, 0.0], 'rewardMean': 0.8155820155590433, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1109.8417625844093
'totalSteps': 10240, 'rewardStep': 0.923815628839709, 'errorList': [], 'lossList': [0.0, -1.3674576914310455, 0.0, 465.2456455993652, 0.0, 0.0, 0.0], 'rewardMean': 0.8291112172191266, 'totalEpisodes': 38, 'stepsPerEpisode': 86, 'rewardPerEpisode': 77.83931366556384
'totalSteps': 11520, 'rewardStep': 0.8982615914139945, 'errorList': [], 'lossList': [0.0, -1.3676695340871812, 0.0, 568.0745893859863, 0.0, 0.0, 0.0], 'rewardMean': 0.8367945921296676, 'totalEpisodes': 86, 'stepsPerEpisode': 49, 'rewardPerEpisode': 41.08552207039516
'totalSteps': 12800, 'rewardStep': 0.8974973146433816, 'errorList': [], 'lossList': [0.0, -1.369250460267067, 0.0, 250.4338877105713, 0.0, 0.0, 0.0], 'rewardMean': 0.8428648643810389, 'totalEpisodes': 117, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.629522918722019
'totalSteps': 14080, 'rewardStep': 0.4462949828001854, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7904113514654663, 'totalEpisodes': 151, 'stepsPerEpisode': 17, 'rewardPerEpisode': 10.821728157081008
'totalSteps': 15360, 'rewardStep': 0.6913939029564995, 'errorList': [], 'lossList': [0.0, -1.371454556584358, 0.0, 110.90286195755004, 0.0, 0.0, 0.0], 'rewardMean': 0.7701106437610734, 'totalEpisodes': 184, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.582029193362832
'totalSteps': 16640, 'rewardStep': 0.7379539748077164, 'errorList': [], 'lossList': [0.0, -1.373741662502289, 0.0, 46.53274071693421, 0.0, 0.0, 0.0], 'rewardMean': 0.778920687883831, 'totalEpisodes': 214, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.860561668734993
'totalSteps': 17920, 'rewardStep': 0.4256589451841228, 'errorList': [], 'lossList': [0.0, -1.3756342375278472, 0.0, 23.31151372909546, 0.0, 0.0, 0.0], 'rewardMean': 0.7274157415791387, 'totalEpisodes': 238, 'stepsPerEpisode': 28, 'rewardPerEpisode': 17.768251476808018
'totalSteps': 19200, 'rewardStep': 0.7680762712709074, 'errorList': [], 'lossList': [0.0, -1.3674739444255828, 0.0, 23.54665600299835, 0.0, 0.0, 0.0], 'rewardMean': 0.7189261348103321, 'totalEpisodes': 261, 'stepsPerEpisode': 24, 'rewardPerEpisode': 17.309641434868688
'totalSteps': 20480, 'rewardStep': 0.6968216217763983, 'errorList': [], 'lossList': [0.0, -1.3503508067131043, 0.0, 12.683498318195342, 0.0, 0.0, 0.0], 'rewardMean': 0.6932069216493101, 'totalEpisodes': 274, 'stepsPerEpisode': 84, 'rewardPerEpisode': 71.93755904715998
'totalSteps': 21760, 'rewardStep': 0.7791036157790495, 'errorList': [], 'lossList': [0.0, -1.3301853358745575, 0.0, 15.15903352022171, 0.0, 0.0, 0.0], 'rewardMean': 0.678735720343244, 'totalEpisodes': 284, 'stepsPerEpisode': 57, 'rewardPerEpisode': 41.00271738792615
'totalSteps': 23040, 'rewardStep': 0.792221984638495, 'errorList': [], 'lossList': [0.0, -1.3252065175771712, 0.0, 7.415902061462402, 0.0, 0.0, 0.0], 'rewardMean': 0.668131759665694, 'totalEpisodes': 293, 'stepsPerEpisode': 87, 'rewardPerEpisode': 71.51255385861947
'totalSteps': 24320, 'rewardStep': 0.8387150557577423, 'errorList': [], 'lossList': [0.0, -1.3171515995264054, 0.0, 6.786885732412339, 0.0, 0.0, 0.0], 'rewardMean': 0.6622535337771301, 'totalEpisodes': 300, 'stepsPerEpisode': 137, 'rewardPerEpisode': 122.22813313185165
'totalSteps': 25600, 'rewardStep': 0.9304038848265983, 'errorList': [0.6523148418636125, 31.451317189243444, 5.991598801757936, 44.27667083110487, 6.241665617257502, 87.26642144581504, 7.879761951106357, 5.534284315178519, 146.56470870650796, 62.70448975797569, 53.814989654097594, 43.17904035920617, 11.022905335174482, 1.3582430433484196, 0.7352924135646882, 56.21165278544197, 6.205530379071589, 158.57104919908016, 20.965303096236724, 37.34771538731085, 107.81143644308906, 9.083826258566468, 13.401854546467007, 57.52844699527524, 9.772815360586842, 64.92593862548483, 2.852717581476019, 113.43907819857043, 63.75160072257186, 77.40633234812053, 31.663332993753663, 126.06013044111047, 19.827606765800585, 21.487485649980318, 0.976148265151494, 13.557585216751214, 77.22509525358004, 21.39806652576956, 1.9212054909298717, 121.03472737658248, 107.62094670449166, 10.928105824630965, 34.97405860774786, 4.316875586684115, 98.05244408381373, 46.86479805699736, 9.617922379264977, 54.63679132731661, 10.320067776747358, 13.470428495218314], 'lossList': [0.0, -1.3148776483535767, 0.0, 5.271407681703567, 0.0, 0.0, 0.0], 'rewardMean': 0.7106644239797715, 'totalEpisodes': 307, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.241803832104765, 'successfulTests': 0
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=79.77
