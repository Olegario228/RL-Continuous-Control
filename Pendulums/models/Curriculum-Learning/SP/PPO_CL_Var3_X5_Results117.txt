#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 117
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.8551816578510647, 'errorList': [], 'lossList': [0.0, -1.4427585762739181, 0.0, 27.666921031475066, 0.0, 0.0, 0.0], 'rewardMean': 0.6664837165369315, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 654.8289136538676
'totalSteps': 3840, 'rewardStep': 0.8446777784455471, 'errorList': [], 'lossList': [0.0, -1.4598957425355912, 0.0, 43.18202404499054, 0.0, 0.0, 0.0], 'rewardMean': 0.7258817371731366, 'totalEpisodes': 12, 'stepsPerEpisode': 665, 'rewardPerEpisode': 518.8126580397162
'totalSteps': 5120, 'rewardStep': 0.9323096034487001, 'errorList': [], 'lossList': [0.0, -1.4645884478092193, 0.0, 28.67020777821541, 0.0, 0.0, 0.0], 'rewardMean': 0.7774887037420275, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.0137624023533
'totalSteps': 6400, 'rewardStep': 0.6766635214933084, 'errorList': [], 'lossList': [0.0, -1.4488282346725463, 0.0, 24.244713990688325, 0.0, 0.0, 0.0], 'rewardMean': 0.7573236672922837, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.442708879048
'totalSteps': 7680, 'rewardStep': 0.76952093103018, 'errorList': [], 'lossList': [0.0, -1.428814549446106, 0.0, 14.957035411000252, 0.0, 0.0, 0.0], 'rewardMean': 0.7593565445819331, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.3481950471753
'totalSteps': 8960, 'rewardStep': 0.8357879101967199, 'errorList': [], 'lossList': [0.0, -1.4221413290500642, 0.0, 12.804607255756855, 0.0, 0.0, 0.0], 'rewardMean': 0.7702753110983311, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.7517322957592
'totalSteps': 10240, 'rewardStep': 0.6567615675251444, 'errorList': [], 'lossList': [0.0, -1.410420652627945, 0.0, 524.4330317687989, 0.0, 0.0, 0.0], 'rewardMean': 0.7560860931516828, 'totalEpisodes': 47, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.1301641670160265
'totalSteps': 11520, 'rewardStep': 0.6277836766974452, 'errorList': [], 'lossList': [0.0, -1.4097797226905824, 0.0, 501.37074813842776, 0.0, 0.0, 0.0], 'rewardMean': 0.7418302691012121, 'totalEpisodes': 94, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.024370605808146
'totalSteps': 12800, 'rewardStep': 0.5398769830588519, 'errorList': [], 'lossList': [0.0, -1.4098468631505967, 0.0, 311.3922201538086, 0.0, 0.0, 0.0], 'rewardMean': 0.721634940496976, 'totalEpisodes': 138, 'stepsPerEpisode': 9, 'rewardPerEpisode': 4.781670926114441
'totalSteps': 14080, 'rewardStep': 0.8361813473294002, 'errorList': [], 'lossList': [0.0, -1.4101772397756576, 0.0, 113.9285525894165, 0.0, 0.0, 0.0], 'rewardMean': 0.7574744977076362, 'totalEpisodes': 174, 'stepsPerEpisode': 50, 'rewardPerEpisode': 41.058050573673334
'totalSteps': 15360, 'rewardStep': 0.8382644140434711, 'errorList': [], 'lossList': [0.0, -1.4086025202274322, 0.0, 59.13441854476929, 0.0, 0.0, 0.0], 'rewardMean': 0.7557827733268769, 'totalEpisodes': 220, 'stepsPerEpisode': 34, 'rewardPerEpisode': 29.952300895143697
'totalSteps': 16640, 'rewardStep': 0.6802011122273184, 'errorList': [], 'lossList': [0.0, -1.4110009741783143, 0.0, 32.34523555755615, 0.0, 0.0, 0.0], 'rewardMean': 0.739335106705054, 'totalEpisodes': 249, 'stepsPerEpisode': 58, 'rewardPerEpisode': 49.6374322404893
'totalSteps': 17920, 'rewardStep': 0.8657254307422815, 'errorList': [], 'lossList': [0.0, -1.4139058285951613, 0.0, 25.5072607088089, 0.0, 0.0, 0.0], 'rewardMean': 0.7326766894344121, 'totalEpisodes': 268, 'stepsPerEpisode': 44, 'rewardPerEpisode': 38.54339228281611
'totalSteps': 19200, 'rewardStep': 0.6836552959321012, 'errorList': [], 'lossList': [0.0, -1.4060209381580353, 0.0, 32.03702651500702, 0.0, 0.0, 0.0], 'rewardMean': 0.7333758668782914, 'totalEpisodes': 279, 'stepsPerEpisode': 121, 'rewardPerEpisode': 103.83491977750784
'totalSteps': 20480, 'rewardStep': 0.7715375327555692, 'errorList': [], 'lossList': [0.0, -1.4019471102952956, 0.0, 13.679727296829224, 0.0, 0.0, 0.0], 'rewardMean': 0.7335775270508302, 'totalEpisodes': 284, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.56112445621654
'totalSteps': 21760, 'rewardStep': 0.9234099421351379, 'errorList': [], 'lossList': [0.0, -1.396386461853981, 0.0, 11.915121706724166, 0.0, 0.0, 0.0], 'rewardMean': 0.7423397302446721, 'totalEpisodes': 287, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.229945162979607
'totalSteps': 23040, 'rewardStep': 0.7785126310592186, 'errorList': [], 'lossList': [0.0, -1.3721511721611024, 0.0, 5.933360292315483, 0.0, 0.0, 0.0], 'rewardMean': 0.7545148365980795, 'totalEpisodes': 287, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.1112120438802
'totalSteps': 24320, 'rewardStep': 0.9536003993972268, 'errorList': [0.0965502843376797, 0.09649978295012202, 0.09461494758453963, 0.10017208414770454, 0.11236492400439413, 0.10247299955407123, 0.10336274984749744, 0.097707856990771, 0.09690137928339176, 0.09789357167806571, 0.09628702336389118, 0.10140900971528546, 0.093676608572914, 0.10264434221924196, 0.1021282509685544, 0.10264034756690667, 0.09298889248502824, 0.10340308327425732, 0.09306480981980136, 0.10083278411559411, 0.0936395154766776, 0.0959662702400764, 0.10327207042424857, 0.09346000834793497, 0.09488007273580171, 0.09686849593774093, 0.0985514737140755, 0.10171512526856229, 0.09687171414746226, 0.10470521887578513, 0.09841574874543797, 0.09725627751747341, 0.10975996563423279, 0.09837943124169846, 0.102956288236409, 0.10595377065172376, 0.10276731341105388, 0.0997932094258036, 0.09733094669338001, 0.09429534632285685, 0.09711637040096717, 0.10452611286125567, 0.10310071387628611, 0.10544447522653037, 0.09835862542518749, 0.10872196651284169, 0.09555678014031502, 0.10384424954572219, 0.09694654552208125, 0.10876133698545166], 'lossList': [0.0, -1.3330413436889648, 0.0, 6.032881917208433, 0.0, 0.0, 0.0], 'rewardMean': 0.7870965088680577, 'totalEpisodes': 287, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.156955166682, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=80.25
