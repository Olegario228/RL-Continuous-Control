#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 118
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.584670476446502, 'errorList': [], 'lossList': [0.0, -1.4217237800359726, 0.0, 30.98244956254959, 0.0, 0.0, 0.0], 'rewardMean': 0.7214412925010208, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.43888054737684
'totalSteps': 3840, 'rewardStep': 0.9713302479039428, 'errorList': [], 'lossList': [0.0, -1.4213420575857163, 0.0, 32.05898622393608, 0.0, 0.0, 0.0], 'rewardMean': 0.8047376109686616, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 391.75070343678794
'totalSteps': 5120, 'rewardStep': 0.8353719183171846, 'errorList': [], 'lossList': [0.0, -1.4178190571069718, 0.0, 31.021320879161358, 0.0, 0.0, 0.0], 'rewardMean': 0.8123961878057924, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.0113550030653
'totalSteps': 6400, 'rewardStep': 0.7795229929829277, 'errorList': [], 'lossList': [0.0, -1.3993193382024764, 0.0, 22.714420632123947, 0.0, 0.0, 0.0], 'rewardMean': 0.8058215488412195, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.1704123785955
'totalSteps': 7680, 'rewardStep': 0.9924182127069715, 'errorList': [], 'lossList': [0.0, -1.3810434299707413, 0.0, 17.001547059416772, 0.0, 0.0, 0.0], 'rewardMean': 0.8369209928188449, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.254758044817
'totalSteps': 8960, 'rewardStep': 0.8214960162163865, 'errorList': [], 'lossList': [0.0, -1.3622397935390473, 0.0, 9.143110319972038, 0.0, 0.0, 0.0], 'rewardMean': 0.8347174247327793, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1084.1683044123786
'totalSteps': 10240, 'rewardStep': 0.6278760587266213, 'errorList': [], 'lossList': [0.0, -1.3482986283302307, 0.0, 577.6416798400879, 0.0, 0.0, 0.0], 'rewardMean': 0.8088622539820096, 'totalEpisodes': 53, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.572121367274283
'totalSteps': 11520, 'rewardStep': 0.5901263708211603, 'errorList': [], 'lossList': [0.0, -1.3482134473323821, 0.0, 364.26552520751954, 0.0, 0.0, 0.0], 'rewardMean': 0.7845582669641374, 'totalEpisodes': 86, 'stepsPerEpisode': 46, 'rewardPerEpisode': 38.342773226726834
'totalSteps': 12800, 'rewardStep': 0.602133276846514, 'errorList': [], 'lossList': [0.0, -1.3484231448173523, 0.0, 221.96507934570312, 0.0, 0.0, 0.0], 'rewardMean': 0.7663157679523751, 'totalEpisodes': 131, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.813674871666612
'totalSteps': 14080, 'rewardStep': 0.8341035302789338, 'errorList': [], 'lossList': [0.0, -1.3492155748605728, 0.0, 73.42520668029785, 0.0, 0.0, 0.0], 'rewardMean': 0.7639049101247145, 'totalEpisodes': 165, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.236818461618505
'totalSteps': 15360, 'rewardStep': 0.9519913585168402, 'errorList': [240.83428455160865, 236.55094893973953, 130.43816192768122, 246.95189142473993, 249.9061702755522, 231.63123437729865, 226.50234958738935, 251.19621376286455, 244.02817584711946, 236.28759609219003, 239.7331293454277, 247.02166417605392, 195.0979372250516, 241.9968242665675, 241.73910558909432, 241.43505363426578, 239.57258703164763, 236.90363276709894, 247.22623228229006, 254.23553560827688, 212.44697311647093, 134.57571708026722, 236.59516377180458, 247.05786103455708, 200.18255963591605, 225.4745164908789, 210.94930245033038, 256.9136805820301, 219.58923948088835, 197.4743603505659, 216.00164064839592, 252.94689174845647, 230.21149159581557, 215.04854872020204, 186.1508174188715, 213.13335491952017, 181.09967912889434, 251.09409762099367, 233.67318235578668, 247.0274422095716, 242.5224422355553, 255.88361348886139, 255.91662494640545, 214.37705657756737, 187.05172397687235, 201.81705023044267, 243.9932769450085, 238.35680320749458, 242.00030730165383, 240.08293251929751], 'lossList': [0.0, -1.3499255275726318, 0.0, 24.04346806049347, 0.0, 0.0, 0.0], 'rewardMean': 0.8006369983317482, 'totalEpisodes': 196, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.64319674799782, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.46878910742335506, 'errorList': [], 'lossList': [0.0, -1.3453466749191285, 0.0, 13.735163428783416, 0.0, 0.0, 0.0], 'rewardMean': 0.7503828842836896, 'totalEpisodes': 216, 'stepsPerEpisode': 50, 'rewardPerEpisode': 33.92625596276787
'totalSteps': 17920, 'rewardStep': 0.8893129644739266, 'errorList': [], 'lossList': [0.0, -1.331751121878624, 0.0, 14.766691892147064, 0.0, 0.0, 0.0], 'rewardMean': 0.7557769888993636, 'totalEpisodes': 232, 'stepsPerEpisode': 69, 'rewardPerEpisode': 64.53475479501525
'totalSteps': 19200, 'rewardStep': 0.7263627960220712, 'errorList': [], 'lossList': [0.0, -1.325271676182747, 0.0, 15.926734046936035, 0.0, 0.0, 0.0], 'rewardMean': 0.7504609692032781, 'totalEpisodes': 242, 'stepsPerEpisode': 52, 'rewardPerEpisode': 44.20740797297482
'totalSteps': 20480, 'rewardStep': 0.8603588977937198, 'errorList': [], 'lossList': [0.0, -1.3180943256616593, 0.0, 9.55063595533371, 0.0, 0.0, 0.0], 'rewardMean': 0.7372550377119529, 'totalEpisodes': 250, 'stepsPerEpisode': 84, 'rewardPerEpisode': 74.23882318099682
'totalSteps': 21760, 'rewardStep': 0.9263230778369258, 'errorList': [], 'lossList': [0.0, -1.313698925971985, 0.0, 7.587461193799973, 0.0, 0.0, 0.0], 'rewardMean': 0.7477377438740069, 'totalEpisodes': 256, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.172299267466958
'totalSteps': 23040, 'rewardStep': 0.7327736151711243, 'errorList': [], 'lossList': [0.0, -1.3109153670072555, 0.0, 7.051889171600342, 0.0, 0.0, 0.0], 'rewardMean': 0.7582274995184571, 'totalEpisodes': 259, 'stepsPerEpisode': 274, 'rewardPerEpisode': 220.58147953487284
'totalSteps': 24320, 'rewardStep': 0.4179240692210651, 'errorList': [], 'lossList': [0.0, -1.2861659997701644, 0.0, 6.808670928478241, 0.0, 0.0, 0.0], 'rewardMean': 0.7410072693584476, 'totalEpisodes': 260, 'stepsPerEpisode': 1004, 'rewardPerEpisode': 835.1815254883765
'totalSteps': 25600, 'rewardStep': 0.9753675594974958, 'errorList': [0.07381340046857747, 0.07395317417984004, 0.0725565932956467, 0.07309743679321623, 0.07385845751239029, 0.09873175617172099, 0.07329091312241223, 0.1018389677741999, 0.07295194218494302, 0.07368506888639006, 0.07170654067365617, 0.07334529956303805, 0.12764028226720192, 0.07288665824718134, 0.07367953264940465, 0.08104175124760503, 0.07217682884241491, 0.07382503738322894, 0.07352917235960425, 0.07629590606729242, 0.07371485083815059, 0.07352525983637037, 0.07157973640262863, 0.07334018585860141, 0.07359804178506682, 0.07238250653624972, 0.07252595182278904, 0.08803486958542654, 0.11098356249773657, 0.07387187454877005, 0.07276735821548233, 0.09177887366882381, 0.0739557309113039, 0.07383915031581444, 0.07387140826468787, 0.07365066777383712, 0.07344333727479865, 0.07166716424878183, 0.07248030971656651, 0.0822480616934563, 0.08107304981395043, 0.072901754668644, 0.07369523547481895, 0.10347023497296132, 0.07281479841601515, 0.07370965132329563, 0.09038916293637121, 0.07359040088785138, 0.07364044046875481, 0.07224283034949688], 'lossList': [0.0, -1.2621604907512665, 0.0, 4.722441079318523, 0.0, 0.0, 0.0], 'rewardMean': 0.7783306976235458, 'totalEpisodes': 262, 'stepsPerEpisode': 148, 'rewardPerEpisode': 126.94837539288602, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=101.58
