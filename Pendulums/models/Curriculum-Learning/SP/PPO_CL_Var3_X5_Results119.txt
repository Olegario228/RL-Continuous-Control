#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 119
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8807747561078104, 'errorList': [], 'lossList': [0.0, -1.4239100015163422, 0.0, 34.10936054885387, 0.0, 0.0, 0.0], 'rewardMean': 0.8020628399935557, 'totalEpisodes': 13, 'stepsPerEpisode': 311, 'rewardPerEpisode': 259.51723324065296
'totalSteps': 3840, 'rewardStep': 0.7150598304710174, 'errorList': [], 'lossList': [0.0, -1.4262149101495742, 0.0, 33.55905915737152, 0.0, 0.0, 0.0], 'rewardMean': 0.7730618368193762, 'totalEpisodes': 16, 'stepsPerEpisode': 174, 'rewardPerEpisode': 131.5030522453976
'totalSteps': 5120, 'rewardStep': 0.7079474950036354, 'errorList': [], 'lossList': [0.0, -1.4303288710117341, 0.0, 20.383761838674545, 0.0, 0.0, 0.0], 'rewardMean': 0.756783251365441, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 942.3648661947173
'totalSteps': 6400, 'rewardStep': 0.8924717036945007, 'errorList': [], 'lossList': [0.0, -1.4041253435611725, 0.0, 17.516511427164076, 0.0, 0.0, 0.0], 'rewardMean': 0.7839209418312529, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 975.5063556133056
'totalSteps': 7680, 'rewardStep': 0.8603845930562988, 'errorList': [], 'lossList': [0.0, -1.41244737803936, 0.0, 13.368615590333938, 0.0, 0.0, 0.0], 'rewardMean': 0.796664883702094, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 982.3626345810653
'totalSteps': 8960, 'rewardStep': 0.6220244079463193, 'errorList': [], 'lossList': [0.0, -1.3990350103378295, 0.0, 52.23893964290619, 0.0, 0.0, 0.0], 'rewardMean': 0.7717162443084119, 'totalEpisodes': 19, 'stepsPerEpisode': 1051, 'rewardPerEpisode': 747.4292943124482
'totalSteps': 10240, 'rewardStep': 0.8268681075087151, 'errorList': [], 'lossList': [0.0, -1.392494769692421, 0.0, 420.11180686950684, 0.0, 0.0, 0.0], 'rewardMean': 0.7786102272084497, 'totalEpisodes': 66, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.481170703482179
'totalSteps': 11520, 'rewardStep': 0.8101419233827831, 'errorList': [], 'lossList': [0.0, -1.390235971212387, 0.0, 155.18989170074462, 0.0, 0.0, 0.0], 'rewardMean': 0.7821137490055979, 'totalEpisodes': 117, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.258433356622338
'totalSteps': 12800, 'rewardStep': 0.7294834154900172, 'errorList': [], 'lossList': [0.0, -1.3841472071409227, 0.0, 70.19722972869873, 0.0, 0.0, 0.0], 'rewardMean': 0.7768507156540398, 'totalEpisodes': 156, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.948802599581544
'totalSteps': 14080, 'rewardStep': 0.49811577053315964, 'errorList': [], 'lossList': [0.0, -1.3760054129362107, 0.0, 44.04074295043945, 0.0, 0.0, 0.0], 'rewardMean': 0.7543272003194257, 'totalEpisodes': 191, 'stepsPerEpisode': 26, 'rewardPerEpisode': 18.94050722144915
'totalSteps': 15360, 'rewardStep': 0.5675271293137214, 'errorList': [], 'lossList': [0.0, -1.358035715818405, 0.0, 33.421573419570926, 0.0, 0.0, 0.0], 'rewardMean': 0.7230024376400168, 'totalEpisodes': 210, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.7396318698767654
'totalSteps': 16640, 'rewardStep': 0.522029373176866, 'errorList': [], 'lossList': [0.0, -1.3357510834932327, 0.0, 45.16362480163574, 0.0, 0.0, 0.0], 'rewardMean': 0.7036993919106017, 'totalEpisodes': 227, 'stepsPerEpisode': 83, 'rewardPerEpisode': 67.3363468136608
'totalSteps': 17920, 'rewardStep': 0.6521824863670677, 'errorList': [], 'lossList': [0.0, -1.322372276186943, 0.0, 17.576737971305846, 0.0, 0.0, 0.0], 'rewardMean': 0.6981228910469448, 'totalEpisodes': 238, 'stepsPerEpisode': 51, 'rewardPerEpisode': 40.47354804876062
'totalSteps': 19200, 'rewardStep': 0.33061663117586304, 'errorList': [], 'lossList': [0.0, -1.3116715025901795, 0.0, 11.586111943721772, 0.0, 0.0, 0.0], 'rewardMean': 0.641937383795081, 'totalEpisodes': 244, 'stepsPerEpisode': 148, 'rewardPerEpisode': 100.41483478073407
'totalSteps': 20480, 'rewardStep': 0.7656539766211861, 'errorList': [], 'lossList': [0.0, -1.3036849170923233, 0.0, 13.91089935541153, 0.0, 0.0, 0.0], 'rewardMean': 0.6324643221515698, 'totalEpisodes': 250, 'stepsPerEpisode': 96, 'rewardPerEpisode': 63.54462404059008
'totalSteps': 21760, 'rewardStep': 0.1982491589559932, 'errorList': [], 'lossList': [0.0, -1.3032802736759186, 0.0, 8.689669535160064, 0.0, 0.0, 0.0], 'rewardMean': 0.5900867972525372, 'totalEpisodes': 253, 'stepsPerEpisode': 180, 'rewardPerEpisode': 117.91325005542305
'totalSteps': 23040, 'rewardStep': 0.7542004979281648, 'errorList': [], 'lossList': [0.0, -1.305493419766426, 0.0, 18.6128432738781, 0.0, 0.0, 0.0], 'rewardMean': 0.5828200362944822, 'totalEpisodes': 256, 'stepsPerEpisode': 211, 'rewardPerEpisode': 167.90335413218827
'totalSteps': 24320, 'rewardStep': 0.5604903321903283, 'errorList': [], 'lossList': [0.0, -1.295576764345169, 0.0, 6.021860670447349, 0.0, 0.0, 0.0], 'rewardMean': 0.5578548771752366, 'totalEpisodes': 256, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.5800617439749
'totalSteps': 25600, 'rewardStep': 0.9337140234206489, 'errorList': [0.12472477315742066, 0.11488171509551491, 0.11300610348875882, 0.1149425942816941, 0.11003365021079195, 0.1274794927630683, 0.1296777959135414, 0.118460717194184, 0.12368796039119234, 0.1267018490334878, 0.120781758665298, 0.11045092976760601, 0.11256087677752878, 0.11287844541386934, 0.13006196236321146, 0.1250098258899267, 0.13088046942827838, 0.11832416083626096, 0.11439164562499789, 0.12655582229631737, 0.1277145007584144, 0.11974834461492512, 0.15390700287303868, 0.12302138595643544, 0.10924582050259318, 0.18405496560940116, 0.11383929156898381, 0.11293270993375408, 0.11343850284572886, 0.1255985388941471, 0.12562406295581427, 0.12368906292737249, 0.12248749786866343, 0.11338582535233994, 0.12327357229360023, 0.11413649745446763, 0.111776602862273, 0.13374001350619721, 0.102747223042486, 0.1133948716481919, 0.138789068684826, 0.1169944685972306, 0.12879689012830117, 0.10933737327334517, 0.11836677200197238, 0.12757026274191083, 0.11490585723102528, 0.13553953800129775, 0.11605041700089379, 0.16149138006813119], 'lossList': [0.0, -1.2827141267061233, 0.0, 5.274997403621674, 0.0, 0.0, 0.0], 'rewardMean': 0.5782779379682998, 'totalEpisodes': 257, 'stepsPerEpisode': 135, 'rewardPerEpisode': 110.80202925650936, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.94
