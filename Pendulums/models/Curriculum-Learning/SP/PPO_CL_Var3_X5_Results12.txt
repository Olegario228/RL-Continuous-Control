#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 12
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.8173229205157532, 'errorList': [], 'lossList': [0.0, -1.4439427202939987, 0.0, 25.702661809921263, 0.0, 0.0, 0.0], 'rewardMean': 0.6276767563608135, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.6659326075063
'totalSteps': 3840, 'rewardStep': 0.8890248116460636, 'errorList': [], 'lossList': [0.0, -1.4504529875516892, 0.0, 38.25150670528412, 0.0, 0.0, 0.0], 'rewardMean': 0.7147927747892302, 'totalEpisodes': 14, 'stepsPerEpisode': 485, 'rewardPerEpisode': 388.7112983530536
'totalSteps': 5120, 'rewardStep': 0.6633454505042834, 'errorList': [], 'lossList': [0.0, -1.430121882557869, 0.0, 20.55585546851158, 0.0, 0.0, 0.0], 'rewardMean': 0.7019309437179935, 'totalEpisodes': 15, 'stepsPerEpisode': 67, 'rewardPerEpisode': 46.113297311370914
'totalSteps': 6400, 'rewardStep': 0.8005676725387579, 'errorList': [], 'lossList': [0.0, -1.419344755411148, 0.0, 245.59711959838867, 0.0, 0.0, 0.0], 'rewardMean': 0.7216582894821464, 'totalEpisodes': 79, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.569477707269049
'totalSteps': 7680, 'rewardStep': 0.6300059842114641, 'errorList': [], 'lossList': [0.0, -1.407930194735527, 0.0, 108.87423908233643, 0.0, 0.0, 0.0], 'rewardMean': 0.706382905270366, 'totalEpisodes': 127, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.597756906506766
'totalSteps': 8960, 'rewardStep': 0.8747509345762772, 'errorList': [], 'lossList': [0.0, -1.3855972969532013, 0.0, 65.76073362350463, 0.0, 0.0, 0.0], 'rewardMean': 0.7304354808854961, 'totalEpisodes': 166, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7723383524925125
'totalSteps': 10240, 'rewardStep': 0.45985370633068484, 'errorList': [], 'lossList': [0.0, -1.3760959947109221, 0.0, 67.73430711746215, 0.0, 0.0, 0.0], 'rewardMean': 0.6966127590661447, 'totalEpisodes': 191, 'stepsPerEpisode': 10, 'rewardPerEpisode': 5.970403888622579
'totalSteps': 11520, 'rewardStep': 0.8264540293146696, 'errorList': [], 'lossList': [0.0, -1.3791484010219575, 0.0, 48.51407315254212, 0.0, 0.0, 0.0], 'rewardMean': 0.7110395668715364, 'totalEpisodes': 204, 'stepsPerEpisode': 130, 'rewardPerEpisode': 96.80807928895953
'totalSteps': 12800, 'rewardStep': 0.4715982083000762, 'errorList': [], 'lossList': [0.0, -1.3743658369779588, 0.0, 56.552948455810544, 0.0, 0.0, 0.0], 'rewardMean': 0.6870954310143904, 'totalEpisodes': 216, 'stepsPerEpisode': 107, 'rewardPerEpisode': 73.51832584475912
'totalSteps': 14080, 'rewardStep': 0.42768373139483756, 'errorList': [], 'lossList': [0.0, -1.3664083433151246, 0.0, 30.539663362503052, 0.0, 0.0, 0.0], 'rewardMean': 0.6860607449332867, 'totalEpisodes': 225, 'stepsPerEpisode': 92, 'rewardPerEpisode': 65.06462474437524
'totalSteps': 15360, 'rewardStep': 0.9154652681083763, 'errorList': [], 'lossList': [0.0, -1.3675207817554473, 0.0, 17.38632752418518, 0.0, 0.0, 0.0], 'rewardMean': 0.695874979692549, 'totalEpisodes': 231, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.40124574166298
'totalSteps': 16640, 'rewardStep': 0.7890331863746753, 'errorList': [], 'lossList': [0.0, -1.3756458586454392, 0.0, 11.902825009822845, 0.0, 0.0, 0.0], 'rewardMean': 0.6858758171654104, 'totalEpisodes': 236, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.0540504273121742
'totalSteps': 17920, 'rewardStep': 0.795704354940401, 'errorList': [], 'lossList': [0.0, -1.3814203971624375, 0.0, 8.015841710567475, 0.0, 0.0, 0.0], 'rewardMean': 0.6991117076090221, 'totalEpisodes': 240, 'stepsPerEpisode': 80, 'rewardPerEpisode': 68.8435286855957
'totalSteps': 19200, 'rewardStep': 0.9515005116044426, 'errorList': [27.82495246962474, 0.11717644511713987, 14.64670205258512, 69.57106597500956, 25.712563114341155, 1.6204891430576538, 19.667136810210465, 69.27010228187291, 60.908090792922195, 60.99118188584007, 43.14116706674367, 2.014536037991327, 12.814574611470958, 22.497207200728127, 7.297556398811351, 5.770958658244901, 2.9767799729578606, 64.228173513985, 21.417341754547298, 6.839579925774492, 1.690766249235533, 13.29601230391213, 24.682872940184225, 12.616458268366024, 22.96164929823318, 0.14086284239411667, 9.806193324339896, 46.240851350770555, 0.8281642687209134, 12.174450473804054, 52.802335069291395, 0.9539871636688909, 42.67411312680489, 37.562853949508686, 5.967506423507924, 5.280474226865697, 18.881415587890256, 19.11850846687841, 40.90120512524239, 3.0677414259279256, 2.8995713161595162, 7.654559059487263, 3.6171479129160824, 60.18374275823961, 2.2488531448763114, 1.473840308678376, 4.22561476806068, 0.42005387133141237, 38.02469498147543, 42.38622663947063], 'lossList': [0.0, -1.3635173368453979, 0.0, 13.368249802589416, 0.0, 0.0, 0.0], 'rewardMean': 0.7142049915155905, 'totalEpisodes': 244, 'stepsPerEpisode': 80, 'rewardPerEpisode': 66.92150042472213, 'successfulTests': 2
'totalSteps': 20480, 'rewardStep': 0.8095494591068364, 'errorList': [], 'lossList': [0.0, -1.3464044690132142, 0.0, 12.548621829748154, 0.0, 0.0, 0.0], 'rewardMean': 0.7321593390051278, 'totalEpisodes': 247, 'stepsPerEpisode': 200, 'rewardPerEpisode': 174.43071532609258
'totalSteps': 21760, 'rewardStep': 0.8582046953478094, 'errorList': [], 'lossList': [0.0, -1.33949955701828, 0.0, 4.534353780150414, 0.0, 0.0, 0.0], 'rewardMean': 0.730504715082281, 'totalEpisodes': 251, 'stepsPerEpisode': 156, 'rewardPerEpisode': 142.00879019391874
'totalSteps': 23040, 'rewardStep': 0.7934780542699169, 'errorList': [], 'lossList': [0.0, -1.331837911605835, 0.0, 3.0121354392170905, 0.0, 0.0, 0.0], 'rewardMean': 0.7638671498762041, 'totalEpisodes': 254, 'stepsPerEpisode': 158, 'rewardPerEpisode': 127.20325648608751
'totalSteps': 24320, 'rewardStep': 0.300969055783768, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6942557372714833, 'totalEpisodes': 257, 'stepsPerEpisode': 372, 'rewardPerEpisode': 277.656059580703
'totalSteps': 25600, 'rewardStep': 0.4395553952066351, 'errorList': [], 'lossList': [0.0, -1.3095568454265594, 0.0, 13.034143623411655, 0.0, 0.0, 0.0], 'rewardMean': 0.6954429036526629, 'totalEpisodes': 259, 'stepsPerEpisode': 566, 'rewardPerEpisode': 448.6112071965161
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=19200, timeSpent=71.56
