#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 123
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5743653851910347, 'errorList': [], 'lossList': [0.0, -1.422778044939041, 0.0, 32.65632889866829, 0.0, 0.0, 0.0], 'rewardMean': 0.7366479005088875, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.26866794583453
'totalSteps': 3840, 'rewardStep': 0.9642502227954737, 'errorList': [], 'lossList': [0.0, -1.4228160285949707, 0.0, 36.04677828073502, 0.0, 0.0, 0.0], 'rewardMean': 0.8125153412710828, 'totalEpisodes': 18, 'stepsPerEpisode': 486, 'rewardPerEpisode': 404.430119092943
'totalSteps': 5120, 'rewardStep': 0.8503086521215835, 'errorList': [], 'lossList': [0.0, -1.4184568899869918, 0.0, 32.89324400365353, 0.0, 0.0, 0.0], 'rewardMean': 0.821963668983708, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1095.5894308891902
'totalSteps': 6400, 'rewardStep': 0.8262233730123325, 'errorList': [], 'lossList': [0.0, -1.4022026842832565, 0.0, 24.574208799004555, 0.0, 0.0, 0.0], 'rewardMean': 0.8228156097894329, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.3864700945237
'totalSteps': 7680, 'rewardStep': 0.9712371207689986, 'errorList': [], 'lossList': [0.0, -1.390044018626213, 0.0, 18.262330727875234, 0.0, 0.0, 0.0], 'rewardMean': 0.8475525282860272, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.0288439340113
'totalSteps': 8960, 'rewardStep': 0.8054499249970841, 'errorList': [], 'lossList': [0.0, -1.373518744111061, 0.0, 9.007008238136768, 0.0, 0.0, 0.0], 'rewardMean': 0.841537870673321, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.062134087738
'totalSteps': 10240, 'rewardStep': 0.9879769949131716, 'errorList': [82.02952898128984, 74.9688582379479, 76.77839718390825, 82.74481897266065, 77.33210207214235, 83.59662932572361, 78.82693667822166, 77.86160620655826, 80.6008234821374, 83.77698151138193, 77.81521338283723, 82.71266384854499, 82.82282488533588, 82.64417336698625, 80.74337209217597, 81.63724351795626, 82.96828731579461, 81.17591469497262, 85.64707873473863, 85.75153282630511, 83.73399520471135, 77.6091263890538, 82.21339000689173, 81.69239310626924, 82.52569440423106, 84.06172649306127, 80.33250148109789, 81.31696191719188, 83.6309051754163, 75.04985797664727, 76.85663843354006, 82.52047652197199, 68.78940202922146, 83.33440489891088, 80.52175014812137, 83.94155257383957, 75.561802229517, 80.02073764135493, 82.21243553230637, 75.47167188644151, 83.141831921075, 64.74219587455659, 79.13546856514562, 76.22750021745297, 82.68290982129712, 82.79270923727267, 82.3410631141082, 82.00625861494817, 83.92527183746081, 80.85939811987804], 'lossList': [0.0, -1.360704231262207, 0.0, 669.8179356384277, 0.0, 0.0, 0.0], 'rewardMean': 0.8598427612033024, 'totalEpisodes': 57, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.27749725544112, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8569466581816455, 'errorList': [], 'lossList': [0.0, -1.3603497123718262, 0.0, 489.86473808288576, 0.0, 0.0, 0.0], 'rewardMean': 0.8595209719786738, 'totalEpisodes': 97, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.741609074620057
'totalSteps': 12800, 'rewardStep': 0.7982759238153405, 'errorList': [], 'lossList': [0.0, -1.360955803990364, 0.0, 269.35318058013917, 0.0, 0.0, 0.0], 'rewardMean': 0.8533964671623405, 'totalEpisodes': 137, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.255139925974037
'totalSteps': 14080, 'rewardStep': 0.5063424562151768, 'errorList': [], 'lossList': [0.0, -1.3622076672315597, 0.0, 57.649671363830564, 0.0, 0.0, 0.0], 'rewardMean': 0.8141376712011841, 'totalEpisodes': 170, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.5217679776772863
'totalSteps': 15360, 'rewardStep': 0.48512469758973087, 'errorList': [], 'lossList': [0.0, -1.3587044471502303, 0.0, 42.03851895332336, 0.0, 0.0, 0.0], 'rewardMean': 0.8052136024410539, 'totalEpisodes': 202, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.640251796304135
'totalSteps': 16640, 'rewardStep': 0.6885291413624317, 'errorList': [], 'lossList': [0.0, -1.355548183321953, 0.0, 28.76480391025543, 0.0, 0.0, 0.0], 'rewardMean': 0.7776414942977496, 'totalEpisodes': 232, 'stepsPerEpisode': 39, 'rewardPerEpisode': 30.798040990458873
'totalSteps': 17920, 'rewardStep': 0.9177434068988645, 'errorList': [], 'lossList': [0.0, -1.3476296293735504, 0.0, 19.79238353729248, 0.0, 0.0, 0.0], 'rewardMean': 0.7843849697754777, 'totalEpisodes': 247, 'stepsPerEpisode': 36, 'rewardPerEpisode': 33.272494181920415
'totalSteps': 19200, 'rewardStep': 0.7615947735506149, 'errorList': [], 'lossList': [0.0, -1.3375343865156173, 0.0, 30.197814359664918, 0.0, 0.0, 0.0], 'rewardMean': 0.777922109829306, 'totalEpisodes': 258, 'stepsPerEpisode': 76, 'rewardPerEpisode': 62.286886092461124
'totalSteps': 20480, 'rewardStep': 0.8474266183800371, 'errorList': [], 'lossList': [0.0, -1.3258187621831894, 0.0, 13.159156861305236, 0.0, 0.0, 0.0], 'rewardMean': 0.7655410595904099, 'totalEpisodes': 267, 'stepsPerEpisode': 42, 'rewardPerEpisode': 36.55022960726411
'totalSteps': 21760, 'rewardStep': 0.47051878454144036, 'errorList': [], 'lossList': [0.0, -1.3195174384117125, 0.0, 9.946334050893784, 0.0, 0.0, 0.0], 'rewardMean': 0.7320479455448454, 'totalEpisodes': 273, 'stepsPerEpisode': 178, 'rewardPerEpisode': 143.9043004760536
'totalSteps': 23040, 'rewardStep': 0.8328244263420463, 'errorList': [], 'lossList': [0.0, -1.3092927610874177, 0.0, 7.220626301765442, 0.0, 0.0, 0.0], 'rewardMean': 0.7165326886877328, 'totalEpisodes': 279, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.380005295529461
'totalSteps': 24320, 'rewardStep': 0.5939776821603794, 'errorList': [], 'lossList': [0.0, -1.2840356087684632, 0.0, 9.017727327346801, 0.0, 0.0, 0.0], 'rewardMean': 0.6902357910856063, 'totalEpisodes': 281, 'stepsPerEpisode': 147, 'rewardPerEpisode': 122.22576593819211
'totalSteps': 25600, 'rewardStep': 0.902128397145798, 'errorList': [], 'lossList': [0.0, -1.2427759760618209, 0.0, 5.294824100136757, 0.0, 0.0, 0.0], 'rewardMean': 0.700621038418652, 'totalEpisodes': 281, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1062.6100710603248
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.4
