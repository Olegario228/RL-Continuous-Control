#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 9000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 124
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 9000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8201731908327831, 'errorList': [], 'lossList': [0.0, -1.4210875189304353, 0.0, 28.443773643970488, 0.0, 0.0, 0.0], 'rewardMean': 0.7847320275660321, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9722527192322
'totalSteps': 3840, 'rewardStep': 0.7652991862219564, 'errorList': [], 'lossList': [0.0, -1.4348080033063888, 0.0, 35.20321884870529, 0.0, 0.0, 0.0], 'rewardMean': 0.7782544137846735, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.41326545455007
'totalSteps': 5120, 'rewardStep': 0.7029229705351443, 'errorList': [], 'lossList': [0.0, -1.4309562581777573, 0.0, 25.981035161614418, 0.0, 0.0, 0.0], 'rewardMean': 0.7594215529722912, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.5197987267121
'totalSteps': 6400, 'rewardStep': 0.9074940561906198, 'errorList': [], 'lossList': [0.0, -1.4246027535200119, 0.0, 18.64153469443321, 0.0, 0.0, 0.0], 'rewardMean': 0.789036053615957, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1016.5370618915229
'totalSteps': 7680, 'rewardStep': 0.8521970820976548, 'errorList': [], 'lossList': [0.0, -1.428161770105362, 0.0, 13.622619956731796, 0.0, 0.0, 0.0], 'rewardMean': 0.79956289169624, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1023.887510948363
'totalSteps': 8960, 'rewardStep': 0.6561914119868034, 'errorList': [], 'lossList': [0.0, -1.4152700418233872, 0.0, 21.43304598867893, 0.0, 0.0, 0.0], 'rewardMean': 0.7790812517377491, 'totalEpisodes': 15, 'stepsPerEpisode': 1126, 'rewardPerEpisode': 867.4483723162006
'totalSteps': 10240, 'rewardStep': 0.5500829593272624, 'errorList': [], 'lossList': [0.0, -1.4096749848127366, 0.0, 558.0011293029785, 0.0, 0.0, 0.0], 'rewardMean': 0.7504564651864382, 'totalEpisodes': 59, 'stepsPerEpisode': 19, 'rewardPerEpisode': 13.047042284433346
'totalSteps': 11520, 'rewardStep': 0.5069294625410756, 'errorList': [], 'lossList': [0.0, -1.4082223922014236, 0.0, 292.4657312011719, 0.0, 0.0, 0.0], 'rewardMean': 0.7233979093369535, 'totalEpisodes': 106, 'stepsPerEpisode': 34, 'rewardPerEpisode': 23.655931958358703
'totalSteps': 12800, 'rewardStep': 0.7028692590750284, 'errorList': [], 'lossList': [0.0, -1.4039922326803207, 0.0, 117.55877117156983, 0.0, 0.0, 0.0], 'rewardMean': 0.721345044310761, 'totalEpisodes': 152, 'stepsPerEpisode': 24, 'rewardPerEpisode': 18.97297526066871
'totalSteps': 14080, 'rewardStep': 0.45154013946321914, 'errorList': [], 'lossList': [0.0, -1.3948223692178727, 0.0, 61.726306705474855, 0.0, 0.0, 0.0], 'rewardMean': 0.6915699718271549, 'totalEpisodes': 186, 'stepsPerEpisode': 41, 'rewardPerEpisode': 24.419751939552754
'totalSteps': 15360, 'rewardStep': 0.956317611179883, 'errorList': [178.6050273616785, 172.10508949263786, 181.81895714484855, 185.35402358698482, 124.20896263908301, 157.8216290741192, 163.8768216685223, 141.2965991238777, 191.68144978576913, 154.63490588743025, 156.79157701826557, 154.33180710516044, 175.44504506240008, 173.4426440899474, 186.81762378698426, 152.35770948334766, 186.3228865007472, 129.65652743869555, 166.10243177440825, 153.4135279065671, 160.59789390505932, 184.79212202331138, 185.73395731286988, 188.96080062069282, 160.0458443752194, 139.54936565553803, 8.420606713204444, 180.03611828030165, 171.19931138085357, 117.29282792456233, 183.94431402129933, 179.5934264457302, 90.73926587406068, 110.80201446194638, 124.11941436141028, 173.24297080766866, 166.54194570180746, 155.61676390288835, 178.98445961795034, 122.94070077621755, 180.85592716000085, 189.7838244661596, 125.4435814198427, 186.3557674559514, 164.1297240933224, 176.842478544163, 179.90729910845863, 174.59016061368357, 189.38914440619357, 175.1885150405877], 'lossList': [0.0, -1.3861040806770324, 0.0, 52.69341679573059, 0.0, 0.0, 0.0], 'rewardMean': 0.7051844138618647, 'totalEpisodes': 209, 'stepsPerEpisode': 66, 'rewardPerEpisode': 61.71971329389599, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6545027266420385, 'errorList': [], 'lossList': [0.0, -1.3763363480567932, 0.0, 50.20607694625855, 0.0, 0.0, 0.0], 'rewardMean': 0.694104767903873, 'totalEpisodes': 228, 'stepsPerEpisode': 149, 'rewardPerEpisode': 107.13157046283942
'totalSteps': 17920, 'rewardStep': 0.42552791618474295, 'errorList': [], 'lossList': [0.0, -1.3555539798736573, 0.0, 27.484578866958618, 0.0, 0.0, 0.0], 'rewardMean': 0.6663652624688329, 'totalEpisodes': 240, 'stepsPerEpisode': 102, 'rewardPerEpisode': 65.03160290253604
'totalSteps': 19200, 'rewardStep': 0.6648043972446037, 'errorList': [], 'lossList': [0.0, -1.3309661626815796, 0.0, 34.431424083709715, 0.0, 0.0, 0.0], 'rewardMean': 0.6420962965742312, 'totalEpisodes': 250, 'stepsPerEpisode': 150, 'rewardPerEpisode': 126.27765680135066
'totalSteps': 20480, 'rewardStep': 0.8956497632782692, 'errorList': [], 'lossList': [0.0, -1.3190347367525102, 0.0, 12.864399256706237, 0.0, 0.0, 0.0], 'rewardMean': 0.6464415646922926, 'totalEpisodes': 255, 'stepsPerEpisode': 203, 'rewardPerEpisode': 165.11472241783886
'totalSteps': 21760, 'rewardStep': 0.6935831414992542, 'errorList': [], 'lossList': [0.0, -1.3025311732292175, 0.0, 9.403951625823975, 0.0, 0.0, 0.0], 'rewardMean': 0.6501807376435378, 'totalEpisodes': 260, 'stepsPerEpisode': 127, 'rewardPerEpisode': 108.06638604533192
'totalSteps': 23040, 'rewardStep': 0.41067657604421365, 'errorList': [], 'lossList': [0.0, -1.2831707221269608, 0.0, 37.86105747222901, 0.0, 0.0, 0.0], 'rewardMean': 0.6362400993152328, 'totalEpisodes': 266, 'stepsPerEpisode': 250, 'rewardPerEpisode': 185.34824378354628
'totalSteps': 24320, 'rewardStep': 0.7093432309017352, 'errorList': [], 'lossList': [0.0, -1.276456423997879, 0.0, 6.289623480439186, 0.0, 0.0, 0.0], 'rewardMean': 0.6564814761512988, 'totalEpisodes': 271, 'stepsPerEpisode': 110, 'rewardPerEpisode': 98.88027157810497
'totalSteps': 25600, 'rewardStep': 0.9551365351465516, 'errorList': [3.5441795233732383, 0.6252676784107023, 0.07548623841196149, 1.464304625889716, 1.91609131861798, 0.7397313912795717, 0.5981607060603954, 5.6312467107564, 0.7894846879028925, 0.23330638852482377, 0.18869769613636858, 5.656342251183241, 5.16948331559309, 0.2659229915047382, 1.7742041093209195, 0.6108896930992962, 1.8103286614370087, 6.3827445621913705, 1.9499956524534818, 0.2170113735677021, 0.3752933850840759, 0.7549016709435128, 0.08630326158815782, 0.8555835205092033, 0.8441559298222868, 2.919862256538518, 5.598084983445949, 0.22718620779072568, 0.43161517405770367, 3.630852376661125, 0.3206270410699327, 2.4900792113760497, 0.581833863361967, 3.7041187028504647, 2.092212897456478, 0.21121109242964817, 2.9694396320561145, 6.19005463619837, 3.476768742489976, 1.1604919164350613, 1.3217778093899477, 9.42284915391981, 0.05524348187205489, 2.2162868842368044, 0.7101662859431217, 0.8086079138097425, 3.465236862900458, 2.915011963370744, 1.2564731330366985, 1.6031481214922019], 'lossList': [0.0, -1.2837203627824783, 0.0, 4.947856376767159, 0.0, 0.0, 0.0], 'rewardMean': 0.6817082037584512, 'totalEpisodes': 273, 'stepsPerEpisode': 212, 'rewardPerEpisode': 190.10814478261224, 'successfulTests': 4
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=25600, timeSpent=101.72
