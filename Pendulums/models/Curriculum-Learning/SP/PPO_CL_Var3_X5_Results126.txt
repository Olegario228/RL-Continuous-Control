#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 126
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980766922056406, 'errorList': [], 'lossList': [0.0, -1.4175031673908234, 0.0, 30.411768112182617, 0.0, 0.0, 0.0], 'rewardMean': 0.7065634125235241, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.343494911843784
'totalSteps': 3840, 'rewardStep': 0.7335093433000681, 'errorList': [], 'lossList': [0.0, -1.405883014202118, 0.0, 37.44174638271332, 0.0, 0.0, 0.0], 'rewardMean': 0.7155453894490388, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.246030202015504
'totalSteps': 5120, 'rewardStep': 0.7140839287409158, 'errorList': [], 'lossList': [0.0, -1.403234094977379, 0.0, 25.380862419605254, 0.0, 0.0, 0.0], 'rewardMean': 0.715180024272008, 'totalEpisodes': 75, 'stepsPerEpisode': 36, 'rewardPerEpisode': 25.150373162803646
'totalSteps': 6400, 'rewardStep': 0.5274162349025555, 'errorList': [], 'lossList': [0.0, -1.3946526181697845, 0.0, 28.19766307592392, 0.0, 0.0, 0.0], 'rewardMean': 0.6776272663981174, 'totalEpisodes': 79, 'stepsPerEpisode': 80, 'rewardPerEpisode': 63.744402637797116
'totalSteps': 7680, 'rewardStep': 0.6574219168221676, 'errorList': [], 'lossList': [0.0, -1.3831075674295426, 0.0, 11.714487302303315, 0.0, 0.0, 0.0], 'rewardMean': 0.6742597081354592, 'totalEpisodes': 79, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.6568551093467
'totalSteps': 8960, 'rewardStep': 0.7998339238379183, 'errorList': [], 'lossList': [0.0, -1.3718643581867218, 0.0, 21.97117555141449, 0.0, 0.0, 0.0], 'rewardMean': 0.6921988818072391, 'totalEpisodes': 80, 'stepsPerEpisode': 448, 'rewardPerEpisode': 352.0823276568193
'totalSteps': 10240, 'rewardStep': 0.8027847957461565, 'errorList': [], 'lossList': [0.0, -1.3503885036706924, 0.0, 11.273152652978897, 0.0, 0.0, 0.0], 'rewardMean': 0.7060221210496037, 'totalEpisodes': 81, 'stepsPerEpisode': 56, 'rewardPerEpisode': 40.04219377791705
'totalSteps': 11520, 'rewardStep': 0.9691356130354826, 'errorList': [189.1292471529432, 207.69376035106598, 194.20307845863704, 201.8171666184935, 213.34128976029436, 177.12133864595603, 169.48504871924817, 165.52803039118302, 176.14735176831877, 2.0547285370565165, 178.82197025119953, 114.87682088586239, 108.51914250055586, 83.36605909966458, 154.97271924244964, 128.4773670430141, 209.56938659554592, 183.88551667067506, 186.7861725905618, 105.4897402682202, 255.2459723146475, 223.678659087162, 154.42646003493803, 210.77306713864093, 122.1798424649144, 229.43256189392056, 103.75227702596925, 126.8598902327802, 203.26047953982794, 202.2784276877255, 192.44149088683488, 152.44967675231928, 99.40991560773254, 174.91595962974847, 227.0357688104729, 112.29233118139969, 108.63381621012648, 162.27588246560697, 85.02843498136662, 234.33032484704847, 133.05175613219316, 206.5276211998603, 231.59660178030427, 164.48763796626247, 112.07698990270562, 178.37780106548072, 212.32794884256026, 201.5224877787122, 176.7527202953155, 233.75270986344785], 'lossList': [0.0, -1.336937712430954, 0.0, 123.35818481445312, 0.0, 0.0, 0.0], 'rewardMean': 0.7352569534924791, 'totalEpisodes': 97, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.886656084229758, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.957775622134856, 'errorList': [8.589275230193975, 8.96758340863154, 10.398290282293493, 7.257473673277664, 2.9541626855669794, 7.750274603175802, 8.838527398948852, 10.94406574693505, 10.229569971582595, 10.94035886658742, 4.902380482339265, 9.827162329524278, 10.96340106799074, 9.852181738179443, 7.145194476053982, 9.644894580245166, 10.7903428768061, 7.7076148468781795, 8.645910445735954, 10.838891358847608, 2.9797385809636414, 10.849556607151179, 2.6248423126958755, 10.866175044444772, 6.891573243085141, 10.151016116439381, 10.95286853105778, 9.038412112894772, 9.394909474960272, 10.968317192182804, 10.867121434493006, 1.8048259733313456, 0.7788449870127432, 10.837947798351614, 5.932026232219363, 1.0949317667137275, 10.745173470654754, 10.37648090310362, 10.943297341504955, 4.1527726104998095, 9.299369294385809, 9.873860542826833, 7.6354996651180755, 4.299664558370446, 10.660119259465102, 10.09185168053882, 10.14387393023678, 10.96832113602968, 10.631037977147818, 10.498878480057108], 'lossList': [0.0, -1.341148384809494, 0.0, 80.18571063995361, 0.0, 0.0, 0.0], 'rewardMean': 0.7575088203567168, 'totalEpisodes': 110, 'stepsPerEpisode': 43, 'rewardPerEpisode': 36.0212621950037, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.48523241448592097, 'errorList': [], 'lossList': [0.0, -1.3379831057786942, 0.0, 28.740414962768554, 0.0, 0.0, 0.0], 'rewardMean': 0.7245270485211682, 'totalEpisodes': 117, 'stepsPerEpisode': 225, 'rewardPerEpisode': 166.72832847426943
'totalSteps': 15360, 'rewardStep': 0.8425261302193688, 'errorList': [], 'lossList': [0.0, -1.3445497280359269, 0.0, 7.368434416055679, 0.0, 0.0, 0.0], 'rewardMean': 0.748971992322541, 'totalEpisodes': 121, 'stepsPerEpisode': 190, 'rewardPerEpisode': 152.3128714892091
'totalSteps': 16640, 'rewardStep': 0.628162138843428, 'errorList': [], 'lossList': [0.0, -1.3361714529991149, 0.0, 9.14952436208725, 0.0, 0.0, 0.0], 'rewardMean': 0.738437271876877, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 848.0600127847285
'totalSteps': 17920, 'rewardStep': 0.6959086928078559, 'errorList': [], 'lossList': [0.0, -1.3148242044448852, 0.0, 22.420672233104707, 0.0, 0.0, 0.0], 'rewardMean': 0.736619748283571, 'totalEpisodes': 122, 'stepsPerEpisode': 926, 'rewardPerEpisode': 534.9458850554449
'totalSteps': 19200, 'rewardStep': 0.9230772851076489, 'errorList': [], 'lossList': [0.0, -1.286224964261055, 0.0, 2.9512028658390044, 0.0, 0.0, 0.0], 'rewardMean': 0.7761858533040804, 'totalEpisodes': 122, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 941.388166865291
'totalSteps': 20480, 'rewardStep': 0.7949558320949024, 'errorList': [], 'lossList': [0.0, -1.22757015645504, 0.0, 2.108939503580332, 0.0, 0.0, 0.0], 'rewardMean': 0.7899392448313538, 'totalEpisodes': 122, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.7306949121728
'totalSteps': 21760, 'rewardStep': 0.7618634529129511, 'errorList': [], 'lossList': [0.0, -1.1691173726320268, 0.0, 2.1876620915532112, 0.0, 0.0, 0.0], 'rewardMean': 0.786142197738857, 'totalEpisodes': 122, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1134.6808010102616
'totalSteps': 23040, 'rewardStep': 0.9344234774967026, 'errorList': [0.08504373799399774, 0.08064228961444118, 0.0927520910154748, 0.07263612420115757, 0.09553289397417827, 0.10342758776915702, 0.07295651618262418, 0.09741094887429151, 0.09056775040327687, 0.09609117703725883, 0.1021406309503377, 0.08901745152689507, 0.0857262946416916, 0.1099180441338252, 0.08537105239943046, 0.07122537418872554, 0.09355044872847719, 0.07114142123106747, 0.08074886386630589, 0.10733398173764369, 0.1035173876961629, 0.09017068273585992, 0.11852369641031077, 0.10608801881253536, 0.09082795561462356, 0.10188769549172964, 0.08796598167071513, 0.07537636233599465, 0.10148588418741163, 0.10535466701311476, 0.10908724552134927, 0.10724033701246254, 0.08562517078513023, 0.08914564065457525, 0.10572152686190572, 0.1034018990103322, 0.09232832651286525, 0.08626417775720267, 0.08987084744328139, 0.09739301991996457, 0.07261526463629141, 0.09007242856574006, 0.11969307117465082, 0.10957353914601671, 0.09766228820182192, 0.07048414270807973, 0.09657304394677615, 0.07786290335170244, 0.07786070155884157, 0.09009412468428046], 'lossList': [0.0, -1.127328936457634, 0.0, 1.1636999349296093, 0.0, 0.0, 0.0], 'rewardMean': 0.7993060659139118, 'totalEpisodes': 122, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.6716391843543, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=115.1
