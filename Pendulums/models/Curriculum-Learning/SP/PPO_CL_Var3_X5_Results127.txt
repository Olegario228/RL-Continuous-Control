#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 127
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092944679997932, 'errorList': [], 'lossList': [0.0, -1.423095936179161, 0.0, 34.861028623580935, 0.0, 0.0, 0.0], 'rewardMean': 0.8624659048917513, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.766608314541353
'totalSteps': 3840, 'rewardStep': 0.5569519115497429, 'errorList': [], 'lossList': [0.0, -1.4135958170890808, 0.0, 33.45046165466309, 0.0, 0.0, 0.0], 'rewardMean': 0.7606279071110819, 'totalEpisodes': 82, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.981091767011876
'totalSteps': 5120, 'rewardStep': 0.3958475191803854, 'errorList': [], 'lossList': [0.0, -1.3943838667869568, 0.0, 22.372482113838196, 0.0, 0.0, 0.0], 'rewardMean': 0.6694328101284077, 'totalEpisodes': 91, 'stepsPerEpisode': 209, 'rewardPerEpisode': 110.03978597874617
'totalSteps': 6400, 'rewardStep': 0.5779639620613548, 'errorList': [], 'lossList': [0.0, -1.379915582537651, 0.0, 53.445106725692746, 0.0, 0.0, 0.0], 'rewardMean': 0.651139040514997, 'totalEpisodes': 100, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5779639620613548
'totalSteps': 7680, 'rewardStep': 0.7844705457191119, 'errorList': [], 'lossList': [0.0, -1.3730674755573273, 0.0, 46.2739671087265, 0.0, 0.0, 0.0], 'rewardMean': 0.6733609580490162, 'totalEpisodes': 106, 'stepsPerEpisode': 350, 'rewardPerEpisode': 264.2729749901153
'totalSteps': 8960, 'rewardStep': 0.6423007999329344, 'errorList': [], 'lossList': [0.0, -1.3721534836292266, 0.0, 30.88586230993271, 0.0, 0.0, 0.0], 'rewardMean': 0.6689237926038617, 'totalEpisodes': 110, 'stepsPerEpisode': 135, 'rewardPerEpisode': 106.34111876454959
'totalSteps': 10240, 'rewardStep': 0.7208357404567971, 'errorList': [], 'lossList': [0.0, -1.3709995579719543, 0.0, 12.651411695480347, 0.0, 0.0, 0.0], 'rewardMean': 0.6754127860854786, 'totalEpisodes': 115, 'stepsPerEpisode': 40, 'rewardPerEpisode': 35.20708110674619
'totalSteps': 11520, 'rewardStep': 0.7881822562799985, 'errorList': [], 'lossList': [0.0, -1.3743489378690719, 0.0, 216.68723678588867, 0.0, 0.0, 0.0], 'rewardMean': 0.687942727218203, 'totalEpisodes': 139, 'stepsPerEpisode': 71, 'rewardPerEpisode': 60.448260307470804
'totalSteps': 12800, 'rewardStep': 0.3952429986015686, 'errorList': [], 'lossList': [0.0, -1.3732657593488693, 0.0, 64.67755671501159, 0.0, 0.0, 0.0], 'rewardMean': 0.6586727543565396, 'totalEpisodes': 159, 'stepsPerEpisode': 61, 'rewardPerEpisode': 32.872102647437984
'totalSteps': 14080, 'rewardStep': 0.9373237398006281, 'errorList': [1.3683519441740195, 1.6947199171516971, 1.185895412035757, 0.4783867058953808, 0.5471044393449203, 0.810124435994632, 0.8865309039887701, 2.074847925504462, 1.3794844847991528, 1.611583733814537, 2.013619622699654, 0.4300631573615467, 1.0972291937496006, 0.9296029931574326, 2.0450365977515186, 0.8981814763658487, 1.5909448014517316, 0.6147432891698322, 1.5530964543330292, 2.1827793378462403, 0.4873390482963884, 1.8504165875475114, 0.6313620901080885, 2.4593910839033537, 1.547512425750227, 0.7640381768697175, 0.9816123282391995, 0.9144041756291164, 0.22584941400373487, 0.4735423719124922, 1.209019484705607, 1.688503615442249, 0.7273594213711172, 0.9539476943424212, 1.6100378783004465, 0.5175434867467209, 0.8133625706806493, 1.183282325161537, 0.57740103044739, 1.2812255997666213, 1.156324467040818, 0.8092097885587473, 1.632338065629341, 2.0448286971967646, 1.901564653404363, 2.2920147208967867, 2.445302633386275, 1.005684702559978, 1.6705460347768661, 1.7971813099623042], 'lossList': [0.0, -1.3634647083282472, 0.0, 19.48233295440674, 0.0, 0.0, 0.0], 'rewardMean': 0.6708413941582314, 'totalEpisodes': 168, 'stepsPerEpisode': 41, 'rewardPerEpisode': 32.61935993775624, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.8649265473563224, 'errorList': [], 'lossList': [0.0, -1.3519828820228577, 0.0, 27.056508550643922, 0.0, 0.0, 0.0], 'rewardMean': 0.6664046020938844, 'totalEpisodes': 171, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.89470761902631
'totalSteps': 16640, 'rewardStep': 0.7070903829954176, 'errorList': [], 'lossList': [0.0, -1.3535430473089218, 0.0, 8.66726062297821, 0.0, 0.0, 0.0], 'rewardMean': 0.6814184492384519, 'totalEpisodes': 172, 'stepsPerEpisode': 142, 'rewardPerEpisode': 105.09123733499408
'totalSteps': 17920, 'rewardStep': 0.6837138605457631, 'errorList': [], 'lossList': [0.0, -1.357289769053459, 0.0, 6.265534752607346, 0.0, 0.0, 0.0], 'rewardMean': 0.7102050833749896, 'totalEpisodes': 173, 'stepsPerEpisode': 1176, 'rewardPerEpisode': 847.2136706870294
'totalSteps': 19200, 'rewardStep': 0.9480272488836833, 'errorList': [0.07782864758306114, 0.06831747186981124, 0.1495875124256375, 0.10039173684871906, 0.07415535618771042, 0.08234449377937848, 0.15404154183211277, 0.12567333961462776, 0.08577945797678506, 0.05982937901230067, 0.09930337986374814, 0.08979368998514282, 0.0643726280498424, 0.06610435657592242, 0.06254125218879088, 0.0591697631026927, 0.16160154318911066, 0.13891205873562273, 0.07389930402511248, 0.08437517120115964, 0.07681829572448369, 0.0736308031977843, 0.15329333147139737, 0.06798457007499804, 0.06870461658891427, 0.05364981271696453, 0.06844810543506183, 0.05931370316323196, 0.06437667225275462, 0.07016903930381733, 0.058233830103491015, 0.13167322009212293, 0.048254933152203826, 0.11326518435178737, 0.08782966706593447, 0.1138466377058068, 0.04762041774986913, 0.13600292649463303, 0.07847420137843186, 0.06908801621434758, 0.06841757050814491, 0.08229643706950401, 0.07873437757912932, 0.12496609994413471, 0.09828517807887342, 0.0682137974020858, 0.08002409506475876, 0.056733000961291705, 0.05160755438584317, 0.08009814057078742], 'lossList': [0.0, -1.3225966000556946, 0.0, 14.846675659418105, 0.0, 0.0, 0.0], 'rewardMean': 0.7472114120572225, 'totalEpisodes': 174, 'stepsPerEpisode': 756, 'rewardPerEpisode': 642.3622665557684, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=85.98
