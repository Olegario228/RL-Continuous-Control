#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 129
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764211320772066, 'errorList': [], 'lossList': [0.0, -1.4117758798599243, 0.0, 34.56691107749939, 0.0, 0.0, 0.0], 'rewardMean': 0.8487556331143069, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.540071737387237
'totalSteps': 3840, 'rewardStep': 0.8314133703754654, 'errorList': [], 'lossList': [0.0, -1.415155175924301, 0.0, 39.141159563064576, 0.0, 0.0, 0.0], 'rewardMean': 0.8429748788680264, 'totalEpisodes': 84, 'stepsPerEpisode': 56, 'rewardPerEpisode': 44.851622698779664
'totalSteps': 5120, 'rewardStep': 0.7656029326677253, 'errorList': [], 'lossList': [0.0, -1.4122928673028945, 0.0, 27.02300934791565, 0.0, 0.0, 0.0], 'rewardMean': 0.8236318923179511, 'totalEpisodes': 89, 'stepsPerEpisode': 144, 'rewardPerEpisode': 114.3902840464416
'totalSteps': 6400, 'rewardStep': 0.8721392433282699, 'errorList': [], 'lossList': [0.0, -1.4099507975578307, 0.0, 33.53543915748596, 0.0, 0.0, 0.0], 'rewardMean': 0.8333333625200149, 'totalEpisodes': 95, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.430298540029213
'totalSteps': 7680, 'rewardStep': 0.6219215881017356, 'errorList': [], 'lossList': [0.0, -1.394019491672516, 0.0, 18.90658831000328, 0.0, 0.0, 0.0], 'rewardMean': 0.798098066783635, 'totalEpisodes': 97, 'stepsPerEpisode': 428, 'rewardPerEpisode': 320.0461487869775
'totalSteps': 8960, 'rewardStep': 0.6839040496557205, 'errorList': [], 'lossList': [0.0, -1.376902871131897, 0.0, 32.182437039613724, 0.0, 0.0, 0.0], 'rewardMean': 0.7817846357653615, 'totalEpisodes': 100, 'stepsPerEpisode': 346, 'rewardPerEpisode': 253.4129722402358
'totalSteps': 10240, 'rewardStep': 0.8567808557599345, 'errorList': [], 'lossList': [0.0, -1.3708539247512816, 0.0, 45.639391841888425, 0.0, 0.0, 0.0], 'rewardMean': 0.791159163264683, 'totalEpisodes': 103, 'stepsPerEpisode': 413, 'rewardPerEpisode': 324.93440825780203
'totalSteps': 11520, 'rewardStep': 0.8139381965232597, 'errorList': [], 'lossList': [0.0, -1.3583635944128036, 0.0, 207.43960624694824, 0.0, 0.0, 0.0], 'rewardMean': 0.7936901669600805, 'totalEpisodes': 129, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.022813996568207
'totalSteps': 12800, 'rewardStep': 0.8061840946588618, 'errorList': [], 'lossList': [0.0, -1.3562781888246536, 0.0, 91.1508067703247, 0.0, 0.0, 0.0], 'rewardMean': 0.7949395597299587, 'totalEpisodes': 146, 'stepsPerEpisode': 45, 'rewardPerEpisode': 40.74703874015616
'totalSteps': 14080, 'rewardStep': 0.6486971206127471, 'errorList': [], 'lossList': [0.0, -1.3469132751226425, 0.0, 20.318681330680846, 0.0, 0.0, 0.0], 'rewardMean': 0.7677002583760926, 'totalEpisodes': 154, 'stepsPerEpisode': 269, 'rewardPerEpisode': 211.72831903644436
'totalSteps': 15360, 'rewardStep': 0.586143611429076, 'errorList': [], 'lossList': [0.0, -1.33964368224144, 0.0, 18.09188868999481, 0.0, 0.0, 0.0], 'rewardMean': 0.7486725063112795, 'totalEpisodes': 162, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.586143611429076
'totalSteps': 16640, 'rewardStep': 0.023646934030769118, 'errorList': [], 'lossList': [0.0, -1.3409374356269836, 0.0, 13.223956233263015, 0.0, 0.0, 0.0], 'rewardMean': 0.6678958626768099, 'totalEpisodes': 166, 'stepsPerEpisode': 213, 'rewardPerEpisode': 127.14654557999467
'totalSteps': 17920, 'rewardStep': 0.633189002433755, 'errorList': [], 'lossList': [0.0, -1.3210350960493087, 0.0, 6.964178358316421, 0.0, 0.0, 0.0], 'rewardMean': 0.6546544696534129, 'totalEpisodes': 167, 'stepsPerEpisode': 1275, 'rewardPerEpisode': 926.66779398262
'totalSteps': 19200, 'rewardStep': 0.7254582891738799, 'errorList': [], 'lossList': [0.0, -1.2767372435331346, 0.0, 3.3119460287690163, 0.0, 0.0, 0.0], 'rewardMean': 0.639986374237974, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1001.8147192379907
'totalSteps': 20480, 'rewardStep': 0.9393665170519748, 'errorList': [0.14654125158910902, 0.03745946110954469, 0.17486589361186458, 0.08020575229223544, 0.13057191027439832, 0.037441954565676054, 0.13032835669426365, 0.06895889988216955, 0.09386050097943867, 0.054252607029737526, 0.11919007949442448, 0.14178842735002387, 0.032496084866682866, 0.05084902163820332, 0.06517649482061831, 0.033160443082848295, 0.0366956227310171, 0.06479325239499717, 0.03185166709765125, 0.11076889806524406, 0.03378758787703452, 0.03452847901635588, 0.03431966922031025, 0.18286078018123816, 0.04333268246094518, 0.03404947991868949, 0.0747005119076374, 0.03439081677132826, 0.03309956975034348, 0.08602507318401262, 0.03429310291101677, 0.03942868329028434, 0.07430182263921105, 0.06492339211566464, 0.10699047685836943, 0.03263571058939987, 0.13031634300981884, 0.1434155983278656, 0.04139415290279142, 0.03290162949669224, 0.03319427577454212, 0.11485220273205791, 0.12230182343630565, 0.03577506184752929, 0.05415955382424002, 0.033289361774603496, 0.06226536386674053, 0.029748527400954048, 0.03142183643980145, 0.08189184310665829], 'lossList': [0.0, -1.2523812353610992, 0.0, 4.838332498073578, 0.0, 0.0, 0.0], 'rewardMean': 0.6717308671329979, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.5197895148312, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=69.66
