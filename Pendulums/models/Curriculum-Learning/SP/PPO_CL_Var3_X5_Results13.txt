#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 13
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6570189896177849, 'errorList': [], 'lossList': [0.0, -1.421935514807701, 0.0, 27.94980682015419, 0.0, 0.0, 0.0], 'rewardMean': 0.7111724112824205, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.150581789968847
'totalSteps': 3840, 'rewardStep': 0.9586348309326908, 'errorList': [], 'lossList': [0.0, -1.415233200788498, 0.0, 26.532954198122024, 0.0, 0.0, 0.0], 'rewardMean': 0.7936598844991772, 'totalEpisodes': 18, 'stepsPerEpisode': 492, 'rewardPerEpisode': 373.4612187537469
'totalSteps': 5120, 'rewardStep': 0.7568225462973095, 'errorList': [], 'lossList': [0.0, -1.4085589003562928, 0.0, 29.232001550793647, 0.0, 0.0, 0.0], 'rewardMean': 0.7844505499487102, 'totalEpisodes': 19, 'stepsPerEpisode': 181, 'rewardPerEpisode': 152.20740927935663
'totalSteps': 6400, 'rewardStep': 0.9135641597161964, 'errorList': [], 'lossList': [0.0, -1.410924835205078, 0.0, 235.89253921508788, 0.0, 0.0, 0.0], 'rewardMean': 0.8102732719022075, 'totalEpisodes': 67, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.076800499016237
'totalSteps': 7680, 'rewardStep': 0.6933911683427294, 'errorList': [], 'lossList': [0.0, -1.4065742820501328, 0.0, 99.94638099670411, 0.0, 0.0, 0.0], 'rewardMean': 0.7907929213089612, 'totalEpisodes': 112, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.6778945601087543
'totalSteps': 8960, 'rewardStep': 0.4219796756422094, 'errorList': [], 'lossList': [0.0, -1.395420281291008, 0.0, 76.87068267822265, 0.0, 0.0, 0.0], 'rewardMean': 0.7381053147851394, 'totalEpisodes': 149, 'stepsPerEpisode': 107, 'rewardPerEpisode': 83.42322115921793
'totalSteps': 10240, 'rewardStep': 0.6594598943148285, 'errorList': [], 'lossList': [0.0, -1.3879931157827377, 0.0, 57.00115419387817, 0.0, 0.0, 0.0], 'rewardMean': 0.7282746372263507, 'totalEpisodes': 181, 'stepsPerEpisode': 52, 'rewardPerEpisode': 44.05584933474053
'totalSteps': 11520, 'rewardStep': 0.6364855646884158, 'errorList': [], 'lossList': [0.0, -1.3982675933837891, 0.0, 36.22415573120117, 0.0, 0.0, 0.0], 'rewardMean': 0.7180758513888024, 'totalEpisodes': 198, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.9286307026188583
'totalSteps': 12800, 'rewardStep': 0.19792525209293954, 'errorList': [], 'lossList': [0.0, -1.3973031985759734, 0.0, 26.738020136356354, 0.0, 0.0, 0.0], 'rewardMean': 0.666060791459216, 'totalEpisodes': 206, 'stepsPerEpisode': 185, 'rewardPerEpisode': 137.17470886053596
'totalSteps': 14080, 'rewardStep': 0.42303347565607546, 'errorList': [], 'lossList': [0.0, -1.4001827156543731, 0.0, 14.781257977485657, 0.0, 0.0, 0.0], 'rewardMean': 0.6318315557301181, 'totalEpisodes': 211, 'stepsPerEpisode': 230, 'rewardPerEpisode': 175.80626161153944
'totalSteps': 15360, 'rewardStep': 0.6085109551858916, 'errorList': [], 'lossList': [0.0, -1.3912534290552139, 0.0, 13.039662277698516, 0.0, 0.0, 0.0], 'rewardMean': 0.6269807522869287, 'totalEpisodes': 216, 'stepsPerEpisode': 43, 'rewardPerEpisode': 32.92118733929608
'totalSteps': 16640, 'rewardStep': 0.9674666127652949, 'errorList': [0.35044639728463234, 0.28931604427063434, 3.2779194515910666, 0.17724734469504974, 0.060061953597310334, 0.39477866915033416, 1.3277494404172343, 1.334451175839175, 2.7065237986619195, 1.4737394724267088, 0.26950566922683933, 1.9219795584965846, 2.3490066420031614, 1.3339876534832242, 1.2452555321447547, 0.22523784251082796, 1.1409519222705875, 0.19910052908490888, 2.077627660454422, 0.8048218724406967, 0.6251303523316306, 0.6778322780229191, 0.22002415886367022, 2.5489227873996563, 0.9913296410805231, 0.5183466256226384, 2.2744369124631927, 1.0484144077299231, 2.4157328973526666, 1.6368527060825797, 0.22117217622949495, 1.6249194557451108, 2.5238188866462625, 2.0298880643956894, 0.6538747561566415, 0.31655844387417104, 0.39667939520885936, 0.5184816037948604, 0.7015682654246205, 1.6016441558386563, 3.114415244764118, 2.2656777944436945, 0.18648866268803077, 2.820527558522818, 1.606383006847885, 0.3865973782140279, 0.994779313097253, 0.8034137934581205, 0.24948875271512838, 1.2085035682343017], 'lossList': [0.0, -1.3870301061868668, 0.0, 9.795218431949616, 0.0, 0.0, 0.0], 'rewardMean': 0.6278639304701891, 'totalEpisodes': 220, 'stepsPerEpisode': 42, 'rewardPerEpisode': 35.559847780098096, 'successfulTests': 4
'totalSteps': 17920, 'rewardStep': 0.7177033655314435, 'errorList': [], 'lossList': [0.0, -1.3713802522420884, 0.0, 5.654801366329193, 0.0, 0.0, 0.0], 'rewardMean': 0.6239520123936024, 'totalEpisodes': 221, 'stepsPerEpisode': 799, 'rewardPerEpisode': 622.0728317495394
'totalSteps': 19200, 'rewardStep': 0.5351026427550225, 'errorList': [], 'lossList': [0.0, -1.3349262392520904, 0.0, 4.013947956562042, 0.0, 0.0, 0.0], 'rewardMean': 0.586105860697485, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 922.3772928673884
'totalSteps': 20480, 'rewardStep': 0.7592156621011705, 'errorList': [], 'lossList': [0.0, -1.3087402909994126, 0.0, 4.755774032473564, 0.0, 0.0, 0.0], 'rewardMean': 0.5926883100733292, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 891.2216785617692
'totalSteps': 21760, 'rewardStep': 0.8304834896966583, 'errorList': [], 'lossList': [0.0, -1.2895996898412705, 0.0, 2.7864997744560243, 0.0, 0.0, 0.0], 'rewardMean': 0.6335386914787742, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 946.5624238427743
'totalSteps': 23040, 'rewardStep': 0.7424807731902581, 'errorList': [], 'lossList': [0.0, -1.2731982111930846, 0.0, 2.015077954083681, 0.0, 0.0, 0.0], 'rewardMean': 0.641840779366317, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.1034885366832
'totalSteps': 24320, 'rewardStep': 0.7705790980463804, 'errorList': [], 'lossList': [0.0, -1.215567135810852, 0.0, 1.945642372518778, 0.0, 0.0, 0.0], 'rewardMean': 0.6552501327021135, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.5672575936771
'totalSteps': 25600, 'rewardStep': 0.9793724805559013, 'errorList': [0.06564826270498517, 0.05022928494147247, 0.07386438030516537, 0.05797763599566391, 0.0671948449573369, 0.08227587716009241, 0.05871843526840445, 0.07771221570507476, 0.05373638307319087, 0.058053656875234214, 0.08801001712979283, 0.07322831057015249, 0.08907537917495795, 0.09823112520219839, 0.07883590177298, 0.07848114569723519, 0.06103149656436466, 0.053888826113115317, 0.048948069079977195, 0.07082897510615675, 0.056764440631501524, 0.0501893952663173, 0.05115169077607152, 0.05880544169196805, 0.05096928904812405, 0.0487201789467097, 0.06372259263235167, 0.06736262746245396, 0.09883070476450312, 0.04794012911330546, 0.05374215462617946, 0.08812142914154097, 0.05326140344217309, 0.08932751053812982, 0.047124285676732165, 0.049772978877930946, 0.05228146235566945, 0.05664614542717627, 0.07064770441696529, 0.07405788386803179, 0.09125555422775875, 0.05072661873252111, 0.050632236350822646, 0.0855406002865427, 0.05492052357235383, 0.04824824424012442, 0.06874290184455181, 0.09515640098160136, 0.073355061169068, 0.05468071832807416], 'lossList': [0.0, -1.1779599791765214, 0.0, 1.602270246297121, 0.0, 0.0, 0.0], 'rewardMean': 0.7333948555484098, 'totalEpisodes': 221, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.1879192294484, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=91.96
