#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 130
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.8755250183974979, 'errorList': [], 'lossList': [0.0, -1.3985840630531312, 0.0, 28.8167115855217, 0.0, 0.0, 0.0], 'rewardMean': 0.7430975586696742, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.951834601282375
'totalSteps': 3840, 'rewardStep': 0.7238554632879386, 'errorList': [], 'lossList': [0.0, -1.3883284229040145, 0.0, 30.59730744600296, 0.0, 0.0, 0.0], 'rewardMean': 0.7366835268757623, 'totalEpisodes': 26, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.3946975491307
'totalSteps': 5120, 'rewardStep': 0.6645624999155265, 'errorList': [], 'lossList': [0.0, -1.3988670462369919, 0.0, 16.052485060691833, 0.0, 0.0, 0.0], 'rewardMean': 0.7186532701357033, 'totalEpisodes': 27, 'stepsPerEpisode': 121, 'rewardPerEpisode': 99.54068318595398
'totalSteps': 6400, 'rewardStep': 0.9537654450275037, 'errorList': [], 'lossList': [0.0, -1.4091540706157684, 0.0, 16.182398335933684, 0.0, 0.0, 0.0], 'rewardMean': 0.7656757051140634, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 949.1304254177608
'totalSteps': 7680, 'rewardStep': 0.7025248596067343, 'errorList': [], 'lossList': [0.0, -1.3944904452562332, 0.0, 11.227797597646713, 0.0, 0.0, 0.0], 'rewardMean': 0.7551505641961752, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 955.4591738556131
'totalSteps': 8960, 'rewardStep': 0.7036241140203569, 'errorList': [], 'lossList': [0.0, -1.3636373418569565, 0.0, 7.630278871059418, 0.0, 0.0, 0.0], 'rewardMean': 0.7477896427424869, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 908.5139204072632
'totalSteps': 10240, 'rewardStep': 0.7813721630095811, 'errorList': [], 'lossList': [0.0, -1.3567644494771958, 0.0, 56.72982279539108, 0.0, 0.0, 0.0], 'rewardMean': 0.7519874577758736, 'totalEpisodes': 30, 'stepsPerEpisode': 110, 'rewardPerEpisode': 92.27255825432036
'totalSteps': 11520, 'rewardStep': 0.8066024116270283, 'errorList': [], 'lossList': [0.0, -1.351173570752144, 0.0, 401.5232558441162, 0.0, 0.0, 0.0], 'rewardMean': 0.7580557859815574, 'totalEpisodes': 69, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.706674814320767
'totalSteps': 12800, 'rewardStep': 0.4710873368994163, 'errorList': [], 'lossList': [0.0, -1.3480056339502335, 0.0, 135.75606945037842, 0.0, 0.0, 0.0], 'rewardMean': 0.7293589410733434, 'totalEpisodes': 98, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.422377844760089
'totalSteps': 14080, 'rewardStep': 0.9158054964105341, 'errorList': [], 'lossList': [0.0, -1.3488519644737245, 0.0, 123.17015357971191, 0.0, 0.0, 0.0], 'rewardMean': 0.7598724808202117, 'totalEpisodes': 127, 'stepsPerEpisode': 36, 'rewardPerEpisode': 32.45943835911803
'totalSteps': 15360, 'rewardStep': 0.6932431707099396, 'errorList': [], 'lossList': [0.0, -1.3491709291934968, 0.0, 78.76790715217591, 0.0, 0.0, 0.0], 'rewardMean': 0.7416442960514559, 'totalEpisodes': 151, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.1911308238657075
'totalSteps': 16640, 'rewardStep': 0.7234954490702737, 'errorList': [], 'lossList': [0.0, -1.3459672290086746, 0.0, 67.72619659423827, 0.0, 0.0, 0.0], 'rewardMean': 0.7416082946296895, 'totalEpisodes': 172, 'stepsPerEpisode': 39, 'rewardPerEpisode': 30.887878818650613
'totalSteps': 17920, 'rewardStep': 0.9551488514775469, 'errorList': [126.01907784648168, 178.92942513002282, 204.32340536328138, 157.42744050839866, 152.160511187456, 181.86311301647274, 214.19921493544467, 128.31827775510314, 128.67551164220598, 143.40328158609339, 18.61393664463884, 203.83865548969004, 188.49270119302764, 171.42841568124965, 204.9926198925301, 146.05067785021248, 221.84659182216467, 165.71387461491813, 94.68298818766851, 200.0709527345937, 136.6749669871044, 122.19229533923667, 212.6296827363047, 206.10593348072237, 202.71386485576411, 94.31564867442698, 128.1477480346485, 88.6871022026644, 77.02536296413044, 213.3425042637816, 135.66079060331597, 206.25240818375522, 180.82369186648077, 201.35991279907674, 87.08906599197665, 217.6376000704141, 185.51707613507608, 198.9387624774275, 179.20711487922406, 70.54111969327714, 183.17537367729008, 253.04896854228443, 161.4415356214379, 158.5694498199198, 225.19307424209885, 202.25163558479656, 171.4988679504445, 132.59804756399433, 179.69675444939332, 133.20135833967188], 'lossList': [0.0, -1.3409166437387467, 0.0, 46.81786903381348, 0.0, 0.0, 0.0], 'rewardMean': 0.7706669297858915, 'totalEpisodes': 189, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.605780639128252, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.6986857617666289, 'errorList': [], 'lossList': [0.0, -1.3302204024791717, 0.0, 37.9498610162735, 0.0, 0.0, 0.0], 'rewardMean': 0.745158961459804, 'totalEpisodes': 202, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.509855527606494
'totalSteps': 20480, 'rewardStep': 0.6959272358191474, 'errorList': [], 'lossList': [0.0, -1.3353266519308091, 0.0, 25.866628613471985, 0.0, 0.0, 0.0], 'rewardMean': 0.7444991990810452, 'totalEpisodes': 210, 'stepsPerEpisode': 66, 'rewardPerEpisode': 52.11925356019941
'totalSteps': 21760, 'rewardStep': 0.7819931788311411, 'errorList': [], 'lossList': [0.0, -1.347929172515869, 0.0, 37.05222630023956, 0.0, 0.0, 0.0], 'rewardMean': 0.7523361055621237, 'totalEpisodes': 219, 'stepsPerEpisode': 59, 'rewardPerEpisode': 47.87989479557607
'totalSteps': 23040, 'rewardStep': 0.49365582488795917, 'errorList': [], 'lossList': [0.0, -1.341685145497322, 0.0, 20.253346989154817, 0.0, 0.0, 0.0], 'rewardMean': 0.7235644717499615, 'totalEpisodes': 225, 'stepsPerEpisode': 159, 'rewardPerEpisode': 113.06691736029146
'totalSteps': 24320, 'rewardStep': 0.7570057782173018, 'errorList': [], 'lossList': [0.0, -1.31913898229599, 0.0, 16.607748210430145, 0.0, 0.0, 0.0], 'rewardMean': 0.7186048084089889, 'totalEpisodes': 232, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.857647518516339
'totalSteps': 25600, 'rewardStep': 0.8347827519296946, 'errorList': [], 'lossList': [0.0, -1.313662759065628, 0.0, 15.109496319293976, 0.0, 0.0, 0.0], 'rewardMean': 0.7549743499120167, 'totalEpisodes': 237, 'stepsPerEpisode': 222, 'rewardPerEpisode': 201.93820466511494
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.34
