#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 132
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.6024409734472589, 'errorList': [], 'lossList': [0.0, -1.4165353786945343, 0.0, 23.934984185695647, 0.0, 0.0, 0.0], 'rewardMean': 0.698105487313705, 'totalEpisodes': 20, 'stepsPerEpisode': 134, 'rewardPerEpisode': 81.2906198128033
'totalSteps': 3840, 'rewardStep': 0.8527179670967782, 'errorList': [], 'lossList': [0.0, -1.4050010961294175, 0.0, 27.30099864721298, 0.0, 0.0, 0.0], 'rewardMean': 0.7496429805747294, 'totalEpisodes': 24, 'stepsPerEpisode': 75, 'rewardPerEpisode': 59.113257366651034
'totalSteps': 5120, 'rewardStep': 0.9162079383789338, 'errorList': [], 'lossList': [0.0, -1.385135572552681, 0.0, 16.953872284889222, 0.0, 0.0, 0.0], 'rewardMean': 0.7912842200257805, 'totalEpisodes': 29, 'stepsPerEpisode': 27, 'rewardPerEpisode': 24.39905815546266
'totalSteps': 6400, 'rewardStep': 0.1619198643010371, 'errorList': [], 'lossList': [0.0, -1.3772017127275467, 0.0, 24.904979593753815, 0.0, 0.0, 0.0], 'rewardMean': 0.6654113488808318, 'totalEpisodes': 30, 'stepsPerEpisode': 573, 'rewardPerEpisode': 395.1952588660558
'totalSteps': 7680, 'rewardStep': 0.6339756581601074, 'errorList': [], 'lossList': [0.0, -1.3643361967802048, 0.0, 15.649739245176315, 0.0, 0.0, 0.0], 'rewardMean': 0.6601720670940444, 'totalEpisodes': 31, 'stepsPerEpisode': 817, 'rewardPerEpisode': 647.4801939902449
'totalSteps': 8960, 'rewardStep': 0.7907774552342817, 'errorList': [], 'lossList': [0.0, -1.3565333771705628, 0.0, 7.8681603062152865, 0.0, 0.0, 0.0], 'rewardMean': 0.678829979685507, 'totalEpisodes': 31, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 941.5889681547332
'totalSteps': 10240, 'rewardStep': 0.8031763126603839, 'errorList': [], 'lossList': [0.0, -1.359998146891594, 0.0, 6.79304627507925, 0.0, 0.0, 0.0], 'rewardMean': 0.6943732713073665, 'totalEpisodes': 31, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1037.9879244769784
'totalSteps': 11520, 'rewardStep': 0.6447390369779481, 'errorList': [], 'lossList': [0.0, -1.353546023964882, 0.0, 485.83401237487794, 0.0, 0.0, 0.0], 'rewardMean': 0.6888583563818755, 'totalEpisodes': 67, 'stepsPerEpisode': 46, 'rewardPerEpisode': 37.2416494882018
'totalSteps': 12800, 'rewardStep': 0.9524374177889005, 'errorList': [283.1650023599347, 289.20382083684456, 246.48628217083768, 261.33227576880023, 280.62312723816393, 282.86648154432123, 280.9236624333208, 273.9911184633691, 275.9761922109578, 279.28167713705596, 235.56546444656792, 241.04970319857264, 286.48774604835495, 267.4677094210478, 267.4787425507532, 253.5876394710484, 190.19529349609147, 273.55877914833354, 289.58351897816675, 279.43363123897626, 289.85997712054007, 265.7203946039926, 271.46643827026844, 288.14314689141594, 281.7267693825126, 262.84974332858764, 288.5490863536855, 262.90627378943265, 280.4972270822004, 289.5481672419296, 260.29328501167925, 254.17673717871702, 280.8619444744722, 290.3405492061424, 245.2130686317297, 253.51050883142256, 265.3902250252717, 289.1950503442747, 233.1751260529142, 276.0971523518047, 245.5380517313187, 272.9602509653684, 269.7453297707473, 269.3783483404713, 290.75248446457994, 285.54521086958454, 259.9161555192365, 210.42102936101617, 193.33778279282632, 261.11945116909936], 'lossList': [0.0, -1.350831109881401, 0.0, 179.08483261108398, 0.0, 0.0, 0.0], 'rewardMean': 0.715216262522578, 'totalEpisodes': 97, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.985436020063233, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.48079426031617173, 'errorList': [], 'lossList': [0.0, -1.3469272685050964, 0.0, 99.6170573425293, 0.0, 0.0, 0.0], 'rewardMean': 0.6839186884361801, 'totalEpisodes': 119, 'stepsPerEpisode': 36, 'rewardPerEpisode': 23.35688998877927
'totalSteps': 15360, 'rewardStep': 0.701795067807367, 'errorList': [], 'lossList': [0.0, -1.346736341714859, 0.0, 66.2986449432373, 0.0, 0.0, 0.0], 'rewardMean': 0.693854097872191, 'totalEpisodes': 149, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.701795067807367
'totalSteps': 16640, 'rewardStep': 0.5767195324120087, 'errorList': [], 'lossList': [0.0, -1.3455667865276337, 0.0, 27.017961292266847, 0.0, 0.0, 0.0], 'rewardMean': 0.6662542544037139, 'totalEpisodes': 163, 'stepsPerEpisode': 41, 'rewardPerEpisode': 25.031543198476243
'totalSteps': 17920, 'rewardStep': 0.66050273450128, 'errorList': [], 'lossList': [0.0, -1.3425565725564956, 0.0, 12.82978413105011, 0.0, 0.0, 0.0], 'rewardMean': 0.6406837340159486, 'totalEpisodes': 174, 'stepsPerEpisode': 60, 'rewardPerEpisode': 51.53137338613432
'totalSteps': 19200, 'rewardStep': 0.35520415420301327, 'errorList': [], 'lossList': [0.0, -1.3468019872903825, 0.0, 12.126366872787475, 0.0, 0.0, 0.0], 'rewardMean': 0.6600121630061462, 'totalEpisodes': 180, 'stepsPerEpisode': 240, 'rewardPerEpisode': 190.96178083193396
'totalSteps': 20480, 'rewardStep': 0.704015388613638, 'errorList': [], 'lossList': [0.0, -1.3505072367191315, 0.0, 13.261265798807145, 0.0, 0.0, 0.0], 'rewardMean': 0.6670161360514993, 'totalEpisodes': 188, 'stepsPerEpisode': 93, 'rewardPerEpisode': 73.44395737760483
'totalSteps': 21760, 'rewardStep': 0.6584947745830725, 'errorList': [], 'lossList': [0.0, -1.3457323855161667, 0.0, 6.708556063175202, 0.0, 0.0, 0.0], 'rewardMean': 0.6537878679863784, 'totalEpisodes': 193, 'stepsPerEpisode': 268, 'rewardPerEpisode': 211.47485503513585
'totalSteps': 23040, 'rewardStep': 0.881222348121688, 'errorList': [], 'lossList': [0.0, -1.324899863600731, 0.0, 12.078634402751923, 0.0, 0.0, 0.0], 'rewardMean': 0.6615924715325089, 'totalEpisodes': 199, 'stepsPerEpisode': 136, 'rewardPerEpisode': 117.80078574082033
'totalSteps': 24320, 'rewardStep': 0.8336176776477962, 'errorList': [], 'lossList': [0.0, -1.3177336198091507, 0.0, 3.5665445363521577, 0.0, 0.0, 0.0], 'rewardMean': 0.6804803355994935, 'totalEpisodes': 203, 'stepsPerEpisode': 46, 'rewardPerEpisode': 37.279115571212124
'totalSteps': 25600, 'rewardStep': 0.7971902228401757, 'errorList': [], 'lossList': [0.0, -1.3073656964302063, 0.0, 4.0892325472831725, 0.0, 0.0, 0.0], 'rewardMean': 0.6649556161046212, 'totalEpisodes': 204, 'stepsPerEpisode': 582, 'rewardPerEpisode': 468.03692675806195
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=78.34
