#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 134
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7930394493382569, 'errorList': [], 'lossList': [0.0, -1.4125491392612457, 0.0, 24.135942618846894, 0.0, 0.0, 0.0], 'rewardMean': 0.5785547775569228, 'totalEpisodes': 20, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.512102401743196
'totalSteps': 3840, 'rewardStep': 0.7210889690725582, 'errorList': [], 'lossList': [0.0, -1.4235481196641921, 0.0, 20.88394351005554, 0.0, 0.0, 0.0], 'rewardMean': 0.6260661747288012, 'totalEpisodes': 28, 'stepsPerEpisode': 171, 'rewardPerEpisode': 119.67860194613712
'totalSteps': 5120, 'rewardStep': 0.6383314281247933, 'errorList': [], 'lossList': [0.0, -1.4322594922780991, 0.0, 18.126739854812623, 0.0, 0.0, 0.0], 'rewardMean': 0.6291324880777992, 'totalEpisodes': 29, 'stepsPerEpisode': 1062, 'rewardPerEpisode': 733.5829429274514
'totalSteps': 6400, 'rewardStep': 0.6831006924896567, 'errorList': [], 'lossList': [0.0, -1.4334013861417771, 0.0, 32.0533949136734, 0.0, 0.0, 0.0], 'rewardMean': 0.6399261289601708, 'totalEpisodes': 31, 'stepsPerEpisode': 866, 'rewardPerEpisode': 678.1288831384397
'totalSteps': 7680, 'rewardStep': 0.8727275965487855, 'errorList': [], 'lossList': [0.0, -1.4225784510374069, 0.0, 14.32263326048851, 0.0, 0.0, 0.0], 'rewardMean': 0.6787263735582733, 'totalEpisodes': 31, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.429612158309
'totalSteps': 8960, 'rewardStep': 0.8049013302431687, 'errorList': [], 'lossList': [0.0, -1.4012776041030883, 0.0, 24.930140526890753, 0.0, 0.0, 0.0], 'rewardMean': 0.6967513673704012, 'totalEpisodes': 32, 'stepsPerEpisode': 1124, 'rewardPerEpisode': 937.7787256842212
'totalSteps': 10240, 'rewardStep': 0.9041232791023146, 'errorList': [], 'lossList': [0.0, -1.391481568813324, 0.0, 6.099148234128952, 0.0, 0.0, 0.0], 'rewardMean': 0.7226728563368903, 'totalEpisodes': 32, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.5983130259463
'totalSteps': 11520, 'rewardStep': 0.3843172352993688, 'errorList': [], 'lossList': [0.0, -1.3820973920822144, 0.0, 278.3472027587891, 0.0, 0.0, 0.0], 'rewardMean': 0.6850777873327212, 'totalEpisodes': 59, 'stepsPerEpisode': 42, 'rewardPerEpisode': 24.466106617717443
'totalSteps': 12800, 'rewardStep': 0.7759037492206303, 'errorList': [], 'lossList': [0.0, -1.3800450789928436, 0.0, 100.81086055755615, 0.0, 0.0, 0.0], 'rewardMean': 0.6941603835215121, 'totalEpisodes': 84, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.698174610260303
'totalSteps': 14080, 'rewardStep': 0.9667666278435402, 'errorList': [228.66521676744017, 250.2930059709936, 247.30084764118075, 252.44642499986082, 156.13315023387494, 297.96741143959633, 141.6683093148371, 255.96973729918247, 170.6800748157539, 254.1202526816078, 314.52533276570443, 217.43536545839697, 264.06201809666317, 282.670124675367, 244.51051064245232, 180.3866217127361, 256.8820221438941, 254.36561859470518, 248.94562480039679, 125.83411935951654, 244.08112018686094, 189.19977828399212, 180.55576861257225, 280.8537049804226, 182.54837101800496, 263.26710197092183, 265.33175166658145, 270.8382881986665, 287.7378334769325, 230.70030350848114, 296.94202162315855, 170.20885219331404, 284.2791126409187, 291.1175120808192, 272.4383853692849, 276.1069741083413, 194.64724543562062, 100.40248886136331, 206.00417232486868, 161.35676321550918, 276.100851608682, 280.4481846026943, 151.25550422820896, 256.03221860447184, 241.64616560040383, 204.89769403529448, 192.36261948976082, 113.2792673071941, 260.5632021458058, 286.18694421702867], 'lossList': [0.0, -1.375786936879158, 0.0, 20.487195682525634, 0.0, 0.0, 0.0], 'rewardMean': 0.7544300357283074, 'totalEpisodes': 100, 'stepsPerEpisode': 23, 'rewardPerEpisode': 21.521048141052542, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.2986786782140271, 'errorList': [], 'lossList': [0.0, -1.3703030413389206, 0.0, 27.84545518875122, 0.0, 0.0, 0.0], 'rewardMean': 0.7049939586158843, 'totalEpisodes': 114, 'stepsPerEpisode': 83, 'rewardPerEpisode': 56.990768276905975
'totalSteps': 16640, 'rewardStep': 0.8084869719253938, 'errorList': [], 'lossList': [0.0, -1.3646910715103149, 0.0, 20.519508409500123, 0.0, 0.0, 0.0], 'rewardMean': 0.7137337589011679, 'totalEpisodes': 124, 'stepsPerEpisode': 163, 'rewardPerEpisode': 129.97368717352205
'totalSteps': 17920, 'rewardStep': 0.8485671809839104, 'errorList': [], 'lossList': [0.0, -1.3578538113832475, 0.0, 15.60366124868393, 0.0, 0.0, 0.0], 'rewardMean': 0.7347573341870797, 'totalEpisodes': 129, 'stepsPerEpisode': 27, 'rewardPerEpisode': 25.67952748769376
'totalSteps': 19200, 'rewardStep': 0.5231611420643497, 'errorList': [], 'lossList': [0.0, -1.3516477382183074, 0.0, 14.67592655658722, 0.0, 0.0, 0.0], 'rewardMean': 0.7187633791445489, 'totalEpisodes': 132, 'stepsPerEpisode': 140, 'rewardPerEpisode': 107.4435378172401
'totalSteps': 20480, 'rewardStep': 0.876288241260889, 'errorList': [], 'lossList': [0.0, -1.3375750893354417, 0.0, 10.476730742454528, 0.0, 0.0, 0.0], 'rewardMean': 0.7191194436157593, 'totalEpisodes': 136, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.39935763939942
'totalSteps': 21760, 'rewardStep': 0.7538844566651548, 'errorList': [], 'lossList': [0.0, -1.329990211725235, 0.0, 8.215981947183609, 0.0, 0.0, 0.0], 'rewardMean': 0.7140177562579579, 'totalEpisodes': 139, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.82610741408908
'totalSteps': 23040, 'rewardStep': 0.8047572260906201, 'errorList': [], 'lossList': [0.0, -1.3291935849189758, 0.0, 5.960092111825943, 0.0, 0.0, 0.0], 'rewardMean': 0.7040811509567885, 'totalEpisodes': 140, 'stepsPerEpisode': 222, 'rewardPerEpisode': 182.42504134359996
'totalSteps': 24320, 'rewardStep': 0.6966872104137705, 'errorList': [], 'lossList': [0.0, -1.3134437239170074, 0.0, 2.8222279351949693, 0.0, 0.0, 0.0], 'rewardMean': 0.7353181484682286, 'totalEpisodes': 140, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1018.2009446378826
'totalSteps': 25600, 'rewardStep': 0.9500959487735181, 'errorList': [0.0406779249992286, 0.01866175504299598, 0.05231617939414367, 0.05671802109684726, 0.022591118866094464, 0.0514008511170565, 0.017289536587648813, 0.059732785189782776, 0.020023641696637787, 0.021494372244142943, 0.0459547805278326, 0.07018446626814946, 0.017493079664171275, 0.046901476861291584, 0.01976986113432729, 0.056975690690787915, 0.01996456162766586, 0.020480951801061437, 0.02012933441459262, 0.01859313818320002, 0.02109439965738367, 0.02015550223991294, 0.0198829854668046, 0.03242043511273841, 0.029716818660663846, 0.018409089623682883, 0.01950417664907563, 0.018324523442347796, 0.027406874915872875, 0.03764121658500061, 0.04188031599077783, 0.01732561719271715, 0.018072883615581015, 0.017296505331454992, 0.039766798288799955, 0.020126939719844313, 0.03401005030525205, 0.01991598857730228, 0.028408950063116965, 0.01901382964236784, 0.045395609477462545, 0.02526476574599091, 0.019474217646153393, 0.01792851848557308, 0.03277118258788278, 0.020787532409548257, 0.04826337409155664, 0.01968182052500702, 0.03691747633433257, 0.01913221205296841], 'lossList': [0.0, -1.271583669781685, 0.0, 1.2813803286105394, 0.0, 0.0, 0.0], 'rewardMean': 0.7527373684235175, 'totalEpisodes': 140, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.929151849668, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=93.12
