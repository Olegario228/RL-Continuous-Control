#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 139
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8691077463116506, 'errorList': [], 'lossList': [0.0, -1.4240562838315964, 0.0, 30.645360610485078, 0.0, 0.0, 0.0], 'rewardMean': 0.770511576227497, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 252.45244492043656
'totalSteps': 3840, 'rewardStep': 0.7731592763795602, 'errorList': [], 'lossList': [0.0, -1.4417803210020066, 0.0, 30.44170731782913, 0.0, 0.0, 0.0], 'rewardMean': 0.7713941429448514, 'totalEpisodes': 15, 'stepsPerEpisode': 168, 'rewardPerEpisode': 126.69362462760176
'totalSteps': 5120, 'rewardStep': 0.6101094225101074, 'errorList': [], 'lossList': [0.0, -1.445222972035408, 0.0, 22.918448247909545, 0.0, 0.0, 0.0], 'rewardMean': 0.7310729628361654, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 955.7154049607483
'totalSteps': 6400, 'rewardStep': 0.8463077136726231, 'errorList': [], 'lossList': [0.0, -1.4325646126270295, 0.0, 18.986805750727655, 0.0, 0.0, 0.0], 'rewardMean': 0.7541199130034569, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1000.4269224628457
'totalSteps': 7680, 'rewardStep': 0.9268389101643254, 'errorList': [], 'lossList': [0.0, -1.4301065319776536, 0.0, 12.144483472704888, 0.0, 0.0, 0.0], 'rewardMean': 0.7829064125302684, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.9345012521537
'totalSteps': 8960, 'rewardStep': 0.6470001617020125, 'errorList': [], 'lossList': [0.0, -1.420833404660225, 0.0, 23.69558769226074, 0.0, 0.0, 0.0], 'rewardMean': 0.7634912338405176, 'totalEpisodes': 16, 'stepsPerEpisode': 1141, 'rewardPerEpisode': 844.5483023431517
'totalSteps': 10240, 'rewardStep': 0.9303882596364805, 'errorList': [190.5252938961625, 191.35474470652196, 184.50760262549926, 207.31308205812596, 185.58148826080503, 202.9514345855122, 180.20058159343978, 192.91581130800049, 195.63513226274785, 186.25092586660813, 199.59428757659745, 192.0531193947142, 191.26964057416112, 200.8529427582255, 191.71994824687917, 198.92295167794398, 203.8112217183647, 192.80335017664476, 199.55599763825887, 187.54276001950512, 188.41111581252974, 196.18757127956974, 188.93102765970656, 192.8837403656603, 192.86632772643787, 195.80358909007583, 197.50396081562985, 153.18092143408995, 164.51453896475036, 200.44003354845574, 186.83621559334566, 200.1208939369515, 198.82524922112748, 194.9146818787362, 165.19534860233426, 172.25000379220373, 172.3918068924482, 160.59685157530095, 197.21840928393345, 202.07946488006715, 170.37816119828256, 164.26101393472263, 200.3760559537993, 190.19547753851108, 206.75797526113013, 200.47193899812973, 201.1874045848349, 192.69951576881067, 182.85204022938385, 200.03366291426948], 'lossList': [0.0, -1.4137143176794051, 0.0, 62.28610392570496, 0.0, 0.0, 0.0], 'rewardMean': 0.7843533620650129, 'totalEpisodes': 19, 'stepsPerEpisode': 55, 'rewardPerEpisode': 47.20097311924007, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.9038659691878119, 'errorList': [], 'lossList': [0.0, -1.4001332503557204, 0.0, 486.72032745361327, 0.0, 0.0, 0.0], 'rewardMean': 0.7976325406342128, 'totalEpisodes': 53, 'stepsPerEpisode': 83, 'rewardPerEpisode': 71.68378800375326
'totalSteps': 12800, 'rewardStep': 0.782297356032282, 'errorList': [], 'lossList': [0.0, -1.3995222783088683, 0.0, 310.67479393005374, 0.0, 0.0, 0.0], 'rewardMean': 0.7960990221740197, 'totalEpisodes': 83, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.185330973788503
'totalSteps': 14080, 'rewardStep': 0.6541541393869293, 'errorList': [], 'lossList': [0.0, -1.3982568049430848, 0.0, 182.02165721893311, 0.0, 0.0, 0.0], 'rewardMean': 0.7943228954983782, 'totalEpisodes': 121, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.445385817667802
'totalSteps': 15360, 'rewardStep': 0.5335926749096358, 'errorList': [], 'lossList': [0.0, -1.3954986155033111, 0.0, 101.19512268066406, 0.0, 0.0, 0.0], 'rewardMean': 0.7607713883581768, 'totalEpisodes': 146, 'stepsPerEpisode': 46, 'rewardPerEpisode': 32.38525695988749
'totalSteps': 16640, 'rewardStep': 0.927945045712276, 'errorList': [], 'lossList': [0.0, -1.391617450118065, 0.0, 63.97182590484619, 0.0, 0.0, 0.0], 'rewardMean': 0.7762499652914483, 'totalEpisodes': 165, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.499075942412059
'totalSteps': 17920, 'rewardStep': 0.83975094040371, 'errorList': [], 'lossList': [0.0, -1.3870924717187882, 0.0, 40.444229531288144, 0.0, 0.0, 0.0], 'rewardMean': 0.7992141170808087, 'totalEpisodes': 173, 'stepsPerEpisode': 85, 'rewardPerEpisode': 76.82522017082435
'totalSteps': 19200, 'rewardStep': 0.8848519537140335, 'errorList': [], 'lossList': [0.0, -1.3796853077411653, 0.0, 36.84939667701721, 0.0, 0.0, 0.0], 'rewardMean': 0.8030685410849496, 'totalEpisodes': 180, 'stepsPerEpisode': 91, 'rewardPerEpisode': 82.00572695968097
'totalSteps': 20480, 'rewardStep': 0.9199777145870699, 'errorList': [], 'lossList': [0.0, -1.3689560186862946, 0.0, 15.574764572381973, 0.0, 0.0, 0.0], 'rewardMean': 0.8023824215272242, 'totalEpisodes': 185, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.043148039500405
'totalSteps': 21760, 'rewardStep': 0.4673720674093026, 'errorList': [], 'lossList': [0.0, -1.3466579669713974, 0.0, 6.831071417331696, 0.0, 0.0, 0.0], 'rewardMean': 0.7844196120979531, 'totalEpisodes': 189, 'stepsPerEpisode': 208, 'rewardPerEpisode': 147.20771716455775
'totalSteps': 23040, 'rewardStep': 0.765050124546004, 'errorList': [], 'lossList': [0.0, -1.3217114919424058, 0.0, 13.231141830682754, 0.0, 0.0, 0.0], 'rewardMean': 0.7678857985889055, 'totalEpisodes': 194, 'stepsPerEpisode': 227, 'rewardPerEpisode': 188.09623915498366
'totalSteps': 24320, 'rewardStep': 0.6441954522891227, 'errorList': [], 'lossList': [0.0, -1.32647121489048, 0.0, 3.556287495493889, 0.0, 0.0, 0.0], 'rewardMean': 0.7419187468990366, 'totalEpisodes': 197, 'stepsPerEpisode': 292, 'rewardPerEpisode': 231.67340189577018
'totalSteps': 25600, 'rewardStep': 0.4934217033121993, 'errorList': [], 'lossList': [0.0, -1.3246430099010467, 0.0, 3.6622546875476836, 0.0, 0.0, 0.0], 'rewardMean': 0.7130311816270283, 'totalEpisodes': 198, 'stepsPerEpisode': 326, 'rewardPerEpisode': 226.02689806694812
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=74.69
