#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 14
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8797924938144496, 'errorList': [], 'lossList': [0.0, -1.42221364736557, 0.0, 30.05404085755348, 0.0, 0.0, 0.0], 'rewardMean': 0.7758539499788965, 'totalEpisodes': 13, 'stepsPerEpisode': 319, 'rewardPerEpisode': 253.96148496598047
'totalSteps': 3840, 'rewardStep': 0.7620823983306297, 'errorList': [], 'lossList': [0.0, -1.430723203420639, 0.0, 32.26831874012947, 0.0, 0.0, 0.0], 'rewardMean': 0.7712634327628075, 'totalEpisodes': 15, 'stepsPerEpisode': 187, 'rewardPerEpisode': 132.19665579925322
'totalSteps': 5120, 'rewardStep': 0.5257504492708216, 'errorList': [], 'lossList': [0.0, -1.4343161088228227, 0.0, 16.668943725824356, 0.0, 0.0, 0.0], 'rewardMean': 0.7098851868898111, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 881.4049623145763
'totalSteps': 6400, 'rewardStep': 0.7455501598730134, 'errorList': [], 'lossList': [0.0, -1.4144312560558319, 0.0, 255.00225540161134, 0.0, 0.0, 0.0], 'rewardMean': 0.7170181814864515, 'totalEpisodes': 78, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.305222099755863
'totalSteps': 7680, 'rewardStep': 0.7278328245111665, 'errorList': [], 'lossList': [0.0, -1.4133891028165817, 0.0, 121.14266918182373, 0.0, 0.0, 0.0], 'rewardMean': 0.7188206219905706, 'totalEpisodes': 130, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.3893074228311155
'totalSteps': 8960, 'rewardStep': 0.5445755272324416, 'errorList': [], 'lossList': [0.0, -1.4071937954425813, 0.0, 64.93936807632446, 0.0, 0.0, 0.0], 'rewardMean': 0.6939284655965522, 'totalEpisodes': 177, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.337359150685899
'totalSteps': 10240, 'rewardStep': 0.9645569676604665, 'errorList': [226.61277664895624, 195.26239687482425, 216.5317343065202, 160.9733999519296, 148.06565801434465, 224.8396906593354, 213.6445300005297, 228.46159414037572, 215.96488975552083, 209.2058521366434, 195.560268061456, 202.82875116458848, 199.49732970659352, 205.92661574385568, 220.41046847145327, 229.20296566226594, 145.77215413581558, 174.50270370492876, 231.28494945161177, 205.6576137025037, 101.91233988406202, 207.37668783802468, 217.8819464834839, 208.06597325509938, 194.54470624835827, 116.60778638752464, 213.72781762976447, 213.9112744577879, 207.37006803586996, 203.91621945339784, 175.84751537400555, 215.11978379679474, 207.17883299070758, 200.81709491552076, 216.94148753377493, 156.75766186642844, 227.95224392604916, 214.13365694518737, 162.22297973018556, 213.33005717743555, 199.19302073131223, 157.16303098115722, 229.6874266213105, 107.60367436257678, 147.22808887965516, 217.64203140314225, 197.26910425776686, 227.47088495247107, 216.26434664243163, 218.1467390421862], 'lossList': [0.0, -1.3922080320119858, 0.0, 50.26842254638672, 0.0, 0.0, 0.0], 'rewardMean': 0.7277570283545416, 'totalEpisodes': 204, 'stepsPerEpisode': 15, 'rewardPerEpisode': 14.512557426823856, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8085042162560903, 'errorList': [], 'lossList': [0.0, -1.382909808754921, 0.0, 54.88415559768677, 0.0, 0.0, 0.0], 'rewardMean': 0.7367289381213804, 'totalEpisodes': 226, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.62893410620168
'totalSteps': 12800, 'rewardStep': 0.6728513307558291, 'errorList': [], 'lossList': [0.0, -1.3751900297403337, 0.0, 35.957143020629886, 0.0, 0.0, 0.0], 'rewardMean': 0.7303411773848253, 'totalEpisodes': 239, 'stepsPerEpisode': 64, 'rewardPerEpisode': 51.201311457907515
'totalSteps': 14080, 'rewardStep': 0.7619889723899433, 'errorList': [], 'lossList': [0.0, -1.3549176442623139, 0.0, 30.446048555374144, 0.0, 0.0, 0.0], 'rewardMean': 0.7393485340094852, 'totalEpisodes': 247, 'stepsPerEpisode': 69, 'rewardPerEpisode': 53.88177840179972
'totalSteps': 15360, 'rewardStep': 0.9746242583765726, 'errorList': [156.20394373149895, 151.4741502507563, 167.39070349554194, 161.917101898913, 11.579902711465955, 66.58188795510227, 75.38517673484647, 130.179878957985, 172.29212199121815, 150.98591347225334, 132.21511467274178, 31.168857749401656, 153.83451952933234, 115.60406830601273, 191.92735340721725, 53.50383450744666, 119.66756838151274, 20.652448644887254, 75.05672784387419, 172.14605836961664, 23.20154466285441, 155.83890051966176, 147.03267812991803, 105.3285485803904, 100.39967575679327, 140.56889498125705, 158.87849847659945, 18.870254016206463, 156.23431399171318, 163.59946821663868, 32.104802117360336, 188.04299102668355, 80.26204894441038, 164.16305882940998, 66.7895681361085, 111.35892573283923, 41.662618718007906, 188.0522161963599, 125.58484667022125, 136.1627201441294, 28.62322105089462, 107.81274670394704, 153.69083255659655, 197.32885068589485, 183.2713343666425, 179.91665891097833, 163.1604705792891, 110.89830696930915, 125.31964449198612, 97.34603056788862], 'lossList': [0.0, -1.3351519405841827, 0.0, 36.03885652065277, 0.0, 0.0, 0.0], 'rewardMean': 0.7488317104656976, 'totalEpisodes': 256, 'stepsPerEpisode': 161, 'rewardPerEpisode': 136.22210618074038, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.44041958034335416, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7081323417742231, 'totalEpisodes': 265, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.432974817377037
'totalSteps': 17920, 'rewardStep': 0.697420722993096, 'errorList': [], 'lossList': [0.0, -1.3285917472839355, 0.0, 12.761653006076813, 0.0, 0.0, 0.0], 'rewardMean': 0.7033193980862313, 'totalEpisodes': 271, 'stepsPerEpisode': 64, 'rewardPerEpisode': 53.768982390109606
'totalSteps': 19200, 'rewardStep': 0.9571893617753132, 'errorList': [30.92818727414768, 62.73710284161074, 6.843004310604891, 1.61646108339327, 54.95216149676775, 85.7129106054232, 10.34625299483145, 118.20050434393148, 113.67341015465688, 96.12998886977041, 105.73682901065432, 5.399080091311531, 25.001912928824613, 9.24880771089866, 44.966354178468535, 113.36133417766335, 101.0058048164374, 41.38858933729349, 42.91246779912745, 105.61844166199674, 16.207191914112894, 30.86595322609799, 143.41809378352318, 54.89378697975168, 96.11022916037955, 110.25521519662632, 102.44855623558394, 126.59313804176281, 4.3502616924025475, 40.031892906587444, 2.3275996508333243, 156.2456008239579, 85.26164192111239, 60.65475557252248, 60.32582565218966, 27.06042939875552, 145.6147597574419, 70.73034558100046, 46.452357153986135, 61.444596489416355, 138.54669480167303, 106.42295246545922, 118.50594550684181, 155.76866504888878, 91.34464657629427, 17.646363005411633, 112.42372277956046, 65.28078395969195, 119.51132194390546, 84.5586515036162], 'lossList': [0.0, -1.336473559141159, 0.0, 12.791180963516235, 0.0, 0.0, 0.0], 'rewardMean': 0.726255051812646, 'totalEpisodes': 279, 'stepsPerEpisode': 62, 'rewardPerEpisode': 54.56360838584001, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.9268805740268934, 'errorList': [], 'lossList': [0.0, -1.3412866252660751, 0.0, 9.277109533548355, 0.0, 0.0, 0.0], 'rewardMean': 0.7644855564920913, 'totalEpisodes': 284, 'stepsPerEpisode': 242, 'rewardPerEpisode': 201.06337481804067
'totalSteps': 21760, 'rewardStep': 0.5241200967420414, 'errorList': [], 'lossList': [0.0, -1.3235346227884293, 0.0, 5.312738754749298, 0.0, 0.0, 0.0], 'rewardMean': 0.7204418694002488, 'totalEpisodes': 290, 'stepsPerEpisode': 88, 'rewardPerEpisode': 64.75619446614748
'totalSteps': 23040, 'rewardStep': 0.7995784058870519, 'errorList': [], 'lossList': [0.0, -1.3106451350450516, 0.0, 6.196622664928436, 0.0, 0.0, 0.0], 'rewardMean': 0.7195492883633449, 'totalEpisodes': 294, 'stepsPerEpisode': 203, 'rewardPerEpisode': 176.2125796802764
'totalSteps': 24320, 'rewardStep': 0.48061289030798365, 'errorList': [], 'lossList': [0.0, -1.2987073105573654, 0.0, 4.3747482341527935, 0.0, 0.0, 0.0], 'rewardMean': 0.7003254443185603, 'totalEpisodes': 297, 'stepsPerEpisode': 272, 'rewardPerEpisode': 211.48474394968312
'totalSteps': 25600, 'rewardStep': 0.6824314895790774, 'errorList': [], 'lossList': [0.0, -1.297491882443428, 0.0, 3.398287463188171, 0.0, 0.0, 0.0], 'rewardMean': 0.6923696960374738, 'totalEpisodes': 299, 'stepsPerEpisode': 602, 'rewardPerEpisode': 507.70345516358924
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=108.17
