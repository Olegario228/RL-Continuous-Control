#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 146
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8507800270463783, 'errorList': [], 'lossList': [0.0, -1.4478685998916625, 0.0, 39.365658025741574, 0.0, 0.0, 0.0], 'rewardMean': 0.7219789233132688, 'totalEpisodes': 12, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.24598327778762
'totalSteps': 3840, 'rewardStep': 0.9119660047858666, 'errorList': [], 'lossList': [0.0, -1.4647505152225495, 0.0, 34.20932492554188, 0.0, 0.0, 0.0], 'rewardMean': 0.7853079504708015, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.5529992064537
'totalSteps': 5120, 'rewardStep': 0.6901893254984176, 'errorList': [], 'lossList': [0.0, -1.4507897424697875, 0.0, 29.3745927298069, 0.0, 0.0, 0.0], 'rewardMean': 0.7615282942277055, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.2309631369535
'totalSteps': 6400, 'rewardStep': 0.934179387509339, 'errorList': [], 'lossList': [0.0, -1.447676036953926, 0.0, 23.77142547547817, 0.0, 0.0, 0.0], 'rewardMean': 0.7960585128840323, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.3376955322146
'totalSteps': 7680, 'rewardStep': 0.8357471481486605, 'errorList': [], 'lossList': [0.0, -1.4284545880556108, 0.0, 20.1346517534554, 0.0, 0.0, 0.0], 'rewardMean': 0.802673285428137, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.0396818364932
'totalSteps': 8960, 'rewardStep': 0.9697475323448883, 'errorList': [], 'lossList': [0.0, -1.408563517332077, 0.0, 10.874843030869961, 0.0, 0.0, 0.0], 'rewardMean': 0.8265410349876728, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1137.1333425748508
'totalSteps': 10240, 'rewardStep': 0.9362371978588304, 'errorList': [116.97907017865704, 114.54173881388259, 109.23718695268305, 112.11028860249246, 102.08464719922735, 114.50788947381977, 102.95468117430178, 114.22968461253323, 112.0838856382232, 115.3891952167462, 99.73968571761161, 117.0429752660195, 115.95537670779933, 109.63204462235433, 113.45891648344794, 116.75387273591419, 103.05150647995804, 109.01114559105918, 116.74742457763622, 113.99188182474025, 105.16442375351029, 107.68775914915868, 110.20997759616122, 113.41590794361618, 102.98863408158012, 114.74491404696549, 115.58683384999372, 118.1475384019189, 115.841620703738, 114.72614736485228, 100.13639913788889, 105.0954078052773, 106.25980838329771, 115.59578574500742, 106.3176860986389, 105.36416119018382, 116.50202852499267, 116.01204593646936, 94.70322601775632, 118.64909532462038, 107.22496771454844, 114.41073681826188, 117.81228187179141, 116.09389174409554, 108.06068103142493, 108.53124873384284, 110.98658755415616, 110.76151888842351, 114.36343134544623, 115.36637070545582], 'lossList': [0.0, -1.3760668462514878, 0.0, 7.554862009286881, 0.0, 0.0, 0.0], 'rewardMean': 0.8402530553465676, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.014066041107, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6507100412147985, 'errorList': [], 'lossList': [0.0, -1.3474437874555587, 0.0, 789.3853657531738, 0.0, 0.0, 0.0], 'rewardMean': 0.8191927204430377, 'totalEpisodes': 50, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.60427378011955
'totalSteps': 12800, 'rewardStep': 0.8240204750460759, 'errorList': [], 'lossList': [0.0, -1.3466978561878205, 0.0, 552.6341344451904, 0.0, 0.0, 0.0], 'rewardMean': 0.8196754959033414, 'totalEpisodes': 84, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.70012794431799
'totalSteps': 14080, 'rewardStep': 0.9785948626893364, 'errorList': [143.65891105981197, 135.83960638393216, 130.15130751921694, 149.1250119747537, 152.8423988263731, 146.11819691767795, 140.34383484943206, 141.44389599405105, 153.58557325205126, 146.32052498125617, 143.31027839234957, 140.88928068873835, 109.23620782763076, 148.9847933708437, 139.67211926734325, 149.80849307524136, 131.29539691848854, 137.32154308866697, 132.52748634636527, 148.4761240949244, 149.80477143025425, 141.44318080175043, 151.23682553073922, 147.73481225617323, 135.31132849764424, 131.24741638018207, 152.37156473456866, 135.26131101152853, 144.77392564858826, 135.70769543413843, 143.13642251659974, 152.8362219111667, 150.3249308026704, 149.44924799492324, 114.00491746541667, 149.59228616810506, 129.1887648097176, 144.54773918150948, 141.9752567861264, 142.6639642525925, 126.51635726034414, 149.94717888702183, 129.59596283334838, 145.30499886505282, 137.02144936688097, 144.78237948068877, 151.15204923426242, 154.23406664981783, 132.8697369848971, 132.8989665572843], 'lossList': [0.0, -1.3458668166399002, 0.0, 402.4438922119141, 0.0, 0.0, 0.0], 'rewardMean': 0.8582172002142592, 'totalEpisodes': 125, 'stepsPerEpisode': 14, 'rewardPerEpisode': 12.87813233209353, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.9518001425097314, 'errorList': [162.73217896826293, 197.38281532826912, 196.64658833290866, 195.61649297441724, 185.3898078523093, 203.51183066458174, 191.15595412273595, 199.60899483330206, 191.51694376636692, 187.8386258245077, 204.21201323345184, 194.17074489913625, 200.18830704440947, 199.52709319212573, 120.39499895575099, 179.36994183110366, 195.17981512901113, 197.89457670261348, 181.8848526445485, 179.30193041749146, 199.2847138768866, 197.65388879467096, 176.53554926895578, 205.16658788385513, 164.86430379789397, 204.9839171781148, 191.41878115868747, 189.40785434190113, 192.5180998215864, 166.48882115984566, 189.4593090533513, 194.57141154349364, 190.89201000541178, 184.96384199681816, 197.3077457739324, 186.94606803722962, 157.61964112608047, 177.02798813223382, 165.3096593822491, 200.67091726250655, 172.32528110335795, 131.27017730628737, 207.02193174336983, 180.01411655974167, 204.74182735729315, 172.22334486794438, 204.4817068290268, 204.67429676660433, 193.85927147921274, 164.03664326885266], 'lossList': [0.0, -1.3454772961139678, 0.0, 81.91262811660766, 0.0, 0.0, 0.0], 'rewardMean': 0.8683192117605945, 'totalEpisodes': 160, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.801907557748017, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.45856974092226144, 'errorList': [], 'lossList': [0.0, -1.3443765741586686, 0.0, 41.303444414138795, 0.0, 0.0, 0.0], 'rewardMean': 0.822979585374234, 'totalEpisodes': 185, 'stepsPerEpisode': 65, 'rewardPerEpisode': 53.23411554948108
'totalSteps': 17920, 'rewardStep': 0.6974706292228, 'errorList': [], 'lossList': [0.0, -1.3405340361595153, 0.0, 22.555419878959654, 0.0, 0.0, 0.0], 'rewardMean': 0.8237077157466721, 'totalEpisodes': 212, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4170038047363829
'totalSteps': 19200, 'rewardStep': 0.6394066923761862, 'errorList': [], 'lossList': [0.0, -1.3345767652988434, 0.0, 19.772819881439208, 0.0, 0.0, 0.0], 'rewardMean': 0.7942304462333569, 'totalEpisodes': 228, 'stepsPerEpisode': 37, 'rewardPerEpisode': 24.989698475228984
'totalSteps': 20480, 'rewardStep': 0.8020808751288332, 'errorList': [], 'lossList': [0.0, -1.3303699123859405, 0.0, 18.558821034431457, 0.0, 0.0, 0.0], 'rewardMean': 0.7908638189313741, 'totalEpisodes': 240, 'stepsPerEpisode': 131, 'rewardPerEpisode': 99.4776276371261
'totalSteps': 21760, 'rewardStep': 0.786823434797085, 'errorList': [], 'lossList': [0.0, -1.3304080587625504, 0.0, 16.047863953113556, 0.0, 0.0, 0.0], 'rewardMean': 0.7725714091765938, 'totalEpisodes': 246, 'stepsPerEpisode': 86, 'rewardPerEpisode': 61.45193275482183
'totalSteps': 23040, 'rewardStep': 0.6433405231840659, 'errorList': [], 'lossList': [0.0, -1.3228693097829818, 0.0, 11.925159565210343, 0.0, 0.0, 0.0], 'rewardMean': 0.7432817417091173, 'totalEpisodes': 250, 'stepsPerEpisode': 353, 'rewardPerEpisode': 299.0918334710453
'totalSteps': 24320, 'rewardStep': 0.7421278238696681, 'errorList': [], 'lossList': [0.0, -1.3147705972194672, 0.0, 7.308650745153427, 0.0, 0.0, 0.0], 'rewardMean': 0.7524235199746043, 'totalEpisodes': 255, 'stepsPerEpisode': 210, 'rewardPerEpisode': 183.8447075210055
'totalSteps': 25600, 'rewardStep': 0.6318996819591007, 'errorList': [], 'lossList': [0.0, -1.3094871228933334, 0.0, 6.2332197701931, 0.0, 0.0, 0.0], 'rewardMean': 0.7332114406659068, 'totalEpisodes': 256, 'stepsPerEpisode': 674, 'rewardPerEpisode': 544.9493190496976
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=66.4
