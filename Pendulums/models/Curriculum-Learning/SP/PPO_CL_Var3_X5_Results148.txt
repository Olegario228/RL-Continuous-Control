#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 148
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5743667246935684, 'errorList': [], 'lossList': [0.0, -1.4227781456708908, 0.0, 32.65670664548874, 0.0, 0.0, 0.0], 'rewardMean': 0.7366485702601544, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.26877973275243
'totalSteps': 3840, 'rewardStep': 0.9642586212704569, 'errorList': [], 'lossList': [0.0, -1.4228177058696747, 0.0, 36.05832980632782, 0.0, 0.0, 0.0], 'rewardMean': 0.8125185872635886, 'totalEpisodes': 18, 'stepsPerEpisode': 486, 'rewardPerEpisode': 404.4665510892371
'totalSteps': 5120, 'rewardStep': 0.8508296216158928, 'errorList': [], 'lossList': [0.0, -1.4184774017333985, 0.0, 32.966194609999654, 0.0, 0.0, 0.0], 'rewardMean': 0.8220963458516646, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.1579618041003
'totalSteps': 6400, 'rewardStep': 0.8292202825605123, 'errorList': [], 'lossList': [0.0, -1.4022836434841155, 0.0, 24.80774012327194, 0.0, 0.0, 0.0], 'rewardMean': 0.8235211331934341, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1115.5573745304355
'totalSteps': 7680, 'rewardStep': 0.9700305222549787, 'errorList': [], 'lossList': [0.0, -1.390618885755539, 0.0, 18.898429242670534, 0.0, 0.0, 0.0], 'rewardMean': 0.8479393647036915, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.1699604080286
'totalSteps': 8960, 'rewardStep': 0.8243197281035533, 'errorList': [], 'lossList': [0.0, -1.3761669754981996, 0.0, 10.306228601485492, 0.0, 0.0, 0.0], 'rewardMean': 0.8445651309036718, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.260057300939
'totalSteps': 10240, 'rewardStep': 0.9390953590255092, 'errorList': [104.2736054206724, 99.35566031262861, 107.01843657171226, 118.85613308316266, 108.4642018351793, 116.6145200954498, 117.47931466952427, 74.29677915029193, 96.04353843541412, 107.47657983165895, 104.30616915987966, 97.53029652958368, 118.98595240488677, 111.34703719487638, 101.74731596161533, 109.22983060002964, 112.65774250553334, 118.85474149310546, 110.46381663907383, 106.4384070709692, 111.2332934437594, 109.47420478248341, 120.22012303728658, 108.83270653918768, 105.8654658160797, 102.81475014029515, 114.93019675831755, 102.42194483460081, 98.54281768126341, 107.64481854478042, 111.07236789601701, 98.93989194021955, 111.31532485429732, 110.0821082570842, 96.20829627084686, 109.6910558053517, 111.79961110738932, 121.63259675402358, 109.60505011468979, 95.46575909182677, 98.17165837770024, 113.788629668771, 94.98522857383637, 121.60897738735193, 99.4185314212775, 117.55377153283222, 111.290362538389, 109.69024410188777, 104.97826234611097, 114.5022124950784], 'lossList': [0.0, -1.3587167078256608, 0.0, 7.57806249409914, 0.0, 0.0, 0.0], 'rewardMean': 0.8563814094189015, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.922029648771, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5134423129368859, 'errorList': [], 'lossList': [0.0, -1.3457229578495025, 0.0, 808.3878077697753, 0.0, 0.0, 0.0], 'rewardMean': 0.8182770653653442, 'totalEpisodes': 56, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.57071428686495
'totalSteps': 12800, 'rewardStep': 0.6130516780104536, 'errorList': [], 'lossList': [0.0, -1.3452064669132233, 0.0, 653.0279904174805, 0.0, 0.0, 0.0], 'rewardMean': 0.7977545266298551, 'totalEpisodes': 98, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.782430500940059
'totalSteps': 14080, 'rewardStep': 0.9650721481730031, 'errorList': [139.04714091608875, 128.96665821203652, 135.08823049373711, 99.01041169172176, 135.46959417333815, 144.24820523308878, 143.509805079661, 142.90704198687325, 144.99390681284936, 135.4891144195392, 136.08572781136124, 136.08549559247746, 138.9445935957139, 140.6166865734764, 127.99627232643415, 142.85829978489284, 146.19731082410354, 142.06523702664515, 126.18425572548513, 142.66677164787964, 143.37290774862453, 137.2109950228475, 142.3538184877656, 137.16762531231694, 121.10954272630424, 145.44782993954445, 125.65683025823704, 140.63092880455227, 137.94940112211347, 109.4788377883607, 127.61970930609574, 134.93778867256205, 140.7609689951433, 143.29319000124025, 139.9893339573071, 111.99405979456773, 129.22030218575065, 132.76720703572536, 140.97984784347042, 120.56990245406287, 138.55817488619442, 123.37239885132608, 102.38879470218454, 128.02025548654973, 144.4794549364947, 131.87608407880973, 141.36133190182818, 139.0093864456017, 121.86808513920059, 142.10907201585738], 'lossList': [0.0, -1.3441520416736603, 0.0, 334.48859085083006, 0.0, 0.0, 0.0], 'rewardMean': 0.8043686998644815, 'totalEpisodes': 136, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.491195669053582, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.6063811210687966, 'errorList': [], 'lossList': [0.0, -1.3422222167253495, 0.0, 82.42706146240235, 0.0, 0.0, 0.0], 'rewardMean': 0.8075701395020042, 'totalEpisodes': 170, 'stepsPerEpisode': 15, 'rewardPerEpisode': 9.697976283972032
'totalSteps': 16640, 'rewardStep': 0.6286244749564023, 'errorList': [], 'lossList': [0.0, -1.341848664879799, 0.0, 42.71873344421387, 0.0, 0.0, 0.0], 'rewardMean': 0.7740067248705989, 'totalEpisodes': 208, 'stepsPerEpisode': 87, 'rewardPerEpisode': 73.38963135143703
'totalSteps': 17920, 'rewardStep': 0.9062273520130358, 'errorList': [], 'lossList': [0.0, -1.333642801642418, 0.0, 23.322776794433594, 0.0, 0.0, 0.0], 'rewardMean': 0.779546497910313, 'totalEpisodes': 239, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.597481130555154
'totalSteps': 19200, 'rewardStep': 0.4400423344411342, 'errorList': [], 'lossList': [0.0, -1.3281925892829896, 0.0, 25.552229080200195, 0.0, 0.0, 0.0], 'rewardMean': 0.7406287030983753, 'totalEpisodes': 251, 'stepsPerEpisode': 129, 'rewardPerEpisode': 99.94517932339927
'totalSteps': 20480, 'rewardStep': 0.6828473166097774, 'errorList': [], 'lossList': [0.0, -1.326306595802307, 0.0, 14.14335305929184, 0.0, 0.0, 0.0], 'rewardMean': 0.7119103825338551, 'totalEpisodes': 261, 'stepsPerEpisode': 104, 'rewardPerEpisode': 81.88631316806483
'totalSteps': 21760, 'rewardStep': 0.5206002623907937, 'errorList': [], 'lossList': [0.0, -1.3133212697505952, 0.0, 12.60194011926651, 0.0, 0.0, 0.0], 'rewardMean': 0.6815384359625792, 'totalEpisodes': 267, 'stepsPerEpisode': 137, 'rewardPerEpisode': 98.4997133231869
'totalSteps': 23040, 'rewardStep': 0.8579962624731526, 'errorList': [], 'lossList': [0.0, -1.2916031992435455, 0.0, 8.797128723859787, 0.0, 0.0, 0.0], 'rewardMean': 0.6734285263073435, 'totalEpisodes': 273, 'stepsPerEpisode': 311, 'rewardPerEpisode': 267.84693249151553
'totalSteps': 24320, 'rewardStep': 0.22030821541524143, 'errorList': [], 'lossList': [0.0, -1.275876505970955, 0.0, 9.918638585805892, 0.0, 0.0, 0.0], 'rewardMean': 0.6441151165551791, 'totalEpisodes': 276, 'stepsPerEpisode': 127, 'rewardPerEpisode': 85.80037650382445
'totalSteps': 25600, 'rewardStep': 0.8903186515944573, 'errorList': [], 'lossList': [0.0, -1.2639216786623002, 0.0, 7.172328577041626, 0.0, 0.0, 0.0], 'rewardMean': 0.6718418139135796, 'totalEpisodes': 278, 'stepsPerEpisode': 644, 'rewardPerEpisode': 537.8972237264916
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=55.56
