#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 10000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 149
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 10000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8201684769572979, 'errorList': [], 'lossList': [0.0, -1.4210873800516128, 0.0, 28.444127711057664, 0.0, 0.0, 0.0], 'rewardMean': 0.7847296706282895, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9761498400827
'totalSteps': 3840, 'rewardStep': 0.7653373832477092, 'errorList': [], 'lossList': [0.0, -1.4348077225685119, 0.0, 35.210834572315214, 0.0, 0.0, 0.0], 'rewardMean': 0.7782655748347627, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.42383945361449
'totalSteps': 5120, 'rewardStep': 0.7037243804382802, 'errorList': [], 'lossList': [0.0, -1.4309587329626083, 0.0, 26.054333394765855, 0.0, 0.0, 0.0], 'rewardMean': 0.759630276235642, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.2315231867453
'totalSteps': 6400, 'rewardStep': 0.9045158821659532, 'errorList': [], 'lossList': [0.0, -1.423608484864235, 0.0, 19.108394244909288, 0.0, 0.0, 0.0], 'rewardMean': 0.7886073974217043, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1021.8268351816707
'totalSteps': 7680, 'rewardStep': 0.8621821448988516, 'errorList': [], 'lossList': [0.0, -1.4302021086215972, 0.0, 14.129696964025497, 0.0, 0.0, 0.0], 'rewardMean': 0.8008698553345622, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.0003036673606
'totalSteps': 8960, 'rewardStep': 0.6667118019305125, 'errorList': [], 'lossList': [0.0, -1.4237660312652587, 0.0, 8.302888275682927, 0.0, 0.0, 0.0], 'rewardMean': 0.7817044191339837, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1026.870155947701
'totalSteps': 10240, 'rewardStep': 0.8107103756854488, 'errorList': [], 'lossList': [0.0, -1.4242564284801482, 0.0, 48.33401102185249, 0.0, 0.0, 0.0], 'rewardMean': 0.7853301637029169, 'totalEpisodes': 16, 'stepsPerEpisode': 943, 'rewardPerEpisode': 753.9717531789779
'totalSteps': 11520, 'rewardStep': 0.46009041760667924, 'errorList': [], 'lossList': [0.0, -1.4154478460550308, 0.0, 834.440231628418, 0.0, 0.0, 0.0], 'rewardMean': 0.7491924141366683, 'totalEpisodes': 71, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.834705990853081
'totalSteps': 12800, 'rewardStep': 0.9295478374539256, 'errorList': [], 'lossList': [0.0, -1.41552927672863, 0.0, 407.37702384948733, 0.0, 0.0, 0.0], 'rewardMean': 0.7672279564683939, 'totalEpisodes': 119, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.31306128130452
'totalSteps': 14080, 'rewardStep': 0.5508937857259949, 'errorList': [], 'lossList': [0.0, -1.4149910378456116, 0.0, 138.9151463317871, 0.0, 0.0, 0.0], 'rewardMean': 0.7473882486110652, 'totalEpisodes': 165, 'stepsPerEpisode': 10, 'rewardPerEpisode': 5.821378869264843
'totalSteps': 15360, 'rewardStep': 0.4778169990580155, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6844010624021677, 'totalEpisodes': 217, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.113170160007824
'totalSteps': 16640, 'rewardStep': 0.9575197527094099, 'errorList': [106.62950804356744, 111.79712032222729, 102.78279013107438, 113.98451638708639, 105.32645915658428, 107.2902515694276, 112.30096346927347, 113.48445057180439, 115.59702179257468, 112.29187418707022, 116.28294994048393, 115.44249858805188, 112.4428639052206, 109.10618523849527, 116.69678143620706, 115.35795160704946, 91.09883455446207, 107.3048399549029, 106.70347476705399, 115.43708853917965, 114.73960244738447, 113.03974167815143, 110.5673791473641, 108.76131994171004, 111.47927166837987, 114.41876072082898, 110.73109793499498, 111.8400471334334, 114.43136622663202, 112.534249508465, 105.72892400961564, 109.62219802679692, 114.48368309163398, 104.63274625199894, 105.58938803936039, 115.19586491322947, 110.38802969957847, 107.39760778588021, 113.3586099611491, 105.45233709541394, 108.61976619155699, 117.9417333610474, 106.98043055848588, 97.35537672639073, 101.34934765801427, 113.38142566227536, 110.38663664706789, 114.84778904933303, 117.38766890839464, 108.0410849779803], 'lossList': [0.0, -1.4126128023862838, 0.0, 105.09701538085938, 0.0, 0.0, 0.0], 'rewardMean': 0.7097805996292806, 'totalEpisodes': 266, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.944677464633396, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.7330616575090847, 'errorList': [], 'lossList': [0.0, -1.4037403219938278, 0.0, 70.09304634094238, 0.0, 0.0, 0.0], 'rewardMean': 0.6926351771635938, 'totalEpisodes': 303, 'stepsPerEpisode': 28, 'rewardPerEpisode': 25.106391947409676
'totalSteps': 19200, 'rewardStep': 0.9212655973608683, 'errorList': [], 'lossList': [0.0, -1.3920763486623764, 0.0, 50.87371835708618, 0.0, 0.0, 0.0], 'rewardMean': 0.6985435224097956, 'totalEpisodes': 332, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.429621333765908
'totalSteps': 20480, 'rewardStep': 0.8779549556566372, 'errorList': [], 'lossList': [0.0, -1.3877579081058502, 0.0, 45.20934562683105, 0.0, 0.0, 0.0], 'rewardMean': 0.719667837782408, 'totalEpisodes': 352, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.173220814184088
'totalSteps': 21760, 'rewardStep': 0.7879308284504722, 'errorList': [], 'lossList': [0.0, -1.3843102639913558, 0.0, 28.999657554626467, 0.0, 0.0, 0.0], 'rewardMean': 0.7173898830589103, 'totalEpisodes': 369, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.7900630473529295
'totalSteps': 23040, 'rewardStep': 0.8603894535802521, 'errorList': [], 'lossList': [0.0, -1.3729122549295425, 0.0, 26.78269642829895, 0.0, 0.0, 0.0], 'rewardMean': 0.7574197866562675, 'totalEpisodes': 383, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.30402283214203
'totalSteps': 24320, 'rewardStep': 0.7708306154323238, 'errorList': [], 'lossList': [0.0, -1.3583063077926636, 0.0, 16.75922151327133, 0.0, 0.0, 0.0], 'rewardMean': 0.7415480644541075, 'totalEpisodes': 391, 'stepsPerEpisode': 80, 'rewardPerEpisode': 61.94392328615881
'totalSteps': 25600, 'rewardStep': 0.6274744058117175, 'errorList': [], 'lossList': [0.0, -1.3554108864068986, 0.0, 12.400207097530364, 0.0, 0.0, 0.0], 'rewardMean': 0.7492061264626797, 'totalEpisodes': 398, 'stepsPerEpisode': 109, 'rewardPerEpisode': 93.14080879852132
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=45.46
