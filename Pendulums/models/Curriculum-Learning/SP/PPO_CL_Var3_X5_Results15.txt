#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 15
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9359216278712528, 'errorList': [], 'lossList': [0.0, -1.4349353033304215, 0.0, 30.4917695581913, 0.0, 0.0, 0.0], 'rewardMean': 0.9157516095397099, 'totalEpisodes': 8, 'stepsPerEpisode': 530, 'rewardPerEpisode': 378.11768025622206
'totalSteps': 3840, 'rewardStep': 0.7507532819216919, 'errorList': [], 'lossList': [0.0, -1.4197757613658906, 0.0, 32.278425612449645, 0.0, 0.0, 0.0], 'rewardMean': 0.8607521670003706, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 203.45878386949764
'totalSteps': 5120, 'rewardStep': 0.7421004353044488, 'errorList': [], 'lossList': [0.0, -1.4145870125293731, 0.0, 22.58486584305763, 0.0, 0.0, 0.0], 'rewardMean': 0.8310892340763901, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 963.3102983851447
'totalSteps': 6400, 'rewardStep': 0.9540780532178494, 'errorList': [63.53774236162497, 65.38927050353178, 64.0103815023496, 57.424073136534844, 62.71637321196594, 64.0649277720772, 62.88585262250634, 61.611327268216094, 61.97551579952342, 64.71899926685417, 67.24988909324219, 64.63502553504685, 64.25769931866098, 62.76407744493086, 66.04436497167421, 65.70050411973111, 61.903832965882934, 51.03981990022625, 66.56699958311553, 63.41208847288115, 64.81741445230672, 65.1257360272808, 64.63785187191071, 64.9222401877893, 64.96377223168093, 64.6975572812884, 63.621966033519335, 62.34094093850563, 60.35159735094701, 62.16829182401193, 58.97491219788573, 64.30633745242964, 63.88493034703245, 63.866665198070194, 66.78413444167256, 64.18521849722663, 64.27980075201597, 62.546964531839926, 63.41105996350431, 62.91739146229308, 65.52280437078421, 66.11144439092108, 63.0834498350911, 65.96037058256606, 64.1277356494285, 64.36099985301779, 55.779726981269555, 64.42620113743061, 56.72815248953575, 65.83612344282422], 'lossList': [0.0, -1.40107797563076, 0.0, 310.8257455444336, 0.0, 0.0, 0.0], 'rewardMean': 0.855686997904682, 'totalEpisodes': 86, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.480145860534096, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.5343523399289998, 'errorList': [], 'lossList': [0.0, -1.403128217458725, 0.0, 115.98814847946167, 0.0, 0.0, 0.0], 'rewardMean': 0.8021312215754016, 'totalEpisodes': 154, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.2291243418468065
'totalSteps': 8960, 'rewardStep': 0.8702634253409414, 'errorList': [], 'lossList': [0.0, -1.3986881297826768, 0.0, 51.912151927947995, 0.0, 0.0, 0.0], 'rewardMean': 0.8118643935419073, 'totalEpisodes': 212, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.255062727758713
'totalSteps': 10240, 'rewardStep': 0.7079414892634612, 'errorList': [], 'lossList': [0.0, -1.383061962723732, 0.0, 40.78079921722412, 0.0, 0.0, 0.0], 'rewardMean': 0.7988740305071016, 'totalEpisodes': 250, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.228629786542445
'totalSteps': 11520, 'rewardStep': 0.901180951963292, 'errorList': [], 'lossList': [0.0, -1.366253098845482, 0.0, 37.59646785736084, 0.0, 0.0, 0.0], 'rewardMean': 0.8102414662244561, 'totalEpisodes': 271, 'stepsPerEpisode': 69, 'rewardPerEpisode': 56.43547833425355
'totalSteps': 12800, 'rewardStep': 0.8446194560579501, 'errorList': [], 'lossList': [0.0, -1.3423085176944733, 0.0, 31.36347668647766, 0.0, 0.0, 0.0], 'rewardMean': 0.8136792652078055, 'totalEpisodes': 283, 'stepsPerEpisode': 96, 'rewardPerEpisode': 85.87471873800206
'totalSteps': 14080, 'rewardStep': 0.5405305421240729, 'errorList': [], 'lossList': [0.0, -1.326451011300087, 0.0, 26.817881422042845, 0.0, 0.0, 0.0], 'rewardMean': 0.7781741602993961, 'totalEpisodes': 289, 'stepsPerEpisode': 64, 'rewardPerEpisode': 51.479750121977325
'totalSteps': 15360, 'rewardStep': 0.8197620966315946, 'errorList': [], 'lossList': [0.0, -1.3270023715496064, 0.0, 21.615444917678833, 0.0, 0.0, 0.0], 'rewardMean': 0.7665582071754302, 'totalEpisodes': 295, 'stepsPerEpisode': 68, 'rewardPerEpisode': 61.586935470341466
'totalSteps': 16640, 'rewardStep': 0.7741212776249863, 'errorList': [], 'lossList': [0.0, -1.330428947210312, 0.0, 10.253272017240525, 0.0, 0.0, 0.0], 'rewardMean': 0.7688950067457596, 'totalEpisodes': 298, 'stepsPerEpisode': 199, 'rewardPerEpisode': 167.8966879511444
'totalSteps': 17920, 'rewardStep': 0.4192814761019482, 'errorList': [], 'lossList': [0.0, -1.3180525624752044, 0.0, 7.2197349727153775, 0.0, 0.0, 0.0], 'rewardMean': 0.7366131108255096, 'totalEpisodes': 301, 'stepsPerEpisode': 241, 'rewardPerEpisode': 165.81537186013298
'totalSteps': 19200, 'rewardStep': 0.8948237785681742, 'errorList': [], 'lossList': [0.0, -1.2990622437000274, 0.0, 5.502177760004997, 0.0, 0.0, 0.0], 'rewardMean': 0.730687683360542, 'totalEpisodes': 304, 'stepsPerEpisode': 244, 'rewardPerEpisode': 221.60587720085564
'totalSteps': 20480, 'rewardStep': 0.7910974727296489, 'errorList': [], 'lossList': [0.0, -1.2890023016929626, 0.0, 4.1123827549815175, 0.0, 0.0, 0.0], 'rewardMean': 0.756362196640607, 'totalEpisodes': 305, 'stepsPerEpisode': 512, 'rewardPerEpisode': 431.41401645833076
'totalSteps': 21760, 'rewardStep': 0.9470874754722702, 'errorList': [0.05866914753503328, 0.040350601407875, 0.012744643161651753, 0.05324092879849067, 0.03697499908739059, 0.03082854638297539, 0.05438019584678893, 0.03156752106058865, 0.012076957615010795, 0.052667441199201505, 0.012403689845227817, 0.03440553114142579, 0.03600971735104388, 0.029459506562704643, 0.053874327427200615, 0.04657232753158407, 0.04616845362386471, 0.018266991174145875, 0.0611697792387157, 0.015280225296076599, 0.02656280747497517, 0.048691166343171784, 0.018739425960197214, 0.03499988150231313, 0.012210208896752632, 0.012797403965288636, 0.14353927000951286, 0.012234236091779246, 0.013074949771447636, 0.012150424408449797, 0.02312638425108704, 0.012451177387894613, 0.061947055701499346, 0.027992797444516163, 0.012727591789621037, 0.03705523786625171, 0.1349447630289381, 0.05251874344160194, 0.012453540824241332, 0.040271196582042146, 0.01252594925961166, 0.024637670090405846, 0.019211547205259902, 0.07125274359536835, 0.04857907338830566, 0.09399347704954056, 0.024315801025978065, 0.012214877050512487, 0.01955334530327855, 0.02475309344721294], 'lossList': [0.0, -1.2929818952083587, 0.0, 2.9959208419919015, 0.0, 0.0, 0.0], 'rewardMean': 0.7640446016537398, 'totalEpisodes': 306, 'stepsPerEpisode': 855, 'rewardPerEpisode': 756.3563628629595, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=84.8
