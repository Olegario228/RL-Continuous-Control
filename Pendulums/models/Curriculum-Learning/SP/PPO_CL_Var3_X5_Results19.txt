#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 19
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8806248035683253, 'errorList': [], 'lossList': [0.0, -1.4239072281122207, 0.0, 34.09518264055252, 0.0, 0.0, 0.0], 'rewardMean': 0.8019878637238131, 'totalEpisodes': 13, 'stepsPerEpisode': 311, 'rewardPerEpisode': 259.4799239876371
'totalSteps': 3840, 'rewardStep': 0.7168829905413274, 'errorList': [], 'lossList': [0.0, -1.4269083529710769, 0.0, 32.94262966871261, 0.0, 0.0, 0.0], 'rewardMean': 0.7736195726629846, 'totalEpisodes': 16, 'stepsPerEpisode': 174, 'rewardPerEpisode': 130.93798700995907
'totalSteps': 5120, 'rewardStep': 0.6126538173409493, 'errorList': [], 'lossList': [0.0, -1.431207708120346, 0.0, 16.367607214450835, 0.0, 0.0, 0.0], 'rewardMean': 0.7333781338324759, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 885.87052130995
'totalSteps': 6400, 'rewardStep': 0.5822083726728645, 'errorList': [], 'lossList': [0.0, -1.414381292462349, 0.0, 334.1155379486084, 0.0, 0.0, 0.0], 'rewardMean': 0.7031441816005536, 'totalEpisodes': 102, 'stepsPerEpisode': 25, 'rewardPerEpisode': 17.58594682921495
'totalSteps': 7680, 'rewardStep': 0.9693473597052701, 'errorList': [91.72779220314062, 78.87425536358944, 72.91963637478479, 81.25653288286772, 83.82450918161499, 81.44556428077203, 80.45420399347891, 82.7779986468493, 75.08216402987044, 84.48106487961, 87.17924834379956, 78.30487823885642, 82.37034626658237, 80.87050554747537, 92.73613041474509, 81.56171978176366, 83.5312990898878, 83.43158329155752, 87.14806590137674, 83.43567795289522, 83.40876262540644, 74.66886237146062, 81.47926363543125, 85.98198446570507, 76.03906994453781, 80.340051069787, 79.92421210009677, 87.15944874080749, 84.03816814204367, 90.03234564524377, 82.23312011622865, 72.26610588674913, 83.73781885243221, 88.04855249469847, 81.01827188886408, 67.07928222317796, 80.1953006967688, 83.95721689162166, 81.34343697545737, 80.3759445613481, 92.62665843907621, 82.3652055532127, 91.10063328759631, 74.04116431470577, 82.85122169067192, 90.29628175637745, 81.750151501273, 79.24873268668712, 83.74337995601464, 68.39260136511672], 'lossList': [0.0, -1.4154789543151856, 0.0, 145.69689373016357, 0.0, 0.0, 0.0], 'rewardMean': 0.7475113779513397, 'totalEpisodes': 169, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9693473597052701, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.8178517964649374, 'errorList': [], 'lossList': [0.0, -1.4080605775117874, 0.0, 53.03233215332031, 0.0, 0.0, 0.0], 'rewardMean': 0.7575600091675679, 'totalEpisodes': 249, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.8445489096818344
'totalSteps': 10240, 'rewardStep': 0.8216262472538559, 'errorList': [], 'lossList': [0.0, -1.3945507544279099, 0.0, 46.41549383163452, 0.0, 0.0, 0.0], 'rewardMean': 0.7655682889283539, 'totalEpisodes': 280, 'stepsPerEpisode': 27, 'rewardPerEpisode': 21.147366512661247
'totalSteps': 11520, 'rewardStep': 0.8162966641831622, 'errorList': [], 'lossList': [0.0, -1.3790635657310486, 0.0, 43.3777560043335, 0.0, 0.0, 0.0], 'rewardMean': 0.7712047750677771, 'totalEpisodes': 291, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.808511365026484
'totalSteps': 12800, 'rewardStep': 0.7225670857598462, 'errorList': [], 'lossList': [0.0, -1.3632328313589097, 0.0, 39.97044792175293, 0.0, 0.0, 0.0], 'rewardMean': 0.766341006136984, 'totalEpisodes': 298, 'stepsPerEpisode': 65, 'rewardPerEpisode': 52.89415683976971
'totalSteps': 14080, 'rewardStep': 0.8302330504431223, 'errorList': [], 'lossList': [0.0, -1.3570188224315642, 0.0, 18.37487945199013, 0.0, 0.0, 0.0], 'rewardMean': 0.7770292187933661, 'totalEpisodes': 303, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.13315615519187
'totalSteps': 15360, 'rewardStep': 0.9473591534504079, 'errorList': [0.1684656540892252, 14.021366617963565, 6.946243738815364, 19.594015242444698, 7.4693635198255866, 26.040340118591494, 19.10404114038245, 21.736340635349265, 2.7562072057562275, 13.815007854109266, 8.58404588481036, 22.491329372535503, 19.237783453238947, 15.404129662126094, 4.021021821888877, 12.70868220528303, 19.18348698448198, 15.404961665354113, 2.585237218184133, 5.572502380577004, 6.238067555671871, 2.1665801593282334, 1.5989185949964768, 5.2738242843114085, 3.192968933522099, 12.500873905598526, 10.802636865174017, 8.430107541423556, 10.71147891271248, 7.01555885774608, 2.856722327075299, 1.7545323782452233, 1.3850069006750925, 16.562863196193945, 5.65643468740974, 10.395971361915231, 0.1790990387951306, 11.523370867729158, 13.366606363037029, 8.914357446182422, 2.484679417143482, 11.166571655561025, 15.615489766973774, 5.601154783163159, 7.168914602622276, 6.7407595950324675, 14.924433282336228, 11.529840280013593, 11.150330945999015, 19.409004091361243], 'lossList': [0.0, -1.3535947889089583, 0.0, 47.65285728931427, 0.0, 0.0, 0.0], 'rewardMean': 0.7837026537815743, 'totalEpisodes': 310, 'stepsPerEpisode': 110, 'rewardPerEpisode': 93.07761575851647, 'successfulTests': 2
'totalSteps': 16640, 'rewardStep': 0.45241183526860407, 'errorList': [], 'lossList': [0.0, -1.345120330452919, 0.0, 17.912334901690482, 0.0, 0.0, 0.0], 'rewardMean': 0.757255538254302, 'totalEpisodes': 315, 'stepsPerEpisode': 182, 'rewardPerEpisode': 148.08593400034593
'totalSteps': 17920, 'rewardStep': 0.6166045984876408, 'errorList': [], 'lossList': [0.0, -1.3366681504249573, 0.0, 8.429647480249406, 0.0, 0.0, 0.0], 'rewardMean': 0.7576506163689711, 'totalEpisodes': 321, 'stepsPerEpisode': 167, 'rewardPerEpisode': 116.79562963532555
'totalSteps': 19200, 'rewardStep': 0.6821251595159914, 'errorList': [], 'lossList': [0.0, -1.3308641731739044, 0.0, 25.045243532657622, 0.0, 0.0, 0.0], 'rewardMean': 0.7676422950532837, 'totalEpisodes': 328, 'stepsPerEpisode': 129, 'rewardPerEpisode': 102.95802169170662
'totalSteps': 20480, 'rewardStep': 0.9415485699126172, 'errorList': [1.5949720431199057, 23.712834689804595, 21.052428436378797, 1.2161759971361499, 6.394141147639061, 27.253191773753684, 2.7898665346827394, 6.265922409284699, 38.415452006476016, 0.3246769071941513, 8.444122065252401, 8.084667016642447, 9.18664321832006, 3.1734924296941953, 45.823780275622156, 13.872755619101534, 13.948593306848869, 37.132860338415114, 13.636435788191895, 5.940181069722067, 3.174059667706341, 5.343315097658273, 11.673780350149148, 14.657445114950914, 2.083916660837427, 16.98651574452719, 28.804013817822764, 19.316856588409166, 5.488948970630238, 14.518073358105417, 4.566233411726498, 7.89088728681764, 13.123414965263184, 28.899123133958994, 11.795659042353947, 9.23696563787019, 16.85074578592117, 7.184560125838432, 2.6767943205130784, 1.50368450832169, 0.5653705949825818, 3.5080720948447843, 8.669898591685145, 32.304974272690956, 1.3531033783983868, 27.38443662550438, 37.76824900218007, 17.14222132766089, 9.302412382047734, 14.240682901059511], 'lossList': [0.0, -1.3144259333610535, 0.0, 7.229621465206146, 0.0, 0.0, 0.0], 'rewardMean': 0.7648624160740185, 'totalEpisodes': 335, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.019094837952096, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.6798337134282197, 'errorList': [], 'lossList': [0.0, -1.3241660928726195, 0.0, 4.186617407798767, 0.0, 0.0, 0.0], 'rewardMean': 0.7510606077703468, 'totalEpisodes': 338, 'stepsPerEpisode': 323, 'rewardPerEpisode': 266.9139823328102
'totalSteps': 23040, 'rewardStep': 0.7460372754108775, 'errorList': [], 'lossList': [0.0, -1.3220064038038253, 0.0, 4.259479047060013, 0.0, 0.0, 0.0], 'rewardMean': 0.743501710586049, 'totalEpisodes': 342, 'stepsPerEpisode': 282, 'rewardPerEpisode': 236.46319601574038
'totalSteps': 24320, 'rewardStep': 0.5593617922893617, 'errorList': [], 'lossList': [0.0, -1.321476680636406, 0.0, 3.27536440461874, 0.0, 0.0, 0.0], 'rewardMean': 0.7178082233966687, 'totalEpisodes': 345, 'stepsPerEpisode': 176, 'rewardPerEpisode': 136.4566070878547
'totalSteps': 25600, 'rewardStep': 0.8931641554330252, 'errorList': [], 'lossList': [0.0, -1.314415028691292, 0.0, 3.3333280158042906, 0.0, 0.0, 0.0], 'rewardMean': 0.7348679303639868, 'totalEpisodes': 348, 'stepsPerEpisode': 196, 'rewardPerEpisode': 171.62059543037392
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=15360, timeSpent=109.63
