#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092112940900525, 'errorList': [], 'lossList': [0.0, -1.4230972903966903, 0.0, 34.855349187850955, 0.0, 0.0, 0.0], 'rewardMean': 0.862424317936881, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.76631046929405
'totalSteps': 3840, 'rewardStep': 0.5446330944215152, 'errorList': [], 'lossList': [0.0, -1.414960142970085, 0.0, 33.65882135391235, 0.0, 0.0, 0.0], 'rewardMean': 0.7564939100984258, 'totalEpisodes': 83, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.876714155227667
'totalSteps': 5120, 'rewardStep': 0.4825407219578186, 'errorList': [], 'lossList': [0.0, -1.4055249005556107, 0.0, 28.203263006210328, 0.0, 0.0, 0.0], 'rewardMean': 0.688005613063274, 'totalEpisodes': 93, 'stepsPerEpisode': 84, 'rewardPerEpisode': 57.01516837048006
'totalSteps': 6400, 'rewardStep': 0.8621409749927988, 'errorList': [], 'lossList': [0.0, -1.4076056492328644, 0.0, 93.66425373077392, 0.0, 0.0, 0.0], 'rewardMean': 0.722832685449179, 'totalEpisodes': 132, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.064569092652846
'totalSteps': 7680, 'rewardStep': 0.8148372938382837, 'errorList': [], 'lossList': [0.0, -1.4011501735448837, 0.0, 76.22239740371704, 0.0, 0.0, 0.0], 'rewardMean': 0.7381667868473633, 'totalEpisodes': 164, 'stepsPerEpisode': 57, 'rewardPerEpisode': 42.481207116870266
'totalSteps': 8960, 'rewardStep': 0.4776426533625578, 'errorList': [], 'lossList': [0.0, -1.3863172900676728, 0.0, 42.96898303985596, 0.0, 0.0, 0.0], 'rewardMean': 0.7009490534923911, 'totalEpisodes': 178, 'stepsPerEpisode': 13, 'rewardPerEpisode': 7.644229194007423
'totalSteps': 10240, 'rewardStep': 0.6891040176786956, 'errorList': [], 'lossList': [0.0, -1.3670335495471955, 0.0, 10.74235629916191, 0.0, 0.0, 0.0], 'rewardMean': 0.699468424015679, 'totalEpisodes': 186, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.049198103282151
'totalSteps': 11520, 'rewardStep': 0.8377096145873285, 'errorList': [], 'lossList': [0.0, -1.3613397783041001, 0.0, 16.99572187423706, 0.0, 0.0, 0.0], 'rewardMean': 0.7148285563014178, 'totalEpisodes': 192, 'stepsPerEpisode': 154, 'rewardPerEpisode': 130.69233379410082
'totalSteps': 12800, 'rewardStep': 0.9855856953804321, 'errorList': [18.58004036184304, 4.644834458572219, 9.169079628561828, 56.32645963969566, 0.9458778568549498, 4.017072039403176, 12.016846951725336, 25.500087243044668, 5.095494194031249, 39.88876066990535, 41.2061569469563, 9.44185900994634, 3.2301501243855983, 5.961776001091259, 42.58520496040375, 5.752074441676168, 6.976558584444107, 30.418255924122708, 9.175495582288466, 0.9811838027754803, 31.103638885153796, 22.611511120763804, 9.326394941689813, 6.910679130644168, 0.3608622886803771, 3.1766305809419113, 18.42263862924179, 3.1017643514897113, 2.7221711237257242, 3.930103741724159, 12.245696053936923, 9.516125542941314, 7.92317540424974, 11.99780534689556, 4.418150499904758, 3.6039691836345153, 18.245258381250526, 17.19322784631333, 50.19285647797365, 11.233402602650411, 1.5136861180813872, 1.4697876493058746, 22.499342962438046, 1.558255052067799, 44.06401674027831, 3.584599915570123, 4.255482532027309, 9.361114586388465, 3.8674334611279066, 5.476035598796601], 'lossList': [0.0, -1.3464031636714935, 0.0, 23.55749650478363, 0.0, 0.0, 0.0], 'rewardMean': 0.7419042702093193, 'totalEpisodes': 199, 'stepsPerEpisode': 35, 'rewardPerEpisode': 32.774621599641144, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.8777493909787423, 'errorList': [], 'lossList': [0.0, -1.3261473286151886, 0.0, 6.979041098356247, 0.0, 0.0, 0.0], 'rewardMean': 0.7481154751288226, 'totalEpisodes': 203, 'stepsPerEpisode': 103, 'rewardPerEpisode': 90.14875925955877
'totalSteps': 15360, 'rewardStep': 0.6352760907251157, 'errorList': [], 'lossList': [0.0, -1.3324040281772613, 0.0, 9.046988945007325, 0.0, 0.0, 0.0], 'rewardMean': 0.7207219547923288, 'totalEpisodes': 206, 'stepsPerEpisode': 557, 'rewardPerEpisode': 431.16195471557717
'totalSteps': 16640, 'rewardStep': 0.8729349193381468, 'errorList': [], 'lossList': [0.0, -1.3372042214870452, 0.0, 5.5746030658483505, 0.0, 0.0, 0.0], 'rewardMean': 0.753552137283992, 'totalEpisodes': 208, 'stepsPerEpisode': 114, 'rewardPerEpisode': 97.97391806263774
'totalSteps': 17920, 'rewardStep': 0.6897357669720756, 'errorList': [], 'lossList': [0.0, -1.3078758329153062, 0.0, 3.9948441910743715, 0.0, 0.0, 0.0], 'rewardMean': 0.7742716417854176, 'totalEpisodes': 208, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 947.1272938190707
'totalSteps': 19200, 'rewardStep': 0.9680723612090896, 'errorList': [0.11622466658809301, 0.08865707269085847, 0.15339693313360975, 0.12469260673682188, 0.08309755410425272, 0.07929390323750891, 0.13461759491457584, 0.09422107410846414, 0.1512397331990327, 0.06652662230858265, 0.10924976268349566, 0.07840928355500508, 0.05877006689868371, 0.07669452122763563, 0.0876148261007409, 0.05871227181350155, 0.161500064085495, 0.1751742154400258, 0.10582359992373894, 0.04057844888754059, 0.054538524134176924, 0.10202124172667529, 0.1583063314003024, 0.08968254170500055, 0.09303663952914842, 0.06462022032306518, 0.04019608859911349, 0.06692586765336998, 0.07414650385881168, 0.10441504659098237, 0.037435565954175694, 0.12230438743643382, 0.04465668148915514, 0.10240408220021706, 0.15690865873406043, 0.1016226552034543, 0.04552647831473524, 0.1511820275155024, 0.043570872379848864, 0.09705811758308149, 0.08726488789713213, 0.1143676265723107, 0.12107575779918103, 0.122984869431135, 0.06472398787171073, 0.0873138402802507, 0.0809043771986724, 0.0546775207089948, 0.06346618721814563, 0.13128367389148934], 'lossList': [0.0, -1.2705308097600936, 0.0, 17.973278911113738, 0.0, 0.0, 0.0], 'rewardMean': 0.7848647804070468, 'totalEpisodes': 209, 'stepsPerEpisode': 755, 'rewardPerEpisode': 631.6341177023643, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=76.45
