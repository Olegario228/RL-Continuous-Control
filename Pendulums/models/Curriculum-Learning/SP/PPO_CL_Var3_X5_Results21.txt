#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 21
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8507169024171919, 'errorList': [], 'lossList': [0.0, -1.4478617000579834, 0.0, 39.35378210067749, 0.0, 0.0, 0.0], 'rewardMean': 0.7219473609986756, 'totalEpisodes': 12, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.24259516119521
'totalSteps': 3840, 'rewardStep': 0.908970069478866, 'errorList': [], 'lossList': [0.0, -1.4645809137821197, 0.0, 33.690715857744216, 0.0, 0.0, 0.0], 'rewardMean': 0.7842882638254057, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1023.2112949823936
'totalSteps': 5120, 'rewardStep': 0.6280485708979626, 'errorList': [], 'lossList': [0.0, -1.4504030686616898, 0.0, 25.801240480542184, 0.0, 0.0, 0.0], 'rewardMean': 0.7452283405935449, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1018.3211706023568
'totalSteps': 6400, 'rewardStep': 0.4519418631600508, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6474661814490469, 'totalEpisodes': 73, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.047333457946397
'totalSteps': 7680, 'rewardStep': 0.7512539876051896, 'errorList': [], 'lossList': [0.0, -1.4457806116342544, 0.0, 287.8534496307373, 0.0, 0.0, 0.0], 'rewardMean': 0.6622930108999244, 'totalEpisodes': 126, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.608396032516554
'totalSteps': 8960, 'rewardStep': 0.7848544978072386, 'errorList': [], 'lossList': [0.0, -1.440095726251602, 0.0, 154.21083118438722, 0.0, 0.0, 0.0], 'rewardMean': 0.6776131967633386, 'totalEpisodes': 180, 'stepsPerEpisode': 37, 'rewardPerEpisode': 32.164853422925
'totalSteps': 10240, 'rewardStep': 0.8786817243857614, 'errorList': [], 'lossList': [0.0, -1.437867022752762, 0.0, 65.52374237060548, 0.0, 0.0, 0.0], 'rewardMean': 0.6999541442769411, 'totalEpisodes': 214, 'stepsPerEpisode': 94, 'rewardPerEpisode': 75.10717932117072
'totalSteps': 11520, 'rewardStep': 0.7412829679934047, 'errorList': [], 'lossList': [0.0, -1.436027324795723, 0.0, 44.34106892585754, 0.0, 0.0, 0.0], 'rewardMean': 0.7040870266485875, 'totalEpisodes': 236, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.12560677305431
'totalSteps': 12800, 'rewardStep': 0.643251294620756, 'errorList': [], 'lossList': [0.0, -1.4137369149923324, 0.0, 42.14444287776947, 0.0, 0.0, 0.0], 'rewardMean': 0.7090943741526472, 'totalEpisodes': 256, 'stepsPerEpisode': 120, 'rewardPerEpisode': 91.33911614097488
'totalSteps': 14080, 'rewardStep': 0.8881261943372177, 'errorList': [], 'lossList': [0.0, -1.3842348182201385, 0.0, 25.728915452957153, 0.0, 0.0, 0.0], 'rewardMean': 0.7128353033446497, 'totalEpisodes': 266, 'stepsPerEpisode': 100, 'rewardPerEpisode': 81.73719844679324
'totalSteps': 15360, 'rewardStep': 0.7962505785165733, 'errorList': [], 'lossList': [0.0, -1.3753535205125809, 0.0, 12.30014729142189, 0.0, 0.0, 0.0], 'rewardMean': 0.7015633542484205, 'totalEpisodes': 271, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.166228435945239
'totalSteps': 16640, 'rewardStep': 0.9499422263647654, 'errorList': [22.988902908033552, 0.7329222422752106, 1.1509937261719247, 21.217737367019076, 3.70027667863699, 5.235903834415233, 35.295064156852455, 3.562442165292946, 3.6052204071841296, 0.2917858970351257, 3.2573819213791713, 0.9580874185656889, 1.1926964916167084, 2.6826463085782346, 10.540286056162817, 0.10994461079153184, 2.696676962024456, 9.185072896391851, 2.723936613765514, 2.13689283173126, 0.10091647703859678, 7.581560430188509, 3.318654974052305, 4.650380948525728, 4.526892676062066, 13.437618800354324, 12.37196141879407, 18.62060515092901, 11.768556735073874, 15.49571004597521, 19.589553609115967, 3.937642119675665, 2.801242460686381, 1.133714547559085, 9.62615917044942, 9.792459686685934, 2.7358094356359404, 6.743498575289505, 33.18145777971755, 6.690575879347158, 4.524770764040184, 4.394294887588245, 7.843427844561548, 45.662090702701185, 2.08942533385853, 5.601799447468243, 1.188043193452363, 1.7803444555723065, 6.787398799362968, 8.244894700535466], 'lossList': [0.0, -1.3724762052297592, 0.0, 18.33574227809906, 0.0, 0.0, 0.0], 'rewardMean': 0.7337527197951007, 'totalEpisodes': 277, 'stepsPerEpisode': 71, 'rewardPerEpisode': 64.78343363604903, 'successfulTests': 2
'totalSteps': 17920, 'rewardStep': 0.7752490449646506, 'errorList': [], 'lossList': [0.0, -1.366426876783371, 0.0, 9.89572086572647, 0.0, 0.0, 0.0], 'rewardMean': 0.7660834379755608, 'totalEpisodes': 280, 'stepsPerEpisode': 146, 'rewardPerEpisode': 117.16304649837738
'totalSteps': 19200, 'rewardStep': 0.8248524130474874, 'errorList': [], 'lossList': [0.0, -1.3454216986894607, 0.0, 7.195806723833084, 0.0, 0.0, 0.0], 'rewardMean': 0.8033744929643044, 'totalEpisodes': 283, 'stepsPerEpisode': 72, 'rewardPerEpisode': 49.95979066770802
'totalSteps': 20480, 'rewardStep': 0.3322147775568404, 'errorList': [], 'lossList': [0.0, -1.3357012128829957, 0.0, 5.482439293861389, 0.0, 0.0, 0.0], 'rewardMean': 0.7614705719594695, 'totalEpisodes': 287, 'stepsPerEpisode': 199, 'rewardPerEpisode': 149.39866692188966
'totalSteps': 21760, 'rewardStep': 0.7672466568569709, 'errorList': [], 'lossList': [0.0, -1.329228305220604, 0.0, 5.400706918239593, 0.0, 0.0, 0.0], 'rewardMean': 0.7597097878644428, 'totalEpisodes': 292, 'stepsPerEpisode': 86, 'rewardPerEpisode': 69.36702290407048
'totalSteps': 23040, 'rewardStep': 0.9057995186296295, 'errorList': [], 'lossList': [0.0, -1.3216720867156981, 0.0, 4.601731489896775, 0.0, 0.0, 0.0], 'rewardMean': 0.7624215672888296, 'totalEpisodes': 294, 'stepsPerEpisode': 73, 'rewardPerEpisode': 61.4048320826875
'totalSteps': 24320, 'rewardStep': 0.7182235923917322, 'errorList': [], 'lossList': [0.0, -1.3074375784397125, 0.0, 3.3682271260023118, 0.0, 0.0, 0.0], 'rewardMean': 0.7601156297286623, 'totalEpisodes': 294, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.0468651786853
'totalSteps': 25600, 'rewardStep': 0.8812045581057035, 'errorList': [], 'lossList': [0.0, -1.2386273729801178, 0.0, 1.3087196691334249, 0.0, 0.0, 0.0], 'rewardMean': 0.7839109560771571, 'totalEpisodes': 294, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.9120982100794
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=16640, timeSpent=71.85
