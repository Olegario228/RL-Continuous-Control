#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 22
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8663774666288572, 'errorList': [], 'lossList': [0.0, -1.4424580883979798, 0.0, 30.234408762454986, 0.0, 0.0, 0.0], 'rewardMean': 0.6847391572664345, 'totalEpisodes': 10, 'stepsPerEpisode': 905, 'rewardPerEpisode': 676.0619995944902
'totalSteps': 3840, 'rewardStep': 0.8316958710698239, 'errorList': [], 'lossList': [0.0, -1.4593782252073288, 0.0, 44.221633479595184, 0.0, 0.0, 0.0], 'rewardMean': 0.7337247285342311, 'totalEpisodes': 12, 'stepsPerEpisode': 668, 'rewardPerEpisode': 528.4892580260791
'totalSteps': 5120, 'rewardStep': 0.8901832977047248, 'errorList': [], 'lossList': [0.0, -1.4601039570569991, 0.0, 24.83459147453308, 0.0, 0.0, 0.0], 'rewardMean': 0.7728393708268545, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 986.3917416425978
'totalSteps': 6400, 'rewardStep': 0.7992418004916011, 'errorList': [], 'lossList': [0.0, -1.4447348356246947, 0.0, 293.4643972015381, 0.0, 0.0, 0.0], 'rewardMean': 0.7781198567598038, 'totalEpisodes': 76, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.5676623590234418
'totalSteps': 7680, 'rewardStep': 0.8939679388966961, 'errorList': [], 'lossList': [0.0, -1.4412610417604446, 0.0, 159.35845306396484, 0.0, 0.0, 0.0], 'rewardMean': 0.7974278704492859, 'totalEpisodes': 141, 'stepsPerEpisode': 24, 'rewardPerEpisode': 19.29763182230101
'totalSteps': 8960, 'rewardStep': 0.6343689983709628, 'errorList': [], 'lossList': [0.0, -1.437120289206505, 0.0, 76.3960251045227, 0.0, 0.0, 0.0], 'rewardMean': 0.7741337458666683, 'totalEpisodes': 199, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.068207286258108
'totalSteps': 10240, 'rewardStep': 0.5784511831813608, 'errorList': [], 'lossList': [0.0, -1.4423747164011003, 0.0, 52.15925779342651, 0.0, 0.0, 0.0], 'rewardMean': 0.7496734255310048, 'totalEpisodes': 236, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.996899334801142
'totalSteps': 11520, 'rewardStep': 0.7142210833232383, 'errorList': [], 'lossList': [0.0, -1.4396669042110444, 0.0, 50.431238651275635, 0.0, 0.0, 0.0], 'rewardMean': 0.7457342763968086, 'totalEpisodes': 260, 'stepsPerEpisode': 64, 'rewardPerEpisode': 52.37702410214969
'totalSteps': 12800, 'rewardStep': 0.8613491370992884, 'errorList': [], 'lossList': [0.0, -1.422890880703926, 0.0, 39.3941703414917, 0.0, 0.0, 0.0], 'rewardMean': 0.7572957624670564, 'totalEpisodes': 276, 'stepsPerEpisode': 61, 'rewardPerEpisode': 54.66385381431978
'totalSteps': 14080, 'rewardStep': 0.4449031142494877, 'errorList': [], 'lossList': [0.0, -1.4181407833099364, 0.0, 28.637079186439514, 0.0, 0.0, 0.0], 'rewardMean': 0.7514759891016041, 'totalEpisodes': 287, 'stepsPerEpisode': 119, 'rewardPerEpisode': 87.896666879336
'totalSteps': 15360, 'rewardStep': 0.8786813306402654, 'errorList': [], 'lossList': [0.0, -1.4173325884342194, 0.0, 45.04074899673462, 0.0, 0.0, 0.0], 'rewardMean': 0.752706375502745, 'totalEpisodes': 299, 'stepsPerEpisode': 39, 'rewardPerEpisode': 34.18007939134767
'totalSteps': 16640, 'rewardStep': 0.9669744878125466, 'errorList': [99.71014382325464, 31.1026968850563, 0.942189890501228, 69.96032704411449, 20.783645282459347, 69.72189657815085, 73.36568515361878, 130.02046719198793, 43.677124282579406, 105.85301648980922, 72.73419620167829, 30.4408613532279, 79.93999997587021, 24.851142922967853, 4.729621138571673, 4.5273000219122, 29.27513685305082, 70.64439499555436, 3.461779850320524, 91.12348255242141, 49.83613859675259, 8.051111878639775, 48.021510303628, 35.996972109966954, 57.84149708816426, 93.31893233050724, 146.30589706538765, 0.3091926120451346, 97.85836554544623, 104.40188162267619, 34.318660572852785, 65.85539579129015, 39.741195447558475, 47.041967453636126, 7.115013569024309, 94.13344683678247, 29.45180290594424, 119.37443683940522, 32.08787661792515, 82.68161319980477, 6.216932035975612, 67.03389368494727, 95.14706569246117, 66.37527248927503, 64.52416127722368, 12.821212386430465, 35.21722220857309, 65.90450486352424, 7.231750401062322, 48.39124584575331], 'lossList': [0.0, -1.4327747803926467, 0.0, 24.756035733222962, 0.0, 0.0, 0.0], 'rewardMean': 0.7662342371770172, 'totalEpisodes': 306, 'stepsPerEpisode': 73, 'rewardPerEpisode': 64.27841707975061, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.822201139611783, 'errorList': [], 'lossList': [0.0, -1.449283810853958, 0.0, 29.674633157253265, 0.0, 0.0, 0.0], 'rewardMean': 0.759436021367723, 'totalEpisodes': 310, 'stepsPerEpisode': 45, 'rewardPerEpisode': 39.15107092208878
'totalSteps': 19200, 'rewardStep': 0.7098481698598316, 'errorList': [], 'lossList': [0.0, -1.4279808700084686, 0.0, 28.66605047225952, 0.0, 0.0, 0.0], 'rewardMean': 0.750496658304546, 'totalEpisodes': 314, 'stepsPerEpisode': 492, 'rewardPerEpisode': 391.7167537802211
'totalSteps': 20480, 'rewardStep': 0.906574258244238, 'errorList': [], 'lossList': [0.0, -1.4011098051071167, 0.0, 17.87802326798439, 0.0, 0.0, 0.0], 'rewardMean': 0.7517572902393003, 'totalEpisodes': 318, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.260161348380084
'totalSteps': 21760, 'rewardStep': 0.4945487058928195, 'errorList': [], 'lossList': [0.0, -1.3884321665763855, 0.0, 6.1885285553336145, 0.0, 0.0, 0.0], 'rewardMean': 0.7377752609914859, 'totalEpisodes': 319, 'stepsPerEpisode': 791, 'rewardPerEpisode': 640.5079402181328
'totalSteps': 23040, 'rewardStep': 0.6386425720382354, 'errorList': [], 'lossList': [0.0, -1.375707231760025, 0.0, 10.712311060130595, 0.0, 0.0, 0.0], 'rewardMean': 0.7437943998771734, 'totalEpisodes': 320, 'stepsPerEpisode': 1237, 'rewardPerEpisode': 1016.2682601716244
'totalSteps': 24320, 'rewardStep': 0.9484147430580099, 'errorList': [0.30504450649537773, 0.33235041247266894, 0.37091826750901097, 0.3483467562374089, 0.35831912815758143, 0.3309798835576375, 0.3235639762860366, 0.2954066447336223, 0.379365618309593, 0.3186698867371554, 0.2834683078785199, 0.29468957713871496, 0.3051397131157957, 0.2927040972008851, 0.2821935928190391, 0.3005424436356534, 0.2909746315216748, 0.29066706753879373, 0.3034715240568133, 0.32016824029275726, 0.29731275525713824, 0.3121185368827513, 0.29767807278520525, 0.2892911208730755, 0.3627486835319208, 0.29038803488491016, 0.3134023658732282, 0.3094882210331265, 0.30822809988844924, 0.31524689705914016, 0.36829463694954495, 0.29361957154141516, 0.31003846883561714, 0.3124487603912956, 0.2941067711008137, 0.286288751280345, 0.3014176974107152, 0.3142792135245691, 0.3161915051643679, 0.3116628134331083, 0.33163183792369155, 0.31820134508879505, 0.2941445974434056, 0.3615114933283628, 0.3280508571576664, 0.2899361450826358, 0.30828621461635575, 0.2936224007241252, 0.3252835499445317, 0.3039197918232782], 'lossList': [0.0, -1.353535698056221, 0.0, 9.974942839443683, 0.0, 0.0, 0.0], 'rewardMean': 0.7672137658506506, 'totalEpisodes': 321, 'stepsPerEpisode': 1099, 'rewardPerEpisode': 928.936226334998, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.8412428195780604, 'errorList': [], 'lossList': [0.0, -1.3293724852800368, 0.0, 2.7043599659204482, 0.0, 0.0, 0.0], 'rewardMean': 0.7652031340985277, 'totalEpisodes': 321, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.8393578803089
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=94.48
