#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 24
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.820380209392914, 'errorList': [], 'lossList': [0.0, -1.4210933870077134, 0.0, 28.428321870565416, 0.0, 0.0, 0.0], 'rewardMean': 0.7848355368460975, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.8017954780258
'totalSteps': 3840, 'rewardStep': 0.7638039259033497, 'errorList': [], 'lossList': [0.0, -1.4348244148492812, 0.0, 34.8623926949501, 0.0, 0.0, 0.0], 'rewardMean': 0.7778249998651816, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 124.93312524819889
'totalSteps': 5120, 'rewardStep': 0.6477053025201914, 'errorList': [], 'lossList': [0.0, -1.4303606480360032, 0.0, 22.205181601047517, 0.0, 0.0, 0.0], 'rewardMean': 0.745295075528934, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 963.3777843955074
'totalSteps': 6400, 'rewardStep': 0.6253706607114999, 'errorList': [], 'lossList': [0.0, -1.4252506220340728, 0.0, 333.4794296264648, 0.0, 0.0, 0.0], 'rewardMean': 0.7213101925654473, 'totalEpisodes': 91, 'stepsPerEpisode': 31, 'rewardPerEpisode': 23.773372250931814
'totalSteps': 7680, 'rewardStep': 0.6272777719484671, 'errorList': [], 'lossList': [0.0, -1.4253858584165573, 0.0, 132.6780226135254, 0.0, 0.0, 0.0], 'rewardMean': 0.7056381224626173, 'totalEpisodes': 160, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.745012799547915
'totalSteps': 8960, 'rewardStep': 0.9371924082389353, 'errorList': [231.96770488898886, 221.04615533331884, 221.23876419942277, 219.62990355379782, 235.5279414457354, 195.95499917465926, 214.8186697614454, 214.81260768011404, 223.54874503689445, 215.78303960509277, 158.3269748153396, 232.2051721947154, 227.75524631055828, 169.3346387558287, 187.94016669775954, 210.66423325747232, 230.85178028159726, 210.40086004977186, 132.70341976511963, 210.13454336354386, 199.38874454232138, 229.34637313672422, 227.16290291759023, 228.21425629816284, 217.10362664755803, 214.33589174386415, 217.81270990029608, 233.03437405365833, 222.53837980382744, 226.85690013754044, 205.1972258076593, 226.74755244119163, 217.4090662563387, 222.10932021855774, 233.89577141927776, 204.3076375650374, 212.6673274394202, 214.00612146493395, 207.32931331747002, 212.21630110304181, 209.93354739656368, 217.61628942248814, 200.67731019172956, 180.9196298210542, 203.6326458982929, 223.16413304451518, 231.36905186364618, 216.67998346686153, 216.33193993163945, 183.56968569760718], 'lossList': [0.0, -1.4188930702209472, 0.0, 56.4936555480957, 0.0, 0.0, 0.0], 'rewardMean': 0.7387173061449485, 'totalEpisodes': 216, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.789447797468233, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.6879656721531353, 'errorList': [], 'lossList': [0.0, -1.3951537328958512, 0.0, 43.166800003051755, 0.0, 0.0, 0.0], 'rewardMean': 0.7323733518959717, 'totalEpisodes': 255, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.117015083336869
'totalSteps': 11520, 'rewardStep': 0.8350122970630574, 'errorList': [], 'lossList': [0.0, -1.3731758922338486, 0.0, 50.1683106136322, 0.0, 0.0, 0.0], 'rewardMean': 0.7437776791367591, 'totalEpisodes': 275, 'stepsPerEpisode': 79, 'rewardPerEpisode': 66.484969962889
'totalSteps': 12800, 'rewardStep': 0.7895913540440868, 'errorList': [], 'lossList': [0.0, -1.3618631452322005, 0.0, 35.17973418235779, 0.0, 0.0, 0.0], 'rewardMean': 0.7483590466274919, 'totalEpisodes': 283, 'stepsPerEpisode': 65, 'rewardPerEpisode': 53.85579460859206
'totalSteps': 14080, 'rewardStep': 0.7757993083392873, 'errorList': [], 'lossList': [0.0, -1.3424612027406693, 0.0, 22.90460908651352, 0.0, 0.0, 0.0], 'rewardMean': 0.7510098910314924, 'totalEpisodes': 286, 'stepsPerEpisode': 255, 'rewardPerEpisode': 199.6753861698586
'totalSteps': 15360, 'rewardStep': 0.8754142401822227, 'errorList': [], 'lossList': [0.0, -1.3266159456968307, 0.0, 53.89939227581024, 0.0, 0.0, 0.0], 'rewardMean': 0.7565132941104233, 'totalEpisodes': 293, 'stepsPerEpisode': 123, 'rewardPerEpisode': 102.91488997415414
'totalSteps': 16640, 'rewardStep': 0.2124492691934608, 'errorList': [], 'lossList': [0.0, -1.3025666517019272, 0.0, 13.32441306591034, 0.0, 0.0, 0.0], 'rewardMean': 0.7013778284394344, 'totalEpisodes': 296, 'stepsPerEpisode': 455, 'rewardPerEpisode': 299.8347535475681
'totalSteps': 17920, 'rewardStep': 0.761874888015678, 'errorList': [], 'lossList': [0.0, -1.2743455421924592, 0.0, 12.51721858382225, 0.0, 0.0, 0.0], 'rewardMean': 0.712794786988983, 'totalEpisodes': 302, 'stepsPerEpisode': 68, 'rewardPerEpisode': 57.79997012012341
'totalSteps': 19200, 'rewardStep': 0.8667203527105111, 'errorList': [], 'lossList': [0.0, -1.2527898734807967, 0.0, 16.99428449869156, 0.0, 0.0, 0.0], 'rewardMean': 0.7369297561888841, 'totalEpisodes': 308, 'stepsPerEpisode': 321, 'rewardPerEpisode': 275.87487610150725
'totalSteps': 20480, 'rewardStep': 0.9066012748192556, 'errorList': [], 'lossList': [0.0, -1.2404103988409043, 0.0, 5.4450971204042435, 0.0, 0.0, 0.0], 'rewardMean': 0.764862106475963, 'totalEpisodes': 315, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.060273420837277
'totalSteps': 21760, 'rewardStep': 0.5468578600024434, 'errorList': [], 'lossList': [0.0, -1.2275875157117844, 0.0, 4.670375205874443, 0.0, 0.0, 0.0], 'rewardMean': 0.7258286516523139, 'totalEpisodes': 319, 'stepsPerEpisode': 174, 'rewardPerEpisode': 125.23292503780475
'totalSteps': 23040, 'rewardStep': 0.9746602882630708, 'errorList': [4.191541830729864, 14.132694924219903, 0.37291261046294133, 41.75053113627911, 6.928783212946656, 8.896010861802928, 38.19780093281119, 62.68310853080511, 70.04445807435086, 55.538770649163176, 4.488389288098981, 23.08103476409172, 6.961387551202037, 10.058726126252443, 8.13095490331575, 37.17050060238414, 66.81385978548767, 27.014688135445088, 61.79435218072369, 5.5438224696483, 76.12191881205938, 6.282489097449263, 3.5854883531464963, 1.1696556322759595, 30.56678514728439, 0.10187020643996546, 49.23270790747065, 1.2347866216938068, 28.980404698480555, 38.68606465198357, 66.90295673423496, 92.26203676471347, 29.762415316494327, 32.05292135628226, 40.87884421473494, 16.042047861355666, 1.3520383653348156, 86.48223607801152, 78.93394627939989, 33.04804159264744, 2.8701186061056965, 42.96193671360554, 25.00085132444761, 62.875887175777336, 68.21351438904185, 19.49684005765131, 9.706164885443098, 58.117227036981184, 41.00196940574966, 48.50693085297949], 'lossList': [0.0, -1.193737067580223, 0.0, 4.308692511320114, 0.0, 0.0, 0.0], 'rewardMean': 0.7544981132633074, 'totalEpisodes': 326, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.803641288845702, 'successfulTests': 1
'totalSteps': 24320, 'rewardStep': 0.8276227933540905, 'errorList': [], 'lossList': [0.0, -1.1935339814424515, 0.0, 4.557632665038109, 0.0, 0.0, 0.0], 'rewardMean': 0.7537591628924107, 'totalEpisodes': 330, 'stepsPerEpisode': 231, 'rewardPerEpisode': 208.64276685361105
'totalSteps': 25600, 'rewardStep': 0.9030856985690602, 'errorList': [], 'lossList': [0.0, -1.2112239962816238, 0.0, 3.8084653663635253, 0.0, 0.0, 0.0], 'rewardMean': 0.7651085973449081, 'totalEpisodes': 334, 'stepsPerEpisode': 84, 'rewardPerEpisode': 74.33906628848017
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=23040, timeSpent=94.09
