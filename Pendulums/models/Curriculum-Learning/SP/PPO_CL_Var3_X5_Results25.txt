#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 25
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4442227409755177, 'errorList': [], 'lossList': [0.0, -1.4175787383317948, 0.0, 46.70278791427612, 0.0, 0.0, 0.0], 'rewardMean': 0.4442227409755177, 'totalEpisodes': 32, 'stepsPerEpisode': 45, 'rewardPerEpisode': 33.7273362039585
'totalSteps': 2560, 'rewardStep': 0.7612795637646368, 'errorList': [], 'lossList': [0.0, -1.4163111621141433, 0.0, 34.0380592918396, 0.0, 0.0, 0.0], 'rewardMean': 0.6027511523700773, 'totalEpisodes': 54, 'stepsPerEpisode': 22, 'rewardPerEpisode': 14.042664877534763
'totalSteps': 3840, 'rewardStep': 0.07860878830783469, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.3406799703389559, 'totalEpisodes': 71, 'stepsPerEpisode': 96, 'rewardPerEpisode': 64.87601713780035
'totalSteps': 5120, 'rewardStep': 0.9777756436502333, 'errorList': [], 'lossList': [0.0, -1.4186732929944992, 0.0, 37.82288429260254, 0.0, 0.0, 0.0], 'rewardMean': 0.46809910500121144, 'totalEpisodes': 88, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.288277902479766
'totalSteps': 6400, 'rewardStep': 0.9236239163123714, 'errorList': [], 'lossList': [0.0, -1.4215597891807557, 0.0, 49.22370953559876, 0.0, 0.0, 0.0], 'rewardMean': 0.5440199068864048, 'totalEpisodes': 102, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.289023144072175
'totalSteps': 7680, 'rewardStep': 0.8229576554004435, 'errorList': [], 'lossList': [0.0, -1.4146458005905151, 0.0, 103.29794479370118, 0.0, 0.0, 0.0], 'rewardMean': 0.5838681566741245, 'totalEpisodes': 141, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.82072759862626
'totalSteps': 8960, 'rewardStep': 0.8178067979536392, 'errorList': [], 'lossList': [0.0, -1.3935875302553178, 0.0, 90.62614053726196, 0.0, 0.0, 0.0], 'rewardMean': 0.6131104868340639, 'totalEpisodes': 177, 'stepsPerEpisode': 49, 'rewardPerEpisode': 41.06759077012729
'totalSteps': 10240, 'rewardStep': 0.6286164589799973, 'errorList': [], 'lossList': [0.0, -1.3825543576478958, 0.0, 50.186890516281125, 0.0, 0.0, 0.0], 'rewardMean': 0.6148333726280565, 'totalEpisodes': 196, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.4275795159646245
'totalSteps': 11520, 'rewardStep': 0.5375225128638753, 'errorList': [], 'lossList': [0.0, -1.363174489736557, 0.0, 31.472122943401338, 0.0, 0.0, 0.0], 'rewardMean': 0.6071022866516383, 'totalEpisodes': 203, 'stepsPerEpisode': 66, 'rewardPerEpisode': 50.56071795407171
'totalSteps': 12800, 'rewardStep': 0.3604151305784261, 'errorList': [], 'lossList': [0.0, -1.3264945805072785, 0.0, 13.052246336936951, 0.0, 0.0, 0.0], 'rewardMean': 0.5987215256119292, 'totalEpisodes': 205, 'stepsPerEpisode': 820, 'rewardPerEpisode': 550.8960676662391
'totalSteps': 14080, 'rewardStep': 0.645710112868243, 'errorList': [], 'lossList': [0.0, -1.3142485344409942, 0.0, 13.596745505332947, 0.0, 0.0, 0.0], 'rewardMean': 0.5871645805222898, 'totalEpisodes': 207, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 874.6472843562386
'totalSteps': 15360, 'rewardStep': 0.5106828950863963, 'errorList': [], 'lossList': [0.0, -1.3098920595645904, 0.0, 5.943304177522659, 0.0, 0.0, 0.0], 'rewardMean': 0.630371991200146, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 899.9516593017574
'totalSteps': 16640, 'rewardStep': 0.8926737237204719, 'errorList': [], 'lossList': [0.0, -1.3045314759016038, 0.0, 9.291085878014565, 0.0, 0.0, 0.0], 'rewardMean': 0.7117784847414097, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.8830140196565
'totalSteps': 17920, 'rewardStep': 0.8090237058821745, 'errorList': [], 'lossList': [0.0, -1.2844389832019807, 0.0, 5.921264457777142, 0.0, 0.0, 0.0], 'rewardMean': 0.6949032909646038, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.928951636468
'totalSteps': 19200, 'rewardStep': 0.9271531468530146, 'errorList': [], 'lossList': [0.0, -1.2484463191032409, 0.0, 4.623504405021667, 0.0, 0.0, 0.0], 'rewardMean': 0.6952562140186682, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1127.4706448804543
'totalSteps': 20480, 'rewardStep': 0.9261785089689917, 'errorList': [], 'lossList': [0.0, -1.236549241542816, 0.0, 3.857520329616964, 0.0, 0.0, 0.0], 'rewardMean': 0.705578299375523, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.8406241595842
'totalSteps': 21760, 'rewardStep': 0.8960258595449571, 'errorList': [], 'lossList': [0.0, -1.2425252509117126, 0.0, 2.190206498093903, 0.0, 0.0, 0.0], 'rewardMean': 0.7134002055346547, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1167.6554684712635
'totalSteps': 23040, 'rewardStep': 0.9107402962479498, 'errorList': [], 'lossList': [0.0, -1.2278618091344833, 0.0, 1.1490479018352926, 0.0, 0.0, 0.0], 'rewardMean': 0.74161258926145, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1163.010104429463
'totalSteps': 24320, 'rewardStep': 0.8187461330522726, 'errorList': [], 'lossList': [0.0, -1.1902458375692369, 0.0, 0.6883198607806116, 0.0, 0.0, 0.0], 'rewardMean': 0.7697349512802898, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1166.6483261534443
'totalSteps': 25600, 'rewardStep': 0.9639844446168083, 'errorList': [0.061259080262534266, 0.1609298541671561, 0.16031899166366145, 0.08854525342481094, 0.1600281999318082, 0.09385118212656629, 0.181772423137269, 0.12200440074112685, 0.1165441909508287, 0.1627260911913492, 0.16368042744483519, 0.126406575103515, 0.06501959588766298, 0.10015460896608461, 0.10326628456519753, 0.16436786787521085, 0.15454684507012756, 0.17896677373735792, 0.0935411521293049, 0.1642710034844283, 0.1942419199679987, 0.2500968737663967, 0.13264964473121288, 0.1571348189962922, 0.1940633203508422, 0.12631192322126117, 0.14247366819645307, 0.0922728149077695, 0.1080223423461482, 0.1795450530070841, 0.23771733048006713, 0.06834569450330116, 0.16939622398958, 0.15449641837479092, 0.15872424870190158, 0.09418169578895157, 0.1259183657128899, 0.09426815044300946, 0.12702171318702515, 0.11791920924001796, 0.1179710364123875, 0.10236860733921704, 0.14290977876309602, 0.19783450358465735, 0.08070905482982321, 0.10610798569451446, 0.15270216139251583, 0.14157010338350462, 0.0960019639468771, 0.07404630063264159], 'lossList': [0.0, -1.1506028294563293, 0.0, 0.8562181988544763, 0.0, 0.0, 0.0], 'rewardMean': 0.8300918826841281, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1209.5589151734819, 'successfulTests': 48
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=25600, timeSpent=72.89
