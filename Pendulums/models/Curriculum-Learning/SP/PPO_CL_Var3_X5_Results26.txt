#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 26
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980545212200532, 'errorList': [], 'lossList': [0.0, -1.4174901527166366, 0.0, 30.407760000228883, 0.0, 0.0, 0.0], 'rewardMean': 0.7065523270307303, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.342671484442306
'totalSteps': 3840, 'rewardStep': 0.7308169600956648, 'errorList': [], 'lossList': [0.0, -1.4046733671426772, 0.0, 37.27468172550201, 0.0, 0.0, 0.0], 'rewardMean': 0.7146405380523752, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.224160093491516
'totalSteps': 5120, 'rewardStep': 0.8859508331255, 'errorList': [], 'lossList': [0.0, -1.399260874390602, 0.0, 25.345460922718047, 0.0, 0.0, 0.0], 'rewardMean': 0.7574681118206563, 'totalEpisodes': 75, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.17309189859134
'totalSteps': 6400, 'rewardStep': 0.897997294640649, 'errorList': [], 'lossList': [0.0, -1.386724608540535, 0.0, 38.72907695770264, 0.0, 0.0, 0.0], 'rewardMean': 0.7855739483846549, 'totalEpisodes': 82, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.897997294640649
'totalSteps': 7680, 'rewardStep': 0.44567101122616093, 'errorList': [], 'lossList': [0.0, -1.3786423963308334, 0.0, 88.55072910308837, 0.0, 0.0, 0.0], 'rewardMean': 0.7289234588582393, 'totalEpisodes': 104, 'stepsPerEpisode': 83, 'rewardPerEpisode': 63.22740833932583
'totalSteps': 8960, 'rewardStep': 0.5998218551514631, 'errorList': [], 'lossList': [0.0, -1.3773252522945405, 0.0, 68.75944664001464, 0.0, 0.0, 0.0], 'rewardMean': 0.7104803726144141, 'totalEpisodes': 123, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.817784070220976
'totalSteps': 10240, 'rewardStep': 0.8931040759464118, 'errorList': [], 'lossList': [0.0, -1.373008947968483, 0.0, 20.49034580230713, 0.0, 0.0, 0.0], 'rewardMean': 0.7333083355309138, 'totalEpisodes': 131, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.211839637614721
'totalSteps': 11520, 'rewardStep': 0.792386678872918, 'errorList': [], 'lossList': [0.0, -1.370000314116478, 0.0, 27.977664382457732, 0.0, 0.0, 0.0], 'rewardMean': 0.7398725959022476, 'totalEpisodes': 135, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.467630055437972
'totalSteps': 12800, 'rewardStep': 0.8281918041024314, 'errorList': [], 'lossList': [0.0, -1.3597572928667068, 0.0, 11.594008561372757, 0.0, 0.0, 0.0], 'rewardMean': 0.748704516722266, 'totalEpisodes': 138, 'stepsPerEpisode': 172, 'rewardPerEpisode': 142.2698075781014
'totalSteps': 14080, 'rewardStep': 0.5831658617803002, 'errorList': [], 'lossList': [0.0, -1.3457625722885131, 0.0, 6.608636784553528, 0.0, 0.0, 0.0], 'rewardMean': 0.7255160896161552, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 887.7416211011357
'totalSteps': 15360, 'rewardStep': 0.8139469974153203, 'errorList': [], 'lossList': [0.0, -1.3422770953178407, 0.0, 5.394061412215233, 0.0, 0.0, 0.0], 'rewardMean': 0.747105337235682, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 845.7946216560152
'totalSteps': 16640, 'rewardStep': 0.8164921400859781, 'errorList': [], 'lossList': [0.0, -1.3079903101921082, 0.0, 4.05423542201519, 0.0, 0.0, 0.0], 'rewardMean': 0.7556728552347133, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1059.3220537397017
'totalSteps': 17920, 'rewardStep': 0.8606487102653533, 'errorList': [], 'lossList': [0.0, -1.2623693454265594, 0.0, 4.123278152421117, 0.0, 0.0, 0.0], 'rewardMean': 0.7531426429486986, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.346309219935
'totalSteps': 19200, 'rewardStep': 0.857525065148114, 'errorList': [], 'lossList': [0.0, -1.2199030143022538, 0.0, 3.120676855891943, 0.0, 0.0, 0.0], 'rewardMean': 0.7490954199994453, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.9876760035029
'totalSteps': 20480, 'rewardStep': 0.8370234331498718, 'errorList': [], 'lossList': [0.0, -1.1844120407104493, 0.0, 1.9201701090857386, 0.0, 0.0, 0.0], 'rewardMean': 0.7882306621918163, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1168.3345218555817
'totalSteps': 21760, 'rewardStep': 0.865248473030224, 'errorList': [], 'lossList': [0.0, -1.1272286087274552, 0.0, 1.485904102921486, 0.0, 0.0, 0.0], 'rewardMean': 0.8147733239796923, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1187.1198455761046
'totalSteps': 23040, 'rewardStep': 0.9937191269027593, 'errorList': [0.13772459779905888, 0.18492163834882855, 0.16620754122493153, 0.1352228909243263, 0.13229203466142767, 0.17920731195188974, 0.12818115545228798, 0.15022969512830456, 0.14747939614782077, 0.1283883883861652, 0.1591600485401618, 0.14347417510321106, 0.16826041790596413, 0.1274051787523811, 0.12279371953620553, 0.13285315634673653, 0.1708196142524434, 0.16407247749070447, 0.2106293202342412, 0.14084608781548336, 0.11365182306498235, 0.1512506093529482, 0.13193850547930036, 0.19361649785434104, 0.1308124477128025, 0.17445677140384602, 0.14666123700590444, 0.14346883260399018, 0.1318412115086055, 0.13178658827907416, 0.16472144611857922, 0.17106506993177786, 0.22595294528323173, 0.1261692731523691, 0.18913358192235466, 0.10838231755128117, 0.15074813857628197, 0.12994658660454017, 0.15964193141901153, 0.17060397369136093, 0.15483318290789713, 0.14932840659385693, 0.1543940808554567, 0.13356281716470036, 0.15816657288117278, 0.17474624209712572, 0.17170979434057165, 0.13330462526144796, 0.16619348679643633, 0.16287051309740075], 'lossList': [0.0, -1.0854281377792359, 0.0, 1.0344950721412898, 0.0, 0.0, 0.0], 'rewardMean': 0.824834829075327, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1209.0177482853676, 'successfulTests': 48
'totalSteps': 24320, 'rewardStep': 0.9120736057015496, 'errorList': [], 'lossList': [0.0, -1.0471527147293092, 0.0, 0.4192273817397654, 0.0, 0.0, 0.0], 'rewardMean': 0.8368035217581902, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.6360149799466
'totalSteps': 25600, 'rewardStep': 0.9864833966294307, 'errorList': [0.07070218261930683, 0.07141573078727231, 0.09259815603860552, 0.16215170728812203, 0.10199584392538916, 0.2367948119485003, 0.09457756676314304, 0.09895797471352347, 0.10886665972880354, 0.08177877871492958, 0.06303416760306027, 0.07919526831116475, 0.24267585715681245, 0.18801584664759813, 0.0631975181710269, 0.08088016190845458, 0.1242405336918427, 0.22761730192259072, 0.3630249464617521, 0.13020646304214978, 0.10561465520176791, 0.11753435918516662, 0.06884819550349267, 0.1256428922032573, 0.13120740309550624, 0.07433375408349126, 0.1140033635801355, 0.07512826595533416, 0.21931590796419687, 0.07706660701351226, 0.08135822495658299, 0.07569824105112902, 0.0710777089427585, 0.0996747861995348, 0.07145828377137302, 0.06964051016278693, 0.16626581251443973, 0.08300626588121757, 0.08069011006661175, 0.06652309240263757, 0.12762170787212918, 0.10344477785739245, 0.17311390582875918, 0.07725063090837075, 0.07991707230728329, 0.07448309782260587, 0.1275437095212643, 0.1531038704042574, 0.11095536493743914, 0.07776718845312695], 'lossList': [0.0, -1.0088241520524024, 0.0, 0.591316592944786, 0.0, 0.0, 0.0], 'rewardMean': 0.8526326810108902, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1230.342636411363, 'successfulTests': 45
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=23040, timeSpent=95.09
