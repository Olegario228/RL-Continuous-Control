#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 27
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092626543635013, 'errorList': [], 'lossList': [0.0, -1.4231047475337981, 0.0, 34.85885494232178, 0.0, 0.0, 0.0], 'rewardMean': 0.8624499980736053, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.766494382381786
'totalSteps': 3840, 'rewardStep': 0.5487726069301929, 'errorList': [], 'lossList': [0.0, -1.4143376046419143, 0.0, 33.504342546463015, 0.0, 0.0, 0.0], 'rewardMean': 0.7578908676924678, 'totalEpisodes': 82, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.91220468096148
'totalSteps': 5120, 'rewardStep': 0.5063588732322735, 'errorList': [], 'lossList': [0.0, -1.4122148478031158, 0.0, 22.017206201553346, 0.0, 0.0, 0.0], 'rewardMean': 0.6950078690774193, 'totalEpisodes': 91, 'stepsPerEpisode': 213, 'rewardPerEpisode': 121.32605016400164
'totalSteps': 6400, 'rewardStep': 0.7664250088557757, 'errorList': [], 'lossList': [0.0, -1.4273687720298767, 0.0, 64.98922215461731, 0.0, 0.0, 0.0], 'rewardMean': 0.7092912970330906, 'totalEpisodes': 104, 'stepsPerEpisode': 24, 'rewardPerEpisode': 18.215133017590954
'totalSteps': 7680, 'rewardStep': 0.9742567140899616, 'errorList': [95.86490160877293, 316.334648043964, 290.26508779420874, 302.14069217266143, 250.98026227572944, 201.59711368246988, 260.1304103245512, 261.2743231435259, 217.05270522952344, 236.9571777759206, 181.05062961553193, 151.66315131130702, 255.59351689108075, 244.35624110023915, 121.86513524782097, 121.63873159547869, 243.67778403718265, 224.7677939181232, 261.1100403673227, 271.90039771718637, 269.074387421767, 144.16779412692316, 287.94820813366425, 39.58005888490968, 184.16955049184378, 310.3191626026615, 182.42231189259968, 191.30149261117788, 316.79556953906956, 204.92236366497775, 290.70137643250615, 264.91417914230874, 236.93038645858195, 160.56730072174602, 200.14514044436032, 245.06132921846645, 283.6398953597583, 301.44931650262856, 112.17337722034955, 213.6900663021905, 274.4131844756333, 251.140488367464, 130.40944788374028, 267.6771636630983, 294.73078986224436, 221.6566080612559, 310.68040086578543, 311.8566482019347, 183.58196471702072, 289.5894608508159], 'lossList': [0.0, -1.4147068333625794, 0.0, 131.5880902862549, 0.0, 0.0, 0.0], 'rewardMean': 0.7534521998759024, 'totalEpisodes': 140, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.458656928909944, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.6342205085987447, 'errorList': [], 'lossList': [0.0, -1.4019747686386108, 0.0, 85.50145927429199, 0.0, 0.0, 0.0], 'rewardMean': 0.7364191011220227, 'totalEpisodes': 160, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.730177090250436
'totalSteps': 10240, 'rewardStep': 0.1254665767647078, 'errorList': [], 'lossList': [0.0, -1.3940384376049042, 0.0, 45.886566190719606, 0.0, 0.0, 0.0], 'rewardMean': 0.6600500355773584, 'totalEpisodes': 169, 'stepsPerEpisode': 112, 'rewardPerEpisode': 73.65751423764112
'totalSteps': 11520, 'rewardStep': 0.8193939066145125, 'errorList': [], 'lossList': [0.0, -1.3940821474790572, 0.0, 34.412678525447845, 0.0, 0.0, 0.0], 'rewardMean': 0.6777549101370421, 'totalEpisodes': 177, 'stepsPerEpisode': 110, 'rewardPerEpisode': 92.07207248440699
'totalSteps': 12800, 'rewardStep': 0.7315303170986497, 'errorList': [], 'lossList': [0.0, -1.3920552223920821, 0.0, 36.90639308691025, 0.0, 0.0, 0.0], 'rewardMean': 0.683132450833203, 'totalEpisodes': 184, 'stepsPerEpisode': 176, 'rewardPerEpisode': 124.35842018937984
'totalSteps': 14080, 'rewardStep': 0.5773826261919263, 'errorList': [], 'lossList': [0.0, -1.392490838766098, 0.0, 16.756024131774904, 0.0, 0.0, 0.0], 'rewardMean': 0.6593069792740246, 'totalEpisodes': 188, 'stepsPerEpisode': 57, 'rewardPerEpisode': 36.3633346334598
'totalSteps': 15360, 'rewardStep': 0.5505436475360982, 'errorList': [], 'lossList': [0.0, -1.401557292342186, 0.0, 21.8781200671196, 0.0, 0.0, 0.0], 'rewardMean': 0.6234350785912842, 'totalEpisodes': 192, 'stepsPerEpisode': 506, 'rewardPerEpisode': 383.07911304018756
'totalSteps': 16640, 'rewardStep': 0.8963491789735302, 'errorList': [], 'lossList': [0.0, -1.3931798648834228, 0.0, 12.668346627950669, 0.0, 0.0, 0.0], 'rewardMean': 0.658192735795618, 'totalEpisodes': 198, 'stepsPerEpisode': 153, 'rewardPerEpisode': 138.65923114723074
'totalSteps': 17920, 'rewardStep': 0.5428923221343787, 'errorList': [], 'lossList': [0.0, -1.3792962563037872, 0.0, 6.882849719524383, 0.0, 0.0, 0.0], 'rewardMean': 0.6618460806858286, 'totalEpisodes': 202, 'stepsPerEpisode': 270, 'rewardPerEpisode': 222.84726454702724
'totalSteps': 19200, 'rewardStep': 0.8985769258566552, 'errorList': [], 'lossList': [0.0, -1.3757395178079606, 0.0, 7.008386632800102, 0.0, 0.0, 0.0], 'rewardMean': 0.6750612723859165, 'totalEpisodes': 205, 'stepsPerEpisode': 562, 'rewardPerEpisode': 505.64296337304717
'totalSteps': 20480, 'rewardStep': 0.8513775401365563, 'errorList': [], 'lossList': [0.0, -1.3836530512571334, 0.0, 3.2959141516685486, 0.0, 0.0, 0.0], 'rewardMean': 0.662773354990576, 'totalEpisodes': 207, 'stepsPerEpisode': 244, 'rewardPerEpisode': 220.78258690443258
'totalSteps': 21760, 'rewardStep': 0.6280319926992828, 'errorList': [], 'lossList': [0.0, -1.3873687690496446, 0.0, 3.573363003730774, 0.0, 0.0, 0.0], 'rewardMean': 0.6621545034006298, 'totalEpisodes': 209, 'stepsPerEpisode': 299, 'rewardPerEpisode': 250.14010723477054
'totalSteps': 23040, 'rewardStep': 0.7394509554919018, 'errorList': [], 'lossList': [0.0, -1.380319181084633, 0.0, 2.0402615839242935, 0.0, 0.0, 0.0], 'rewardMean': 0.7235529412733491, 'totalEpisodes': 209, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 789.8997014869482
'totalSteps': 24320, 'rewardStep': 0.8363603660169636, 'errorList': [], 'lossList': [0.0, -1.3740475875139238, 0.0, 4.073737741708755, 0.0, 0.0, 0.0], 'rewardMean': 0.7252495872135942, 'totalEpisodes': 211, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.373554478093002
'totalSteps': 25600, 'rewardStep': 0.6371355334884696, 'errorList': [], 'lossList': [0.0, -1.370405052304268, 0.0, 2.163398154973984, 0.0, 0.0, 0.0], 'rewardMean': 0.7158101088525763, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 963.4040722720522
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=76.81
