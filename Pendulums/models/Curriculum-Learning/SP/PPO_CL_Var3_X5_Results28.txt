#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 28
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.43569537358775035, 'errorList': [], 'lossList': [0.0, -1.4341126853227615, 0.0, 31.662300491333006, 0.0, 0.0, 0.0], 'rewardMean': 0.4703173001640676, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.31233917195806
'totalSteps': 3840, 'rewardStep': 0.963509690911094, 'errorList': [], 'lossList': [0.0, -1.4257750415802002, 0.0, 36.24250479698181, 0.0, 0.0, 0.0], 'rewardMean': 0.6347147637464098, 'totalEpisodes': 73, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.155101482161555
'totalSteps': 5120, 'rewardStep': 0.8405061678902332, 'errorList': [], 'lossList': [0.0, -1.4148189216852187, 0.0, 44.10500597000122, 0.0, 0.0, 0.0], 'rewardMean': 0.6861626147823656, 'totalEpisodes': 81, 'stepsPerEpisode': 73, 'rewardPerEpisode': 64.64649304443884
'totalSteps': 6400, 'rewardStep': 0.6461780541291144, 'errorList': [], 'lossList': [0.0, -1.3989027351140977, 0.0, 50.344911918640136, 0.0, 0.0, 0.0], 'rewardMean': 0.6781657026517154, 'totalEpisodes': 89, 'stepsPerEpisode': 165, 'rewardPerEpisode': 125.85004557133453
'totalSteps': 7680, 'rewardStep': 0.46869969277967766, 'errorList': [], 'lossList': [0.0, -1.3849765056371688, 0.0, 131.12303886413574, 0.0, 0.0, 0.0], 'rewardMean': 0.6432547010063757, 'totalEpisodes': 120, 'stepsPerEpisode': 25, 'rewardPerEpisode': 13.37634290654566
'totalSteps': 8960, 'rewardStep': 0.48048376296122586, 'errorList': [], 'lossList': [0.0, -1.3814217728376388, 0.0, 83.85071546554565, 0.0, 0.0, 0.0], 'rewardMean': 0.6200017098570686, 'totalEpisodes': 139, 'stepsPerEpisode': 70, 'rewardPerEpisode': 50.368821802296715
'totalSteps': 10240, 'rewardStep': 0.8659544118452774, 'errorList': [], 'lossList': [0.0, -1.3721437984704972, 0.0, 43.10905759334564, 0.0, 0.0, 0.0], 'rewardMean': 0.6507457976055947, 'totalEpisodes': 148, 'stepsPerEpisode': 165, 'rewardPerEpisode': 141.5651999229612
'totalSteps': 11520, 'rewardStep': 0.6691819097706477, 'errorList': [], 'lossList': [0.0, -1.3685550981760024, 0.0, 20.610310213565825, 0.0, 0.0, 0.0], 'rewardMean': 0.6527942545128228, 'totalEpisodes': 154, 'stepsPerEpisode': 61, 'rewardPerEpisode': 49.5995333493606
'totalSteps': 12800, 'rewardStep': 0.671515989594611, 'errorList': [], 'lossList': [0.0, -1.3747098600864411, 0.0, 22.17342504262924, 0.0, 0.0, 0.0], 'rewardMean': 0.6546664280210016, 'totalEpisodes': 160, 'stepsPerEpisode': 233, 'rewardPerEpisode': 196.25754202242555
'totalSteps': 14080, 'rewardStep': 0.494510839018439, 'errorList': [], 'lossList': [0.0, -1.386323041319847, 0.0, 9.523901728391648, 0.0, 0.0, 0.0], 'rewardMean': 0.653623589248807, 'totalEpisodes': 164, 'stepsPerEpisode': 276, 'rewardPerEpisode': 196.08024232939138
'totalSteps': 15360, 'rewardStep': 0.5551168956926715, 'errorList': [], 'lossList': [0.0, -1.3843462365865706, 0.0, 10.025640454292297, 0.0, 0.0, 0.0], 'rewardMean': 0.665565741459299, 'totalEpisodes': 167, 'stepsPerEpisode': 447, 'rewardPerEpisode': 343.9158890859212
'totalSteps': 16640, 'rewardStep': 0.6631933607133432, 'errorList': [], 'lossList': [0.0, -1.3800593316555023, 0.0, 4.513958957493305, 0.0, 0.0, 0.0], 'rewardMean': 0.6355341084395242, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.7708705933513
'totalSteps': 17920, 'rewardStep': 0.8477078542914069, 'errorList': [], 'lossList': [0.0, -1.3496805030107497, 0.0, 3.482461755275726, 0.0, 0.0, 0.0], 'rewardMean': 0.6362542770796414, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.213367294636
'totalSteps': 19200, 'rewardStep': 0.85893876793265, 'errorList': [], 'lossList': [0.0, -1.3040146201848983, 0.0, 2.3588163942098617, 0.0, 0.0, 0.0], 'rewardMean': 0.6575303484599951, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1085.481784072545
'totalSteps': 20480, 'rewardStep': 0.966642443758672, 'errorList': [0.06072477199055101, 0.05618812187779563, 0.06255110576303805, 0.08899074970129107, 0.06516614168349985, 0.07037577162519229, 0.10643560645983992, 0.07597140112129362, 0.07299374247647321, 0.09788920304084933, 0.06839227642122513, 0.07377329079054908, 0.07076936962815214, 0.08497946784946735, 0.07272335203939347, 0.05619098826574686, 0.08930935423336671, 0.10779597539131441, 0.09128078950702638, 0.09871045617149696, 0.05756920663721192, 0.07899860370917741, 0.10093322171282566, 0.09271315232723763, 0.06403324178975492, 0.061352638280381094, 0.09011999090163583, 0.09597549163983167, 0.10395513036426104, 0.09948493556154156, 0.05777051100194871, 0.10666809486025698, 0.06534055187849837, 0.08824067315134249, 0.12589254079176723, 0.09104291091755057, 0.10537238763383694, 0.08227111854510845, 0.05754820227948025, 0.056759010625860896, 0.06841196362565631, 0.10640271728581946, 0.08222242555928755, 0.08143199584131401, 0.09626890410820285, 0.062379674002795316, 0.05445397301664127, 0.07230438902957238, 0.09443774921766039, 0.09660153235992396], 'lossList': [0.0, -1.2669336259365083, 0.0, 1.8830311214923858, 0.0, 0.0, 0.0], 'rewardMean': 0.7073246235578944, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1116.2047018743767, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=63.11
