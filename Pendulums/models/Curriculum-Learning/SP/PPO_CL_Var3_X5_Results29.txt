#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 29
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7763896557439173, 'errorList': [], 'lossList': [0.0, -1.411834201812744, 0.0, 34.56262944221496, 0.0, 0.0, 0.0], 'rewardMean': 0.8487398949476622, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.539920469997384
'totalSteps': 3840, 'rewardStep': 0.8260992302511535, 'errorList': [], 'lossList': [0.0, -1.417921866774559, 0.0, 38.39704627037048, 0.0, 0.0, 0.0], 'rewardMean': 0.8411930067154927, 'totalEpisodes': 85, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.787361640953
'totalSteps': 5120, 'rewardStep': 0.7898368104000799, 'errorList': [], 'lossList': [0.0, -1.4185836327075958, 0.0, 26.686397647857667, 0.0, 0.0, 0.0], 'rewardMean': 0.8283539576366394, 'totalEpisodes': 94, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.78769748755371
'totalSteps': 6400, 'rewardStep': 0.662344237190418, 'errorList': [], 'lossList': [0.0, -1.4111649870872498, 0.0, 41.71482927799225, 0.0, 0.0, 0.0], 'rewardMean': 0.7951520135473952, 'totalEpisodes': 105, 'stepsPerEpisode': 95, 'rewardPerEpisode': 70.45098577284179
'totalSteps': 7680, 'rewardStep': 0.6176995894811192, 'errorList': [], 'lossList': [0.0, -1.4024196779727935, 0.0, 134.01797374725342, 0.0, 0.0, 0.0], 'rewardMean': 0.7655766095363492, 'totalEpisodes': 145, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2703406202981902
'totalSteps': 8960, 'rewardStep': 0.7745435271958482, 'errorList': [], 'lossList': [0.0, -1.3845513236522675, 0.0, 71.71409059524537, 0.0, 0.0, 0.0], 'rewardMean': 0.7668575977734206, 'totalEpisodes': 177, 'stepsPerEpisode': 27, 'rewardPerEpisode': 21.093510176068637
'totalSteps': 10240, 'rewardStep': 0.7678405715263258, 'errorList': [], 'lossList': [0.0, -1.3569686603546143, 0.0, 36.45286671638489, 0.0, 0.0, 0.0], 'rewardMean': 0.7669804694925336, 'totalEpisodes': 190, 'stepsPerEpisode': 131, 'rewardPerEpisode': 101.78251939659346
'totalSteps': 11520, 'rewardStep': 0.8919484132625046, 'errorList': [], 'lossList': [0.0, -1.3427103769779205, 0.0, 11.230808985233306, 0.0, 0.0, 0.0], 'rewardMean': 0.780865796578086, 'totalEpisodes': 197, 'stepsPerEpisode': 41, 'rewardPerEpisode': 35.62279862286662
'totalSteps': 12800, 'rewardStep': 0.6338529133270014, 'errorList': [], 'lossList': [0.0, -1.3380251055955887, 0.0, 30.57068131685257, 0.0, 0.0, 0.0], 'rewardMean': 0.7661645082529775, 'totalEpisodes': 207, 'stepsPerEpisode': 63, 'rewardPerEpisode': 45.629137009964865
'totalSteps': 14080, 'rewardStep': 0.9446325346966904, 'errorList': [55.05560706993563, 22.061424902057603, 28.837389389246034, 1.3293866111275194, 8.328975612124184, 7.638090546776328, 1.260894749969713, 28.532244140454495, 12.443225731238869, 6.069263661818165, 11.401386492832911, 0.924234410399768, 20.24140417145216, 0.6434731221677469, 4.074628306273157, 87.64533333547209, 6.462885034133527, 3.451585075939319, 22.626287097837054, 3.570861300654983, 30.003886147837612, 0.9251362077429599, 1.3408043501330897, 16.715258734467888, 25.34624243387702, 8.894750268807377, 0.5705101589644791, 41.0302090027972, 0.7853731375978498, 48.44286197797281, 0.7297601756958647, 0.17377859562376677, 30.367466217139935, 39.83209371401431, 33.45611243407181, 55.74440288644146, 8.912407205181614, 36.9931235323235, 4.75356579000059, 1.39305576232653, 1.7471266084831631, 0.7465303497907121, 1.573948768977107, 8.0858256167736, 7.361790769012744, 6.556891716180769, 0.1655239505783591, 44.391849726374836, 6.004596674561744, 5.307679845789468], 'lossList': [0.0, -1.32535861492157, 0.0, 10.251487120389939, 0.0, 0.0, 0.0], 'rewardMean': 0.7685187483075058, 'totalEpisodes': 213, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.780338888955804, 'successfulTests': 2
'totalSteps': 15360, 'rewardStep': 0.8237558407937878, 'errorList': [], 'lossList': [0.0, -1.318622076511383, 0.0, 6.2676405787467955, 0.0, 0.0, 0.0], 'rewardMean': 0.7732553668124929, 'totalEpisodes': 218, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.390882281072823
'totalSteps': 16640, 'rewardStep': 0.7063165913696478, 'errorList': [], 'lossList': [0.0, -1.3091302800178528, 0.0, 5.872633057832718, 0.0, 0.0, 0.0], 'rewardMean': 0.7612771029243424, 'totalEpisodes': 219, 'stepsPerEpisode': 978, 'rewardPerEpisode': 797.0641421242756
'totalSteps': 17920, 'rewardStep': 0.5406781123172206, 'errorList': [], 'lossList': [0.0, -1.2855721396207809, 0.0, 3.1047340655326843, 0.0, 0.0, 0.0], 'rewardMean': 0.7363612331160564, 'totalEpisodes': 220, 'stepsPerEpisode': 566, 'rewardPerEpisode': 423.99611421435344
'totalSteps': 19200, 'rewardStep': 0.856269094728998, 'errorList': [], 'lossList': [0.0, -1.2355101841688156, 0.0, 3.509095295369625, 0.0, 0.0, 0.0], 'rewardMean': 0.7557537188699144, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.5954300777817
'totalSteps': 20480, 'rewardStep': 0.9120014883085594, 'errorList': [], 'lossList': [0.0, -1.1917611873149871, 0.0, 2.5860865479707718, 0.0, 0.0, 0.0], 'rewardMean': 0.7851839087526583, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.0218713126815
'totalSteps': 21760, 'rewardStep': 0.9527696410256783, 'errorList': [0.01696117021871671, 0.018445054296481585, 0.016815519948253557, 0.0329099584126948, 0.030252206543794173, 0.020440355755135444, 0.027764012601369577, 0.01912400683253017, 0.032291983238992114, 0.02628420088471515, 0.02356633425300559, 0.01997830219184221, 0.016565256970079583, 0.017359445988249206, 0.020556473874700523, 0.031778452365449, 0.024441278368486224, 0.017463382963442554, 0.019585837433144678, 0.023825524437907013, 0.019285206790888156, 0.017649706083436134, 0.01708403517117917, 0.03148595695677015, 0.03604864250880378, 0.03699548533251759, 0.03128554352602349, 0.02505588672520204, 0.025371684877370175, 0.025981028087031, 0.017221483873357087, 0.03274548298722516, 0.01933049421759628, 0.01866388179815948, 0.018978219207654537, 0.017781380129996402, 0.0377203300133579, 0.03528958860980398, 0.028881659444581397, 0.022341156558013154, 0.017472324403187283, 0.017374617929364133, 0.018259776504686416, 0.01877233791143811, 0.018325233420970248, 0.01792605899311781, 0.03774907407990945, 0.0413490852699303, 0.01821262756664965, 0.027097321442890614], 'lossList': [0.0, -1.1557006925344466, 0.0, 2.4069608386233448, 0.0, 0.0, 0.0], 'rewardMean': 0.8030065201356414, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.4751742890328, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=21760, timeSpent=88.01
