#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.4353795813109249, 'errorList': [], 'lossList': [0.0, -1.4341241866350174, 0.0, 31.655139603614806, 0.0, 0.0, 0.0], 'rewardMean': 0.47015940402565487, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.310724388754313
'totalSteps': 3840, 'rewardStep': 0.8727051736556336, 'errorList': [], 'lossList': [0.0, -1.4242951929569245, 0.0, 37.115786080360415, 0.0, 0.0, 0.0], 'rewardMean': 0.6043413272356478, 'totalEpisodes': 74, 'stepsPerEpisode': 49, 'rewardPerEpisode': 38.246058859740465
'totalSteps': 5120, 'rewardStep': 0.8229580971540039, 'errorList': [], 'lossList': [0.0, -1.4164610642194748, 0.0, 41.583689427375795, 0.0, 0.0, 0.0], 'rewardMean': 0.6589955197152368, 'totalEpisodes': 81, 'stepsPerEpisode': 71, 'rewardPerEpisode': 59.50190770717384
'totalSteps': 6400, 'rewardStep': 0.8398833463662536, 'errorList': [], 'lossList': [0.0, -1.4142040508985518, 0.0, 118.35479515075684, 0.0, 0.0, 0.0], 'rewardMean': 0.6951730850454402, 'totalEpisodes': 114, 'stepsPerEpisode': 60, 'rewardPerEpisode': 50.53706504796563
'totalSteps': 7680, 'rewardStep': 0.7470320538048448, 'errorList': [], 'lossList': [0.0, -1.402199821472168, 0.0, 75.17795581817627, 0.0, 0.0, 0.0], 'rewardMean': 0.7038162465053409, 'totalEpisodes': 133, 'stepsPerEpisode': 53, 'rewardPerEpisode': 37.7536251471934
'totalSteps': 8960, 'rewardStep': 0.9503305054188356, 'errorList': [4.714017443855612, 18.93643376226921, 53.335531588482404, 17.330106695807665, 2.8731696286967803, 83.78477329412149, 61.69749815959556, 10.310305975398094, 67.701903022793, 70.81243502850899, 58.75111104411075, 37.065954995377055, 23.84314423226795, 81.9036345468219, 9.174736022117349, 17.18893689819332, 3.1553036536185624, 21.198567543459568, 96.37619409614834, 27.531072191409873, 40.85638313605707, 0.6377352599077885, 0.5887582250742477, 13.165972906652378, 28.738635229658108, 75.88498294519096, 22.696456703507707, 36.83420681914282, 28.695529586841154, 66.84855892501118, 87.03325997763318, 45.44053983664603, 90.53029190902025, 93.6672589494615, 20.842793248447087, 9.301345297570025, 51.41773825629987, 52.760347921176304, 56.461523158469106, 58.588955076294994, 68.72540861084013, 87.02392876829492, 1.9941525768250574, 62.779821923572506, 33.71674119263803, 87.95984828200257, 80.54158648105334, 72.15913801647578, 99.50432319420842, 26.65062050983079], 'lossList': [0.0, -1.3877966290712356, 0.0, 49.94570518493652, 0.0, 0.0, 0.0], 'rewardMean': 0.7390325692072688, 'totalEpisodes': 143, 'stepsPerEpisode': 44, 'rewardPerEpisode': 38.08653512893735, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.8756148406509993, 'errorList': [], 'lossList': [0.0, -1.3827548748254777, 0.0, 32.057011966705325, 0.0, 0.0, 0.0], 'rewardMean': 0.7561053531377351, 'totalEpisodes': 148, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.254044343570959
'totalSteps': 11520, 'rewardStep': 0.67988440371259, 'errorList': [], 'lossList': [0.0, -1.3838614988327027, 0.0, 15.236203434467315, 0.0, 0.0, 0.0], 'rewardMean': 0.7476363587571634, 'totalEpisodes': 153, 'stepsPerEpisode': 21, 'rewardPerEpisode': 14.280207304526284
'totalSteps': 12800, 'rewardStep': 0.8423634994184968, 'errorList': [], 'lossList': [0.0, -1.368939922451973, 0.0, 49.23160449504852, 0.0, 0.0, 0.0], 'rewardMean': 0.7571090728232968, 'totalEpisodes': 159, 'stepsPerEpisode': 146, 'rewardPerEpisode': 131.98659566424288
'totalSteps': 14080, 'rewardStep': 0.7369645437648908, 'errorList': [], 'lossList': [0.0, -1.3760430657863616, 0.0, 25.60223487496376, 0.0, 0.0, 0.0], 'rewardMean': 0.7803116045257473, 'totalEpisodes': 162, 'stepsPerEpisode': 536, 'rewardPerEpisode': 421.1912157261721
'totalSteps': 15360, 'rewardStep': 0.4415668774243, 'errorList': [], 'lossList': [0.0, -1.3965191107988357, 0.0, 11.547882763147355, 0.0, 0.0, 0.0], 'rewardMean': 0.7809303341370848, 'totalEpisodes': 165, 'stepsPerEpisode': 282, 'rewardPerEpisode': 204.21519141404576
'totalSteps': 16640, 'rewardStep': 0.45450660167957796, 'errorList': [], 'lossList': [0.0, -1.389144452214241, 0.0, 12.699906306266785, 0.0, 0.0, 0.0], 'rewardMean': 0.7391104769394793, 'totalEpisodes': 168, 'stepsPerEpisode': 453, 'rewardPerEpisode': 309.6675748662302
'totalSteps': 17920, 'rewardStep': 0.8451986942481649, 'errorList': [], 'lossList': [0.0, -1.3872132015228271, 0.0, 6.912338633537292, 0.0, 0.0, 0.0], 'rewardMean': 0.7413345366488953, 'totalEpisodes': 170, 'stepsPerEpisode': 232, 'rewardPerEpisode': 195.00787577820728
'totalSteps': 19200, 'rewardStep': 0.7471317745221396, 'errorList': [], 'lossList': [0.0, -1.382731837630272, 0.0, 3.5179553031921387, 0.0, 0.0, 0.0], 'rewardMean': 0.7320593794644839, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.7645671285997
'totalSteps': 20480, 'rewardStep': 0.9135119648643113, 'errorList': [], 'lossList': [0.0, -1.3426768743991853, 0.0, 2.035493063777685, 0.0, 0.0, 0.0], 'rewardMean': 0.7487073705704306, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1051.0388717183696
'totalSteps': 21760, 'rewardStep': 0.8779388782047347, 'errorList': [], 'lossList': [0.0, -1.2968651914596558, 0.0, 1.627848808467388, 0.0, 0.0, 0.0], 'rewardMean': 0.7414682078490207, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1056.5665175330942
'totalSteps': 23040, 'rewardStep': 0.7751225437933251, 'errorList': [], 'lossList': [0.0, -1.264458172917366, 0.0, 1.5682046877220273, 0.0, 0.0, 0.0], 'rewardMean': 0.7314189781632532, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.3259143763173
'totalSteps': 24320, 'rewardStep': 0.8538208898048111, 'errorList': [], 'lossList': [0.0, -1.2096939069032668, 0.0, 1.1473738100007176, 0.0, 0.0, 0.0], 'rewardMean': 0.7488126267724751, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.1583743593255
'totalSteps': 25600, 'rewardStep': 0.965466159698584, 'errorList': [0.15495943301016024, 0.1034446918893502, 0.08540038300844673, 0.12286999686685368, 0.14616004681365172, 0.09861476171212015, 0.17134345037838658, 0.09815249799840162, 0.13632870935727417, 0.1258026085817662, 0.15764403730983584, 0.10001616328792111, 0.10585229343454902, 0.16538677527201268, 0.1301770017559135, 0.11775418573093897, 0.1467695174764926, 0.13653114887827258, 0.13367939198324544, 0.1563756281640142, 0.12004795051335737, 0.16292299000652852, 0.101649570222053, 0.11797671963239145, 0.11837503954382304, 0.0957500022261239, 0.1064534578129723, 0.13780951048680568, 0.10344587223569733, 0.1457301856938163, 0.14086553186681464, 0.09971812166071961, 0.1444150836616365, 0.13631730236603837, 0.14372688134466524, 0.10718058151532434, 0.1483086361218194, 0.1335262571943571, 0.10947225794517625, 0.1139354427204883, 0.12135895635969829, 0.12206835916456363, 0.13522052410235436, 0.1594814261119265, 0.12230615240904713, 0.12540540139799392, 0.13939169277122712, 0.16316409704573945, 0.16876033351752634, 0.161453688950182], 'lossList': [0.0, -1.1757774645090102, 0.0, 0.7801810906454921, 0.0, 0.0, 0.0], 'rewardMean': 0.761122892800484, 'totalEpisodes': 170, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.8669745936127, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=88.79
