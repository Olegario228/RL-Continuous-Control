#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 31
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6321610332439946, 'errorList': [], 'lossList': [0.0, -1.4222370427846909, 0.0, 26.874282820224764, 0.0, 0.0, 0.0], 'rewardMean': 0.7977252925773521, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.7880474250382
'totalSteps': 3840, 'rewardStep': 0.8354264007710153, 'errorList': [], 'lossList': [0.0, -1.4244197034835815, 0.0, 22.27508318185806, 0.0, 0.0, 0.0], 'rewardMean': 0.8102923286419065, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 254.19230988913537
'totalSteps': 5120, 'rewardStep': 0.6715114854034999, 'errorList': [], 'lossList': [0.0, -1.4249127531051635, 0.0, 19.067114824056624, 0.0, 0.0, 0.0], 'rewardMean': 0.7755971178323048, 'totalEpisodes': 23, 'stepsPerEpisode': 71, 'rewardPerEpisode': 50.11353831449149
'totalSteps': 6400, 'rewardStep': 0.4997193786762626, 'errorList': [], 'lossList': [0.0, -1.4078663152456283, 0.0, 41.05933633327484, 0.0, 0.0, 0.0], 'rewardMean': 0.7204215700010963, 'totalEpisodes': 29, 'stepsPerEpisode': 84, 'rewardPerEpisode': 62.168872917344956
'totalSteps': 7680, 'rewardStep': 0.7151457300814913, 'errorList': [], 'lossList': [0.0, -1.3937937527894975, 0.0, 179.83807338714598, 0.0, 0.0, 0.0], 'rewardMean': 0.7195422633478289, 'totalEpisodes': 65, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.554235108384205
'totalSteps': 8960, 'rewardStep': 0.8862081876297166, 'errorList': [], 'lossList': [0.0, -1.3896083068847656, 0.0, 73.07523317337036, 0.0, 0.0, 0.0], 'rewardMean': 0.7433516811023843, 'totalEpisodes': 87, 'stepsPerEpisode': 119, 'rewardPerEpisode': 93.40399710755472
'totalSteps': 10240, 'rewardStep': 0.8413202448134499, 'errorList': [], 'lossList': [0.0, -1.371613580584526, 0.0, 30.729017417430878, 0.0, 0.0, 0.0], 'rewardMean': 0.7555977515662675, 'totalEpisodes': 97, 'stepsPerEpisode': 158, 'rewardPerEpisode': 110.14876389746644
'totalSteps': 11520, 'rewardStep': 0.44449826862822656, 'errorList': [], 'lossList': [0.0, -1.355978420972824, 0.0, 45.477988324165345, 0.0, 0.0, 0.0], 'rewardMean': 0.7210311423509297, 'totalEpisodes': 104, 'stepsPerEpisode': 283, 'rewardPerEpisode': 224.02118181990397
'totalSteps': 12800, 'rewardStep': 0.3842663907209522, 'errorList': [], 'lossList': [0.0, -1.3345303004980087, 0.0, 29.57706878900528, 0.0, 0.0, 0.0], 'rewardMean': 0.6873546671879318, 'totalEpisodes': 108, 'stepsPerEpisode': 494, 'rewardPerEpisode': 334.73418541673124
'totalSteps': 14080, 'rewardStep': 0.5243140893103455, 'errorList': [], 'lossList': [0.0, -1.3196293014287948, 0.0, 7.673690196275711, 0.0, 0.0, 0.0], 'rewardMean': 0.6434571209278954, 'totalEpisodes': 109, 'stepsPerEpisode': 1257, 'rewardPerEpisode': 813.0402366760586
'totalSteps': 15360, 'rewardStep': 0.7342119763565718, 'errorList': [], 'lossList': [0.0, -1.3186563861370086, 0.0, 8.238656459450722, 0.0, 0.0, 0.0], 'rewardMean': 0.6536622152391531, 'totalEpisodes': 110, 'stepsPerEpisode': 524, 'rewardPerEpisode': 436.4413705444974
'totalSteps': 16640, 'rewardStep': 0.7899378431848763, 'errorList': [], 'lossList': [0.0, -1.2845255959033965, 0.0, 4.873936319351197, 0.0, 0.0, 0.0], 'rewardMean': 0.6491133594805392, 'totalEpisodes': 110, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1001.72279006022
'totalSteps': 17920, 'rewardStep': 0.8712301585846548, 'errorList': [], 'lossList': [0.0, -1.2367850732803345, 0.0, 3.80367587544024, 0.0, 0.0, 0.0], 'rewardMean': 0.6690852267986547, 'totalEpisodes': 110, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1095.6605658918165
'totalSteps': 19200, 'rewardStep': 0.7356877151910026, 'errorList': [], 'lossList': [0.0, -1.1957552605867385, 0.0, 3.173737698942423, 0.0, 0.0, 0.0], 'rewardMean': 0.6926820604501287, 'totalEpisodes': 110, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1113.1617482795998
'totalSteps': 20480, 'rewardStep': 0.8819113230500694, 'errorList': [], 'lossList': [0.0, -1.1743570739030837, 0.0, 1.9588452836871146, 0.0, 0.0, 0.0], 'rewardMean': 0.7093586197469866, 'totalEpisodes': 110, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.4858727883548
'totalSteps': 21760, 'rewardStep': 0.8348687266038558, 'errorList': [], 'lossList': [0.0, -1.1243801057338714, 0.0, 1.4684996804594994, 0.0, 0.0, 0.0], 'rewardMean': 0.7042246736444004, 'totalEpisodes': 110, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.7823650037424
'totalSteps': 23040, 'rewardStep': 0.9816446126005264, 'errorList': [0.06857631241383187, 0.10266829292565206, 0.061954255342434116, 0.0979461797705594, 0.0696466945669204, 0.06858170217355175, 0.09133876523580425, 0.0702995202959964, 0.10138717648958301, 0.06673168720577669, 0.06556341322356611, 0.08128683769303972, 0.12751332035462812, 0.0799034175974261, 0.09325640854882021, 0.11107410612555793, 0.0979969009745391, 0.1118840826260164, 0.06622891972479301, 0.1396382995020561, 0.08712342802233627, 0.06179770555157936, 0.07471806504166709, 0.0956631624283339, 0.1279809104027081, 0.06672074899058986, 0.11525136965766257, 0.1025772606804938, 0.07346329803939432, 0.0686858230624693, 0.08167645622385095, 0.12189510700004981, 0.09728869070819908, 0.13171107274083385, 0.06052711246007803, 0.08769144487410442, 0.0956803594669329, 0.132420070502136, 0.09414192604259623, 0.06373246693506847, 0.07518480396974858, 0.07011195529348328, 0.09204068324766242, 0.08356297118321752, 0.09577029218165523, 0.07583519864985731, 0.07071223283832336, 0.12719671410921216, 0.10946099669492497, 0.06530294564048555], 'lossList': [0.0, -1.0996211683750152, 0.0, 0.9903751886636019, 0.0, 0.0, 0.0], 'rewardMean': 0.7182571104231081, 'totalEpisodes': 110, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.5890508120858, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=67.61
