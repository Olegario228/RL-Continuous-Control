#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 33
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.7166571440094316, 'errorList': [], 'lossList': [0.0, -1.411618869304657, 0.0, 24.98788876056671, 0.0, 0.0, 0.0], 'rewardMean': 0.5305061677774432, 'totalEpisodes': 14, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.01879833949441
'totalSteps': 3840, 'rewardStep': 0.9105234114732605, 'errorList': [], 'lossList': [0.0, -1.4015414810180664, 0.0, 27.08570820093155, 0.0, 0.0, 0.0], 'rewardMean': 0.6571785823427156, 'totalEpisodes': 20, 'stepsPerEpisode': 303, 'rewardPerEpisode': 222.820963274746
'totalSteps': 5120, 'rewardStep': 0.7333412618960655, 'errorList': [], 'lossList': [0.0, -1.4107791751623153, 0.0, 28.5128232216835, 0.0, 0.0, 0.0], 'rewardMean': 0.6762192522310531, 'totalEpisodes': 21, 'stepsPerEpisode': 181, 'rewardPerEpisode': 149.2729449510336
'totalSteps': 6400, 'rewardStep': 0.7612277612161499, 'errorList': [], 'lossList': [0.0, -1.4062472838163376, 0.0, 51.441549525260925, 0.0, 0.0, 0.0], 'rewardMean': 0.6932209540280725, 'totalEpisodes': 27, 'stepsPerEpisode': 27, 'rewardPerEpisode': 19.61054347424353
'totalSteps': 7680, 'rewardStep': 0.7985407438020289, 'errorList': [], 'lossList': [0.0, -1.4061686044931412, 0.0, 267.4775358581543, 0.0, 0.0, 0.0], 'rewardMean': 0.7107742523237318, 'totalEpisodes': 74, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.01707017073228
'totalSteps': 8960, 'rewardStep': 0.5328824263421342, 'errorList': [], 'lossList': [0.0, -1.4084630393981934, 0.0, 106.20083271026611, 0.0, 0.0, 0.0], 'rewardMean': 0.6853611343263607, 'totalEpisodes': 122, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5328824263421342
'totalSteps': 10240, 'rewardStep': 0.4608366623259277, 'errorList': [], 'lossList': [0.0, -1.4063531643152236, 0.0, 52.2286455821991, 0.0, 0.0, 0.0], 'rewardMean': 0.6572955753263066, 'totalEpisodes': 143, 'stepsPerEpisode': 28, 'rewardPerEpisode': 18.073116498323543
'totalSteps': 11520, 'rewardStep': 0.6228920008498866, 'errorList': [], 'lossList': [0.0, -1.39611090362072, 0.0, 28.286553978919983, 0.0, 0.0, 0.0], 'rewardMean': 0.6534729559400376, 'totalEpisodes': 155, 'stepsPerEpisode': 48, 'rewardPerEpisode': 37.39278732821131
'totalSteps': 12800, 'rewardStep': 0.9439048892787706, 'errorList': [34.00087283338173, 144.29624959801228, 148.28292197079782, 139.76820814280543, 38.336478872528254, 117.61253997550119, 58.00023968627881, 70.25981955819374, 32.784043322641985, 104.91345854410464, 69.0775891217937, 116.043685937827, 145.66354767637458, 83.97874562759016, 121.94260854455722, 152.5258848162665, 103.46547526256441, 156.92844976871336, 6.2704456320682365, 134.94039443610265, 57.98901979917483, 64.0976414961356, 66.56892654360497, 120.01370156731542, 125.29982359776442, 75.94256705358856, 11.936417355730974, 92.29011372523816, 107.97977219612294, 59.964991471486286, 124.2502445036561, 60.77870133177232, 130.66614696517442, 168.24619583274787, 138.37953209569886, 88.79682018459978, 66.44520060686698, 13.131899793576208, 94.23006567382131, 137.66946523087208, 130.91638129811236, 85.55352706735566, 139.3924086651948, 4.1877359503334715, 86.28237248346286, 147.0870979227072, 1.6458602598686543, 55.7717255686743, 66.64743089516551, 154.7797338487674], 'lossList': [0.0, -1.3929333806037902, 0.0, 18.452209537029265, 0.0, 0.0, 0.0], 'rewardMean': 0.6825161492739109, 'totalEpisodes': 161, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.750139643443602, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.5334007165299829, 'errorList': [], 'lossList': [0.0, -1.4000025004148484, 0.0, 21.56421877861023, 0.0, 0.0, 0.0], 'rewardMean': 0.7014207017723638, 'totalEpisodes': 167, 'stepsPerEpisode': 89, 'rewardPerEpisode': 55.09849872418773
'totalSteps': 15360, 'rewardStep': 0.5400219063969587, 'errorList': [], 'lossList': [0.0, -1.3984583413600922, 0.0, 9.300057046413421, 0.0, 0.0, 0.0], 'rewardMean': 0.6837571780111166, 'totalEpisodes': 174, 'stepsPerEpisode': 93, 'rewardPerEpisode': 75.4272623678992
'totalSteps': 16640, 'rewardStep': 0.20278683449778379, 'errorList': [], 'lossList': [0.0, -1.3874312347173692, 0.0, 5.952579268217087, 0.0, 0.0, 0.0], 'rewardMean': 0.6129835203135688, 'totalEpisodes': 179, 'stepsPerEpisode': 208, 'rewardPerEpisode': 150.93880438032983
'totalSteps': 17920, 'rewardStep': 0.9950510017359536, 'errorList': [2.6887757049827115, 31.665094776130115, 28.46080687432285, 34.677471014158456, 15.42616604970235, 6.955270839236865, 3.550187059404786, 15.920300972775912, 25.911477466993155, 53.67003449185641, 3.281512998784986, 13.670038463396256, 64.57060816438592, 42.11217001290258, 9.342742328625704, 54.90529599297708, 16.868666818245984, 1.7181050972351457, 26.164303032527048, 0.7160443182476037, 36.15082424020741, 16.056924658704805, 25.865645146034417, 49.67764134519931, 51.720632826507945, 29.095972649649312, 10.367610875712318, 36.17138139736222, 47.746302710944875, 0.5190081266768205, 61.264601419936746, 46.113040368485144, 7.234960716670739, 39.749476319746165, 5.205006108637417, 84.65362202927966, 43.75657800826723, 12.765840466608903, 3.519836875795683, 32.033038835808654, 77.78269639872083, 46.18517236765198, 6.545491909136214, 0.962903769109525, 1.700626822895636, 47.38571092563112, 10.07757443443538, 0.934567831606958, 13.477555362561118, 3.5785939978826464], 'lossList': [0.0, -1.3791844737529755, 0.0, 5.062547665834427, 0.0, 0.0, 0.0], 'rewardMean': 0.6391544942975577, 'totalEpisodes': 184, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9950510017359536, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.9232647180501741, 'errorList': [], 'lossList': [0.0, -1.3807350993156433, 0.0, 6.029814771413803, 0.0, 0.0, 0.0], 'rewardMean': 0.6553581899809601, 'totalEpisodes': 188, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.2871188901994
'totalSteps': 20480, 'rewardStep': 0.9062019013567879, 'errorList': [], 'lossList': [0.0, -1.3585455232858659, 0.0, 4.031068477630615, 0.0, 0.0, 0.0], 'rewardMean': 0.6661243057364359, 'totalEpisodes': 192, 'stepsPerEpisode': 236, 'rewardPerEpisode': 204.4555660501833
'totalSteps': 21760, 'rewardStep': 0.8429484029527816, 'errorList': [], 'lossList': [0.0, -1.3541016340255738, 0.0, 4.678686620593071, 0.0, 0.0, 0.0], 'rewardMean': 0.6971309033975007, 'totalEpisodes': 194, 'stepsPerEpisode': 191, 'rewardPerEpisode': 158.62269398757022
'totalSteps': 23040, 'rewardStep': 0.7361262589182888, 'errorList': [], 'lossList': [0.0, -1.3535728448629378, 0.0, 3.456783241033554, 0.0, 0.0, 0.0], 'rewardMean': 0.7246598630567369, 'totalEpisodes': 196, 'stepsPerEpisode': 179, 'rewardPerEpisode': 151.8049339809386
'totalSteps': 24320, 'rewardStep': 0.6252733470109458, 'errorList': [], 'lossList': [0.0, -1.3452345597743989, 0.0, 3.3848693639039995, 0.0, 0.0, 0.0], 'rewardMean': 0.7248979976728428, 'totalEpisodes': 197, 'stepsPerEpisode': 564, 'rewardPerEpisode': 427.5577145418972
'totalSteps': 25600, 'rewardStep': 0.8800315465387131, 'errorList': [], 'lossList': [0.0, -1.3189408272504806, 0.0, 1.1704309190809727, 0.0, 0.0, 0.0], 'rewardMean': 0.7185106633988371, 'totalEpisodes': 197, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1074.366781004317
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=97.15
