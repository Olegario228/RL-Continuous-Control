#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 35
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9248519622068129, 'errorList': [], 'lossList': [0.0, -1.4073586767911912, 0.0, 27.19815846979618, 0.0, 0.0, 0.0], 'rewardMean': 0.8623254613528879, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.0762142306127
'totalSteps': 3840, 'rewardStep': 0.6359618487477414, 'errorList': [], 'lossList': [0.0, -1.3999829375743866, 0.0, 33.715749049186705, 0.0, 0.0, 0.0], 'rewardMean': 0.7868709238178391, 'totalEpisodes': 11, 'stepsPerEpisode': 263, 'rewardPerEpisode': 201.11211177161985
'totalSteps': 5120, 'rewardStep': 0.6988928167673585, 'errorList': [], 'lossList': [0.0, -1.4016289383172988, 0.0, 23.58582045316696, 0.0, 0.0, 0.0], 'rewardMean': 0.7648763970552189, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 945.3330117468062
'totalSteps': 6400, 'rewardStep': 0.8654628912344322, 'errorList': [], 'lossList': [0.0, -1.3930897587537765, 0.0, 32.7783971118927, 0.0, 0.0, 0.0], 'rewardMean': 0.7849936958910616, 'totalEpisodes': 13, 'stepsPerEpisode': 207, 'rewardPerEpisode': 165.12381115313423
'totalSteps': 7680, 'rewardStep': 0.46256286864893015, 'errorList': [], 'lossList': [0.0, -1.393787783384323, 0.0, 301.6061637115479, 0.0, 0.0, 0.0], 'rewardMean': 0.7312552246840397, 'totalEpisodes': 63, 'stepsPerEpisode': 52, 'rewardPerEpisode': 38.20491585546575
'totalSteps': 8960, 'rewardStep': 0.8443735390721854, 'errorList': [], 'lossList': [0.0, -1.3946005219221116, 0.0, 137.72699333190917, 0.0, 0.0, 0.0], 'rewardMean': 0.7474149838823462, 'totalEpisodes': 109, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.65137991743943
'totalSteps': 10240, 'rewardStep': 0.9561648304322701, 'errorList': [53.95973980168091, 8.586036036239646, 217.85620692492466, 131.74792039336842, 129.39649270621143, 149.7817387630935, 118.78488461818768, 206.93256377481575, 199.68550116847064, 185.9816936859962, 40.42760841265691, 189.018112559796, 223.63755960110322, 96.11593146761963, 90.30295599991842, 175.02763533075657, 190.0193636934981, 107.56047077435746, 223.00094336510398, 25.467525262825305, 210.79139908253853, 114.27379820719885, 87.44618090967273, 130.67139052958655, 193.10016851542937, 130.94814004336502, 209.4143984596665, 112.7843433349279, 91.2081042692019, 82.18669192159314, 218.83197472202676, 97.0871267753134, 187.92676997027706, 104.52042682170855, 130.40774250709822, 152.27900641500818, 75.69810614554049, 217.35744909867284, 119.9493402959796, 78.63387592950696, 91.22128925583216, 179.96593627624947, 57.69518579791341, 172.20293329546996, 42.78078267386239, 184.93214610448052, 95.32059914717873, 164.95268368539726, 86.82726655268665, 109.35785496628158], 'lossList': [0.0, -1.394589092731476, 0.0, 88.34404214859009, 0.0, 0.0, 0.0], 'rewardMean': 0.7735087147010866, 'totalEpisodes': 150, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.837222232515234, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5289895367033428, 'errorList': [], 'lossList': [0.0, -1.3886825782060623, 0.0, 56.868578310012815, 0.0, 0.0, 0.0], 'rewardMean': 0.7463399171457817, 'totalEpisodes': 168, 'stepsPerEpisode': 70, 'rewardPerEpisode': 57.42736801275867
'totalSteps': 12800, 'rewardStep': 0.8669799837702753, 'errorList': [], 'lossList': [0.0, -1.3780251228809357, 0.0, 54.01552981376648, 0.0, 0.0, 0.0], 'rewardMean': 0.7584039238082311, 'totalEpisodes': 182, 'stepsPerEpisode': 43, 'rewardPerEpisode': 33.71483392305557
'totalSteps': 14080, 'rewardStep': 0.7000046481360465, 'errorList': [], 'lossList': [0.0, -1.3781464505195617, 0.0, 23.13827003955841, 0.0, 0.0, 0.0], 'rewardMean': 0.7484244925719394, 'totalEpisodes': 186, 'stepsPerEpisode': 403, 'rewardPerEpisode': 318.3402270751402
'totalSteps': 15360, 'rewardStep': 0.4275044314918432, 'errorList': [], 'lossList': [0.0, -1.377720227241516, 0.0, 22.355003273487092, 0.0, 0.0, 0.0], 'rewardMean': 0.6986897395004426, 'totalEpisodes': 194, 'stepsPerEpisode': 99, 'rewardPerEpisode': 55.98118485405796
'totalSteps': 16640, 'rewardStep': 0.8958027303193785, 'errorList': [], 'lossList': [0.0, -1.3724767190217972, 0.0, 9.85013411283493, 0.0, 0.0, 0.0], 'rewardMean': 0.7246738276576062, 'totalEpisodes': 200, 'stepsPerEpisode': 122, 'rewardPerEpisode': 107.82187418486657
'totalSteps': 17920, 'rewardStep': 0.7807633381837441, 'errorList': [], 'lossList': [0.0, -1.385465424656868, 0.0, 9.386237118244171, 0.0, 0.0, 0.0], 'rewardMean': 0.7328608797992449, 'totalEpisodes': 205, 'stepsPerEpisode': 223, 'rewardPerEpisode': 176.40515182049484
'totalSteps': 19200, 'rewardStep': 0.7417850390264646, 'errorList': [], 'lossList': [0.0, -1.3944806694984435, 0.0, 21.5544473528862, 0.0, 0.0, 0.0], 'rewardMean': 0.7204930945784481, 'totalEpisodes': 210, 'stepsPerEpisode': 232, 'rewardPerEpisode': 200.65756672468478
'totalSteps': 20480, 'rewardStep': 0.8623389149890607, 'errorList': [], 'lossList': [0.0, -1.3890485513210296, 0.0, 7.560838533639908, 0.0, 0.0, 0.0], 'rewardMean': 0.7604706992124612, 'totalEpisodes': 213, 'stepsPerEpisode': 263, 'rewardPerEpisode': 212.80484798606503
'totalSteps': 21760, 'rewardStep': 0.9693405904052049, 'errorList': [1.1167868403579404, 0.7128449719894241, 1.155317958089945, 1.4274645168941225, 1.6373121105896584, 0.6567297551862654, 1.0654258509098304, 1.1093535498203464, 1.060998214491345, 0.6847545735614585, 1.113117153553135, 1.0494361501763632, 0.8347643798971224, 0.7277023020733207, 1.1940466853804426, 0.8287254171254432, 0.6617950764671495, 1.0661519392663474, 0.9230356343066121, 1.1823913865576694, 0.5190936440199684, 1.4213677832004628, 1.4714283801987196, 1.1544278459350577, 0.623576559897249, 0.9961918875775955, 1.0225440851804752, 0.8705818791916912, 0.7462714546695213, 0.6373510037541177, 0.7241053630251515, 1.0182450133515843, 0.8669124037635509, 0.7917554496961406, 0.7443223216032355, 0.882606716728356, 1.6748094639626723, 0.9477196604789005, 1.3397179865187836, 1.3993123289795215, 1.0372365649128077, 1.3580492969362945, 1.4771773762471114, 1.2322765034885743, 0.54988722293833, 1.2027610057306914, 1.2370936324785362, 0.8330984335631682, 0.8947176165135958, 0.73697755438412], 'lossList': [0.0, -1.3911038744449615, 0.0, 14.333024152517318, 0.0, 0.0, 0.0], 'rewardMean': 0.772967404345763, 'totalEpisodes': 215, 'stepsPerEpisode': 61, 'rewardPerEpisode': 56.402865889592434, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.6017732641882347, 'errorList': [], 'lossList': [0.0, -1.3766121810674667, 0.0, 5.937223470211029, 0.0, 0.0, 0.0], 'rewardMean': 0.7375282477213594, 'totalEpisodes': 217, 'stepsPerEpisode': 255, 'rewardPerEpisode': 201.2153649142409
'totalSteps': 24320, 'rewardStep': 0.945157551582663, 'errorList': [0.05886056082321678, 0.03585750898559352, 0.0899110869433329, 0.02464336675644065, 0.011884701034417364, 0.017661473847353965, 0.01145436774275618, 0.03572864208873982, 0.01181916576713647, 0.013557963080318763, 0.06858144850536016, 0.04013880438968754, 0.048158906103681315, 0.04539826278395584, 0.014952570759843775, 0.015108335876931776, 0.09171901756613463, 0.03589838141596715, 0.027302104411126973, 0.08721900704629663, 0.012063471641469896, 0.06273864298491849, 0.010881947488829096, 0.012981370831834066, 0.013080563916802785, 0.06201864945361916, 0.051026144028718114, 0.02779572261443009, 0.017395319835198462, 0.04041330203833932, 0.042016471424218514, 0.0670261450186044, 0.04846237543431275, 0.05697967147920242, 0.02033218605695619, 0.026594617850973834, 0.045579436337557754, 0.011783025853105793, 0.05890476072196293, 0.02249458978908925, 0.04171726104101922, 0.011639054490563877, 0.011735421565499032, 0.026933308110520228, 0.05175390623939892, 0.024933162090509164, 0.05131464052642339, 0.060972405853464647, 0.013423022703877296, 0.0250914193189078], 'lossList': [0.0, -1.3604767084121705, 0.0, 2.3435265868902206, 0.0, 0.0, 0.0], 'rewardMean': 0.7791450492092916, 'totalEpisodes': 217, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.6512754030048, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=113.92
