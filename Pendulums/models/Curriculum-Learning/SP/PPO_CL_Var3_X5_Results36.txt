#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 36
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7407010887934742, 'errorList': [], 'lossList': [0.0, -1.4381606233119966, 0.0, 33.305376117229464, 0.0, 0.0, 0.0], 'rewardMean': 0.6287474952464636, 'totalEpisodes': 12, 'stepsPerEpisode': 75, 'rewardPerEpisode': 63.58138556519636
'totalSteps': 3840, 'rewardStep': 0.8651081993313754, 'errorList': [], 'lossList': [0.0, -1.445183921456337, 0.0, 27.453558329939842, 0.0, 0.0, 0.0], 'rewardMean': 0.7075343966081009, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 978.3046969497739
'totalSteps': 5120, 'rewardStep': 0.5773952407088208, 'errorList': [], 'lossList': [0.0, -1.4194166058301925, 0.0, 25.0924144166708, 0.0, 0.0, 0.0], 'rewardMean': 0.6749996076332809, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.6578936548299
'totalSteps': 6400, 'rewardStep': 0.7310497153667207, 'errorList': [], 'lossList': [0.0, -1.4114104360342026, 0.0, 28.118432270288466, 0.0, 0.0, 0.0], 'rewardMean': 0.6862096291799689, 'totalEpisodes': 13, 'stepsPerEpisode': 304, 'rewardPerEpisode': 232.449915698403
'totalSteps': 7680, 'rewardStep': 0.6411003740744171, 'errorList': [], 'lossList': [0.0, -1.3932477205991745, 0.0, 379.7763638305664, 0.0, 0.0, 0.0], 'rewardMean': 0.6786914199957103, 'totalEpisodes': 60, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.940815334995932
'totalSteps': 8960, 'rewardStep': 0.63803052119995, 'errorList': [], 'lossList': [0.0, -1.3919534581899642, 0.0, 171.41351375579833, 0.0, 0.0, 0.0], 'rewardMean': 0.6728827201677444, 'totalEpisodes': 97, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.914707171209642
'totalSteps': 10240, 'rewardStep': 0.5364186023666112, 'errorList': [], 'lossList': [0.0, -1.388800710439682, 0.0, 71.31927202224732, 0.0, 0.0, 0.0], 'rewardMean': 0.6558247054426027, 'totalEpisodes': 134, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.421071599506831
'totalSteps': 11520, 'rewardStep': 0.8913390175597216, 'errorList': [], 'lossList': [0.0, -1.381304116845131, 0.0, 45.34948341369629, 0.0, 0.0, 0.0], 'rewardMean': 0.6819929623445048, 'totalEpisodes': 162, 'stepsPerEpisode': 47, 'rewardPerEpisode': 39.521566387905914
'totalSteps': 12800, 'rewardStep': 0.6386460620275115, 'errorList': [], 'lossList': [0.0, -1.3701447707414627, 0.0, 41.33832424163818, 0.0, 0.0, 0.0], 'rewardMean': 0.6776582723128055, 'totalEpisodes': 178, 'stepsPerEpisode': 83, 'rewardPerEpisode': 62.94125769648825
'totalSteps': 14080, 'rewardStep': 0.7472869079952865, 'errorList': [], 'lossList': [0.0, -1.370961844921112, 0.0, 27.726879663467408, 0.0, 0.0, 0.0], 'rewardMean': 0.7007075729423888, 'totalEpisodes': 184, 'stepsPerEpisode': 121, 'rewardPerEpisode': 92.62820704389476
'totalSteps': 15360, 'rewardStep': 0.5191458016648364, 'errorList': [], 'lossList': [0.0, -1.3786447006464004, 0.0, 19.633171429634093, 0.0, 0.0, 0.0], 'rewardMean': 0.6785520442295252, 'totalEpisodes': 187, 'stepsPerEpisode': 227, 'rewardPerEpisode': 153.01516129783863
'totalSteps': 16640, 'rewardStep': 0.8232523535544378, 'errorList': [], 'lossList': [0.0, -1.3690384370088577, 0.0, 39.712046389579776, 0.0, 0.0, 0.0], 'rewardMean': 0.6743664596518315, 'totalEpisodes': 190, 'stepsPerEpisode': 69, 'rewardPerEpisode': 59.16987411344393
'totalSteps': 17920, 'rewardStep': 0.4988615459561548, 'errorList': [], 'lossList': [0.0, -1.3462210083007813, 0.0, 30.88908721923828, 0.0, 0.0, 0.0], 'rewardMean': 0.6665130901765648, 'totalEpisodes': 192, 'stepsPerEpisode': 402, 'rewardPerEpisode': 251.27082852403507
'totalSteps': 19200, 'rewardStep': 0.8084868610696571, 'errorList': [], 'lossList': [0.0, -1.3161485242843627, 0.0, 3.8447681617736817, 0.0, 0.0, 0.0], 'rewardMean': 0.6742568047468583, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 913.631226977008
'totalSteps': 20480, 'rewardStep': 0.49765569726407277, 'errorList': [], 'lossList': [0.0, -1.2938659465312958, 0.0, 4.632714954018593, 0.0, 0.0, 0.0], 'rewardMean': 0.659912337065824, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 864.9966246511858
'totalSteps': 21760, 'rewardStep': 0.808741830114265, 'errorList': [], 'lossList': [0.0, -1.2921689331531525, 0.0, 7.675558147132397, 0.0, 0.0, 0.0], 'rewardMean': 0.6769834679572555, 'totalEpisodes': 193, 'stepsPerEpisode': 1216, 'rewardPerEpisode': 1008.8871031706868
'totalSteps': 23040, 'rewardStep': 0.8468478942122282, 'errorList': [], 'lossList': [0.0, -1.2942466801404953, 0.0, 2.9974503991007806, 0.0, 0.0, 0.0], 'rewardMean': 0.7080263971418171, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.152812721229
'totalSteps': 24320, 'rewardStep': 0.6122658776487907, 'errorList': [], 'lossList': [0.0, -1.2808744114637376, 0.0, 1.6385370168089866, 0.0, 0.0, 0.0], 'rewardMean': 0.6801190831507241, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1107.2891483841822
'totalSteps': 25600, 'rewardStep': 0.9611186552399034, 'errorList': [0.40458542570267353, 0.27485183672126473, 0.33824486556591477, 0.28566005376573106, 0.3563270656781913, 0.23913363615325822, 0.29133863099408974, 0.2799758410107538, 0.29874387495153654, 0.3149300827308736, 0.4930492272861762, 0.24215699082617945, 0.3122800866077956, 0.4959169757591426, 0.3442327074628006, 0.30897514019534117, 0.3276337963701856, 0.29993456237580685, 0.3007120234285169, 0.23848083439276374, 0.3484743915310742, 0.2847194627581402, 0.3384104841301053, 0.3127636354472299, 0.32089993569769126, 0.42929573069887056, 0.4238328554296676, 0.2762812540277439, 0.3312924400990347, 0.3112146491184696, 0.33976564127094433, 0.3857671398821707, 0.33986227634750626, 0.30196694822454734, 0.37530071639105284, 0.28679366081102564, 0.5079443710924074, 0.31596224570680526, 0.2822682112015418, 0.3910697660290831, 0.46155608089578115, 0.39385620708662084, 0.28649177506551116, 0.3909367277896013, 0.4780890460901163, 0.5139505129226947, 0.28623696019565875, 0.31983158799688954, 0.31673191320574856, 0.2891572929176353], 'lossList': [0.0, -1.2443486243486404, 0.0, 1.810903228893876, 0.0, 0.0, 0.0], 'rewardMean': 0.7123663424719633, 'totalEpisodes': 193, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.3101567334625, 'successfulTests': 0
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=77.45
