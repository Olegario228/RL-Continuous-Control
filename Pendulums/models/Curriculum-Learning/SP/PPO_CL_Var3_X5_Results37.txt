#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 37
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.8174916574059212, 'errorList': [], 'lossList': [0.0, -1.4439504539966583, 0.0, 25.709826090335845, 0.0, 0.0, 0.0], 'rewardMean': 0.6277611248058976, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.7593737456101
'totalSteps': 3840, 'rewardStep': 0.8842154070678298, 'errorList': [], 'lossList': [0.0, -1.4505782306194306, 0.0, 38.609818778038026, 0.0, 0.0, 0.0], 'rewardMean': 0.713245885559875, 'totalEpisodes': 14, 'stepsPerEpisode': 486, 'rewardPerEpisode': 391.3892735668796
'totalSteps': 5120, 'rewardStep': 0.6678626844049899, 'errorList': [], 'lossList': [0.0, -1.4276825374364852, 0.0, 18.79318938612938, 0.0, 0.0, 0.0], 'rewardMean': 0.7019000852711538, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 907.373319526011
'totalSteps': 6400, 'rewardStep': 0.279761471322859, 'errorList': [], 'lossList': [0.0, -1.404539890885353, 0.0, 41.13063128948212, 0.0, 0.0, 0.0], 'rewardMean': 0.6174723624814948, 'totalEpisodes': 17, 'stepsPerEpisode': 188, 'rewardPerEpisode': 137.81533865356664
'totalSteps': 7680, 'rewardStep': 0.63645557316584, 'errorList': [], 'lossList': [0.0, -1.3902856987714767, 0.0, 309.15245780944826, 0.0, 0.0, 0.0], 'rewardMean': 0.6206362309288856, 'totalEpisodes': 70, 'stepsPerEpisode': 56, 'rewardPerEpisode': 40.790722074690855
'totalSteps': 8960, 'rewardStep': 0.9109961917354997, 'errorList': [], 'lossList': [0.0, -1.380363836288452, 0.0, 136.49709861755372, 0.0, 0.0, 0.0], 'rewardMean': 0.6621162253298305, 'totalEpisodes': 117, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7944833255357904
'totalSteps': 10240, 'rewardStep': 0.5418886035463656, 'errorList': [], 'lossList': [0.0, -1.3698152989149093, 0.0, 73.8833032989502, 0.0, 0.0, 0.0], 'rewardMean': 0.6470877726068973, 'totalEpisodes': 151, 'stepsPerEpisode': 111, 'rewardPerEpisode': 97.20364408043247
'totalSteps': 11520, 'rewardStep': 0.7443739897602245, 'errorList': [], 'lossList': [0.0, -1.3639688074588776, 0.0, 49.05348695755005, 0.0, 0.0, 0.0], 'rewardMean': 0.6578973522906003, 'totalEpisodes': 165, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.836482763804915
'totalSteps': 12800, 'rewardStep': 0.8623195155264649, 'errorList': [], 'lossList': [0.0, -1.3584637081623077, 0.0, 33.04340211391449, 0.0, 0.0, 0.0], 'rewardMean': 0.6783395686141868, 'totalEpisodes': 173, 'stepsPerEpisode': 152, 'rewardPerEpisode': 121.74507179574285
'totalSteps': 14080, 'rewardStep': 0.531181946731446, 'errorList': [], 'lossList': [0.0, -1.3670671290159226, 0.0, 23.74758627176285, 0.0, 0.0, 0.0], 'rewardMean': 0.687654704066744, 'totalEpisodes': 179, 'stepsPerEpisode': 81, 'rewardPerEpisode': 45.116693847843614
'totalSteps': 15360, 'rewardStep': 0.968393286981464, 'errorList': [1.5893608800531358, 0.3006872580960258, 3.0266880778867926, 1.2724541047451292, 0.34902472366631493, 2.3847525346450156, 4.599540835802454, 2.9729088310464484, 2.079967627473688, 4.25217840496822, 3.974702223927061, 2.4441751699060337, 1.2821740867036744, 3.459241097997933, 0.7229285105965724, 3.3832662571732537, 0.8358568275252404, 3.89680033207505, 2.336199747779754, 1.5780847893865397, 0.524343799050667, 0.19701648474813718, 1.152159897667726, 2.9661541807929783, 0.5651640161635301, 2.321670254606422, 4.840535494230181, 0.7086823202250861, 3.0354416888380853, 5.474038987237903, 3.0431332781297202, 2.589260806697787, 0.9972081906767526, 4.707883930351682, 3.0767787329767335, 1.1508062398215806, 2.019367452572241, 3.592821486773855, 1.6787106188974874, 4.253385053732739, 2.852483387688766, 0.285679873515042, 2.238404686240762, 3.2334862895562293, 0.5446085446190605, 2.7148923421388593, 1.5163770021361844, 2.618496528176944, 2.0845373449690277, 0.43191701491880935], 'lossList': [0.0, -1.3792077785730361, 0.0, 28.86815043449402, 0.0, 0.0, 0.0], 'rewardMean': 0.7027448670242984, 'totalEpisodes': 185, 'stepsPerEpisode': 315, 'rewardPerEpisode': 243.8162573191987, 'successfulTests': 1
'totalSteps': 16640, 'rewardStep': 0.941783057279958, 'errorList': [2.0012334980898467, 1.39354863297532, 0.8502626794958367, 1.6250231534522452, 0.7951166785289524, 0.335881590729097, 1.0094264989073087, 1.4608329881635675, 1.066912524464324, 1.2197614189870016, 1.3773083204419938, 0.6831439617836256, 1.6402164886867665, 2.1210012587096276, 1.6683269390924402, 2.16912283740065, 1.706100314071507, 1.4932004200026454, 0.8778848313947033, 0.8777757733761958, 0.36065420908646467, 1.6393815265894056, 1.3558266189082357, 1.6285829320956413, 1.0897456424870697, 1.0725698797361074, 1.4786496901692312, 1.1355777194688743, 1.6782043286475246, 0.7528319244057076, 0.31369551507942994, 0.6304906177525565, 1.2221239545269846, 0.6655519113407093, 1.0722320319647756, 1.5803164895850685, 0.6405795108676768, 0.982291635063984, 0.4346005868802445, 1.846661729535463, 0.7032415791897816, 1.650596124122775, 2.0438012992854446, 1.3758449550228922, 1.4290208017891324, 1.9513989942813994, 0.3682294171541304, 1.1471842452254541, 1.3822839476294042, 1.4021435281735553], 'lossList': [0.0, -1.3832195687294007, 0.0, 9.899946138858795, 0.0, 0.0, 0.0], 'rewardMean': 0.7085016320455112, 'totalEpisodes': 189, 'stepsPerEpisode': 95, 'rewardPerEpisode': 86.15401729562568, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.717357081665609, 'errorList': [], 'lossList': [0.0, -1.3778261297941208, 0.0, 7.0932637262344365, 0.0, 0.0, 0.0], 'rewardMean': 0.7134510717715731, 'totalEpisodes': 192, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.42457754427458
'totalSteps': 19200, 'rewardStep': 0.47751098401915554, 'errorList': [], 'lossList': [0.0, -1.3594697123765946, 0.0, 4.058152368962765, 0.0, 0.0, 0.0], 'rewardMean': 0.7332260230412027, 'totalEpisodes': 193, 'stepsPerEpisode': 781, 'rewardPerEpisode': 574.2551757844284
'totalSteps': 20480, 'rewardStep': 0.5927397848591844, 'errorList': [], 'lossList': [0.0, -1.3354581701755524, 0.0, 4.041403647065163, 0.0, 0.0, 0.0], 'rewardMean': 0.7288544442105371, 'totalEpisodes': 194, 'stepsPerEpisode': 1258, 'rewardPerEpisode': 913.7197934705737
'totalSteps': 21760, 'rewardStep': 0.6764240716901468, 'errorList': [], 'lossList': [0.0, -1.3039176118373872, 0.0, 4.851688117980957, 0.0, 0.0, 0.0], 'rewardMean': 0.7053972322060019, 'totalEpisodes': 196, 'stepsPerEpisode': 367, 'rewardPerEpisode': 287.46261656969995
'totalSteps': 23040, 'rewardStep': 0.868400186993947, 'errorList': [], 'lossList': [0.0, -1.2656652891635896, 0.0, 2.6188092048466207, 0.0, 0.0, 0.0], 'rewardMean': 0.73804839055076, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.7512790460376
'totalSteps': 24320, 'rewardStep': 0.9373724236803669, 'errorList': [0.07187605819381379, 0.09271229734437822, 0.07720193219264466, 0.06897311116563372, 0.1464710740085826, 0.06673247456889331, 0.05746843053605266, 0.10340345898583382, 0.09180133188807507, 0.07607446674059262, 0.07435856232300343, 0.06718794795265529, 0.07954416232973509, 0.07021478172293483, 0.057499772708713516, 0.06283890020657122, 0.0809489477221906, 0.08757591770242136, 0.08198568089714295, 0.06784852949831383, 0.08165328264677867, 0.07272924042502359, 0.06998480435967669, 0.07698261226390825, 0.07504090050259288, 0.0703687796625685, 0.07515541121465373, 0.08350775748613484, 0.08933711991835404, 0.1018260530937411, 0.09362594990230279, 0.06798010371071261, 0.12325824839176179, 0.06546753787613045, 0.07454581939036622, 0.09241868965788513, 0.1518187673737119, 0.0622968696471328, 0.07172744720695304, 0.09123668740596061, 0.0671355727350314, 0.14087709027128018, 0.14247269279138733, 0.15924496990641376, 0.0699968538685295, 0.11905248969704042, 0.10580329976169658, 0.10383418333906273, 0.06801581669542944, 0.1477635507904063], 'lossList': [0.0, -1.2262324160337448, 0.0, 1.867388236373663, 0.0, 0.0, 0.0], 'rewardMean': 0.7573482339427742, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.9306107595044, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=113.17
