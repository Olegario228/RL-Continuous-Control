#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 38
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6571201623105263, 'errorList': [], 'lossList': [0.0, -1.42193930208683, 0.0, 27.96087454080582, 0.0, 0.0, 0.0], 'rewardMean': 0.7112229976287912, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.153571933162407
'totalSteps': 3840, 'rewardStep': 0.9600672603386498, 'errorList': [], 'lossList': [0.0, -1.4156641280651092, 0.0, 26.860459023714064, 0.0, 0.0, 0.0], 'rewardMean': 0.794171085198744, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 373.0022593732252
'totalSteps': 5120, 'rewardStep': 0.7935787378817845, 'errorList': [], 'lossList': [0.0, -1.4098810976743699, 0.0, 25.96647956430912, 0.0, 0.0, 0.0], 'rewardMean': 0.7940229983695042, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.5451804254408
'totalSteps': 6400, 'rewardStep': 0.6095028959580269, 'errorList': [], 'lossList': [0.0, -1.3922065609693528, 0.0, 33.91412172436714, 0.0, 0.0, 0.0], 'rewardMean': 0.7571189778872087, 'totalEpisodes': 20, 'stepsPerEpisode': 757, 'rewardPerEpisode': 561.5070582039655
'totalSteps': 7680, 'rewardStep': 0.5075403576690228, 'errorList': [], 'lossList': [0.0, -1.3754707247018814, 0.0, 332.1013858795166, 0.0, 0.0, 0.0], 'rewardMean': 0.7155225411841778, 'totalEpisodes': 69, 'stepsPerEpisode': 27, 'rewardPerEpisode': 17.743981856203746
'totalSteps': 8960, 'rewardStep': 0.7101223581769182, 'errorList': [], 'lossList': [0.0, -1.3748575621843337, 0.0, 91.41573570251465, 0.0, 0.0, 0.0], 'rewardMean': 0.714751086468855, 'totalEpisodes': 123, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.39586595764328
'totalSteps': 10240, 'rewardStep': 0.6285837479553357, 'errorList': [], 'lossList': [0.0, -1.3708105075359345, 0.0, 38.75572817802429, 0.0, 0.0, 0.0], 'rewardMean': 0.703980169154665, 'totalEpisodes': 163, 'stepsPerEpisode': 66, 'rewardPerEpisode': 57.53268661430751
'totalSteps': 11520, 'rewardStep': 0.7472875722268928, 'errorList': [], 'lossList': [0.0, -1.359073413014412, 0.0, 28.64569251060486, 0.0, 0.0, 0.0], 'rewardMean': 0.7087921028293569, 'totalEpisodes': 188, 'stepsPerEpisode': 28, 'rewardPerEpisode': 18.943233770769158
'totalSteps': 12800, 'rewardStep': 0.6131403167871602, 'errorList': [], 'lossList': [0.0, -1.3400072693824767, 0.0, 23.695667552948, 0.0, 0.0, 0.0], 'rewardMean': 0.6992269242251373, 'totalEpisodes': 196, 'stepsPerEpisode': 62, 'rewardPerEpisode': 50.14656463241358
'totalSteps': 14080, 'rewardStep': 0.91612653645013, 'errorList': [], 'lossList': [0.0, -1.321492889523506, 0.0, 20.793722643852234, 0.0, 0.0, 0.0], 'rewardMean': 0.7143069945754448, 'totalEpisodes': 202, 'stepsPerEpisode': 35, 'rewardPerEpisode': 30.064149637670116
'totalSteps': 15360, 'rewardStep': 0.6716727923444695, 'errorList': [], 'lossList': [0.0, -1.3046043348312377, 0.0, 17.00672749042511, 0.0, 0.0, 0.0], 'rewardMean': 0.7157622575788389, 'totalEpisodes': 206, 'stepsPerEpisode': 183, 'rewardPerEpisode': 151.16565544639172
'totalSteps': 16640, 'rewardStep': 0.7489891699887335, 'errorList': [], 'lossList': [0.0, -1.2985054337978363, 0.0, 9.339938458800315, 0.0, 0.0, 0.0], 'rewardMean': 0.6946544485438474, 'totalEpisodes': 210, 'stepsPerEpisode': 250, 'rewardPerEpisode': 210.984095400472
'totalSteps': 17920, 'rewardStep': 0.7581186846279611, 'errorList': [], 'lossList': [0.0, -1.3136954349279404, 0.0, 6.969049464464188, 0.0, 0.0, 0.0], 'rewardMean': 0.6911084432184651, 'totalEpisodes': 212, 'stepsPerEpisode': 560, 'rewardPerEpisode': 477.5238545242121
'totalSteps': 19200, 'rewardStep': 0.9580412122491312, 'errorList': [0.023104058956063937, 0.04775597346651468, 0.13667377305921402, 0.08292272987159273, 0.31479318864564476, 0.042519096863270885, 0.12190730841106509, 0.315394144958588, 0.08832739243176903, 0.15827326825868032, 0.061682240726372135, 0.10693757423838764, 0.1571998350935054, 0.12358876178419426, 0.07471974529513355, 0.27555581987503924, 0.20699137008163168, 0.12663476298209475, 0.32605777367505945, 0.24732956337883302, 0.2804452978686757, 0.2428322115589513, 0.07147426850404698, 0.21260153565033602, 0.11050648559921464, 0.10917144518458674, 0.0440142789272501, 0.1477164876311166, 0.011818818715524426, 0.09008548059455866, 0.27589415917920523, 0.13479712479049646, 0.24344379583034156, 0.35435339093807106, 0.3685856955604373, 0.028763744533987473, 0.4686970932806839, 0.11889801275873903, 0.10581422393031238, 0.10150278971395606, 0.056118897101985844, 0.15460890628470114, 0.02785565260253635, 0.11354507541679244, 0.33061960053006567, 0.3872766503318398, 0.34105063436932503, 0.13045705958907083, 0.1129283062612335, 0.323012233515607], 'lossList': [0.0, -1.3220754420757295, 0.0, 11.42755697965622, 0.0, 0.0, 0.0], 'rewardMean': 0.7259622748475755, 'totalEpisodes': 214, 'stepsPerEpisode': 688, 'rewardPerEpisode': 581.309864594796, 'successfulTests': 32
'totalSteps': 20480, 'rewardStep': 0.6338885790963165, 'errorList': [], 'lossList': [0.0, -1.2791565179824829, 0.0, 4.19429144307971, 0.0, 0.0, 0.0], 'rewardMean': 0.7385970969903048, 'totalEpisodes': 214, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1061.413278292073
'totalSteps': 21760, 'rewardStep': 0.6412222926807971, 'errorList': [], 'lossList': [0.0, -1.251552502512932, 0.0, 2.889630551636219, 0.0, 0.0, 0.0], 'rewardMean': 0.7317070904406927, 'totalEpisodes': 215, 'stepsPerEpisode': 666, 'rewardPerEpisode': 512.8333879213544
'totalSteps': 23040, 'rewardStep': 0.5736855909133345, 'errorList': [], 'lossList': [0.0, -1.253725705742836, 0.0, 4.0524237781763075, 0.0, 0.0, 0.0], 'rewardMean': 0.7262172747364927, 'totalEpisodes': 216, 'stepsPerEpisode': 851, 'rewardPerEpisode': 663.5689312708173
'totalSteps': 24320, 'rewardStep': 0.7586541730515328, 'errorList': [], 'lossList': [0.0, -1.234080959558487, 0.0, 1.8306932799518107, 0.0, 0.0, 0.0], 'rewardMean': 0.7273539348189567, 'totalEpisodes': 216, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1045.521415439596
'totalSteps': 25600, 'rewardStep': 0.9614654506607069, 'errorList': [0.01661291607404669, 0.01138252204094192, 0.011047158367463781, 0.009913940809878622, 0.022877300848400145, 0.01696144133213044, 0.018824987289939338, 0.02020428288244923, 0.011342915669919582, 0.027886878036851432, 0.0068042522379947465, 0.05385519513816312, 0.0050206576520935445, 0.030729510407928824, 0.005478102933981972, 0.01833476366776941, 0.005788607810736847, 0.02784038738550372, 0.017868739045114074, 0.04473071598882361, 0.00786741084561908, 0.007424128728549256, 0.0074565522901434665, 0.027326002044365742, 0.004391525182531411, 0.006076503664025459, 0.01697500124678078, 0.01701648934393284, 0.02918846367571233, 0.005484234711393998, 0.016699626794151507, 0.022905070768161447, 0.009004554073807357, 0.006709848851408265, 0.025421572023639635, 0.029613275254778727, 0.0036698232663050596, 0.028119885372815037, 0.009398388350703724, 0.01678332299491871, 0.027416268233097344, 0.004836612171546524, 0.028734961107707697, 0.032058108676124845, 0.03887757213453452, 0.024796412620871948, 0.0055460781294681535, 0.002840159182360824, 0.020371835416276497, 0.010726600608172562], 'lossList': [0.0, -1.2037255203723907, 0.0, 1.0901968420669437, 0.0, 0.0, 0.0], 'rewardMean': 0.7621864482063113, 'totalEpisodes': 216, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.6046471929058, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=96.56
