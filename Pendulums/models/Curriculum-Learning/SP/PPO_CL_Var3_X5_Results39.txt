#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 39
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8691215657080864, 'errorList': [], 'lossList': [0.0, -1.4240571051836013, 0.0, 30.638251933455468, 0.0, 0.0, 0.0], 'rewardMean': 0.7705184859257148, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 252.4322285407175
'totalSteps': 3840, 'rewardStep': 0.7698139740252374, 'errorList': [], 'lossList': [0.0, -1.44177070915699, 0.0, 30.321251035928725, 0.0, 0.0, 0.0], 'rewardMean': 0.7702836486255557, 'totalEpisodes': 15, 'stepsPerEpisode': 168, 'rewardPerEpisode': 126.47281179298376
'totalSteps': 5120, 'rewardStep': 0.5690203117146297, 'errorList': [], 'lossList': [0.0, -1.4442673462629319, 0.0, 21.44167984902859, 0.0, 0.0, 0.0], 'rewardMean': 0.7199678143978242, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 938.4105335295732
'totalSteps': 6400, 'rewardStep': 0.9639595040470221, 'errorList': [84.4451135474946, 93.81438908687467, 90.36713629592516, 94.61103957229473, 77.70012598352217, 97.7398606803434, 94.48976435251501, 95.97948632360385, 91.02386616440342, 89.16091824932244, 96.408340535895, 93.74214867714143, 96.7675561983514, 90.78766554251034, 88.66901560873646, 96.81981076310704, 94.94386558306844, 91.48962836065448, 95.66176855291243, 92.28053178962728, 84.75372184190684, 92.52663230156709, 93.03722195190075, 87.04691306444363, 78.8602002137268, 95.2217915643857, 94.45422201630633, 97.65668903630852, 91.68546450743825, 93.97179968405214, 91.42293468287778, 95.35321515516713, 94.31016032716184, 91.79789675531056, 91.58994067687023, 96.41885791608404, 92.45117845223095, 96.4310243326797, 91.36597002933298, 84.91672324630008, 89.06891025144853, 89.53288067598352, 88.87476848102457, 96.13399704027329, 91.53733830289292, 87.35922672948537, 96.82120031989459, 72.27675298452752, 89.5413306600973, 92.53928948040816], 'lossList': [0.0, -1.4339848268032074, 0.0, 44.91847055912018, 0.0, 0.0, 0.0], 'rewardMean': 0.7687661523276639, 'totalEpisodes': 19, 'stepsPerEpisode': 34, 'rewardPerEpisode': 28.883645505837475, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.8065939895933378, 'errorList': [], 'lossList': [0.0, -1.436182782649994, 0.0, 322.18013900756836, 0.0, 0.0, 0.0], 'rewardMean': 0.7750707918719429, 'totalEpisodes': 78, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.352376269553742
'totalSteps': 8960, 'rewardStep': 0.8117865666858881, 'errorList': [], 'lossList': [0.0, -1.4350219941139222, 0.0, 107.0595274734497, 0.0, 0.0, 0.0], 'rewardMean': 0.7803159025596493, 'totalEpisodes': 127, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.665267924111164
'totalSteps': 10240, 'rewardStep': 0.7473297282306698, 'errorList': [], 'lossList': [0.0, -1.4356785225868225, 0.0, 70.45820518493652, 0.0, 0.0, 0.0], 'rewardMean': 0.7761926307685267, 'totalEpisodes': 168, 'stepsPerEpisode': 30, 'rewardPerEpisode': 21.73753957815395
'totalSteps': 11520, 'rewardStep': 0.9456720007500831, 'errorList': [6.899712890958515, 166.46157245049582, 209.10673214935778, 125.5729929853958, 51.49307694753628, 153.56349833141974, 181.8580913496194, 189.6241468964468, 191.37201690421568, 157.4215988349785, 178.34499581242497, 159.61908226693106, 149.76778229671322, 196.8815954081688, 124.49609840357904, 141.02000831540127, 104.22875200400003, 106.20013466604267, 179.6991557040506, 178.57869082510828, 32.05473677024986, 119.24894416504821, 140.31930026796698, 180.21313376752764, 171.97502221356388, 153.16581443845453, 205.9417204709384, 178.28988911130443, 141.88834569462114, 178.86198725012167, 134.26249120329675, 188.9424357549137, 187.6495183190424, 160.25271419296993, 204.1615815667944, 208.10945236394568, 101.63731878457625, 75.87230112808632, 107.8952262772161, 188.70403374170274, 171.7927508717048, 193.82159521487915, 72.65946818163559, 133.2536563188943, 214.5440705576173, 203.12314247324915, 50.533820588750466, 204.2811125118908, 85.79556669928215, 217.26774268425905], 'lossList': [0.0, -1.4211720937490464, 0.0, 40.41611694335938, 0.0, 0.0, 0.0], 'rewardMean': 0.7950236718775885, 'totalEpisodes': 185, 'stepsPerEpisode': 57, 'rewardPerEpisode': 47.44951284004101, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.6952263494835305, 'errorList': [], 'lossList': [0.0, -1.4014828026294708, 0.0, 43.24900290489197, 0.0, 0.0, 0.0], 'rewardMean': 0.7850439396381826, 'totalEpisodes': 200, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.688290569706182
'totalSteps': 14080, 'rewardStep': 0.934632181532279, 'errorList': [30.12295869361011, 99.64173624268265, 57.66545262624005, 59.67516811608671, 36.81862168563137, 100.22701109508085, 120.13478972912293, 139.89189966512885, 43.52541375056765, 104.74220852070948, 36.55133899633817, 128.61769892146697, 92.01591177978916, 101.56791764966044, 50.95637663310247, 113.32393911874297, 124.14111570443472, 35.950702996474746, 84.08860349794814, 149.237794099353, 6.956015176613894, 111.6724522754556, 132.16725430523923, 119.22644984704364, 63.85835952038324, 22.33644566450684, 69.84234860949466, 68.32691202540134, 73.04289269202849, 83.09768174402289, 80.69340137543115, 29.502762508452957, 38.26820215328822, 105.29610793385105, 81.01007032688561, 98.22513050290439, 78.89646028461586, 115.54865145463968, 146.23121229627242, 146.73241771436108, 148.19781802351957, 114.85590579899795, 7.267427853330674, 159.42243373578978, 97.12055344001176, 11.40334495504424, 46.884973699372466, 145.90887297202286, 100.98156356964732, 16.858302748758877], 'lossList': [0.0, -1.3910493409633637, 0.0, 17.866427848339082, 0.0, 0.0, 0.0], 'rewardMean': 0.8113156171770765, 'totalEpisodes': 206, 'stepsPerEpisode': 63, 'rewardPerEpisode': 52.45440245445604, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.9554466191138875, 'errorList': [59.751461447973945, 14.59347089026875, 1.2019689584372686, 17.370864885018502, 6.091375725006063, 52.47230829200999, 135.051846624772, 90.14363542857495, 69.92622972115551, 118.86430234761949, 125.04421404656955, 77.74813337409047, 84.27174383678313, 27.604160426442863, 54.24542653052434, 4.284626063300881, 18.470994378189136, 60.8660883434227, 106.29973600867933, 136.6838789579961, 22.793847240139275, 84.63562139123268, 64.85299525999291, 26.898968546773812, 141.06451927933026, 95.8077749579845, 60.79529939722505, 122.09409817658818, 118.29604636097733, 94.67512800125094, 81.65160653649734, 14.056635797411026, 134.6269589110649, 40.759478541968264, 37.60927127021754, 34.844487114152386, 77.65101004304775, 60.402268038055624, 30.920035311496736, 125.15432548828791, 107.85641861697604, 12.546099349042327, 73.41348503046991, 95.42946626787018, 102.40773402922724, 164.51295937163965, 38.82725278755697, 52.63362086942795, 93.68237507409991, 34.281523369089285], 'lossList': [0.0, -1.365820075273514, 0.0, 44.004962449073794, 0.0, 0.0, 0.0], 'rewardMean': 0.8199481225176566, 'totalEpisodes': 215, 'stepsPerEpisode': 68, 'rewardPerEpisode': 60.62781090513723, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.596843510224033, 'errorList': [], 'lossList': [0.0, -1.3576222252845764, 0.0, 17.88309257030487, 0.0, 0.0, 0.0], 'rewardMean': 0.8026510761375361, 'totalEpisodes': 223, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.767394486196874
'totalSteps': 17920, 'rewardStep': 0.5992560304510981, 'errorList': [], 'lossList': [0.0, -1.3494852340221406, 0.0, 9.435999258756638, 0.0, 0.0, 0.0], 'rewardMean': 0.8056746480111829, 'totalEpisodes': 228, 'stepsPerEpisode': 104, 'rewardPerEpisode': 84.52832295667328
'totalSteps': 19200, 'rewardStep': 0.7790710864552841, 'errorList': [], 'lossList': [0.0, -1.3367320001125336, 0.0, 6.167064390182495, 0.0, 0.0, 0.0], 'rewardMean': 0.7871858062520091, 'totalEpisodes': 233, 'stepsPerEpisode': 186, 'rewardPerEpisode': 153.7848609252103
'totalSteps': 20480, 'rewardStep': 0.39783425706312603, 'errorList': [], 'lossList': [0.0, -1.3060170269012452, 0.0, 3.6800604313611984, 0.0, 0.0, 0.0], 'rewardMean': 0.7463098329989879, 'totalEpisodes': 239, 'stepsPerEpisode': 256, 'rewardPerEpisode': 198.81457678584866
'totalSteps': 21760, 'rewardStep': 0.7302567416387677, 'errorList': [], 'lossList': [0.0, -1.2845320624113084, 0.0, 4.669135286211968, 0.0, 0.0, 0.0], 'rewardMean': 0.7381568504942759, 'totalEpisodes': 243, 'stepsPerEpisode': 172, 'rewardPerEpisode': 152.0641176139728
'totalSteps': 23040, 'rewardStep': 0.8897594172152721, 'errorList': [], 'lossList': [0.0, -1.2604712855815887, 0.0, 4.404022295475006, 0.0, 0.0, 0.0], 'rewardMean': 0.7523998193927361, 'totalEpisodes': 247, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.36804510886568
'totalSteps': 24320, 'rewardStep': 0.774238366453522, 'errorList': [], 'lossList': [0.0, -1.2475497722625732, 0.0, 2.7504002624750137, 0.0, 0.0, 0.0], 'rewardMean': 0.73525645596308, 'totalEpisodes': 249, 'stepsPerEpisode': 101, 'rewardPerEpisode': 89.62950829174838
'totalSteps': 25600, 'rewardStep': 0.6028487886203866, 'errorList': [], 'lossList': [0.0, -1.2337168318033218, 0.0, 2.0855554115772246, 0.0, 0.0, 0.0], 'rewardMean': 0.7260186998767656, 'totalEpisodes': 251, 'stepsPerEpisode': 441, 'rewardPerEpisode': 353.5448670315856
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=136.32
