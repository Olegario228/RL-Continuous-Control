#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7763389196168753, 'errorList': [], 'lossList': [0.0, -1.4118305200338364, 0.0, 34.555690603256224, 0.0, 0.0, 0.0], 'rewardMean': 0.8487145268841412, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.539676833611527
'totalSteps': 3840, 'rewardStep': 0.8359029171744546, 'errorList': [], 'lossList': [0.0, -1.4110790288448334, 0.0, 43.791619567871095, 0.0, 0.0, 0.0], 'rewardMean': 0.8444439903142457, 'totalEpisodes': 92, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.0807140844348
'totalSteps': 5120, 'rewardStep': 0.9220590271938145, 'errorList': [], 'lossList': [0.0, -1.391271910071373, 0.0, 37.79684679508209, 0.0, 0.0, 0.0], 'rewardMean': 0.8638477495341379, 'totalEpisodes': 103, 'stepsPerEpisode': 42, 'rewardPerEpisode': 35.23383281456823
'totalSteps': 6400, 'rewardStep': 0.9755691632869258, 'errorList': [186.53732173560633, 26.866363161850373, 214.03149812777684, 78.01278919483848, 188.06034413858197, 203.02274962702077, 116.61467135230527, 253.6428276852497, 188.89171562497071, 163.07585245276775, 246.9870953186256, 182.51953804362674, 232.38775632271825, 239.08120543539533, 227.13772976621604, 80.05508942959827, 215.19860833847207, 262.10265744362874, 34.53284968989627, 113.36018043927116, 184.2146548857752, 85.83224493845582, 263.3523007672048, 19.473499995880413, 106.24924896943065, 166.7576956855856, 143.38375084011463, 222.2653222631356, 244.41465939974714, 237.46930399049447, 250.17941242797002, 164.235959300078, 195.62809575659108, 235.1412038758783, 196.68730066480586, 202.50971271748784, 89.20829536886112, 234.46636359109382, 56.29842049911835, 242.8467889289761, 215.46222548032392, 234.43382346113668, 197.5356741533022, 220.95802213725412, 40.998587605897654, 218.25842026139983, 238.25515836878722, 154.70766871189844, 227.8858147864956, 148.93342485198266], 'lossList': [0.0, -1.3647549998760224, 0.0, 82.82948244094848, 0.0, 0.0, 0.0], 'rewardMean': 0.8861920322846955, 'totalEpisodes': 142, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9755691632869258, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.725886389003833, 'errorList': [], 'lossList': [0.0, -1.351446635723114, 0.0, 66.39836204528808, 0.0, 0.0, 0.0], 'rewardMean': 0.8594744250712183, 'totalEpisodes': 167, 'stepsPerEpisode': 36, 'rewardPerEpisode': 22.515532547285723
'totalSteps': 8960, 'rewardStep': 0.853333230149231, 'errorList': [], 'lossList': [0.0, -1.3351415342092514, 0.0, 35.95124893188476, 0.0, 0.0, 0.0], 'rewardMean': 0.8585971115109344, 'totalEpisodes': 182, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.9502280262334843
'totalSteps': 10240, 'rewardStep': 0.9424037721336409, 'errorList': [0.24191677749782786, 1.5283289874941317, 0.3228358512600039, 1.7147277970357693, 1.0112256237789363, 0.052457713873894134, 0.24718124455900706, 0.44629017092954554, 0.1508492446329027, 0.7196238428051238, 0.08666457798098909, 0.26405664676109464, 1.233028503838361, 0.47339045083545256, 0.8779461240341099, 1.6518383599487458, 0.2745453877587128, 0.417403870575908, 1.6585918369108612, 0.4235030715925755, 1.115621243209573, 1.2587250526136735, 0.34869802992113924, 0.2001770674070624, 0.597206767240553, 3.4498501349013773, 0.5120767935224176, 0.5946689420830737, 1.3506106938115032, 0.041953133062685756, 1.141000588987572, 1.1093320978707912, 0.33057654442558715, 2.2118698135176285, 3.497543138299665, 1.3797726403585864, 0.5034877015465182, 1.5946188582620642, 0.9287875962894461, 0.9736403544621004, 0.4818759792052667, 0.15088467033493985, 0.8375443821049436, 0.026358834075105878, 1.056732891167042, 0.20833007393384526, 0.7851578065937543, 2.274505898811092, 1.6774959861393588, 0.07307938021865588], 'lossList': [0.0, -1.3100410938262939, 0.0, 24.2219992685318, 0.0, 0.0, 0.0], 'rewardMean': 0.8690729440887728, 'totalEpisodes': 186, 'stepsPerEpisode': 96, 'rewardPerEpisode': 83.52929580227037, 'successfulTests': 7
'totalSteps': 11520, 'rewardStep': 0.8622604313811252, 'errorList': [], 'lossList': [0.0, -1.2868334364891052, 0.0, 12.539205441474914, 0.0, 0.0, 0.0], 'rewardMean': 0.8683159982323675, 'totalEpisodes': 188, 'stepsPerEpisode': 136, 'rewardPerEpisode': 104.07492699557785
'totalSteps': 12800, 'rewardStep': 0.6947913570691193, 'errorList': [], 'lossList': [0.0, -1.2720035475492477, 0.0, 18.549992175102233, 0.0, 0.0, 0.0], 'rewardMean': 0.8509635341160428, 'totalEpisodes': 192, 'stepsPerEpisode': 65, 'rewardPerEpisode': 49.25528295131256
'totalSteps': 14080, 'rewardStep': 0.8938794697515297, 'errorList': [], 'lossList': [0.0, -1.261278396844864, 0.0, 9.055361731052399, 0.0, 0.0, 0.0], 'rewardMean': 0.8482424676760549, 'totalEpisodes': 196, 'stepsPerEpisode': 68, 'rewardPerEpisode': 57.9687665521534
'totalSteps': 15360, 'rewardStep': 0.8469812985051025, 'errorList': [], 'lossList': [0.0, -1.2494912540912628, 0.0, 26.312896330356597, 0.0, 0.0, 0.0], 'rewardMean': 0.8553067055648776, 'totalEpisodes': 198, 'stepsPerEpisode': 897, 'rewardPerEpisode': 687.8291046141236
'totalSteps': 16640, 'rewardStep': 0.7225397658655898, 'errorList': [], 'lossList': [0.0, -1.226215495467186, 0.0, 7.581315593123436, 0.0, 0.0, 0.0], 'rewardMean': 0.8439703904339911, 'totalEpisodes': 202, 'stepsPerEpisode': 85, 'rewardPerEpisode': 76.88826523188271
'totalSteps': 17920, 'rewardStep': 0.7593288480341062, 'errorList': [], 'lossList': [0.0, -1.2139030557870865, 0.0, 6.3045895898342135, 0.0, 0.0, 0.0], 'rewardMean': 0.8276973725180203, 'totalEpisodes': 205, 'stepsPerEpisode': 255, 'rewardPerEpisode': 215.9203399372698
'totalSteps': 19200, 'rewardStep': 0.8532207916564368, 'errorList': [], 'lossList': [0.0, -1.1926700621843338, 0.0, 2.2426032757759096, 0.0, 0.0, 0.0], 'rewardMean': 0.8154625353549715, 'totalEpisodes': 206, 'stepsPerEpisode': 379, 'rewardPerEpisode': 305.5222366713126
'totalSteps': 20480, 'rewardStep': 0.8565550473756065, 'errorList': [], 'lossList': [0.0, -1.173743115067482, 0.0, 2.0279347482323646, 0.0, 0.0, 0.0], 'rewardMean': 0.8285294011921488, 'totalEpisodes': 206, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.8055124458815
'totalSteps': 21760, 'rewardStep': 0.8614993919831464, 'errorList': [], 'lossList': [0.0, -1.1593896263837815, 0.0, 1.4904601142555476, 0.0, 0.0, 0.0], 'rewardMean': 0.8293460173755403, 'totalEpisodes': 206, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.3882416572674
'totalSteps': 23040, 'rewardStep': 0.9057360085776883, 'errorList': [], 'lossList': [0.0, -1.1728890705108643, 0.0, 1.2268685630708933, 0.0, 0.0, 0.0], 'rewardMean': 0.8256792410199452, 'totalEpisodes': 206, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.5939835474112
'totalSteps': 24320, 'rewardStep': 0.9205232675196121, 'errorList': [], 'lossList': [0.0, -1.1672216933965682, 0.0, 1.1192870211414994, 0.0, 0.0, 0.0], 'rewardMean': 0.8315055246337938, 'totalEpisodes': 206, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.5935212471745
'totalSteps': 25600, 'rewardStep': 0.9872296106422637, 'errorList': [0.08497950711666485, 0.05452988212314955, 0.03504160376288315, 0.07432958830271363, 0.05419159920640555, 0.03177406073902223, 0.039376351999023916, 0.034889219726978886, 0.04353387970600233, 0.048183566503887366, 0.030456268071538554, 0.04728995702421544, 0.045439473117700684, 0.036619914105079227, 0.055925146015783876, 0.12337664079697483, 0.05519355666976474, 0.0570817620747438, 0.040006800567566905, 0.11289287950735591, 0.07656574810349825, 0.04170140845624631, 0.11284971307624443, 0.05274802083675905, 0.05667032428618432, 0.04820959027666168, 0.13113849577247, 0.1199019729584006, 0.0910157245353544, 0.0827343955497556, 0.07965769486478304, 0.10374794272800342, 0.16624531409520965, 0.12026905028562775, 0.18375337909111822, 0.045936934676525536, 0.03788683556734182, 0.08343086503266087, 0.05534900912912457, 0.042364265984947816, 0.12483687506239116, 0.10279029228654478, 0.048555049966229405, 0.07538413068506272, 0.05179448774062191, 0.15603703059766164, 0.041280104879180386, 0.04320499219610601, 0.13972133374736592, 0.19252830796649942], 'lossList': [0.0, -1.1385647118091584, 0.0, 0.7548540750145912, 0.0, 0.0, 0.0], 'rewardMean': 0.8607493499911081, 'totalEpisodes': 206, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1202.5590494925216, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=109.81
