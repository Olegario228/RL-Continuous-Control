#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 41
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8584572092779964, 'errorList': [], 'lossList': [0.0, -1.4447246259450912, 0.0, 36.90878702163696, 0.0, 0.0, 0.0], 'rewardMean': 0.7085524638771757, 'totalEpisodes': 12, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.026598510164554
'totalSteps': 3840, 'rewardStep': 0.895210834290315, 'errorList': [], 'lossList': [0.0, -1.4571798765659332, 0.0, 30.76148638010025, 0.0, 0.0, 0.0], 'rewardMean': 0.7707719206815554, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1008.7685028187497
'totalSteps': 5120, 'rewardStep': 0.6233991102563647, 'errorList': [], 'lossList': [0.0, -1.439084032177925, 0.0, 28.664685833454133, 0.0, 0.0, 0.0], 'rewardMean': 0.7339287180752578, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1041.914672367165
'totalSteps': 6400, 'rewardStep': 0.8487446050335452, 'errorList': [], 'lossList': [0.0, -1.435669379234314, 0.0, 19.54375437259674, 0.0, 0.0, 0.0], 'rewardMean': 0.7568918954669153, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.4004869522335
'totalSteps': 7680, 'rewardStep': 0.7224096464434965, 'errorList': [], 'lossList': [0.0, -1.4085019469261169, 0.0, 392.775651550293, 0.0, 0.0, 0.0], 'rewardMean': 0.7511448539630122, 'totalEpisodes': 56, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.3267459049467294
'totalSteps': 8960, 'rewardStep': 0.7074426640499641, 'errorList': [], 'lossList': [0.0, -1.406011620759964, 0.0, 259.85713962554934, 0.0, 0.0, 0.0], 'rewardMean': 0.7449016839754339, 'totalEpisodes': 104, 'stepsPerEpisode': 36, 'rewardPerEpisode': 32.80122313456974
'totalSteps': 10240, 'rewardStep': 0.9211084859652185, 'errorList': [], 'lossList': [0.0, -1.4000841027498245, 0.0, 118.39597316741943, 0.0, 0.0, 0.0], 'rewardMean': 0.766927534224157, 'totalEpisodes': 150, 'stepsPerEpisode': 33, 'rewardPerEpisode': 29.22461678892384
'totalSteps': 11520, 'rewardStep': 0.5018243897412972, 'errorList': [], 'lossList': [0.0, -1.3872444003820419, 0.0, 68.22719505310059, 0.0, 0.0, 0.0], 'rewardMean': 0.737471629281617, 'totalEpisodes': 181, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.507315379442893
'totalSteps': 12800, 'rewardStep': 0.5622020728571043, 'errorList': [], 'lossList': [0.0, -1.3697172558307649, 0.0, 54.744203109741214, 0.0, 0.0, 0.0], 'rewardMean': 0.7199446736391658, 'totalEpisodes': 204, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.719296552277077
'totalSteps': 14080, 'rewardStep': 0.9004758011551636, 'errorList': [], 'lossList': [0.0, -1.3725601226091384, 0.0, 42.116143255233766, 0.0, 0.0, 0.0], 'rewardMean': 0.7541274819070466, 'totalEpisodes': 219, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.310298346290065
'totalSteps': 15360, 'rewardStep': 0.7241315891649913, 'errorList': [], 'lossList': [0.0, -1.3830067312717438, 0.0, 19.797386515140534, 0.0, 0.0, 0.0], 'rewardMean': 0.740694919895746, 'totalEpisodes': 230, 'stepsPerEpisode': 58, 'rewardPerEpisode': 45.128724878871175
'totalSteps': 16640, 'rewardStep': 0.8432972357916647, 'errorList': [], 'lossList': [0.0, -1.3847754561901093, 0.0, 25.165942902565003, 0.0, 0.0, 0.0], 'rewardMean': 0.735503560045881, 'totalEpisodes': 238, 'stepsPerEpisode': 69, 'rewardPerEpisode': 55.07504437939532
'totalSteps': 17920, 'rewardStep': 0.6980153138469669, 'errorList': [], 'lossList': [0.0, -1.3850396984815598, 0.0, 17.00432449698448, 0.0, 0.0, 0.0], 'rewardMean': 0.7429651804049413, 'totalEpisodes': 245, 'stepsPerEpisode': 27, 'rewardPerEpisode': 18.26542999790351
'totalSteps': 19200, 'rewardStep': 0.95368129050643, 'errorList': [56.643555641065404, 69.0873977362255, 53.76353897919977, 113.4245833673042, 102.96313552109774, 16.549232328899986, 25.360268378071826, 94.02453276551068, 62.09010229120407, 47.568884071143486, 88.97284487108024, 29.49226198297372, 97.70299148378892, 34.2442687890153, 23.20486325215662, 0.8657455714976109, 23.052834262176535, 71.90250915273977, 32.74673653272611, 9.934991090321082, 9.004587392483414, 88.16689173026411, 16.86159841008276, 49.402314907659374, 25.637756832801532, 86.50084994929908, 17.412562646199692, 50.76390334484879, 85.9255589189835, 15.58041537828421, 50.39874475337464, 8.645915820395592, 39.95095053490756, 4.676416342908627, 15.851531992210479, 92.7656293410867, 25.19748521661887, 5.354942525511127, 42.748579446015746, 22.744979698635767, 90.08998578843307, 16.653467501785414, 72.43736788656439, 66.3262267045524, 4.875461341926732, 124.43917347421127, 1.5867478426125408, 49.11404667094867, 65.59975015016627, 39.093334638400414], 'lossList': [0.0, -1.3759167504310608, 0.0, 7.406734994649887, 0.0, 0.0, 0.0], 'rewardMean': 0.7534588489522297, 'totalEpisodes': 251, 'stepsPerEpisode': 60, 'rewardPerEpisode': 52.385530811936455, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.2563772084274183, 'errorList': [], 'lossList': [0.0, -1.353574162721634, 0.0, 7.593842544555664, 0.0, 0.0, 0.0], 'rewardMean': 0.7068556051506218, 'totalEpisodes': 255, 'stepsPerEpisode': 229, 'rewardPerEpisode': 189.5607203263872
'totalSteps': 21760, 'rewardStep': 0.4473694441806466, 'errorList': [], 'lossList': [0.0, -1.3328097695112229, 0.0, 27.732842931747438, 0.0, 0.0, 0.0], 'rewardMean': 0.6808482831636902, 'totalEpisodes': 260, 'stepsPerEpisode': 243, 'rewardPerEpisode': 191.65919865148706
'totalSteps': 23040, 'rewardStep': 0.8585156115734257, 'errorList': [], 'lossList': [0.0, -1.2925964611768723, 0.0, 4.0532667428255085, 0.0, 0.0, 0.0], 'rewardMean': 0.6745889957245108, 'totalEpisodes': 262, 'stepsPerEpisode': 710, 'rewardPerEpisode': 582.4117984547609
'totalSteps': 24320, 'rewardStep': 0.5790142001416277, 'errorList': [], 'lossList': [0.0, -1.2535051369667054, 0.0, 3.704773429632187, 0.0, 0.0, 0.0], 'rewardMean': 0.6823079767645439, 'totalEpisodes': 263, 'stepsPerEpisode': 650, 'rewardPerEpisode': 508.6558559251379
'totalSteps': 25600, 'rewardStep': 0.9529679841185162, 'errorList': [0.08862081297313609, 0.09970674990584615, 0.08275283888429061, 0.16671845720183395, 0.03561592462560218, 0.06274429295806404, 0.07780566441530187, 0.042117244953121004, 0.061955007006200435, 0.037692265572331025, 0.06715375050103432, 0.05678492952947717, 0.11070792606813434, 0.041447025780742316, 0.14428842276825204, 0.06256182620324527, 0.05176455877600493, 0.09724049107476693, 0.04646333376557722, 0.09338337572884436, 0.061648331197164376, 0.06757470987406457, 0.06747185537145632, 0.1334429980273402, 0.10412425521810327, 0.038280016973536904, 0.1322941739657994, 0.11085068494236901, 0.086150072523151, 0.07176386272136313, 0.05804256599937016, 0.05163896467433568, 0.17318923342920628, 0.18028531145832286, 0.05041908586177248, 0.13751244596181086, 0.049197438889733616, 0.0841788284077398, 0.0503001351765011, 0.14103413139412435, 0.13843964436830494, 0.04704802151603021, 0.10275181724758578, 0.0375426671538535, 0.10451578542547375, 0.0897343290516693, 0.052609058803357624, 0.06936571987448796, 0.04147594107302785, 0.060495108193566835], 'lossList': [0.0, -1.2241671252250672, 0.0, 3.7343501974642277, 0.0, 0.0, 0.0], 'rewardMean': 0.7213845678906852, 'totalEpisodes': 264, 'stepsPerEpisode': 1169, 'rewardPerEpisode': 1037.6738989483015, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=102.28
