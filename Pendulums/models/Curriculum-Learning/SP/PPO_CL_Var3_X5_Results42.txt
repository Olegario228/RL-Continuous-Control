#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 42
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.8551529825251889, 'errorList': [], 'lossList': [0.0, -1.4427573877573012, 0.0, 27.662298382520675, 0.0, 0.0, 0.0], 'rewardMean': 0.6664693788739935, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 654.7861167844826
'totalSteps': 3840, 'rewardStep': 0.8456804795276908, 'errorList': [], 'lossList': [0.0, -1.4596196669340133, 0.0, 42.837343108654025, 0.0, 0.0, 0.0], 'rewardMean': 0.7262064124252259, 'totalEpisodes': 12, 'stepsPerEpisode': 666, 'rewardPerEpisode': 517.7085721849688
'totalSteps': 5120, 'rewardStep': 0.9112557115686515, 'errorList': [], 'lossList': [0.0, -1.4612958920001984, 0.0, 26.257878584861757, 0.0, 0.0, 0.0], 'rewardMean': 0.7724687372110823, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 995.8033551474692
'totalSteps': 6400, 'rewardStep': 0.537555863004714, 'errorList': [], 'lossList': [0.0, -1.44618370950222, 0.0, 45.20426924347878, 0.0, 0.0, 0.0], 'rewardMean': 0.7254861623698087, 'totalEpisodes': 15, 'stepsPerEpisode': 185, 'rewardPerEpisode': 145.295869392185
'totalSteps': 7680, 'rewardStep': 0.6177250761573888, 'errorList': [], 'lossList': [0.0, -1.434653012752533, 0.0, 423.8602547454834, 0.0, 0.0, 0.0], 'rewardMean': 0.7075259813344054, 'totalEpisodes': 79, 'stepsPerEpisode': 57, 'rewardPerEpisode': 41.64065555323925
'totalSteps': 8960, 'rewardStep': 0.7427181067720174, 'errorList': [], 'lossList': [0.0, -1.4343509942293167, 0.0, 256.31252487182616, 0.0, 0.0, 0.0], 'rewardMean': 0.7125534278254929, 'totalEpisodes': 139, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.849304572248883
'totalSteps': 10240, 'rewardStep': 0.901162605112083, 'errorList': [], 'lossList': [0.0, -1.433567967414856, 0.0, 133.99913383483886, 0.0, 0.0, 0.0], 'rewardMean': 0.7361295749863166, 'totalEpisodes': 187, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.026678783834246
'totalSteps': 11520, 'rewardStep': 0.5509323427069811, 'errorList': [], 'lossList': [0.0, -1.4252442193031312, 0.0, 87.70069982528686, 0.0, 0.0, 0.0], 'rewardMean': 0.7155521047330571, 'totalEpisodes': 221, 'stepsPerEpisode': 9, 'rewardPerEpisode': 4.766345970860582
'totalSteps': 12800, 'rewardStep': 0.14956951077278796, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6261322188920293, 'totalEpisodes': 243, 'stepsPerEpisode': 103, 'rewardPerEpisode': 65.56109292464114
'totalSteps': 14080, 'rewardStep': 0.6680217097555815, 'errorList': [], 'lossList': [0.0, -1.4267710775136948, 0.0, 62.69189762115479, 0.0, 0.0, 0.0], 'rewardMean': 0.6074190916150685, 'totalEpisodes': 260, 'stepsPerEpisode': 28, 'rewardPerEpisode': 22.768070318093926
'totalSteps': 15360, 'rewardStep': 0.6053356313987454, 'errorList': [], 'lossList': [0.0, -1.430787529349327, 0.0, 67.16282971382141, 0.0, 0.0, 0.0], 'rewardMean': 0.5833846068021739, 'totalEpisodes': 273, 'stepsPerEpisode': 39, 'rewardPerEpisode': 28.49993040330262
'totalSteps': 16640, 'rewardStep': 0.6145670623213716, 'errorList': [], 'lossList': [0.0, -1.4185437506437302, 0.0, 66.44042440414428, 0.0, 0.0, 0.0], 'rewardMean': 0.5537157418774459, 'totalEpisodes': 281, 'stepsPerEpisode': 194, 'rewardPerEpisode': 158.0722189730808
'totalSteps': 17920, 'rewardStep': 0.8120884995477448, 'errorList': [], 'lossList': [0.0, -1.406945645213127, 0.0, 72.70059169769287, 0.0, 0.0, 0.0], 'rewardMean': 0.5811690055317489, 'totalEpisodes': 289, 'stepsPerEpisode': 89, 'rewardPerEpisode': 79.41949447501986
'totalSteps': 19200, 'rewardStep': 0.9434857268705709, 'errorList': [109.2473796837728, 107.05762292699302, 99.97363690276896, 91.8812696003742, 128.6626581818201, 116.82938245744809, 103.69135111203681, 143.74318935881723, 15.947923783767116, 134.96561156996012, 108.34285607751882, 123.7695429402471, 94.41387754871357, 64.68143765241568, 83.20469922044882, 50.55910974544986, 100.58328973050061, 3.2532398236761724, 138.38074585436252, 28.074555455239427, 42.10875573165806, 122.46709513053767, 95.16790506125415, 150.00197462159306, 13.592582959893067, 77.54099931060153, 75.98093882298672, 21.260115461883096, 110.40904815009496, 10.140380498284836, 124.72690926768094, 155.63658755896424, 152.87554344634586, 128.90292320925445, 128.9315319953538, 6.054966375136399, 47.95289206771084, 83.34163539038832, 110.97560924499506, 87.46256528671472, 168.6161317445026, 138.4637095586722, 72.61470418489792, 21.454843802674386, 183.61785055005132, 87.12473865010563, 8.412708043379615, 152.77226993105876, 101.51466384945606, 93.08670428920107], 'lossList': [0.0, -1.3894202142953873, 0.0, 66.8420884513855, 0.0, 0.0, 0.0], 'rewardMean': 0.613745070603067, 'totalEpisodes': 297, 'stepsPerEpisode': 36, 'rewardPerEpisode': 33.64418634626627, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.7559814859926479, 'errorList': [], 'lossList': [0.0, -1.3999390304088593, 0.0, 87.49490249633789, 0.0, 0.0, 0.0], 'rewardMean': 0.6150714085251302, 'totalEpisodes': 303, 'stepsPerEpisode': 93, 'rewardPerEpisode': 73.9386302239724
'totalSteps': 21760, 'rewardStep': 0.7856347182483678, 'errorList': [], 'lossList': [0.0, -1.414455330967903, 0.0, 110.45517841339111, 0.0, 0.0, 0.0], 'rewardMean': 0.6035186198387587, 'totalEpisodes': 312, 'stepsPerEpisode': 34, 'rewardPerEpisode': 22.244561895310408
'totalSteps': 23040, 'rewardStep': 0.4676124323756556, 'errorList': [], 'lossList': [0.0, -1.420919150710106, 0.0, 39.12686044692993, 0.0, 0.0, 0.0], 'rewardMean': 0.5951866288056261, 'totalEpisodes': 320, 'stepsPerEpisode': 76, 'rewardPerEpisode': 50.75120509319569
'totalSteps': 24320, 'rewardStep': 0.814454135896783, 'errorList': [], 'lossList': [0.0, -1.4286327105760575, 0.0, 12.104842047691346, 0.0, 0.0, 0.0], 'rewardMean': 0.6616750913180256, 'totalEpisodes': 327, 'stepsPerEpisode': 139, 'rewardPerEpisode': 115.31865282704135
'totalSteps': 25600, 'rewardStep': 0.9041815963954407, 'errorList': [], 'lossList': [0.0, -1.4263532119989395, 0.0, 19.53745486497879, 0.0, 0.0, 0.0], 'rewardMean': 0.7371362998802911, 'totalEpisodes': 333, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.86400603729196
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=77.14
