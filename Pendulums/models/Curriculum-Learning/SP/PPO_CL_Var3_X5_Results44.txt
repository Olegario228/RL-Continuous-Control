#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 44
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8807195371004323, 'errorList': [], 'lossList': [0.0, -1.4239089912176133, 0.0, 34.10413732111454, 0.0, 0.0, 0.0], 'rewardMean': 0.8020352304898666, 'totalEpisodes': 13, 'stepsPerEpisode': 311, 'rewardPerEpisode': 259.5034930979563
'totalSteps': 3840, 'rewardStep': 0.7143599561237499, 'errorList': [], 'lossList': [0.0, -1.4260143089294433, 0.0, 33.349375021457675, 0.0, 0.0, 0.0], 'rewardMean': 0.7728101390344944, 'totalEpisodes': 16, 'stepsPerEpisode': 175, 'rewardPerEpisode': 131.6069666603463
'totalSteps': 5120, 'rewardStep': 0.6851255325707349, 'errorList': [], 'lossList': [0.0, -1.430346678495407, 0.0, 18.819806419610977, 0.0, 0.0, 0.0], 'rewardMean': 0.7508889874185545, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.5271608981537
'totalSteps': 6400, 'rewardStep': 0.9759533153382494, 'errorList': [69.19612399249121, 75.16813592182744, 73.60273109754144, 75.71984250579045, 67.00820056411949, 78.26517962459376, 75.57684770512783, 76.51327219796526, 73.67782910823765, 69.32701372928486, 76.75294327224425, 75.26451498872729, 76.96777115006473, 73.53109035391898, 71.46862503850078, 77.03590164975138, 75.93653971251821, 71.7634133647484, 76.13297560480076, 71.98503166560248, 70.18005314482167, 74.37065854630615, 74.71216876539755, 62.95263279119861, 69.17206613664631, 75.9879827593628, 75.66222161606612, 78.30502612228543, 74.09697847091134, 75.3463838498653, 74.11745760361065, 75.96326100988603, 74.26236854683, 74.12786637886902, 73.8894949784682, 76.72395282062575, 74.48938887656986, 76.64818192970938, 74.16859318835508, 70.484207068929, 73.22027777360253, 72.34790018005054, 66.37249072521759, 76.55612142310838, 73.92678073714164, 66.02199554512121, 76.89679805849852, 65.27767047774613, 73.03790206633381, 74.81296618147466], 'lossList': [0.0, -1.408995658159256, 0.0, 34.920639383792874, 0.0, 0.0, 0.0], 'rewardMean': 0.7959018530024935, 'totalEpisodes': 19, 'stepsPerEpisode': 35, 'rewardPerEpisode': 30.885939248259966, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.7219388808419936, 'errorList': [], 'lossList': [0.0, -1.4135401892662047, 0.0, 348.05454139709474, 0.0, 0.0, 0.0], 'rewardMean': 0.7835746909757435, 'totalEpisodes': 75, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.111406349284638
'totalSteps': 8960, 'rewardStep': 0.7800826833451723, 'errorList': [], 'lossList': [0.0, -1.4111630553007126, 0.0, 133.178226852417, 0.0, 0.0, 0.0], 'rewardMean': 0.7830758327428048, 'totalEpisodes': 146, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.724798616412672
'totalSteps': 10240, 'rewardStep': 0.9033872504371598, 'errorList': [], 'lossList': [0.0, -1.4020625191926956, 0.0, 75.39265195846558, 0.0, 0.0, 0.0], 'rewardMean': 0.7981147599545991, 'totalEpisodes': 186, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.774508750986987
'totalSteps': 11520, 'rewardStep': 0.9238735146001148, 'errorList': [], 'lossList': [0.0, -1.3854181134700776, 0.0, 52.11236385345459, 0.0, 0.0, 0.0], 'rewardMean': 0.812087954915212, 'totalEpisodes': 208, 'stepsPerEpisode': 47, 'rewardPerEpisode': 40.13439660555649
'totalSteps': 12800, 'rewardStep': 0.8314860066295766, 'errorList': [], 'lossList': [0.0, -1.3894058299064636, 0.0, 41.29851173400879, 0.0, 0.0, 0.0], 'rewardMean': 0.8140277600866485, 'totalEpisodes': 223, 'stepsPerEpisode': 67, 'rewardPerEpisode': 58.83396094084108
'totalSteps': 14080, 'rewardStep': 0.713656360182185, 'errorList': [], 'lossList': [0.0, -1.396637625694275, 0.0, 27.62445969104767, 0.0, 0.0, 0.0], 'rewardMean': 0.813058303716937, 'totalEpisodes': 231, 'stepsPerEpisode': 65, 'rewardPerEpisode': 48.07370897840826
'totalSteps': 15360, 'rewardStep': 0.8527421328967489, 'errorList': [], 'lossList': [0.0, -1.3842402750253677, 0.0, 40.333761162757874, 0.0, 0.0, 0.0], 'rewardMean': 0.8102605632965686, 'totalEpisodes': 240, 'stepsPerEpisode': 155, 'rewardPerEpisode': 127.80565882670892
'totalSteps': 16640, 'rewardStep': 0.577449457159829, 'errorList': [], 'lossList': [0.0, -1.370666418671608, 0.0, 23.63343364238739, 0.0, 0.0, 0.0], 'rewardMean': 0.7965695134001763, 'totalEpisodes': 246, 'stepsPerEpisode': 221, 'rewardPerEpisode': 185.77249946164352
'totalSteps': 17920, 'rewardStep': 0.7360764176898376, 'errorList': [], 'lossList': [0.0, -1.3697399866580964, 0.0, 7.04321233689785, 0.0, 0.0, 0.0], 'rewardMean': 0.8016646019120867, 'totalEpisodes': 251, 'stepsPerEpisode': 162, 'rewardPerEpisode': 142.1275553162723
'totalSteps': 19200, 'rewardStep': 0.832182951748595, 'errorList': [], 'lossList': [0.0, -1.3648962581157684, 0.0, 6.559733847379684, 0.0, 0.0, 0.0], 'rewardMean': 0.7872875655531213, 'totalEpisodes': 254, 'stepsPerEpisode': 140, 'rewardPerEpisode': 120.00533169054683
'totalSteps': 20480, 'rewardStep': 0.39159607906738736, 'errorList': [], 'lossList': [0.0, -1.3444124048948287, 0.0, 4.166214294433594, 0.0, 0.0, 0.0], 'rewardMean': 0.7542532853756605, 'totalEpisodes': 258, 'stepsPerEpisode': 268, 'rewardPerEpisode': 204.027712751704
'totalSteps': 21760, 'rewardStep': 0.7198621772074462, 'errorList': [], 'lossList': [0.0, -1.3211217880249024, 0.0, 3.474331256747246, 0.0, 0.0, 0.0], 'rewardMean': 0.748231234761888, 'totalEpisodes': 263, 'stepsPerEpisode': 82, 'rewardPerEpisode': 70.62055958309467
'totalSteps': 23040, 'rewardStep': 0.7922103491118262, 'errorList': [], 'lossList': [0.0, -1.299671403169632, 0.0, 4.353378045558929, 0.0, 0.0, 0.0], 'rewardMean': 0.7371135446293546, 'totalEpisodes': 268, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.819458308694887
'totalSteps': 24320, 'rewardStep': 0.7381938273623626, 'errorList': [], 'lossList': [0.0, -1.2903868985176086, 0.0, 2.932452504634857, 0.0, 0.0, 0.0], 'rewardMean': 0.7185455759055795, 'totalEpisodes': 271, 'stepsPerEpisode': 199, 'rewardPerEpisode': 175.15472691120604
'totalSteps': 25600, 'rewardStep': 0.9860750728359051, 'errorList': [0.26008543323431715, 0.07270236956163303, 0.2242284510714573, 0.659697868271482, 0.22305435705702423, 0.27192200484176193, 0.1945869397366943, 0.15222399150566607, 0.19268165254409456, 0.2091282241967076, 0.07367310291218505, 0.0345927980028144, 0.35190098660174046, 0.09693462975002873, 0.07415575226818091, 0.16648672039858659, 0.03356739072065578, 0.27781556142631697, 0.17070271892335823, 0.8184004977389512, 0.46024877892018906, 0.3087401714337369, 0.12148195882958525, 0.48640789964110953, 0.13435770520957255, 0.10240159148328672, 0.10881424240486146, 0.36078214730966357, 0.26593971579128656, 0.170291398954017, 0.2087333707210981, 0.566077405409197, 0.24768840774837114, 0.3233163824174915, 0.6151886315109228, 0.2847217488894545, 0.13489331951132483, 0.17724158821782776, 0.04842426391984483, 0.25201738511516497, 0.1968903735370564, 0.07227691652935962, 0.4312307219279807, 0.3186630247150876, 0.20252776039457732, 0.14378084023708626, 0.05878382835510651, 0.23713741781555664, 0.17539290188577292, 0.1943364068186196], 'lossList': [0.0, -1.2778273332118988, 0.0, 2.3764209115505217, 0.0, 0.0, 0.0], 'rewardMean': 0.7340044825262122, 'totalEpisodes': 273, 'stepsPerEpisode': 507, 'rewardPerEpisode': 473.09950190325014, 'successfulTests': 25
#maxSuccessfulTests=25, maxSuccessfulTestsAtStep=25600, timeSpent=101.61
