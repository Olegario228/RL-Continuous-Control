#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 45
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9119380234457088, 'errorList': [], 'lossList': [0.0, -1.4368713772296906, 0.0, 31.96123038649559, 0.0, 0.0, 0.0], 'rewardMean': 0.9133793270132684, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 383.4636147600976
'totalSteps': 3840, 'rewardStep': 0.718659505027809, 'errorList': [], 'lossList': [0.0, -1.4211602663993836, 0.0, 31.048676054477692, 0.0, 0.0, 0.0], 'rewardMean': 0.848472719684782, 'totalEpisodes': 12, 'stepsPerEpisode': 268, 'rewardPerEpisode': 210.98109144726266
'totalSteps': 5120, 'rewardStep': 0.7304239199701874, 'errorList': [], 'lossList': [0.0, -1.418608603477478, 0.0, 25.589471811056136, 0.0, 0.0, 0.0], 'rewardMean': 0.8189605197561334, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 990.6983596326329
'totalSteps': 6400, 'rewardStep': 0.8814472234325175, 'errorList': [], 'lossList': [0.0, -1.4101038336753846, 0.0, 18.501615973711015, 0.0, 0.0, 0.0], 'rewardMean': 0.8314578604914102, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.2925751659608
'totalSteps': 7680, 'rewardStep': 0.9647437166675333, 'errorList': [56.03131138211709, 52.58294519461354, 55.77751831682365, 52.408570146961424, 50.613113850498095, 56.37954874813076, 51.061222262770926, 54.08545452412456, 54.47910593361355, 52.85060338113766, 54.39539499001224, 54.68704513132215, 54.608982786727594, 53.117191520723054, 51.392752512070665, 54.6814138729862, 56.085107600311105, 55.027765501962826, 52.02742137737096, 54.28989765974153, 55.94874563869777, 51.98215720645522, 55.17092590191343, 51.98732016247879, 50.547212468140444, 55.02904424574148, 51.09372454546403, 55.20468400571484, 54.45257473801375, 51.4112095270573, 55.10797419719329, 53.60510984653278, 54.62692582976114, 54.43262926650017, 50.102698923607505, 49.37452357075899, 51.43009348953192, 55.70583743781919, 55.70434377454793, 49.553291594655356, 52.68746450087914, 53.555070426361866, 55.77961402992334, 54.89530011313682, 50.74734618552811, 55.85852368710172, 48.19561037275225, 50.026517747676806, 52.441112218608836, 56.21833918780979], 'lossList': [0.0, -1.3983613657951355, 0.0, 487.3755693054199, 0.0, 0.0, 0.0], 'rewardMean': 0.8536721698540974, 'totalEpisodes': 81, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.7104040031486, 'successfulTests': 0
'totalSteps': 8960, 'rewardStep': 0.7002287831409175, 'errorList': [], 'lossList': [0.0, -1.3993352115154267, 0.0, 239.74786586761473, 0.0, 0.0, 0.0], 'rewardMean': 0.8317516860379289, 'totalEpisodes': 144, 'stepsPerEpisode': 25, 'rewardPerEpisode': 15.284003419957958
'totalSteps': 10240, 'rewardStep': 0.6611837796188729, 'errorList': [], 'lossList': [0.0, -1.390027244091034, 0.0, 69.51340686798096, 0.0, 0.0, 0.0], 'rewardMean': 0.8104306977355468, 'totalEpisodes': 215, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.124879396328723
'totalSteps': 11520, 'rewardStep': 0.6792656244877854, 'errorList': [], 'lossList': [0.0, -1.3657128262519835, 0.0, 44.2663751411438, 0.0, 0.0, 0.0], 'rewardMean': 0.7958568007080177, 'totalEpisodes': 270, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.579593648084579
'totalSteps': 12800, 'rewardStep': 0.42707101503208394, 'errorList': [], 'lossList': [0.0, -1.3517674499750136, 0.0, 40.15822165489197, 0.0, 0.0, 0.0], 'rewardMean': 0.7589782221404243, 'totalEpisodes': 301, 'stepsPerEpisode': 71, 'rewardPerEpisode': 53.693396105943194
'totalSteps': 14080, 'rewardStep': 0.36811222022440915, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6499248007826525, 'totalEpisodes': 313, 'stepsPerEpisode': 64, 'rewardPerEpisode': 52.04325283839675
'totalSteps': 15360, 'rewardStep': 0.6124662449123048, 'errorList': [], 'lossList': [0.0, -1.3424531638622283, 0.0, 37.804376134872435, 0.0, 0.0, 0.0], 'rewardMean': 0.6393054747711021, 'totalEpisodes': 330, 'stepsPerEpisode': 72, 'rewardPerEpisode': 52.81620643241996
'totalSteps': 16640, 'rewardStep': 0.9011953286408249, 'errorList': [], 'lossList': [0.0, -1.3368265634775163, 0.0, 16.319894149303437, 0.0, 0.0, 0.0], 'rewardMean': 0.6563826156381659, 'totalEpisodes': 337, 'stepsPerEpisode': 61, 'rewardPerEpisode': 53.5153255065001
'totalSteps': 17920, 'rewardStep': 0.9133661568754029, 'errorList': [], 'lossList': [0.0, -1.3324554032087326, 0.0, 22.07939474105835, 0.0, 0.0, 0.0], 'rewardMean': 0.6595745089824544, 'totalEpisodes': 343, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.79882371497944
'totalSteps': 19200, 'rewardStep': 0.7700976018857967, 'errorList': [], 'lossList': [0.0, -1.3190605175495147, 0.0, 30.701165390014648, 0.0, 0.0, 0.0], 'rewardMean': 0.6401098975042807, 'totalEpisodes': 347, 'stepsPerEpisode': 123, 'rewardPerEpisode': 94.40415572630815
'totalSteps': 20480, 'rewardStep': 0.6347183760623818, 'errorList': [], 'lossList': [0.0, -1.313746408224106, 0.0, 11.51332139968872, 0.0, 0.0, 0.0], 'rewardMean': 0.6335588567964272, 'totalEpisodes': 349, 'stepsPerEpisode': 262, 'rewardPerEpisode': 186.84064202161386
'totalSteps': 21760, 'rewardStep': 0.9487559070697731, 'errorList': [0.42817286397447, 0.25802784387731886, 0.40353167524248823, 0.527763732282681, 0.5354074682997925, 0.3825991419141926, 0.42234321567450256, 0.5310486670248891, 0.3480829165456312, 0.366025455855506, 0.453635908167907, 0.6786188108905064, 0.2719122395945969, 0.1405701612051513, 0.7112495275921744, 0.3927657433578189, 0.5896876423061023, 0.5574158878184082, 0.9956910290529251, 0.3456233841277313, 0.4891245481558428, 0.32825317901030854, 0.6073500226191538, 0.8497003540051989, 0.786767926798777, 0.7598381231580679, 0.39229127754498877, 0.38630743359388564, 0.7944010621651775, 0.18743480202286086, 0.5747899650968887, 0.6740424619536199, 0.5513965943478981, 0.5902095333680042, 0.7252573795659398, 0.4712248694135488, 0.48167167249116116, 0.26133423249944693, 0.41517385704857246, 0.5547688588970034, 0.9124572598200601, 0.4579969712551934, 0.5782696539575992, 0.36817052716533116, 0.29328431748161976, 0.5331136116504034, 0.6855346545512613, 0.5967135260250721, 0.27985470477506513, 0.2568229204422398], 'lossList': [0.0, -1.3081996411085128, 0.0, 10.275383192300797, 0.0, 0.0, 0.0], 'rewardMean': 0.6623160695415172, 'totalEpisodes': 352, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.8728680633146, 'successfulTests': 2
'totalSteps': 23040, 'rewardStep': 0.5714318248811896, 'errorList': [], 'lossList': [0.0, -1.2869712072610855, 0.0, 5.86494016110897, 0.0, 0.0, 0.0], 'rewardMean': 0.6515326895808576, 'totalEpisodes': 353, 'stepsPerEpisode': 289, 'rewardPerEpisode': 237.01979246122946
'totalSteps': 24320, 'rewardStep': 0.8383741409716492, 'errorList': [], 'lossList': [0.0, -1.2659987390041352, 0.0, 4.652261873185634, 0.0, 0.0, 0.0], 'rewardMean': 0.6926630021748141, 'totalEpisodes': 353, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.07788257443
'totalSteps': 25600, 'rewardStep': 0.8368593103910597, 'errorList': [], 'lossList': [0.0, -1.2420480668544769, 0.0, 2.838558053597808, 0.0, 0.0, 0.0], 'rewardMean': 0.7395377111914793, 'totalEpisodes': 353, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.753194038224
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=21760, timeSpent=100.26
