#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 6000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 49
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 6000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8202493734178695, 'errorList': [], 'lossList': [0.0, -1.4210896760225296, 0.0, 28.43808599829674, 0.0, 0.0, 0.0], 'rewardMean': 0.7847701188585753, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9095312527938
'totalSteps': 3840, 'rewardStep': 0.7647119203143069, 'errorList': [], 'lossList': [0.0, -1.4348128980398178, 0.0, 35.07950420379639, 0.0, 0.0, 0.0], 'rewardMean': 0.7780840526771525, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.23863092437855
'totalSteps': 5120, 'rewardStep': 0.6898792836541098, 'errorList': [], 'lossList': [0.0, -1.4308497196435928, 0.0, 24.816191177368164, 0.0, 0.0, 0.0], 'rewardMean': 0.7560328604213918, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.1004899816797
'totalSteps': 6400, 'rewardStep': 0.890353000024775, 'errorList': [], 'lossList': [0.0, -1.4268066376447677, 0.0, 13.257289025783539, 0.0, 0.0, 0.0], 'rewardMean': 0.7828968883420685, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 937.1147575291294
'totalSteps': 7680, 'rewardStep': 0.880302831690248, 'errorList': [], 'lossList': [0.0, -1.4273316603899002, 0.0, 478.5963087463379, 0.0, 0.0, 0.0], 'rewardMean': 0.7991312122334318, 'totalEpisodes': 83, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.436753722854187
'totalSteps': 8960, 'rewardStep': 0.9405551243288589, 'errorList': [71.73142078920931, 80.93572095850695, 81.20293526065069, 83.07510226045913, 83.68736810105896, 80.8956689019923, 82.95385809866994, 82.16017790140381, 81.14487851774221, 81.49356140878884, 75.81558783335416, 83.63545039783482, 80.93621730044015, 83.02649807852877, 83.82914691702294, 84.06716144624123, 73.88629026894267, 79.60850914699022, 80.25537795859438, 71.8566974508565, 82.18934999022136, 78.07502581611259, 83.6590608689012, 83.01324484714362, 82.88409830196437, 83.16892897800973, 67.28934826677393, 82.31265814582797, 83.39768984169145, 71.55374670268706, 79.66607316448456, 81.0787533441851, 83.86201809042531, 80.29122080721532, 75.0047126750671, 83.06432061533955, 74.09852075427465, 78.41137039654339, 82.55110396566108, 81.81923789257122, 81.55979530827278, 84.19029595103285, 75.55740205855236, 63.15944329473503, 81.97362326740998, 81.65599202453198, 81.53832728398372, 68.27310968636174, 84.13947506537527, 84.67822699229859], 'lossList': [0.0, -1.4269331836700438, 0.0, 295.7328704833984, 0.0, 0.0, 0.0], 'rewardMean': 0.8193346282470643, 'totalEpisodes': 152, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.285499583285529, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.6451267654600507, 'errorList': [], 'lossList': [0.0, -1.42513541162014, 0.0, 144.24464641571046, 0.0, 0.0, 0.0], 'rewardMean': 0.7975586453986875, 'totalEpisodes': 214, 'stepsPerEpisode': 15, 'rewardPerEpisode': 10.624164538811051
'totalSteps': 11520, 'rewardStep': 0.8130257609324141, 'errorList': [], 'lossList': [0.0, -1.422962896823883, 0.0, 64.11955083847046, 0.0, 0.0, 0.0], 'rewardMean': 0.7992772137913238, 'totalEpisodes': 268, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8130257609324141
'totalSteps': 12800, 'rewardStep': 0.47029989254555005, 'errorList': [], 'lossList': [0.0, -1.4150057619810104, 0.0, 48.72569103240967, 0.0, 0.0, 0.0], 'rewardMean': 0.7663794816667464, 'totalEpisodes': 309, 'stepsPerEpisode': 57, 'rewardPerEpisode': 37.64949069539919
'totalSteps': 14080, 'rewardStep': 0.8422027626416317, 'errorList': [], 'lossList': [0.0, -1.398569787144661, 0.0, 51.740026254653934, 0.0, 0.0, 0.0], 'rewardMean': 0.7756706715009816, 'totalEpisodes': 325, 'stepsPerEpisode': 42, 'rewardPerEpisode': 32.303147461148896
'totalSteps': 15360, 'rewardStep': 0.9831276278658545, 'errorList': [12.98868256336695, 22.418618840472597, 39.154780887281916, 33.717284804711106, 10.934250196685918, 9.22120150942363, 1.3990554280214968, 6.57144402236425, 32.34857946528389, 31.1477035043454, 24.077737448025317, 8.427912681868829, 20.613293349844874, 29.453205573988367, 26.8467026188562, 18.00017200582028, 10.43653375628244, 45.63233182081828, 24.970148489623984, 15.781954363886761, 21.89046102061692, 15.894681847098663, 8.262782579179612, 3.443624292570176, 44.44879851944193, 13.10654867465144, 27.67217356458359, 41.99080991793695, 33.17902623498497, 37.47388766010058, 13.582253992686585, 5.5368612212281585, 2.512511536997485, 2.6868744839247345, 11.395392800423076, 16.287230557654368, 7.09277451949928, 27.070703223561868, 24.164110762610292, 28.91969205570008, 49.434172914694976, 8.186326717876524, 38.280806744053436, 24.488071742587664, 13.401072986282394, 24.595542111688406, 9.827440137164597, 13.733992527722307, 6.658632835707118, 19.630485466278895], 'lossList': [0.0, -1.3872767794132232, 0.0, 53.25260226249695, 0.0, 0.0, 0.0], 'rewardMean': 0.79195849694578, 'totalEpisodes': 337, 'stepsPerEpisode': 19, 'rewardPerEpisode': 16.160312894501708, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7697399129005329, 'errorList': [], 'lossList': [0.0, -1.382280160188675, 0.0, 63.357930507659916, 0.0, 0.0, 0.0], 'rewardMean': 0.7924612962044026, 'totalEpisodes': 348, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.6974939348342266
'totalSteps': 17920, 'rewardStep': 0.7605421950982376, 'errorList': [], 'lossList': [0.0, -1.3848827826976775, 0.0, 42.5571910905838, 0.0, 0.0, 0.0], 'rewardMean': 0.7995275873488155, 'totalEpisodes': 352, 'stepsPerEpisode': 349, 'rewardPerEpisode': 294.99538108005845
'totalSteps': 19200, 'rewardStep': 0.7100189739063822, 'errorList': [], 'lossList': [0.0, -1.3868555450439453, 0.0, 24.157930544614793, 0.0, 0.0, 0.0], 'rewardMean': 0.7814941847369761, 'totalEpisodes': 354, 'stepsPerEpisode': 769, 'rewardPerEpisode': 577.6472090200849
'totalSteps': 20480, 'rewardStep': 0.9091743114916884, 'errorList': [], 'lossList': [0.0, -1.3834473007917405, 0.0, 17.809245417118074, 0.0, 0.0, 0.0], 'rewardMean': 0.7843813327171201, 'totalEpisodes': 360, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.556704269401376
'totalSteps': 21760, 'rewardStep': 0.6155357638527437, 'errorList': [], 'lossList': [0.0, -1.3689726036787033, 0.0, 20.390993655920028, 0.0, 0.0, 0.0], 'rewardMean': 0.7518793966695085, 'totalEpisodes': 366, 'stepsPerEpisode': 151, 'rewardPerEpisode': 113.86315109310094
'totalSteps': 23040, 'rewardStep': 0.7765357098726728, 'errorList': [], 'lossList': [0.0, -1.3639258044958114, 0.0, 24.734921516180037, 0.0, 0.0, 0.0], 'rewardMean': 0.7650202911107707, 'totalEpisodes': 370, 'stepsPerEpisode': 216, 'rewardPerEpisode': 179.28047362467757
'totalSteps': 24320, 'rewardStep': 0.4284463645878799, 'errorList': [], 'lossList': [0.0, -1.35848887860775, 0.0, 3.640823232829571, 0.0, 0.0, 0.0], 'rewardMean': 0.7265623514763174, 'totalEpisodes': 372, 'stepsPerEpisode': 882, 'rewardPerEpisode': 678.199530120809
'totalSteps': 25600, 'rewardStep': 0.7809484219396788, 'errorList': [], 'lossList': [0.0, -1.3607137942314147, 0.0, 3.6050799345970153, 0.0, 0.0, 0.0], 'rewardMean': 0.7576272044157303, 'totalEpisodes': 376, 'stepsPerEpisode': 167, 'rewardPerEpisode': 138.34676352665505
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=97.97
