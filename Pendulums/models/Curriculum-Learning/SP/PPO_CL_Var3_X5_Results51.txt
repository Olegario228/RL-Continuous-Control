#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 51
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980674424293452, 'errorList': [], 'lossList': [0.0, -1.4174977385997771, 0.0, 30.410095658302307, 0.0, 0.0, 0.0], 'rewardMean': 0.7065587876353763, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.34315141124151
'totalSteps': 3840, 'rewardStep': 0.7323913712223517, 'errorList': [], 'lossList': [0.0, -1.4045672231912614, 0.0, 37.52959886074066, 0.0, 0.0, 0.0], 'rewardMean': 0.7151696488310347, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.23693321291275
'totalSteps': 5120, 'rewardStep': 0.825949409741123, 'errorList': [], 'lossList': [0.0, -1.3996495920419694, 0.0, 25.824950580596923, 0.0, 0.0, 0.0], 'rewardMean': 0.7428645890585568, 'totalEpisodes': 75, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.23968415607695
'totalSteps': 6400, 'rewardStep': 0.4461778885252567, 'errorList': [], 'lossList': [0.0, -1.386889597773552, 0.0, 34.52748581647873, 0.0, 0.0, 0.0], 'rewardMean': 0.6835272489518968, 'totalEpisodes': 80, 'stepsPerEpisode': 83, 'rewardPerEpisode': 60.9426213348357
'totalSteps': 7680, 'rewardStep': 0.5714602674926772, 'errorList': [], 'lossList': [0.0, -1.3618111848831176, 0.0, 13.115429484844208, 0.0, 0.0, 0.0], 'rewardMean': 0.6648494187086934, 'totalEpisodes': 81, 'stepsPerEpisode': 959, 'rewardPerEpisode': 731.748998877067
'totalSteps': 8960, 'rewardStep': 0.5100863664990356, 'errorList': [], 'lossList': [0.0, -1.351605668067932, 0.0, 107.91433000564575, 0.0, 0.0, 0.0], 'rewardMean': 0.6427404112501709, 'totalEpisodes': 100, 'stepsPerEpisode': 116, 'rewardPerEpisode': 96.22182967288448
'totalSteps': 10240, 'rewardStep': 0.942368357999256, 'errorList': [0.2578093844557184, 4.9037131776434135, 2.5466117860290245, 6.589250812952853, 3.926149099557176, 1.9964363190843275, 4.250119649135703, 3.6106514247123944, 3.3492399746316277, 5.365669668973225, 0.11638031648816906, 4.0751260217570255, 5.394927655742503, 3.168245946301129, 1.1474656139110129, 5.758491237008527, 6.26917063967751, 5.226483430703227, 4.691781673320626, 2.4887015937649974, 5.915275006536376, 6.220921590463726, 4.519705686831596, 6.314001495673848, 3.0987849272064065, 6.283814953950021, 5.316031956612601, 2.703016860645879, 5.322006127711997, 6.306583158754175, 1.2441969067777037, 5.22459259897515, 2.4740229967696794, 1.3010109645713024, 7.047369803809097, 1.2466172150357007, 4.357301361406271, 5.256538971894954, 1.611588936948038, 3.265252325606352, 1.1631308443911426, 6.307360112091658, 1.346784441980094, 3.8267072363076218, 2.5716349957388807, 3.0447826537092855, 0.5365924119390613, 5.0284723872723, 1.6740032771514883, 1.7860618556727288], 'lossList': [0.0, -1.3495127004384995, 0.0, 37.11963973045349, 0.0, 0.0, 0.0], 'rewardMean': 0.6801939045938066, 'totalEpisodes': 112, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.048309715949008, 'successfulTests': 1
'totalSteps': 11520, 'rewardStep': 0.8206379415870663, 'errorList': [], 'lossList': [0.0, -1.343684377670288, 0.0, 37.76760769844055, 0.0, 0.0, 0.0], 'rewardMean': 0.6957987975930577, 'totalEpisodes': 120, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.490239405554604
'totalSteps': 12800, 'rewardStep': 0.7523605098146183, 'errorList': [], 'lossList': [0.0, -1.3181110680103303, 0.0, 15.507263617515564, 0.0, 0.0, 0.0], 'rewardMean': 0.7014549688152137, 'totalEpisodes': 123, 'stepsPerEpisode': 104, 'rewardPerEpisode': 88.13470354152172
'totalSteps': 14080, 'rewardStep': 0.47482217131453625, 'errorList': [], 'lossList': [0.0, -1.2889341044425964, 0.0, 5.980698541402817, 0.0, 0.0, 0.0], 'rewardMean': 0.6674321726625266, 'totalEpisodes': 125, 'stepsPerEpisode': 505, 'rewardPerEpisode': 389.3060864167279
'totalSteps': 15360, 'rewardStep': 0.795406152601412, 'errorList': [], 'lossList': [0.0, -1.2906852871179582, 0.0, 4.75633107572794, 0.0, 0.0, 0.0], 'rewardMean': 0.6871660436797333, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 792.0138103428769
'totalSteps': 16640, 'rewardStep': 0.6270530126857049, 'errorList': [], 'lossList': [0.0, -1.2847732776403427, 0.0, 8.502577142715454, 0.0, 0.0, 0.0], 'rewardMean': 0.6766322078260687, 'totalEpisodes': 128, 'stepsPerEpisode': 177, 'rewardPerEpisode': 130.2548104859794
'totalSteps': 17920, 'rewardStep': 0.7278355891624726, 'errorList': [], 'lossList': [0.0, -1.2642128014564513, 0.0, 2.417099288702011, 0.0, 0.0, 0.0], 'rewardMean': 0.6668208257682036, 'totalEpisodes': 128, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 751.4978712300841
'totalSteps': 19200, 'rewardStep': 0.715976328246577, 'errorList': [], 'lossList': [0.0, -1.2539762616157533, 0.0, 4.5088145154714585, 0.0, 0.0, 0.0], 'rewardMean': 0.6938006697403355, 'totalEpisodes': 130, 'stepsPerEpisode': 440, 'rewardPerEpisode': 340.82812147682336
'totalSteps': 20480, 'rewardStep': 0.8358453023175466, 'errorList': [], 'lossList': [0.0, -1.2174788719415666, 0.0, 2.6782962669432164, 0.0, 0.0, 0.0], 'rewardMean': 0.7202391732228225, 'totalEpisodes': 130, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.0802201961783
'totalSteps': 21760, 'rewardStep': 0.8594219338134625, 'errorList': [], 'lossList': [0.0, -1.156201914548874, 0.0, 1.8711802797019481, 0.0, 0.0, 0.0], 'rewardMean': 0.7551727299542652, 'totalEpisodes': 130, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.6408513357596
'totalSteps': 23040, 'rewardStep': 0.9574771577987614, 'errorList': [0.08163633264545804, 0.06262404903836051, 0.0399441688109136, 0.053871820568523514, 0.05823621082784362, 0.04867873426682172, 0.030801101434943166, 0.03287817055888468, 0.06244670672379689, 0.021566191815623956, 0.024888157634717373, 0.03133747676073587, 0.030796840340132737, 0.03453196035524913, 0.032152945461749946, 0.03753109037061229, 0.037328118870294406, 0.023519726487049353, 0.03410997302696105, 0.046008175181931836, 0.03949328497010099, 0.06726180383687497, 0.030656440574351906, 0.033000147025467635, 0.031261283092341746, 0.02906114382721574, 0.03403626153791658, 0.035781731330215207, 0.04450405760834519, 0.029778769880423564, 0.04614770416277772, 0.06054114104235958, 0.02616517428715354, 0.04932183958251014, 0.03971481419732793, 0.0509736027881038, 0.06020487607313015, 0.04280640412618969, 0.03206294584765263, 0.0773278464948632, 0.03749385648023511, 0.02628298073041281, 0.044669435413990086, 0.026276033480909684, 0.028178155016280633, 0.07018250872265855, 0.06505180281613657, 0.04674225974844099, 0.0913983995425494, 0.06491867267760434], 'lossList': [0.0, -1.1364482563734055, 0.0, 1.3508464609831572, 0.0, 0.0, 0.0], 'rewardMean': 0.7566836099342157, 'totalEpisodes': 130, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1126.3260798197798, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=93.59
