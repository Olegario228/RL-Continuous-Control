#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 52
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092811913766997, 'errorList': [], 'lossList': [0.0, -1.423109073638916, 0.0, 34.8601215839386, 0.0, 0.0, 0.0], 'rewardMean': 0.8624592665802047, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.7665607639914125
'totalSteps': 3840, 'rewardStep': 0.552967100109591, 'errorList': [], 'lossList': [0.0, -1.4139069372415543, 0.0, 33.43904374122619, 0.0, 0.0, 0.0], 'rewardMean': 0.7592952110900001, 'totalEpisodes': 82, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.954901758611587
'totalSteps': 5120, 'rewardStep': 0.5069349995299774, 'errorList': [], 'lossList': [0.0, -1.397662490606308, 0.0, 22.037939372062684, 0.0, 0.0, 0.0], 'rewardMean': 0.6962051581999944, 'totalEpisodes': 91, 'stepsPerEpisode': 208, 'rewardPerEpisode': 114.87746881858645
'totalSteps': 6400, 'rewardStep': 0.6185740328868041, 'errorList': [], 'lossList': [0.0, -1.3848203468322753, 0.0, 51.735634446144104, 0.0, 0.0, 0.0], 'rewardMean': 0.6806789331373564, 'totalEpisodes': 100, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.1979580533238123
'totalSteps': 7680, 'rewardStep': 0.9197130187674883, 'errorList': [], 'lossList': [0.0, -1.3659024715423584, 0.0, 74.63254796981812, 0.0, 0.0, 0.0], 'rewardMean': 0.7205179474090451, 'totalEpisodes': 112, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.98785934783136
'totalSteps': 8960, 'rewardStep': 0.7819016303222617, 'errorList': [], 'lossList': [0.0, -1.3615877658128739, 0.0, 131.09335060119628, 0.0, 0.0, 0.0], 'rewardMean': 0.729287044968076, 'totalEpisodes': 136, 'stepsPerEpisode': 17, 'rewardPerEpisode': 15.686381810898178
'totalSteps': 10240, 'rewardStep': 0.9400020251211194, 'errorList': [104.77437061271917, 147.78919356705333, 76.31942315614992, 64.87150116005526, 138.88599924461622, 90.42179828626172, 165.22744677103566, 130.62982321129013, 145.9743116979189, 152.35082970819147, 86.06995548584696, 103.92324171398084, 96.88967354438228, 150.7670001152284, 5.984842238287614, 72.63443408395133, 35.344982515106395, 147.59013113422841, 166.8432243672451, 125.62825596521526, 74.36173446059222, 25.503178582708827, 160.31877875112184, 189.64272100700705, 174.81266435707107, 133.8800458521329, 86.45001762850839, 115.76346616970774, 61.79671832678055, 81.58093008767754, 78.32169223698158, 24.52172457980712, 98.94408988449501, 57.558898885663524, 29.450378074005872, 8.850686404368863, 161.59573374748751, 13.269805384674926, 134.1029851899793, 9.340206842568806, 134.63740473537075, 174.89987592927685, 161.72914277504478, 156.5143079239581, 172.86799183425413, 95.00579359357535, 5.435774089822553, 127.06121226372984, 99.78962597642857, 131.60355440976818], 'lossList': [0.0, -1.3605175536870957, 0.0, 52.84892081260681, 0.0, 0.0, 0.0], 'rewardMean': 0.7556264174872064, 'totalEpisodes': 152, 'stepsPerEpisode': 20, 'rewardPerEpisode': 15.312145777263224, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.7470107208721892, 'errorList': [], 'lossList': [0.0, -1.3546173626184463, 0.0, 23.200309500694274, 0.0, 0.0, 0.0], 'rewardMean': 0.7546691178633156, 'totalEpisodes': 160, 'stepsPerEpisode': 66, 'rewardPerEpisode': 50.29909456542264
'totalSteps': 12800, 'rewardStep': 0.8343425914741411, 'errorList': [], 'lossList': [0.0, -1.3461484593153, 0.0, 26.839383511543275, 0.0, 0.0, 0.0], 'rewardMean': 0.7626364652243981, 'totalEpisodes': 167, 'stepsPerEpisode': 142, 'rewardPerEpisode': 111.98095326539931
'totalSteps': 14080, 'rewardStep': 0.7424350572960039, 'errorList': [], 'lossList': [0.0, -1.3489233714342117, 0.0, 10.211933209896088, 0.0, 0.0, 0.0], 'rewardMean': 0.7553162367756275, 'totalEpisodes': 172, 'stepsPerEpisode': 82, 'rewardPerEpisode': 61.459638032147176
'totalSteps': 15360, 'rewardStep': 0.5399070721436776, 'errorList': [], 'lossList': [0.0, -1.3598154550790786, 0.0, 19.574191522598266, 0.0, 0.0, 0.0], 'rewardMean': 0.7183788248523253, 'totalEpisodes': 178, 'stepsPerEpisode': 130, 'rewardPerEpisode': 79.24632155685038
'totalSteps': 16640, 'rewardStep': 0.9270477605649646, 'errorList': [], 'lossList': [0.0, -1.3602940189838408, 0.0, 5.8317389643192294, 0.0, 0.0, 0.0], 'rewardMean': 0.7557868908978628, 'totalEpisodes': 184, 'stepsPerEpisode': 16, 'rewardPerEpisode': 15.337414995844403
'totalSteps': 17920, 'rewardStep': 0.3922551241544559, 'errorList': [], 'lossList': [0.0, -1.3654244667291642, 0.0, 4.649972346425057, 0.0, 0.0, 0.0], 'rewardMean': 0.7443189033603106, 'totalEpisodes': 187, 'stepsPerEpisode': 240, 'rewardPerEpisode': 175.33017996849128
'totalSteps': 19200, 'rewardStep': 0.8282532051397511, 'errorList': [], 'lossList': [0.0, -1.363160810470581, 0.0, 5.406278504133224, 0.0, 0.0, 0.0], 'rewardMean': 0.7652868205856054, 'totalEpisodes': 192, 'stepsPerEpisode': 253, 'rewardPerEpisode': 217.07867549350013
'totalSteps': 20480, 'rewardStep': 0.7015321562961521, 'errorList': [], 'lossList': [0.0, -1.360882841348648, 0.0, 3.0666150833666324, 0.0, 0.0, 0.0], 'rewardMean': 0.7434687343384717, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.8178649188255
'totalSteps': 21760, 'rewardStep': 0.8105112463812585, 'errorList': [], 'lossList': [0.0, -1.3438391035795212, 0.0, 2.2748501277714968, 0.0, 0.0, 0.0], 'rewardMean': 0.7463296959443715, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.101910161233
'totalSteps': 23040, 'rewardStep': 0.9407705440662215, 'errorList': [0.10889956013989631, 0.05231820794912, 0.03198638412455031, 0.09853789464380111, 0.09239001656760394, 0.11404329663396542, 0.06279837300931772, 0.09141919531026373, 0.07365373001250712, 0.05094479438130807, 0.049005158821880064, 0.0407477245405251, 0.062189408746630453, 0.07246903220396526, 0.05582782853797113, 0.044358683357797184, 0.09602113194539003, 0.04599833491829569, 0.07949597647804188, 0.047521703414289594, 0.08014296916189037, 0.08264483204578793, 0.05516048566199478, 0.09805885532878604, 0.053912446124121036, 0.07393313462625692, 0.05349513912152243, 0.05313953660811406, 0.062120569699655986, 0.08746464828793424, 0.0658917814162174, 0.05733393242223066, 0.06669663533233494, 0.05072599014985501, 0.05754351273134158, 0.06800414981636689, 0.09663180433344638, 0.07811988326559205, 0.0569375516260434, 0.0905746972449971, 0.05115306759331192, 0.0690321102715077, 0.06949289012478511, 0.050179817489903696, 0.0628635697194515, 0.08761782461509801, 0.04141217449996089, 0.05063090366303897, 0.032854507435719965, 0.06959885807394958], 'lossList': [0.0, -1.298100539445877, 0.0, 1.9368390857428313, 0.0, 0.0, 0.0], 'rewardMean': 0.7464065478388816, 'totalEpisodes': 192, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.386285891757, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=94.13
