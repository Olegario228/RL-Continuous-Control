#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 53
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.4358093683145122, 'errorList': [], 'lossList': [0.0, -1.43411694586277, 0.0, 31.66483540534973, 0.0, 0.0, 0.0], 'rewardMean': 0.4703742975274485, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.31292295921134
'totalSteps': 3840, 'rewardStep': 0.9602006758591987, 'errorList': [], 'lossList': [0.0, -1.425760423541069, 0.0, 36.28495966434479, 0.0, 0.0, 0.0], 'rewardMean': 0.6336497569713653, 'totalEpisodes': 73, 'stepsPerEpisode': 49, 'rewardPerEpisode': 39.21374311818553
'totalSteps': 5120, 'rewardStep': 0.8460695871301462, 'errorList': [], 'lossList': [0.0, -1.415541225671768, 0.0, 44.65141207695007, 0.0, 0.0, 0.0], 'rewardMean': 0.6867547145110605, 'totalEpisodes': 81, 'stepsPerEpisode': 74, 'rewardPerEpisode': 66.63760418458418
'totalSteps': 6400, 'rewardStep': 0.662848410486184, 'errorList': [], 'lossList': [0.0, -1.4105089938640594, 0.0, 33.60709426403046, 0.0, 0.0, 0.0], 'rewardMean': 0.6819734537060852, 'totalEpisodes': 85, 'stepsPerEpisode': 157, 'rewardPerEpisode': 124.19745488092994
'totalSteps': 7680, 'rewardStep': 0.6922543466223305, 'errorList': [], 'lossList': [0.0, -1.4063767075538636, 0.0, 48.63493349075318, 0.0, 0.0, 0.0], 'rewardMean': 0.6836869358587928, 'totalEpisodes': 91, 'stepsPerEpisode': 86, 'rewardPerEpisode': 70.83170123129463
'totalSteps': 8960, 'rewardStep': 0.9377474929432047, 'errorList': [6.36316663328046, 174.23072866210032, 182.46126038023579, 125.0027431662707, 88.84979133982995, 165.7265760543806, 102.05410955595627, 199.3405263813733, 135.36540185270937, 77.42880432290963, 198.37432651663002, 257.74211925258254, 207.5139960073749, 208.19940457170236, 123.59039105433887, 178.21002059851747, 161.41462544033072, 130.7677728597415, 91.04938717230968, 39.69141658579849, 246.21458997801847, 17.842524095969786, 108.93774055985003, 232.63113306050096, 155.7213070963308, 109.65576202535497, 220.3648878088849, 14.766623576698919, 198.26876224503422, 230.36363122361868, 96.44120691226966, 196.27763657584262, 234.5345878383949, 173.84250088271386, 120.66371252914988, 214.20111875381593, 110.69704793902876, 215.10191797160127, 147.74443345033396, 148.62568980989985, 183.7368507847679, 184.55512341700341, 2.9745266386410365, 132.72706035855373, 217.54983514876878, 6.370388695972456, 187.9747725191461, 213.22449507845755, 162.54003419633642, 179.77516773369263], 'lossList': [0.0, -1.4001517647504806, 0.0, 128.15117965698244, 0.0, 0.0, 0.0], 'rewardMean': 0.7199813011565659, 'totalEpisodes': 111, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.249621536458303, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.8493843566977911, 'errorList': [], 'lossList': [0.0, -1.4097892826795577, 0.0, 44.468904447555545, 0.0, 0.0, 0.0], 'rewardMean': 0.736156683099219, 'totalEpisodes': 123, 'stepsPerEpisode': 53, 'rewardPerEpisode': 48.167251603172254
'totalSteps': 11520, 'rewardStep': 0.6291823501088155, 'errorList': [], 'lossList': [0.0, -1.4185920184850693, 0.0, 32.96500318527222, 0.0, 0.0, 0.0], 'rewardMean': 0.7242706461002854, 'totalEpisodes': 129, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.781943942204445
'totalSteps': 12800, 'rewardStep': 0.6549000804609323, 'errorList': [], 'lossList': [0.0, -1.4188354927301408, 0.0, 53.998020524978635, 0.0, 0.0, 0.0], 'rewardMean': 0.7173335895363501, 'totalEpisodes': 137, 'stepsPerEpisode': 177, 'rewardPerEpisode': 131.30621004801355
'totalSteps': 14080, 'rewardStep': 0.8783903393491362, 'errorList': [], 'lossList': [0.0, -1.41215060710907, 0.0, 26.738615446090698, 0.0, 0.0, 0.0], 'rewardMean': 0.7546787007972252, 'totalEpisodes': 141, 'stepsPerEpisode': 100, 'rewardPerEpisode': 76.27478616375222
'totalSteps': 15360, 'rewardStep': 0.6385311867389182, 'errorList': [], 'lossList': [0.0, -1.398559650182724, 0.0, 24.94743729829788, 0.0, 0.0, 0.0], 'rewardMean': 0.7749508826396657, 'totalEpisodes': 148, 'stepsPerEpisode': 99, 'rewardPerEpisode': 82.38967283610847
'totalSteps': 16640, 'rewardStep': 0.7850164123120936, 'errorList': [], 'lossList': [0.0, -1.4002430713176728, 0.0, 7.999133809804916, 0.0, 0.0, 0.0], 'rewardMean': 0.7574324562849553, 'totalEpisodes': 152, 'stepsPerEpisode': 131, 'rewardPerEpisode': 102.32036650679868
'totalSteps': 17920, 'rewardStep': 0.872133022004536, 'errorList': [], 'lossList': [0.0, -1.4109494179487228, 0.0, 5.542287081480026, 0.0, 0.0, 0.0], 'rewardMean': 0.7600387997723942, 'totalEpisodes': 156, 'stepsPerEpisode': 86, 'rewardPerEpisode': 76.50538761516678
'totalSteps': 19200, 'rewardStep': 0.874667027946137, 'errorList': [], 'lossList': [0.0, -1.4110101824998855, 0.0, 4.855268031656742, 0.0, 0.0, 0.0], 'rewardMean': 0.7812206615183894, 'totalEpisodes': 157, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.25137114304139
'totalSteps': 20480, 'rewardStep': 0.8503819276875905, 'errorList': [], 'lossList': [0.0, -1.413098065853119, 0.0, 1.9992542530596256, 0.0, 0.0, 0.0], 'rewardMean': 0.7970334196249156, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.283930732388
'totalSteps': 21760, 'rewardStep': 0.894208746356403, 'errorList': [], 'lossList': [0.0, -1.4040338826179504, 0.0, 1.424508883357048, 0.0, 0.0, 0.0], 'rewardMean': 0.7926795449662354, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1065.1807733626708
'totalSteps': 23040, 'rewardStep': 0.786245548881953, 'errorList': [], 'lossList': [0.0, -1.357193431854248, 0.0, 1.4585434426367283, 0.0, 0.0, 0.0], 'rewardMean': 0.7863656641846516, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.1477781753142
'totalSteps': 24320, 'rewardStep': 0.8801959727813704, 'errorList': [], 'lossList': [0.0, -1.2997419273853301, 0.0, 1.2638043657317757, 0.0, 0.0, 0.0], 'rewardMean': 0.811467026451907, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.4493322381413
'totalSteps': 25600, 'rewardStep': 0.9531267555819661, 'errorList': [0.039080756961967264, 0.06176170197931218, 0.02583635601692742, 0.02627562902410625, 0.03447390778586499, 0.05968720294408623, 0.02918243960900182, 0.024480802722190016, 0.02894184710193214, 0.03050405191310049, 0.044638016491384734, 0.04731898614975905, 0.0604870259186157, 0.05621649010623508, 0.04747361294030645, 0.03630435605347863, 0.02614439897485607, 0.033512012425357535, 0.06993895964820487, 0.03575535096382116, 0.0363099076222369, 0.033725455895757574, 0.03616707183375802, 0.02326324717257922, 0.066519312123518, 0.05194403617266554, 0.04823882621550594, 0.050281309326316456, 0.02600362201941202, 0.034846912645210074, 0.02921395378947236, 0.041776618650844866, 0.03234317299458887, 0.02631613665555399, 0.027205432292165203, 0.03695304123977006, 0.07003989888105931, 0.02767782615915342, 0.04585246164167426, 0.025364971076335695, 0.04585239843188747, 0.038362379215701965, 0.040931983899448646, 0.060793110351205965, 0.061045862572075116, 0.037194253468113045, 0.038294844899925344, 0.024391536728213682, 0.04467898681533131, 0.05900279854696332], 'lossList': [0.0, -1.2684990179538727, 0.0, 0.963571977429092, 0.0, 0.0, 0.0], 'rewardMean': 0.8412896939640104, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.5795478130212, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=99.83
