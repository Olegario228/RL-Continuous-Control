#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 54
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764079963780335, 'errorList': [], 'lossList': [0.0, -1.411830295920372, 0.0, 34.565115718841554, 0.0, 0.0, 0.0], 'rewardMean': 0.8487490652647203, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.540008571777182
'totalSteps': 3840, 'rewardStep': 0.7998324622988855, 'errorList': [], 'lossList': [0.0, -1.4157325088977815, 0.0, 38.1945924949646, 0.0, 0.0, 0.0], 'rewardMean': 0.8324435309427752, 'totalEpisodes': 84, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.561484775234206
'totalSteps': 5120, 'rewardStep': 0.7214341852444492, 'errorList': [], 'lossList': [0.0, -1.4044497281312942, 0.0, 28.896044626235962, 0.0, 0.0, 0.0], 'rewardMean': 0.8046911945181938, 'totalEpisodes': 95, 'stepsPerEpisode': 52, 'rewardPerEpisode': 41.62893571883579
'totalSteps': 6400, 'rewardStep': 0.6437225379129541, 'errorList': [], 'lossList': [0.0, -1.3868718934059143, 0.0, 32.165562858581545, 0.0, 0.0, 0.0], 'rewardMean': 0.7724974631971459, 'totalEpisodes': 104, 'stepsPerEpisode': 107, 'rewardPerEpisode': 82.86688161800642
'totalSteps': 7680, 'rewardStep': 0.7652048916785045, 'errorList': [], 'lossList': [0.0, -1.370098928809166, 0.0, 48.85481197357178, 0.0, 0.0, 0.0], 'rewardMean': 0.7712820346107057, 'totalEpisodes': 112, 'stepsPerEpisode': 31, 'rewardPerEpisode': 24.029064398228975
'totalSteps': 8960, 'rewardStep': 0.5770090353760071, 'errorList': [], 'lossList': [0.0, -1.3626900404691695, 0.0, 190.72274532318116, 0.0, 0.0, 0.0], 'rewardMean': 0.7435287490057487, 'totalEpisodes': 155, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.3588414097626673
'totalSteps': 10240, 'rewardStep': 0.7929543654534994, 'errorList': [], 'lossList': [0.0, -1.3625608944892884, 0.0, 72.69751941680909, 0.0, 0.0, 0.0], 'rewardMean': 0.7497069510617176, 'totalEpisodes': 182, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.38805832307548
'totalSteps': 11520, 'rewardStep': 0.47407313469880463, 'errorList': [], 'lossList': [0.0, -1.354191557765007, 0.0, 32.15070672988892, 0.0, 0.0, 0.0], 'rewardMean': 0.7190809714658384, 'totalEpisodes': 196, 'stepsPerEpisode': 84, 'rewardPerEpisode': 65.4014948392294
'totalSteps': 12800, 'rewardStep': 0.7555609665028823, 'errorList': [], 'lossList': [0.0, -1.3429458773136138, 0.0, 16.841746773719787, 0.0, 0.0, 0.0], 'rewardMean': 0.7227289709695428, 'totalEpisodes': 203, 'stepsPerEpisode': 63, 'rewardPerEpisode': 52.27784343276941
'totalSteps': 14080, 'rewardStep': 0.8755295354745204, 'errorList': [], 'lossList': [0.0, -1.3372542530298233, 0.0, 10.688722270727158, 0.0, 0.0, 0.0], 'rewardMean': 0.718172911101854, 'totalEpisodes': 208, 'stepsPerEpisode': 64, 'rewardPerEpisode': 53.9457893640352
'totalSteps': 15360, 'rewardStep': 0.5341009970337741, 'errorList': [], 'lossList': [0.0, -1.3299304723739624, 0.0, 25.601478292942048, 0.0, 0.0, 0.0], 'rewardMean': 0.6939422111674282, 'totalEpisodes': 214, 'stepsPerEpisode': 192, 'rewardPerEpisode': 138.74901982882832
'totalSteps': 16640, 'rewardStep': 0.6546110328858388, 'errorList': [], 'lossList': [0.0, -1.3060324841737747, 0.0, 7.904048492312431, 0.0, 0.0, 0.0], 'rewardMean': 0.6794200682261236, 'totalEpisodes': 218, 'stepsPerEpisode': 145, 'rewardPerEpisode': 126.97003847010885
'totalSteps': 17920, 'rewardStep': 0.46210307806993284, 'errorList': [], 'lossList': [0.0, -1.278859804868698, 0.0, 5.094121006727218, 0.0, 0.0, 0.0], 'rewardMean': 0.6534869575086718, 'totalEpisodes': 219, 'stepsPerEpisode': 594, 'rewardPerEpisode': 444.75039309299524
'totalSteps': 19200, 'rewardStep': 0.9083066238827764, 'errorList': [], 'lossList': [0.0, -1.2731066513061524, 0.0, 7.450460467338562, 0.0, 0.0, 0.0], 'rewardMean': 0.679945366105654, 'totalEpisodes': 222, 'stepsPerEpisode': 112, 'rewardPerEpisode': 101.58854714992981
'totalSteps': 20480, 'rewardStep': 0.7207269337989033, 'errorList': [], 'lossList': [0.0, -1.2667355251312256, 0.0, 4.227236686944962, 0.0, 0.0, 0.0], 'rewardMean': 0.6754975703176939, 'totalEpisodes': 223, 'stepsPerEpisode': 466, 'rewardPerEpisode': 359.0875558915898
'totalSteps': 21760, 'rewardStep': 0.6274432569246069, 'errorList': [], 'lossList': [0.0, -1.2486294192075729, 0.0, 2.954025692641735, 0.0, 0.0, 0.0], 'rewardMean': 0.6805409924725538, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 970.3277536242249
'totalSteps': 23040, 'rewardStep': 0.8603668250357328, 'errorList': [], 'lossList': [0.0, -1.2278741592168807, 0.0, 1.6369204403460025, 0.0, 0.0, 0.0], 'rewardMean': 0.6872822384307773, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1045.456863640044
'totalSteps': 24320, 'rewardStep': 0.8328534155868658, 'errorList': [], 'lossList': [0.0, -1.1979803347587585, 0.0, 1.0302764064446093, 0.0, 0.0, 0.0], 'rewardMean': 0.7231602665195833, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1107.0908150596229
'totalSteps': 25600, 'rewardStep': 0.9569402524361738, 'errorList': [0.0435196840072596, 0.04486509052750279, 0.0424808478140763, 0.040576583827148856, 0.0402734975655075, 0.040009648583600524, 0.07731091864490819, 0.04830186198591495, 0.06581622509654757, 0.06471979519658837, 0.05893986821308446, 0.060227693843981044, 0.06524085303283665, 0.04594386025453021, 0.07128787424034283, 0.05938608947112932, 0.04909397811632292, 0.08941859869634296, 0.03825394376040143, 0.044030871892568504, 0.06087361115599887, 0.04192376081317218, 0.05277139996391054, 0.06766483097394788, 0.043395920260145174, 0.04193471405553122, 0.04113682759930099, 0.039573335337618164, 0.04135496409825698, 0.03824611731699845, 0.07025138126130104, 0.04312016803891222, 0.04108335862701949, 0.04104040916608296, 0.04142845785369869, 0.04623780913045542, 0.07778749263183539, 0.04363060428908363, 0.0394649660994537, 0.041544213623663624, 0.044117639862534516, 0.041969254679294125, 0.04773647515394479, 0.045911512738381934, 0.038961473606490354, 0.08465249473623349, 0.044685872926305, 0.06195107166736349, 0.07818406712108623, 0.05324394438685638], 'lossList': [0.0, -1.1665518528223038, 0.0, 1.588250208683312, 0.0, 0.0, 0.0], 'rewardMean': 0.7432981951129125, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1185.4288734181212, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=78.79
