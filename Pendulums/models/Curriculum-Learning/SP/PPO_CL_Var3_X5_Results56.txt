#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 56
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6323082303070476, 'errorList': [], 'lossList': [0.0, -1.4222374892234801, 0.0, 26.877216041088104, 0.0, 0.0, 0.0], 'rewardMean': 0.7977988911088787, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.80044579369883
'totalSteps': 3840, 'rewardStep': 0.8385614841162599, 'errorList': [], 'lossList': [0.0, -1.424397068619728, 0.0, 22.39305002450943, 0.0, 0.0, 0.0], 'rewardMean': 0.811386422111339, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 254.87859508102474
'totalSteps': 5120, 'rewardStep': 0.6669216002336267, 'errorList': [], 'lossList': [0.0, -1.425483152270317, 0.0, 19.20474014878273, 0.0, 0.0, 0.0], 'rewardMean': 0.775270216641911, 'totalEpisodes': 23, 'stepsPerEpisode': 72, 'rewardPerEpisode': 49.94019954490764
'totalSteps': 6400, 'rewardStep': 0.6315752373633639, 'errorList': [], 'lossList': [0.0, -1.4235919737815856, 0.0, 24.44529006123543, 0.0, 0.0, 0.0], 'rewardMean': 0.7465312207862016, 'totalEpisodes': 26, 'stepsPerEpisode': 153, 'rewardPerEpisode': 116.01673955210471
'totalSteps': 7680, 'rewardStep': 0.7636401211654968, 'errorList': [], 'lossList': [0.0, -1.419441849589348, 0.0, 40.777680475711826, 0.0, 0.0, 0.0], 'rewardMean': 0.7493827041827507, 'totalEpisodes': 30, 'stepsPerEpisode': 805, 'rewardPerEpisode': 604.6793574514439
'totalSteps': 8960, 'rewardStep': 0.807290209871558, 'errorList': [], 'lossList': [0.0, -1.4193947368860245, 0.0, 295.2070550537109, 0.0, 0.0, 0.0], 'rewardMean': 0.7576552049954375, 'totalEpisodes': 72, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.2574718760993933
'totalSteps': 10240, 'rewardStep': 0.7214836488553988, 'errorList': [], 'lossList': [0.0, -1.4195476788282395, 0.0, 126.35189476013184, 0.0, 0.0, 0.0], 'rewardMean': 0.7531337604779327, 'totalEpisodes': 104, 'stepsPerEpisode': 42, 'rewardPerEpisode': 27.072222212024997
'totalSteps': 11520, 'rewardStep': 0.525294163373899, 'errorList': [], 'lossList': [0.0, -1.4109617340564728, 0.0, 64.19555200576782, 0.0, 0.0, 0.0], 'rewardMean': 0.7278182496885957, 'totalEpisodes': 133, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.9926218884001
'totalSteps': 12800, 'rewardStep': 0.882039664757146, 'errorList': [], 'lossList': [0.0, -1.400015839934349, 0.0, 34.645166606903075, 0.0, 0.0, 0.0], 'rewardMean': 0.7432403911954506, 'totalEpisodes': 149, 'stepsPerEpisode': 36, 'rewardPerEpisode': 30.784511576988905
'totalSteps': 14080, 'rewardStep': 0.863310572259266, 'errorList': [], 'lossList': [0.0, -1.3922379046678544, 0.0, 35.842142057418826, 0.0, 0.0, 0.0], 'rewardMean': 0.7332424932303063, 'totalEpisodes': 156, 'stepsPerEpisode': 310, 'rewardPerEpisode': 243.4321390737942
'totalSteps': 15360, 'rewardStep': 0.7186818734008716, 'errorList': [], 'lossList': [0.0, -1.3789627796411514, 0.0, 8.616531392335892, 0.0, 0.0, 0.0], 'rewardMean': 0.7418798575396887, 'totalEpisodes': 161, 'stepsPerEpisode': 86, 'rewardPerEpisode': 68.94373658549583
'totalSteps': 16640, 'rewardStep': 0.4101345531153151, 'errorList': [], 'lossList': [0.0, -1.3753635823726653, 0.0, 12.380049335956574, 0.0, 0.0, 0.0], 'rewardMean': 0.6990371644395943, 'totalEpisodes': 166, 'stepsPerEpisode': 276, 'rewardPerEpisode': 191.93414021292395
'totalSteps': 17920, 'rewardStep': 0.7769840444329452, 'errorList': [], 'lossList': [0.0, -1.367095382809639, 0.0, 11.604562621116639, 0.0, 0.0, 0.0], 'rewardMean': 0.7100434088595261, 'totalEpisodes': 170, 'stepsPerEpisode': 49, 'rewardPerEpisode': 41.50773189462234
'totalSteps': 19200, 'rewardStep': 0.7649526073022881, 'errorList': [], 'lossList': [0.0, -1.353715981245041, 0.0, 6.550173822641373, 0.0, 0.0, 0.0], 'rewardMean': 0.7233811458534184, 'totalEpisodes': 174, 'stepsPerEpisode': 367, 'rewardPerEpisode': 294.3028837635506
'totalSteps': 20480, 'rewardStep': 0.8062764459189151, 'errorList': [], 'lossList': [0.0, -1.3596990567445755, 0.0, 6.97913500905037, 0.0, 0.0, 0.0], 'rewardMean': 0.7276447783287603, 'totalEpisodes': 176, 'stepsPerEpisode': 270, 'rewardPerEpisode': 237.86936702795128
'totalSteps': 21760, 'rewardStep': 0.768107466139427, 'errorList': [], 'lossList': [0.0, -1.3539364939928056, 0.0, 2.7474697121977805, 0.0, 0.0, 0.0], 'rewardMean': 0.7237265039555472, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.7814614111153
'totalSteps': 23040, 'rewardStep': 0.9195422945006814, 'errorList': [], 'lossList': [0.0, -1.3122036409378053, 0.0, 1.2417088289558889, 0.0, 0.0, 0.0], 'rewardMean': 0.7435323685200755, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1011.071890208339
'totalSteps': 24320, 'rewardStep': 0.754662943647054, 'errorList': [], 'lossList': [0.0, -1.2775778114795684, 0.0, 1.4648843989521265, 0.0, 0.0, 0.0], 'rewardMean': 0.7664692465473909, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1102.3497935366358
'totalSteps': 25600, 'rewardStep': 0.9549226557104364, 'errorList': [0.06734293758725868, 0.07564111764023809, 0.09702471525131026, 0.0627553853542479, 0.07990936611686605, 0.12050785139213353, 0.1078036340878066, 0.10557525151761847, 0.13070258089581016, 0.08999643137186689, 0.07484234585602932, 0.10574147785020382, 0.11596154520290547, 0.08480408776106346, 0.08753741033289997, 0.11151319787778953, 0.09071116240916982, 0.0759978293359754, 0.12949306998747329, 0.096769036197707, 0.11041975234376093, 0.08802398346385983, 0.09243952282466825, 0.11943583663240707, 0.0891400752102382, 0.09279954223758048, 0.1267734833945773, 0.11651084958879832, 0.10782979923097981, 0.07985761743238792, 0.1059910348333942, 0.06591774179007528, 0.07282185466513376, 0.11988046993121566, 0.11858776567866655, 0.09034323969858699, 0.15188671215104982, 0.08790059417657617, 0.1042993093365611, 0.12243128820006809, 0.08524630456869652, 0.08158396707467198, 0.09183288320873008, 0.11868344621309433, 0.09416837469012035, 0.09410234746659214, 0.08795371909238803, 0.10190243490601472, 0.11874445493646457, 0.07271057577842682], 'lossList': [0.0, -1.228958191871643, 0.0, 1.3793889109045268, 0.0, 0.0, 0.0], 'rewardMean': 0.7737575456427201, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1186.1956207349826, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=79.76
