#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 58
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.716705753837468, 'errorList': [], 'lossList': [0.0, -1.4116206312179564, 0.0, 24.992819174528123, 0.0, 0.0, 0.0], 'rewardMean': 0.5305304726914615, 'totalEpisodes': 14, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.019332497666813
'totalSteps': 3840, 'rewardStep': 0.9200268738171054, 'errorList': [], 'lossList': [0.0, -1.3997423815727235, 0.0, 28.183257937431335, 0.0, 0.0, 0.0], 'rewardMean': 0.6603626064000094, 'totalEpisodes': 20, 'stepsPerEpisode': 300, 'rewardPerEpisode': 220.05368870960996
'totalSteps': 5120, 'rewardStep': 0.744768216646285, 'errorList': [], 'lossList': [0.0, -1.4051164531707763, 0.0, 29.859618639945985, 0.0, 0.0, 0.0], 'rewardMean': 0.6814640089615783, 'totalEpisodes': 21, 'stepsPerEpisode': 180, 'rewardPerEpisode': 149.6366250383903
'totalSteps': 6400, 'rewardStep': 0.4847098523222971, 'errorList': [], 'lossList': [0.0, -1.4003847086429595, 0.0, 21.22789407491684, 0.0, 0.0, 0.0], 'rewardMean': 0.6421131776337221, 'totalEpisodes': 22, 'stepsPerEpisode': 954, 'rewardPerEpisode': 635.2906789700784
'totalSteps': 7680, 'rewardStep': 0.8403554310725392, 'errorList': [], 'lossList': [0.0, -1.3995198065042496, 0.0, 68.06668515205384, 0.0, 0.0, 0.0], 'rewardMean': 0.6751535532068583, 'totalEpisodes': 30, 'stepsPerEpisode': 59, 'rewardPerEpisode': 45.974832343607865
'totalSteps': 8960, 'rewardStep': 0.9437064517841585, 'errorList': [381.8419376570358, 294.9478178577118, 316.3468419487644, 380.80397058754653, 323.7918237703461, 282.0922856384487, 290.17695108104715, 269.6353970962237, 376.4048435985498, 134.7411769059804, 326.3976420237431, 367.99785470368556, 344.785439602945, 355.1607994157503, 331.1548415467232, 225.7104199339835, 265.6952791885005, 357.6378894990981, 286.3968166969384, 340.8486663789283, 201.13783652150767, 344.64301116532727, 319.2453801718559, 366.23746405671733, 306.44276536071175, 335.21197225080067, 174.19260424165202, 337.068995428875, 195.67537396366708, 300.3799277064399, 285.6350388175859, 350.6347369221502, 268.5122106740717, 373.0057998565586, 337.91325813898555, 249.79336299787138, 311.2776963017951, 283.14641905575894, 319.1715974645177, 342.4609067651041, 351.3900681948256, 325.0149239154495, 348.6136535179353, 327.44304720113473, 348.3115591074857, 346.6766083470973, 318.37903536778805, 314.926412379684, 380.13674187980735, 351.67494540814795], 'lossList': [0.0, -1.3998292928934097, 0.0, 216.59549655914307, 0.0, 0.0, 0.0], 'rewardMean': 0.7135182530036154, 'totalEpisodes': 70, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.8417097479646065, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.7975308569705718, 'errorList': [], 'lossList': [0.0, -1.404038164615631, 0.0, 89.37962615966796, 0.0, 0.0, 0.0], 'rewardMean': 0.724019828499485, 'totalEpisodes': 95, 'stepsPerEpisode': 22, 'rewardPerEpisode': 17.96543781733341
'totalSteps': 11520, 'rewardStep': 0.7678312586110986, 'errorList': [], 'lossList': [0.0, -1.4013001203536988, 0.0, 43.464658679962156, 0.0, 0.0, 0.0], 'rewardMean': 0.7288877651785532, 'totalEpisodes': 112, 'stepsPerEpisode': 38, 'rewardPerEpisode': 30.538164684427233
'totalSteps': 12800, 'rewardStep': 0.17283588304462183, 'errorList': [], 'lossList': [0.0, -1.3943462300300598, 0.0, 22.00496291399002, 0.0, 0.0, 0.0], 'rewardMean': 0.67328257696516, 'totalEpisodes': 118, 'stepsPerEpisode': 233, 'rewardPerEpisode': 167.00622224160503
'totalSteps': 14080, 'rewardStep': 0.9564837975061686, 'errorList': [6.4553699294695415, 9.956447442193472, 4.712267668379108, 2.604520298147473, 0.6892513230271727, 1.7909402645970294, 12.420816340626882, 9.602345334120272, 1.2751141841086269, 0.24579576960093916, 15.600341634033697, 12.021209689680846, 8.667687252904182, 0.41493579433627825, 8.382258549475727, 8.580000511509448, 14.428347381450857, 12.378069895696438, 3.9043657338890116, 1.8337127549390022, 4.508563303817601, 0.5874256353791827, 7.5899407414255275, 6.999426895998288, 18.362814290205378, 10.87800843060389, 0.8502735652645442, 8.835003944077435, 2.0046022668802994, 8.303173735100494, 1.0247807050139952, 0.7104438913925943, 10.400007440731082, 11.287422371383634, 4.594613943566955, 19.18092920005056, 1.9256811717548217, 7.0653140154033, 6.356833851692311, 7.360695227579334, 9.209600758398658, 11.38611867890946, 13.913809413551624, 12.371935715030201, 21.094723585744163, 22.131847810506237, 8.252655678102018, 0.41328358760185196, 3.437246657461063, 0.2181184525380179], 'lossList': [0.0, -1.3844328212738037, 0.0, 11.066608762741089, 0.0, 0.0, 0.0], 'rewardMean': 0.7344954375612314, 'totalEpisodes': 122, 'stepsPerEpisode': 399, 'rewardPerEpisode': 317.6524403564917, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.7631607551060913, 'errorList': [], 'lossList': [0.0, -1.3727932918071746, 0.0, 15.668201999664307, 0.0, 0.0, 0.0], 'rewardMean': 0.7391409376880937, 'totalEpisodes': 128, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.748694641667635
'totalSteps': 16640, 'rewardStep': 0.7828022201139699, 'errorList': [], 'lossList': [0.0, -1.3709487360715866, 0.0, 8.861602445840836, 0.0, 0.0, 0.0], 'rewardMean': 0.7254184723177801, 'totalEpisodes': 130, 'stepsPerEpisode': 152, 'rewardPerEpisode': 126.96094634184591
'totalSteps': 17920, 'rewardStep': 0.6764633754512015, 'errorList': [], 'lossList': [0.0, -1.3581134885549546, 0.0, 7.915525826215744, 0.0, 0.0, 0.0], 'rewardMean': 0.7185879881982717, 'totalEpisodes': 132, 'stepsPerEpisode': 335, 'rewardPerEpisode': 278.362660403336
'totalSteps': 19200, 'rewardStep': 0.7704703764837573, 'errorList': [], 'lossList': [0.0, -1.3477038556337357, 0.0, 4.613261025547981, 0.0, 0.0, 0.0], 'rewardMean': 0.747164040614418, 'totalEpisodes': 135, 'stepsPerEpisode': 168, 'rewardPerEpisode': 137.79755037862773
'totalSteps': 20480, 'rewardStep': 0.8843654269328667, 'errorList': [], 'lossList': [0.0, -1.335334774851799, 0.0, 3.7216315603256227, 0.0, 0.0, 0.0], 'rewardMean': 0.7515650402004506, 'totalEpisodes': 137, 'stepsPerEpisode': 243, 'rewardPerEpisode': 212.79609242186368
'totalSteps': 21760, 'rewardStep': 0.8578117056644615, 'errorList': [], 'lossList': [0.0, -1.3279495733976363, 0.0, 3.5690725690126417, 0.0, 0.0, 0.0], 'rewardMean': 0.742975565588481, 'totalEpisodes': 138, 'stepsPerEpisode': 589, 'rewardPerEpisode': 483.8862034518591
'totalSteps': 23040, 'rewardStep': 0.7023256565061687, 'errorList': [], 'lossList': [0.0, -1.3247294330596924, 0.0, 1.1611582545936108, 0.0, 0.0, 0.0], 'rewardMean': 0.7334550455420406, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1040.2250701261569
'totalSteps': 24320, 'rewardStep': 0.8332662793893323, 'errorList': [], 'lossList': [0.0, -1.2873076885938644, 0.0, 1.3957336466014385, 0.0, 0.0, 0.0], 'rewardMean': 0.7399985476198639, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.566475134778
'totalSteps': 25600, 'rewardStep': 0.9678058402661608, 'errorList': [0.06826774928624955, 0.06146884990847124, 0.0642653644417619, 0.06396904182296999, 0.0667223352560876, 0.060838156163156874, 0.1339548049892289, 0.06079550275854591, 0.06572194266835525, 0.06628528412826329, 0.06776140900664673, 0.06254471850002066, 0.061855654080085275, 0.069791237564296, 0.12621407251986777, 0.06329144805091336, 0.06718296122632637, 0.06579494685576459, 0.10529439042834438, 0.06779544554580685, 0.06357637301966493, 0.0680076728705482, 0.06109618493425816, 0.0790045588428218, 0.06270481445659822, 0.060488821386181696, 0.07915323039755191, 0.11938767128678733, 0.06142643638382575, 0.12624967460393766, 0.12386165086633903, 0.06311570691382747, 0.0667624154268016, 0.06573183766816044, 0.0667961038979531, 0.06179566949268468, 0.0665684590291396, 0.06541467245494693, 0.08087353515996114, 0.11480433676415731, 0.06385243881636354, 0.0638818517887542, 0.06495867077810909, 0.07914567977844443, 0.0638756975844764, 0.06430327194328486, 0.1148037157742512, 0.06938874728014369, 0.06992128793374816, 0.09555073747819788], 'lossList': [0.0, -1.2636555898189545, 0.0, 1.1162218177318572, 0.0, 0.0, 0.0], 'rewardMean': 0.8194955433420178, 'totalEpisodes': 138, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1185.3043339174478, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=120.35
