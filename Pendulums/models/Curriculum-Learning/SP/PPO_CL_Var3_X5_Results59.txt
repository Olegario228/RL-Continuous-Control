#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 59
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7930263443093106, 'errorList': [], 'lossList': [0.0, -1.412548435330391, 0.0, 24.133152465820313, 0.0, 0.0, 0.0], 'rewardMean': 0.5785482250424496, 'totalEpisodes': 20, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.511567391706286
'totalSteps': 3840, 'rewardStep': 0.7365054963730826, 'errorList': [], 'lossList': [0.0, -1.4236778700351715, 0.0, 20.52837867498398, 0.0, 0.0, 0.0], 'rewardMean': 0.6312006488193272, 'totalEpisodes': 30, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.438671281086107
'totalSteps': 5120, 'rewardStep': 0.6287413985465747, 'errorList': [], 'lossList': [0.0, -1.4303584170341492, 0.0, 16.64344980478287, 0.0, 0.0, 0.0], 'rewardMean': 0.6305858362511392, 'totalEpisodes': 35, 'stepsPerEpisode': 94, 'rewardPerEpisode': 70.26234832444429
'totalSteps': 6400, 'rewardStep': 0.9604739236756754, 'errorList': [], 'lossList': [0.0, -1.4318459433317186, 0.0, 39.54544678211212, 0.0, 0.0, 0.0], 'rewardMean': 0.6965634537360464, 'totalEpisodes': 40, 'stepsPerEpisode': 34, 'rewardPerEpisode': 29.21189191668418
'totalSteps': 7680, 'rewardStep': 0.809534002494182, 'errorList': [], 'lossList': [0.0, -1.4224791556596756, 0.0, 91.05445222854614, 0.0, 0.0, 0.0], 'rewardMean': 0.715391878529069, 'totalEpisodes': 51, 'stepsPerEpisode': 35, 'rewardPerEpisode': 25.3390040639379
'totalSteps': 8960, 'rewardStep': 0.6198699452156556, 'errorList': [], 'lossList': [0.0, -1.41678948700428, 0.0, 206.26933235168457, 0.0, 0.0, 0.0], 'rewardMean': 0.7017458880557242, 'totalEpisodes': 93, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.3552425176245517
'totalSteps': 10240, 'rewardStep': 0.9682042751563114, 'errorList': [98.38364416571747, 137.99658797172057, 16.76838376260961, 65.05959613164255, 116.83972698748069, 14.04651784006206, 60.63195821704402, 69.55731466997037, 81.89215099377506, 62.3684796489525, 48.07019869884676, 135.33229163944708, 100.33763423363371, 101.24535975229188, 130.42502745933902, 86.60183309503616, 71.40646528708743, 113.90434501626534, 13.044965046168224, 38.72033966757892, 119.18968922968455, 115.55613295892793, 143.05000039181684, 98.59964376510622, 103.19389183898936, 46.29184851226731, 136.40745346254892, 106.65769519617442, 96.36951533112016, 89.4334969862334, 111.22404432094233, 60.643195144204235, 138.5328154636444, 42.497315374794084, 129.5056808880383, 87.45422902625282, 119.5961896818197, 141.77968162197826, 76.468504062632, 115.72708855349579, 119.3614766031593, 152.2223530979202, 121.84166991184298, 58.015182739981654, 40.619283851345706, 117.71229043498673, 54.510178608238725, 20.607778718498732, 11.4422360405931, 19.005348528518436], 'lossList': [0.0, -1.4082522535324096, 0.0, 88.268680934906, 0.0, 0.0, 0.0], 'rewardMean': 0.7350531864432976, 'totalEpisodes': 123, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.305162921536201, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.8349927144131888, 'errorList': [], 'lossList': [0.0, -1.4025332552194596, 0.0, 40.96812394142151, 0.0, 0.0, 0.0], 'rewardMean': 0.7461575784399522, 'totalEpisodes': 136, 'stepsPerEpisode': 82, 'rewardPerEpisode': 65.9528841706359
'totalSteps': 12800, 'rewardStep': 0.7874381524758605, 'errorList': [], 'lossList': [0.0, -1.3843834215402604, 0.0, 27.594213638305664, 0.0, 0.0, 0.0], 'rewardMean': 0.750285635843543, 'totalEpisodes': 145, 'stepsPerEpisode': 63, 'rewardPerEpisode': 55.60875717349876
'totalSteps': 14080, 'rewardStep': 0.4881884715144084, 'errorList': [], 'lossList': [0.0, -1.3645743364095688, 0.0, 9.30354900956154, 0.0, 0.0, 0.0], 'rewardMean': 0.7626974724174249, 'totalEpisodes': 149, 'stepsPerEpisode': 279, 'rewardPerEpisode': 183.4396423083084
'totalSteps': 15360, 'rewardStep': 0.7333813180773542, 'errorList': [], 'lossList': [0.0, -1.3521577686071395, 0.0, 35.56907610893249, 0.0, 0.0, 0.0], 'rewardMean': 0.7567329697942294, 'totalEpisodes': 157, 'stepsPerEpisode': 89, 'rewardPerEpisode': 66.51280334193726
'totalSteps': 16640, 'rewardStep': 0.6258133221214766, 'errorList': [], 'lossList': [0.0, -1.3440047669410706, 0.0, 18.802528120279312, 0.0, 0.0, 0.0], 'rewardMean': 0.7456637523690688, 'totalEpisodes': 162, 'stepsPerEpisode': 87, 'rewardPerEpisode': 75.03924279659675
'totalSteps': 17920, 'rewardStep': 0.7723103024787097, 'errorList': [], 'lossList': [0.0, -1.3397913515567779, 0.0, 5.168127709627152, 0.0, 0.0, 0.0], 'rewardMean': 0.7600206427622822, 'totalEpisodes': 166, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.417429243083667
'totalSteps': 19200, 'rewardStep': 0.9064165902867921, 'errorList': [], 'lossList': [0.0, -1.314051592350006, 0.0, 4.797184085249901, 0.0, 0.0, 0.0], 'rewardMean': 0.7546149094233938, 'totalEpisodes': 168, 'stepsPerEpisode': 339, 'rewardPerEpisode': 292.26333641673483
'totalSteps': 20480, 'rewardStep': 0.8361805837133357, 'errorList': [], 'lossList': [0.0, -1.292727535367012, 0.0, 5.983472409248352, 0.0, 0.0, 0.0], 'rewardMean': 0.7572795675453093, 'totalEpisodes': 171, 'stepsPerEpisode': 201, 'rewardPerEpisode': 165.75232074491183
'totalSteps': 21760, 'rewardStep': 0.43203033460525797, 'errorList': [], 'lossList': [0.0, -1.2764321231842042, 0.0, 3.5123232069611547, 0.0, 0.0, 0.0], 'rewardMean': 0.7384956064842696, 'totalEpisodes': 173, 'stepsPerEpisode': 419, 'rewardPerEpisode': 320.2559980551164
'totalSteps': 23040, 'rewardStep': 0.8429936720372123, 'errorList': [], 'lossList': [0.0, -1.2700515240430832, 0.0, 4.2076052629947664, 0.0, 0.0, 0.0], 'rewardMean': 0.7259745461723596, 'totalEpisodes': 176, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.8419126253204645
'totalSteps': 24320, 'rewardStep': 0.8562592017021067, 'errorList': [], 'lossList': [0.0, -1.2714147198200225, 0.0, 3.2250823479890824, 0.0, 0.0, 0.0], 'rewardMean': 0.7281011949012515, 'totalEpisodes': 178, 'stepsPerEpisode': 138, 'rewardPerEpisode': 127.13051013133199
'totalSteps': 25600, 'rewardStep': 0.8119879144368056, 'errorList': [], 'lossList': [0.0, -1.2662013685703277, 0.0, 2.8364876914024353, 0.0, 0.0, 0.0], 'rewardMean': 0.7305561710973458, 'totalEpisodes': 179, 'stepsPerEpisode': 205, 'rewardPerEpisode': 167.73962318373066
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=79.78
