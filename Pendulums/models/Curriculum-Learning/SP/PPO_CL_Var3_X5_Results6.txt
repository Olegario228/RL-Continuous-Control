#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 6
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6317531691877548, 'errorList': [], 'lossList': [0.0, -1.422174175977707, 0.0, 26.866162285804748, 0.0, 0.0, 0.0], 'rewardMean': 0.7975213605492323, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.7537232999083
'totalSteps': 3840, 'rewardStep': 0.820145863771994, 'errorList': [], 'lossList': [0.0, -1.419988186955452, 0.0, 22.229584603309632, 0.0, 0.0, 0.0], 'rewardMean': 0.8050628616234862, 'totalEpisodes': 21, 'stepsPerEpisode': 380, 'rewardPerEpisode': 250.81742616953767
'totalSteps': 5120, 'rewardStep': 0.7445317113947718, 'errorList': [], 'lossList': [0.0, -1.4149693959951402, 0.0, 26.65073492050171, 0.0, 0.0, 0.0], 'rewardMean': 0.7899300740663076, 'totalEpisodes': 26, 'stepsPerEpisode': 65, 'rewardPerEpisode': 49.1371326329716
'totalSteps': 6400, 'rewardStep': 0.9347090341519244, 'errorList': [276.0695365238969, 311.12181437792304, 317.489406125899, 238.1447291173046, 309.72071459664704, 308.0721168756811, 289.0592043848465, 213.1873434233802, 311.3789733611559, 306.93498100978087, 290.4160150127262, 319.9719712436714, 322.31973411044834, 252.2319027137228, 241.83660698870284, 319.19539515898686, 295.1448766364333, 272.21343144819525, 294.3136471213865, 307.56316831034076, 326.85030734230963, 245.13662248250392, 279.61775184308567, 311.6964064545727, 312.8394761533739, 321.6163416549942, 315.38827407057823, 217.89380797734665, 308.75463936272644, 140.10013554431842, 314.4507293396878, 308.75081385915365, 323.8196607590919, 242.20272106423613, 269.8445174650078, 236.6774036581119, 220.10308157795532, 288.0769364649223, 302.8029059261258, 198.94007110844836, 314.5790177569717, 232.50953685482153, 275.5303694396472, 178.9420412305407, 208.89342542701922, 299.40780467490964, 269.3237451159812, 291.9019162760975, 242.65215518941454, 298.9978697968749], 'lossList': [0.0, -1.4094654995203018, 0.0, 164.9071368789673, 0.0, 0.0, 0.0], 'rewardMean': 0.818885866083431, 'totalEpisodes': 70, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.8278296610410871, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.6149628557153877, 'errorList': [], 'lossList': [0.0, -1.4022754925489425, 0.0, 65.58408378601074, 0.0, 0.0, 0.0], 'rewardMean': 0.7848986976887571, 'totalEpisodes': 94, 'stepsPerEpisode': 38, 'rewardPerEpisode': 23.46495688604864
'totalSteps': 8960, 'rewardStep': 0.46886897740135347, 'errorList': [], 'lossList': [0.0, -1.3994826793670654, 0.0, 68.46610662460327, 0.0, 0.0, 0.0], 'rewardMean': 0.7397515947905565, 'totalEpisodes': 112, 'stepsPerEpisode': 108, 'rewardPerEpisode': 80.70260543924614
'totalSteps': 10240, 'rewardStep': 0.6524666743365074, 'errorList': [], 'lossList': [0.0, -1.379549611210823, 0.0, 26.359155158996582, 0.0, 0.0, 0.0], 'rewardMean': 0.7288409797338004, 'totalEpisodes': 118, 'stepsPerEpisode': 55, 'rewardPerEpisode': 39.198305736289086
'totalSteps': 11520, 'rewardStep': 0.47270704185319223, 'errorList': [], 'lossList': [0.0, -1.3682959127426146, 0.0, 42.62589668273926, 0.0, 0.0, 0.0], 'rewardMean': 0.7003816533026216, 'totalEpisodes': 125, 'stepsPerEpisode': 340, 'rewardPerEpisode': 263.199890584157
'totalSteps': 12800, 'rewardStep': 0.7097556697143043, 'errorList': [], 'lossList': [0.0, -1.3414705109596252, 0.0, 17.70048889875412, 0.0, 0.0, 0.0], 'rewardMean': 0.7013190549437899, 'totalEpisodes': 130, 'stepsPerEpisode': 124, 'rewardPerEpisode': 98.0620938447034
'totalSteps': 14080, 'rewardStep': 0.7727324210873953, 'errorList': [], 'lossList': [0.0, -1.305977777838707, 0.0, 15.542141985893249, 0.0, 0.0, 0.0], 'rewardMean': 0.6822633418614585, 'totalEpisodes': 133, 'stepsPerEpisode': 247, 'rewardPerEpisode': 195.81002124374754
'totalSteps': 15360, 'rewardStep': 0.9228942836540363, 'errorList': [], 'lossList': [0.0, -1.2974453032016755, 0.0, 9.038697175979614, 0.0, 0.0, 0.0], 'rewardMean': 0.7113774533080868, 'totalEpisodes': 135, 'stepsPerEpisode': 55, 'rewardPerEpisode': 50.005385151128365
'totalSteps': 16640, 'rewardStep': 0.6328364346771806, 'errorList': [], 'lossList': [0.0, -1.281126903295517, 0.0, 20.996317552328108, 0.0, 0.0, 0.0], 'rewardMean': 0.6926465103986054, 'totalEpisodes': 136, 'stepsPerEpisode': 508, 'rewardPerEpisode': 364.15841474156616
'totalSteps': 17920, 'rewardStep': 0.5105521451930275, 'errorList': [], 'lossList': [0.0, -1.2409527486562728, 0.0, 6.50534620642662, 0.0, 0.0, 0.0], 'rewardMean': 0.669248553778431, 'totalEpisodes': 137, 'stepsPerEpisode': 323, 'rewardPerEpisode': 219.74336994232354
'totalSteps': 19200, 'rewardStep': 0.9042356579827437, 'errorList': [], 'lossList': [0.0, -1.2088700777292252, 0.0, 3.1279194982349874, 0.0, 0.0, 0.0], 'rewardMean': 0.666201216161513, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 999.8648855423669
'totalSteps': 20480, 'rewardStep': 0.7637592328625482, 'errorList': [], 'lossList': [0.0, -1.1614944303035737, 0.0, 2.406017774194479, 0.0, 0.0, 0.0], 'rewardMean': 0.6810808538762289, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.6387125815506
'totalSteps': 21760, 'rewardStep': 0.8060330625062275, 'errorList': [], 'lossList': [0.0, -1.095569960474968, 0.0, 1.6222645231336354, 0.0, 0.0, 0.0], 'rewardMean': 0.7147972623867164, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.3935035410761
'totalSteps': 23040, 'rewardStep': 0.972306468513128, 'errorList': [0.0475988443113817, 0.02407846634484631, 0.017269873218022692, 0.02748464602047232, 0.044017475941293585, 0.02020497703790074, 0.02944568774981325, 0.03434296691371323, 0.02647936751065481, 0.005599350795492166, 0.011684819816661494, 0.029913516344835495, 0.009280713158010134, 0.0129897651049791, 0.008243050440633544, 0.02877071014999167, 0.009780749168582625, 0.006493354593081417, 0.013132883029616365, 0.020686259645688036, 0.022759010057953707, 0.03738808430972417, 0.02803276518206147, 0.021866768883290668, 0.009361666651096692, 0.0241279116788416, 0.01056264645002142, 0.02097406248948125, 0.020419779355150262, 0.02579438592609318, 0.020449086922022703, 0.03160431465866357, 0.015241508233614698, 0.022060130016880886, 0.015037314543841251, 0.024840153034534556, 0.029784722038224395, 0.025053446241072527, 0.010572029412422499, 0.0438114463203594, 0.030909110881029073, 0.01610110476007904, 0.01912321014087235, 0.01615724740181066, 0.016524047563912847, 0.037850360442556186, 0.027564271122041763, 0.02093798958091158, 0.03962876564330555, 0.03504017147353004], 'lossList': [0.0, -1.0698799258470535, 0.0, 1.3207190139591694, 0.0, 0.0, 0.0], 'rewardMean': 0.7467812418043783, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1163.688754871339, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=84.97
