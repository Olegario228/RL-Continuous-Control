#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 60
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.9248644163331359, 'errorList': [], 'lossList': [0.0, -1.4073573076725006, 0.0, 27.20149599313736, 0.0, 0.0, 0.0], 'rewardMean': 0.8623316884160495, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.09096652239896
'totalSteps': 3840, 'rewardStep': 0.6380331282108834, 'errorList': [], 'lossList': [0.0, -1.399961821436882, 0.0, 33.834878063201906, 0.0, 0.0, 0.0], 'rewardMean': 0.787565501680994, 'totalEpisodes': 11, 'stepsPerEpisode': 263, 'rewardPerEpisode': 201.3859423005741
'totalSteps': 5120, 'rewardStep': 0.7043460157561164, 'errorList': [], 'lossList': [0.0, -1.4015581732988358, 0.0, 24.543422877788544, 0.0, 0.0, 0.0], 'rewardMean': 0.7667606301997747, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 956.2006034806814
'totalSteps': 6400, 'rewardStep': 0.8484601974460763, 'errorList': [], 'lossList': [0.0, -1.395039238333702, 0.0, 18.825911986231805, 0.0, 0.0, 0.0], 'rewardMean': 0.783100543649035, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.39144551867
'totalSteps': 7680, 'rewardStep': 0.7928478953487765, 'errorList': [], 'lossList': [0.0, -1.375085562467575, 0.0, 59.61205154895782, 0.0, 0.0, 0.0], 'rewardMean': 0.7847251022656586, 'totalEpisodes': 15, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.618360022774606
'totalSteps': 8960, 'rewardStep': 0.8129248472267545, 'errorList': [], 'lossList': [0.0, -1.3658619284629823, 0.0, 327.6121936035156, 0.0, 0.0, 0.0], 'rewardMean': 0.7887536372601008, 'totalEpisodes': 61, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.9826401697584526
'totalSteps': 10240, 'rewardStep': 0.5945126613640436, 'errorList': [], 'lossList': [0.0, -1.3609114408493042, 0.0, 118.01089012145997, 0.0, 0.0, 0.0], 'rewardMean': 0.7644735152730937, 'totalEpisodes': 105, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.0622203294636483
'totalSteps': 11520, 'rewardStep': 0.46279521189209394, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7041378545968937, 'totalEpisodes': 141, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.964995895860943
'totalSteps': 12800, 'rewardStep': 0.9804710754277252, 'errorList': [247.94257835462938, 252.40797057208727, 258.37827416682677, 222.5847964768051, 262.52968517185, 180.7119751664562, 276.18809831632825, 212.6509156141855, 272.7719167218679, 273.52085656898487, 247.79800742215275, 279.61637536522534, 263.22511488694767, 292.85730731502696, 209.1648703536396, 264.3716583969831, 288.6624047947335, 202.34929282219426, 210.4495887454855, 280.8237149954642, 266.540248031283, 208.32382946364694, 277.8585004660315, 230.61805773731695, 221.80332435109605, 278.64591607946556, 287.09067027668374, 237.2669821848737, 225.64635603937566, 286.9084767561302, 245.57536553343954, 236.89947282489288, 254.1868033424283, 248.991762724334, 288.72184406989, 288.2777331474221, 284.3225144691138, 234.1765215684309, 269.04136674765135, 202.06822101381195, 235.0747338169547, 172.86186923839813, 233.614066571484, 250.37104574199537, 276.15229173880783, 232.39119212685057, 287.8436737357752, 284.43448679119075, 295.1048227291531, 264.6281742941415], 'lossList': [0.0, -1.3516983300447465, 0.0, 61.33958520889282, 0.0, 0.0, 0.0], 'rewardMean': 0.7222050660897699, 'totalEpisodes': 182, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.938662752212466, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.7648740004236585, 'errorList': [], 'lossList': [0.0, -1.3479598838090896, 0.0, 42.8993030834198, 0.0, 0.0, 0.0], 'rewardMean': 0.7062060244988222, 'totalEpisodes': 204, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.451825694769568
'totalSteps': 15360, 'rewardStep': 0.7716189398563674, 'errorList': [], 'lossList': [0.0, -1.3404362726211547, 0.0, 38.439951705932614, 0.0, 0.0, 0.0], 'rewardMean': 0.7195646056633705, 'totalEpisodes': 223, 'stepsPerEpisode': 73, 'rewardPerEpisode': 55.78346781931074
'totalSteps': 16640, 'rewardStep': 0.9301870346198906, 'errorList': [160.86462742509275, 14.343694486517371, 96.56524679141363, 107.44679778654272, 186.0169912872379, 87.30215398698209, 143.51973103337954, 153.328084058306, 105.57189358004719, 199.35955778113444, 151.03527082981813, 28.62159224436542, 177.1766632611796, 130.89028674766803, 216.90532178169624, 152.75947626879895, 99.67940185121147, 127.90014609193524, 159.5839832739856, 176.5882637350411, 169.14502577014173, 112.81348469994012, 189.0093140670192, 106.6490324532232, 157.84594064388327, 130.90818946450074, 9.816189195336968, 76.34635612746949, 82.19941953853032, 83.98180710755405, 58.50221970114943, 163.45616429552672, 174.1306623972315, 110.88215319242077, 150.2510109082676, 58.18600084101451, 135.19194823820246, 178.2646671429749, 132.69437453381613, 156.65373338980334, 175.90440873445047, 124.50506650496452, 182.28717330909942, 78.20410252616105, 119.38000354152327, 220.5576764993652, 140.08515770350664, 101.12345166263907, 112.43376750379636, 99.3379428196986], 'lossList': [0.0, -1.3361585879325866, 0.0, 18.894827404022216, 0.0, 0.0, 0.0], 'rewardMean': 0.7421487075497482, 'totalEpisodes': 232, 'stepsPerEpisode': 113, 'rewardPerEpisode': 102.78821272448427, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.8460787770853575, 'errorList': [], 'lossList': [0.0, -1.3356213039159774, 0.0, 14.229073252677917, 0.0, 0.0, 0.0], 'rewardMean': 0.7419105655136762, 'totalEpisodes': 240, 'stepsPerEpisode': 156, 'rewardPerEpisode': 127.71786123058575
'totalSteps': 19200, 'rewardStep': 0.9121751778497674, 'errorList': [], 'lossList': [0.0, -1.3252234190702439, 0.0, 11.380099318027497, 0.0, 0.0, 0.0], 'rewardMean': 0.7538432937637752, 'totalEpisodes': 245, 'stepsPerEpisode': 160, 'rewardPerEpisode': 140.7594747471151
'totalSteps': 20480, 'rewardStep': 0.9440679430014284, 'errorList': [18.436867978906754, 0.6782193629016018, 113.70783968873462, 124.40716445594954, 1.1031142659467368, 29.549579188433558, 85.94072287948981, 33.01997365164547, 91.69437641569375, 8.18284099609639, 46.92337054017861, 141.6804827842735, 78.91079267265899, 79.74686574904038, 54.838974217914156, 21.769006698276204, 8.554361834684544, 2.197685456466574, 14.575097863616042, 118.4540121332947, 54.23025801982888, 3.0371536551071343, 72.59931077509675, 9.440212831913616, 2.743035307550899, 94.24275450868906, 96.60136053347713, 6.982591913429556, 96.76779068113882, 151.2280462498016, 4.415997321696309, 9.463388128988477, 120.523198846608, 86.42131132435547, 19.52926367795995, 91.69785873216038, 2.0673499058978324, 34.33401866487228, 91.80687862206037, 6.434079592864617, 105.72618539193589, 38.93884366128903, 146.32386736203853, 13.485814031761857, 12.227177395406017, 61.939032750720315, 0.1270739161746064, 64.08435462079815, 44.61061627966624, 124.7497250283623], 'lossList': [0.0, -1.3172940504550934, 0.0, 7.868186610937118, 0.0, 0.0, 0.0], 'rewardMean': 0.7669576033412426, 'totalEpisodes': 251, 'stepsPerEpisode': 27, 'rewardPerEpisode': 24.052460643876554, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.9570268190744793, 'errorList': [1.4180323195728188, 6.6521277598064685, 21.719843789366003, 71.31981924532242, 0.8245020901958159, 18.076735629080694, 72.9249850145661, 7.286392790356201, 49.57738361796572, 40.02149947380886, 120.81230942604421, 4.462957720514747, 23.107045672274502, 3.017729030786749, 40.21755112263804, 95.92125045637033, 86.53421382148332, 79.04900514378059, 7.1436825653954354, 8.4627765543579, 85.21898582777587, 2.56795693566138, 37.915467882719156, 64.63266215534722, 39.62008517038764, 32.62818064526247, 65.0936967149531, 15.458425517442214, 19.888160548618238, 0.13088004233641046, 13.781020088087924, 35.606953883251734, 107.51789286714774, 9.281538738082203, 47.6317861512204, 2.298568889057188, 0.600903062324436, 34.39540176346057, 58.671215616567025, 39.23184048996778, 0.12524502995285505, 0.2143651394112706, 34.087526802735916, 2.8092557642307554, 33.96403128475111, 43.22747207634923, 0.1714656974530587, 61.40295885850362, 1.074981005004899, 42.358972322460104], 'lossList': [0.0, -1.3159698307514192, 0.0, 6.790557407736778, 0.0, 0.0, 0.0], 'rewardMean': 0.8032090191122861, 'totalEpisodes': 254, 'stepsPerEpisode': 266, 'rewardPerEpisode': 235.75678302780665, 'successfulTests': 3
'totalSteps': 23040, 'rewardStep': 0.9113630006460173, 'errorList': [], 'lossList': [0.0, -1.3194352757930756, 0.0, 6.455490797758102, 0.0, 0.0, 0.0], 'rewardMean': 0.8480657979876784, 'totalEpisodes': 258, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.7215198077154605
'totalSteps': 24320, 'rewardStep': 0.7163009351357869, 'errorList': [], 'lossList': [0.0, -1.3145149672031402, 0.0, 4.239013674259186, 0.0, 0.0, 0.0], 'rewardMean': 0.8734163703120478, 'totalEpisodes': 261, 'stepsPerEpisode': 211, 'rewardPerEpisode': 168.3673568397902
'totalSteps': 25600, 'rewardStep': 0.8364636363789203, 'errorList': [], 'lossList': [0.0, -1.2815509057044983, 0.0, 4.610612787008286, 0.0, 0.0, 0.0], 'rewardMean': 0.8590156264071673, 'totalEpisodes': 263, 'stepsPerEpisode': 317, 'rewardPerEpisode': 281.11309877528674
#maxSuccessfulTests=3, maxSuccessfulTestsAtStep=21760, timeSpent=138.98
