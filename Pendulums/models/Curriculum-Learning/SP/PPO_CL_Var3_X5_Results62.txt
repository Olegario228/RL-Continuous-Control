#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 62
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.817552336829657, 'errorList': [], 'lossList': [0.0, -1.443952760696411, 0.0, 25.712413383722307, 0.0, 0.0, 0.0], 'rewardMean': 0.6277914645177655, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.7931220135748
'totalSteps': 3840, 'rewardStep': 0.8828496046248812, 'errorList': [], 'lossList': [0.0, -1.4527870959043503, 0.0, 40.35117565870285, 0.0, 0.0, 0.0], 'rewardMean': 0.7128108445534708, 'totalEpisodes': 14, 'stepsPerEpisode': 487, 'rewardPerEpisode': 392.7548601859025
'totalSteps': 5120, 'rewardStep': 0.6757187348225571, 'errorList': [], 'lossList': [0.0, -1.440088103413582, 0.0, 18.710867348909378, 0.0, 0.0, 0.0], 'rewardMean': 0.7035378171207423, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 904.6940466719079
'totalSteps': 6400, 'rewardStep': 0.449069541337584, 'errorList': [], 'lossList': [0.0, -1.4194559305906296, 0.0, 35.71639046192169, 0.0, 0.0, 0.0], 'rewardMean': 0.6526441619641106, 'totalEpisodes': 16, 'stepsPerEpisode': 558, 'rewardPerEpisode': 424.14682757950715
'totalSteps': 7680, 'rewardStep': 0.6016167283328606, 'errorList': [], 'lossList': [0.0, -1.3947082781791686, 0.0, 122.60426511764527, 0.0, 0.0, 0.0], 'rewardMean': 0.6441395896922356, 'totalEpisodes': 29, 'stepsPerEpisode': 58, 'rewardPerEpisode': 47.005703329947465
'totalSteps': 8960, 'rewardStep': 0.7139393378297085, 'errorList': [], 'lossList': [0.0, -1.3888026314973831, 0.0, 228.9323571777344, 0.0, 0.0, 0.0], 'rewardMean': 0.6541109822833031, 'totalEpisodes': 81, 'stepsPerEpisode': 72, 'rewardPerEpisode': 52.6057702216791
'totalSteps': 10240, 'rewardStep': 0.8008544832573735, 'errorList': [], 'lossList': [0.0, -1.3780114406347275, 0.0, 91.36287143707276, 0.0, 0.0, 0.0], 'rewardMean': 0.672453919905062, 'totalEpisodes': 115, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.681890929045522
'totalSteps': 11520, 'rewardStep': 0.6810428895895454, 'errorList': [], 'lossList': [0.0, -1.3796855878829957, 0.0, 71.5782228088379, 0.0, 0.0, 0.0], 'rewardMean': 0.6734082498700046, 'totalEpisodes': 140, 'stepsPerEpisode': 93, 'rewardPerEpisode': 67.59426850339916
'totalSteps': 12800, 'rewardStep': 0.6321958545867621, 'errorList': [], 'lossList': [0.0, -1.385511536002159, 0.0, 67.70786346435547, 0.0, 0.0, 0.0], 'rewardMean': 0.6692870103416804, 'totalEpisodes': 158, 'stepsPerEpisode': 101, 'rewardPerEpisode': 77.86625655142882
'totalSteps': 14080, 'rewardStep': 0.6185234053431116, 'errorList': [], 'lossList': [0.0, -1.396303755044937, 0.0, 21.55521014213562, 0.0, 0.0, 0.0], 'rewardMean': 0.6873362916554042, 'totalEpisodes': 167, 'stepsPerEpisode': 44, 'rewardPerEpisode': 34.64476043892388
'totalSteps': 15360, 'rewardStep': 0.8440859308088985, 'errorList': [], 'lossList': [0.0, -1.3900972872972488, 0.0, 34.976580667495725, 0.0, 0.0, 0.0], 'rewardMean': 0.6899896510533282, 'totalEpisodes': 177, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.648354919626101
'totalSteps': 16640, 'rewardStep': 0.8652588783054967, 'errorList': [], 'lossList': [0.0, -1.3806435871124267, 0.0, 17.071532056331634, 0.0, 0.0, 0.0], 'rewardMean': 0.6882305784213898, 'totalEpisodes': 183, 'stepsPerEpisode': 73, 'rewardPerEpisode': 62.369955515680786
'totalSteps': 17920, 'rewardStep': 0.3239485769523444, 'errorList': [], 'lossList': [0.0, -1.3743295168876648, 0.0, 22.105600395202636, 0.0, 0.0, 0.0], 'rewardMean': 0.6530535626343685, 'totalEpisodes': 187, 'stepsPerEpisode': 309, 'rewardPerEpisode': 235.57090273079868
'totalSteps': 19200, 'rewardStep': 0.8582243610197671, 'errorList': [], 'lossList': [0.0, -1.3534780359268188, 0.0, 15.587523440122604, 0.0, 0.0, 0.0], 'rewardMean': 0.6939690446025868, 'totalEpisodes': 192, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.196587684404836
'totalSteps': 20480, 'rewardStep': 0.571911543938036, 'errorList': [], 'lossList': [0.0, -1.3318149191141129, 0.0, 5.7345949339866635, 0.0, 0.0, 0.0], 'rewardMean': 0.6909985261631043, 'totalEpisodes': 195, 'stepsPerEpisode': 360, 'rewardPerEpisode': 291.39629541506656
'totalSteps': 21760, 'rewardStep': 0.7202276030842653, 'errorList': [], 'lossList': [0.0, -1.32424574136734, 0.0, 6.881787026226521, 0.0, 0.0, 0.0], 'rewardMean': 0.6916273526885601, 'totalEpisodes': 197, 'stepsPerEpisode': 359, 'rewardPerEpisode': 312.7055610028685
'totalSteps': 23040, 'rewardStep': 0.7586246787058674, 'errorList': [], 'lossList': [0.0, -1.3172382563352585, 0.0, 7.491351226568222, 0.0, 0.0, 0.0], 'rewardMean': 0.6874043722334094, 'totalEpisodes': 201, 'stepsPerEpisode': 228, 'rewardPerEpisode': 188.3377134031029
'totalSteps': 24320, 'rewardStep': 0.9668326570435487, 'errorList': [0.490052876260364, 0.6336266060165832, 1.0167786533955592, 1.151763203067772, 1.2905772281983192, 2.4170032832816797, 1.5113218940610211, 1.3847853383647895, 0.9046025331845825, 1.2276988887097542, 0.17697721329947713, 0.710419160290988, 0.4224650356844849, 0.5136313501472869, 1.4529846453897164, 0.23888301690831484, 0.4995936208349829, 0.570665119799697, 0.6807886353318559, 0.9869469220752378, 0.6697042838205653, 1.0976845065259833, 0.5664685640249478, 0.7775924996417004, 0.6812752495411294, 0.8254781440181853, 2.5699557703567715, 1.2362377091528831, 1.746182311923252, 0.9665621223710479, 0.7959052642695001, 1.6574403063381433, 1.046273068205604, 0.3825122131593359, 0.6302488532486151, 0.54732182755037, 0.69225519496516, 1.1496595016002789, 0.7018202163278167, 0.5285012852745219, 0.5735560045032734, 1.3157709309186016, 0.31070204245828187, 1.31157825031642, 2.9702662982195545, 1.0531114759873301, 1.7427409822741398, 1.4390555538455905, 0.2017930241170077, 1.2729595774188505], 'lossList': [0.0, -1.3115326821804048, 0.0, 4.076886817216873, 0.0, 0.0, 0.0], 'rewardMean': 0.7159833489788098, 'totalEpisodes': 203, 'stepsPerEpisode': 460, 'rewardPerEpisode': 399.84298982345035, 'successfulTests': 1
'totalSteps': 25600, 'rewardStep': 0.6796857370889888, 'errorList': [], 'lossList': [0.0, -1.3089327681064606, 0.0, 5.1842578738927845, 0.0, 0.0, 0.0], 'rewardMean': 0.7207323372290324, 'totalEpisodes': 205, 'stepsPerEpisode': 248, 'rewardPerEpisode': 201.3188532283971
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=24320, timeSpent=78.92
