#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 63
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6571566041489469, 'errorList': [], 'lossList': [0.0, -1.421940701007843, 0.0, 27.964865639209748, 0.0, 0.0, 0.0], 'rewardMean': 0.7112412185480015, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.154652308946247
'totalSteps': 3840, 'rewardStep': 0.9606809785786662, 'errorList': [], 'lossList': [0.0, -1.415666555762291, 0.0, 27.01202066540718, 0.0, 0.0, 0.0], 'rewardMean': 0.7943878052248897, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 373.6662006850443
'totalSteps': 5120, 'rewardStep': 0.8027346657113195, 'errorList': [], 'lossList': [0.0, -1.40975578725338, 0.0, 27.00848393380642, 0.0, 0.0, 0.0], 'rewardMean': 0.7964745203464971, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.789563192295
'totalSteps': 6400, 'rewardStep': 0.6382884536025405, 'errorList': [], 'lossList': [0.0, -1.392143896818161, 0.0, 17.727452446222305, 0.0, 0.0, 0.0], 'rewardMean': 0.7648373069977058, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.240390475873
'totalSteps': 7680, 'rewardStep': 0.7599483943573666, 'errorList': [], 'lossList': [0.0, -1.365172164440155, 0.0, 100.18946962356567, 0.0, 0.0, 0.0], 'rewardMean': 0.764022488224316, 'totalEpisodes': 26, 'stepsPerEpisode': 67, 'rewardPerEpisode': 50.08578828275836
'totalSteps': 8960, 'rewardStep': 0.6365122923762385, 'errorList': [], 'lossList': [0.0, -1.364085087776184, 0.0, 309.0509840393066, 0.0, 0.0, 0.0], 'rewardMean': 0.7458067459603049, 'totalEpisodes': 75, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.117468356914456
'totalSteps': 10240, 'rewardStep': 0.935255500242492, 'errorList': [253.7957971434724, 235.2051625142774, 236.90012992159748, 234.94880383011386, 178.21829484694075, 258.4632988479613, 215.53510634192907, 214.89805807832283, 131.5282918283165, 230.36979612682225, 258.2690712810905, 256.07661193298514, 193.9976130790423, 217.4057254908194, 237.89893376962857, 198.98429665313807, 251.75337873810395, 247.434550723723, 191.27106414490567, 242.318396235184, 231.5997458574558, 178.3663692881501, 269.8210621932561, 260.5598051456396, 249.04739326311858, 232.64246154005244, 242.41677862616763, 208.0334674573291, 224.77774283300712, 233.4017626485307, 260.6028163217543, 261.03710143665376, 243.57706104660713, 252.6395841499686, 184.55292372903776, 257.89303694669263, 243.37571148821246, 247.8968923759846, 246.70410460896025, 258.121209477984, 210.95941434635043, 240.2513117292636, 253.4142930016516, 262.0246908748989, 254.04790717216247, 277.32257575576074, 215.5220287737043, 243.6906254429086, 241.28948041665643, 247.3187578350879], 'lossList': [0.0, -1.3604237270355224, 0.0, 104.92339769363403, 0.0, 0.0, 0.0], 'rewardMean': 0.7694878402455783, 'totalEpisodes': 111, 'stepsPerEpisode': 42, 'rewardPerEpisode': 36.637836249997385, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.7276519938990734, 'errorList': [], 'lossList': [0.0, -1.355084965825081, 0.0, 52.88021935462952, 0.0, 0.0, 0.0], 'rewardMean': 0.7648394128737444, 'totalEpisodes': 139, 'stepsPerEpisode': 34, 'rewardPerEpisode': 25.399428243491712
'totalSteps': 12800, 'rewardStep': 0.33456484926543206, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6787358581447508, 'totalEpisodes': 153, 'stepsPerEpisode': 103, 'rewardPerEpisode': 57.42825982608053
'totalSteps': 14080, 'rewardStep': 0.23522322760345737, 'errorList': [], 'lossList': [0.0, -1.349225673675537, 0.0, 34.68815962791443, 0.0, 0.0, 0.0], 'rewardMean': 0.6365425204902018, 'totalEpisodes': 169, 'stepsPerEpisode': 131, 'rewardPerEpisode': 95.11768362599102
'totalSteps': 15360, 'rewardStep': 0.8458683687677272, 'errorList': [], 'lossList': [0.0, -1.341626699566841, 0.0, 37.89776771068573, 0.0, 0.0, 0.0], 'rewardMean': 0.6250612595091078, 'totalEpisodes': 178, 'stepsPerEpisode': 6, 'rewardPerEpisode': 5.17849503774995
'totalSteps': 16640, 'rewardStep': 0.7280572153572428, 'errorList': [], 'lossList': [0.0, -1.3233885759115218, 0.0, 10.581006698608398, 0.0, 0.0, 0.0], 'rewardMean': 0.6175935144737001, 'totalEpisodes': 180, 'stepsPerEpisode': 384, 'rewardPerEpisode': 257.0526517791462
'totalSteps': 17920, 'rewardStep': 0.38560162096039324, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5548901538697883, 'totalEpisodes': 181, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 927.1496866951194
'totalSteps': 19200, 'rewardStep': 0.9202746316074492, 'errorList': [], 'lossList': [0.0, -1.2910582107305526, 0.0, 14.413523980379104, 0.0, 0.0, 0.0], 'rewardMean': 0.5832663877929093, 'totalEpisodes': 182, 'stepsPerEpisode': 72, 'rewardPerEpisode': 58.6621512022831
'totalSteps': 20480, 'rewardStep': 0.5332293398112566, 'errorList': [], 'lossList': [0.0, -1.2832049876451492, 0.0, 9.831972382068635, 0.0, 0.0, 0.0], 'rewardMean': 0.5430637717497857, 'totalEpisodes': 183, 'stepsPerEpisode': 786, 'rewardPerEpisode': 530.8762532743939
'totalSteps': 21760, 'rewardStep': 0.733350536080313, 'errorList': [], 'lossList': [0.0, -1.2818824291229247, 0.0, 9.088057426214219, 0.0, 0.0, 0.0], 'rewardMean': 0.5436336259679098, 'totalEpisodes': 184, 'stepsPerEpisode': 893, 'rewardPerEpisode': 634.5834929215434
'totalSteps': 23040, 'rewardStep': 0.549182014200103, 'errorList': [], 'lossList': [0.0, -1.2606527853012084, 0.0, 1.5403592702746391, 0.0, 0.0, 0.0], 'rewardMean': 0.5650953424613768, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 916.9213278500557
'totalSteps': 24320, 'rewardStep': 0.7325979090238928, 'errorList': [], 'lossList': [0.0, -1.2328299683332444, 0.0, 1.8854455350339412, 0.0, 0.0, 0.0], 'rewardMean': 0.6048986484372229, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 992.8048498153871
'totalSteps': 25600, 'rewardStep': 0.842993940634785, 'errorList': [], 'lossList': [0.0, -1.2081835770606995, 0.0, 1.9345428385585546, 0.0, 0.0, 0.0], 'rewardMean': 0.6656757197403557, 'totalEpisodes': 184, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.708581454417
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=78.7
