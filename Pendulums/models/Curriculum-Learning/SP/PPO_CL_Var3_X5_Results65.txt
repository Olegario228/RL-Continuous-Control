#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 65
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.895581591208167, 'errorList': [], 'lossList': [0.0, -1.4293043220043182, 0.0, 83.43141898155213, 0.0, 0.0, 0.0], 'rewardMean': 0.895581591208167, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 100.8434843835356
'totalSteps': 2560, 'rewardStep': 0.9359728325804046, 'errorList': [], 'lossList': [0.0, -1.4348878401517868, 0.0, 30.503372081518172, 0.0, 0.0, 0.0], 'rewardMean': 0.9157772118942858, 'totalEpisodes': 8, 'stepsPerEpisode': 530, 'rewardPerEpisode': 378.1755505161005
'totalSteps': 3840, 'rewardStep': 0.7514780530911576, 'errorList': [], 'lossList': [0.0, -1.4202707833051682, 0.0, 32.707541060447696, 0.0, 0.0, 0.0], 'rewardMean': 0.8610108256265764, 'totalEpisodes': 11, 'stepsPerEpisode': 257, 'rewardPerEpisode': 204.51812961064036
'totalSteps': 5120, 'rewardStep': 0.7662666586198895, 'errorList': [], 'lossList': [0.0, -1.4163640701770783, 0.0, 27.12810863316059, 0.0, 0.0, 0.0], 'rewardMean': 0.8373247838749047, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.5051511062211
'totalSteps': 6400, 'rewardStep': 0.9643155344690332, 'errorList': [], 'lossList': [0.0, -1.40284520983696, 0.0, 21.017273052036764, 0.0, 0.0, 0.0], 'rewardMean': 0.8627229339937305, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.694544591151
'totalSteps': 7680, 'rewardStep': 0.7916612532902183, 'errorList': [], 'lossList': [0.0, -1.3943606132268906, 0.0, 78.5546821975708, 0.0, 0.0, 0.0], 'rewardMean': 0.8508793205431452, 'totalEpisodes': 16, 'stepsPerEpisode': 299, 'rewardPerEpisode': 226.79560257884756
'totalSteps': 8960, 'rewardStep': 0.9322904205794429, 'errorList': [111.19646246360534, 109.74312958678054, 111.18397431143933, 98.13258762060363, 104.69683929912382, 100.24851020187214, 104.89372998606686, 109.09640029405439, 100.92296302598976, 98.4169632154933, 107.61477541028894, 109.35693770172027, 108.24234081798723, 108.45398251799702, 109.89356902937791, 109.04570549712516, 110.2665397830623, 92.30655313337174, 106.30479769984183, 108.71180937423775, 103.3892404381343, 102.20660976997765, 106.64450369776108, 108.75862363869899, 107.07679926358443, 97.02569053637107, 103.77851787873135, 107.26537190896579, 110.01943802558333, 107.11437038925631, 102.99471495638353, 95.65504612487199, 97.81673570749409, 110.67329760983347, 110.15683002624907, 98.87740055709864, 99.5420956024524, 110.3560391450821, 97.57691373263111, 111.31194248054686, 103.37480767663423, 110.22943162909358, 110.5604996205805, 95.76728805006609, 97.2318328078705, 66.18748984534191, 108.96089294415285, 106.32568798669317, 105.21504034575415, 108.36930639497491], 'lossList': [0.0, -1.3862416762113572, 0.0, 444.76145454406736, 0.0, 0.0, 0.0], 'rewardMean': 0.8625094776911878, 'totalEpisodes': 65, 'stepsPerEpisode': 51, 'rewardPerEpisode': 43.63027371379346, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.7846848358647439, 'errorList': [], 'lossList': [0.0, -1.3855779749155044, 0.0, 278.9466345214844, 0.0, 0.0, 0.0], 'rewardMean': 0.8527813974628822, 'totalEpisodes': 107, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.837096131531236
'totalSteps': 11520, 'rewardStep': 0.5569075238036157, 'errorList': [], 'lossList': [0.0, -1.3859010326862335, 0.0, 180.6205512237549, 0.0, 0.0, 0.0], 'rewardMean': 0.8199065226118525, 'totalEpisodes': 151, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.553381279652843
'totalSteps': 12800, 'rewardStep': 0.9240365231764174, 'errorList': [], 'lossList': [0.0, -1.3840816557407378, 0.0, 99.07120967864991, 0.0, 0.0, 0.0], 'rewardMean': 0.8303195226683091, 'totalEpisodes': 186, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.934623778922468
'totalSteps': 14080, 'rewardStep': 0.6981932909559805, 'errorList': [], 'lossList': [0.0, -1.3871379882097243, 0.0, 58.533509464263915, 0.0, 0.0, 0.0], 'rewardMean': 0.8105806926430903, 'totalEpisodes': 208, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.188531201774429
'totalSteps': 15360, 'rewardStep': 0.6157796906548003, 'errorList': [], 'lossList': [0.0, -1.3902004581689835, 0.0, 48.786451482772826, 0.0, 0.0, 0.0], 'rewardMean': 0.77856137845053, 'totalEpisodes': 225, 'stepsPerEpisode': 68, 'rewardPerEpisode': 58.95282009851428
'totalSteps': 16640, 'rewardStep': 0.5045581032105115, 'errorList': [], 'lossList': [0.0, -1.393131674528122, 0.0, 26.787348499298094, 0.0, 0.0, 0.0], 'rewardMean': 0.7538693834624652, 'totalEpisodes': 234, 'stepsPerEpisode': 126, 'rewardPerEpisode': 95.32753179021786
'totalSteps': 17920, 'rewardStep': 0.8921170046372566, 'errorList': [], 'lossList': [0.0, -1.403653604388237, 0.0, 38.77132853984833, 0.0, 0.0, 0.0], 'rewardMean': 0.7664544180642021, 'totalEpisodes': 244, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.935204897551685
'totalSteps': 19200, 'rewardStep': 0.7429469384051268, 'errorList': [], 'lossList': [0.0, -1.4028904914855957, 0.0, 20.395033519268036, 0.0, 0.0, 0.0], 'rewardMean': 0.7443175584578114, 'totalEpisodes': 250, 'stepsPerEpisode': 428, 'rewardPerEpisode': 367.4620416756958
'totalSteps': 20480, 'rewardStep': 0.6502684223442587, 'errorList': [], 'lossList': [0.0, -1.407758184671402, 0.0, 17.21290913820267, 0.0, 0.0, 0.0], 'rewardMean': 0.7301782753632156, 'totalEpisodes': 255, 'stepsPerEpisode': 133, 'rewardPerEpisode': 103.15683838917398
'totalSteps': 21760, 'rewardStep': 0.8797822373386677, 'errorList': [], 'lossList': [0.0, -1.399425404071808, 0.0, 26.737709312438966, 0.0, 0.0, 0.0], 'rewardMean': 0.7249274570391379, 'totalEpisodes': 262, 'stepsPerEpisode': 63, 'rewardPerEpisode': 54.50837805070859
'totalSteps': 23040, 'rewardStep': 0.7029491571281178, 'errorList': [], 'lossList': [0.0, -1.4010355049371719, 0.0, 11.646315747499466, 0.0, 0.0, 0.0], 'rewardMean': 0.7167538891654753, 'totalEpisodes': 266, 'stepsPerEpisode': 145, 'rewardPerEpisode': 123.89746876021124
'totalSteps': 24320, 'rewardStep': 0.9516737416675596, 'errorList': [15.302048403711662, 9.291594738937807, 4.575975068712541, 6.614727314293128, 3.679917646351003, 3.6583707420796623, 4.302063878304421, 1.3252931628470972, 0.11438924461611188, 6.826145931802779, 20.829252524385698, 5.5790885708918605, 0.9313258659786954, 6.011176208029328, 26.159893227938177, 2.857306848322405, 1.7202874289720957, 20.576014060237576, 12.31583313498045, 1.5707570919026848, 17.795935824534492, 12.544083253744207, 11.028274601032503, 8.800572478518035, 9.303628330832115, 34.64237490601958, 7.2130815556164976, 11.169867455381958, 4.941308318616101, 3.38385247486826, 1.7512996490720112, 7.006228129077559, 0.786296441339494, 16.698279516743217, 19.825839649154386, 11.803115329192638, 8.511824055021066, 26.4424328530706, 31.72370368498407, 9.190310210044617, 2.2313717101163513, 21.875509368530324, 2.6229285507510816, 8.265567319318134, 13.787419236027041, 1.286224271423879, 11.86611300960433, 24.874873724782567, 0.07117544032296082, 30.692611650836493], 'lossList': [0.0, -1.3804397082328796, 0.0, 10.65614748120308, 0.0, 0.0, 0.0], 'rewardMean': 0.7562305109518697, 'totalEpisodes': 271, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.723507664728485, 'successfulTests': 2
'totalSteps': 25600, 'rewardStep': 0.1734654715717831, 'errorList': [], 'lossList': [0.0, -1.3585122966766356, 0.0, 5.087874850034714, 0.0, 0.0, 0.0], 'rewardMean': 0.6811734057914063, 'totalEpisodes': 274, 'stepsPerEpisode': 235, 'rewardPerEpisode': 164.20102541891293
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=24320, timeSpent=100.99
