#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 67
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.4777857752227982, 'errorList': [], 'lossList': [0.0, -1.4259126722812652, 0.0, 73.77557284832001, 0.0, 0.0, 0.0], 'rewardMean': 0.4777857752227982, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 172.19596951306696
'totalSteps': 2560, 'rewardStep': 0.8551707444476918, 'errorList': [], 'lossList': [0.0, -1.4427581012248993, 0.0, 27.665159254074098, 0.0, 0.0, 0.0], 'rewardMean': 0.666478259835245, 'totalEpisodes': 10, 'stepsPerEpisode': 906, 'rewardPerEpisode': 654.8125984903742
'totalSteps': 3840, 'rewardStep': 0.8450520375572059, 'errorList': [], 'lossList': [0.0, -1.459772054553032, 0.0, 42.96931900024414, 0.0, 0.0, 0.0], 'rewardMean': 0.726002852409232, 'totalEpisodes': 12, 'stepsPerEpisode': 666, 'rewardPerEpisode': 518.3181783474672
'totalSteps': 5120, 'rewardStep': 0.9023856314762106, 'errorList': [], 'lossList': [0.0, -1.464071934223175, 0.0, 27.083920362591744, 0.0, 0.0, 0.0], 'rewardMean': 0.7700985471759767, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1003.2258662761291
'totalSteps': 6400, 'rewardStep': 0.6273039578372985, 'errorList': [], 'lossList': [0.0, -1.448522049188614, 0.0, 31.213698909282684, 0.0, 0.0, 0.0], 'rewardMean': 0.7415396293082411, 'totalEpisodes': 13, 'stepsPerEpisode': 560, 'rewardPerEpisode': 453.3715262877327
'totalSteps': 7680, 'rewardStep': 0.582203421181748, 'errorList': [], 'lossList': [0.0, -1.4263922518491745, 0.0, 111.5396922492981, 0.0, 0.0, 0.0], 'rewardMean': 0.7149835946204922, 'totalEpisodes': 22, 'stepsPerEpisode': 42, 'rewardPerEpisode': 34.9773557270063
'totalSteps': 8960, 'rewardStep': 0.8061790262108616, 'errorList': [], 'lossList': [0.0, -1.4202133285999299, 0.0, 300.4096590423584, 0.0, 0.0, 0.0], 'rewardMean': 0.7280115134191164, 'totalEpisodes': 75, 'stepsPerEpisode': 16, 'rewardPerEpisode': 10.198416408685828
'totalSteps': 10240, 'rewardStep': 0.8667144581090379, 'errorList': [], 'lossList': [0.0, -1.4127161771059036, 0.0, 111.38046346664429, 0.0, 0.0, 0.0], 'rewardMean': 0.7453493815053566, 'totalEpisodes': 117, 'stepsPerEpisode': 51, 'rewardPerEpisode': 43.14427443781943
'totalSteps': 11520, 'rewardStep': 0.7650593476020753, 'errorList': [], 'lossList': [0.0, -1.4099676674604416, 0.0, 67.21350389480591, 0.0, 0.0, 0.0], 'rewardMean': 0.7475393777383252, 'totalEpisodes': 144, 'stepsPerEpisode': 44, 'rewardPerEpisode': 34.63457788699339
'totalSteps': 12800, 'rewardStep': 0.9959082909043447, 'errorList': [237.81749495370792, 166.80095928187197, 170.31739185310443, 150.39110592613517, 222.29062142219777, 242.59126330976358, 257.7838030457521, 262.70090113518705, 265.664867927659, 155.08154298310677, 114.60103321978971, 267.9218468545856, 190.33646246556916, 162.7093605329835, 90.79611567403151, 253.62650905294626, 253.11196061640888, 209.4602043483044, 282.2003532087614, 256.3278951777465, 256.4850283569932, 197.69615847687976, 187.23015269168428, 237.88057881275998, 256.8969950312958, 247.4802092707512, 253.170114181479, 251.2800727699281, 220.65339817266317, 197.14240445704937, 262.31641036648733, 212.21210231716927, 218.21794052212317, 138.22579731736576, 174.9355665264082, 156.50366879649613, 235.01467985144063, 219.45160420993523, 224.29434004588896, 250.86071585735755, 234.99569855922098, 230.02419470856984, 238.72272360285191, 254.76693819654295, 166.38740044235354, 186.28980321301447, 251.95547645323447, 267.0026945312697, 187.95781654133063, 274.9906336028513], 'lossList': [0.0, -1.4133342564105988, 0.0, 55.71311754226684, 0.0, 0.0, 0.0], 'rewardMean': 0.7723762690549273, 'totalEpisodes': 166, 'stepsPerEpisode': 32, 'rewardPerEpisode': 30.23526098107591, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.8573805384010256, 'errorList': [], 'lossList': [0.0, -1.4221833580732346, 0.0, 35.60403409004211, 0.0, 0.0, 0.0], 'rewardMean': 0.8103357453727501, 'totalEpisodes': 180, 'stepsPerEpisode': 44, 'rewardPerEpisode': 35.10604447838762
'totalSteps': 15360, 'rewardStep': 0.9290477513488148, 'errorList': [], 'lossList': [0.0, -1.432479909658432, 0.0, 33.4341173362732, 0.0, 0.0, 0.0], 'rewardMean': 0.8177234460628624, 'totalEpisodes': 189, 'stepsPerEpisode': 99, 'rewardPerEpisode': 77.11444408968633
'totalSteps': 16640, 'rewardStep': 0.6912689453801556, 'errorList': [], 'lossList': [0.0, -1.4241837638616561, 0.0, 16.391356782913206, 0.0, 0.0, 0.0], 'rewardMean': 0.8023451368451573, 'totalEpisodes': 196, 'stepsPerEpisode': 155, 'rewardPerEpisode': 132.71484365057012
'totalSteps': 17920, 'rewardStep': 0.7773731300352723, 'errorList': [], 'lossList': [0.0, -1.3983437025547027, 0.0, 25.65535099506378, 0.0, 0.0, 0.0], 'rewardMean': 0.7898438867010634, 'totalEpisodes': 203, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.456262781087876
'totalSteps': 19200, 'rewardStep': 0.4786139581417225, 'errorList': [], 'lossList': [0.0, -1.365533475279808, 0.0, 24.86521337747574, 0.0, 0.0, 0.0], 'rewardMean': 0.7749748867315058, 'totalEpisodes': 207, 'stepsPerEpisode': 339, 'rewardPerEpisode': 234.412901480692
'totalSteps': 20480, 'rewardStep': 0.48331138224507597, 'errorList': [], 'lossList': [0.0, -1.3325455051660537, 0.0, 26.991716926693915, 0.0, 0.0, 0.0], 'rewardMean': 0.7650856828378387, 'totalEpisodes': 210, 'stepsPerEpisode': 442, 'rewardPerEpisode': 369.7670533561241
'totalSteps': 21760, 'rewardStep': 0.6015503053804205, 'errorList': [], 'lossList': [0.0, -1.31333136677742, 0.0, 8.797091023921967, 0.0, 0.0, 0.0], 'rewardMean': 0.7446228107547945, 'totalEpisodes': 212, 'stepsPerEpisode': 800, 'rewardPerEpisode': 621.1593259485263
'totalSteps': 23040, 'rewardStep': 0.9122170587640325, 'errorList': [], 'lossList': [0.0, -1.2901789569854736, 0.0, 5.216313460767269, 0.0, 0.0, 0.0], 'rewardMean': 0.7491730708202942, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.4706904554325
'totalSteps': 24320, 'rewardStep': 0.9433797823576164, 'errorList': [0.09542497435065973, 0.1227018891259896, 0.09418184874151014, 0.08561469522774619, 0.09620877716663664, 0.07499834894099826, 0.07340595199448852, 0.0934559891320582, 0.08108547974309614, 0.10634666173062161, 0.12464519467072227, 0.07888663244176777, 0.08306475910283702, 0.0950237361991357, 0.09567231370703425, 0.09919105803463794, 0.14807594303026325, 0.08316736815692577, 0.10437699663881951, 0.11114223775160621, 0.11757587348672184, 0.09476396197446453, 0.07295512468761042, 0.09250499771041498, 0.07476080680912149, 0.10880678772206101, 0.11001604450494223, 0.11037010150499348, 0.11262327428150949, 0.10052826451005654, 0.099583046270554, 0.10355219329284773, 0.11176418458195238, 0.09918966214260903, 0.09057476254591966, 0.09341288734843879, 0.08800298683038052, 0.0740967825116685, 0.08975661784457345, 0.13963931159500698, 0.11400604068429654, 0.08507847790124018, 0.09628154446289326, 0.08815591497576963, 0.10055938620829497, 0.10710877747656058, 0.10948983811745519, 0.11601035711784027, 0.07261257713444617, 0.09989976435021009], 'lossList': [0.0, -1.252581096291542, 0.0, 3.992270735874772, 0.0, 0.0, 0.0], 'rewardMean': 0.7670051142958482, 'totalEpisodes': 212, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.3921853900065, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24320, timeSpent=98.03
