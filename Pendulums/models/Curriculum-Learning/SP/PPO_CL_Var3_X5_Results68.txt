#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 68
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8582121085555396, 'errorList': [], 'lossList': [0.0, -1.4206470352411271, 0.0, 68.74230011940003, 0.0, 0.0, 0.0], 'rewardMean': 0.8582121085555396, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.623383994113787
'totalSteps': 2560, 'rewardStep': 0.5846686724087011, 'errorList': [], 'lossList': [0.0, -1.4217230433225632, 0.0, 30.980060106515886, 0.0, 0.0, 0.0], 'rewardMean': 0.7214403904821204, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 31.438134529413777
'totalSteps': 3840, 'rewardStep': 0.9713201596868362, 'errorList': [], 'lossList': [0.0, -1.4213490653038026, 0.0, 31.979434378147126, 0.0, 0.0, 0.0], 'rewardMean': 0.8047336468836924, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 391.46637756789676
'totalSteps': 5120, 'rewardStep': 0.8323486458609061, 'errorList': [], 'lossList': [0.0, -1.4174350595474243, 0.0, 30.588583220243454, 0.0, 0.0, 0.0], 'rewardMean': 0.8116373966279958, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1071.4434640650093
'totalSteps': 6400, 'rewardStep': 0.7382076795189765, 'errorList': [], 'lossList': [0.0, -1.3989275366067886, 0.0, 20.795211565494537, 0.0, 0.0, 0.0], 'rewardMean': 0.796951453206192, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.924286069025
'totalSteps': 7680, 'rewardStep': 0.7954819604852995, 'errorList': [], 'lossList': [0.0, -1.3725217473506928, 0.0, 72.2742927312851, 0.0, 0.0, 0.0], 'rewardMean': 0.7967065377527099, 'totalEpisodes': 23, 'stepsPerEpisode': 107, 'rewardPerEpisode': 77.57170117929903
'totalSteps': 8960, 'rewardStep': 0.707317750466248, 'errorList': [], 'lossList': [0.0, -1.365657177567482, 0.0, 365.9535855102539, 0.0, 0.0, 0.0], 'rewardMean': 0.7839367109975012, 'totalEpisodes': 72, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.660810640975818
'totalSteps': 10240, 'rewardStep': 0.6760698033719713, 'errorList': [], 'lossList': [0.0, -1.3576642686128617, 0.0, 134.787067527771, 0.0, 0.0, 0.0], 'rewardMean': 0.7704533475443098, 'totalEpisodes': 125, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.297364127289576
'totalSteps': 11520, 'rewardStep': 0.787229714852428, 'errorList': [], 'lossList': [0.0, -1.3581200951337815, 0.0, 69.60932430267334, 0.0, 0.0, 0.0], 'rewardMean': 0.7723173883563229, 'totalEpisodes': 172, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.210557000103348
'totalSteps': 12800, 'rewardStep': 0.8546875008232306, 'errorList': [], 'lossList': [0.0, -1.3608842980861664, 0.0, 37.99970897674561, 0.0, 0.0, 0.0], 'rewardMean': 0.7805543996030136, 'totalEpisodes': 194, 'stepsPerEpisode': 23, 'rewardPerEpisode': 18.59184741270649
'totalSteps': 14080, 'rewardStep': 0.7029627185018861, 'errorList': [], 'lossList': [0.0, -1.3437040114402772, 0.0, 39.55389276504516, 0.0, 0.0, 0.0], 'rewardMean': 0.7650294605976483, 'totalEpisodes': 209, 'stepsPerEpisode': 103, 'rewardPerEpisode': 75.16041456929845
'totalSteps': 15360, 'rewardStep': 0.6053145073456832, 'errorList': [], 'lossList': [0.0, -1.321947721838951, 0.0, 28.735065195560455, 0.0, 0.0, 0.0], 'rewardMean': 0.7670940440913465, 'totalEpisodes': 221, 'stepsPerEpisode': 21, 'rewardPerEpisode': 14.924487613420865
'totalSteps': 16640, 'rewardStep': 0.44573503472552156, 'errorList': [], 'lossList': [0.0, -1.3254010939598084, 0.0, 10.056151903867722, 0.0, 0.0, 0.0], 'rewardMean': 0.714535531595215, 'totalEpisodes': 225, 'stepsPerEpisode': 405, 'rewardPerEpisode': 315.8562756844197
'totalSteps': 17920, 'rewardStep': 0.740591539170774, 'errorList': [], 'lossList': [0.0, -1.326133359670639, 0.0, 7.905595599412918, 0.0, 0.0, 0.0], 'rewardMean': 0.7053598209262019, 'totalEpisodes': 230, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.708758762985323
'totalSteps': 19200, 'rewardStep': 0.6962305089170487, 'errorList': [], 'lossList': [0.0, -1.3200207149982452, 0.0, 6.403217831850052, 0.0, 0.0, 0.0], 'rewardMean': 0.7011621038660091, 'totalEpisodes': 234, 'stepsPerEpisode': 137, 'rewardPerEpisode': 108.30353179878982
'totalSteps': 20480, 'rewardStep': 0.7421980873476539, 'errorList': [], 'lossList': [0.0, -1.3030455493927002, 0.0, 7.5380908870697025, 0.0, 0.0, 0.0], 'rewardMean': 0.6958337165522445, 'totalEpisodes': 236, 'stepsPerEpisode': 319, 'rewardPerEpisode': 250.17616793902656
'totalSteps': 21760, 'rewardStep': 0.903593333672301, 'errorList': [], 'lossList': [0.0, -1.2801925909519196, 0.0, 5.215461151599884, 0.0, 0.0, 0.0], 'rewardMean': 0.7154612748728499, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 989.6653082208694
'totalSteps': 23040, 'rewardStep': 0.7717850909737974, 'errorList': [], 'lossList': [0.0, -1.2327114701271058, 0.0, 3.1550221711397173, 0.0, 0.0, 0.0], 'rewardMean': 0.7250328036330325, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.1604278972097
'totalSteps': 24320, 'rewardStep': 0.7700191559603626, 'errorList': [], 'lossList': [0.0, -1.1791031521558761, 0.0, 2.5817056725919247, 0.0, 0.0, 0.0], 'rewardMean': 0.723311747743826, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.8053380636893
'totalSteps': 25600, 'rewardStep': 0.9891904949199102, 'errorList': [0.038069340456348764, 0.01742814811360184, 0.01797265398336457, 0.023810416809353484, 0.03199949294510705, 0.017710432242035997, 0.019627548122253525, 0.017039664261398496, 0.029200206928325898, 0.022390667000307604, 0.03589728754186993, 0.01694768870131866, 0.017158104934834297, 0.04487857384714979, 0.020591025552480904, 0.02107876976496156, 0.03384689219690907, 0.03062502197670217, 0.018955622893300265, 0.035569612649631474, 0.022524296702808824, 0.035914926630123974, 0.01662063521778222, 0.0174620146865701, 0.02223624268797654, 0.018019096656490135, 0.01802323383299575, 0.019821503557483414, 0.018421664547556824, 0.019918714054065707, 0.02005492580930248, 0.017047132680648298, 0.03260509910476659, 0.03058639396354716, 0.03340079863920761, 0.01908231379822633, 0.03155307810679141, 0.02893265362075138, 0.017678813595115254, 0.020404106691738248, 0.022596994110269844, 0.02296514817189179, 0.02698888793395132, 0.0314742773024852, 0.023542638017769027, 0.025035257612961704, 0.01929229981148071, 0.04290753241987317, 0.04321038571910412, 0.02483047360848097], 'lossList': [0.0, -1.1418078446388245, 0.0, 2.012763161584735, 0.0, 0.0, 0.0], 'rewardMean': 0.7367620471534938, 'totalEpisodes': 236, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.6944906404037, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=82.76
