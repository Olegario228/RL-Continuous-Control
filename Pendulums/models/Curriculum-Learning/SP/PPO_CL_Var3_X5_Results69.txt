#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 69
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7233509238793009, 'errorList': [], 'lossList': [0.0, -1.419480619430542, 0.0, 68.41126418113708, 0.0, 0.0, 0.0], 'rewardMean': 0.7233509238793009, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 108.83559939602664
'totalSteps': 2560, 'rewardStep': 0.8807536885511713, 'errorList': [], 'lossList': [0.0, -1.4239096134901046, 0.0, 34.10736844480038, 0.0, 0.0, 0.0], 'rewardMean': 0.802052306215236, 'totalEpisodes': 13, 'stepsPerEpisode': 311, 'rewardPerEpisode': 259.5119967855353
'totalSteps': 3840, 'rewardStep': 0.7152748877030167, 'errorList': [], 'lossList': [0.0, -1.4262228697538375, 0.0, 33.502629640102384, 0.0, 0.0, 0.0], 'rewardMean': 0.7731265000444963, 'totalEpisodes': 16, 'stepsPerEpisode': 174, 'rewardPerEpisode': 131.42556229312322
'totalSteps': 5120, 'rewardStep': 0.7005272885790238, 'errorList': [], 'lossList': [0.0, -1.4305617439746856, 0.0, 19.841788766384123, 0.0, 0.0, 0.0], 'rewardMean': 0.7549766971781282, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 935.6611700419793
'totalSteps': 6400, 'rewardStep': 0.8878983215451038, 'errorList': [], 'lossList': [0.0, -1.4079571002721787, 0.0, 16.14713418006897, 0.0, 0.0, 0.0], 'rewardMean': 0.7815610220515233, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 954.6657840041732
'totalSteps': 7680, 'rewardStep': 0.8161068924007319, 'errorList': [], 'lossList': [0.0, -1.4120262140035629, 0.0, 91.88545268058778, 0.0, 0.0, 0.0], 'rewardMean': 0.7873186671097248, 'totalEpisodes': 24, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.644445467250055
'totalSteps': 8960, 'rewardStep': 0.8181122403523434, 'errorList': [], 'lossList': [0.0, -1.4043041402101517, 0.0, 349.38338996887205, 0.0, 0.0, 0.0], 'rewardMean': 0.7917177490015275, 'totalEpisodes': 87, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.71488547063028
'totalSteps': 10240, 'rewardStep': 0.8968998284143349, 'errorList': [], 'lossList': [0.0, -1.3963123828172683, 0.0, 104.82737106323242, 0.0, 0.0, 0.0], 'rewardMean': 0.8048655089281284, 'totalEpisodes': 132, 'stepsPerEpisode': 36, 'rewardPerEpisode': 30.217504099300772
'totalSteps': 11520, 'rewardStep': 0.6818026560952534, 'errorList': [], 'lossList': [0.0, -1.379989966750145, 0.0, 69.99843683242798, 0.0, 0.0, 0.0], 'rewardMean': 0.7911918586133644, 'totalEpisodes': 168, 'stepsPerEpisode': 47, 'rewardPerEpisode': 35.94519462555482
'totalSteps': 12800, 'rewardStep': 0.5249700937252744, 'errorList': [], 'lossList': [0.0, -1.3617100352048874, 0.0, 57.749798183441165, 0.0, 0.0, 0.0], 'rewardMean': 0.7645696821245554, 'totalEpisodes': 191, 'stepsPerEpisode': 65, 'rewardPerEpisode': 53.04388121496059
'totalSteps': 14080, 'rewardStep': 0.6606013669246548, 'errorList': [], 'lossList': [0.0, -1.35238185942173, 0.0, 40.97760983467102, 0.0, 0.0, 0.0], 'rewardMean': 0.7582947264290909, 'totalEpisodes': 204, 'stepsPerEpisode': 71, 'rewardPerEpisode': 54.29509480062573
'totalSteps': 15360, 'rewardStep': 0.8165075764797458, 'errorList': [], 'lossList': [0.0, -1.34680771112442, 0.0, 50.79899216651916, 0.0, 0.0, 0.0], 'rewardMean': 0.7518701152219484, 'totalEpisodes': 217, 'stepsPerEpisode': 43, 'rewardPerEpisode': 35.50214739716742
'totalSteps': 16640, 'rewardStep': 0.7583061067529936, 'errorList': [], 'lossList': [0.0, -1.3323188936710357, 0.0, 37.618451390266415, 0.0, 0.0, 0.0], 'rewardMean': 0.7561732371269458, 'totalEpisodes': 228, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7583061067529936
'totalSteps': 17920, 'rewardStep': 0.7708789797561642, 'errorList': [], 'lossList': [0.0, -1.3322595566511155, 0.0, 12.113830287456512, 0.0, 0.0, 0.0], 'rewardMean': 0.76320840624466, 'totalEpisodes': 234, 'stepsPerEpisode': 194, 'rewardPerEpisode': 161.58873372546418
'totalSteps': 19200, 'rewardStep': 0.9650526503813892, 'errorList': [40.328084026319154, 20.897197241070085, 24.852828536489337, 5.82985843921815, 56.37070970357503, 41.5815409207266, 18.576280956670374, 17.81172528891288, 0.04503653325272838, 43.7972690559445, 3.6510120651073734, 3.404046402232535, 59.50661325040143, 7.590497410920559, 2.7753422253371545, 36.54501891692586, 52.05714039211591, 2.1366040518916867, 2.556735592009722, 19.53889880504698, 11.75622462366413, 5.320322725699588, 39.68804113777741, 14.813821786332333, 0.12753673109294733, 54.69164507255375, 32.025048170601224, 27.420205325551926, 12.145006078128228, 9.179672404574617, 22.995236841280754, 0.9902773319153778, 10.311647836369003, 59.805588259179025, 30.77566281987313, 21.122939055516394, 0.8702143438446891, 16.650252936308974, 8.36029825801933, 14.945769196688014, 5.079368804386993, 16.802714842588536, 31.29931883759763, 18.18835269994939, 4.593123994820622, 1.1254343609832609, 0.7275385467653386, 7.244717820505055, 7.215485788838473, 55.24552581731393], 'lossList': [0.0, -1.3333759552240372, 0.0, 20.3343399643898, 0.0, 0.0, 0.0], 'rewardMean': 0.7709238391282887, 'totalEpisodes': 242, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.8527576017525065, 'successfulTests': 2
'totalSteps': 20480, 'rewardStep': 0.9699613079883763, 'errorList': [17.565373964425895, 15.679882712383083, 37.173406538135, 9.172023558040802, 8.674632437231475, 8.57319434701221, 41.101889040354024, 23.2651885651228, 35.790633219447585, 12.164218128681878, 29.958843484811894, 7.194595003004079, 30.988320769616575, 16.64392883543335, 29.16268190032827, 15.267179869765695, 1.8975417072780938, 3.525013962452743, 26.967797106589167, 42.56775574992398, 30.28425892882153, 40.57061477189239, 12.64583532467866, 6.498121127599873, 12.803113757273064, 10.515757024832972, 10.77038882681086, 35.10435992041207, 12.618620523960653, 4.055886471259738, 40.18550018635787, 17.125685320389476, 40.94111732057582, 2.1674414340126344, 8.110266726021074, 24.046169815913466, 37.98397278882026, 17.649779792092705, 17.39163440968123, 26.225533292774887, 12.829785845604231, 37.14005590389477, 14.431909889695579, 31.27634004316706, 19.144191005271846, 18.64330306919312, 2.419221139892999, 20.658750689199554, 11.804234478634072, 12.697833867736357], 'lossList': [0.0, -1.3242626243829727, 0.0, 17.255364223122598, 0.0, 0.0, 0.0], 'rewardMean': 0.7863092806870531, 'totalEpisodes': 246, 'stepsPerEpisode': 42, 'rewardPerEpisode': 33.48345359556994, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.22466063684187704, 'errorList': [], 'lossList': [0.0, -1.32367318212986, 0.0, 5.7352260458469395, 0.0, 0.0, 0.0], 'rewardMean': 0.7269641203360064, 'totalEpisodes': 249, 'stepsPerEpisode': 362, 'rewardPerEpisode': 282.5330071333756
'totalSteps': 23040, 'rewardStep': 0.5068795132188993, 'errorList': [], 'lossList': [0.0, -1.3283768743276596, 0.0, 8.357813777923583, 0.0, 0.0, 0.0], 'rewardMean': 0.6879620888164628, 'totalEpisodes': 254, 'stepsPerEpisode': 173, 'rewardPerEpisode': 112.52910490179599
'totalSteps': 24320, 'rewardStep': 0.5721025275499521, 'errorList': [], 'lossList': [0.0, -1.319499715566635, 0.0, 3.760266410112381, 0.0, 0.0, 0.0], 'rewardMean': 0.6769920759619327, 'totalEpisodes': 258, 'stepsPerEpisode': 294, 'rewardPerEpisode': 253.1740959348674
'totalSteps': 25600, 'rewardStep': 0.9354474076711587, 'errorList': [0.49541869244225784, 0.48706317443896224, 0.41128026046314964, 0.04372093167394646, 0.5898366529616601, 0.9539402391707075, 0.12425528810677974, 0.10711027548858117, 0.12038018457506741, 0.059462206140250946, 0.7414036158688442, 0.2522250445182287, 0.09746047964642544, 0.26577003539806854, 0.6310401074512332, 0.08112060093243839, 0.46160376173981305, 0.14835810614331768, 0.6835745574538961, 0.229574762650658, 0.37508674807524556, 0.12314432780937305, 0.8069492539458044, 0.6090730921772558, 0.4440280247200178, 0.37282729818491683, 0.5285258664155039, 0.9648174314578201, 0.7443408221746703, 0.0531933604592094, 0.062201751357996105, 0.3904313274807791, 0.500655419489642, 0.2281486871099187, 0.23897811723985796, 0.2159585603468084, 0.23849854986305688, 0.4359461853614081, 0.04746195409104857, 0.0922015545202375, 0.8446768873457554, 0.6631537070379142, 0.23708289918996273, 0.3440529624195325, 0.21680379836893218, 0.753807108749045, 0.05123740664164187, 0.1529643426145852, 0.10530558167336497, 0.06486331153213469], 'lossList': [0.0, -1.3041951513290406, 0.0, 2.9624528521299363, 0.0, 0.0, 0.0], 'rewardMean': 0.7180398073565211, 'totalEpisodes': 261, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.800280389494649, 'successfulTests': 17
#maxSuccessfulTests=17, maxSuccessfulTestsAtStep=25600, timeSpent=120.69
