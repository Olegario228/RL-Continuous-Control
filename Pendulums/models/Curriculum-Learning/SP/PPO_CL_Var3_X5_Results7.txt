#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 7
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7937700011801512, 'errorList': [], 'lossList': [0.0, -1.4170372021198272, 0.0, 59.478896398544315, 0.0, 0.0, 0.0], 'rewardMean': 0.7937700011801512, 'totalEpisodes': 14, 'stepsPerEpisode': 222, 'rewardPerEpisode': 161.05631695916898
'totalSteps': 2560, 'rewardStep': 0.6016461351230915, 'errorList': [], 'lossList': [0.0, -1.4171767109632492, 0.0, 23.918690268993377, 0.0, 0.0, 0.0], 'rewardMean': 0.6977080681516213, 'totalEpisodes': 20, 'stepsPerEpisode': 134, 'rewardPerEpisode': 81.25512742074005
'totalSteps': 3840, 'rewardStep': 0.841214262877872, 'errorList': [], 'lossList': [0.0, -1.4055532318353654, 0.0, 26.610541751384734, 0.0, 0.0, 0.0], 'rewardMean': 0.745543466393705, 'totalEpisodes': 24, 'stepsPerEpisode': 76, 'rewardPerEpisode': 59.455637809592396
'totalSteps': 5120, 'rewardStep': 0.8785999131770099, 'errorList': [], 'lossList': [0.0, -1.379502055644989, 0.0, 17.111113471984865, 0.0, 0.0, 0.0], 'rewardMean': 0.7788075780895312, 'totalEpisodes': 30, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.13365347726461
'totalSteps': 6400, 'rewardStep': 0.950319510547307, 'errorList': [204.61477885719034, 196.4461560096412, 184.45673866879088, 180.17061954410568, 199.02240193369133, 198.0270223980033, 201.72766041110353, 200.90163861055373, 203.1868886421703, 186.98123443895042, 189.12202821707302, 198.16660615413002, 210.73317606942274, 195.08434513466568, 207.08942773689793, 190.55932252733317, 177.87286075115713, 196.02739444293425, 176.0241545506235, 203.48656234615024, 193.74278361148728, 209.22559136130755, 155.26193358009897, 209.19604691819362, 206.04453260756796, 200.48391284905793, 211.4176874505539, 191.52508413799893, 201.98645910953306, 183.20390175646196, 213.4738709246859, 206.46619756124431, 190.17137778700445, 200.53644067047796, 181.56714339176958, 212.08002625014257, 184.68695451894968, 187.04434239613886, 200.83447086029904, 202.96473522689925, 183.15041652107163, 186.64262692808725, 198.90917607905203, 190.21475124071628, 210.70284853030304, 200.21554281499775, 202.7133125866957, 182.55357738705158, 207.33587818898553, 193.62685114561936], 'lossList': [0.0, -1.3668928188085556, 0.0, 183.872932510376, 0.0, 0.0, 0.0], 'rewardMean': 0.8131099645810863, 'totalEpisodes': 74, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.950319510547307, 'successfulTests': 0
'totalSteps': 7680, 'rewardStep': 0.9169709439109465, 'errorList': [], 'lossList': [0.0, -1.3629973757266998, 0.0, 106.20123168945312, 0.0, 0.0, 0.0], 'rewardMean': 0.8304201278027298, 'totalEpisodes': 116, 'stepsPerEpisode': 21, 'rewardPerEpisode': 19.115295898776864
'totalSteps': 8960, 'rewardStep': 0.5025495246444628, 'errorList': [], 'lossList': [0.0, -1.3617379319667817, 0.0, 50.93724621772766, 0.0, 0.0, 0.0], 'rewardMean': 0.7835814702086916, 'totalEpisodes': 140, 'stepsPerEpisode': 35, 'rewardPerEpisode': 22.4387009599843
'totalSteps': 10240, 'rewardStep': 0.6118993490358229, 'errorList': [], 'lossList': [0.0, -1.361110087633133, 0.0, 23.541504480838775, 0.0, 0.0, 0.0], 'rewardMean': 0.7621212050620829, 'totalEpisodes': 149, 'stepsPerEpisode': 111, 'rewardPerEpisode': 81.7074344211632
'totalSteps': 11520, 'rewardStep': 0.8717860640015704, 'errorList': [], 'lossList': [0.0, -1.350574722290039, 0.0, 19.858017094135285, 0.0, 0.0, 0.0], 'rewardMean': 0.7743061893886927, 'totalEpisodes': 157, 'stepsPerEpisode': 65, 'rewardPerEpisode': 56.17470682261432
'totalSteps': 12800, 'rewardStep': 0.9612367944023824, 'errorList': [13.64588530291743, 11.106504777870716, 15.250022440939638, 8.604222084928502, 1.5326882778266369, 7.246149210514475, 14.247532450191718, 6.251807124164559, 6.468856136530796, 9.08421009694432, 2.841041616559181, 7.7308872764076755, 15.722403083100758, 5.9564150534753875, 0.8251411345030353, 3.424395681504144, 1.4018405711363866, 3.267491380454569, 3.977931923884715, 32.057370765341965, 0.18208203609072515, 1.4978367631426794, 0.386411783864421, 1.3532208817468914, 16.871877493096402, 3.631406390041211, 6.6803362903529875, 6.62846785203483, 5.19352937893935, 4.8303871993135195, 33.06862729528731, 2.6583916338605467, 15.765313414380905, 10.58340272699738, 3.0672460115297424, 4.863018851335153, 1.25297934422135, 8.572072406488504, 5.228212444326325, 11.340737305153786, 3.62840584848236, 0.5974462623619416, 10.33949268495344, 12.893788051508194, 12.769145218190662, 8.378740708864013, 9.434142750269702, 17.158929066605697, 13.031693300685252, 8.299967754775517], 'lossList': [0.0, -1.336030631661415, 0.0, 25.276471500396728, 0.0, 0.0, 0.0], 'rewardMean': 0.7929992498900617, 'totalEpisodes': 163, 'stepsPerEpisode': 147, 'rewardPerEpisode': 122.94266274730647, 'successfulTests': 1
'totalSteps': 14080, 'rewardStep': 0.4776513787653066, 'errorList': [], 'lossList': [0.0, -1.338763496875763, 0.0, 8.15360874414444, 0.0, 0.0, 0.0], 'rewardMean': 0.7613873876485772, 'totalEpisodes': 167, 'stepsPerEpisode': 169, 'rewardPerEpisode': 126.71155562094059
'totalSteps': 15360, 'rewardStep': 0.6487570194104435, 'errorList': [], 'lossList': [0.0, -1.3508738148212434, 0.0, 21.480649453401565, 0.0, 0.0, 0.0], 'rewardMean': 0.7660984760773124, 'totalEpisodes': 170, 'stepsPerEpisode': 422, 'rewardPerEpisode': 318.9660572597758
'totalSteps': 16640, 'rewardStep': 0.7432940216223176, 'errorList': [], 'lossList': [0.0, -1.3547026962041855, 0.0, 6.494140895903111, 0.0, 0.0, 0.0], 'rewardMean': 0.756306451951757, 'totalEpisodes': 171, 'stepsPerEpisode': 1200, 'rewardPerEpisode': 1022.7913018576407
'totalSteps': 17920, 'rewardStep': 0.7473424194136219, 'errorList': [], 'lossList': [0.0, -1.3421896409988403, 0.0, 3.382467430830002, 0.0, 0.0, 0.0], 'rewardMean': 0.7431807025754182, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 857.5578555426001
'totalSteps': 19200, 'rewardStep': 0.6152308974775109, 'errorList': [], 'lossList': [0.0, -1.308555822968483, 0.0, 3.1826669907569887, 0.0, 0.0, 0.0], 'rewardMean': 0.7096718412684385, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 842.3725864783626
'totalSteps': 20480, 'rewardStep': 0.7796278138223117, 'errorList': [], 'lossList': [0.0, -1.2783421576023102, 0.0, 2.2261540305614473, 0.0, 0.0, 0.0], 'rewardMean': 0.6959375282595752, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 913.4248001261
'totalSteps': 21760, 'rewardStep': 0.9178686352933322, 'errorList': [], 'lossList': [0.0, -1.2463617581129074, 0.0, 3.424620907977223, 0.0, 0.0, 0.0], 'rewardMean': 0.737469439324462, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.2337317094864
'totalSteps': 23040, 'rewardStep': 0.9677736592785342, 'errorList': [0.2076378818333187, 0.17558692377339302, 0.45294261834548044, 0.5100301945436131, 0.1954401849174209, 0.23285837125759634, 0.20638454462407518, 0.5213806971033582, 0.3903762204621771, 0.20553375769903726, 0.2901275791619367, 0.47168072283080176, 0.3114435315723374, 0.4117870447753042, 0.4860066695865053, 0.327534671679461, 0.17455052482625785, 0.44518323406078264, 0.542019240133234, 0.20474304081264527, 0.21645043357123797, 0.46573461780831465, 0.6011450364929313, 0.19407279912915593, 0.23000864953887257, 0.49967674369371257, 0.353880706302567, 0.27073781557330123, 0.5046677190340159, 0.2523818174835678, 0.5305615710282575, 0.2179067076084999, 0.31691518171747185, 0.5629531534321609, 0.25819079494266456, 0.28447583636956375, 0.5173300211965134, 0.2601607469348568, 0.3249026583334744, 0.4091147864415765, 0.18833886592601248, 0.4511946369322949, 0.473202972757641, 0.49507157481478, 0.19619732762799275, 0.39024135625026246, 0.3396327673940906, 0.37664761171205685, 0.5910745643101634, 0.17526958006192417], 'lossList': [0.0, -1.204377108812332, 0.0, 2.987425277940929, 0.0, 0.0, 0.0], 'rewardMean': 0.7730568703487333, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1205.0869536105613, 'successfulTests': 7
'totalSteps': 24320, 'rewardStep': 0.6986871211639891, 'errorList': [], 'lossList': [0.0, -1.1907914292812347, 0.0, 1.2238303303718567, 0.0, 0.0, 0.0], 'rewardMean': 0.7557469760649751, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.4551500925022
'totalSteps': 25600, 'rewardStep': 0.8269087834312874, 'errorList': [], 'lossList': [0.0, -1.1913554990291595, 0.0, 1.0897760229744018, 0.0, 0.0, 0.0], 'rewardMean': 0.7423141749678654, 'totalEpisodes': 171, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.1670250104798
#maxSuccessfulTests=7, maxSuccessfulTestsAtStep=23040, timeSpent=108.77
