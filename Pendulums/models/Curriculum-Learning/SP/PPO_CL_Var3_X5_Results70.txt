#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 70
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9119870968047733, 'errorList': [], 'lossList': [0.0, -1.4368716353178024, 0.0, 31.963775281906127, 0.0, 0.0, 0.0], 'rewardMean': 0.9134038636928007, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 383.4795940759268
'totalSteps': 3840, 'rewardStep': 0.7194446286459995, 'errorList': [], 'lossList': [0.0, -1.4211593759059906, 0.0, 31.167446579933166, 0.0, 0.0, 0.0], 'rewardMean': 0.8487507853438668, 'totalEpisodes': 12, 'stepsPerEpisode': 268, 'rewardPerEpisode': 211.23073759537291
'totalSteps': 5120, 'rewardStep': 0.7353217434813949, 'errorList': [], 'lossList': [0.0, -1.4188472366333007, 0.0, 26.510116696357727, 0.0, 0.0, 0.0], 'rewardMean': 0.8203935248782489, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1000.1779510967784
'totalSteps': 6400, 'rewardStep': 0.9243553036426532, 'errorList': [], 'lossList': [0.0, -1.4077788358926773, 0.0, 23.245953181982042, 0.0, 0.0, 0.0], 'rewardMean': 0.8411858806311298, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.6835210601
'totalSteps': 7680, 'rewardStep': 0.8873682468386629, 'errorList': [], 'lossList': [0.0, -1.391972354054451, 0.0, 89.9627686214447, 0.0, 0.0, 0.0], 'rewardMean': 0.8488829416657185, 'totalEpisodes': 18, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.188774764815296
'totalSteps': 8960, 'rewardStep': 0.9123028149354245, 'errorList': [], 'lossList': [0.0, -1.3856197339296341, 0.0, 327.9087243652344, 0.0, 0.0, 0.0], 'rewardMean': 0.8579429235613908, 'totalEpisodes': 56, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.649160796166422
'totalSteps': 10240, 'rewardStep': 0.4542648862580513, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7682366930495377, 'totalEpisodes': 103, 'stepsPerEpisode': 7, 'rewardPerEpisode': 3.8351229633758734
'totalSteps': 11520, 'rewardStep': 0.8897286497228133, 'errorList': [], 'lossList': [0.0, -1.3787889581918717, 0.0, 191.62330936431886, 0.0, 0.0, 0.0], 'rewardMean': 0.7803858887168652, 'totalEpisodes': 157, 'stepsPerEpisode': 24, 'rewardPerEpisode': 22.558806877775158
'totalSteps': 12800, 'rewardStep': 0.5515981533567986, 'errorList': [], 'lossList': [0.0, -1.3664821565151215, 0.0, 81.42446321487427, 0.0, 0.0, 0.0], 'rewardMean': 0.7440636409944623, 'totalEpisodes': 207, 'stepsPerEpisode': 26, 'rewardPerEpisode': 19.949513595845595
'totalSteps': 14080, 'rewardStep': 0.6084675338109651, 'errorList': [], 'lossList': [0.0, -1.3638179624080657, 0.0, 50.76547553062439, 0.0, 0.0, 0.0], 'rewardMean': 0.7137116846950814, 'totalEpisodes': 232, 'stepsPerEpisode': 58, 'rewardPerEpisode': 49.94313594354488
'totalSteps': 15360, 'rewardStep': 0.5648918318264022, 'errorList': [], 'lossList': [0.0, -1.36038303732872, 0.0, 47.64131058692932, 0.0, 0.0, 0.0], 'rewardMean': 0.6982564050131217, 'totalEpisodes': 254, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.999842125346094
'totalSteps': 16640, 'rewardStep': 0.24523312227950222, 'errorList': [], 'lossList': [0.0, -1.3503793048858643, 0.0, 21.18531280040741, 0.0, 0.0, 0.0], 'rewardMean': 0.6492475428929324, 'totalEpisodes': 263, 'stepsPerEpisode': 142, 'rewardPerEpisode': 104.39757583924882
'totalSteps': 17920, 'rewardStep': 0.793158509254838, 'errorList': [], 'lossList': [0.0, -1.3401064610481261, 0.0, 15.249919002056123, 0.0, 0.0, 0.0], 'rewardMean': 0.636127863454151, 'totalEpisodes': 271, 'stepsPerEpisode': 138, 'rewardPerEpisode': 110.47144469744211
'totalSteps': 19200, 'rewardStep': 0.3650019443566426, 'errorList': [], 'lossList': [0.0, -1.3286872202157973, 0.0, 22.673497076034547, 0.0, 0.0, 0.0], 'rewardMean': 0.5838912332059489, 'totalEpisodes': 276, 'stepsPerEpisode': 427, 'rewardPerEpisode': 318.1088612987529
'totalSteps': 20480, 'rewardStep': 0.7031270886280598, 'errorList': [], 'lossList': [0.0, -1.3089273089170457, 0.0, 23.351184089183807, 0.0, 0.0, 0.0], 'rewardMean': 0.5629736605752125, 'totalEpisodes': 280, 'stepsPerEpisode': 164, 'rewardPerEpisode': 124.7452043372736
'totalSteps': 21760, 'rewardStep': 0.9364782659174246, 'errorList': [0.4151979399934103, 0.3268497622235793, 0.3779317296039589, 0.15449473420456278, 0.5607577401513069, 0.09313662045751195, 0.10901987879824336, 0.24395620672996707, 0.111141997618806, 0.17483189009675063, 0.47613873633754034, 0.08187952486254098, 0.26939689551583895, 0.6464972000512342, 0.07564920205080625, 0.6151484312242973, 0.3743103272216043, 0.14940778920993603, 0.3842661540731788, 0.4172334500614172, 0.07739042954950791, 0.5644813904323452, 0.11094119699739945, 0.17369869128165025, 0.2155588728692952, 0.18895983940243616, 0.3506515525988514, 0.25109919549612225, 0.10266457590523213, 0.5500558633869586, 0.2985451245597848, 0.25592849741421675, 0.08794193249138922, 0.6053977694637132, 0.03860634044017969, 0.04011302298415299, 0.14408890901716193, 0.18872676563355198, 0.10552827768085818, 0.5402151309131824, 0.0912593415772593, 0.3133646998748911, 0.6258130470719176, 0.17312389914629359, 0.12776363105196006, 0.2310593634622542, 0.1836471902812304, 0.08109999574030104, 0.14832812769696402, 0.4973366204697851], 'lossList': [0.0, -1.292260622382164, 0.0, 24.169559156894685, 0.0, 0.0, 0.0], 'rewardMean': 0.6111949985411498, 'totalEpisodes': 283, 'stepsPerEpisode': 62, 'rewardPerEpisode': 56.27793690441302, 'successfulTests': 25
'totalSteps': 23040, 'rewardStep': 0.47974585760540184, 'errorList': [], 'lossList': [0.0, -1.2922616821527482, 0.0, 6.081506962180137, 0.0, 0.0, 0.0], 'rewardMean': 0.6137430956758847, 'totalEpisodes': 283, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.9969715414454
'totalSteps': 24320, 'rewardStep': 0.832388124784518, 'errorList': [], 'lossList': [0.0, -1.270895209312439, 0.0, 4.617670573145151, 0.0, 0.0, 0.0], 'rewardMean': 0.6080090431820553, 'totalEpisodes': 283, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.6739455618629
'totalSteps': 25600, 'rewardStep': 0.8733450754033665, 'errorList': [], 'lossList': [0.0, -1.233099592924118, 0.0, 2.690683261305094, 0.0, 0.0, 0.0], 'rewardMean': 0.6401837353867121, 'totalEpisodes': 283, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.5612534898155
#maxSuccessfulTests=25, maxSuccessfulTestsAtStep=21760, timeSpent=78.11
