#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 73
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5743571833397343, 'errorList': [], 'lossList': [0.0, -1.4227774053812028, 0.0, 32.654014377593995, 0.0, 0.0, 0.0], 'rewardMean': 0.7366437995832373, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.26798297864301
'totalSteps': 3840, 'rewardStep': 0.964239902513621, 'errorList': [], 'lossList': [0.0, -1.423566682934761, 0.0, 35.74605711936951, 0.0, 0.0, 0.0], 'rewardMean': 0.8125091672266986, 'totalEpisodes': 18, 'stepsPerEpisode': 487, 'rewardPerEpisode': 403.51376465038953
'totalSteps': 5120, 'rewardStep': 0.8507265909850596, 'errorList': [], 'lossList': [0.0, -1.420801618695259, 0.0, 32.834689078330996, 0.0, 0.0, 0.0], 'rewardMean': 0.822063523166289, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.8264506995035
'totalSteps': 6400, 'rewardStep': 0.8046100409689688, 'errorList': [], 'lossList': [0.0, -1.4026148462295531, 0.0, 23.344995158910752, 0.0, 0.0, 0.0], 'rewardMean': 0.818572826726825, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1101.5489490790906
'totalSteps': 7680, 'rewardStep': 0.8584564445748334, 'errorList': [], 'lossList': [0.0, -1.3862013274431229, 0.0, 26.765070971250534, 0.0, 0.0, 0.0], 'rewardMean': 0.8252200963681596, 'totalEpisodes': 19, 'stepsPerEpisode': 107, 'rewardPerEpisode': 83.8483289791937
'totalSteps': 8960, 'rewardStep': 0.9824015325746004, 'errorList': [75.24866608354955, 77.75743573261227, 75.8963502563616, 76.07203363462887, 74.06092948516611, 73.81375169456851, 75.01111079536427, 80.86996631038713, 75.56215037141985, 74.49745328478762, 80.3299363952644, 77.87891927588778, 75.69640826751196, 75.8334305587227, 76.31226429178594, 75.95540737335457, 80.51694425279331, 73.1279580881275, 74.65673381868469, 76.00658093284709, 69.2113752600158, 75.38270680580177, 71.3830434881985, 78.94532266425766, 78.54179852469896, 79.77711915131238, 79.01744173259733, 75.60567790665345, 78.49401043937772, 79.7451791395822, 72.12165109504127, 79.69058724822592, 78.97084513612968, 77.94894801235493, 78.7282806225194, 56.25821031231276, 73.81373489759906, 79.07216893162185, 78.42617358104174, 74.7244565801618, 77.47839206495412, 80.15250862093163, 77.99344066227991, 78.7847941242368, 71.94197554064417, 76.3491827723093, 80.17740898316265, 76.09480547260657, 76.67805745510138, 76.366872819468], 'lossList': [0.0, -1.3811782294511794, 0.0, 513.9710700988769, 0.0, 0.0, 0.0], 'rewardMean': 0.847674587254794, 'totalEpisodes': 67, 'stepsPerEpisode': 42, 'rewardPerEpisode': 38.131306057422485, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.850276970377375, 'errorList': [], 'lossList': [0.0, -1.377794536948204, 0.0, 217.74206104278565, 0.0, 0.0, 0.0], 'rewardMean': 0.8479998851451167, 'totalEpisodes': 122, 'stepsPerEpisode': 44, 'rewardPerEpisode': 36.72573015167402
'totalSteps': 11520, 'rewardStep': 0.6847521184602501, 'errorList': [], 'lossList': [0.0, -1.3690085518360138, 0.0, 78.8587844657898, 0.0, 0.0, 0.0], 'rewardMean': 0.8298612444023538, 'totalEpisodes': 160, 'stepsPerEpisode': 17, 'rewardPerEpisode': 13.342735172527053
'totalSteps': 12800, 'rewardStep': 0.848743394717483, 'errorList': [], 'lossList': [0.0, -1.3590393620729446, 0.0, 62.98889636993408, 0.0, 0.0, 0.0], 'rewardMean': 0.8317494594338667, 'totalEpisodes': 190, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.205903907555726
'totalSteps': 14080, 'rewardStep': 0.8828579391542253, 'errorList': [], 'lossList': [0.0, -1.3498306912183762, 0.0, 39.533947010040286, 0.0, 0.0, 0.0], 'rewardMean': 0.8301422117666151, 'totalEpisodes': 202, 'stepsPerEpisode': 65, 'rewardPerEpisode': 53.86228655577768
'totalSteps': 15360, 'rewardStep': 0.6265439410246056, 'errorList': [], 'lossList': [0.0, -1.3378094279766082, 0.0, 37.3294337272644, 0.0, 0.0, 0.0], 'rewardMean': 0.8353608875351022, 'totalEpisodes': 212, 'stepsPerEpisode': 94, 'rewardPerEpisode': 63.53585371368992
'totalSteps': 16640, 'rewardStep': 0.5105756290938718, 'errorList': [], 'lossList': [0.0, -1.328992720246315, 0.0, 17.553597345352173, 0.0, 0.0, 0.0], 'rewardMean': 0.7899944601931272, 'totalEpisodes': 217, 'stepsPerEpisode': 245, 'rewardPerEpisode': 175.82020102401333
'totalSteps': 17920, 'rewardStep': 0.7528357430786774, 'errorList': [], 'lossList': [0.0, -1.3142572617530823, 0.0, 8.756912326216698, 0.0, 0.0, 0.0], 'rewardMean': 0.7802053754024891, 'totalEpisodes': 220, 'stepsPerEpisode': 461, 'rewardPerEpisode': 384.93278412185396
'totalSteps': 19200, 'rewardStep': 0.6861896282637092, 'errorList': [], 'lossList': [0.0, -1.294158374071121, 0.0, 5.344822480678558, 0.0, 0.0, 0.0], 'rewardMean': 0.7683633341319631, 'totalEpisodes': 223, 'stepsPerEpisode': 270, 'rewardPerEpisode': 213.55781139937827
'totalSteps': 20480, 'rewardStep': 0.7394000375489578, 'errorList': [], 'lossList': [0.0, -1.277489547729492, 0.0, 18.27435306787491, 0.0, 0.0, 0.0], 'rewardMean': 0.7564576934293755, 'totalEpisodes': 226, 'stepsPerEpisode': 331, 'rewardPerEpisode': 271.05752467545597
'totalSteps': 21760, 'rewardStep': 0.9024823069840574, 'errorList': [], 'lossList': [0.0, -1.2615548527240754, 0.0, 6.776986104249954, 0.0, 0.0, 0.0], 'rewardMean': 0.7484657708703213, 'totalEpisodes': 227, 'stepsPerEpisode': 671, 'rewardPerEpisode': 584.7035848990822
'totalSteps': 23040, 'rewardStep': 0.6921652033908221, 'errorList': [], 'lossList': [0.0, -1.2487252783775329, 0.0, 5.020341604948044, 0.0, 0.0, 0.0], 'rewardMean': 0.732654594171666, 'totalEpisodes': 229, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.752388459865306
'totalSteps': 24320, 'rewardStep': 0.5355704308185181, 'errorList': [], 'lossList': [0.0, -1.2224238014221191, 0.0, 2.2613221229612828, 0.0, 0.0, 0.0], 'rewardMean': 0.7177364254074927, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1062.4754377372074
'totalSteps': 25600, 'rewardStep': 0.9720338633808175, 'errorList': [0.030626974029153715, 0.03216964446191774, 0.03322280493410898, 0.027792101230184645, 0.03147923562346092, 0.02796359112982642, 0.03640990135554359, 0.03388623716211384, 0.03120215244878974, 0.028055474704469185, 0.027863928076588256, 0.04319105907980543, 0.02882922914526105, 0.03747804443975023, 0.028097970848839724, 0.03592183574691991, 0.02786281086693953, 0.0365568459889134, 0.0379901148936651, 0.04081896781403221, 0.034965505202101635, 0.034330657835852374, 0.027714285371777926, 0.028191730198689216, 0.03108813962621942, 0.02789133715220907, 0.03045456447144144, 0.03672902540680535, 0.03601001155786127, 0.031561709798370176, 0.028659977189221888, 0.033849818811795794, 0.03136692476634511, 0.02800379092578793, 0.03653524710457785, 0.028128125613116277, 0.030177800382676805, 0.0288342370705981, 0.03170224773742234, 0.0375182966414222, 0.02963784576016678, 0.027717779925382588, 0.03212804912312735, 0.02819020500079318, 0.028354200886648147, 0.028122556802111932, 0.027756483763191907, 0.027836283223410117, 0.037426241934672894, 0.03042072448870259], 'lossList': [0.0, -1.1878439247608186, 0.0, 2.019337628185749, 0.0, 0.0, 0.0], 'rewardMean': 0.7300654722738262, 'totalEpisodes': 229, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1118.7194379336365, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=101.6
