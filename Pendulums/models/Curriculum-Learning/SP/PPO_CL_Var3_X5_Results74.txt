#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 7000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 74
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 7000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8202022319801423, 'errorList': [], 'lossList': [0.0, -1.4210883539915085, 0.0, 28.441605204343794, 0.0, 0.0, 0.0], 'rewardMean': 0.7847465481397117, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9483502789903
'totalSteps': 3840, 'rewardStep': 0.7650720646167022, 'errorList': [], 'lossList': [0.0, -1.4348094528913498, 0.0, 35.15623064041138, 0.0, 0.0, 0.0], 'rewardMean': 0.7781883869653751, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.34713148942757
'totalSteps': 5120, 'rewardStep': 0.6978784862631658, 'errorList': [], 'lossList': [0.0, -1.4309153425693513, 0.0, 25.5285982465744, 0.0, 0.0, 0.0], 'rewardMean': 0.7581109117898228, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.1025225726307
'totalSteps': 6400, 'rewardStep': 0.9062771974346564, 'errorList': [], 'lossList': [0.0, -1.4269216084480285, 0.0, 16.742627209424974, 0.0, 0.0, 0.0], 'rewardMean': 0.7877441689187895, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 991.2804398853809
'totalSteps': 7680, 'rewardStep': 0.8903595735440717, 'errorList': [], 'lossList': [0.0, -1.4256191730499268, 0.0, 38.52212861061096, 0.0, 0.0, 0.0], 'rewardMean': 0.8048467363563366, 'totalEpisodes': 17, 'stepsPerEpisode': 192, 'rewardPerEpisode': 137.33888035310133
'totalSteps': 8960, 'rewardStep': 0.7424401942997725, 'errorList': [], 'lossList': [0.0, -1.4201540207862855, 0.0, 430.30315574645994, 0.0, 0.0, 0.0], 'rewardMean': 0.7959315160625418, 'totalEpisodes': 72, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.406246492543536
'totalSteps': 10240, 'rewardStep': 0.8399514273197647, 'errorList': [], 'lossList': [0.0, -1.4149526584148406, 0.0, 151.27544300079344, 0.0, 0.0, 0.0], 'rewardMean': 0.8014340049696945, 'totalEpisodes': 121, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.160260893741646
'totalSteps': 11520, 'rewardStep': 0.8914569983103784, 'errorList': [], 'lossList': [0.0, -1.4022035962343216, 0.0, 98.47894161224366, 0.0, 0.0, 0.0], 'rewardMean': 0.811436559785326, 'totalEpisodes': 168, 'stepsPerEpisode': 20, 'rewardPerEpisode': 17.736728764354567
'totalSteps': 12800, 'rewardStep': 0.8600089517640032, 'errorList': [], 'lossList': [0.0, -1.3936967271566392, 0.0, 51.726543254852295, 0.0, 0.0, 0.0], 'rewardMean': 0.8162937989831937, 'totalEpisodes': 191, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.529299478238675
'totalSteps': 14080, 'rewardStep': 0.7382656251769867, 'errorList': [], 'lossList': [0.0, -1.372737203836441, 0.0, 42.55265856742859, 0.0, 0.0, 0.0], 'rewardMean': 0.8151912750709643, 'totalEpisodes': 205, 'stepsPerEpisode': 46, 'rewardPerEpisode': 38.508951398185324
'totalSteps': 15360, 'rewardStep': 0.9132120989869833, 'errorList': [], 'lossList': [0.0, -1.3493837380409242, 0.0, 46.011485867500305, 0.0, 0.0, 0.0], 'rewardMean': 0.8244922617716485, 'totalEpisodes': 215, 'stepsPerEpisode': 133, 'rewardPerEpisode': 108.44475650255752
'totalSteps': 16640, 'rewardStep': 0.6162248656921919, 'errorList': [], 'lossList': [0.0, -1.3410991960763932, 0.0, 57.115673789978025, 0.0, 0.0, 0.0], 'rewardMean': 0.8096075418791975, 'totalEpisodes': 228, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6162248656921919
'totalSteps': 17920, 'rewardStep': 0.6560565800273302, 'errorList': [], 'lossList': [0.0, -1.346877982020378, 0.0, 12.142351653575897, 0.0, 0.0, 0.0], 'rewardMean': 0.8054253512556138, 'totalEpisodes': 234, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.3180528693089781
'totalSteps': 19200, 'rewardStep': 0.8778757728776989, 'errorList': [], 'lossList': [0.0, -1.3538760781288146, 0.0, 8.40853661775589, 0.0, 0.0, 0.0], 'rewardMean': 0.8025852087999181, 'totalEpisodes': 237, 'stepsPerEpisode': 373, 'rewardPerEpisode': 311.79853155475746
'totalSteps': 20480, 'rewardStep': 0.9284293392960706, 'errorList': [], 'lossList': [0.0, -1.3456975257396697, 0.0, 6.965852814912796, 0.0, 0.0, 0.0], 'rewardMean': 0.806392185375118, 'totalEpisodes': 239, 'stepsPerEpisode': 146, 'rewardPerEpisode': 121.77359486984197
'totalSteps': 21760, 'rewardStep': 0.8120847208829892, 'errorList': [], 'lossList': [0.0, -1.3451872318983078, 0.0, 5.380337560325861, 0.0, 0.0, 0.0], 'rewardMean': 0.8133566380334397, 'totalEpisodes': 240, 'stepsPerEpisode': 850, 'rewardPerEpisode': 738.9972220400133
'totalSteps': 23040, 'rewardStep': 0.8782894414543908, 'errorList': [], 'lossList': [0.0, -1.3227344411611557, 0.0, 2.8979672184586525, 0.0, 0.0, 0.0], 'rewardMean': 0.8171904394469023, 'totalEpisodes': 240, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.4467229486097
'totalSteps': 24320, 'rewardStep': 0.8447748451314346, 'errorList': [], 'lossList': [0.0, -1.2943809032440186, 0.0, 3.1404236825555563, 0.0, 0.0, 0.0], 'rewardMean': 0.812522224129008, 'totalEpisodes': 240, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.081757339616
'totalSteps': 25600, 'rewardStep': 0.9871249509729287, 'errorList': [0.05245039909795022, 0.07961997880789408, 0.05198975419762808, 0.0554969037930947, 0.05108760356884907, 0.07809248624504778, 0.05128103983342358, 0.0502407393470498, 0.07748719221005931, 0.05402326122928922, 0.04967262803581699, 0.05149204144084917, 0.08135567166294357, 0.08061137326491782, 0.059123897065774506, 0.06125747848436208, 0.06936011850623922, 0.07348537615856217, 0.09341772529754339, 0.05673558359440019, 0.09766596671712346, 0.05210941509320845, 0.049737210685214285, 0.054360911981348015, 0.052886866174302966, 0.049998608683191496, 0.0794276884549936, 0.07655414923882677, 0.051969119767653144, 0.06170288008972734, 0.05102047143787666, 0.05589550103877565, 0.05208518895815718, 0.05279900055110537, 0.05594635833776776, 0.10325500700542722, 0.05021221214004206, 0.04995451872368633, 0.05115921927794345, 0.051727110384896996, 0.08820748054932302, 0.05970089207155046, 0.05404021012499045, 0.05269189654283149, 0.052480878791338476, 0.047256156195116784, 0.04709795545865427, 0.050711195610176577, 0.04998173371490937, 0.05013738695173818], 'lossList': [0.0, -1.2824265432357789, 0.0, 2.480560745894909, 0.0, 0.0, 0.0], 'rewardMean': 0.8252338240499004, 'totalEpisodes': 240, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1174.8180139151807, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.94
