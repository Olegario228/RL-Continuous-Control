#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 76
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8150501328414074, 'errorList': [], 'lossList': [0.0, -1.4206861442327499, 0.0, 42.88936342716217, 0.0, 0.0, 0.0], 'rewardMean': 0.8150501328414074, 'totalEpisodes': 33, 'stepsPerEpisode': 32, 'rewardPerEpisode': 27.030369173222955
'totalSteps': 2560, 'rewardStep': 0.5980728639696608, 'errorList': [], 'lossList': [0.0, -1.4175009137392045, 0.0, 30.411074995994568, 0.0, 0.0, 0.0], 'rewardMean': 0.7065614984055341, 'totalEpisodes': 53, 'stepsPerEpisode': 39, 'rewardPerEpisode': 33.34335271502231
'totalSteps': 3840, 'rewardStep': 0.7330469279365734, 'errorList': [], 'lossList': [0.0, -1.4049468553066253, 0.0, 37.47469141483307, 0.0, 0.0, 0.0], 'rewardMean': 0.7153899749158805, 'totalEpisodes': 68, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.242264674726297
'totalSteps': 5120, 'rewardStep': 0.8347010695763886, 'errorList': [], 'lossList': [0.0, -1.400660866498947, 0.0, 26.249958124160766, 0.0, 0.0, 0.0], 'rewardMean': 0.7452177485810075, 'totalEpisodes': 75, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.31198908554502
'totalSteps': 6400, 'rewardStep': 0.488131513055283, 'errorList': [], 'lossList': [0.0, -1.3892224514484406, 0.0, 21.766725466251373, 0.0, 0.0, 0.0], 'rewardMean': 0.6938005014758627, 'totalEpisodes': 78, 'stepsPerEpisode': 83, 'rewardPerEpisode': 66.79251691425303
'totalSteps': 7680, 'rewardStep': 0.581286380200412, 'errorList': [], 'lossList': [0.0, -1.3813911026716232, 0.0, 13.056067025959491, 0.0, 0.0, 0.0], 'rewardMean': 0.6750481479299543, 'totalEpisodes': 80, 'stepsPerEpisode': 740, 'rewardPerEpisode': 567.891340680006
'totalSteps': 8960, 'rewardStep': 0.809102737187264, 'errorList': [], 'lossList': [0.0, -1.4022342437505722, 0.0, 75.96977818012238, 0.0, 0.0, 0.0], 'rewardMean': 0.6941988035381413, 'totalEpisodes': 89, 'stepsPerEpisode': 110, 'rewardPerEpisode': 91.24391151060392
'totalSteps': 10240, 'rewardStep': 0.9083444487457011, 'errorList': [], 'lossList': [0.0, -1.398948438167572, 0.0, 97.1763909626007, 0.0, 0.0, 0.0], 'rewardMean': 0.7209670091890863, 'totalEpisodes': 104, 'stepsPerEpisode': 57, 'rewardPerEpisode': 45.54068403673813
'totalSteps': 11520, 'rewardStep': 0.5640415062907795, 'errorList': [], 'lossList': [0.0, -1.3871856719255446, 0.0, 35.92046510219574, 0.0, 0.0, 0.0], 'rewardMean': 0.7035308422003855, 'totalEpisodes': 112, 'stepsPerEpisode': 390, 'rewardPerEpisode': 299.7116691080057
'totalSteps': 12800, 'rewardStep': 0.6879470965163004, 'errorList': [], 'lossList': [0.0, -1.3631061524152757, 0.0, 8.915108754634858, 0.0, 0.0, 0.0], 'rewardMean': 0.701972467631977, 'totalEpisodes': 118, 'stepsPerEpisode': 95, 'rewardPerEpisode': 71.70719722796247
'totalSteps': 14080, 'rewardStep': 0.6227541029817872, 'errorList': [], 'lossList': [0.0, -1.3559906023740769, 0.0, 3.8780675250291825, 0.0, 0.0, 0.0], 'rewardMean': 0.682742864646015, 'totalEpisodes': 123, 'stepsPerEpisode': 321, 'rewardPerEpisode': 251.11985823680033
'totalSteps': 15360, 'rewardStep': 0.8105597248405333, 'errorList': [], 'lossList': [0.0, -1.381497146487236, 0.0, 3.6077061420679093, 0.0, 0.0, 0.0], 'rewardMean': 0.7039915507331022, 'totalEpisodes': 128, 'stepsPerEpisode': 120, 'rewardPerEpisode': 92.92289380449367
'totalSteps': 16640, 'rewardStep': 0.8794943130492073, 'errorList': [], 'lossList': [0.0, -1.3831611067056655, 0.0, 13.165151860713959, 0.0, 0.0, 0.0], 'rewardMean': 0.7186362892443656, 'totalEpisodes': 132, 'stepsPerEpisode': 143, 'rewardPerEpisode': 128.19322581413533
'totalSteps': 17920, 'rewardStep': 0.38740479962437857, 'errorList': [], 'lossList': [0.0, -1.3710413038730622, 0.0, 4.229384710788727, 0.0, 0.0, 0.0], 'rewardMean': 0.6739066622491646, 'totalEpisodes': 134, 'stepsPerEpisode': 274, 'rewardPerEpisode': 186.75143096835052
'totalSteps': 19200, 'rewardStep': 0.6696425895833931, 'errorList': [], 'lossList': [0.0, -1.3478458577394485, 0.0, 3.566961081624031, 0.0, 0.0, 0.0], 'rewardMean': 0.6920577699019757, 'totalEpisodes': 136, 'stepsPerEpisode': 503, 'rewardPerEpisode': 377.4170946827956
'totalSteps': 20480, 'rewardStep': 0.5413022793486562, 'errorList': [], 'lossList': [0.0, -1.3314212203025817, 0.0, 9.762885349988938, 0.0, 0.0, 0.0], 'rewardMean': 0.6880593598168001, 'totalEpisodes': 138, 'stepsPerEpisode': 493, 'rewardPerEpisode': 428.2532399615451
'totalSteps': 21760, 'rewardStep': 0.7192806619532226, 'errorList': [], 'lossList': [0.0, -1.322459802031517, 0.0, 8.453053249120712, 0.0, 0.0, 0.0], 'rewardMean': 0.6790771522933958, 'totalEpisodes': 139, 'stepsPerEpisode': 487, 'rewardPerEpisode': 410.686413513091
'totalSteps': 23040, 'rewardStep': 0.873159274320566, 'errorList': [], 'lossList': [0.0, -1.312069057226181, 0.0, 1.217822152376175, 0.0, 0.0, 0.0], 'rewardMean': 0.6755586348508824, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.532692842944
'totalSteps': 24320, 'rewardStep': 0.8682764345234246, 'errorList': [], 'lossList': [0.0, -1.270064640045166, 0.0, 0.7674979276955127, 0.0, 0.0, 0.0], 'rewardMean': 0.7059821276741468, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1119.9762674262026
'totalSteps': 25600, 'rewardStep': 0.9555546122030851, 'errorList': [0.04049391827890204, 0.017956614601958104, 0.04549998156055353, 0.051662493306734704, 0.03355092224041586, 0.04537652946098874, 0.05151809967374133, 0.08067578311036273, 0.08645377654026917, 0.06735723291028783, 0.008913225954363729, 0.007543969842230786, 0.05351629384456835, 0.02124876139436524, 0.016140429064942674, 0.06671787023741031, 0.011022067897815223, 0.011770727197596406, 0.019821859835882055, 0.024084161781688988, 0.007179348547229142, 0.021043515868918287, 0.04916274527518408, 0.13129907290491338, 0.0646772332607073, 0.04466972762117109, 0.04127011735532836, 0.08138026348421729, 0.018322751684653117, 0.024376514184417428, 0.06205495490034699, 0.019204096363026057, 0.09530129940159307, 0.12499870996693971, 0.03292564797624243, 0.06380174639141817, 0.0845486561381568, 0.03234874746916727, 0.061789810572275096, 0.030652364651063517, 0.007145405673236287, 0.021799342725355827, 0.019857549526646534, 0.014803925934338153, 0.04740235660448946, 0.08791647605241563, 0.012670500543741274, 0.10095378605442813, 0.019186086521623208, 0.009585103784095326], 'lossList': [0.0, -1.1987084782123565, 0.0, 0.6741478276252747, 0.0, 0.0, 0.0], 'rewardMean': 0.7327428792428253, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.4354579157714, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.14
