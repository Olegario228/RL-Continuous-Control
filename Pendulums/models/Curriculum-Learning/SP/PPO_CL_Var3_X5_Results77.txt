#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 77
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8156373417837095, 'errorList': [], 'lossList': [0.0, -1.4179689127206803, 0.0, 36.897707586288455, 0.0, 0.0, 0.0], 'rewardMean': 0.8156373417837095, 'totalEpisodes': 39, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.72023071677928
'totalSteps': 2560, 'rewardStep': 0.9092889628419989, 'errorList': [], 'lossList': [0.0, -1.4230946618318558, 0.0, 34.86065362930298, 0.0, 0.0, 0.0], 'rewardMean': 0.8624631523128542, 'totalEpisodes': 66, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.766588599953459
'totalSteps': 3840, 'rewardStep': 0.5557168774715248, 'errorList': [], 'lossList': [0.0, -1.4147667288780212, 0.0, 33.652716579437254, 0.0, 0.0, 0.0], 'rewardMean': 0.7602143940324111, 'totalEpisodes': 82, 'stepsPerEpisode': 32, 'rewardPerEpisode': 21.9684424698277
'totalSteps': 5120, 'rewardStep': 0.3498230000275299, 'errorList': [], 'lossList': [0.0, -1.3989657247066498, 0.0, 23.37756782770157, 0.0, 0.0, 0.0], 'rewardMean': 0.6576165455311908, 'totalEpisodes': 92, 'stepsPerEpisode': 92, 'rewardPerEpisode': 62.37832591465146
'totalSteps': 6400, 'rewardStep': 0.6050955188102577, 'errorList': [], 'lossList': [0.0, -1.3857063353061676, 0.0, 41.14668959379196, 0.0, 0.0, 0.0], 'rewardMean': 0.6471123401870041, 'totalEpisodes': 99, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6050955188102577
'totalSteps': 7680, 'rewardStep': 0.4534475963535578, 'errorList': [], 'lossList': [0.0, -1.358381206393242, 0.0, 50.38396752357483, 0.0, 0.0, 0.0], 'rewardMean': 0.6148348828814297, 'totalEpisodes': 104, 'stepsPerEpisode': 316, 'rewardPerEpisode': 238.3004338055149
'totalSteps': 8960, 'rewardStep': 0.4650813448009937, 'errorList': [], 'lossList': [0.0, -1.3383220303058625, 0.0, 43.308805928230285, 0.0, 0.0, 0.0], 'rewardMean': 0.5934415202985104, 'totalEpisodes': 110, 'stepsPerEpisode': 135, 'rewardPerEpisode': 99.03324025481008
'totalSteps': 10240, 'rewardStep': 0.8350251395221941, 'errorList': [], 'lossList': [0.0, -1.33135822057724, 0.0, 136.91420776367187, 0.0, 0.0, 0.0], 'rewardMean': 0.6236394727014708, 'totalEpisodes': 126, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.948359608340567
'totalSteps': 11520, 'rewardStep': 0.7125638444699759, 'errorList': [], 'lossList': [0.0, -1.338144844174385, 0.0, 41.59091014862061, 0.0, 0.0, 0.0], 'rewardMean': 0.6335199584535269, 'totalEpisodes': 134, 'stepsPerEpisode': 192, 'rewardPerEpisode': 147.541413481183
'totalSteps': 12800, 'rewardStep': 0.9022954248472713, 'errorList': [], 'lossList': [0.0, -1.3425543737411498, 0.0, 36.38069339752197, 0.0, 0.0, 0.0], 'rewardMean': 0.6603975050929014, 'totalEpisodes': 138, 'stepsPerEpisode': 141, 'rewardPerEpisode': 120.83099272780527
'totalSteps': 14080, 'rewardStep': 0.5620838772838552, 'errorList': [], 'lossList': [0.0, -1.3132917785644531, 0.0, 8.038301368951798, 0.0, 0.0, 0.0], 'rewardMean': 0.635042158642916, 'totalEpisodes': 139, 'stepsPerEpisode': 873, 'rewardPerEpisode': 638.4766518033225
'totalSteps': 15360, 'rewardStep': 0.7101918898437389, 'errorList': [], 'lossList': [0.0, -1.268407000899315, 0.0, 5.643858639597893, 0.0, 0.0, 0.0], 'rewardMean': 0.6151324513430899, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 923.5183536461132
'totalSteps': 16640, 'rewardStep': 0.6667979367972319, 'errorList': [], 'lossList': [0.0, -1.2552440214157103, 0.0, 3.6169412490725517, 0.0, 0.0, 0.0], 'rewardMean': 0.6262405572756606, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 937.9798566403553
'totalSteps': 17920, 'rewardStep': 0.8336076716107296, 'errorList': [], 'lossList': [0.0, -1.2288662123680114, 0.0, 4.001482338905334, 0.0, 0.0, 0.0], 'rewardMean': 0.6746190244339806, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1084.2016681424006
'totalSteps': 19200, 'rewardStep': 0.9020628835797055, 'errorList': [], 'lossList': [0.0, -1.2057784521579742, 0.0, 3.0331664684414865, 0.0, 0.0, 0.0], 'rewardMean': 0.7043157609109254, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.3873754236847
'totalSteps': 20480, 'rewardStep': 0.9846507313036097, 'errorList': [0.021553374851417444, 0.03845449143172559, 0.16327265333855803, 0.050378427802947746, 0.08397377800492824, 0.03717979535901575, 0.05798156645657569, 0.08151855272968915, 0.012064608579861556, 0.020495739617210313, 0.04161802903497083, 0.043594134477234196, 0.07880944134297459, 0.03709004024525104, 0.09114485912301833, 0.01943372094472497, 0.04403859713426218, 0.042012197669738835, 0.060026094748804326, 0.036340249974385924, 0.033611585136943664, 0.07361589424998254, 0.10482794378949646, 0.01971231164500045, 0.03650352432783952, 0.06819448650861291, 0.052821027720082, 0.03723373103507159, 0.020456997789308377, 0.10238371286754866, 0.061323605894987726, 0.08472122158095119, 0.05319163460191448, 0.06343386922733599, 0.008389290797347383, 0.024676356982700633, 0.026912310482042713, 0.06218780616523206, 0.056288896119011396, 0.03166197255754363, 0.058858261246372806, 0.0676579217152711, 0.08225778642573589, 0.10468500932010236, 0.018976641270054374, 0.10152558889679873, 0.020786803796632503, 0.06128759677341244, 0.04182582916805004, 0.045777853774048714], 'lossList': [0.0, -1.167220698595047, 0.0, 3.0758604092895983, 0.0, 0.0, 0.0], 'rewardMean': 0.7574360744059306, 'totalEpisodes': 139, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1177.4320406023348, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=67.95
