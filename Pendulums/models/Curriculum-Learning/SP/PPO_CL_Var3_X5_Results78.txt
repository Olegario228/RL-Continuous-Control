#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 78
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5049392267403848, 'errorList': [], 'lossList': [0.0, -1.4247832185029983, 0.0, 38.34356307029724, 0.0, 0.0, 0.0], 'rewardMean': 0.5049392267403848, 'totalEpisodes': 36, 'stepsPerEpisode': 71, 'rewardPerEpisode': 56.220570384230356
'totalSteps': 2560, 'rewardStep': 0.4358571861668494, 'errorList': [], 'lossList': [0.0, -1.434118731021881, 0.0, 31.66590000152588, 0.0, 0.0, 0.0], 'rewardMean': 0.4703982064536171, 'totalEpisodes': 60, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.31316783735886
'totalSteps': 3840, 'rewardStep': 0.9568119539963843, 'errorList': [], 'lossList': [0.0, -1.426927828192711, 0.0, 33.81852091789246, 0.0, 0.0, 0.0], 'rewardMean': 0.6325361223012061, 'totalEpisodes': 72, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.19752826038453
'totalSteps': 5120, 'rewardStep': 0.7347417371203913, 'errorList': [], 'lossList': [0.0, -1.4173165702819823, 0.0, 40.89082258224487, 0.0, 0.0, 0.0], 'rewardMean': 0.6580875260060024, 'totalEpisodes': 79, 'stepsPerEpisode': 73, 'rewardPerEpisode': 56.781888479672816
'totalSteps': 6400, 'rewardStep': 0.7476159042207124, 'errorList': [], 'lossList': [0.0, -1.4073707258701325, 0.0, 53.1183141040802, 0.0, 0.0, 0.0], 'rewardMean': 0.6759932016489444, 'totalEpisodes': 88, 'stepsPerEpisode': 25, 'rewardPerEpisode': 18.96679967635577
'totalSteps': 7680, 'rewardStep': 0.8531636214104137, 'errorList': [], 'lossList': [0.0, -1.3969842052459718, 0.0, 49.86275074958801, 0.0, 0.0, 0.0], 'rewardMean': 0.7055216049425225, 'totalEpisodes': 96, 'stepsPerEpisode': 83, 'rewardPerEpisode': 67.02212121711021
'totalSteps': 8960, 'rewardStep': 0.9406475746381842, 'errorList': [68.04958282875262, 188.94994837796477, 209.7465409019201, 119.0511051294617, 144.4262897790832, 206.02834342219705, 222.4749880476085, 102.52211366740774, 180.90857201144377, 73.20718041906677, 79.46679069683901, 198.61703569614028, 218.09238004059492, 11.775881997064063, 165.8812989217818, 82.93632573042812, 152.53201113995905, 171.6926492167479, 156.58388146724593, 206.19889618350524, 57.07352730221328, 224.4079061720862, 180.7336100935845, 170.47279594923776, 67.3754642630397, 184.9770314473718, 191.1331382006926, 234.47681594538065, 107.94346568762418, 145.03095923779134, 202.69736876615914, 159.30603499764106, 77.11942878764776, 185.76027684669967, 198.25403293077252, 43.634144860760586, 208.5541544796276, 173.43465799942925, 210.38724925388908, 65.4098838883559, 92.11260239547809, 182.94015146449559, 202.5470972156026, 180.19849942378735, 188.81937435544438, 124.31071088868184, 126.92465130776145, 151.31232619325087, 72.05993057262717, 186.3849356961429], 'lossList': [0.0, -1.366922550201416, 0.0, 110.60984363555909, 0.0, 0.0, 0.0], 'rewardMean': 0.7391110291847599, 'totalEpisodes': 110, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.44795133558309, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.2083791745185014, 'errorList': [], 'lossList': [0.0, -1.3526560616493226, 0.0, 69.31591039657593, 0.0, 0.0, 0.0], 'rewardMean': 0.6727695473514776, 'totalEpisodes': 121, 'stepsPerEpisode': 140, 'rewardPerEpisode': 92.15965602565231
'totalSteps': 11520, 'rewardStep': 0.8961622902712532, 'errorList': [], 'lossList': [0.0, -1.3361738008260726, 0.0, 31.706623435020447, 0.0, 0.0, 0.0], 'rewardMean': 0.6975909632314526, 'totalEpisodes': 130, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.943808800062087
'totalSteps': 12800, 'rewardStep': 0.31720540510271145, 'errorList': [], 'lossList': [0.0, -1.3267176312208175, 0.0, 36.65837682723999, 0.0, 0.0, 0.0], 'rewardMean': 0.6595524074185786, 'totalEpisodes': 136, 'stepsPerEpisode': 185, 'rewardPerEpisode': 149.77536422280647
'totalSteps': 14080, 'rewardStep': 0.6095243137814351, 'errorList': [], 'lossList': [0.0, -1.3330582410097123, 0.0, 35.7642800283432, 0.0, 0.0, 0.0], 'rewardMean': 0.6700109161226836, 'totalEpisodes': 144, 'stepsPerEpisode': 73, 'rewardPerEpisode': 52.86355549809155
'totalSteps': 15360, 'rewardStep': 0.49221264542032583, 'errorList': [], 'lossList': [0.0, -1.3355607384443282, 0.0, 24.684580554962157, 0.0, 0.0, 0.0], 'rewardMean': 0.6756464620480311, 'totalEpisodes': 149, 'stepsPerEpisode': 176, 'rewardPerEpisode': 135.1998752869468
'totalSteps': 16640, 'rewardStep': 0.8169683574716138, 'errorList': [], 'lossList': [0.0, -1.3312370294332505, 0.0, 8.000601804852487, 0.0, 0.0, 0.0], 'rewardMean': 0.6616621023955542, 'totalEpisodes': 152, 'stepsPerEpisode': 58, 'rewardPerEpisode': 46.94593468996594
'totalSteps': 17920, 'rewardStep': 0.9290863360316256, 'errorList': [], 'lossList': [0.0, -1.3387191486358643, 0.0, 4.631670893132687, 0.0, 0.0, 0.0], 'rewardMean': 0.6810965622866776, 'totalEpisodes': 154, 'stepsPerEpisode': 248, 'rewardPerEpisode': 213.1024658730023
'totalSteps': 19200, 'rewardStep': 0.8755461311033177, 'errorList': [], 'lossList': [0.0, -1.3220322465896606, 0.0, 6.67683837890625, 0.0, 0.0, 0.0], 'rewardMean': 0.6938895849749381, 'totalEpisodes': 158, 'stepsPerEpisode': 60, 'rewardPerEpisode': 49.141639773899854
'totalSteps': 20480, 'rewardStep': 0.8291941378929872, 'errorList': [], 'lossList': [0.0, -1.2901292902231216, 0.0, 5.782759687900543, 0.0, 0.0, 0.0], 'rewardMean': 0.6914926366231955, 'totalEpisodes': 159, 'stepsPerEpisode': 347, 'rewardPerEpisode': 284.54760178382026
'totalSteps': 21760, 'rewardStep': 0.8031466275854111, 'errorList': [], 'lossList': [0.0, -1.2746767050027847, 0.0, 6.869833112955093, 0.0, 0.0, 0.0], 'rewardMean': 0.6777425419179183, 'totalEpisodes': 162, 'stepsPerEpisode': 227, 'rewardPerEpisode': 197.17687611338621
'totalSteps': 23040, 'rewardStep': 0.8294836989522232, 'errorList': [], 'lossList': [0.0, -1.284635204076767, 0.0, 5.105215615034103, 0.0, 0.0, 0.0], 'rewardMean': 0.7398529943612904, 'totalEpisodes': 165, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.662824861147335
'totalSteps': 24320, 'rewardStep': 0.7245542918555407, 'errorList': [], 'lossList': [0.0, -1.2610858339071274, 0.0, 1.8655114635825156, 0.0, 0.0, 0.0], 'rewardMean': 0.7226921945197192, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.0670881193798
'totalSteps': 25600, 'rewardStep': 0.8750683412008649, 'errorList': [], 'lossList': [0.0, -1.2459759849309922, 0.0, 1.3601977837085724, 0.0, 0.0, 0.0], 'rewardMean': 0.7784784881295346, 'totalEpisodes': 165, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.5223640757883
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.43
