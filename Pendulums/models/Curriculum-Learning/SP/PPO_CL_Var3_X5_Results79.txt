#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 2.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 79
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 2.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9210901341514072, 'errorList': [], 'lossList': [0.0, -1.4135488986968994, 0.0, 40.81660542964935, 0.0, 0.0, 0.0], 'rewardMean': 0.9210901341514072, 'totalEpisodes': 40, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.730166898158885
'totalSteps': 2560, 'rewardStep': 0.7764156797508459, 'errorList': [], 'lossList': [0.0, -1.4118308931589127, 0.0, 34.566165647506715, 0.0, 0.0, 0.0], 'rewardMean': 0.8487529069511266, 'totalEpisodes': 67, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.54004547296811
'totalSteps': 3840, 'rewardStep': 0.79875858499291, 'errorList': [], 'lossList': [0.0, -1.4157278323173523, 0.0, 38.239025087356566, 0.0, 0.0, 0.0], 'rewardMean': 0.8320881329650544, 'totalEpisodes': 84, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.56910426609122
'totalSteps': 5120, 'rewardStep': 0.7679237262037389, 'errorList': [], 'lossList': [0.0, -1.4155820137262345, 0.0, 23.019551396369934, 0.0, 0.0, 0.0], 'rewardMean': 0.8160470312747256, 'totalEpisodes': 93, 'stepsPerEpisode': 41, 'rewardPerEpisode': 33.94679971262372
'totalSteps': 6400, 'rewardStep': 0.5738418932499253, 'errorList': [], 'lossList': [0.0, -1.4151037055253983, 0.0, 24.180312571525572, 0.0, 0.0, 0.0], 'rewardMean': 0.7676060036697655, 'totalEpisodes': 101, 'stepsPerEpisode': 141, 'rewardPerEpisode': 113.42928934613582
'totalSteps': 7680, 'rewardStep': 0.9309216349049713, 'errorList': [], 'lossList': [0.0, -1.4023575270175934, 0.0, 24.387373979091645, 0.0, 0.0, 0.0], 'rewardMean': 0.7948252755422999, 'totalEpisodes': 107, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.808915503856802
'totalSteps': 8960, 'rewardStep': 0.6777846824368304, 'errorList': [], 'lossList': [0.0, -1.3942174273729324, 0.0, 146.22048126220704, 0.0, 0.0, 0.0], 'rewardMean': 0.778105190812947, 'totalEpisodes': 128, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.600901436313304
'totalSteps': 10240, 'rewardStep': 0.6784992016035677, 'errorList': [], 'lossList': [0.0, -1.3795365476608277, 0.0, 133.22266288757325, 0.0, 0.0, 0.0], 'rewardMean': 0.7656544421617746, 'totalEpisodes': 161, 'stepsPerEpisode': 20, 'rewardPerEpisode': 13.952580535398077
'totalSteps': 11520, 'rewardStep': 0.770154233120809, 'errorList': [], 'lossList': [0.0, -1.371776695251465, 0.0, 46.05384386062622, 0.0, 0.0, 0.0], 'rewardMean': 0.7661544189350006, 'totalEpisodes': 179, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.1971413814384104
'totalSteps': 12800, 'rewardStep': 0.6403922811496903, 'errorList': [], 'lossList': [0.0, -1.3660508006811143, 0.0, 26.63697298526764, 0.0, 0.0, 0.0], 'rewardMean': 0.7535782051564696, 'totalEpisodes': 191, 'stepsPerEpisode': 61, 'rewardPerEpisode': 49.85655418545115
'totalSteps': 14080, 'rewardStep': 0.8552865312395982, 'errorList': [], 'lossList': [0.0, -1.3615316289663315, 0.0, 20.630904967784883, 0.0, 0.0, 0.0], 'rewardMean': 0.7469978448652888, 'totalEpisodes': 197, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.74447901671822
'totalSteps': 15360, 'rewardStep': 0.49812558819376407, 'errorList': [], 'lossList': [0.0, -1.3523888170719147, 0.0, 28.352692992687224, 0.0, 0.0, 0.0], 'rewardMean': 0.7191688357095806, 'totalEpisodes': 203, 'stepsPerEpisode': 242, 'rewardPerEpisode': 184.1623519757263
'totalSteps': 16640, 'rewardStep': 0.2805524492713935, 'errorList': [], 'lossList': [0.0, -1.327970072031021, 0.0, 10.440321973562241, 0.0, 0.0, 0.0], 'rewardMean': 0.6673482221374288, 'totalEpisodes': 206, 'stepsPerEpisode': 516, 'rewardPerEpisode': 327.03791233826246
'totalSteps': 17920, 'rewardStep': 0.835342016935489, 'errorList': [], 'lossList': [0.0, -1.2983246737718581, 0.0, 5.563211739361286, 0.0, 0.0, 0.0], 'rewardMean': 0.674090051210604, 'totalEpisodes': 207, 'stepsPerEpisode': 522, 'rewardPerEpisode': 405.805964401184
'totalSteps': 19200, 'rewardStep': 0.8915237571672964, 'errorList': [], 'lossList': [0.0, -1.274755784869194, 0.0, 4.975512093901634, 0.0, 0.0, 0.0], 'rewardMean': 0.705858237602341, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 929.3506829555176
'totalSteps': 20480, 'rewardStep': 0.9175390199293866, 'errorList': [], 'lossList': [0.0, -1.2391217356920243, 0.0, 4.05460283100605, 0.0, 0.0, 0.0], 'rewardMean': 0.7045199761047826, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.6984229481732
'totalSteps': 21760, 'rewardStep': 0.8018553803319955, 'errorList': [], 'lossList': [0.0, -1.2296638637781143, 0.0, 2.453924568668008, 0.0, 0.0, 0.0], 'rewardMean': 0.7169270458942991, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.1693249550244
'totalSteps': 23040, 'rewardStep': 0.8563313956055089, 'errorList': [], 'lossList': [0.0, -1.219191659092903, 0.0, 1.6700588139891623, 0.0, 0.0, 0.0], 'rewardMean': 0.7347102652944932, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1107.6663176390707
'totalSteps': 24320, 'rewardStep': 0.9288169796805058, 'errorList': [], 'lossList': [0.0, -1.1984984719753264, 0.0, 1.5606501739658416, 0.0, 0.0, 0.0], 'rewardMean': 0.7505765399504629, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.156844162236
'totalSteps': 25600, 'rewardStep': 0.9858596845185823, 'errorList': [0.06448122792214526, 0.06743359185684525, 0.060235887424016685, 0.062052834311213105, 0.06164476921269592, 0.06137972064872761, 0.12513392841029222, 0.07105454897976073, 0.10220552912056163, 0.09708586099378463, 0.0908596933815434, 0.09352856869854582, 0.10392541693946128, 0.067654676895468, 0.11433576881022016, 0.0945696881278341, 0.06923426228910708, 0.14226731846777688, 0.058144647865529475, 0.06556598363990791, 0.09130929500940611, 0.0636196964196449, 0.07842486035343497, 0.10748604741256446, 0.06505561040862336, 0.06347000582317722, 0.059689625559196116, 0.06057550222439232, 0.06260269793389003, 0.05837340644863146, 0.11372349409045206, 0.06535753119933808, 0.062157227478804165, 0.06250906787765326, 0.0635435652792662, 0.06791085115241226, 0.1245398261225966, 0.06581968851164136, 0.06277015049750181, 0.06307308722907899, 0.06595364839395015, 0.06289549858680238, 0.06948956307329404, 0.06864927957716727, 0.05966600472920923, 0.1305630943798961, 0.06739061401457518, 0.09869402500669033, 0.11800230250451291, 0.07930941284875746], 'lossList': [0.0, -1.1751486253738403, 0.0, 1.519327655751258, 0.0, 0.0, 0.0], 'rewardMean': 0.785123280287352, 'totalEpisodes': 207, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1211.6791997190373, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=81.67
