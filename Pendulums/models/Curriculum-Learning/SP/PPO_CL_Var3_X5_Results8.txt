#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 5000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 8
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 5000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.34435519154545485, 'errorList': [], 'lossList': [0.0, -1.4222215378284455, 0.0, 56.922558603286745, 0.0, 0.0, 0.0], 'rewardMean': 0.34435519154545485, 'totalEpisodes': 12, 'stepsPerEpisode': 74, 'rewardPerEpisode': 53.72682020267954
'totalSteps': 2560, 'rewardStep': 0.7165223720051882, 'errorList': [], 'lossList': [0.0, -1.411998040676117, 0.0, 24.974215261936187, 0.0, 0.0, 0.0], 'rewardMean': 0.5304387817753216, 'totalEpisodes': 14, 'stepsPerEpisode': 26, 'rewardPerEpisode': 22.01731806933703
'totalSteps': 3840, 'rewardStep': 0.9069092335425408, 'errorList': [], 'lossList': [0.0, -1.3990557622909545, 0.0, 27.278293694257737, 0.0, 0.0, 0.0], 'rewardMean': 0.6559289323643946, 'totalEpisodes': 20, 'stepsPerEpisode': 300, 'rewardPerEpisode': 216.66075518893388
'totalSteps': 5120, 'rewardStep': 0.7502089229445459, 'errorList': [], 'lossList': [0.0, -1.4022177916765213, 0.0, 46.07228693962097, 0.0, 0.0, 0.0], 'rewardMean': 0.6794989300094325, 'totalEpisodes': 25, 'stepsPerEpisode': 77, 'rewardPerEpisode': 66.79622649278868
'totalSteps': 6400, 'rewardStep': 0.655619326952256, 'errorList': [], 'lossList': [0.0, -1.3958987641334533, 0.0, 172.5557841873169, 0.0, 0.0, 0.0], 'rewardMean': 0.6747230093979972, 'totalEpisodes': 70, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.79556473851465
'totalSteps': 7680, 'rewardStep': 0.4648539962268316, 'errorList': [], 'lossList': [0.0, -1.3854259008169174, 0.0, 86.96283702850342, 0.0, 0.0, 0.0], 'rewardMean': 0.6397448405361362, 'totalEpisodes': 122, 'stepsPerEpisode': 25, 'rewardPerEpisode': 14.849286332055428
'totalSteps': 8960, 'rewardStep': 0.8104643312758711, 'errorList': [], 'lossList': [0.0, -1.379644793868065, 0.0, 44.06206792831421, 0.0, 0.0, 0.0], 'rewardMean': 0.6641333392132412, 'totalEpisodes': 147, 'stepsPerEpisode': 68, 'rewardPerEpisode': 49.91645770001312
'totalSteps': 10240, 'rewardStep': 0.6709696231443698, 'errorList': [], 'lossList': [0.0, -1.3568668156862258, 0.0, 33.951377778053285, 0.0, 0.0, 0.0], 'rewardMean': 0.6649878747046323, 'totalEpisodes': 156, 'stepsPerEpisode': 164, 'rewardPerEpisode': 131.52153141151746
'totalSteps': 11520, 'rewardStep': 0.30818268835269275, 'errorList': [], 'lossList': [0.0, -1.323654718399048, 0.0, 17.943219051361083, 0.0, 0.0, 0.0], 'rewardMean': 0.6253428539988612, 'totalEpisodes': 163, 'stepsPerEpisode': 199, 'rewardPerEpisode': 136.80091208790628
'totalSteps': 12800, 'rewardStep': 0.8147799937640517, 'errorList': [], 'lossList': [0.0, -1.3222208458185196, 0.0, 19.579075729846956, 0.0, 0.0, 0.0], 'rewardMean': 0.6442865679753803, 'totalEpisodes': 168, 'stepsPerEpisode': 166, 'rewardPerEpisode': 141.7705981257526
'totalSteps': 14080, 'rewardStep': 0.4123341508473038, 'errorList': [], 'lossList': [0.0, -1.3407832896709442, 0.0, 8.434527096748353, 0.0, 0.0, 0.0], 'rewardMean': 0.6510844639055653, 'totalEpisodes': 170, 'stepsPerEpisode': 531, 'rewardPerEpisode': 329.6992120541718
'totalSteps': 15360, 'rewardStep': 0.46531124660961565, 'errorList': [], 'lossList': [0.0, -1.3589204436540603, 0.0, 6.029516676068306, 0.0, 0.0, 0.0], 'rewardMean': 0.6259633513660079, 'totalEpisodes': 171, 'stepsPerEpisode': 1259, 'rewardPerEpisode': 873.4987288860651
'totalSteps': 16640, 'rewardStep': 0.7784289324180204, 'errorList': [], 'lossList': [0.0, -1.3638737440109252, 0.0, 10.518144719004631, 0.0, 0.0, 0.0], 'rewardMean': 0.6131153212535558, 'totalEpisodes': 172, 'stepsPerEpisode': 1258, 'rewardPerEpisode': 1092.5053647284965
'totalSteps': 17920, 'rewardStep': 0.7708572510427163, 'errorList': [], 'lossList': [0.0, -1.3297913140058517, 0.0, 3.1577376885712147, 0.0, 0.0, 0.0], 'rewardMean': 0.6151801540633729, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1039.0041234159596
'totalSteps': 19200, 'rewardStep': 0.8420709244374204, 'errorList': [], 'lossList': [0.0, -1.2970826929807664, 0.0, 1.8827667325735091, 0.0, 0.0, 0.0], 'rewardMean': 0.6338253138118894, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1029.8882001807144
'totalSteps': 20480, 'rewardStep': 0.950553220805964, 'errorList': [0.08470912238109328, 0.06268777914150277, 0.08176202674983894, 0.0865090178508461, 0.09263166521580721, 0.0784449091103548, 0.0982145407111357, 0.07832169337884641, 0.07496163102093771, 0.09300135098745392, 0.08333885104495806, 0.0833215846171256, 0.07574814978048787, 0.08335139291110787, 0.07574186041615716, 0.06290640251786685, 0.08640343412188148, 0.09915833947350593, 0.08793355133392637, 0.10156807459755947, 0.07357803142213284, 0.0914198184356915, 0.12810061504026812, 0.08943357424801403, 0.07035250553907865, 0.06917097622482965, 0.08692368565669834, 0.09079373066659079, 0.09666904292960603, 0.09160928412416847, 0.06895382793499365, 0.09814248067956821, 0.0845409626622346, 0.08561025973815153, 0.1103730163347937, 0.08809060386045447, 0.09737131746695628, 0.09403650248740013, 0.06986501947369674, 0.07427219854460615, 0.07269774932411494, 0.11049805911934321, 0.08299973745934672, 0.08010840251058181, 0.09157760411121314, 0.0902178055261965, 0.06268067871195812, 0.07561495259139432, 0.0981947390926174, 0.09124391674199908], 'lossList': [0.0, -1.262759911417961, 0.0, 2.1995855809748175, 0.0, 0.0, 0.0], 'rewardMean': 0.6823952362698026, 'totalEpisodes': 172, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1125.9940106434015, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=59.08
