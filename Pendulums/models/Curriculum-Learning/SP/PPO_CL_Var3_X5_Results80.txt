#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 80
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6106700989418505, 'errorList': [], 'lossList': [0.0, -1.4112318223714828, 0.0, 58.445557613372806, 0.0, 0.0, 0.0], 'rewardMean': 0.6106700989418505, 'totalEpisodes': 10, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.638917481994007
'totalSteps': 2560, 'rewardStep': 0.875512609306003, 'errorList': [], 'lossList': [0.0, -1.3985797357559204, 0.0, 28.815553860664366, 0.0, 0.0, 0.0], 'rewardMean': 0.7430913541239268, 'totalEpisodes': 17, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.951738625324866
'totalSteps': 3840, 'rewardStep': 0.7198197310530141, 'errorList': [], 'lossList': [0.0, -1.3880592238903047, 0.0, 30.408141717910766, 0.0, 0.0, 0.0], 'rewardMean': 0.7353341464336225, 'totalEpisodes': 26, 'stepsPerEpisode': 153, 'rewardPerEpisode': 129.3572483507257
'totalSteps': 5120, 'rewardStep': 0.60333023820753, 'errorList': [], 'lossList': [0.0, -1.3952900624275208, 0.0, 16.04129508256912, 0.0, 0.0, 0.0], 'rewardMean': 0.7023331693770993, 'totalEpisodes': 27, 'stepsPerEpisode': 205, 'rewardPerEpisode': 157.2963932129793
'totalSteps': 6400, 'rewardStep': 0.9390114660918574, 'errorList': [], 'lossList': [0.0, -1.3970063799619674, 0.0, 14.534793253540993, 0.0, 0.0, 0.0], 'rewardMean': 0.7496688287200509, 'totalEpisodes': 27, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 920.5333145828969
'totalSteps': 7680, 'rewardStep': 0.7188140017046611, 'errorList': [], 'lossList': [0.0, -1.376649871468544, 0.0, 48.422594313621524, 0.0, 0.0, 0.0], 'rewardMean': 0.7445263575508193, 'totalEpisodes': 30, 'stepsPerEpisode': 187, 'rewardPerEpisode': 149.11918766352224
'totalSteps': 8960, 'rewardStep': 0.8911061985433215, 'errorList': [], 'lossList': [0.0, -1.3420183289051055, 0.0, 127.88429244995118, 0.0, 0.0, 0.0], 'rewardMean': 0.7654663348354624, 'totalEpisodes': 40, 'stepsPerEpisode': 101, 'rewardPerEpisode': 81.76593375622186
'totalSteps': 10240, 'rewardStep': 0.8335584233926775, 'errorList': [], 'lossList': [0.0, -1.3318772727251054, 0.0, 206.21747158050536, 0.0, 0.0, 0.0], 'rewardMean': 0.7739778459051143, 'totalEpisodes': 69, 'stepsPerEpisode': 106, 'rewardPerEpisode': 86.53353020708394
'totalSteps': 11520, 'rewardStep': 0.531844514120411, 'errorList': [], 'lossList': [0.0, -1.3273317748308182, 0.0, 126.5355033493042, 0.0, 0.0, 0.0], 'rewardMean': 0.7470741423734806, 'totalEpisodes': 94, 'stepsPerEpisode': 58, 'rewardPerEpisode': 48.826086500407385
'totalSteps': 12800, 'rewardStep': 0.39859670534779845, 'errorList': [], 'lossList': [0.0, -1.3182458704710007, 0.0, 90.3909687423706, 0.0, 0.0, 0.0], 'rewardMean': 0.7122263986709123, 'totalEpisodes': 110, 'stepsPerEpisode': 122, 'rewardPerEpisode': 93.54478620195795
'totalSteps': 14080, 'rewardStep': 0.34271574242783015, 'errorList': [], 'lossList': [0.0, -1.314941661953926, 0.0, 36.250415334701536, 0.0, 0.0, 0.0], 'rewardMean': 0.6854309630195105, 'totalEpisodes': 118, 'stepsPerEpisode': 334, 'rewardPerEpisode': 264.9270316916294
'totalSteps': 15360, 'rewardStep': 0.4584763469298625, 'errorList': [], 'lossList': [0.0, -1.3138167834281922, 0.0, 62.674194688796995, 0.0, 0.0, 0.0], 'rewardMean': 0.6437273367818964, 'totalEpisodes': 130, 'stepsPerEpisode': 68, 'rewardPerEpisode': 47.021811080137205
'totalSteps': 16640, 'rewardStep': 0.7445951718294188, 'errorList': [], 'lossList': [0.0, -1.3068870228528977, 0.0, 12.372867847681045, 0.0, 0.0, 0.0], 'rewardMean': 0.6462048808595368, 'totalEpisodes': 136, 'stepsPerEpisode': 71, 'rewardPerEpisode': 50.087861886322614
'totalSteps': 17920, 'rewardStep': 0.466566082832659, 'errorList': [], 'lossList': [0.0, -1.2918818986415863, 0.0, 10.150600298643113, 0.0, 0.0, 0.0], 'rewardMean': 0.6325284653220498, 'totalEpisodes': 141, 'stepsPerEpisode': 134, 'rewardPerEpisode': 88.55048371159654
'totalSteps': 19200, 'rewardStep': 0.2418703893536278, 'errorList': [], 'lossList': [0.0, -1.2767514401674271, 0.0, 53.812065920829774, 0.0, 0.0, 0.0], 'rewardMean': 0.5628143576482268, 'totalEpisodes': 150, 'stepsPerEpisode': 163, 'rewardPerEpisode': 107.14177352100303
'totalSteps': 20480, 'rewardStep': 0.9077166315796791, 'errorList': [], 'lossList': [0.0, -1.2842329329252242, 0.0, 8.341771404743195, 0.0, 0.0, 0.0], 'rewardMean': 0.5817046206357286, 'totalEpisodes': 155, 'stepsPerEpisode': 213, 'rewardPerEpisode': 166.23465757673537
'totalSteps': 21760, 'rewardStep': 0.933491324328732, 'errorList': [1.046118165207409, 0.5842687684290803, 1.8511142808841623, 0.39422940126488654, 0.2862187550828918, 1.6436473625366779, 0.30013295075046703, 0.8313798731883805, 0.4998624942400365, 0.6301665516510587, 1.3175392987339685, 0.20458102959030208, 1.6776410473197712, 2.1876184354691377, 1.0039756966501685, 0.4045509370472877, 0.7240937792821562, 0.8237693593764198, 1.1695720926257431, 0.3016831235953417, 0.14939417751351464, 1.6056107695228716, 0.7282684519196605, 0.3275512408096218, 1.462000803304126, 0.5743831486694283, 0.7890477704522382, 0.17475915366264214, 0.8738011710134278, 1.688266031956774, 0.5881043087764493, 1.8631920826805535, 0.19176455249757626, 0.26916544667994985, 2.4139983220204475, 2.6200775409001826, 0.16461726272374805, 0.7695660732921977, 2.3130036545062818, 1.061836624014752, 0.24546232157955605, 0.4501266864671852, 0.39968028916450177, 2.0233263245196302, 0.6394566049691465, 2.1229627415044163, 2.178952204119555, 0.2563648219312538, 1.7519137249786285, 1.006240644352999], 'lossList': [0.0, -1.2876969438791275, 0.0, 26.07400738596916, 0.0, 0.0, 0.0], 'rewardMean': 0.5859431332142696, 'totalEpisodes': 159, 'stepsPerEpisode': 61, 'rewardPerEpisode': 54.700210972633414, 'successfulTests': 4
'totalSteps': 23040, 'rewardStep': 0.4449249358964343, 'errorList': [], 'lossList': [0.0, -1.2810555326938629, 0.0, 4.625250502824783, 0.0, 0.0, 0.0], 'rewardMean': 0.5470797844646453, 'totalEpisodes': 160, 'stepsPerEpisode': 727, 'rewardPerEpisode': 523.7910700922318
'totalSteps': 24320, 'rewardStep': 0.9004222170301466, 'errorList': [], 'lossList': [0.0, -1.2827264940738679, 0.0, 6.750115938186646, 0.0, 0.0, 0.0], 'rewardMean': 0.5839375547556189, 'totalEpisodes': 161, 'stepsPerEpisode': 46, 'rewardPerEpisode': 36.54280027793422
'totalSteps': 25600, 'rewardStep': 0.8011068668492743, 'errorList': [], 'lossList': [0.0, -1.2655183291435241, 0.0, 2.3897080701589584, 0.0, 0.0, 0.0], 'rewardMean': 0.6241885709057665, 'totalEpisodes': 161, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1031.9767194685767
#maxSuccessfulTests=4, maxSuccessfulTestsAtStep=21760, timeSpent=82.01
