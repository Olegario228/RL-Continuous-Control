#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 81
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9632895519107098, 'errorList': [], 'lossList': [0.0, -1.41792535841465, 0.0, 60.19137001037598, 0.0, 0.0, 0.0], 'rewardMean': 0.9632895519107098, 'totalEpisodes': 10, 'stepsPerEpisode': 92, 'rewardPerEpisode': 76.00567614410966
'totalSteps': 2560, 'rewardStep': 0.6323699011450243, 'errorList': [], 'lossList': [0.0, -1.4222376906871796, 0.0, 26.8784486579895, 0.0, 0.0, 0.0], 'rewardMean': 0.7978297265278671, 'totalEpisodes': 16, 'stepsPerEpisode': 358, 'rewardPerEpisode': 247.80564193027658
'totalSteps': 3840, 'rewardStep': 0.8394607018819621, 'errorList': [], 'lossList': [0.0, -1.4245054936408996, 0.0, 22.429085078239442, 0.0, 0.0, 0.0], 'rewardMean': 0.8117067183125655, 'totalEpisodes': 21, 'stepsPerEpisode': 381, 'rewardPerEpisode': 255.1865892253867
'totalSteps': 5120, 'rewardStep': 0.7217593516542608, 'errorList': [], 'lossList': [0.0, -1.425193514227867, 0.0, 16.39477586388588, 0.0, 0.0, 0.0], 'rewardMean': 0.7892198766479893, 'totalEpisodes': 23, 'stepsPerEpisode': 73, 'rewardPerEpisode': 53.11432250268246
'totalSteps': 6400, 'rewardStep': 0.6488666269858592, 'errorList': [], 'lossList': [0.0, -1.4180734914541244, 0.0, 24.542480294704436, 0.0, 0.0, 0.0], 'rewardMean': 0.7611492267155633, 'totalEpisodes': 25, 'stepsPerEpisode': 303, 'rewardPerEpisode': 180.93116652480415
'totalSteps': 7680, 'rewardStep': 0.5566030708397901, 'errorList': [], 'lossList': [0.0, -1.3985272246599196, 0.0, 7.213856859207153, 0.0, 0.0, 0.0], 'rewardMean': 0.7270582007362677, 'totalEpisodes': 25, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 886.2848178334202
'totalSteps': 8960, 'rewardStep': 0.3860627747911002, 'errorList': [], 'lossList': [0.0, -1.3741996002197265, 0.0, 121.26508508682251, 0.0, 0.0, 0.0], 'rewardMean': 0.6783445684583868, 'totalEpisodes': 34, 'stepsPerEpisode': 207, 'rewardPerEpisode': 147.10834972901517
'totalSteps': 10240, 'rewardStep': 0.8846396202647973, 'errorList': [], 'lossList': [0.0, -1.3635737413167954, 0.0, 250.68595474243165, 0.0, 0.0, 0.0], 'rewardMean': 0.704131449934188, 'totalEpisodes': 68, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.805017412781012
'totalSteps': 11520, 'rewardStep': 0.5635581768990762, 'errorList': [], 'lossList': [0.0, -1.3637065905332566, 0.0, 79.5241166973114, 0.0, 0.0, 0.0], 'rewardMean': 0.6885121973747311, 'totalEpisodes': 93, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.791264346600543
'totalSteps': 12800, 'rewardStep': 0.7341064415906362, 'errorList': [], 'lossList': [0.0, -1.3575400865077973, 0.0, 54.04245371818543, 0.0, 0.0, 0.0], 'rewardMean': 0.6930716217963215, 'totalEpisodes': 111, 'stepsPerEpisode': 24, 'rewardPerEpisode': 19.789087033998403
'totalSteps': 14080, 'rewardStep': 0.5118553194064412, 'errorList': [], 'lossList': [0.0, -1.3504921090602875, 0.0, 23.271736290454864, 0.0, 0.0, 0.0], 'rewardMean': 0.6479281985458949, 'totalEpisodes': 115, 'stepsPerEpisode': 615, 'rewardPerEpisode': 410.043032007343
'totalSteps': 15360, 'rewardStep': 0.525455303976287, 'errorList': [], 'lossList': [0.0, -1.345145629644394, 0.0, 21.737309248447417, 0.0, 0.0, 0.0], 'rewardMean': 0.637236738829021, 'totalEpisodes': 117, 'stepsPerEpisode': 178, 'rewardPerEpisode': 120.74643916419649
'totalSteps': 16640, 'rewardStep': 0.7743222260213887, 'errorList': [], 'lossList': [0.0, -1.3178261244297027, 0.0, 23.170826708078383, 0.0, 0.0, 0.0], 'rewardMean': 0.6307228912429637, 'totalEpisodes': 118, 'stepsPerEpisode': 509, 'rewardPerEpisode': 419.6552451099611
'totalSteps': 17920, 'rewardStep': 0.8557737810060009, 'errorList': [], 'lossList': [0.0, -1.278386008143425, 0.0, 6.4119501662254335, 0.0, 0.0, 0.0], 'rewardMean': 0.6441243341781376, 'totalEpisodes': 119, 'stepsPerEpisode': 113, 'rewardPerEpisode': 96.17423726988771
'totalSteps': 19200, 'rewardStep': 0.9530937272319415, 'errorList': [0.03409565635889435, 0.02181589136826274, 0.04933372698259811, 0.037483756002616574, 0.03698322949463072, 0.013968469692287912, 0.09783911501455439, 0.011659676921049756, 0.10045762436524214, 0.11261038620636338, 0.023050720909374166, 0.025163184051399654, 0.0187885943508293, 0.06717485741684674, 0.027906203243664364, 0.13004390473615277, 0.022720375890069984, 0.045796057044625624, 0.11673250844636067, 0.06334419086967989, 0.07050874275704137, 0.12910154247806038, 0.10188059954897237, 0.06358417796358277, 0.11736200361514608, 0.022268427594088005, 0.09435485525500532, 0.1210469839230997, 0.017474391486227772, 0.06242712790912278, 0.13721533585920492, 0.10082373273433042, 0.14634474148902196, 0.04024995253717022, 0.025882688287874284, 0.06208316316995714, 0.02346781659617883, 0.04114251935820465, 0.10921651493747235, 0.03602109535778605, 0.028362856130379553, 0.1281033746532161, 0.06790709261362507, 0.02366333885139383, 0.06438313253405394, 0.05877388446572799, 0.0567191223925683, 0.014163010890645191, 0.018615892643421682, 0.045979854302641385], 'lossList': [0.0, -1.2407226622104646, 0.0, 3.5812440632283686, 0.0, 0.0, 0.0], 'rewardMean': 0.6745470442027459, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1036.9862727932632, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=63.93
