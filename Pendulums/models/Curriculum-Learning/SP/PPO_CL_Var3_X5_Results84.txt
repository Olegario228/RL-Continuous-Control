#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 4.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 84
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 4.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3640701057755886, 'errorList': [], 'lossList': [0.0, -1.414980375766754, 0.0, 54.09083191871643, 0.0, 0.0, 0.0], 'rewardMean': 0.3640701057755886, 'totalEpisodes': 12, 'stepsPerEpisode': 142, 'rewardPerEpisode': 80.30234201980976
'totalSteps': 2560, 'rewardStep': 0.7930340237788471, 'errorList': [], 'lossList': [0.0, -1.412548166513443, 0.0, 24.13478826522827, 0.0, 0.0, 0.0], 'rewardMean': 0.5785520647772179, 'totalEpisodes': 20, 'stepsPerEpisode': 37, 'rewardPerEpisode': 29.511880748294793
'totalSteps': 3840, 'rewardStep': 0.8573597435011746, 'errorList': [], 'lossList': [0.0, -1.422642281651497, 0.0, 20.50326807975769, 0.0, 0.0, 0.0], 'rewardMean': 0.6714879576852034, 'totalEpisodes': 28, 'stepsPerEpisode': 182, 'rewardPerEpisode': 113.97356190306633
'totalSteps': 5120, 'rewardStep': 0.5735860994435112, 'errorList': [], 'lossList': [0.0, -1.4334207701683044, 0.0, 22.975838510990144, 0.0, 0.0, 0.0], 'rewardMean': 0.6470124931247804, 'totalEpisodes': 32, 'stepsPerEpisode': 206, 'rewardPerEpisode': 172.31941279442412
'totalSteps': 6400, 'rewardStep': 0.6581396321306519, 'errorList': [], 'lossList': [0.0, -1.438363163471222, 0.0, 27.712778882980345, 0.0, 0.0, 0.0], 'rewardMean': 0.6492379209259547, 'totalEpisodes': 34, 'stepsPerEpisode': 866, 'rewardPerEpisode': 637.4239953502085
'totalSteps': 7680, 'rewardStep': 0.6696190892818481, 'errorList': [], 'lossList': [0.0, -1.4192543441057206, 0.0, 22.485688506364824, 0.0, 0.0, 0.0], 'rewardMean': 0.6526347823186035, 'totalEpisodes': 35, 'stepsPerEpisode': 428, 'rewardPerEpisode': 294.8072713762799
'totalSteps': 8960, 'rewardStep': 0.6436011734724025, 'errorList': [], 'lossList': [0.0, -1.3904706263542175, 0.0, 108.97057344436645, 0.0, 0.0, 0.0], 'rewardMean': 0.6513442667691463, 'totalEpisodes': 44, 'stepsPerEpisode': 63, 'rewardPerEpisode': 49.19347310311941
'totalSteps': 10240, 'rewardStep': 0.9712266346961329, 'errorList': [315.1328685942749, 271.94018497627354, 291.8927113169194, 255.87554477455342, 317.3679910266901, 242.91483912762965, 253.76007770596334, 285.5546190001046, 315.2177145976441, 295.9569119558694, 302.14535464575357, 301.74448011726605, 317.1922121491105, 207.72871548750754, 333.5345707643194, 274.77350880603984, 137.6510405740123, 309.3469438962686, 299.98898805928945, 281.4968114472147, 328.31965995189813, 265.20488080705155, 308.0736166643792, 276.52485528124447, 228.7823231492496, 281.3781451545851, 245.27910616529363, 138.4930561554746, 279.70702055590266, 299.5607019673734, 271.48425019673954, 231.16215434414855, 237.9081193510137, 329.93423297551107, 248.1082649586699, 317.38487678266625, 308.5139939821944, 327.81783081635695, 341.67476482669946, 184.65633829279042, 292.98023130236624, 229.20941071144992, 253.1333646802767, 226.48479284989182, 320.3007654637645, 304.6670888362731, 295.55882834329043, 183.7877436946486, 277.21980478084316, 313.0789156500539], 'lossList': [0.0, -1.3872510713338853, 0.0, 272.3427229309082, 0.0, 0.0, 0.0], 'rewardMean': 0.6913295627600197, 'totalEpisodes': 82, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.66503483900895, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.778131185291197, 'errorList': [], 'lossList': [0.0, -1.383168415427208, 0.0, 77.15509470939637, 0.0, 0.0, 0.0], 'rewardMean': 0.7009741874857061, 'totalEpisodes': 102, 'stepsPerEpisode': 99, 'rewardPerEpisode': 83.90699978026359
'totalSteps': 12800, 'rewardStep': 0.8229076266243008, 'errorList': [], 'lossList': [0.0, -1.3833843088150024, 0.0, 46.69481930732727, 0.0, 0.0, 0.0], 'rewardMean': 0.7131675313995655, 'totalEpisodes': 120, 'stepsPerEpisode': 121, 'rewardPerEpisode': 93.67839085832685
'totalSteps': 14080, 'rewardStep': 0.6365020045623013, 'errorList': [], 'lossList': [0.0, -1.3877241265773774, 0.0, 11.108767750263214, 0.0, 0.0, 0.0], 'rewardMean': 0.7404107212782367, 'totalEpisodes': 129, 'stepsPerEpisode': 100, 'rewardPerEpisode': 73.8279297302198
'totalSteps': 15360, 'rewardStep': 0.913658713200532, 'errorList': [], 'lossList': [0.0, -1.3846093422174455, 0.0, 28.16627419948578, 0.0, 0.0, 0.0], 'rewardMean': 0.7524731902204052, 'totalEpisodes': 138, 'stepsPerEpisode': 162, 'rewardPerEpisode': 147.46956363053206
'totalSteps': 16640, 'rewardStep': 0.6436236540454452, 'errorList': [], 'lossList': [0.0, -1.3737313413619996, 0.0, 23.264039046764374, 0.0, 0.0, 0.0], 'rewardMean': 0.7310995812748323, 'totalEpisodes': 144, 'stepsPerEpisode': 19, 'rewardPerEpisode': 15.45249464899483
'totalSteps': 17920, 'rewardStep': 0.7617487145521428, 'errorList': [], 'lossList': [0.0, -1.3435860830545425, 0.0, 8.334541724920273, 0.0, 0.0, 0.0], 'rewardMean': 0.7499158427856955, 'totalEpisodes': 146, 'stepsPerEpisode': 261, 'rewardPerEpisode': 221.587449690663
'totalSteps': 19200, 'rewardStep': 0.7167364954583006, 'errorList': [], 'lossList': [0.0, -1.3247852450609208, 0.0, 7.088386678695679, 0.0, 0.0, 0.0], 'rewardMean': 0.7557755291184604, 'totalEpisodes': 148, 'stepsPerEpisode': 275, 'rewardPerEpisode': 220.96309817478297
'totalSteps': 20480, 'rewardStep': 0.9647462994792391, 'errorList': [0.08884478141956986, 0.12263090987530831, 0.0876293567140913, 0.15227814639579534, 0.08436607238037787, 0.08169265485937602, 0.1061623742222722, 0.1700485771495185, 0.08475091222808906, 0.08190993172134871, 0.08562579705558963, 0.08766049143957018, 0.08777401549834314, 0.08937201948504382, 0.19508809104840955, 0.08226585356887343, 0.11903083965523727, 0.08749988841680503, 0.08287479224696985, 0.08488648264832617, 0.21326614552102954, 0.13668766954172198, 0.08445430715678219, 0.08588242976020477, 0.08409059831466861, 0.11575554411313622, 0.12799951946560595, 0.08609707257736109, 0.0862633712484487, 0.08351015165840261, 0.08942379089485306, 0.08799627063771409, 0.08618445769260438, 0.187287463930045, 0.08759159338815238, 0.1002098455961167, 0.10947035820876294, 0.08252370153615773, 0.1524807273403833, 0.084032791878012, 0.08486837322092944, 0.21502907474022884, 0.1408566899240098, 0.08690269668126505, 0.15521900160269395, 0.15832335644403916, 0.1130465439092843, 0.15775786129547054, 0.08585860710830386, 0.08312660016415628], 'lossList': [0.0, -1.3065530967712402, 0.0, 4.894721028506756, 0.0, 0.0, 0.0], 'rewardMean': 0.7852882501381995, 'totalEpisodes': 148, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.0199191752304, 'successfulTests': 48
'totalSteps': 21760, 'rewardStep': 0.886786685698706, 'errorList': [], 'lossList': [0.0, -1.2820980894565581, 0.0, 3.2017591750621794, 0.0, 0.0, 0.0], 'rewardMean': 0.8096068013608297, 'totalEpisodes': 148, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1096.9175825942148
'totalSteps': 23040, 'rewardStep': 0.9151311158007379, 'errorList': [], 'lossList': [0.0, -1.2696613711118698, 0.0, 2.4202391659095883, 0.0, 0.0, 0.0], 'rewardMean': 0.8039972494712903, 'totalEpisodes': 148, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.8485234196814
'totalSteps': 24320, 'rewardStep': 0.9090415604425398, 'errorList': [], 'lossList': [0.0, -1.2517255926132203, 0.0, 2.0847163861989975, 0.0, 0.0, 0.0], 'rewardMean': 0.8170882869864246, 'totalEpisodes': 148, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1198.9617094908024
'totalSteps': 25600, 'rewardStep': 0.9876203953145998, 'errorList': [0.13389382740740893, 0.05877319879422096, 0.05792623777771203, 0.1422578042110098, 0.19509506229990256, 0.10747795736234525, 0.1145204328290185, 0.06944881286942182, 0.048977544035645255, 0.09433585468686412, 0.0698556427251281, 0.05275701965511585, 0.22558663570711524, 0.3065994785972426, 0.138552086228056, 0.05434570936984412, 0.10118913896991878, 0.13213548853872664, 0.2515889094747797, 0.0835245732983014, 0.1853246904318318, 0.15329770541966625, 0.201228580083765, 0.053352058546332244, 0.11376122686571837, 0.06326583839818017, 0.05464922803437784, 0.09605173564574199, 0.11035116397685493, 0.15327643240669933, 0.05239910600873452, 0.05173102298369963, 0.09274382781451934, 0.24493996235040058, 0.0668496514013828, 0.10039318430675065, 0.1092451451674463, 0.09602162090814384, 0.06169034033458047, 0.2477493911386587, 0.09131494989358177, 0.15544769383196036, 0.06636154721348031, 0.06989431697830037, 0.05246043161210004, 0.11327781871968431, 0.26766086880217377, 0.06292516774816335, 0.09278026340013236, 0.23393895574244886], 'lossList': [0.0, -1.220077207684517, 0.0, 1.7942376053333282, 0.0, 0.0, 0.0], 'rewardMean': 0.8335595638554544, 'totalEpisodes': 148, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1212.2526912727103, 'successfulTests': 42
#maxSuccessfulTests=48, maxSuccessfulTestsAtStep=20480, timeSpent=121.78
