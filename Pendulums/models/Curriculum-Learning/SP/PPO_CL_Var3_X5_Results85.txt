#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 85
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.799798960498963, 'errorList': [], 'lossList': [0.0, -1.416634669303894, 0.0, 79.9338010263443, 0.0, 0.0, 0.0], 'rewardMean': 0.799798960498963, 'totalEpisodes': 6, 'stepsPerEpisode': 191, 'rewardPerEpisode': 140.93933898498813
'totalSteps': 2560, 'rewardStep': 0.924869694755601, 'errorList': [], 'lossList': [0.0, -1.4073568898439408, 0.0, 27.202898162007333, 0.0, 0.0, 0.0], 'rewardMean': 0.862334327627282, 'totalEpisodes': 8, 'stepsPerEpisode': 536, 'rewardPerEpisode': 384.0971510663814
'totalSteps': 3840, 'rewardStep': 0.6387782889952165, 'errorList': [], 'lossList': [0.0, -1.3999533694982529, 0.0, 33.877861287593845, 0.0, 0.0, 0.0], 'rewardMean': 0.7878156480832601, 'totalEpisodes': 11, 'stepsPerEpisode': 263, 'rewardPerEpisode': 201.48618782092407
'totalSteps': 5120, 'rewardStep': 0.7064513760001889, 'errorList': [], 'lossList': [0.0, -1.4015226769447326, 0.0, 24.924056622982025, 0.0, 0.0, 0.0], 'rewardMean': 0.7674745800624923, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 960.3944173330253
'totalSteps': 6400, 'rewardStep': 0.8556259669550167, 'errorList': [], 'lossList': [0.0, -1.396283997297287, 0.0, 20.14863885730505, 0.0, 0.0, 0.0], 'rewardMean': 0.7851048574409971, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1025.6435063640506
'totalSteps': 7680, 'rewardStep': 0.8234052866935326, 'errorList': [], 'lossList': [0.0, -1.3812221330404282, 0.0, 14.03148919969797, 0.0, 0.0, 0.0], 'rewardMean': 0.7914882623164198, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.3470492612364
'totalSteps': 8960, 'rewardStep': 0.8522116027574185, 'errorList': [], 'lossList': [0.0, -1.3599173104763032, 0.0, 39.429351723194124, 0.0, 0.0, 0.0], 'rewardMean': 0.8001630252365625, 'totalEpisodes': 13, 'stepsPerEpisode': 225, 'rewardPerEpisode': 170.0058702139471
'totalSteps': 10240, 'rewardStep': 0.6645745434593507, 'errorList': [], 'lossList': [0.0, -1.3574163234233856, 0.0, 423.55863716125486, 0.0, 0.0, 0.0], 'rewardMean': 0.783214465014411, 'totalEpisodes': 49, 'stepsPerEpisode': 51, 'rewardPerEpisode': 36.71394879286474
'totalSteps': 11520, 'rewardStep': 0.8441689151394558, 'errorList': [], 'lossList': [0.0, -1.3569835329055786, 0.0, 198.5937693786621, 0.0, 0.0, 0.0], 'rewardMean': 0.7899871816949715, 'totalEpisodes': 99, 'stepsPerEpisode': 33, 'rewardPerEpisode': 31.543471192915682
'totalSteps': 12800, 'rewardStep': 0.7511396756710641, 'errorList': [], 'lossList': [0.0, -1.3573082041740419, 0.0, 88.89622030258178, 0.0, 0.0, 0.0], 'rewardMean': 0.7861024310925808, 'totalEpisodes': 121, 'stepsPerEpisode': 110, 'rewardPerEpisode': 94.8806657337513
'totalSteps': 14080, 'rewardStep': 0.3598927353463147, 'errorList': [], 'lossList': [0.0, -1.3581699132919312, 0.0, 59.72076881408692, 0.0, 0.0, 0.0], 'rewardMean': 0.742111808577316, 'totalEpisodes': 139, 'stepsPerEpisode': 98, 'rewardPerEpisode': 60.96402348121057
'totalSteps': 15360, 'rewardStep': 0.8844086937442357, 'errorList': [], 'lossList': [0.0, -1.3490893352031708, 0.0, 56.000568828582765, 0.0, 0.0, 0.0], 'rewardMean': 0.7380657084761795, 'totalEpisodes': 152, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7586957409736144
'totalSteps': 16640, 'rewardStep': 0.6146785218007456, 'errorList': [], 'lossList': [0.0, -1.3419244599342346, 0.0, 51.27595823287964, 0.0, 0.0, 0.0], 'rewardMean': 0.7356557317567323, 'totalEpisodes': 158, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.324047719369349
'totalSteps': 17920, 'rewardStep': 0.9568213240714049, 'errorList': [14.76312158611749, 16.971383334104452, 12.930205323965623, 16.06551736765299, 7.03486461308054, 15.413194044610718, 16.536898750846785, 7.268127810575939, 15.191001752616334, 12.162225181163464, 16.231408535796817, 16.945590493350792, 15.085325316571886, 10.14682201340286, 8.536922090011256, 14.702617303380999, 3.3971412231936973, 12.718740826898662, 14.282521673249704, 12.765985004669504, 13.423620807461727, 14.082212347396515, 8.7791819404001, 5.8690601041679225, 11.089419781256316, 9.759822907051458, 14.215158427855867, 8.36369581027688, 10.935289793851856, 15.121428642507134, 10.902204985236946, 8.509276767156342, 5.356430597503363, 12.098959992853064, 2.533295709418398, 2.2007127303624547, 7.586220174064267, 4.476368881572421, 2.2706432230442006, 17.632585561573794, 0.43801932947500405, 12.607684654092463, 4.134797584422856, 2.2033764292706417, 9.6266262061071, 14.346262972008798, 12.22787972514105, 4.705760492635353, 1.4503221946313065, 0.27671684283500564], 'lossList': [0.0, -1.3374023681879044, 0.0, 35.487940874099735, 0.0, 0.0, 0.0], 'rewardMean': 0.7606927265638539, 'totalEpisodes': 164, 'stepsPerEpisode': 54, 'rewardPerEpisode': 47.17986200883994, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.7600595589244525, 'errorList': [], 'lossList': [0.0, -1.334563924074173, 0.0, 35.04503894805908, 0.0, 0.0, 0.0], 'rewardMean': 0.7511360857607976, 'totalEpisodes': 168, 'stepsPerEpisode': 70, 'rewardPerEpisode': 55.27545352398567
'totalSteps': 20480, 'rewardStep': 0.8921146903080824, 'errorList': [], 'lossList': [0.0, -1.3327471774816513, 0.0, 54.84417071342468, 0.0, 0.0, 0.0], 'rewardMean': 0.7580070261222525, 'totalEpisodes': 174, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.3722825128378
'totalSteps': 21760, 'rewardStep': 0.924746691969551, 'errorList': [], 'lossList': [0.0, -1.3361963963508605, 0.0, 53.37401918888092, 0.0, 0.0, 0.0], 'rewardMean': 0.7652605350434657, 'totalEpisodes': 179, 'stepsPerEpisode': 61, 'rewardPerEpisode': 56.07880278215904
'totalSteps': 23040, 'rewardStep': 0.6304642343952586, 'errorList': [], 'lossList': [0.0, -1.3374696004390716, 0.0, 25.936647762060165, 0.0, 0.0, 0.0], 'rewardMean': 0.7618495041370565, 'totalEpisodes': 182, 'stepsPerEpisode': 289, 'rewardPerEpisode': 236.49556909255057
'totalSteps': 24320, 'rewardStep': 0.5810862292952059, 'errorList': [], 'lossList': [0.0, -1.3371988999843598, 0.0, 26.613491139411927, 0.0, 0.0, 0.0], 'rewardMean': 0.7355412355526315, 'totalEpisodes': 187, 'stepsPerEpisode': 167, 'rewardPerEpisode': 132.27664671352395
'totalSteps': 25600, 'rewardStep': 0.8253702511237954, 'errorList': [], 'lossList': [0.0, -1.3321215885877609, 0.0, 7.267019989490509, 0.0, 0.0, 0.0], 'rewardMean': 0.7429642930979046, 'totalEpisodes': 190, 'stepsPerEpisode': 519, 'rewardPerEpisode': 429.2216619806944
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.81
