#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 86
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5167939016994528, 'errorList': [], 'lossList': [0.0, -1.4211588287353516, 0.0, 77.08162875175476, 0.0, 0.0, 0.0], 'rewardMean': 0.5167939016994528, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 71.20955638214707
'totalSteps': 2560, 'rewardStep': 0.7407404498652908, 'errorList': [], 'lossList': [0.0, -1.4381526517868042, 0.0, 33.31090434074402, 0.0, 0.0, 0.0], 'rewardMean': 0.6287671757823718, 'totalEpisodes': 12, 'stepsPerEpisode': 75, 'rewardPerEpisode': 63.58393403861199
'totalSteps': 3840, 'rewardStep': 0.8635375059590926, 'errorList': [], 'lossList': [0.0, -1.4452201694250106, 0.0, 27.52407440185547, 0.0, 0.0, 0.0], 'rewardMean': 0.7070239525079455, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.0110753582114
'totalSteps': 5120, 'rewardStep': 0.5752365274735453, 'errorList': [], 'lossList': [0.0, -1.4194746124744415, 0.0, 26.260542992949485, 0.0, 0.0, 0.0], 'rewardMean': 0.6740770962493454, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1014.2376198994879
'totalSteps': 6400, 'rewardStep': 0.9295671272282515, 'errorList': [], 'lossList': [0.0, -1.4136227852106094, 0.0, 22.946033432483674, 0.0, 0.0, 0.0], 'rewardMean': 0.7251751024451266, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.5080907535432
'totalSteps': 7680, 'rewardStep': 0.8831174069745973, 'errorList': [], 'lossList': [0.0, -1.3903988873958588, 0.0, 16.92140663340688, 0.0, 0.0, 0.0], 'rewardMean': 0.7514988198667051, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.1159981395083
'totalSteps': 8960, 'rewardStep': 0.931693208101418, 'errorList': [183.7602192038961, 154.78129894530733, 177.5172736652788, 178.352946591158, 186.39067253406517, 165.94395290635768, 176.50962091956117, 179.02241149835666, 167.50320824014673, 187.64910989372035, 172.23033509702296, 165.6499018182373, 149.64601582053197, 177.1600371228821, 178.06405565597987, 181.0124742028139, 183.0015408202052, 155.5123972203718, 190.0965805652433, 149.9000805984556, 181.19542054140405, 165.7256091493717, 162.619217971952, 174.59364703699978, 182.59440728203396, 150.41217652896498, 182.2803191555173, 168.87761883003964, 183.34680834075235, 154.39960258022097, 189.58116528830607, 188.50185674038946, 184.73515954874514, 186.74752477261524, 184.57289699713954, 185.08510336078723, 185.43306571306312, 183.8491813153412, 190.70869468900526, 164.06820255166818, 164.83139507244917, 188.0920306151024, 171.33626599102578, 191.77693000089945, 170.04887566580186, 181.06775706398284, 185.14435154432587, 158.4343635203562, 181.22201381657692, 173.60242009957594], 'lossList': [0.0, -1.3872245544195174, 0.0, 70.67502520561219, 0.0, 0.0, 0.0], 'rewardMean': 0.7772408753288069, 'totalEpisodes': 16, 'stepsPerEpisode': 116, 'rewardPerEpisode': 94.67690314106905, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.4524768268407191, 'errorList': [], 'lossList': [0.0, -1.3868950873613357, 0.0, 376.1522998046875, 0.0, 0.0, 0.0], 'rewardMean': 0.7366453692677959, 'totalEpisodes': 56, 'stepsPerEpisode': 8, 'rewardPerEpisode': 4.070273587835954
'totalSteps': 11520, 'rewardStep': 0.861863937548467, 'errorList': [], 'lossList': [0.0, -1.3854209417104721, 0.0, 156.27312168121338, 0.0, 0.0, 0.0], 'rewardMean': 0.7505585435212038, 'totalEpisodes': 99, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.215988929822777
'totalSteps': 12800, 'rewardStep': 0.6042512801481841, 'errorList': [], 'lossList': [0.0, -1.383268706202507, 0.0, 76.98133504867553, 0.0, 0.0, 0.0], 'rewardMean': 0.7359278171839019, 'totalEpisodes': 128, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.072927900464921
'totalSteps': 14080, 'rewardStep': 0.5428676015995123, 'errorList': [], 'lossList': [0.0, -1.3834182494878768, 0.0, 75.7713655090332, 0.0, 0.0, 0.0], 'rewardMean': 0.7385351871739079, 'totalEpisodes': 153, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.9158990305358516
'totalSteps': 15360, 'rewardStep': 0.5320288047441432, 'errorList': [], 'lossList': [0.0, -1.3712104707956314, 0.0, 74.72307319641114, 0.0, 0.0, 0.0], 'rewardMean': 0.7176640226617931, 'totalEpisodes': 170, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.97984660586486
'totalSteps': 16640, 'rewardStep': 0.7896643349046306, 'errorList': [], 'lossList': [0.0, -1.3685922425985337, 0.0, 53.32628147125244, 0.0, 0.0, 0.0], 'rewardMean': 0.7102767055563468, 'totalEpisodes': 183, 'stepsPerEpisode': 154, 'rewardPerEpisode': 134.11745776644582
'totalSteps': 17920, 'rewardStep': 0.49551414644843816, 'errorList': [], 'lossList': [0.0, -1.3850265872478484, 0.0, 25.145629122257233, 0.0, 0.0, 0.0], 'rewardMean': 0.7023044674538361, 'totalEpisodes': 193, 'stepsPerEpisode': 89, 'rewardPerEpisode': 64.44113161757001
'totalSteps': 19200, 'rewardStep': 0.9346519585039121, 'errorList': [211.8214357110956, 86.16767578630062, 77.63587054747376, 196.2120003742426, 175.42157745016638, 144.11648667187472, 68.25252082477604, 140.6844634113445, 109.27471375561778, 71.4082328098711, 147.95212193143658, 199.0976742357449, 155.2055442183717, 133.04248564729528, 181.8179865038309, 136.12231608833554, 202.81021147667852, 123.57783082901068, 200.11440690931644, 63.461424272657226, 165.0853080934623, 159.25564272716295, 98.19173600190946, 211.1591315625859, 28.91835930597728, 175.68656024614887, 196.49140111729278, 210.65753995629376, 229.72854464459257, 176.43660711142024, 78.41322542531265, 86.32214363663601, 87.68980392139083, 139.26770221560048, 189.33867050477923, 238.23334957943948, 48.2927020701738, 201.51226311317006, 215.6554734285272, 10.15699837229066, 239.0826039910805, 148.12244737863938, 156.36605367923562, 105.43405151118785, 141.34629421142245, 198.52865878224068, 158.69381315974223, 208.0764385789073, 103.51363444559195, 202.07525384634695], 'lossList': [0.0, -1.4040833228826524, 0.0, 21.76920141220093, 0.0, 0.0, 0.0], 'rewardMean': 0.7028129505814023, 'totalEpisodes': 202, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.137436494527833, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.8501408869680167, 'errorList': [], 'lossList': [0.0, -1.4038483554124832, 0.0, 21.254711599349974, 0.0, 0.0, 0.0], 'rewardMean': 0.6995152985807442, 'totalEpisodes': 209, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.561988529210603
'totalSteps': 21760, 'rewardStep': 0.8042107457313491, 'errorList': [], 'lossList': [0.0, -1.401810411810875, 0.0, 17.06184150457382, 0.0, 0.0, 0.0], 'rewardMean': 0.6867670523437372, 'totalEpisodes': 217, 'stepsPerEpisode': 56, 'rewardPerEpisode': 46.953204403977274
'totalSteps': 23040, 'rewardStep': 0.9752841092382636, 'errorList': [0.3206019234246772, 27.41559779568882, 0.41589782973985207, 46.37523184046919, 14.778045372205975, 69.52611710126811, 0.19300224229342064, 1.3027882666613322, 70.47272204048832, 0.645491344471721, 83.9884519560682, 15.29257429030252, 3.2702375454935457, 53.72967976011716, 43.59204538422732, 14.351915101209967, 2.277567995734216, 1.751793472136745, 106.16077808877914, 4.920845859178604, 61.93010184303529, 111.91385884184295, 28.72945627511988, 12.581379234512855, 19.290441130785936, 7.0909248298207945, 123.68694412629978, 0.9238630489916262, 69.01184727669322, 46.75478614919868, 0.4280266173579068, 6.022529412024952, 1.537087372292316, 30.344555260064904, 136.76619151895895, 50.76093994100198, 0.5220599753874386, 14.882366652117222, 0.5048474522696821, 1.6784910425143331, 0.8482081718155542, 0.3488480844822278, 19.791487600964974, 0.7600921719735717, 40.415094071424235, 57.4553491516974, 0.2505154011840072, 5.818456944941025, 1.3033514047061958, 63.70636336004744], 'lossList': [0.0, -1.3950232201814652, 0.0, 9.672327370643616, 0.0, 0.0, 0.0], 'rewardMean': 0.7390477805834917, 'totalEpisodes': 222, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.9229716947352107, 'successfulTests': 1
'totalSteps': 24320, 'rewardStep': 0.25492233977501844, 'errorList': [], 'lossList': [0.0, -1.3899944746494293, 0.0, 8.8053069293499, 0.0, 0.0, 0.0], 'rewardMean': 0.6783536208061467, 'totalEpisodes': 224, 'stepsPerEpisode': 646, 'rewardPerEpisode': 476.38718272761326
'totalSteps': 25600, 'rewardStep': 0.7029211564638356, 'errorList': [], 'lossList': [0.0, -1.3596215921640395, 0.0, 8.219259535074235, 0.0, 0.0, 0.0], 'rewardMean': 0.6882206084377119, 'totalEpisodes': 227, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.318519944996721
#maxSuccessfulTests=1, maxSuccessfulTestsAtStep=23040, timeSpent=121.92
