#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 87
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.438030592205874, 'errorList': [], 'lossList': [0.0, -1.4255891716480256, 0.0, 65.8821933555603, 0.0, 0.0, 0.0], 'rewardMean': 0.438030592205874, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 163.46467513842236
'totalSteps': 2560, 'rewardStep': 0.8175777637530325, 'errorList': [], 'lossList': [0.0, -1.443953737616539, 0.0, 25.713502714633943, 0.0, 0.0, 0.0], 'rewardMean': 0.6278041779794532, 'totalEpisodes': 11, 'stepsPerEpisode': 901, 'rewardPerEpisode': 609.8072786978247
'totalSteps': 3840, 'rewardStep': 0.8824554433367778, 'errorList': [], 'lossList': [0.0, -1.4527363342046737, 0.0, 40.4124601483345, 0.0, 0.0, 0.0], 'rewardMean': 0.7126879330985614, 'totalEpisodes': 14, 'stepsPerEpisode': 487, 'rewardPerEpisode': 392.9386311570449
'totalSteps': 5120, 'rewardStep': 0.685480408978183, 'errorList': [], 'lossList': [0.0, -1.440084753036499, 0.0, 19.122410880327223, 0.0, 0.0, 0.0], 'rewardMean': 0.7058860520684668, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 910.8265490156793
'totalSteps': 6400, 'rewardStep': 0.5041328283414174, 'errorList': [], 'lossList': [0.0, -1.4242267733812333, 0.0, 29.19693428874016, 0.0, 0.0, 0.0], 'rewardMean': 0.6655354073230569, 'totalEpisodes': 15, 'stepsPerEpisode': 575, 'rewardPerEpisode': 440.78088450829966
'totalSteps': 7680, 'rewardStep': 0.6378071213574138, 'errorList': [], 'lossList': [0.0, -1.4137076777219773, 0.0, 33.88581299185753, 0.0, 0.0, 0.0], 'rewardMean': 0.660914026328783, 'totalEpisodes': 17, 'stepsPerEpisode': 827, 'rewardPerEpisode': 634.9469441973304
'totalSteps': 8960, 'rewardStep': 0.6544660375307085, 'errorList': [], 'lossList': [0.0, -1.4126702600717544, 0.0, 164.7686893081665, 0.0, 0.0, 0.0], 'rewardMean': 0.6599928850719152, 'totalEpisodes': 31, 'stepsPerEpisode': 30, 'rewardPerEpisode': 19.685964211553085
'totalSteps': 10240, 'rewardStep': 0.9257226714917315, 'errorList': [], 'lossList': [0.0, -1.4131033200025558, 0.0, 352.11714347839353, 0.0, 0.0, 0.0], 'rewardMean': 0.6932091083743923, 'totalEpisodes': 81, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.642474468830953
'totalSteps': 11520, 'rewardStep': 0.6989099599471836, 'errorList': [], 'lossList': [0.0, -1.4097078156471252, 0.0, 94.71999280929566, 0.0, 0.0, 0.0], 'rewardMean': 0.6938425363269247, 'totalEpisodes': 119, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.96294014727966
'totalSteps': 12800, 'rewardStep': 0.6900372849952696, 'errorList': [], 'lossList': [0.0, -1.4073931527137757, 0.0, 47.48794280052185, 0.0, 0.0, 0.0], 'rewardMean': 0.6934620111937592, 'totalEpisodes': 144, 'stepsPerEpisode': 129, 'rewardPerEpisode': 112.24570231793066
'totalSteps': 14080, 'rewardStep': 0.9200290911555592, 'errorList': [], 'lossList': [0.0, -1.3955548244714737, 0.0, 18.220834770202636, 0.0, 0.0, 0.0], 'rewardMean': 0.7416618610887277, 'totalEpisodes': 167, 'stepsPerEpisode': 28, 'rewardPerEpisode': 22.93239127325935
'totalSteps': 15360, 'rewardStep': 0.9456299888110338, 'errorList': [94.9056142712024, 7.793199655366782, 78.42377406049648, 194.80717475772127, 107.06110522166833, 94.35735878498852, 230.11319327166012, 89.06300882516197, 124.0259122825396, 231.27689578083752, 203.3343483501516, 223.23033104818944, 199.63583836863168, 218.33105987692286, 31.859281946017873, 137.51009162509544, 122.49710088404335, 116.13819265769885, 153.43554563996915, 197.7014250716645, 13.273908870724629, 248.31364285257533, 103.37702587212523, 209.29250608002195, 78.06506737939604, 66.2627091978643, 115.60705076759197, 225.22931710742014, 170.81717532638612, 151.88142097004754, 133.41536323709659, 216.00623502964638, 118.33335245366513, 187.9644369044387, 135.4998228867746, 122.80332325341564, 208.66578594749498, 217.78529628192697, 227.02859677490244, 189.84216786293345, 48.66887411438296, 221.34392652936845, 192.8780588296123, 230.38752497464841, 223.4085672474214, 76.50977389158511, 31.411919375900624, 176.83097697518153, 90.21594936459616, 208.5758847925067], 'lossList': [0.0, -1.3829494708776473, 0.0, 19.646857800483705, 0.0, 0.0, 0.0], 'rewardMean': 0.7544670835945277, 'totalEpisodes': 182, 'stepsPerEpisode': 37, 'rewardPerEpisode': 31.8791591259797, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.8792693374358426, 'errorList': [], 'lossList': [0.0, -1.3719445514678954, 0.0, 11.619575754404067, 0.0, 0.0, 0.0], 'rewardMean': 0.7541484730044343, 'totalEpisodes': 190, 'stepsPerEpisode': 42, 'rewardPerEpisode': 34.47599404434903
'totalSteps': 17920, 'rewardStep': 0.6130348971425117, 'errorList': [], 'lossList': [0.0, -1.365167321562767, 0.0, 11.931046257019043, 0.0, 0.0, 0.0], 'rewardMean': 0.7469039218208671, 'totalEpisodes': 196, 'stepsPerEpisode': 234, 'rewardPerEpisode': 180.21477914230556
'totalSteps': 19200, 'rewardStep': 0.9736098838626246, 'errorList': [13.416406409973932, 3.5428118419956904, 11.053480416207623, 46.88338881779004, 14.395673638390297, 1.924254869385708, 12.350419834605855, 46.23331228243089, 39.30466230824025, 35.652513822397275, 24.4528479198098, 1.902902083780924, 8.274546260238912, 11.852746493136468, 7.39311349286496, 0.9873922032168412, 0.3924950367305708, 41.34268391305114, 16.61885898299483, 6.410913026171103, 4.590097392163881, 10.349640923556748, 23.57840990495231, 16.57307819826007, 17.146146891612894, 0.4903328980586703, 5.17627664199814, 30.692439892954386, 3.536486907515185, 8.213528771322684, 31.488636888547376, 1.382217285354578, 21.790676507987808, 24.45256041225058, 10.685931268003712, 6.93828228830452, 12.939147938258586, 21.42244322544289, 23.409179072408328, 6.808172191641742, 0.5595304550454059, 10.445791606916266, 3.910994453848254, 43.42478024715941, 3.3631517781327167, 6.123546853638357, 3.457359392467376, 3.612318221545122, 24.03614376385754, 25.389758441309098], 'lossList': [0.0, -1.3516993552446366, 0.0, 25.542676651477812, 0.0, 0.0, 0.0], 'rewardMean': 0.7938516273729879, 'totalEpisodes': 204, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.83724859333627, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.6893776331973629, 'errorList': [], 'lossList': [0.0, -1.3341683280467986, 0.0, 6.578980643749237, 0.0, 0.0, 0.0], 'rewardMean': 0.7990086785569828, 'totalEpisodes': 208, 'stepsPerEpisode': 206, 'rewardPerEpisode': 175.64391720742552
'totalSteps': 21760, 'rewardStep': 0.7062872764245003, 'errorList': [], 'lossList': [0.0, -1.3097403913736343, 0.0, 4.946549471020699, 0.0, 0.0, 0.0], 'rewardMean': 0.804190802446362, 'totalEpisodes': 210, 'stepsPerEpisode': 612, 'rewardPerEpisode': 416.62086899228274
'totalSteps': 23040, 'rewardStep': 0.9671528651487844, 'errorList': [0.05420154700338921, 0.0661951483028995, 0.07417239001137539, 0.015152173011183157, 0.047338892494747944, 0.015543827092689852, 0.014082337567308097, 0.03696934510314205, 0.04130571486039354, 0.03268917386988335, 0.029368326217934008, 0.012009856411772006, 0.03930304171879669, 0.07846625979712418, 0.03049407013865066, 0.029760881491509183, 0.015780800588223346, 0.05886207506779423, 0.03165222816655111, 0.015356762692852029, 0.054451580918265546, 0.021171648242051585, 0.02565670136991284, 0.012368119183248824, 0.028455112266658266, 0.027425999301071795, 0.047239229444191934, 0.04862037835163545, 0.06660440844724443, 0.046597875164506426, 0.03847712861865238, 0.04032918591321934, 0.027674083973782116, 0.045469968323337974, 0.04070735474050938, 0.012600456112276111, 0.04517936294029223, 0.0495568720903594, 0.06325023345438073, 0.015506076242041767, 0.013219943612464364, 0.03246383775051234, 0.04812900473464654, 0.06953564037709285, 0.042869876069456574, 0.03133985449227316, 0.018592556902444463, 0.06274821637164574, 0.01318915923220907, 0.04938576059005168], 'lossList': [0.0, -1.2797748333215713, 0.0, 6.309943533241749, 0.0, 0.0, 0.0], 'rewardMean': 0.8083338218120673, 'totalEpisodes': 210, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.872221001581, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=118.94
