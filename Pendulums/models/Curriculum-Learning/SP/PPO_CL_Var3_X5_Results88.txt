#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 88
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.765325832947056, 'errorList': [], 'lossList': [0.0, -1.420974037051201, 0.0, 62.66942523956299, 0.0, 0.0, 0.0], 'rewardMean': 0.765325832947056, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.659053390085028
'totalSteps': 2560, 'rewardStep': 0.6571718613021138, 'errorList': [], 'lossList': [0.0, -1.4219412606954576, 0.0, 27.96654240012169, 0.0, 0.0, 0.0], 'rewardMean': 0.7112488471245849, 'totalEpisodes': 16, 'stepsPerEpisode': 42, 'rewardPerEpisode': 31.15510540907105
'totalSteps': 3840, 'rewardStep': 0.9609414984312489, 'errorList': [], 'lossList': [0.0, -1.4156751644611358, 0.0, 27.07490775704384, 0.0, 0.0, 0.0], 'rewardMean': 0.7944797308934729, 'totalEpisodes': 18, 'stepsPerEpisode': 490, 'rewardPerEpisode': 373.94197474499026
'totalSteps': 5120, 'rewardStep': 0.8062571268847128, 'errorList': [], 'lossList': [0.0, -1.4097467511892319, 0.0, 27.39792986482382, 0.0, 0.0, 0.0], 'rewardMean': 0.7974240798912829, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1048.1473760994636
'totalSteps': 6400, 'rewardStep': 0.6684953005534989, 'errorList': [], 'lossList': [0.0, -1.3903327506780625, 0.0, 19.143252571225165, 0.0, 0.0, 0.0], 'rewardMean': 0.7716383240237261, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.5402239338703
'totalSteps': 7680, 'rewardStep': 0.911169219244107, 'errorList': [], 'lossList': [0.0, -1.370753224492073, 0.0, 14.739661353826524, 0.0, 0.0, 0.0], 'rewardMean': 0.794893473227123, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.744157122929
'totalSteps': 8960, 'rewardStep': 0.9306902874516565, 'errorList': [232.67017431515208, 210.35498399452266, 271.12508809557215, 223.21530429794507, 247.09086282537754, 250.4999548586354, 285.3406235510425, 261.4406103999081, 230.62746493446016, 225.89617266799624, 245.52260513831175, 263.56932094001394, 253.14058858573205, 256.4802388373098, 245.01339283099622, 263.0803097152301, 252.94481839889076, 181.02590270413594, 232.36660531884115, 269.66688650757084, 275.1549244890178, 248.41778562123253, 268.3431982108262, 261.20834608529873, 152.46863546006733, 192.0237773831512, 251.1217704952749, 181.3934367959317, 263.7945989219714, 223.91244920990627, 254.29100066164307, 256.91028341548247, 265.47330187548675, 250.26484653770495, 250.45191010217562, 262.856286501862, 202.49263228637062, 275.877477777454, 259.451090083752, 264.85499441842984, 247.4684026500134, 256.7839145897918, 258.8906309503397, 239.0021487527351, 237.4704078171973, 157.1622825645874, 257.60857262001355, 265.1690712880294, 240.05817010977532, 245.67308489962966], 'lossList': [0.0, -1.364513264298439, 0.0, 80.91227418899535, 0.0, 0.0, 0.0], 'rewardMean': 0.8142930181163421, 'totalEpisodes': 23, 'stepsPerEpisode': 40, 'rewardPerEpisode': 29.95651331663157, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.7915412526946167, 'errorList': [], 'lossList': [0.0, -1.3651331102848052, 0.0, 275.47245014190673, 0.0, 0.0, 0.0], 'rewardMean': 0.8114490474386263, 'totalEpisodes': 62, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.381800302522667
'totalSteps': 11520, 'rewardStep': 0.6483746600591459, 'errorList': [], 'lossList': [0.0, -1.3631305795907975, 0.0, 88.52797204971313, 0.0, 0.0, 0.0], 'rewardMean': 0.7933296710631285, 'totalEpisodes': 89, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.890633432666146
'totalSteps': 12800, 'rewardStep': 0.2364378289005905, 'errorList': [], 'lossList': [0.0, -1.3588267195224761, 0.0, 57.56274547576904, 0.0, 0.0, 0.0], 'rewardMean': 0.7376404868468747, 'totalEpisodes': 109, 'stepsPerEpisode': 123, 'rewardPerEpisode': 91.03859451549198
'totalSteps': 14080, 'rewardStep': 0.7856868207554749, 'errorList': [], 'lossList': [0.0, -1.3526479786634444, 0.0, 48.48870175361633, 0.0, 0.0, 0.0], 'rewardMean': 0.7396765856277167, 'totalEpisodes': 127, 'stepsPerEpisode': 27, 'rewardPerEpisode': 19.12939033502844
'totalSteps': 15360, 'rewardStep': 0.5001476932039757, 'errorList': [], 'lossList': [0.0, -1.3404092717170715, 0.0, 50.58260983467102, 0.0, 0.0, 0.0], 'rewardMean': 0.7239741688179029, 'totalEpisodes': 137, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.5111156816526856
'totalSteps': 16640, 'rewardStep': 0.8715903165954088, 'errorList': [], 'lossList': [0.0, -1.3293386632204056, 0.0, 33.38855749607086, 0.0, 0.0, 0.0], 'rewardMean': 0.7150390506343187, 'totalEpisodes': 141, 'stepsPerEpisode': 46, 'rewardPerEpisode': 39.639446337460605
'totalSteps': 17920, 'rewardStep': 0.8553759951914284, 'errorList': [], 'lossList': [0.0, -1.3316422426700592, 0.0, 9.951946040391922, 0.0, 0.0, 0.0], 'rewardMean': 0.7199509374649903, 'totalEpisodes': 143, 'stepsPerEpisode': 206, 'rewardPerEpisode': 177.27303828962758
'totalSteps': 19200, 'rewardStep': 0.804525360640933, 'errorList': [], 'lossList': [0.0, -1.3194538348913192, 0.0, 5.963446364998817, 0.0, 0.0, 0.0], 'rewardMean': 0.7335539434737337, 'totalEpisodes': 144, 'stepsPerEpisode': 232, 'rewardPerEpisode': 172.5097330585637
'totalSteps': 20480, 'rewardStep': 0.7348003573632582, 'errorList': [], 'lossList': [0.0, -1.2818225997686385, 0.0, 3.374624042510986, 0.0, 0.0, 0.0], 'rewardMean': 0.7159170572856489, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 938.805815625077
'totalSteps': 21760, 'rewardStep': 0.8377201027311844, 'errorList': [], 'lossList': [0.0, -1.2547051256895065, 0.0, 2.5884401056170465, 0.0, 0.0, 0.0], 'rewardMean': 0.7066200388136017, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 953.2855022881635
'totalSteps': 23040, 'rewardStep': 0.7311041535494355, 'errorList': [], 'lossList': [0.0, -1.236880585551262, 0.0, 1.6524398232996464, 0.0, 0.0, 0.0], 'rewardMean': 0.7005763288990836, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1064.947910705105
'totalSteps': 24320, 'rewardStep': 0.7652982301367761, 'errorList': [], 'lossList': [0.0, -1.1927900040149688, 0.0, 1.9216838655620814, 0.0, 0.0, 0.0], 'rewardMean': 0.7122686859068466, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.9129923250946
'totalSteps': 25600, 'rewardStep': 0.929926772068192, 'errorList': [], 'lossList': [0.0, -1.179388090968132, 0.0, 1.5302865455299617, 0.0, 0.0, 0.0], 'rewardMean': 0.7816175802236066, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1152.5380896727643
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=81.15
