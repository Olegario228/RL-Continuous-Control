#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 6.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 89
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 6.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6719154061433433, 'errorList': [], 'lossList': [0.0, -1.4175392007827758, 0.0, 61.81661130905152, 0.0, 0.0, 0.0], 'rewardMean': 0.6719154061433433, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 102.26368277715707
'totalSteps': 2560, 'rewardStep': 0.8691101347824172, 'errorList': [], 'lossList': [0.0, -1.4240564143657684, 0.0, 30.64413312256336, 0.0, 0.0, 0.0], 'rewardMean': 0.7705127704628802, 'totalEpisodes': 13, 'stepsPerEpisode': 316, 'rewardPerEpisode': 252.4489483861818
'totalSteps': 3840, 'rewardStep': 0.772590116789299, 'errorList': [], 'lossList': [0.0, -1.4417839348316193, 0.0, 30.421109310388566, 0.0, 0.0, 0.0], 'rewardMean': 0.7712052192383533, 'totalEpisodes': 15, 'stepsPerEpisode': 168, 'rewardPerEpisode': 126.65570317591657
'totalSteps': 5120, 'rewardStep': 0.6036902377462999, 'errorList': [], 'lossList': [0.0, -1.4453035473823548, 0.0, 22.69026637673378, 0.0, 0.0, 0.0], 'rewardMean': 0.7293264738653399, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 953.1231931161379
'totalSteps': 6400, 'rewardStep': 0.8407876849090513, 'errorList': [], 'lossList': [0.0, -1.4347717565298082, 0.0, 17.947501916885376, 0.0, 0.0, 0.0], 'rewardMean': 0.7516187160740821, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 986.5414233145294
'totalSteps': 7680, 'rewardStep': 0.9819442234360595, 'errorList': [], 'lossList': [0.0, -1.4258932375907898, 0.0, 35.20587812542915, 0.0, 0.0, 0.0], 'rewardMean': 0.7900063006344117, 'totalEpisodes': 17, 'stepsPerEpisode': 428, 'rewardPerEpisode': 298.05366703527517
'totalSteps': 8960, 'rewardStep': 0.7944700397431259, 'errorList': [], 'lossList': [0.0, -1.4125389850139618, 0.0, 250.71284713745118, 0.0, 0.0, 0.0], 'rewardMean': 0.7906439776499423, 'totalEpisodes': 41, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.5713987542623684
'totalSteps': 10240, 'rewardStep': 0.9114674176362979, 'errorList': [], 'lossList': [0.0, -1.4125296878814697, 0.0, 194.14723140716552, 0.0, 0.0, 0.0], 'rewardMean': 0.8057469076482368, 'totalEpisodes': 79, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.649304047195136
'totalSteps': 11520, 'rewardStep': 0.6431075919131517, 'errorList': [], 'lossList': [0.0, -1.4122374713420869, 0.0, 107.37272121429443, 0.0, 0.0, 0.0], 'rewardMean': 0.7876758725665607, 'totalEpisodes': 112, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.9245157845205556
'totalSteps': 12800, 'rewardStep': 0.5233247130546331, 'errorList': [], 'lossList': [0.0, -1.4071581882238389, 0.0, 81.07721242904663, 0.0, 0.0, 0.0], 'rewardMean': 0.7612407566153678, 'totalEpisodes': 134, 'stepsPerEpisode': 63, 'rewardPerEpisode': 51.738708917821896
'totalSteps': 14080, 'rewardStep': 0.3575024213808333, 'errorList': [], 'lossList': [0.0, -1.3983711844682694, 0.0, 42.78641937255859, 0.0, 0.0, 0.0], 'rewardMean': 0.7297994581391168, 'totalEpisodes': 146, 'stepsPerEpisode': 46, 'rewardPerEpisode': 23.52144288647047
'totalSteps': 15360, 'rewardStep': 0.9843247058429385, 'errorList': [46.60744161811364, 184.36982007778084, 166.32896633821986, 0.517143940202041, 191.70541021646804, 176.17927447474568, 186.54878623006596, 88.93159091809349, 98.07065512050782, 191.17425816419097, 135.16291918244545, 170.58846129713794, 158.43750048432312, 126.83944050315228, 132.19297285490956, 125.53175377631241, 159.66825466055982, 5.291890878384928, 163.53302963413483, 74.63117650648459, 166.3390620480481, 153.21850342546506, 139.2315148965277, 87.50812545437455, 193.67734304578022, 107.03636895545011, 147.1109971031542, 167.07197773249092, 170.8542787145807, 117.81827622090194, 152.05491591851577, 176.2254522088333, 97.7053942404794, 108.38244753067643, 54.18910712008571, 185.23484784615349, 187.50921002636304, 167.22407398654317, 126.50205343680784, 143.2684687578564, 163.58673472543205, 63.65222481620632, 189.95680478292735, 28.659706545679644, 84.76721966133347, 160.98978350147055, 78.70758418420458, 117.04559835020721, 148.37116646253426, 181.07192424712326], 'lossList': [0.0, -1.3890215587615966, 0.0, 77.07573230743408, 0.0, 0.0, 0.0], 'rewardMean': 0.7413209152451691, 'totalEpisodes': 166, 'stepsPerEpisode': 135, 'rewardPerEpisode': 108.38981357809331, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.4606829928221501, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6958294783560391, 'totalEpisodes': 182, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.204677336641296
'totalSteps': 17920, 'rewardStep': 0.9260713382215358, 'errorList': [], 'lossList': [0.0, -1.3879904556274414, 0.0, 26.541221342086793, 0.0, 0.0, 0.0], 'rewardMean': 0.7043578436872876, 'totalEpisodes': 189, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.23373446727602
'totalSteps': 19200, 'rewardStep': 0.5897448773964428, 'errorList': [], 'lossList': [0.0, -1.382862765789032, 0.0, 19.856362121105192, 0.0, 0.0, 0.0], 'rewardMean': 0.6651379090833258, 'totalEpisodes': 196, 'stepsPerEpisode': 13, 'rewardPerEpisode': 7.021205436760109
'totalSteps': 20480, 'rewardStep': 0.6227829952398194, 'errorList': [], 'lossList': [0.0, -1.3765300458669663, 0.0, 22.510733733177187, 0.0, 0.0, 0.0], 'rewardMean': 0.6479692046329952, 'totalEpisodes': 202, 'stepsPerEpisode': 148, 'rewardPerEpisode': 114.61994854151312
'totalSteps': 21760, 'rewardStep': 0.6223496281004162, 'errorList': [], 'lossList': [0.0, -1.3666869843006133, 0.0, 9.433670179843903, 0.0, 0.0, 0.0], 'rewardMean': 0.6190574256794071, 'totalEpisodes': 208, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.8268598450423648
'totalSteps': 23040, 'rewardStep': 0.6804306179515858, 'errorList': [], 'lossList': [0.0, -1.366782752275467, 0.0, 24.891812827587128, 0.0, 0.0, 0.0], 'rewardMean': 0.6227897282832504, 'totalEpisodes': 213, 'stepsPerEpisode': 175, 'rewardPerEpisode': 135.16530300211875
'totalSteps': 24320, 'rewardStep': 0.6572042761559727, 'errorList': [], 'lossList': [0.0, -1.356497247815132, 0.0, 6.25861709356308, 0.0, 0.0, 0.0], 'rewardMean': 0.6361776845933844, 'totalEpisodes': 216, 'stepsPerEpisode': 367, 'rewardPerEpisode': 320.18400664874383
'totalSteps': 25600, 'rewardStep': 0.9542705487537267, 'errorList': [0.5770294948792837, 0.46200298064828127, 0.3579100732560475, 0.5516642372314735, 0.3790728317680576, 0.4569520376514763, 0.15717299103648727, 0.29276065521084116, 0.5823557258612031, 0.45402378389657816, 0.2836121523478764, 0.7057883613237683, 0.760842174262253, 0.3375126415154323, 0.7595225590347913, 0.6693151595436029, 0.5771159887389222, 0.6997484278140164, 0.2585130849575478, 0.839532748142908, 1.1538562992943955, 0.38190718225281994, 0.3734896911150891, 0.36653946793060266, 0.495043385129015, 0.33163164858682886, 0.30545831721482264, 0.7549873223161052, 1.0027025735305704, 0.5369746581019122, 0.47464473783529443, 0.4778057157276631, 0.7707823511211214, 0.13768413950598224, 0.23743039099324037, 0.4429984798165884, 0.591192248346094, 0.7233592899692755, 0.46323785628061254, 0.39803819036845994, 0.9984213203151404, 0.7300474301465968, 0.7834585605440154, 0.4782402160065388, 0.2926626282346449, 0.22940660115981334, 0.8368101948642699, 0.23593557291207296, 0.2767532918813592, 0.3987206719799872], 'lossList': [0.0, -1.330165902376175, 0.0, 3.980494146347046, 0.0, 0.0, 0.0], 'rewardMean': 0.6958544973306738, 'totalEpisodes': 218, 'stepsPerEpisode': 515, 'rewardPerEpisode': 458.60506668429326, 'successfulTests': 2
#maxSuccessfulTests=2, maxSuccessfulTestsAtStep=25600, timeSpent=100.39
