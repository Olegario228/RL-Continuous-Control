#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 8.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 91
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 8.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5586477184763551, 'errorList': [], 'lossList': [0.0, -1.422742450237274, 0.0, 84.1290883731842, 0.0, 0.0, 0.0], 'rewardMean': 0.5586477184763551, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 73.88594114249605
'totalSteps': 2560, 'rewardStep': 0.8584749130566374, 'errorList': [], 'lossList': [0.0, -1.4446185475587845, 0.0, 36.9135284614563, 0.0, 0.0, 0.0], 'rewardMean': 0.7085613157664963, 'totalEpisodes': 12, 'stepsPerEpisode': 65, 'rewardPerEpisode': 59.027927949763466
'totalSteps': 3840, 'rewardStep': 0.894296332009464, 'errorList': [], 'lossList': [0.0, -1.4568090349435807, 0.0, 30.42414413034916, 0.0, 0.0, 0.0], 'rewardMean': 0.7704729878474855, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.6580082482449
'totalSteps': 5120, 'rewardStep': 0.6483208349119359, 'errorList': [], 'lossList': [0.0, -1.4395978343486786, 0.0, 29.824871512651445, 0.0, 0.0, 0.0], 'rewardMean': 0.7399349496135982, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1054.1322760091803
'totalSteps': 6400, 'rewardStep': 0.9366033260081976, 'errorList': [], 'lossList': [0.0, -1.4379056566953659, 0.0, 23.067540453374384, 0.0, 0.0, 0.0], 'rewardMean': 0.779268624892518, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.6447509124916
'totalSteps': 7680, 'rewardStep': 0.8576919842008632, 'errorList': [], 'lossList': [0.0, -1.415737401843071, 0.0, 17.245647786334157, 0.0, 0.0, 0.0], 'rewardMean': 0.7923391847772422, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.6584456554415
'totalSteps': 8960, 'rewardStep': 0.9557424918302109, 'errorList': [244.7978540549096, 195.1096498536273, 240.80164670667628, 233.37405355963642, 251.32785657430935, 188.25306686709123, 227.9649103594202, 243.6986236432594, 231.86163616612112, 253.9966426921479, 219.86295383572164, 183.1041280744891, 221.51353971445835, 227.27276335962642, 231.71794914001646, 245.27724032310766, 245.6034705896279, 223.9109461959907, 256.67007965184325, 207.53196304851076, 240.12908606103016, 229.33967707499178, 176.20322210514107, 226.88732736036243, 248.65214363176418, 214.8211280634282, 247.88367866974465, 185.16486188319848, 245.50545321566875, 200.60170458532636, 258.43271548827613, 254.25411232933533, 248.87938853757106, 252.72793490170463, 249.79714412885934, 250.41857246764457, 251.0376463875344, 247.923871828803, 260.4197036203817, 166.0619186864157, 180.66779215460636, 254.6891353400168, 235.40012772020845, 262.5007963366323, 212.35853087185194, 241.03898415124306, 250.82411124432699, 202.4302185294376, 246.18943184201692, 238.33162142680592], 'lossList': [0.0, -1.4069828844070436, 0.0, 81.83470154762269, 0.0, 0.0, 0.0], 'rewardMean': 0.8156825143562377, 'totalEpisodes': 16, 'stepsPerEpisode': 117, 'rewardPerEpisode': 95.982063118769, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.4817227579204451, 'errorList': [], 'lossList': [0.0, -1.4047222363948821, 0.0, 357.4581908416748, 0.0, 0.0, 0.0], 'rewardMean': 0.7739375448017636, 'totalEpisodes': 49, 'stepsPerEpisode': 53, 'rewardPerEpisode': 40.12688598484796
'totalSteps': 11520, 'rewardStep': 0.5803164918150937, 'errorList': [], 'lossList': [0.0, -1.399831503033638, 0.0, 163.12987144470216, 0.0, 0.0, 0.0], 'rewardMean': 0.7524240944699113, 'totalEpisodes': 78, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2019609288517996
'totalSteps': 12800, 'rewardStep': 0.8551312156214228, 'errorList': [], 'lossList': [0.0, -1.3984612995386123, 0.0, 105.20529359817505, 0.0, 0.0, 0.0], 'rewardMean': 0.7626948065850625, 'totalEpisodes': 97, 'stepsPerEpisode': 20, 'rewardPerEpisode': 18.038753147913884
'totalSteps': 14080, 'rewardStep': 0.6105047221419637, 'errorList': [], 'lossList': [0.0, -1.3980177903175355, 0.0, 86.48059511184692, 0.0, 0.0, 0.0], 'rewardMean': 0.7678805069516235, 'totalEpisodes': 112, 'stepsPerEpisode': 53, 'rewardPerEpisode': 27.424875019369228
'totalSteps': 15360, 'rewardStep': 0.6318571909294068, 'errorList': [], 'lossList': [0.0, -1.412266709804535, 0.0, 45.61811815738678, 0.0, 0.0, 0.0], 'rewardMean': 0.7452187347389003, 'totalEpisodes': 121, 'stepsPerEpisode': 178, 'rewardPerEpisode': 131.8835074971811
'totalSteps': 16640, 'rewardStep': 0.7502320548882555, 'errorList': [], 'lossList': [0.0, -1.441405622959137, 0.0, 58.303704986572264, 0.0, 0.0, 0.0], 'rewardMean': 0.7308123070267796, 'totalEpisodes': 130, 'stepsPerEpisode': 70, 'rewardPerEpisode': 59.42858999296488
'totalSteps': 17920, 'rewardStep': 0.9377020019397952, 'errorList': [0.13955576391863814, 4.539557666862622, 0.18333201534204746, 1.0903330027488767, 0.9535381781303732, 2.850815520537752, 1.1915293633772066, 0.7326630451668779, 3.4197678416613817, 0.7680510425503491, 1.8434600690083678, 0.47439669116575023, 1.3029416830175684, 0.8290295162438371, 1.049511991862647, 0.9020823656866216, 1.0413773927071723, 0.3770837953899626, 1.135688543422062, 0.3654915901946674, 1.8219656993988882, 0.40101417752952734, 2.2404701037528545, 1.3317562162988834, 0.8081004265290107, 1.219263445325556, 0.1847806434057439, 2.5706559033346403, 0.5313763827326171, 2.334141241618324, 0.7154085581751005, 2.988283496042907, 0.8480395214054954, 0.15107165125097144, 0.9016408294108602, 1.7862972511505069, 0.5425782551750751, 0.34488114289298005, 0.36749659692153896, 1.4954575581650777, 0.26516340737745275, 0.6367653056064307, 0.1503347851555378, 0.2219627039886856, 0.8916265946316235, 1.1250379830787094, 1.3937586596000053, 0.4941222764234174, 0.8622317645457966, 0.45782386057196756], 'lossList': [0.0, -1.4575083708763124, 0.0, 34.24671285390854, 0.0, 0.0, 0.0], 'rewardMean': 0.7597504237295654, 'totalEpisodes': 134, 'stepsPerEpisode': 403, 'rewardPerEpisode': 352.21550821259694, 'successfulTests': 5
'totalSteps': 19200, 'rewardStep': 0.9137395101444592, 'errorList': [], 'lossList': [0.0, -1.4661989092826844, 0.0, 12.650874019265174, 0.0, 0.0, 0.0], 'rewardMean': 0.7574640421431917, 'totalEpisodes': 137, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.89295680799112
'totalSteps': 20480, 'rewardStep': 0.6097352793658789, 'errorList': [], 'lossList': [0.0, -1.4614055156707764, 0.0, 4.758072734177112, 0.0, 0.0, 0.0], 'rewardMean': 0.7326683716596932, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 933.5782828224469
'totalSteps': 21760, 'rewardStep': 0.7870539829161209, 'errorList': [], 'lossList': [0.0, -1.4326849591732025, 0.0, 3.9346738776564596, 0.0, 0.0, 0.0], 'rewardMean': 0.7157995207682842, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 987.6175378211138
'totalSteps': 23040, 'rewardStep': 0.9284303096302141, 'errorList': [], 'lossList': [0.0, -1.4095452791452407, 0.0, 3.379263217151165, 0.0, 0.0, 0.0], 'rewardMean': 0.7604702759392612, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.8176720893566
'totalSteps': 24320, 'rewardStep': 0.7358201855644126, 'errorList': [], 'lossList': [0.0, -1.3706559520959853, 0.0, 2.6357948775589466, 0.0, 0.0, 0.0], 'rewardMean': 0.7760206453141929, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.26950601434
'totalSteps': 25600, 'rewardStep': 0.9498129818811842, 'errorList': [0.18789113587153458, 0.16160930850835628, 0.17898400486551064, 0.15774021527875057, 0.19266295691236068, 0.1433931920908604, 0.16358218012814296, 0.15795253679541296, 0.1666934712636222, 0.17261396173402818, 0.25999407222069637, 0.1427696610356361, 0.17018832570203407, 0.25548319138702946, 0.17997152940975283, 0.1711400329800537, 0.17262059448214762, 0.16719729408225836, 0.16189979581451205, 0.14270400364346236, 0.19482664359669566, 0.15868626803257876, 0.17936670406614444, 0.17231689950336143, 0.17632027115431062, 0.20188660501242114, 0.1945307667270619, 0.16917587485606428, 0.18522385739271732, 0.17025365743503923, 0.1894055733335555, 0.2116815728225426, 0.18207826836201418, 0.16792885013087808, 0.18528071534152535, 0.1620901140977812, 0.262313434508749, 0.17182595006596746, 0.1596909915615046, 0.21298840185643864, 0.24489532968786254, 0.20953080496107157, 0.16199726152391783, 0.1876478353685361, 0.21551304872672816, 0.26458873217670215, 0.15796995915318024, 0.18362142326406067, 0.17362330143372287, 0.1564137677584691], 'lossList': [0.0, -1.313741177916527, 0.0, 2.190042161270976, 0.0, 0.0, 0.0], 'rewardMean': 0.785488821940169, 'totalEpisodes': 137, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1183.292325220203, 'successfulTests': 40
#maxSuccessfulTests=40, maxSuccessfulTestsAtStep=25600, timeSpent=123.83
