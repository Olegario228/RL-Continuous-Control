#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 1
#computationIndex = 95
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9148206305808281, 'errorList': [], 'lossList': [0.0, -1.430473182797432, 0.0, 88.2532748413086, 0.0, 0.0, 0.0], 'rewardMean': 0.9148206305808281, 'totalEpisodes': 6, 'stepsPerEpisode': 119, 'rewardPerEpisode': 103.40669342337553
'totalSteps': 2560, 'rewardStep': 0.9120077042407186, 'errorList': [], 'lossList': [0.0, -1.4368717914819717, 0.0, 31.96484030485153, 0.0, 0.0, 0.0], 'rewardMean': 0.9134141674107734, 'totalEpisodes': 8, 'stepsPerEpisode': 528, 'rewardPerEpisode': 383.486297417202
'totalSteps': 3840, 'rewardStep': 0.7197694338923317, 'errorList': [], 'lossList': [0.0, -1.4210993820428848, 0.0, 31.217008900642394, 0.0, 0.0, 0.0], 'rewardMean': 0.8488659229046261, 'totalEpisodes': 12, 'stepsPerEpisode': 268, 'rewardPerEpisode': 211.33384700955204
'totalSteps': 5120, 'rewardStep': 0.7379858605387459, 'errorList': [], 'lossList': [0.0, -1.4188255035877229, 0.0, 26.91089489340782, 0.0, 0.0, 0.0], 'rewardMean': 0.8211459073131562, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1004.2508070258322
'totalSteps': 6400, 'rewardStep': 0.9095685625635477, 'errorList': [], 'lossList': [0.0, -1.407564975619316, 0.0, 24.2962427341938, 0.0, 0.0, 0.0], 'rewardMean': 0.8388304383632346, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.9807973907741
'totalSteps': 7680, 'rewardStep': 0.6748059477728728, 'errorList': [], 'lossList': [0.0, -1.3966711562871934, 0.0, 15.0752350923419, 0.0, 0.0, 0.0], 'rewardMean': 0.8114930232648409, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1084.6537019623731
'totalSteps': 8960, 'rewardStep': 0.6489183345478224, 'errorList': [], 'lossList': [0.0, -1.3512610048055649, 0.0, 83.24539481163025, 0.0, 0.0, 0.0], 'rewardMean': 0.7882680677338383, 'totalEpisodes': 17, 'stepsPerEpisode': 82, 'rewardPerEpisode': 70.0441879008647
'totalSteps': 10240, 'rewardStep': 0.7864750015678781, 'errorList': [], 'lossList': [0.0, -1.3489998489618302, 0.0, 452.0290811920166, 0.0, 0.0, 0.0], 'rewardMean': 0.7880439344630932, 'totalEpisodes': 65, 'stepsPerEpisode': 13, 'rewardPerEpisode': 9.865574133305032
'totalSteps': 11520, 'rewardStep': 0.5833438416639586, 'errorList': [], 'lossList': [0.0, -1.3465957587957382, 0.0, 183.60960712432862, 0.0, 0.0, 0.0], 'rewardMean': 0.7652994797076338, 'totalEpisodes': 108, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.9142538742418282
'totalSteps': 12800, 'rewardStep': 0.6272721906741603, 'errorList': [], 'lossList': [0.0, -1.3359711796045304, 0.0, 95.24824241638184, 0.0, 0.0, 0.0], 'rewardMean': 0.7514967508042865, 'totalEpisodes': 150, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.4978347435281
'totalSteps': 14080, 'rewardStep': 0.9381861643465547, 'errorList': [258.2043505972258, 187.19066351420904, 225.62952641200803, 215.64826617785772, 247.93382033711427, 248.63787954572354, 252.12860866964368, 253.66580984407926, 247.63688080654754, 248.24130444826116, 166.23829884831997, 259.1900801653579, 242.14850184306817, 217.6809652936388, 272.0232039653483, 253.32097808926974, 242.04923276380757, 246.89248277329446, 166.39540178243539, 221.0478414450959, 201.63300334898472, 268.770271352765, 258.24517780192116, 224.91762077116258, 227.6541278733378, 243.25917447544688, 262.9572566671786, 278.91954239317016, 271.4896057006725, 245.4010903453493, 238.47631863686195, 210.32346418551654, 268.6805794738176, 239.85959459487069, 259.2350797935548, 250.37576882525718, 201.7326004288158, 239.91578603642992, 223.97245485897216, 263.13728644463873, 152.7769571257874, 249.4192438929921, 224.7756386746914, 240.95112795231324, 202.01502196164668, 260.1177594555374, 109.77884474986817, 225.45875992194902, 183.2697007171454, 218.08717606543684], 'lossList': [0.0, -1.3238401627540588, 0.0, 54.457826118469235, 0.0, 0.0, 0.0], 'rewardMean': 0.7538333041808591, 'totalEpisodes': 179, 'stepsPerEpisode': 18, 'rewardPerEpisode': 13.607025696697713, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.818862927376158, 'errorList': [], 'lossList': [0.0, -1.3145337450504302, 0.0, 36.40491875648499, 0.0, 0.0, 0.0], 'rewardMean': 0.7445188264944032, 'totalEpisodes': 209, 'stepsPerEpisode': 36, 'rewardPerEpisode': 25.85257060951646
'totalSteps': 16640, 'rewardStep': 0.4085840083421064, 'errorList': [], 'lossList': [0.0, -1.2998411792516709, 0.0, 23.535339579582214, 0.0, 0.0, 0.0], 'rewardMean': 0.7134002839393805, 'totalEpisodes': 226, 'stepsPerEpisode': 92, 'rewardPerEpisode': 65.61592011002737
'totalSteps': 17920, 'rewardStep': 0.974309315872887, 'errorList': [134.8195372999001, 162.8622565277396, 179.97359896023858, 167.84185306755722, 171.82699738586237, 211.29584565663677, 170.20224180346563, 138.6030842121367, 131.6840122888431, 174.78363602629395, 226.63871319522212, 209.71786485594177, 150.54006074442026, 205.52055798666944, 160.06608207638456, 77.60235542892038, 211.2320454772212, 122.39967166596749, 139.93899020820453, 203.53148265116346, 173.13148631314127, 107.05301974352744, 128.15182217372077, 188.5531039343022, 58.44618663474916, 179.8237863897409, 225.62799145350402, 138.91549660169542, 144.535926186709, 137.07462420334858, 136.80654146264692, 114.81579518454656, 194.76310921232917, 223.04644776981613, 211.87806598111797, 56.370443741355494, 119.33627712990409, 186.63927780019088, 227.18070702072012, 230.17344912825385, 186.25507329340087, 212.93191549349004, 141.82057772544852, 18.47540411889617, 236.9258938880146, 132.56395136414582, 199.98892097581412, 210.417873350307, 151.3825957279942, 4.032321624430916], 'lossList': [0.0, -1.2948849588632583, 0.0, 20.249455742836, 0.0, 0.0, 0.0], 'rewardMean': 0.7370326294727946, 'totalEpisodes': 242, 'stepsPerEpisode': 38, 'rewardPerEpisode': 33.99874744864289, 'successfulTests': 0
'totalSteps': 19200, 'rewardStep': 0.27989944055617116, 'errorList': [], 'lossList': [0.0, -1.2908482456207275, 0.0, 26.55311549663544, 0.0, 0.0, 0.0], 'rewardMean': 0.6740657172720569, 'totalEpisodes': 252, 'stepsPerEpisode': 107, 'rewardPerEpisode': 71.18107626783652
'totalSteps': 20480, 'rewardStep': 0.8646032837798275, 'errorList': [], 'lossList': [0.0, -1.2872640073299408, 0.0, 9.704977407455445, 0.0, 0.0, 0.0], 'rewardMean': 0.6930454508727524, 'totalEpisodes': 258, 'stepsPerEpisode': 82, 'rewardPerEpisode': 69.89034239145765
'totalSteps': 21760, 'rewardStep': 0.940131469801155, 'errorList': [0.7963522307140612, 0.12780270429647084, 0.7804540915924022, 0.7069496719720072, 0.3222190664871114, 0.0749310008005146, 5.7298548290831715, 0.10307240981780018, 1.0100104031783825, 0.10972813059007, 1.1130846733781454, 0.3224158565824084, 0.10026348848848812, 0.6534386617324996, 0.31702754526223437, 0.08778359830590633, 0.8341674321534666, 0.40569162771501105, 0.15270772004536212, 0.3952341140126673, 0.3303209700299559, 0.5634383351723243, 0.4188006408762209, 0.9117187439023962, 0.5218596055342869, 0.2647852465950918, 0.2682203387031465, 0.09987027391218913, 0.21455761754917488, 1.3065671133700467, 0.4515506157333273, 0.09490882476397865, 1.7386496651238212, 0.21209567542664698, 13.006818416880453, 0.1340043646917591, 0.4751727265290823, 0.6571869541622336, 0.34872221959526545, 0.09182944124835894, 0.5380344585240413, 0.24842544549636708, 1.9350932667401337, 0.31521426184703166, 0.7570985931160176, 7.755175958383103, 6.053106760154403, 0.14112362036389767, 0.1365701912192447, 0.13433335985519793], 'lossList': [0.0, -1.275699497461319, 0.0, 16.803981568813324, 0.0, 0.0, 0.0], 'rewardMean': 0.7221667643980857, 'totalEpisodes': 262, 'stepsPerEpisode': 61, 'rewardPerEpisode': 54.48529084058238, 'successfulTests': 14
'totalSteps': 23040, 'rewardStep': 0.6875577999304675, 'errorList': [], 'lossList': [0.0, -1.25782495200634, 0.0, 6.669931505322456, 0.0, 0.0, 0.0], 'rewardMean': 0.7122750442343446, 'totalEpisodes': 263, 'stepsPerEpisode': 139, 'rewardPerEpisode': 118.67524823576949
'totalSteps': 24320, 'rewardStep': 0.8713602422153335, 'errorList': [], 'lossList': [0.0, -1.2247655421495438, 0.0, 7.071437439918518, 0.0, 0.0, 0.0], 'rewardMean': 0.741076684289482, 'totalEpisodes': 264, 'stepsPerEpisode': 369, 'rewardPerEpisode': 298.5923566588943
'totalSteps': 25600, 'rewardStep': 0.8276107564286835, 'errorList': [], 'lossList': [0.0, -1.1816914808750152, 0.0, 4.480990963280201, 0.0, 0.0, 0.0], 'rewardMean': 0.7611105408649343, 'totalEpisodes': 264, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1088.90697072705
#maxSuccessfulTests=14, maxSuccessfulTestsAtStep=21760, timeSpent=120.51
