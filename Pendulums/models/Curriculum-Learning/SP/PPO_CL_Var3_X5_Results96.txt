#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 2
#computationIndex = 96
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5931778195801594, 'errorList': [], 'lossList': [0.0, -1.4235882580280304, 0.0, 88.58074667930603, 0.0, 0.0, 0.0], 'rewardMean': 0.5931778195801594, 'totalEpisodes': 6, 'stepsPerEpisode': 109, 'rewardPerEpisode': 75.37753892112138
'totalSteps': 2560, 'rewardStep': 0.8507758592896241, 'errorList': [], 'lossList': [0.0, -1.447867882847786, 0.0, 39.36487149715423, 0.0, 0.0, 0.0], 'rewardMean': 0.7219768394348918, 'totalEpisodes': 12, 'stepsPerEpisode': 63, 'rewardPerEpisode': 57.245759495656955
'totalSteps': 3840, 'rewardStep': 0.9118034403074198, 'errorList': [], 'lossList': [0.0, -1.4647365891933442, 0.0, 34.18075919508934, 0.0, 0.0, 0.0], 'rewardMean': 0.7852523730590678, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1027.315747781007
'totalSteps': 5120, 'rewardStep': 0.6863882788264563, 'errorList': [], 'lossList': [0.0, -1.4507139486074447, 0.0, 29.176136791706085, 0.0, 0.0, 0.0], 'rewardMean': 0.7605363495009149, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.4130181720918
'totalSteps': 6400, 'rewardStep': 0.9321260558444097, 'errorList': [], 'lossList': [0.0, -1.4472701096534728, 0.0, 23.235850473642348, 0.0, 0.0, 0.0], 'rewardMean': 0.7948542907696139, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.9252548515346
'totalSteps': 7680, 'rewardStep': 0.8360052402210666, 'errorList': [], 'lossList': [0.0, -1.4251259434223176, 0.0, 18.144048410728573, 0.0, 0.0, 0.0], 'rewardMean': 0.8017127823448559, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.5789536488032
'totalSteps': 8960, 'rewardStep': 0.957540213527908, 'errorList': [119.60252648015263, 95.29343001031036, 116.32498009197732, 116.08283029953071, 120.34528883213844, 108.079718155704, 114.75907593779343, 117.39155595347295, 112.10291041154916, 121.85250365241383, 112.66680243889547, 107.84829665332747, 107.44250582362136, 115.11203774425107, 115.9511040009171, 118.15331834787739, 119.64521089310263, 108.49467606494116, 122.26159214596447, 97.74298893890939, 118.27711596997386, 111.59100672831657, 103.87316776761678, 114.29873862691606, 119.2211165776811, 99.59545497434527, 118.98405128989071, 109.09644803500294, 119.50234219537244, 84.65404956676626, 123.13053621839056, 121.3076113563608, 120.38167077631273, 120.74470967174638, 120.47232378801282, 119.90983817636622, 120.92921194854814, 120.21029882287645, 123.83002735865672, 105.78739645632491, 106.20047467520999, 121.52789229057038, 113.67429280629355, 124.42398794989276, 111.25514659059492, 118.39002032467793, 120.84108203609811, 89.80365414918123, 118.09489886720982, 114.85083888150686], 'lossList': [0.0, -1.4021340519189835, 0.0, 96.53648678779602, 0.0, 0.0, 0.0], 'rewardMean': 0.8239738439424348, 'totalEpisodes': 17, 'stepsPerEpisode': 88, 'rewardPerEpisode': 76.60751749649214, 'successfulTests': 0
'totalSteps': 10240, 'rewardStep': 0.7460867782543753, 'errorList': [], 'lossList': [0.0, -1.4028994286060332, 0.0, 387.44603843688964, 0.0, 0.0, 0.0], 'rewardMean': 0.8142379607314274, 'totalEpisodes': 53, 'stepsPerEpisode': 71, 'rewardPerEpisode': 61.30558830710273
'totalSteps': 11520, 'rewardStep': 0.8493697757913744, 'errorList': [], 'lossList': [0.0, -1.4013451594114303, 0.0, 181.73739067077636, 0.0, 0.0, 0.0], 'rewardMean': 0.8181414957380881, 'totalEpisodes': 91, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.376447920160544
'totalSteps': 12800, 'rewardStep': 0.9408170491511607, 'errorList': [213.8559858083484, 251.8878312968678, 183.56535895777728, 240.90593695823767, 249.04329966806932, 178.92581212839232, 235.74656137401104, 244.46124762609074, 258.06390626328124, 260.1115618411439, 168.28304552878532, 210.34829277857102, 237.77477367844222, 222.06344399488134, 233.21461856556522, 198.41670018148645, 254.00160882978363, 222.72637771716123, 162.1661820513468, 255.18728750032687, 240.51437775532384, 187.89796907940428, 161.96550893704682, 253.34560459836356, 199.4996341724705, 167.45542981250887, 217.34104365120817, 256.1717326801628, 56.47612540919155, 255.25846115877172, 262.61153349146184, 255.11783243796864, 186.1814789041998, 136.191044773072, 192.94922394275815, 203.78547914739048, 170.85918862761642, 225.46975929917195, 204.39916257792302, 218.25020770206496, 244.36236438167478, 244.82119664494613, 201.10578215367823, 234.77997410159088, 249.8861364777528, 256.1943787804478, 234.43825510549624, 227.50036794437347, 209.9329866087558, 246.49884659338392], 'lossList': [0.0, -1.397368034720421, 0.0, 105.29760303497315, 0.0, 0.0, 0.0], 'rewardMean': 0.8304090510793956, 'totalEpisodes': 124, 'stepsPerEpisode': 23, 'rewardPerEpisode': 20.912764345720625, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.7721787505587803, 'errorList': [], 'lossList': [0.0, -1.398230738043785, 0.0, 56.74441567420959, 0.0, 0.0, 0.0], 'rewardMean': 0.8483091441772574, 'totalEpisodes': 141, 'stepsPerEpisode': 93, 'rewardPerEpisode': 78.67335553047367
'totalSteps': 15360, 'rewardStep': 0.5408441710010192, 'errorList': [], 'lossList': [0.0, -1.4018488782644272, 0.0, 40.19889429569244, 0.0, 0.0, 0.0], 'rewardMean': 0.8173159753483971, 'totalEpisodes': 153, 'stepsPerEpisode': 144, 'rewardPerEpisode': 119.0234508031919
'totalSteps': 16640, 'rewardStep': 0.7867397441660422, 'errorList': [], 'lossList': [0.0, -1.4032223725318909, 0.0, 21.177648198604583, 0.0, 0.0, 0.0], 'rewardMean': 0.8048096057342592, 'totalEpisodes': 162, 'stepsPerEpisode': 46, 'rewardPerEpisode': 39.10007467407488
'totalSteps': 17920, 'rewardStep': 0.4487072486000898, 'errorList': [], 'lossList': [0.0, -1.380910066962242, 0.0, 37.138705024719236, 0.0, 0.0, 0.0], 'rewardMean': 0.7810415027116225, 'totalEpisodes': 171, 'stepsPerEpisode': 116, 'rewardPerEpisode': 78.59600626028985
'totalSteps': 19200, 'rewardStep': 0.928088740556216, 'errorList': [], 'lossList': [0.0, -1.3880557280778885, 0.0, 11.877416841983795, 0.0, 0.0, 0.0], 'rewardMean': 0.7806377711828032, 'totalEpisodes': 177, 'stepsPerEpisode': 76, 'rewardPerEpisode': 65.54431896590123
'totalSteps': 20480, 'rewardStep': 0.4787995357680933, 'errorList': [], 'lossList': [0.0, -1.3928551149368287, 0.0, 9.652999427318573, 0.0, 0.0, 0.0], 'rewardMean': 0.744917200737506, 'totalEpisodes': 183, 'stepsPerEpisode': 129, 'rewardPerEpisode': 106.1414922617945
'totalSteps': 21760, 'rewardStep': 0.5385163165555824, 'errorList': [], 'lossList': [0.0, -1.367742539048195, 0.0, 13.544459416866303, 0.0, 0.0, 0.0], 'rewardMean': 0.7030148110402734, 'totalEpisodes': 189, 'stepsPerEpisode': 255, 'rewardPerEpisode': 210.93856556216667
'totalSteps': 23040, 'rewardStep': 0.7055610308511455, 'errorList': [], 'lossList': [0.0, -1.3453874969482422, 0.0, 4.508074409961701, 0.0, 0.0, 0.0], 'rewardMean': 0.6989622362999504, 'totalEpisodes': 193, 'stepsPerEpisode': 377, 'rewardPerEpisode': 317.3221102948988
'totalSteps': 24320, 'rewardStep': 0.8050651153774379, 'errorList': [], 'lossList': [0.0, -1.3340414249897004, 0.0, 5.06306057035923, 0.0, 0.0, 0.0], 'rewardMean': 0.6945317702585567, 'totalEpisodes': 196, 'stepsPerEpisode': 368, 'rewardPerEpisode': 308.14635413111097
'totalSteps': 25600, 'rewardStep': 0.904676233251842, 'errorList': [], 'lossList': [0.0, -1.3041217720508576, 0.0, 2.2050875429809094, 0.0, 0.0, 0.0], 'rewardMean': 0.6909176886686249, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.5929206546648
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=105.16
