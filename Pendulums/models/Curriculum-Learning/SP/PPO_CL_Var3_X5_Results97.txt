#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 3
#computationIndex = 97
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5031008479040118, 'errorList': [], 'lossList': [0.0, -1.426186809539795, 0.0, 78.71894006252289, 0.0, 0.0, 0.0], 'rewardMean': 0.5031008479040118, 'totalEpisodes': 7, 'stepsPerEpisode': 257, 'rewardPerEpisode': 177.20252901206598
'totalSteps': 2560, 'rewardStep': 0.8663771044193703, 'errorList': [], 'lossList': [0.0, -1.442423214316368, 0.0, 30.245499093532562, 0.0, 0.0, 0.0], 'rewardMean': 0.6847389761616911, 'totalEpisodes': 10, 'stepsPerEpisode': 905, 'rewardPerEpisode': 676.1524439222885
'totalSteps': 3840, 'rewardStep': 0.8320175941670513, 'errorList': [], 'lossList': [0.0, -1.4595734870433807, 0.0, 44.81756754398346, 0.0, 0.0, 0.0], 'rewardMean': 0.7338318488301446, 'totalEpisodes': 12, 'stepsPerEpisode': 668, 'rewardPerEpisode': 531.3188124885305
'totalSteps': 5120, 'rewardStep': 0.9384131010455535, 'errorList': [], 'lossList': [0.0, -1.4607241833209992, 0.0, 29.422164636850358, 0.0, 0.0, 0.0], 'rewardMean': 0.7849771618839968, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1029.4682070909428
'totalSteps': 6400, 'rewardStep': 0.6840437047685054, 'errorList': [], 'lossList': [0.0, -1.4477986365556716, 0.0, 24.384143233299255, 0.0, 0.0, 0.0], 'rewardMean': 0.7647904704608985, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.769465440869
'totalSteps': 7680, 'rewardStep': 0.7592992760073058, 'errorList': [], 'lossList': [0.0, -1.4280514580011368, 0.0, 13.9384316021204, 0.0, 0.0, 0.0], 'rewardMean': 0.7638752713852996, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1042.9396280254318
'totalSteps': 8960, 'rewardStep': 0.8185521289703659, 'errorList': [], 'lossList': [0.0, -1.4254817855358124, 0.0, 158.26941984176636, 0.0, 0.0, 0.0], 'rewardMean': 0.7716862510403091, 'totalEpisodes': 22, 'stepsPerEpisode': 117, 'rewardPerEpisode': 92.89344076409867
'totalSteps': 10240, 'rewardStep': 0.6011906335908791, 'errorList': [], 'lossList': [0.0, -1.4248956817388534, 0.0, 360.6297938537598, 0.0, 0.0, 0.0], 'rewardMean': 0.7503742988591304, 'totalEpisodes': 56, 'stepsPerEpisode': 31, 'rewardPerEpisode': 18.280864536540086
'totalSteps': 11520, 'rewardStep': 0.34784664059365467, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6698687672060354, 'totalEpisodes': 92, 'stepsPerEpisode': 62, 'rewardPerEpisode': 45.09064762876094
'totalSteps': 12800, 'rewardStep': 0.9117590571441435, 'errorList': [], 'lossList': [0.0, -1.4236612462997436, 0.0, 288.91901596069334, 0.0, 0.0, 0.0], 'rewardMean': 0.7107345881300485, 'totalEpisodes': 145, 'stepsPerEpisode': 25, 'rewardPerEpisode': 22.512545156277834
'totalSteps': 14080, 'rewardStep': 0.8609083761316703, 'errorList': [], 'lossList': [0.0, -1.4204281574487687, 0.0, 81.80344715118409, 0.0, 0.0, 0.0], 'rewardMean': 0.7101877153012784, 'totalEpisodes': 187, 'stepsPerEpisode': 60, 'rewardPerEpisode': 47.102227626484584
'totalSteps': 15360, 'rewardStep': 0.7125346335580999, 'errorList': [], 'lossList': [0.0, -1.4164620411396027, 0.0, 63.38471609115601, 0.0, 0.0, 0.0], 'rewardMean': 0.6982394192403832, 'totalEpisodes': 215, 'stepsPerEpisode': 37, 'rewardPerEpisode': 32.274132990219336
'totalSteps': 16640, 'rewardStep': 0.8921578255160545, 'errorList': [], 'lossList': [0.0, -1.4098970419168473, 0.0, 41.74089250564575, 0.0, 0.0, 0.0], 'rewardMean': 0.6936138916874335, 'totalEpisodes': 237, 'stepsPerEpisode': 12, 'rewardPerEpisode': 11.238294911559883
'totalSteps': 17920, 'rewardStep': 0.2946415777594399, 'errorList': [], 'lossList': [0.0, -1.4111658948659898, 0.0, 27.4523748254776, 0.0, 0.0, 0.0], 'rewardMean': 0.6546736789865268, 'totalEpisodes': 250, 'stepsPerEpisode': 94, 'rewardPerEpisode': 59.87870365323219
'totalSteps': 19200, 'rewardStep': 0.9478794427951199, 'errorList': [40.99904092323154, 159.65716401466716, 62.74482821777133, 135.60879824882062, 167.12604800500927, 177.35855546119927, 72.15441359228409, 128.19901661457573, 144.78963294210826, 94.22015842648091, 10.270300420792566, 154.617045739164, 148.41746525878318, 122.63426777406438, 1.4954990366823668, 8.670027300121305, 174.64139226088787, 3.1807139010952903, 71.71190736183985, 73.21731337567581, 34.60941785401341, 130.62886279312283, 131.28892967312393, 0.9111677957592627, 145.27969852224902, 204.48606310637004, 113.42292659276067, 174.22037398476053, 22.284822018994713, 55.51951012093819, 43.31827566195337, 9.284310403265021, 119.968133271736, 78.43117061349997, 57.05564767828665, 103.52317555951953, 24.05279335109957, 18.384771751628637, 169.08373967000037, 9.97120638287687, 13.868607423808077, 47.36041971296972, 15.39683683292848, 136.18635848049317, 150.2718136572404, 60.84337767967085, 19.932096695268772, 99.13686575819652, 114.20369649213073, 106.49656066309623], 'lossList': [0.0, -1.4048429560661315, 0.0, 25.523114075660704, 0.0, 0.0, 0.0], 'rewardMean': 0.6735316956653082, 'totalEpisodes': 259, 'stepsPerEpisode': 76, 'rewardPerEpisode': 70.59416019684446, 'successfulTests': 0
'totalSteps': 20480, 'rewardStep': 0.7391365844240542, 'errorList': [], 'lossList': [0.0, -1.3908426535129548, 0.0, 17.064944517612457, 0.0, 0.0, 0.0], 'rewardMean': 0.665590141210677, 'totalEpisodes': 265, 'stepsPerEpisode': 70, 'rewardPerEpisode': 56.6599774508832
'totalSteps': 21760, 'rewardStep': 0.7007389879050523, 'errorList': [], 'lossList': [0.0, -1.3955053246021272, 0.0, 18.317192063331603, 0.0, 0.0, 0.0], 'rewardMean': 0.6755449766420945, 'totalEpisodes': 269, 'stepsPerEpisode': 124, 'rewardPerEpisode': 93.30691867468309
'totalSteps': 23040, 'rewardStep': 0.8759142922095706, 'errorList': [], 'lossList': [0.0, -1.3955281180143357, 0.0, 14.722486991882324, 0.0, 0.0, 0.0], 'rewardMean': 0.728351741803686, 'totalEpisodes': 273, 'stepsPerEpisode': 99, 'rewardPerEpisode': 82.9863212407961
'totalSteps': 24320, 'rewardStep': 0.8006646702005478, 'errorList': [], 'lossList': [0.0, -1.3856294041872024, 0.0, 7.8800655989348884, 0.0, 0.0, 0.0], 'rewardMean': 0.7736335447643753, 'totalEpisodes': 273, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.3768522565467
'totalSteps': 25600, 'rewardStep': 0.6571612069602717, 'errorList': [], 'lossList': [0.0, -1.3687146884202956, 0.0, 5.498871014863252, 0.0, 0.0, 0.0], 'rewardMean': 0.7481737597459881, 'totalEpisodes': 273, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1081.795299849653
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.38
