#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 4
#computationIndex = 98
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8989304158267404, 'errorList': [], 'lossList': [0.0, -1.4210914880037309, 0.0, 72.74409552574157, 0.0, 0.0, 0.0], 'rewardMean': 0.8989304158267404, 'totalEpisodes': 13, 'stepsPerEpisode': 29, 'rewardPerEpisode': 25.324097304620388
'totalSteps': 2560, 'rewardStep': 0.5743627656933739, 'errorList': [], 'lossList': [0.0, -1.4227778202295303, 0.0, 32.65558960080147, 0.0, 0.0, 0.0], 'rewardMean': 0.7366465907600571, 'totalEpisodes': 16, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.26844935126647
'totalSteps': 3840, 'rewardStep': 0.964233653347865, 'errorList': [], 'lossList': [0.0, -1.4228087550401687, 0.0, 36.02404155015945, 0.0, 0.0, 0.0], 'rewardMean': 0.8125089449559931, 'totalEpisodes': 18, 'stepsPerEpisode': 486, 'rewardPerEpisode': 404.35850857668345
'totalSteps': 5120, 'rewardStep': 0.8489332800466802, 'errorList': [], 'lossList': [0.0, -1.4183608996868133, 0.0, 32.79766618013382, 0.0, 0.0, 0.0], 'rewardMean': 0.8216150287286649, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.797824144021
'totalSteps': 6400, 'rewardStep': 0.8187298368218313, 'errorList': [], 'lossList': [0.0, -1.4018660306930542, 0.0, 24.05562243402004, 0.0, 0.0, 0.0], 'rewardMean': 0.8210379903472982, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.5241271955426
'totalSteps': 7680, 'rewardStep': 0.9756297585513771, 'errorList': [], 'lossList': [0.0, -1.3894088619947433, 0.0, 16.675635159760713, 0.0, 0.0, 0.0], 'rewardMean': 0.846803285047978, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.0084869820691
'totalSteps': 8960, 'rewardStep': 0.8396524351924309, 'errorList': [], 'lossList': [0.0, -1.371964653134346, 0.0, 24.447524691820146, 0.0, 0.0, 0.0], 'rewardMean': 0.845781735068614, 'totalEpisodes': 19, 'stepsPerEpisode': 790, 'rewardPerEpisode': 593.4132039520994
'totalSteps': 10240, 'rewardStep': 0.9925259831520848, 'errorList': [94.6883271993593, 78.78388898299605, 87.00098661199326, 89.90916039941477, 83.40926160876728, 99.77394303611308, 86.33468639485555, 88.48320912449132, 91.95237777459216, 99.9742105033791, 85.1467295617154, 98.76623906930097, 97.84187489242214, 95.99366209275026, 87.91479306021407, 94.97650279217333, 98.83136593344686, 87.24704508999974, 101.45070950001713, 100.75287620478386, 98.92663099198047, 94.03004190176065, 90.01266030274219, 89.3059377513549, 97.8427489457619, 91.17931662668056, 87.70789391711571, 95.44418360999559, 90.87074488287111, 82.93584243409248, 81.94200063625844, 97.25755306453297, 80.78279186673716, 90.97457617540576, 92.14381694260537, 91.40870836799803, 77.5299733420038, 86.40766838082764, 97.39038015659884, 79.28431555156024, 90.58962715048467, 84.80410457220532, 86.1539490918981, 88.70655586674454, 89.49292243809964, 89.44343152417687, 89.96354126756542, 88.87591678087536, 99.53228616015626, 88.33852407615278], 'lossList': [0.0, -1.3612025040388107, 0.0, 428.2628514099121, 0.0, 0.0, 0.0], 'rewardMean': 0.864124766079048, 'totalEpisodes': 58, 'stepsPerEpisode': 9, 'rewardPerEpisode': 8.261568304928234, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.6243195498620281, 'errorList': [], 'lossList': [0.0, -1.3618285846710205, 0.0, 193.43902759552003, 0.0, 0.0, 0.0], 'rewardMean': 0.8374797420549346, 'totalEpisodes': 94, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.2821626957512557
'totalSteps': 12800, 'rewardStep': 0.8495529066772863, 'errorList': [], 'lossList': [0.0, -1.3611896544694901, 0.0, 102.28559434890747, 0.0, 0.0, 0.0], 'rewardMean': 0.8386870585171697, 'totalEpisodes': 134, 'stepsPerEpisode': 55, 'rewardPerEpisode': 47.79879930090784
'totalSteps': 14080, 'rewardStep': 0.8904347507741134, 'errorList': [], 'lossList': [0.0, -1.3627387964725495, 0.0, 83.85586719512939, 0.0, 0.0, 0.0], 'rewardMean': 0.8378374920119072, 'totalEpisodes': 171, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8904347507741134
'totalSteps': 15360, 'rewardStep': 0.6415766996206667, 'errorList': [], 'lossList': [0.0, -1.3715533083677292, 0.0, 75.44888788223267, 0.0, 0.0, 0.0], 'rewardMean': 0.8445588854046363, 'totalEpisodes': 205, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.954728327305919
'totalSteps': 16640, 'rewardStep': 0.6830333681753508, 'errorList': [], 'lossList': [0.0, -1.3832264471054077, 0.0, 48.61411396980286, 0.0, 0.0, 0.0], 'rewardMean': 0.816438856887385, 'totalEpisodes': 221, 'stepsPerEpisode': 35, 'rewardPerEpisode': 23.248634637170806
'totalSteps': 17920, 'rewardStep': 0.26575087408284986, 'errorList': [], 'lossList': [0.0, -1.3956611555814744, 0.0, 42.4472604560852, 0.0, 0.0, 0.0], 'rewardMean': 0.7581206162910019, 'totalEpisodes': 232, 'stepsPerEpisode': 84, 'rewardPerEpisode': 45.366426499233334
'totalSteps': 19200, 'rewardStep': 0.4954459910104724, 'errorList': [], 'lossList': [0.0, -1.3939846390485764, 0.0, 22.72939248561859, 0.0, 0.0, 0.0], 'rewardMean': 0.725792231709866, 'totalEpisodes': 242, 'stepsPerEpisode': 87, 'rewardPerEpisode': 62.208046715091676
'totalSteps': 20480, 'rewardStep': 0.8411336926875959, 'errorList': [], 'lossList': [0.0, -1.3787620347738265, 0.0, 11.853824237585068, 0.0, 0.0, 0.0], 'rewardMean': 0.7123426251234879, 'totalEpisodes': 249, 'stepsPerEpisode': 74, 'rewardPerEpisode': 61.313669833278084
'totalSteps': 21760, 'rewardStep': 0.5868103303475348, 'errorList': [], 'lossList': [0.0, -1.379712725877762, 0.0, 9.000842792987823, 0.0, 0.0, 0.0], 'rewardMean': 0.6870584146389983, 'totalEpisodes': 255, 'stepsPerEpisode': 129, 'rewardPerEpisode': 98.89449882013794
'totalSteps': 23040, 'rewardStep': 0.5982657669910494, 'errorList': [], 'lossList': [0.0, -1.3844645726680755, 0.0, 5.227979772686958, 0.0, 0.0, 0.0], 'rewardMean': 0.6476323930228947, 'totalEpisodes': 259, 'stepsPerEpisode': 174, 'rewardPerEpisode': 129.36840510968088
'totalSteps': 24320, 'rewardStep': 0.38928401305862625, 'errorList': [], 'lossList': [0.0, -1.387666144967079, 0.0, 5.261671732664109, 0.0, 0.0, 0.0], 'rewardMean': 0.6241288393425546, 'totalEpisodes': 262, 'stepsPerEpisode': 184, 'rewardPerEpisode': 148.08689611536366
'totalSteps': 25600, 'rewardStep': 0.8647624064388938, 'errorList': [], 'lossList': [0.0, -1.3716474342346192, 0.0, 13.429686925411225, 0.0, 0.0, 0.0], 'rewardMean': 0.6256497893187154, 'totalEpisodes': 263, 'stepsPerEpisode': 1254, 'rewardPerEpisode': 872.0665862359239
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=80.64
