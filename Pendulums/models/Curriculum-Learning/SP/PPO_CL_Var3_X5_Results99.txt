#parameter variation file for learning
#varied parameters:
#decaySteps_0 = 0
#decaySteps_1 = 8000.0
#controlValues_00 = 1
#controlValues_01 = 10.0
#controlValues_10 = 0
#controlValues_11 = 0
#case = 5
#computationIndex = 99
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 25000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_CL_Var3_X5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'x5', 'decaySteps': [0, 8000.0], 'controlValues': [[1, 10.0], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.749290864299281, 'errorList': [], 'lossList': [0.0, -1.419513486623764, 0.0, 72.59169123649598, 0.0, 0.0, 0.0], 'rewardMean': 0.749290864299281, 'totalEpisodes': 9, 'stepsPerEpisode': 167, 'rewardPerEpisode': 112.50973888254191
'totalSteps': 2560, 'rewardStep': 0.8201824534634126, 'errorList': [], 'lossList': [0.0, -1.4210877710580825, 0.0, 28.443082044124605, 0.0, 0.0, 0.0], 'rewardMean': 0.7847366588813468, 'totalEpisodes': 11, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 776.9646258462448
'totalSteps': 3840, 'rewardStep': 0.7652269385136491, 'errorList': [], 'lossList': [0.0, -1.4348081308603287, 0.0, 35.18830914735794, 0.0, 0.0, 0.0], 'rewardMean': 0.7782334187587808, 'totalEpisodes': 14, 'stepsPerEpisode': 161, 'rewardPerEpisode': 125.39238123598057
'totalSteps': 5120, 'rewardStep': 0.7014982546468707, 'errorList': [], 'lossList': [0.0, -1.4309523165225984, 0.0, 25.846593386530877, 0.0, 0.0, 0.0], 'rewardMean': 0.7590496277308033, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1001.2207698075897
'totalSteps': 6400, 'rewardStep': 0.9059531615022453, 'errorList': [], 'lossList': [0.0, -1.4251890110969543, 0.0, 18.064820623993874, 0.0, 0.0, 0.0], 'rewardMean': 0.7884303344850917, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.1081791507426
'totalSteps': 7680, 'rewardStep': 0.8437781606744512, 'errorList': [], 'lossList': [0.0, -1.4305635356903077, 0.0, 11.476042535305023, 0.0, 0.0, 0.0], 'rewardMean': 0.7976549721833183, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 972.0676399195088
'totalSteps': 8960, 'rewardStep': 0.8034907535988353, 'errorList': [], 'lossList': [0.0, -1.428200621008873, 0.0, 297.78040321350096, 0.0, 0.0, 0.0], 'rewardMean': 0.7984886552426778, 'totalEpisodes': 42, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.164127204441339
'totalSteps': 10240, 'rewardStep': 0.7956827452410081, 'errorList': [], 'lossList': [0.0, -1.4265147024393081, 0.0, 266.73659576416014, 0.0, 0.0, 0.0], 'rewardMean': 0.7981379164924692, 'totalEpisodes': 99, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.352740311491521
'totalSteps': 11520, 'rewardStep': 0.7022561843103433, 'errorList': [], 'lossList': [0.0, -1.423108474612236, 0.0, 90.8167195892334, 0.0, 0.0, 0.0], 'rewardMean': 0.7874843906944552, 'totalEpisodes': 153, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.339535011331575
'totalSteps': 12800, 'rewardStep': 0.7764205698206463, 'errorList': [], 'lossList': [0.0, -1.4172244548797608, 0.0, 59.59179212570191, 0.0, 0.0, 0.0], 'rewardMean': 0.7863780086070744, 'totalEpisodes': 194, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.084295854067809
'totalSteps': 14080, 'rewardStep': 0.7207304512152657, 'errorList': [], 'lossList': [0.0, -1.409565726518631, 0.0, 44.077004528045656, 0.0, 0.0, 0.0], 'rewardMean': 0.7835219672986727, 'totalEpisodes': 215, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.83597544876084
'totalSteps': 15360, 'rewardStep': 0.8679541357344096, 'errorList': [], 'lossList': [0.0, -1.4009267854690552, 0.0, 37.35843976020813, 0.0, 0.0, 0.0], 'rewardMean': 0.7882991355257725, 'totalEpisodes': 233, 'stepsPerEpisode': 54, 'rewardPerEpisode': 49.92741395418407
'totalSteps': 16640, 'rewardStep': 0.36603215251744947, 'errorList': [], 'lossList': [0.0, -1.3953314739465714, 0.0, 32.50548079490662, 0.0, 0.0, 0.0], 'rewardMean': 0.7483796569261525, 'totalEpisodes': 244, 'stepsPerEpisode': 220, 'rewardPerEpisode': 165.6981913009674
'totalSteps': 17920, 'rewardStep': 0.5804945569218178, 'errorList': [], 'lossList': [0.0, -1.3968123608827592, 0.0, 17.89472769021988, 0.0, 0.0, 0.0], 'rewardMean': 0.7362792871536473, 'totalEpisodes': 251, 'stepsPerEpisode': 157, 'rewardPerEpisode': 119.6211582604102
'totalSteps': 19200, 'rewardStep': 0.8432933591574089, 'errorList': [], 'lossList': [0.0, -1.3924569809436798, 0.0, 28.586006650924684, 0.0, 0.0, 0.0], 'rewardMean': 0.7300133069191636, 'totalEpisodes': 258, 'stepsPerEpisode': 102, 'rewardPerEpisode': 81.23869472362563
'totalSteps': 20480, 'rewardStep': 0.8194221009219386, 'errorList': [], 'lossList': [0.0, -1.3809297543764114, 0.0, 24.855593376159668, 0.0, 0.0, 0.0], 'rewardMean': 0.7275777009439123, 'totalEpisodes': 261, 'stepsPerEpisode': 167, 'rewardPerEpisode': 139.55770277594274
'totalSteps': 21760, 'rewardStep': 0.3787752485560393, 'errorList': [], 'lossList': [0.0, -1.376892812848091, 0.0, 6.6605986493825915, 0.0, 0.0, 0.0], 'rewardMean': 0.6851061504396327, 'totalEpisodes': 263, 'stepsPerEpisode': 357, 'rewardPerEpisode': 273.62717721119026
'totalSteps': 23040, 'rewardStep': 0.8332746405720728, 'errorList': [], 'lossList': [0.0, -1.380662037730217, 0.0, 7.8051541727781295, 0.0, 0.0, 0.0], 'rewardMean': 0.6888653399727392, 'totalEpisodes': 265, 'stepsPerEpisode': 723, 'rewardPerEpisode': 595.4209381941247
'totalSteps': 24320, 'rewardStep': 0.7412925675116173, 'errorList': [], 'lossList': [0.0, -1.362127010822296, 0.0, 3.26191348195076, 0.0, 0.0, 0.0], 'rewardMean': 0.6927689782928665, 'totalEpisodes': 265, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.1066004513511
'totalSteps': 25600, 'rewardStep': 0.9661039217517775, 'errorList': [0.0558769844056249, 0.050455716760476436, 0.04975348610054922, 0.05282733322406591, 0.05622881741158164, 0.050458636990769384, 0.05635660027829269, 0.049540708463491616, 0.04837707638420714, 0.049293863663643016, 0.049064161731939, 0.04904382316788737, 0.06194494407014638, 0.06871825470556431, 0.050838380356317894, 0.049690370312438836, 0.04917609541280748, 0.0541735839528326, 0.06519272489467719, 0.052714357429924856, 0.051283632210816124, 0.05782406631407998, 0.05145594252279644, 0.04857989093407523, 0.049681043832282154, 0.0496265095375565, 0.04970602979306777, 0.047804231828753, 0.053145612561390804, 0.05921907059521418, 0.048604298539646186, 0.049017688902792504, 0.05111181579442616, 0.061621804094711415, 0.05026551998214269, 0.051372717048819704, 0.05227221448496729, 0.04958059062975367, 0.04877495438727309, 0.06358082070189706, 0.049688442482247734, 0.05139126131319172, 0.04916056392402619, 0.04902303635630955, 0.04853340565026256, 0.0497666508382133, 0.0666931306751802, 0.053122657141986794, 0.047420049480182344, 0.05221077038568624], 'lossList': [0.0, -1.329025981426239, 0.0, 3.376563694179058, 0.0, 0.0, 0.0], 'rewardMean': 0.7117373134859797, 'totalEpisodes': 265, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.0480185622812, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.73
