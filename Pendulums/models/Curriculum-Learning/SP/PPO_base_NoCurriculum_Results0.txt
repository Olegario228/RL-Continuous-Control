#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': (0, 5000, 8000), 'controlValues': [[2, 4], [0, 0], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9543837330523498, 'errorList': [], 'lossList': [0.0, -1.3992200100421905, 0.0, 38.18412993431091, 0.0, 0.0, 0.0], 'rewardMean': 0.9543837330523498, 'totalEpisodes': 40, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.8454518244399503
'totalSteps': 2560, 'rewardStep': 0.8009670208883529, 'errorList': [], 'lossList': [0.0, -1.369187970161438, 0.0, 30.795922451019287, 0.0, 0.0, 0.0], 'rewardMean': 0.8776753769703514, 'totalEpisodes': 104, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.902812113962465
'totalSteps': 3840, 'rewardStep': 0.8184124663159601, 'errorList': [], 'lossList': [0.0, -1.3425186043977737, 0.0, 35.52550619125366, 0.0, 0.0, 0.0], 'rewardMean': 0.8579210734188876, 'totalEpisodes': 164, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.422317421918774
'totalSteps': 5120, 'rewardStep': 0.7811463180754711, 'errorList': [], 'lossList': [0.0, -1.3217716032266618, 0.0, 37.34587900161743, 0.0, 0.0, 0.0], 'rewardMean': 0.8387273845830334, 'totalEpisodes': 198, 'stepsPerEpisode': 68, 'rewardPerEpisode': 52.08825138228561
'totalSteps': 6400, 'rewardStep': 0.8916202459419956, 'errorList': [], 'lossList': [0.0, -1.3104299092292786, 0.0, 39.19432192325592, 0.0, 0.0, 0.0], 'rewardMean': 0.8493059568548258, 'totalEpisodes': 214, 'stepsPerEpisode': 128, 'rewardPerEpisode': 104.29766842699148
'totalSteps': 7680, 'rewardStep': 0.6457266228406807, 'errorList': [], 'lossList': [0.0, -1.3002967536449432, 0.0, 22.255210318565368, 0.0, 0.0, 0.0], 'rewardMean': 0.8153760678524683, 'totalEpisodes': 220, 'stepsPerEpisode': 193, 'rewardPerEpisode': 139.32892152008813
'totalSteps': 8960, 'rewardStep': 0.7102028072029292, 'errorList': [], 'lossList': [0.0, -1.2991305130720139, 0.0, 17.148789333105086, 0.0, 0.0, 0.0], 'rewardMean': 0.8003513163311056, 'totalEpisodes': 225, 'stepsPerEpisode': 142, 'rewardPerEpisode': 111.69589911953446
'totalSteps': 10240, 'rewardStep': 0.502549898621705, 'errorList': [], 'lossList': [0.0, -1.3084392482042313, 0.0, 45.14903494358063, 0.0, 0.0, 0.0], 'rewardMean': 0.7631261391174305, 'totalEpisodes': 230, 'stepsPerEpisode': 110, 'rewardPerEpisode': 80.45513060944148
'totalSteps': 11520, 'rewardStep': 0.7491061748810778, 'errorList': [], 'lossList': [0.0, -1.2963365304470063, 0.0, 42.886843633651736, 0.0, 0.0, 0.0], 'rewardMean': 0.7615683653133913, 'totalEpisodes': 236, 'stepsPerEpisode': 67, 'rewardPerEpisode': 59.160094633077684
'totalSteps': 12800, 'rewardStep': 0.7946180417527352, 'errorList': [], 'lossList': [0.0, -1.2816386222839355, 0.0, 10.507856440544128, 0.0, 0.0, 0.0], 'rewardMean': 0.7648733329573257, 'totalEpisodes': 237, 'stepsPerEpisode': 633, 'rewardPerEpisode': 524.6348735510327
'totalSteps': 14080, 'rewardStep': 0.29017821455932025, 'errorList': [], 'lossList': [0.0, -1.258625215291977, 0.0, 6.9225532031059265, 0.0, 0.0, 0.0], 'rewardMean': 0.6984527811080228, 'totalEpisodes': 239, 'stepsPerEpisode': 453, 'rewardPerEpisode': 344.8950330100729
'totalSteps': 15360, 'rewardStep': 0.5122026146668461, 'errorList': [], 'lossList': [0.0, -1.2468344837427139, 0.0, 4.560393881797791, 0.0, 0.0, 0.0], 'rewardMean': 0.669576340485872, 'totalEpisodes': 241, 'stepsPerEpisode': 328, 'rewardPerEpisode': 234.3260555762286
'totalSteps': 16640, 'rewardStep': 0.6879299969095933, 'errorList': [], 'lossList': [0.0, -1.2408664441108703, 0.0, 4.805232934951782, 0.0, 0.0, 0.0], 'rewardMean': 0.6565280935452353, 'totalEpisodes': 243, 'stepsPerEpisode': 685, 'rewardPerEpisode': 556.7729625682322
'totalSteps': 17920, 'rewardStep': 0.8366099898467518, 'errorList': [], 'lossList': [0.0, -1.2444295936822891, 0.0, 4.2677878832817076, 0.0, 0.0, 0.0], 'rewardMean': 0.6620744607223635, 'totalEpisodes': 246, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.83816605451227
'totalSteps': 19200, 'rewardStep': 0.8582012834075259, 'errorList': [], 'lossList': [0.0, -1.241588187813759, 0.0, 6.181357176303863, 0.0, 0.0, 0.0], 'rewardMean': 0.6587325644689166, 'totalEpisodes': 247, 'stepsPerEpisode': 818, 'rewardPerEpisode': 684.4821989521083
'totalSteps': 20480, 'rewardStep': 0.6206654974397636, 'errorList': [], 'lossList': [0.0, -1.2352756297588348, 0.0, 3.722170453071594, 0.0, 0.0, 0.0], 'rewardMean': 0.6562264519288249, 'totalEpisodes': 248, 'stepsPerEpisode': 736, 'rewardPerEpisode': 601.3602878983925
'totalSteps': 21760, 'rewardStep': 0.9682556892415165, 'errorList': [0.02625814473478606, 0.011154157150351493, 0.02589247998134297, 0.19167338388492342, 0.07767959961959121, 0.009630093741264868, 0.09988526864662585, 0.033326929124835404, 0.13165254486272934, 0.05827787617737376, 0.07086453608338014, 0.08263351194694549, 0.1695849340987379, 0.02847757488120436, 0.06700690163438976, 0.06856394049528605, 0.028441611744688115, 0.24239991606047784, 0.029068787796294855, 0.04461093578724209, 0.21576461506988545, 0.010189853603430957, 0.03295406578790758, 0.09870025606004457, 0.05611421391769993, 0.014823843069751702, 0.0397334329047721, 0.007544436556949422, 0.17736276738621445, 0.01502690543785684, 0.028920838396746845, 0.09508343638090565, 0.08092689680042603, 0.12146091007770012, 0.04491516573811855, 0.056962982279290475, 0.03974380418016408, 0.023487151601361833, 0.018595296460112695, 0.049073051847930656, 0.123180434533937, 0.050741983699094814, 0.046211122903615294, 0.1911050515045127, 0.030984365535004013, 0.12353987560054411, 0.07199857619158498, 0.13754074589735804, 0.11560840386062068, 0.08772602294220201], 'lossList': [0.0, -1.2346455121040345, 0.0, 3.9988222943246363, 0.0, 0.0, 0.0], 'rewardMean': 0.6820317401326835, 'totalEpisodes': 249, 'stepsPerEpisode': 1101, 'rewardPerEpisode': 953.5436006925172, 'successfulTests': 48
'totalSteps': 23040, 'rewardStep': 0.8043990708199299, 'errorList': [], 'lossList': [0.0, -1.222902343273163, 0.0, 0.8901597853749991, 0.0, 0.0, 0.0], 'rewardMean': 0.712216657352506, 'totalEpisodes': 249, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.0583586221994
'totalSteps': 24320, 'rewardStep': 0.8086074655562494, 'errorList': [], 'lossList': [0.0, -1.1952798640727997, 0.0, 0.7094929916411639, 0.0, 0.0, 0.0], 'rewardMean': 0.7181667864200232, 'totalEpisodes': 249, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.3871130539385
'totalSteps': 25600, 'rewardStep': 0.8668629331993419, 'errorList': [], 'lossList': [0.0, -1.1521198326349258, 0.0, 0.4621140969917178, 0.0, 0.0, 0.0], 'rewardMean': 0.725391275564684, 'totalEpisodes': 249, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1149.1157259002528
'totalSteps': 26880, 'rewardStep': 0.9658067283305147, 'errorList': [0.05439920808104003, 0.05546113219296004, 0.10707538192437058, 0.050740813603470006, 0.04841960541220366, 0.11649955467234523, 0.08890632889626286, 0.0987357866941035, 0.060613934218571544, 0.05409608904965684, 0.07025974236377262, 0.06997074602713167, 0.09804423351798566, 0.05291012002782057, 0.07624573092598447, 0.055107666578133374, 0.09711993423334413, 0.06183731136499827, 0.05594863367127215, 0.059693360555212044, 0.0544605238578916, 0.058913708986433416, 0.06365009175231424, 0.10502493542403453, 0.06216618385652122, 0.05468071601885843, 0.07471160116311366, 0.05423525537155438, 0.054065134264308215, 0.13701563562667438, 0.10106657254196999, 0.08365243568794013, 0.05786094694766037, 0.06089176739837651, 0.09640737916853956, 0.04879558028376301, 0.09846189786589218, 0.05905731569288184, 0.07473000047838191, 0.05833307457018083, 0.05168791802294634, 0.166076796408868, 0.113424197910799, 0.051244203423812106, 0.16369670707378428, 0.13035996957819962, 0.15747323984657616, 0.13768591040954734, 0.05488344609608316, 0.07198372552556406], 'lossList': [0.0, -1.1182512772083282, 0.0, 0.29866031667217613, 0.0, 0.0, 0.0], 'rewardMean': 0.7929541269418034, 'totalEpisodes': 249, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.133723411089, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=51.6
