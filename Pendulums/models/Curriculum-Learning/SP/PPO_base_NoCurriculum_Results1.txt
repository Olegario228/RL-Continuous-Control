#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Results', 'verbose': True}
#
'totalSteps': 1280, 'rewardStep': 0.687987641911959, 'errorList': [], 'lossList': [0.0, -1.4191754603385924, 0.0, 16.04847419500351, 0.0, 0.0, 0.0], 'rewardMean': 0.687987641911959, 'totalEpisodes': 99, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.537836170517011
'totalSteps': 2560, 'rewardStep': 0.6251784188944718, 'errorList': [], 'lossList': [0.0, -1.4170118248462678, 0.0, 16.550645933151245, 0.0, 0.0, 0.0], 'rewardMean': 0.6565830304032154, 'totalEpisodes': 173, 'stepsPerEpisode': 37, 'rewardPerEpisode': 22.305172529653603
'totalSteps': 3840, 'rewardStep': 0.4650992857805812, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5608411580918983, 'totalEpisodes': 209, 'stepsPerEpisode': 44, 'rewardPerEpisode': 32.39380666863888
'totalSteps': 5120, 'rewardStep': 0.7584096910282726, 'errorList': [], 'lossList': [0.0, -1.4067720687389373, 0.0, 32.2962686920166, 0.0, 0.0, 0.0], 'rewardMean': 0.6003548646791732, 'totalEpisodes': 251, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.8482104313457204
'totalSteps': 6400, 'rewardStep': 0.9050843450654195, 'errorList': [], 'lossList': [0.0, -1.38900161921978, 0.0, 44.26989214897156, 0.0, 0.0, 0.0], 'rewardMean': 0.6511431114102142, 'totalEpisodes': 275, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9050843450654195
'totalSteps': 7680, 'rewardStep': 0.7436851844925962, 'errorList': [], 'lossList': [0.0, -1.3709790050983428, 0.0, 39.22954465866089, 0.0, 0.0, 0.0], 'rewardMean': 0.6643634075648402, 'totalEpisodes': 282, 'stepsPerEpisode': 20, 'rewardPerEpisode': 16.565200450197388
'totalSteps': 8960, 'rewardStep': 0.6189324461603539, 'errorList': [], 'lossList': [0.0, -1.3557147657871247, 0.0, 42.78244973182678, 0.0, 0.0, 0.0], 'rewardMean': 0.6586845373892793, 'totalEpisodes': 289, 'stepsPerEpisode': 112, 'rewardPerEpisode': 83.71280749349754
'totalSteps': 10240, 'rewardStep': 0.815991801899591, 'errorList': [], 'lossList': [0.0, -1.3450020098686217, 0.0, 31.556744730472566, 0.0, 0.0, 0.0], 'rewardMean': 0.6761631223348695, 'totalEpisodes': 293, 'stepsPerEpisode': 55, 'rewardPerEpisode': 49.53687288669183
'totalSteps': 11520, 'rewardStep': 0.7898398790684501, 'errorList': [], 'lossList': [0.0, -1.3283045834302902, 0.0, 55.17230949878692, 0.0, 0.0, 0.0], 'rewardMean': 0.6875307980082275, 'totalEpisodes': 298, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.847359353387285
'totalSteps': 12800, 'rewardStep': 0.524516626646953, 'errorList': [], 'lossList': [0.0, -1.319054554104805, 0.0, 23.698463526964186, 0.0, 0.0, 0.0], 'rewardMean': 0.671183696481727, 'totalEpisodes': 301, 'stepsPerEpisode': 306, 'rewardPerEpisode': 223.33658474434208
'totalSteps': 14080, 'rewardStep': 0.452211517509811, 'errorList': [], 'lossList': [0.0, -1.3196577429771423, 0.0, 13.222097153663634, 0.0, 0.0, 0.0], 'rewardMean': 0.6538870063432609, 'totalEpisodes': 304, 'stepsPerEpisode': 348, 'rewardPerEpisode': 250.21403113039395
'totalSteps': 15360, 'rewardStep': 0.8335405692090287, 'errorList': [], 'lossList': [0.0, -1.3301449722051621, 0.0, 13.593349862098695, 0.0, 0.0, 0.0], 'rewardMean': 0.6907311346861057, 'totalEpisodes': 309, 'stepsPerEpisode': 26, 'rewardPerEpisode': 20.806287280556965
'totalSteps': 16640, 'rewardStep': 0.8302473800048944, 'errorList': [], 'lossList': [0.0, -1.3159498304128647, 0.0, 18.336135445833207, 0.0, 0.0, 0.0], 'rewardMean': 0.7272459441085369, 'totalEpisodes': 311, 'stepsPerEpisode': 514, 'rewardPerEpisode': 432.9016121335305
'totalSteps': 17920, 'rewardStep': 0.8419336271829889, 'errorList': [], 'lossList': [0.0, -1.3041565769910812, 0.0, 33.62023570537567, 0.0, 0.0, 0.0], 'rewardMean': 0.7355983377240086, 'totalEpisodes': 315, 'stepsPerEpisode': 124, 'rewardPerEpisode': 107.84504240756675
'totalSteps': 19200, 'rewardStep': 0.8743333021857426, 'errorList': [], 'lossList': [0.0, -1.2868296790122986, 0.0, 2.5979101088643075, 0.0, 0.0, 0.0], 'rewardMean': 0.732523233436041, 'totalEpisodes': 317, 'stepsPerEpisode': 535, 'rewardPerEpisode': 444.68890869108856
'totalSteps': 20480, 'rewardStep': 0.5374609610801584, 'errorList': [], 'lossList': [0.0, -1.2456105709075929, 0.0, 3.4944011580944063, 0.0, 0.0, 0.0], 'rewardMean': 0.7119008110947972, 'totalEpisodes': 318, 'stepsPerEpisode': 401, 'rewardPerEpisode': 351.4011853544553
'totalSteps': 21760, 'rewardStep': 0.7346516899937269, 'errorList': [], 'lossList': [0.0, -1.2212027603387832, 0.0, 9.865490239858627, 0.0, 0.0, 0.0], 'rewardMean': 0.7234727354781344, 'totalEpisodes': 322, 'stepsPerEpisode': 158, 'rewardPerEpisode': 131.60353426892576
'totalSteps': 23040, 'rewardStep': 0.7250492712198163, 'errorList': [], 'lossList': [0.0, -1.2196904802322388, 0.0, 3.413088328242302, 0.0, 0.0, 0.0], 'rewardMean': 0.714378482410157, 'totalEpisodes': 322, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 915.2555493496927
'totalSteps': 24320, 'rewardStep': 0.9542680363015934, 'errorList': [0.4292876175745011, 0.36785609780040673, 0.3668402236042005, 0.4073053665843532, 0.36813837268561883, 0.3784454156226662, 0.37913342884108736, 0.374877584639999, 0.36748937342255994, 0.3727141721330859, 0.3683213542228684, 0.3654942904669853, 0.37781176511175024, 0.3732833228008876, 0.3781837034772418, 0.36999827904324595, 0.375999482734399, 0.372678552464697, 0.3771085913575307, 0.37393701601060586, 0.37536795060957634, 0.3697289533492459, 0.3771497559290112, 0.4045452048862611, 0.37177129203501463, 0.3733947619639542, 0.37247711752726276, 0.37522117102035163, 0.38507174883314765, 0.3792956567374491, 0.3720968669259749, 0.3624451793630359, 0.37838307182755176, 0.37709595074850766, 0.3755793675257284, 0.3772097517902532, 0.38702964762641395, 0.36963290628244044, 0.37499755609956503, 0.3713052825500737, 0.369079459680133, 0.5126929407048562, 0.37390500915749814, 0.3770672407191228, 0.37656982597446537, 0.41016157539843495, 0.37311387155010656, 0.37299911392964413, 0.36905159135687055, 0.37331203984184874], 'lossList': [0.0, -1.214172922372818, 0.0, 7.242362591028214, 0.0, 0.0, 0.0], 'rewardMean': 0.7308212981334714, 'totalEpisodes': 323, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.139571696993249, 'successfulTests': 0
'totalSteps': 25600, 'rewardStep': 0.791206173925241, 'errorList': [], 'lossList': [0.0, -1.1812240147590638, 0.0, 1.1860398936271668, 0.0, 0.0, 0.0], 'rewardMean': 0.7574902528613002, 'totalEpisodes': 323, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 965.6547566031877
'totalSteps': 26880, 'rewardStep': 0.9768358065187047, 'errorList': [0.028941885404760264, 0.03554504536269986, 0.06027083205702822, 0.07655347254847535, 0.03454094415078123, 0.06759482820613383, 0.03479291883518325, 0.0473810351885529, 0.04108530059895664, 0.05525240567867924, 0.035263323988965915, 0.028966824815324723, 0.03550375946267289, 0.07625594447022442, 0.05055737573003847, 0.0708422150605774, 0.03543364680454607, 0.02855735843546233, 0.12684026526754305, 0.03865174843658334, 0.03849697453208986, 0.0628319028036781, 0.032363495894525564, 0.04369051657827845, 0.10005972956399092, 0.06793742521816813, 0.051491533300388456, 0.07456829655571234, 0.057301978089383476, 0.04348422348834328, 0.032804658625454675, 0.03697116767082171, 0.06491628742899605, 0.02946765690443992, 0.03639576161757614, 0.023466952442376024, 0.06082958475624135, 0.06186856923871828, 0.0314808341200854, 0.09668841335632465, 0.032213371537422576, 0.07650817431099355, 0.07331450443984162, 0.04765553505980548, 0.04600136937097018, 0.05766597610357464, 0.0359939392087547, 0.05561868575683996, 0.08300814892382351, 0.05900514273158202], 'lossList': [0.0, -1.1195111364126205, 0.0, 0.5628760232776403, 0.0, 0.0, 0.0], 'rewardMean': 0.8099526817621896, 'totalEpisodes': 323, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1105.0646365627988, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=90.81
