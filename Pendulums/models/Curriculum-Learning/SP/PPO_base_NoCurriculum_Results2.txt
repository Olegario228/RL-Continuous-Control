#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Results', 'verbose': True}
#
'totalSteps': 1280, 'rewardStep': 0.7600691109095269, 'errorList': [], 'lossList': [0.0, -1.4245474088191985, 0.0, 22.688835773468018, 0.0, 0.0, 0.0], 'rewardMean': 0.7600691109095269, 'totalEpisodes': 79, 'stepsPerEpisode': 48, 'rewardPerEpisode': 34.99868657516198
'totalSteps': 2560, 'rewardStep': 0.42807672861053825, 'errorList': [], 'lossList': [0.0, -1.4324368864297867, 0.0, 17.971188950538636, 0.0, 0.0, 0.0], 'rewardMean': 0.5940729197600325, 'totalEpisodes': 154, 'stepsPerEpisode': 55, 'rewardPerEpisode': 36.51062570637793
'totalSteps': 3840, 'rewardStep': 0.8798416570609477, 'errorList': [], 'lossList': [0.0, -1.417926596403122, 0.0, 28.178714780807496, 0.0, 0.0, 0.0], 'rewardMean': 0.6893291655270043, 'totalEpisodes': 211, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.12421415699426
'totalSteps': 5120, 'rewardStep': 0.699533399391885, 'errorList': [], 'lossList': [0.0, -1.3968521791696549, 0.0, 34.841800146102905, 0.0, 0.0, 0.0], 'rewardMean': 0.6918802239932245, 'totalEpisodes': 248, 'stepsPerEpisode': 29, 'rewardPerEpisode': 20.549161818414788
'totalSteps': 6400, 'rewardStep': 0.8131077132660316, 'errorList': [], 'lossList': [0.0, -1.3820616149902343, 0.0, 43.864781055450436, 0.0, 0.0, 0.0], 'rewardMean': 0.7161257218477859, 'totalEpisodes': 266, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8131077132660316
'totalSteps': 7680, 'rewardStep': 0.858255428906598, 'errorList': [], 'lossList': [0.0, -1.3652518278360366, 0.0, 40.9276761007309, 0.0, 0.0, 0.0], 'rewardMean': 0.7398140063575879, 'totalEpisodes': 278, 'stepsPerEpisode': 58, 'rewardPerEpisode': 41.988740953940216
'totalSteps': 8960, 'rewardStep': 0.4497024567544197, 'errorList': [], 'lossList': [0.0, -1.3545305353403092, 0.0, 32.81784322023392, 0.0, 0.0, 0.0], 'rewardMean': 0.698369499271421, 'totalEpisodes': 286, 'stepsPerEpisode': 135, 'rewardPerEpisode': 101.64572647795681
'totalSteps': 10240, 'rewardStep': 0.4984059812638541, 'errorList': [], 'lossList': [0.0, -1.3406229156255722, 0.0, 19.04173915863037, 0.0, 0.0, 0.0], 'rewardMean': 0.6733740595204751, 'totalEpisodes': 290, 'stepsPerEpisode': 345, 'rewardPerEpisode': 238.17625160325156
'totalSteps': 11520, 'rewardStep': 0.8507220361018999, 'errorList': [], 'lossList': [0.0, -1.3318915605545043, 0.0, 37.12999329805374, 0.0, 0.0, 0.0], 'rewardMean': 0.6930793902517445, 'totalEpisodes': 294, 'stepsPerEpisode': 176, 'rewardPerEpisode': 146.2695088484578
'totalSteps': 12800, 'rewardStep': 0.6754399771611155, 'errorList': [], 'lossList': [0.0, -1.3302673703432084, 0.0, 37.826105728149415, 0.0, 0.0, 0.0], 'rewardMean': 0.6913154489426816, 'totalEpisodes': 296, 'stepsPerEpisode': 563, 'rewardPerEpisode': 363.4637222382635
'totalSteps': 14080, 'rewardStep': 0.5863853099434345, 'errorList': [], 'lossList': [0.0, -1.3099454158544541, 0.0, 68.77073705673217, 0.0, 0.0, 0.0], 'rewardMean': 0.6739470688460725, 'totalEpisodes': 300, 'stepsPerEpisode': 320, 'rewardPerEpisode': 223.6501972813895
'totalSteps': 15360, 'rewardStep': 0.8824022559918115, 'errorList': [], 'lossList': [0.0, -1.3201241904497147, 0.0, 53.2310147190094, 0.0, 0.0, 0.0], 'rewardMean': 0.7193796215841998, 'totalEpisodes': 304, 'stepsPerEpisode': 85, 'rewardPerEpisode': 75.2846399394907
'totalSteps': 16640, 'rewardStep': 0.5929159498405118, 'errorList': [], 'lossList': [0.0, -1.329077341556549, 0.0, 51.23959574699402, 0.0, 0.0, 0.0], 'rewardMean': 0.6906870508621562, 'totalEpisodes': 308, 'stepsPerEpisode': 132, 'rewardPerEpisode': 90.77829787761976
'totalSteps': 17920, 'rewardStep': 0.676388907920803, 'errorList': [], 'lossList': [0.0, -1.334887807369232, 0.0, 51.30299016714096, 0.0, 0.0, 0.0], 'rewardMean': 0.688372601715048, 'totalEpisodes': 312, 'stepsPerEpisode': 211, 'rewardPerEpisode': 178.6689842615878
'totalSteps': 19200, 'rewardStep': 0.6144526620367751, 'errorList': [], 'lossList': [0.0, -1.3461808341741561, 0.0, 21.29637488245964, 0.0, 0.0, 0.0], 'rewardMean': 0.6685070965921223, 'totalEpisodes': 314, 'stepsPerEpisode': 299, 'rewardPerEpisode': 227.28823256209498
'totalSteps': 20480, 'rewardStep': 0.4293671809955722, 'errorList': [], 'lossList': [0.0, -1.3426483100652695, 0.0, 11.584338863492013, 0.0, 0.0, 0.0], 'rewardMean': 0.6256182718010198, 'totalEpisodes': 315, 'stepsPerEpisode': 1035, 'rewardPerEpisode': 784.8003153605949
'totalSteps': 21760, 'rewardStep': 0.7831356422822092, 'errorList': [], 'lossList': [0.0, -1.3305916225910186, 0.0, 4.963639027848839, 0.0, 0.0, 0.0], 'rewardMean': 0.6589615903537986, 'totalEpisodes': 316, 'stepsPerEpisode': 1276, 'rewardPerEpisode': 1016.1510727068214
'totalSteps': 23040, 'rewardStep': 0.49929373724973986, 'errorList': [], 'lossList': [0.0, -1.3163611888885498, 0.0, 15.498649349212647, 0.0, 0.0, 0.0], 'rewardMean': 0.6590503659523874, 'totalEpisodes': 318, 'stepsPerEpisode': 595, 'rewardPerEpisode': 474.4894247392944
'totalSteps': 24320, 'rewardStep': 0.8410599377352813, 'errorList': [], 'lossList': [0.0, -1.3252535009384154, 0.0, 18.645830006599425, 0.0, 0.0, 0.0], 'rewardMean': 0.6580841561157253, 'totalEpisodes': 322, 'stepsPerEpisode': 24, 'rewardPerEpisode': 19.306222636968034
'totalSteps': 25600, 'rewardStep': 0.7395962917695944, 'errorList': [], 'lossList': [0.0, -1.3283266013860702, 0.0, 10.962662476301194, 0.0, 0.0, 0.0], 'rewardMean': 0.6644997875765732, 'totalEpisodes': 324, 'stepsPerEpisode': 344, 'rewardPerEpisode': 287.2829765497796
'totalSteps': 26880, 'rewardStep': 0.8743570590558641, 'errorList': [], 'lossList': [0.0, -1.2844206660985946, 0.0, 1.1922662442177534, 0.0, 0.0, 0.0], 'rewardMean': 0.6932969624878161, 'totalEpisodes': 324, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.4487720442826
'totalSteps': 28160, 'rewardStep': 0.7434783720294715, 'errorList': [], 'lossList': [0.0, -1.2457830339670182, 0.0, 5.695132570415735, 0.0, 0.0, 0.0], 'rewardMean': 0.6794045740915824, 'totalEpisodes': 327, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.7162496248695525
'totalSteps': 29440, 'rewardStep': 0.6191782273604016, 'errorList': [], 'lossList': [0.0, -1.249212824702263, 0.0, 3.231938747316599, 0.0, 0.0, 0.0], 'rewardMean': 0.6820308018435711, 'totalEpisodes': 328, 'stepsPerEpisode': 471, 'rewardPerEpisode': 384.6397743949837
'totalSteps': 30720, 'rewardStep': 0.9334869692849475, 'errorList': [0.6742915857464478, 0.9876885422615242, 0.5192869065925635, 0.5503088572103918, 0.7169832558336143, 0.9395608630582926, 0.9624303342908131, 0.7373944458185558, 0.6506331721109154, 0.8523180793379639, 0.7674916735228882, 0.9238975609193703, 0.9904564698361171, 0.5600821216019949, 0.917800736533529, 1.0631600666455339, 0.7956052715808453, 0.6381496680780211, 0.6069378442196417, 0.7559175605859141, 0.7027871027728775, 0.7525623150278397, 0.7021579121197845, 1.1590042347302962, 0.862780973789071, 0.6738179316011015, 0.7320081472777029, 0.6263306287307121, 0.7921387430646175, 0.5457721182665297, 0.7572970151749685, 0.7894166373657259, 0.8003000523518119, 0.7879283310452087, 0.7695903440839081, 0.520115342004834, 0.6864044050913233, 0.7834721560340436, 0.8529607141103879, 0.6161555426537026, 0.827431418641324, 0.9842046536891983, 0.743522588608519, 0.6986547469456308, 0.8003816478898143, 0.5216693707408294, 0.9187051272985245, 0.8432662816430163, 0.6219612836514178, 0.6863966811825687], 'lossList': [0.0, -1.249767758846283, 0.0, 4.189976296126843, 0.0, 0.0, 0.0], 'rewardMean': 0.7077406079799857, 'totalEpisodes': 332, 'stepsPerEpisode': 27, 'rewardPerEpisode': 22.20055198642783, 'successfulTests': 0
'totalSteps': 32000, 'rewardStep': 0.5517067516880518, 'errorList': [], 'lossList': [0.0, -1.2478694665431975, 0.0, 1.8483306494355203, 0.0, 0.0, 0.0], 'rewardMean': 0.7014660169451133, 'totalEpisodes': 333, 'stepsPerEpisode': 637, 'rewardPerEpisode': 496.28471566957944
'totalSteps': 33280, 'rewardStep': 0.6998518882920293, 'errorList': [], 'lossList': [0.0, -1.250872865319252, 0.0, 1.9358262513577937, 0.0, 0.0, 0.0], 'rewardMean': 0.7285144876747591, 'totalEpisodes': 333, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.0382829689534
'totalSteps': 34560, 'rewardStep': 0.5966981438146097, 'errorList': [], 'lossList': [0.0, -1.2612870699167251, 0.0, 0.5476987512409687, 0.0, 0.0, 0.0], 'rewardMean': 0.7098707378279991, 'totalEpisodes': 333, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1037.2814240450193
'totalSteps': 35840, 'rewardStep': 0.8282101207165747, 'errorList': [], 'lossList': [0.0, -1.2677532827854157, 0.0, 1.2675967009365559, 0.0, 0.0, 0.0], 'rewardMean': 0.7427623761746827, 'totalEpisodes': 333, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 998.116583470948
#less than 4 successful tests, storing failed model
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=86.57
