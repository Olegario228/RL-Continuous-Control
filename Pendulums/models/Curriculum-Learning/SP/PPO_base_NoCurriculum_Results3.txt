#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Results', 'verbose': True}
#
'totalSteps': 1280, 'rewardStep': 0.7426365845702666, 'errorList': [], 'lossList': [0.0, -1.4033270698785782, 0.0, 21.789175868034363, 0.0, 0.0, 0.0], 'rewardMean': 0.7426365845702666, 'totalEpisodes': 81, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.047358348693027
'totalSteps': 2560, 'rewardStep': 0.833579965197561, 'errorList': [], 'lossList': [0.0, -1.385125496983528, 0.0, 19.134704184532165, 0.0, 0.0, 0.0], 'rewardMean': 0.7881082748839138, 'totalEpisodes': 158, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.7029122895120103
'totalSteps': 3840, 'rewardStep': 0.8631808195989996, 'errorList': [], 'lossList': [0.0, -1.3804205501079558, 0.0, 28.604172945022583, 0.0, 0.0, 0.0], 'rewardMean': 0.8131324564556092, 'totalEpisodes': 210, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.136454288725897
'totalSteps': 5120, 'rewardStep': 0.9694981971444341, 'errorList': [10.145962475965609, 33.04394303502961, 14.706487998031095, 9.016437192162028, 3.188097308146381, 4.3373947602332965, 0.852543947682299, 31.255874558300206, 2.1233550652190307, 0.08614710215931969, 21.435390008579436, 14.816851332471417, 10.97577912114182, 5.374378091437637, 1.1738764335593117, 10.897551448838744, 15.54027077013642, 1.1965563855023251, 31.5211705452796, 1.6589169943185404, 7.138734254643341, 9.320572551558147, 3.2795191981828746, 8.981502977556492, 9.259179180412348, 10.891593405813257, 6.6858942838698106, 14.239861499945082, 15.872829102852236, 1.5897304889131776, 1.7883681889328888, 1.3729915088820428, 6.4674145055563494, 7.41828966237646, 2.271973513513643, 1.1297091990982489, 0.9334330395533398, 6.236696249662354, 17.93946809745977, 5.208419046462342, 0.8567984440715265, 9.673344488708755, 3.9665052879865024, 5.172506906706083, 5.214845740711004, 0.345017183962772, 4.607674617652447, 14.71788745531269, 3.7687301315466444, 15.844780386402952], 'lossList': [0.0, -1.369448009133339, 0.0, 34.84324004173279, 0.0, 0.0, 0.0], 'rewardMean': 0.8522238916278153, 'totalEpisodes': 227, 'stepsPerEpisode': 71, 'rewardPerEpisode': 57.524735368245274, 'successfulTests': 1
'totalSteps': 6400, 'rewardStep': 0.7948173369258612, 'errorList': [], 'lossList': [0.0, -1.352264745235443, 0.0, 41.58145796298981, 0.0, 0.0, 0.0], 'rewardMean': 0.8407425806874246, 'totalEpisodes': 236, 'stepsPerEpisode': 13, 'rewardPerEpisode': 12.127768929700457
'totalSteps': 7680, 'rewardStep': 0.9288941760671869, 'errorList': [], 'lossList': [0.0, -1.3451095271110534, 0.0, 43.820604567527774, 0.0, 0.0, 0.0], 'rewardMean': 0.8554345132507183, 'totalEpisodes': 243, 'stepsPerEpisode': 54, 'rewardPerEpisode': 46.50376955081249
'totalSteps': 8960, 'rewardStep': 0.561329649593494, 'errorList': [], 'lossList': [0.0, -1.3436619466543198, 0.0, 24.28282740831375, 0.0, 0.0, 0.0], 'rewardMean': 0.8134195327282577, 'totalEpisodes': 250, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.7556734496524928
'totalSteps': 10240, 'rewardStep': 0.5554141847781798, 'errorList': [], 'lossList': [0.0, -1.3349982368946076, 0.0, 16.851167056560516, 0.0, 0.0, 0.0], 'rewardMean': 0.7811688642344979, 'totalEpisodes': 255, 'stepsPerEpisode': 282, 'rewardPerEpisode': 204.46297029191922
'totalSteps': 11520, 'rewardStep': 0.818980865660073, 'errorList': [], 'lossList': [0.0, -1.323162672519684, 0.0, 28.482844412326813, 0.0, 0.0, 0.0], 'rewardMean': 0.7853701977262285, 'totalEpisodes': 260, 'stepsPerEpisode': 91, 'rewardPerEpisode': 80.45824880223115
'totalSteps': 12800, 'rewardStep': 0.7990038558996942, 'errorList': [], 'lossList': [0.0, -1.310743522644043, 0.0, 15.44724203824997, 0.0, 0.0, 0.0], 'rewardMean': 0.786733563543575, 'totalEpisodes': 265, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.8392723049091
'totalSteps': 14080, 'rewardStep': 0.8711162942663218, 'errorList': [], 'lossList': [0.0, -1.3082188683748246, 0.0, 9.125271980762482, 0.0, 0.0, 0.0], 'rewardMean': 0.7995815345131806, 'totalEpisodes': 267, 'stepsPerEpisode': 328, 'rewardPerEpisode': 272.6482705247349
'totalSteps': 15360, 'rewardStep': 0.7566567958008885, 'errorList': [], 'lossList': [0.0, -1.3272101205587388, 0.0, 11.105531919002534, 0.0, 0.0, 0.0], 'rewardMean': 0.7918892175735134, 'totalEpisodes': 270, 'stepsPerEpisode': 203, 'rewardPerEpisode': 179.221733332763
'totalSteps': 16640, 'rewardStep': 0.9040365598327911, 'errorList': [], 'lossList': [0.0, -1.3278741884231566, 0.0, 5.811293854564428, 0.0, 0.0, 0.0], 'rewardMean': 0.7959747915968924, 'totalEpisodes': 271, 'stepsPerEpisode': 564, 'rewardPerEpisode': 498.3778053508293
'totalSteps': 17920, 'rewardStep': 0.4525364400990469, 'errorList': [], 'lossList': [0.0, -1.3320148772001266, 0.0, 2.5120745746791364, 0.0, 0.0, 0.0], 'rewardMean': 0.7442786158923538, 'totalEpisodes': 271, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 999.6554963921526
'totalSteps': 19200, 'rewardStep': 0.9067688098862446, 'errorList': [], 'lossList': [0.0, -1.3429402959346772, 0.0, 2.9364172266423703, 0.0, 0.0, 0.0], 'rewardMean': 0.7554737631883921, 'totalEpisodes': 272, 'stepsPerEpisode': 1273, 'rewardPerEpisode': 941.2666357717096
'totalSteps': 20480, 'rewardStep': 0.9641543498613797, 'errorList': [0.370135263951216, 0.39607253535627956, 0.39838452724903, 0.38255537212823515, 0.3814069256623314, 0.40543539891844144, 0.35500563356885034, 0.4046904015779357, 0.4015883541035619, 0.39016059966240985, 0.3872222927811188, 0.39291234505143385, 0.40191223572623497, 0.4001287096745139, 0.4034511755416665, 0.40975717326517447, 0.4345365826757561, 0.3944365457904209, 0.37228790416750956, 0.39982077537327027, 0.4023378492622153, 0.40808211303655745, 0.3617364049590304, 0.4120870166527045, 0.42193847021345854, 0.36851225201462934, 0.39837095474239775, 0.38006812326760153, 0.39327229883092396, 0.39401751630141923, 0.4340017608354875, 0.3894355005693151, 0.39100872426050104, 0.379477794505466, 0.3992409693498548, 0.40464254741215905, 0.42209492360116724, 0.43170604942813545, 0.3983078887467243, 0.40562466165034194, 0.3812766304406134, 0.39230270297269054, 0.39206372025219366, 0.36796353177226654, 0.4070249477106178, 0.41899318922167994, 0.35900335759358915, 0.40212379219874594, 0.4423069327341919, 0.40944252806954945], 'lossList': [0.0, -1.304480065703392, 0.0, 1.387291115000844, 0.0, 0.0, 0.0], 'rewardMean': 0.7589997805678114, 'totalEpisodes': 272, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.7306413126053, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.8206156738506849, 'errorList': [], 'lossList': [0.0, -1.2650735741853714, 0.0, 0.8530661733448506, 0.0, 0.0, 0.0], 'rewardMean': 0.7849283829935304, 'totalEpisodes': 272, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 954.8196702941397
'totalSteps': 23040, 'rewardStep': 0.783888907511286, 'errorList': [], 'lossList': [0.0, -1.2445393389463424, 0.0, 0.45522614315152166, 0.0, 0.0, 0.0], 'rewardMean': 0.807775855266841, 'totalEpisodes': 272, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1047.895710565193
'totalSteps': 24320, 'rewardStep': 0.8037910382419845, 'errorList': [], 'lossList': [0.0, -1.2190132713317872, 0.0, 0.49968700524419546, 0.0, 0.0, 0.0], 'rewardMean': 0.8062568725250323, 'totalEpisodes': 272, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.1447465243116
'totalSteps': 25600, 'rewardStep': 0.9453298369798304, 'errorList': [0.02102780326393829, 0.04318014794154044, 0.0335442061805187, 0.04344036844769506, 0.027754586913371272, 0.058356764401854805, 0.01325310670746915, 0.05456490106924302, 0.009609416881393364, 0.023672258119513664, 0.05193528663853457, 0.018175016001426552, 0.10762578518424623, 0.04081300251058139, 0.037816667110261784, 0.054901001850250514, 0.03423431758398291, 0.06312668092186398, 0.016076664722798024, 0.07380731308085421, 0.026913225834761485, 0.014690203674837086, 0.06789927169444317, 0.04285790181956825, 0.020438238933866235, 0.02616269563927016, 0.02078763427731296, 0.04009852712434092, 0.08311527945991905, 0.015508808356721446, 0.018575376675999046, 0.06361175078560684, 0.04424419375806187, 0.029492934366743186, 0.03881045857994154, 0.027815866900291225, 0.028186395196252233, 0.035565088165617036, 0.03716462438506493, 0.03466433516400335, 0.03793650247914306, 0.0340892113304619, 0.018059486262055426, 0.06398111432955424, 0.034952779530484236, 0.011020668361544637, 0.057240607178639734, 0.05720934432136432, 0.06884252454634092, 0.03959918794554901], 'lossList': [0.0, -1.200858432650566, 0.0, 0.3843940671160817, 0.0, 0.0, 0.0], 'rewardMean': 0.8208894706330458, 'totalEpisodes': 272, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.2021879246117, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=106.46
