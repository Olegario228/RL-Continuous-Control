#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': (0, 5000, 8000), 'controlValues': [[2, 4], [0, 0], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5809953713846492, 'errorList': [], 'lossList': [0.0, -1.4171791988611222, 0.0, 34.73336408615112, 0.0, 0.0, 0.0], 'rewardMean': 0.5809953713846492, 'totalEpisodes': 44, 'stepsPerEpisode': 71, 'rewardPerEpisode': 52.210451438359044
'totalSteps': 2560, 'rewardStep': 0.3960989288286612, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.45773107634732385, 'totalEpisodes': 121, 'stepsPerEpisode': 25, 'rewardPerEpisode': 16.416795648338265
'totalSteps': 3840, 'rewardStep': 0.9701321611193551, 'errorList': [], 'lossList': [0.0, -1.4181232899427414, 0.0, 22.27298406600952, 0.0, 0.0, 0.0], 'rewardMean': 0.5858313475403316, 'totalEpisodes': 208, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.808664604628984
'totalSteps': 5120, 'rewardStep': 0.7375084121669864, 'errorList': [], 'lossList': [0.0, -1.4123434430360795, 0.0, 33.125492906570436, 0.0, 0.0, 0.0], 'rewardMean': 0.6161667604656625, 'totalEpisodes': 252, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.126794945130566
'totalSteps': 6400, 'rewardStep': 0.6782463666570824, 'errorList': [], 'lossList': [0.0, -1.394448310136795, 0.0, 39.6250985622406, 0.0, 0.0, 0.0], 'rewardMean': 0.6265133614975659, 'totalEpisodes': 291, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.50318755100374
'totalSteps': 7680, 'rewardStep': 0.8720670798428891, 'errorList': [], 'lossList': [0.0, -1.383730829358101, 0.0, 43.55489761352539, 0.0, 0.0, 0.0], 'rewardMean': 0.6615924641183264, 'totalEpisodes': 314, 'stepsPerEpisode': 53, 'rewardPerEpisode': 42.76821547723519
'totalSteps': 8960, 'rewardStep': 0.8914584646716237, 'errorList': [], 'lossList': [0.0, -1.3660733890533447, 0.0, 49.79567212104797, 0.0, 0.0, 0.0], 'rewardMean': 0.6903257141874886, 'totalEpisodes': 328, 'stepsPerEpisode': 61, 'rewardPerEpisode': 47.993189840242444
'totalSteps': 10240, 'rewardStep': 0.2938790068808103, 'errorList': [], 'lossList': [0.0, -1.3575741797685623, 0.0, 27.43122216463089, 0.0, 0.0, 0.0], 'rewardMean': 0.6462760800423021, 'totalEpisodes': 333, 'stepsPerEpisode': 162, 'rewardPerEpisode': 119.22992438594021
'totalSteps': 11520, 'rewardStep': 0.6442358378524446, 'errorList': [], 'lossList': [0.0, -1.363227071762085, 0.0, 23.082563433647156, 0.0, 0.0, 0.0], 'rewardMean': 0.6460720558233163, 'totalEpisodes': 341, 'stepsPerEpisode': 101, 'rewardPerEpisode': 73.34677124982835
'totalSteps': 12800, 'rewardStep': 0.23229395170275519, 'errorList': [], 'lossList': [0.0, -1.3708248388767243, 0.0, 34.07436210155487, 0.0, 0.0, 0.0], 'rewardMean': 0.6112019138551268, 'totalEpisodes': 349, 'stepsPerEpisode': 359, 'rewardPerEpisode': 272.22840368210217
'totalSteps': 14080, 'rewardStep': 0.6994068704679477, 'errorList': [], 'lossList': [0.0, -1.3726037281751633, 0.0, 26.37909597635269, 0.0, 0.0, 0.0], 'rewardMean': 0.6415327080190556, 'totalEpisodes': 357, 'stepsPerEpisode': 227, 'rewardPerEpisode': 194.6490521212701
'totalSteps': 15360, 'rewardStep': 0.8706379934645458, 'errorList': [], 'lossList': [0.0, -1.3785472667217256, 0.0, 8.005643677711486, 0.0, 0.0, 0.0], 'rewardMean': 0.688986614482644, 'totalEpisodes': 363, 'stepsPerEpisode': 65, 'rewardPerEpisode': 56.14699143973033
'totalSteps': 16640, 'rewardStep': 0.6455016494205832, 'errorList': [], 'lossList': [0.0, -1.393067052960396, 0.0, 6.180614956021309, 0.0, 0.0, 0.0], 'rewardMean': 0.6565235633127668, 'totalEpisodes': 367, 'stepsPerEpisode': 225, 'rewardPerEpisode': 183.0856911605345
'totalSteps': 17920, 'rewardStep': 0.845447833419624, 'errorList': [], 'lossList': [0.0, -1.3934728771448135, 0.0, 19.290404970645906, 0.0, 0.0, 0.0], 'rewardMean': 0.6673175054380306, 'totalEpisodes': 374, 'stepsPerEpisode': 159, 'rewardPerEpisode': 138.39037478988757
'totalSteps': 19200, 'rewardStep': 0.630284116595402, 'errorList': [], 'lossList': [0.0, -1.398692547082901, 0.0, 4.6991288483142855, 0.0, 0.0, 0.0], 'rewardMean': 0.6625212804318625, 'totalEpisodes': 378, 'stepsPerEpisode': 102, 'rewardPerEpisode': 81.42317092351624
'totalSteps': 20480, 'rewardStep': 0.9058082381106717, 'errorList': [], 'lossList': [0.0, -1.4017216008901596, 0.0, 2.8839447432756424, 0.0, 0.0, 0.0], 'rewardMean': 0.6658953962586409, 'totalEpisodes': 382, 'stepsPerEpisode': 151, 'rewardPerEpisode': 133.84698631630863
'totalSteps': 21760, 'rewardStep': 0.6463523660396305, 'errorList': [], 'lossList': [0.0, -1.3973887360095978, 0.0, 2.847335078716278, 0.0, 0.0, 0.0], 'rewardMean': 0.6413847863954415, 'totalEpisodes': 384, 'stepsPerEpisode': 220, 'rewardPerEpisode': 172.1942862483928
'totalSteps': 23040, 'rewardStep': 0.8348549295000345, 'errorList': [], 'lossList': [0.0, -1.405316748023033, 0.0, 2.648209349513054, 0.0, 0.0, 0.0], 'rewardMean': 0.695482378657364, 'totalEpisodes': 387, 'stepsPerEpisode': 380, 'rewardPerEpisode': 330.8333497340182
'totalSteps': 24320, 'rewardStep': 0.7212490267517173, 'errorList': [], 'lossList': [0.0, -1.3930506664514541, 0.0, 2.8791621351242065, 0.0, 0.0, 0.0], 'rewardMean': 0.7031836975472912, 'totalEpisodes': 389, 'stepsPerEpisode': 299, 'rewardPerEpisode': 254.2046585039292
'totalSteps': 25600, 'rewardStep': 0.8680900963876983, 'errorList': [], 'lossList': [0.0, -1.3594108945131302, 0.0, 1.9658211445808411, 0.0, 0.0, 0.0], 'rewardMean': 0.7667633120157855, 'totalEpisodes': 389, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.4661357740526
'totalSteps': 26880, 'rewardStep': 0.9935021672918155, 'errorList': [0.132186331511533, 0.1293707241757847, 0.12941712998832594, 0.13190715424632188, 0.13374490764543173, 0.1429531418853641, 0.12834985700174426, 0.13246511883145834, 0.130340366996522, 0.15378141332215556, 0.1331696036060901, 0.12435552539047429, 0.13105240179029143, 0.12737078054467027, 0.1325579785597845, 0.1299623462877149, 0.13274673844932572, 0.12787421741641825, 0.12732632080041836, 0.12570548128853617, 0.12910122731150261, 0.1288253177460588, 0.13171047299602717, 0.17528042282459966, 0.13024156985264068, 0.13317830204736308, 0.13314484160143372, 0.12790019356848534, 0.12777151461356298, 0.130137221210698, 0.1319083032652911, 0.12868135164661187, 0.12983623902582986, 0.13312861268990314, 0.12758232904916014, 0.16319512316955254, 0.1302827240915184, 0.15010092988490528, 0.13015170294759867, 0.12742696107377813, 0.14214071804510708, 0.13138796263946975, 0.13492741349802997, 0.17013600758215386, 0.21031245873801752, 0.16189692877170436, 0.13180792112738252, 0.1327231146309844, 0.12730277744934632, 0.1305755596645391], 'lossList': [0.0, -1.3342298555374146, 0.0, 1.9440387788414955, 0.0, 0.0, 0.0], 'rewardMean': 0.7961728416981723, 'totalEpisodes': 391, 'stepsPerEpisode': 89, 'rewardPerEpisode': 80.45359015716102, 'successfulTests': 49
'totalSteps': 28160, 'rewardStep': 0.7463066451360931, 'errorList': [], 'lossList': [0.0, -1.3217193573713302, 0.0, 1.0755297195911409, 0.0, 0.0, 0.0], 'rewardMean': 0.7837397068653271, 'totalEpisodes': 391, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.8562831033787
'totalSteps': 29440, 'rewardStep': 0.8861072712780047, 'errorList': [], 'lossList': [0.0, -1.2915037697553635, 0.0, 0.5326878539845348, 0.0, 0.0, 0.0], 'rewardMean': 0.8078002690510692, 'totalEpisodes': 391, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.4574108306972
'totalSteps': 30720, 'rewardStep': 0.8785906668422954, 'errorList': [], 'lossList': [0.0, -1.2671493077278138, 0.0, 0.28364441704005006, 0.0, 0.0, 0.0], 'rewardMean': 0.8111145523933363, 'totalEpisodes': 391, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1179.109194744152
'totalSteps': 32000, 'rewardStep': 0.9354453483667163, 'errorList': [0.025505033657239128, 0.04284538612576318, 0.04650841851260112, 0.025068359541087413, 0.0318344111527145, 0.03912473098468088, 0.02635056078376193, 0.025981530357011083, 0.03294440225472441, 0.032370560653569004, 0.03250715830080625, 0.04158871089540401, 0.0267538737848102, 0.02652671795729738, 0.03603953047643587, 0.03515667746803595, 0.035567943837149084, 0.03627316383006073, 0.025799185810443496, 0.03364589511973861, 0.025521511738342495, 0.04294429574149858, 0.025348204140043937, 0.03409695971021291, 0.03540059632619069, 0.03052663199582063, 0.026416757106376254, 0.03834522027839142, 0.025410724674707827, 0.03874204434083927, 0.032819217464674755, 0.028709753832528676, 0.04246253441466375, 0.032784493719254064, 0.043360711374027965, 0.0259050675713699, 0.04003411128817343, 0.025269729640909074, 0.028570366382315363, 0.025294106425139424, 0.03230495263354563, 0.025947439322363347, 0.03852119385270223, 0.03630611824493691, 0.0398663057959749, 0.03280041135438407, 0.03404545498405317, 0.029839373064264915, 0.025268998324861, 0.03763729596475197], 'lossList': [0.0, -1.2427946734428406, 0.0, 0.28167487248778345, 0.0, 0.0, 0.0], 'rewardMean': 0.8416306755704677, 'totalEpisodes': 391, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1209.1600429932116, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=32000, timeSpent=55.81
