#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_base_NoCurriculum_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': (0, 5000, 8000), 'controlValues': [[2, 4], [0, 0], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.958751204070418, 'errorList': [], 'lossList': [0.0, -1.4130720180273055, 0.0, 33.84995708942413, 0.0, 0.0, 0.0], 'rewardMean': 0.958751204070418, 'totalEpisodes': 43, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.7196585142108782
'totalSteps': 2560, 'rewardStep': 0.7692715426112485, 'errorList': [], 'lossList': [0.0, -1.4044731611013412, 0.0, 25.884782762527465, 0.0, 0.0, 0.0], 'rewardMean': 0.8640113733408332, 'totalEpisodes': 107, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.648975452797156
'totalSteps': 3840, 'rewardStep': 0.7913399850448508, 'errorList': [], 'lossList': [0.0, -1.400973949432373, 0.0, 35.71725554466248, 0.0, 0.0, 0.0], 'rewardMean': 0.8397875772421725, 'totalEpisodes': 161, 'stepsPerEpisode': 58, 'rewardPerEpisode': 45.137267331829754
'totalSteps': 5120, 'rewardStep': 0.8493188893852514, 'errorList': [], 'lossList': [0.0, -1.385332209467888, 0.0, 42.81628315925598, 0.0, 0.0, 0.0], 'rewardMean': 0.8421704052779422, 'totalEpisodes': 197, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.23790969324272
'totalSteps': 6400, 'rewardStep': 0.8447257795459774, 'errorList': [], 'lossList': [0.0, -1.373136692047119, 0.0, 37.72304794788361, 0.0, 0.0, 0.0], 'rewardMean': 0.8426814801315492, 'totalEpisodes': 210, 'stepsPerEpisode': 153, 'rewardPerEpisode': 116.10122333301912
'totalSteps': 7680, 'rewardStep': 0.7004332749490558, 'errorList': [], 'lossList': [0.0, -1.3612823832035064, 0.0, 32.34270566463471, 0.0, 0.0, 0.0], 'rewardMean': 0.818973445934467, 'totalEpisodes': 219, 'stepsPerEpisode': 41, 'rewardPerEpisode': 29.554338372186095
'totalSteps': 8960, 'rewardStep': 0.6691910477434824, 'errorList': [], 'lossList': [0.0, -1.348465212583542, 0.0, 44.06114615917206, 0.0, 0.0, 0.0], 'rewardMean': 0.7975759604786121, 'totalEpisodes': 229, 'stepsPerEpisode': 114, 'rewardPerEpisode': 90.60158438977814
'totalSteps': 10240, 'rewardStep': 0.9660540021346754, 'errorList': [16.959134301584875, 5.38627178866238, 38.2885895484794, 26.39131418917177, 2.525578035824702, 85.59301120205167, 17.268345266854954, 1.0649507172015322, 43.560668896321545, 43.27961405096298, 47.19143346306103, 78.12504114847339, 71.17255368318204, 37.10170642464753, 48.859246164472644, 5.306328042959841, 23.583843149247233, 45.8173163441084, 5.628819801918265, 7.168100608002112, 42.78861348125883, 1.7684761283126451, 20.983561626497792, 0.9322481547837258, 30.57852601014394, 12.848420000753627, 54.00458786639392, 8.852587836724958, 7.136224042397628, 18.85408639251629, 15.026606722433092, 5.204332732051228, 0.29369907995143935, 42.651185990926436, 0.6958418052867827, 11.450554745772246, 8.947732637925785, 11.666398270000622, 2.6009182770570414, 71.78161833072411, 19.70439454085578, 50.77133370794788, 54.91743352113984, 32.007715554483134, 34.11360252048195, 7.637725906926346, 41.10730049594133, 27.770622169177596, 21.211089362932885, 4.09051354442093], 'lossList': [0.0, -1.350980195403099, 0.0, 23.67386501312256, 0.0, 0.0, 0.0], 'rewardMean': 0.81863571568562, 'totalEpisodes': 235, 'stepsPerEpisode': 235, 'rewardPerEpisode': 194.82454527203421, 'successfulTests': 0
'totalSteps': 11520, 'rewardStep': 0.5999693029324945, 'errorList': [], 'lossList': [0.0, -1.3720182198286057, 0.0, 9.794963563084602, 0.0, 0.0, 0.0], 'rewardMean': 0.7943394476019394, 'totalEpisodes': 239, 'stepsPerEpisode': 108, 'rewardPerEpisode': 79.0780409726521
'totalSteps': 12800, 'rewardStep': 0.8824918678322753, 'errorList': [], 'lossList': [0.0, -1.374738147854805, 0.0, 31.860655853748323, 0.0, 0.0, 0.0], 'rewardMean': 0.8031546896249729, 'totalEpisodes': 245, 'stepsPerEpisode': 64, 'rewardPerEpisode': 58.441235961657796
'totalSteps': 14080, 'rewardStep': 0.8983157722695769, 'errorList': [], 'lossList': [0.0, -1.362476323246956, 0.0, 4.830552510619164, 0.0, 0.0, 0.0], 'rewardMean': 0.7971111464448889, 'totalEpisodes': 249, 'stepsPerEpisode': 130, 'rewardPerEpisode': 117.0074724181568
'totalSteps': 15360, 'rewardStep': 0.9240027850425804, 'errorList': [], 'lossList': [0.0, -1.3413148993253707, 0.0, 4.946986532509327, 0.0, 0.0, 0.0], 'rewardMean': 0.8125842706880221, 'totalEpisodes': 253, 'stepsPerEpisode': 163, 'rewardPerEpisode': 148.3648332544874
'totalSteps': 16640, 'rewardStep': 0.2814715704349669, 'errorList': [], 'lossList': [0.0, -1.3350457972288132, 0.0, 3.9596515715122225, 0.0, 0.0, 0.0], 'rewardMean': 0.7615974292270336, 'totalEpisodes': 256, 'stepsPerEpisode': 310, 'rewardPerEpisode': 239.29901843266165
'totalSteps': 17920, 'rewardStep': 0.828318089331161, 'errorList': [], 'lossList': [0.0, -1.3425365942716598, 0.0, 3.0276247423887255, 0.0, 0.0, 0.0], 'rewardMean': 0.7594973492216246, 'totalEpisodes': 260, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.239704183834437
'totalSteps': 19200, 'rewardStep': 0.9264266151153245, 'errorList': [], 'lossList': [0.0, -1.339697971343994, 0.0, 2.639724564254284, 0.0, 0.0, 0.0], 'rewardMean': 0.7676674327785592, 'totalEpisodes': 262, 'stepsPerEpisode': 366, 'rewardPerEpisode': 332.3781229259723
'totalSteps': 20480, 'rewardStep': 0.8157877127154842, 'errorList': [], 'lossList': [0.0, -1.319117828607559, 0.0, 1.895217588469386, 0.0, 0.0, 0.0], 'rewardMean': 0.7792028765552022, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1153.5598595621198
'totalSteps': 21760, 'rewardStep': 0.7082720274866374, 'errorList': [], 'lossList': [0.0, -1.2862117218971252, 0.0, 1.6250196275115014, 0.0, 0.0, 0.0], 'rewardMean': 0.7831109745295176, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1011.1628702823346
'totalSteps': 23040, 'rewardStep': 0.9432512294554228, 'errorList': [0.04667598732437717, 0.08364546192085041, 0.08097905359483237, 0.04269934059852068, 0.026207236083997304, 0.09216276076376544, 0.06472453644790771, 0.040443113011037896, 0.05855299444854684, 0.06009511916484118, 0.08463360775117253, 0.10184035390720783, 0.022581802538530902, 0.07966624211699581, 0.024571245531043336, 0.02132189767474463, 0.05925066231755088, 0.021374057294865564, 0.022650751934976374, 0.056039098093686, 0.022542015824097525, 0.09173902578638718, 0.029308039929064493, 0.022943310273104145, 0.08091135773684185, 0.04911196664683412, 0.0835567970882375, 0.02306349135838713, 0.021526689316333582, 0.04978182500381833, 0.02181529691133636, 0.023745480720257235, 0.10512857853268212, 0.03098557112101894, 0.02209283660268921, 0.030160202967669147, 0.037089955679798635, 0.06200030891944126, 0.06609056024653516, 0.02552710572026425, 0.05735436770591035, 0.06818824741118837, 0.037049417954869386, 0.020991985600720388, 0.04417518022028246, 0.021217938630674248, 0.020528419037344387, 0.036834459719375885, 0.02318667429308063, 0.059102686978594846], 'lossList': [0.0, -1.2409408420324326, 0.0, 1.2919787722826004, 0.0, 0.0, 0.0], 'rewardMean': 0.7808306972615925, 'totalEpisodes': 262, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.129700644277, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=47.81
