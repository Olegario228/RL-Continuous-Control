#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 2000, 'episodeSteps': 512, 'episodeStepsMax': 640, 'totalLearningSteps': 50000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v0Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v0Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000, 10000], 'controlValues': [[1, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 640, 'rewardStep': 0.41724548439634707, 'errorList': [], 'lossList': [0.0, -1.4227601218223571, 0.0, 112.07625846862793, 0.0, 0.0, 0.0], 'rewardMean': 0.41724548439634707, 'totalEpisodes': 5, 'stepsPerEpisode': 98, 'rewardPerEpisode': 82.27752835355217
'totalSteps': 1280, 'rewardStep': 0.7851114239803991, 'errorList': [], 'lossList': [0.0, -1.4254756844043732, 0.0, 60.64690305709839, 0.0, 0.0, 0.0], 'rewardMean': 0.6011784541883731, 'totalEpisodes': 7, 'stepsPerEpisode': 252, 'rewardPerEpisode': 202.33728021899856
'totalSteps': 1920, 'rewardStep': 0.6537618881449331, 'errorList': [], 'lossList': [0.0, -1.42080273270607, 0.0, 35.62641377449036, 0.0, 0.0, 0.0], 'rewardMean': 0.6187062655072264, 'totalEpisodes': 10, 'stepsPerEpisode': 76, 'rewardPerEpisode': 59.995077503561134
'totalSteps': 2560, 'rewardStep': 0.6196424587452782, 'errorList': [], 'lossList': [0.0, -1.4155885577201843, 0.0, 43.835461902618405, 0.0, 0.0, 0.0], 'rewardMean': 0.6189403138167393, 'totalEpisodes': 10, 'stepsPerEpisode': 640, 'rewardPerEpisode': 458.1294767167397
'totalSteps': 3200, 'rewardStep': 0.8560832518452626, 'errorList': [], 'lossList': [0.0, -1.415295490026474, 0.0, 48.25118795394897, 0.0, 0.0, 0.0], 'rewardMean': 0.666368901422444, 'totalEpisodes': 13, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.887045368587557
'totalSteps': 3840, 'rewardStep': 0.6314743662555561, 'errorList': [], 'lossList': [0.0, -1.4199620580673218, 0.0, 38.75831420898437, 0.0, 0.0, 0.0], 'rewardMean': 0.660553145561296, 'totalEpisodes': 13, 'stepsPerEpisode': 640, 'rewardPerEpisode': 473.233388012366
'totalSteps': 4480, 'rewardStep': 0.8259476560106586, 'errorList': [], 'lossList': [0.0, -1.408467288017273, 0.0, 26.874377918243407, 0.0, 0.0, 0.0], 'rewardMean': 0.6841809327683478, 'totalEpisodes': 13, 'stepsPerEpisode': 640, 'rewardPerEpisode': 459.57765293354163
'totalSteps': 5120, 'rewardStep': 0.5504277775591301, 'errorList': [], 'lossList': [0.0, -1.406962708234787, 0.0, 37.512055711746214, 0.0, 0.0, 0.0], 'rewardMean': 0.6674617883671956, 'totalEpisodes': 15, 'stepsPerEpisode': 153, 'rewardPerEpisode': 106.37714806022953
'totalSteps': 5760, 'rewardStep': 0.612194193528522, 'errorList': [], 'lossList': [0.0, -1.4158646476268768, 0.0, 49.1106774520874, 0.0, 0.0, 0.0], 'rewardMean': 0.6613209444962318, 'totalEpisodes': 16, 'stepsPerEpisode': 475, 'rewardPerEpisode': 387.79344820829743
'totalSteps': 6400, 'rewardStep': 0.8546768480675386, 'errorList': [], 'lossList': [0.0, -1.412453682422638, 0.0, 172.1105686187744, 0.0, 0.0, 0.0], 'rewardMean': 0.6806565348533625, 'totalEpisodes': 25, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.491129745355348
'totalSteps': 7040, 'rewardStep': 0.7340726356082158, 'errorList': [], 'lossList': [0.0, -1.392816399335861, 0.0, 178.90777130126952, 0.0, 0.0, 0.0], 'rewardMean': 0.7123392499745494, 'totalEpisodes': 37, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.027921858388636
'totalSteps': 7680, 'rewardStep': 0.6422395909787384, 'errorList': [], 'lossList': [0.0, -1.3813615190982818, 0.0, 169.81204109191896, 0.0, 0.0, 0.0], 'rewardMean': 0.6980520666743834, 'totalEpisodes': 50, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.959328036791144
'totalSteps': 8320, 'rewardStep': 0.6305616970561853, 'errorList': [], 'lossList': [0.0, -1.366749370098114, 0.0, 126.47738094329834, 0.0, 0.0, 0.0], 'rewardMean': 0.6957320475655087, 'totalEpisodes': 65, 'stepsPerEpisode': 26, 'rewardPerEpisode': 17.82278396684594
'totalSteps': 8960, 'rewardStep': 0.9054738192083277, 'errorList': [], 'lossList': [0.0, -1.3570568835735322, 0.0, 92.77350833892822, 0.0, 0.0, 0.0], 'rewardMean': 0.7243151836118135, 'totalEpisodes': 80, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.354969341279661
'totalSteps': 9600, 'rewardStep': 0.4342516630846759, 'errorList': [], 'lossList': [0.0, -1.349071934223175, 0.0, 76.00690879821778, 0.0, 0.0, 0.0], 'rewardMean': 0.6821320247357547, 'totalEpisodes': 85, 'stepsPerEpisode': 99, 'rewardPerEpisode': 75.03539006734869
'totalSteps': 10240, 'rewardStep': 0.4980772242547467, 'errorList': [], 'lossList': [0.0, -1.3701659846305847, 0.0, 56.198032493591306, 0.0, 0.0, 0.0], 'rewardMean': 0.6687923105356739, 'totalEpisodes': 90, 'stepsPerEpisode': 104, 'rewardPerEpisode': 72.03218513381569
'totalSteps': 10880, 'rewardStep': 0.5396742126307487, 'errorList': [], 'lossList': [0.0, -1.3825553488731384, 0.0, 62.15760608673096, 0.0, 0.0, 0.0], 'rewardMean': 0.6401649661976829, 'totalEpisodes': 96, 'stepsPerEpisode': 39, 'rewardPerEpisode': 29.207883916553016
'totalSteps': 11520, 'rewardStep': 0.8288288249891724, 'errorList': [], 'lossList': [0.0, -1.3888464772701263, 0.0, 20.38090832710266, 0.0, 0.0, 0.0], 'rewardMean': 0.6680050709406872, 'totalEpisodes': 101, 'stepsPerEpisode': 80, 'rewardPerEpisode': 63.413765048080265
'totalSteps': 12160, 'rewardStep': 0.9354773324226755, 'errorList': [], 'lossList': [0.0, -1.3968156027793883, 0.0, 18.78234190940857, 0.0, 0.0, 0.0], 'rewardMean': 0.7003333848301025, 'totalEpisodes': 104, 'stepsPerEpisode': 136, 'rewardPerEpisode': 118.84802016998879
'totalSteps': 12800, 'rewardStep': 0.8958355629012521, 'errorList': [], 'lossList': [0.0, -1.3934434056282043, 0.0, 30.02390690803528, 0.0, 0.0, 0.0], 'rewardMean': 0.7044492563134738, 'totalEpisodes': 106, 'stepsPerEpisode': 275, 'rewardPerEpisode': 223.8935099970354
'totalSteps': 13440, 'rewardStep': 0.9055473956536576, 'errorList': [], 'lossList': [0.0, -1.3873534893989563, 0.0, 16.601754364967345, 0.0, 0.0, 0.0], 'rewardMean': 0.721596732318018, 'totalEpisodes': 108, 'stepsPerEpisode': 84, 'rewardPerEpisode': 73.4842147597744
'totalSteps': 14080, 'rewardStep': 0.7171010815810912, 'errorList': [], 'lossList': [0.0, -1.4015324604511261, 0.0, 8.163287377357483, 0.0, 0.0, 0.0], 'rewardMean': 0.7290828813782533, 'totalEpisodes': 111, 'stepsPerEpisode': 118, 'rewardPerEpisode': 96.25161709404306
'totalSteps': 14720, 'rewardStep': 0.6940124290779334, 'errorList': [], 'lossList': [0.0, -1.4048519217967987, 0.0, 18.299748339653014, 0.0, 0.0, 0.0], 'rewardMean': 0.7354279545804282, 'totalEpisodes': 115, 'stepsPerEpisode': 133, 'rewardPerEpisode': 114.0431161046966
'totalSteps': 15360, 'rewardStep': 0.9494284419087211, 'errorList': [], 'lossList': [0.0, -1.3963276743888855, 0.0, 17.210229864120482, 0.0, 0.0, 0.0], 'rewardMean': 0.7398234168504675, 'totalEpisodes': 118, 'stepsPerEpisode': 55, 'rewardPerEpisode': 41.16226087218361
'totalSteps': 16000, 'rewardStep': 0.5873507213187623, 'errorList': [], 'lossList': [0.0, -1.3993020784854888, 0.0, 8.977988662719726, 0.0, 0.0, 0.0], 'rewardMean': 0.7551333226738761, 'totalEpisodes': 120, 'stepsPerEpisode': 221, 'rewardPerEpisode': 182.1638895671127
'totalSteps': 16640, 'rewardStep': 0.8758035259509476, 'errorList': [], 'lossList': [0.0, -1.3948415076732636, 0.0, 8.335634386539459, 0.0, 0.0, 0.0], 'rewardMean': 0.7929059528434962, 'totalEpisodes': 122, 'stepsPerEpisode': 36, 'rewardPerEpisode': 30.033042285570566
'totalSteps': 17280, 'rewardStep': 0.48867416656953583, 'errorList': [], 'lossList': [0.0, -1.3813006711006164, 0.0, 6.294709060192108, 0.0, 0.0, 0.0], 'rewardMean': 0.7878059482373749, 'totalEpisodes': 122, 'stepsPerEpisode': 640, 'rewardPerEpisode': 512.013930702097
'totalSteps': 17920, 'rewardStep': 0.5099242084015285, 'errorList': [], 'lossList': [0.0, -1.3512017631530762, 0.0, 5.944992444515228, 0.0, 0.0, 0.0], 'rewardMean': 0.7559154865786104, 'totalEpisodes': 124, 'stepsPerEpisode': 205, 'rewardPerEpisode': 151.75890676685896
'totalSteps': 18560, 'rewardStep': 0.8688323713237691, 'errorList': [], 'lossList': [0.0, -1.3282116425037385, 0.0, 24.991499848365784, 0.0, 0.0, 0.0], 'rewardMean': 0.7492509904687199, 'totalEpisodes': 127, 'stepsPerEpisode': 118, 'rewardPerEpisode': 98.97891951411312
'totalSteps': 19200, 'rewardStep': 0.8289265345102658, 'errorList': [], 'lossList': [0.0, -1.3094029688835145, 0.0, 3.1550690722465515, 0.0, 0.0, 0.0], 'rewardMean': 0.7425600876296212, 'totalEpisodes': 128, 'stepsPerEpisode': 91, 'rewardPerEpisode': 70.48251208525762
'totalSteps': 19840, 'rewardStep': 0.7266102869377473, 'errorList': [], 'lossList': [0.0, -1.2927626836299897, 0.0, 3.867022967338562, 0.0, 0.0, 0.0], 'rewardMean': 0.7246663767580301, 'totalEpisodes': 129, 'stepsPerEpisode': 408, 'rewardPerEpisode': 324.25692453241714
'totalSteps': 20480, 'rewardStep': 0.8920488417139512, 'errorList': [], 'lossList': [0.0, -1.290720454454422, 0.0, 3.533426806926727, 0.0, 0.0, 0.0], 'rewardMean': 0.7421611527713161, 'totalEpisodes': 131, 'stepsPerEpisode': 50, 'rewardPerEpisode': 42.31308508622493
'totalSteps': 21120, 'rewardStep': 0.4168154505044996, 'errorList': [], 'lossList': [0.0, -1.2800560975074768, 0.0, 2.2613455986976625, 0.0, 0.0, 0.0], 'rewardMean': 0.7144414549139728, 'totalEpisodes': 132, 'stepsPerEpisode': 392, 'rewardPerEpisode': 290.63741002448904
'totalSteps': 21760, 'rewardStep': 0.4817223744501305, 'errorList': [], 'lossList': [0.0, -1.2590436327457428, 0.0, 1.915319374203682, 0.0, 0.0, 0.0], 'rewardMean': 0.6676708481681137, 'totalEpisodes': 133, 'stepsPerEpisode': 633, 'rewardPerEpisode': 488.90489428536216
'totalSteps': 22400, 'rewardStep': 0.40040538808194165, 'errorList': [], 'lossList': [0.0, -1.2543973910808563, 0.0, 3.1204702353477476, 0.0, 0.0, 0.0], 'rewardMean': 0.6489763148444317, 'totalEpisodes': 133, 'stepsPerEpisode': 640, 'rewardPerEpisode': 327.99708871427447
'totalSteps': 23040, 'rewardStep': 0.965310837257797, 'errorList': [0.14190866473498528, 0.17617978303844292, 0.16822074935749823, 0.1822809481073785, 0.15178304536324952, 0.15233608458196352, 0.15045058620706245, 0.17167162503292185, 0.17356192141953614, 0.1514130475972855, 0.12969718881530756, 0.1823420968262023, 0.1591848044281834, 0.1686552905499826, 0.18021097770189634, 0.18471675733121196, 0.14413609192817534, 0.16548201443001206, 0.16822644210412888, 0.16386753402480267, 0.15906299754077696, 0.18104539582218057, 0.16706226467214727, 0.1579411885234844, 0.16221265790107475, 0.18210436198758245, 0.17467604388406727, 0.14823834564329574, 0.16548339485790164, 0.16273601184534767, 0.14827506223823084, 0.16319907383933746, 0.16520482609996898, 0.17380761554887264, 0.14558984850184925, 0.16967710122284105, 0.1280596010451283, 0.14204392878346742, 0.15604881629630066, 0.16358757583905523, 0.13071044230863382, 0.1784655466798671, 0.17617900066841885, 0.12989837359324047, 0.17248440882260907, 0.1701651767760594, 0.15768029475808298, 0.1490127351897382, 0.21894924089447348, 0.16908008723576035], 'lossList': [0.0, -1.2399068522453307, 0.0, 2.9912302154302597, 0.0, 0.0, 0.0], 'rewardMean': 0.6579270459751166, 'totalEpisodes': 133, 'stepsPerEpisode': 640, 'rewardPerEpisode': 476.54332664028357, 'successfulTests': 49
'totalSteps': 23680, 'rewardStep': 0.7634395775702589, 'errorList': [], 'lossList': [0.0, -1.232475004196167, 0.0, 1.1092909228801728, 0.0, 0.0, 0.0], 'rewardMean': 0.685403587075189, 'totalEpisodes': 133, 'stepsPerEpisode': 640, 'rewardPerEpisode': 536.8970837214619
'totalSteps': 24320, 'rewardStep': 0.9616365933105228, 'errorList': [], 'lossList': [0.0, -1.2169147062301635, 0.0, 0.6958993074297904, 0.0, 0.0, 0.0], 'rewardMean': 0.7305748255660884, 'totalEpisodes': 133, 'stepsPerEpisode': 640, 'rewardPerEpisode': 574.2242413403739
'totalSteps': 24960, 'rewardStep': 0.9069627858640983, 'errorList': [], 'lossList': [0.0, -1.1929020702838897, 0.0, 0.5669022219628096, 0.0, 0.0, 0.0], 'rewardMean': 0.7343878670201213, 'totalEpisodes': 133, 'stepsPerEpisode': 640, 'rewardPerEpisode': 583.7199567481949
'totalSteps': 25600, 'rewardStep': 0.8985001347712948, 'errorList': [], 'lossList': [0.0, -1.1675222969055177, 0.0, 0.4976406903937459, 0.0, 0.0, 0.0], 'rewardMean': 0.7413452270462242, 'totalEpisodes': 133, 'stepsPerEpisode': 640, 'rewardPerEpisode': 588.3877980835503
'totalSteps': 26240, 'rewardStep': 0.9570554221040148, 'errorList': [0.043209124348142655, 0.0426813531330397, 0.034560864690045046, 0.029258594260523817, 0.046703213303148756, 0.03588796975848258, 0.02898332962454655, 0.04011746750408989, 0.04530031944532749, 0.029361295934184718, 0.038322542174764815, 0.035244343558921344, 0.03580124154262212, 0.04661895972328946, 0.03337313933663308, 0.04183771609340647, 0.03577492183793022, 0.03155398457801436, 0.05047219611977028, 0.02837341536921748, 0.03515127608256768, 0.037561984117053464, 0.035241109301028146, 0.0287900440651657, 0.027730277343045128, 0.04246570152814886, 0.03975675140883831, 0.033606681063146474, 0.042933290100873975, 0.03434856673808464, 0.04401370599365691, 0.0483093754127201, 0.04383440704315837, 0.03607641135898993, 0.03182998947537413, 0.03994074829710587, 0.02901949291121269, 0.03554542591251389, 0.03240300463642696, 0.04042458205266571, 0.029172753457827997, 0.02880826934702878, 0.05352818668544973, 0.030938674324768982, 0.0338365356122028, 0.046151861082541504, 0.030874091987965115, 0.04139982465933983, 0.02887980746780385, 0.02882476255772545], 'lossList': [0.0, -1.1397930598258972, 0.0, 0.4762882538512349, 0.0, 0.0, 0.0], 'rewardMean': 0.7643897405628509, 'totalEpisodes': 133, 'stepsPerEpisode': 640, 'rewardPerEpisode': 610.5046652576658, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26240, timeSpent=67.73
