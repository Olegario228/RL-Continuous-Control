#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v10_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v10_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.02, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8192251588837277, 'errorList': [], 'lossList': [0.0, -1.4277081954479218, 0.0, 63.88391847610474, 0.0, 0.0, 0.0], 'rewardMean': 0.8192251588837277, 'totalEpisodes': 8, 'stepsPerEpisode': 230, 'rewardPerEpisode': 156.1498342619166
'totalSteps': 2560, 'rewardStep': 0.8342572871198431, 'errorList': [], 'lossList': [0.0, -1.43206859767437, 0.0, 28.511502556800842, 0.0, 0.0, 0.0], 'rewardMean': 0.8267412230017854, 'totalEpisodes': 19, 'stepsPerEpisode': 34, 'rewardPerEpisode': 24.49365524975698
'totalSteps': 3840, 'rewardStep': 0.69314329289902, 'errorList': [], 'lossList': [0.0, -1.4207222551107406, 0.0, 31.44451387643814, 0.0, 0.0, 0.0], 'rewardMean': 0.782208579634197, 'totalEpisodes': 28, 'stepsPerEpisode': 70, 'rewardPerEpisode': 47.69001823071633
'totalSteps': 5120, 'rewardStep': 0.7273857101532689, 'errorList': [], 'lossList': [0.0, -1.410679157972336, 0.0, 24.084052441120146, 0.0, 0.0, 0.0], 'rewardMean': 0.768502862263965, 'totalEpisodes': 29, 'stepsPerEpisode': 181, 'rewardPerEpisode': 144.6555731007027
'totalSteps': 6400, 'rewardStep': 0.5481439402687539, 'errorList': [], 'lossList': [0.0, -1.402756506204605, 0.0, 35.96460287570953, 0.0, 0.0, 0.0], 'rewardMean': 0.7244310778649228, 'totalEpisodes': 32, 'stepsPerEpisode': 741, 'rewardPerEpisode': 532.1639735354964
'totalSteps': 7680, 'rewardStep': 0.807243993202888, 'errorList': [], 'lossList': [0.0, -1.394320746064186, 0.0, 36.803482848405835, 0.0, 0.0, 0.0], 'rewardMean': 0.7382332304212503, 'totalEpisodes': 35, 'stepsPerEpisode': 50, 'rewardPerEpisode': 33.945658075776045
'totalSteps': 8960, 'rewardStep': 0.9663863865914456, 'errorList': [], 'lossList': [0.0, -1.38781804561615, 0.0, 144.47025428771974, 0.0, 0.0, 0.0], 'rewardMean': 0.770826538445564, 'totalEpisodes': 48, 'stepsPerEpisode': 108, 'rewardPerEpisode': 90.83474086600299
'totalSteps': 10240, 'rewardStep': 0.6171620757330719, 'errorList': [], 'lossList': [0.0, -1.3875333631038667, 0.0, 75.46727825164795, 0.0, 0.0, 0.0], 'rewardMean': 0.7516184806065024, 'totalEpisodes': 60, 'stepsPerEpisode': 69, 'rewardPerEpisode': 54.00577090033303
'totalSteps': 11520, 'rewardStep': 0.8127088679606452, 'errorList': [], 'lossList': [0.0, -1.392410249710083, 0.0, 24.616562848091124, 0.0, 0.0, 0.0], 'rewardMean': 0.7584063014236294, 'totalEpisodes': 70, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.104116954409293
'totalSteps': 12800, 'rewardStep': 0.8381789821858465, 'errorList': [], 'lossList': [0.0, -1.3993554598093032, 0.0, 10.313226835727692, 0.0, 0.0, 0.0], 'rewardMean': 0.7663835694998511, 'totalEpisodes': 75, 'stepsPerEpisode': 139, 'rewardPerEpisode': 109.33569089924391
'totalSteps': 14080, 'rewardStep': 0.8289610646476607, 'errorList': [], 'lossList': [0.0, -1.4037929004430771, 0.0, 7.019495834112167, 0.0, 0.0, 0.0], 'rewardMean': 0.7673571600762444, 'totalEpisodes': 78, 'stepsPerEpisode': 283, 'rewardPerEpisode': 220.6201613167528
'totalSteps': 15360, 'rewardStep': 0.49279130875694, 'errorList': [], 'lossList': [0.0, -1.414015727043152, 0.0, 7.2160551440715786, 0.0, 0.0, 0.0], 'rewardMean': 0.7332105622399542, 'totalEpisodes': 79, 'stepsPerEpisode': 374, 'rewardPerEpisode': 289.10841617557253
'totalSteps': 16640, 'rewardStep': 0.7711382702648523, 'errorList': [], 'lossList': [0.0, -1.4006698733568193, 0.0, 3.6144031006097794, 0.0, 0.0, 0.0], 'rewardMean': 0.7410100599765372, 'totalEpisodes': 79, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 904.4992499574043
'totalSteps': 17920, 'rewardStep': 0.7964162654383958, 'errorList': [], 'lossList': [0.0, -1.356669273376465, 0.0, 3.325372462272644, 0.0, 0.0, 0.0], 'rewardMean': 0.7479131155050499, 'totalEpisodes': 79, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1069.674628424471
'totalSteps': 19200, 'rewardStep': 0.8419421028181595, 'errorList': [], 'lossList': [0.0, -1.3274001014232635, 0.0, 1.8670544910430908, 0.0, 0.0, 0.0], 'rewardMean': 0.7772929317599906, 'totalEpisodes': 79, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1058.7399198586206
'totalSteps': 20480, 'rewardStep': 0.9687134757956547, 'errorList': [0.08827255257784446, 0.09591768830143904, 0.09655235151735075, 0.1277463472258847, 0.08865152275920979, 0.09336804137395638, 0.10707960028710407, 0.0810373205564196, 0.08804860429381324, 0.09602088923536087, 0.09254171612275447, 0.09584117163282196, 0.09598475030032891, 0.09586669581138707, 0.09109570947858521, 0.10437787742778967, 0.09252207511195287, 0.09944426474928315, 0.08946002029483495, 0.1096136991011109, 0.07832520094337636, 0.095212284637964, 0.09698580039534124, 0.09925982431725235, 0.1152167226640035, 0.10587595831444861, 0.10119373333688463, 0.10615587286954967, 0.09906387732110958, 0.09499351856988192, 0.10174311696160947, 0.08867352079882197, 0.09520673098230875, 0.07882032348008156, 0.09110331202758193, 0.10799979435233889, 0.09395614283364648, 0.09714147805233106, 0.09213403192781884, 0.0865369369933285, 0.08358750818061227, 0.08611553399715147, 0.10654030393755046, 0.08561755233584452, 0.11882517660076762, 0.09634227044084807, 0.08844728258363205, 0.09101253090163614, 0.08467281608078106, 0.10521149289308217], 'lossList': [0.0, -1.2930710244178771, 0.0, 1.7109192673861982, 0.0, 0.0, 0.0], 'rewardMean': 0.7934398800192672, 'totalEpisodes': 79, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.4530567514807, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=57.25
