#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v10_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v10_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.02, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7835440647648731, 'errorList': [], 'lossList': [0.0, -1.4133729392290115, 0.0, 72.71623851299286, 0.0, 0.0, 0.0], 'rewardMean': 0.7835440647648731, 'totalEpisodes': 8, 'stepsPerEpisode': 146, 'rewardPerEpisode': 115.73487882869244
'totalSteps': 2560, 'rewardStep': 0.8717966983198038, 'errorList': [], 'lossList': [0.0, -1.4121039372682571, 0.0, 30.561635375022888, 0.0, 0.0, 0.0], 'rewardMean': 0.8276703815423385, 'totalEpisodes': 24, 'stepsPerEpisode': 21, 'rewardPerEpisode': 19.151190964417573
'totalSteps': 3840, 'rewardStep': 0.9042014554048083, 'errorList': [], 'lossList': [0.0, -1.4041640651226044, 0.0, 25.632017414569855, 0.0, 0.0, 0.0], 'rewardMean': 0.8531807394964952, 'totalEpisodes': 35, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.664342482781342
'totalSteps': 5120, 'rewardStep': 0.6743985900238372, 'errorList': [], 'lossList': [0.0, -1.3853211003541945, 0.0, 21.7644322681427, 0.0, 0.0, 0.0], 'rewardMean': 0.8084852021283306, 'totalEpisodes': 38, 'stepsPerEpisode': 738, 'rewardPerEpisode': 532.9786565428683
'totalSteps': 6400, 'rewardStep': 0.6796999363305913, 'errorList': [], 'lossList': [0.0, -1.3760373896360398, 0.0, 32.290167634487155, 0.0, 0.0, 0.0], 'rewardMean': 0.7827281489687827, 'totalEpisodes': 41, 'stepsPerEpisode': 708, 'rewardPerEpisode': 544.5161744631284
'totalSteps': 7680, 'rewardStep': 0.7792640189320387, 'errorList': [], 'lossList': [0.0, -1.3615232664346695, 0.0, 21.12993733406067, 0.0, 0.0, 0.0], 'rewardMean': 0.7821507939626587, 'totalEpisodes': 43, 'stepsPerEpisode': 294, 'rewardPerEpisode': 205.60949746255935
'totalSteps': 8960, 'rewardStep': 0.8226156537648821, 'errorList': [], 'lossList': [0.0, -1.347670195698738, 0.0, 214.22734878540038, 0.0, 0.0, 0.0], 'rewardMean': 0.7879314882201192, 'totalEpisodes': 69, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.3949398589965325
'totalSteps': 10240, 'rewardStep': 0.9226521845421443, 'errorList': [], 'lossList': [0.0, -1.3344089901447296, 0.0, 143.14182483673096, 0.0, 0.0, 0.0], 'rewardMean': 0.8047715752603724, 'totalEpisodes': 96, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.475829102870108
'totalSteps': 11520, 'rewardStep': 0.5053903869490466, 'errorList': [], 'lossList': [0.0, -1.3318664598464967, 0.0, 55.93230540275574, 0.0, 0.0, 0.0], 'rewardMean': 0.7715069987813361, 'totalEpisodes': 116, 'stepsPerEpisode': 37, 'rewardPerEpisode': 25.260456174936124
'totalSteps': 12800, 'rewardStep': 0.7261433435452287, 'errorList': [], 'lossList': [0.0, -1.3256501525640487, 0.0, 27.518963742256165, 0.0, 0.0, 0.0], 'rewardMean': 0.7669706332577254, 'totalEpisodes': 128, 'stepsPerEpisode': 64, 'rewardPerEpisode': 52.64663195097806
'totalSteps': 14080, 'rewardStep': 0.5786581541667675, 'errorList': [], 'lossList': [0.0, -1.3087981152534485, 0.0, 13.13250497341156, 0.0, 0.0, 0.0], 'rewardMean': 0.7464820421979148, 'totalEpisodes': 136, 'stepsPerEpisode': 76, 'rewardPerEpisode': 50.62068016987949
'totalSteps': 15360, 'rewardStep': 0.9891517274872794, 'errorList': [8.120840201889864, 13.79520987662976, 6.0216734621161105, 1.3030007607945862, 3.605533483105262, 5.407878577569168, 3.2729292347720746, 7.2342836461382145, 7.585631140666096, 2.2196424702039685, 9.303660832450534, 2.495589956839993, 6.042669233173135, 11.637836734366374, 10.023094269694976, 10.410992056028938, 10.812192826621894, 9.652614591073291, 9.833388209966296, 4.399020754972996, 3.067525739880218, 8.261859221757065, 4.288869999328681, 8.80337293680449, 6.057262865592998, 5.633140735332783, 7.743472892361946, 5.986125736169914, 5.051125115331025, 10.678770808714503, 4.954875429043482, 13.549070484036834, 1.15126353944056, 8.15547492577964, 3.7726108041449966, 8.565743524320856, 7.840770895626077, 8.450090345082454, 9.143067780891554, 6.090881602231778, 2.459040588532146, 7.163491546017815, 1.51080952084181, 8.453447310833797, 14.1643003474547, 8.554334531907884, 11.8440479630427, 5.264658160961618, 7.086504314842912, 9.481598417851956], 'lossList': [0.0, -1.2918517231941222, 0.0, 32.275495648384094, 0.0, 0.0, 0.0], 'rewardMean': 0.7582175451146623, 'totalEpisodes': 144, 'stepsPerEpisode': 96, 'rewardPerEpisode': 84.98277848143964, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.4367461374581131, 'errorList': [], 'lossList': [0.0, -1.2569148921966553, 0.0, 7.203812160491943, 0.0, 0.0, 0.0], 'rewardMean': 0.7114720133199929, 'totalEpisodes': 150, 'stepsPerEpisode': 81, 'rewardPerEpisode': 63.30883471599675
'totalSteps': 17920, 'rewardStep': 0.43378366963489123, 'errorList': [], 'lossList': [0.0, -1.2481081914901733, 0.0, 7.054963217973709, 0.0, 0.0, 0.0], 'rewardMean': 0.6874105212810983, 'totalEpisodes': 153, 'stepsPerEpisode': 347, 'rewardPerEpisode': 249.60063680775505
'totalSteps': 19200, 'rewardStep': 0.8549321221723095, 'errorList': [], 'lossList': [0.0, -1.2584867906570434, 0.0, 5.8469785273075106, 0.0, 0.0, 0.0], 'rewardMean': 0.7049337398652702, 'totalEpisodes': 156, 'stepsPerEpisode': 83, 'rewardPerEpisode': 71.59518598572403
'totalSteps': 20480, 'rewardStep': 0.8598877675669735, 'errorList': [], 'lossList': [0.0, -1.2567627787590028, 0.0, 6.074536395072937, 0.0, 0.0, 0.0], 'rewardMean': 0.7129961147287636, 'totalEpisodes': 157, 'stepsPerEpisode': 768, 'rewardPerEpisode': 647.6808677143825
'totalSteps': 21760, 'rewardStep': 0.7362956450221488, 'errorList': [], 'lossList': [0.0, -1.2359938740730285, 0.0, 3.862463586330414, 0.0, 0.0, 0.0], 'rewardMean': 0.7043641138544903, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 962.309788655146
'totalSteps': 23040, 'rewardStep': 0.8504177114574344, 'errorList': [], 'lossList': [0.0, -1.1988555496931077, 0.0, 2.0233284628391264, 0.0, 0.0, 0.0], 'rewardMean': 0.6971406665460192, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1050.5627828649413
'totalSteps': 24320, 'rewardStep': 0.8269952602600212, 'errorList': [], 'lossList': [0.0, -1.1712880474328995, 0.0, 0.9818459665030241, 0.0, 0.0, 0.0], 'rewardMean': 0.7293011538771168, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.850358174532
'totalSteps': 25600, 'rewardStep': 0.9458938279457467, 'errorList': [0.10874995067016455, 0.09642874708857091, 0.09885687839594003, 0.09306406231266523, 0.09320960952330747, 0.09598463499669858, 0.09690014260611543, 0.09963984397554822, 0.09875331655497208, 0.08986719254982953, 0.0913851977335193, 0.10457986068472198, 0.11249295435411999, 0.09594927080299935, 0.10898617694763342, 0.10940895993505999, 0.1073708632724441, 0.11658909091081351, 0.10312964069114103, 0.09667585775578053, 0.10248483392537794, 0.09999382173015155, 0.1003012135862, 0.11034155422352052, 0.0997781584562653, 0.09518121170389865, 0.09493866880863573, 0.10842950384868297, 0.09456541551584219, 0.09068173809250268, 0.10805611088283514, 0.09329576955145842, 0.11138416649906117, 0.11901200203394637, 0.10051601938919005, 0.1056696947277768, 0.09959416308704644, 0.11237433138790374, 0.10561686692648645, 0.10135678843839477, 0.1062634396121985, 0.09225866221328244, 0.10629635625764584, 0.09116381154291694, 0.10174794590600765, 0.11624365001985246, 0.10759295570759778, 0.1093665314611552, 0.11630977592156524, 0.11131381206149453], 'lossList': [0.0, -1.1462577772140503, 0.0, 1.2792925641313195, 0.0, 0.0, 0.0], 'rewardMean': 0.7512762023171686, 'totalEpisodes': 157, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1158.3875311796835, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=80.13
