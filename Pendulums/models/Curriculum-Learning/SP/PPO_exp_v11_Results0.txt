#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.01, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8917537992574822, 'errorList': [], 'lossList': [0.0, -1.416672824025154, 0.0, 75.01589797496796, 0.0, 0.0, 0.0], 'rewardMean': 0.8917537992574822, 'totalEpisodes': 11, 'stepsPerEpisode': 157, 'rewardPerEpisode': 126.64751138816906
'totalSteps': 2560, 'rewardStep': 0.8126568824152784, 'errorList': [], 'lossList': [0.0, -1.4104816156625748, 0.0, 32.59547493457794, 0.0, 0.0, 0.0], 'rewardMean': 0.8522053408363803, 'totalEpisodes': 27, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.16370436718997
'totalSteps': 3840, 'rewardStep': 0.592031683851583, 'errorList': [], 'lossList': [0.0, -1.4077059412002564, 0.0, 22.151850113868715, 0.0, 0.0, 0.0], 'rewardMean': 0.7654807885081145, 'totalEpisodes': 40, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.592031683851583
'totalSteps': 5120, 'rewardStep': 0.6953985220453862, 'errorList': [], 'lossList': [0.0, -1.402791792154312, 0.0, 24.014736639261244, 0.0, 0.0, 0.0], 'rewardMean': 0.7479602218924324, 'totalEpisodes': 45, 'stepsPerEpisode': 272, 'rewardPerEpisode': 205.56990623177336
'totalSteps': 6400, 'rewardStep': 0.8890883276642403, 'errorList': [], 'lossList': [0.0, -1.4002481573820114, 0.0, 26.733302346467973, 0.0, 0.0, 0.0], 'rewardMean': 0.7761858430467939, 'totalEpisodes': 46, 'stepsPerEpisode': 207, 'rewardPerEpisode': 174.79279305865612
'totalSteps': 7680, 'rewardStep': 0.7928778751986694, 'errorList': [], 'lossList': [0.0, -1.3667289584875106, 0.0, 22.584079279899598, 0.0, 0.0, 0.0], 'rewardMean': 0.7789678484054399, 'totalEpisodes': 47, 'stepsPerEpisode': 420, 'rewardPerEpisode': 316.22128304317124
'totalSteps': 8960, 'rewardStep': 0.8405678857331675, 'errorList': [], 'lossList': [0.0, -1.3354985892772675, 0.0, 150.89388580322264, 0.0, 0.0, 0.0], 'rewardMean': 0.7877678537379724, 'totalEpisodes': 60, 'stepsPerEpisode': 100, 'rewardPerEpisode': 89.27264549348976
'totalSteps': 10240, 'rewardStep': 0.5401137352623776, 'errorList': [], 'lossList': [0.0, -1.330791835784912, 0.0, 154.51722923278808, 0.0, 0.0, 0.0], 'rewardMean': 0.7568110889285231, 'totalEpisodes': 84, 'stepsPerEpisode': 50, 'rewardPerEpisode': 37.8863015105748
'totalSteps': 11520, 'rewardStep': 0.829005283561845, 'errorList': [], 'lossList': [0.0, -1.316832584142685, 0.0, 76.2579768371582, 0.0, 0.0, 0.0], 'rewardMean': 0.7648326661100033, 'totalEpisodes': 99, 'stepsPerEpisode': 48, 'rewardPerEpisode': 42.016590623929375
'totalSteps': 12800, 'rewardStep': 0.6002467882178816, 'errorList': [], 'lossList': [0.0, -1.299705468416214, 0.0, 34.067854166030884, 0.0, 0.0, 0.0], 'rewardMean': 0.7483740783207911, 'totalEpisodes': 109, 'stepsPerEpisode': 121, 'rewardPerEpisode': 84.45977814631036
'totalSteps': 14080, 'rewardStep': 0.2850532543465603, 'errorList': [], 'lossList': [0.0, -1.296767166852951, 0.0, 10.474319896697999, 0.0, 0.0, 0.0], 'rewardMean': 0.687704023829699, 'totalEpisodes': 115, 'stepsPerEpisode': 152, 'rewardPerEpisode': 103.73383835652025
'totalSteps': 15360, 'rewardStep': 0.511541963719931, 'errorList': [], 'lossList': [0.0, -1.3022814977169037, 0.0, 9.297359005212783, 0.0, 0.0, 0.0], 'rewardMean': 0.6575925319601642, 'totalEpisodes': 120, 'stepsPerEpisode': 229, 'rewardPerEpisode': 172.86756549443814
'totalSteps': 16640, 'rewardStep': 0.6693454463415096, 'errorList': [], 'lossList': [0.0, -1.3043922984600067, 0.0, 8.156528404951096, 0.0, 0.0, 0.0], 'rewardMean': 0.6653239082091568, 'totalEpisodes': 123, 'stepsPerEpisode': 171, 'rewardPerEpisode': 125.64603561912746
'totalSteps': 17920, 'rewardStep': 0.8183904146860863, 'errorList': [], 'lossList': [0.0, -1.2727319765090943, 0.0, 4.419206168353558, 0.0, 0.0, 0.0], 'rewardMean': 0.6776230974732269, 'totalEpisodes': 124, 'stepsPerEpisode': 1143, 'rewardPerEpisode': 930.4121407886342
'totalSteps': 19200, 'rewardStep': 0.8718013687950269, 'errorList': [], 'lossList': [0.0, -1.236239075064659, 0.0, 30.029325827360154, 0.0, 0.0, 0.0], 'rewardMean': 0.6758944015863055, 'totalEpisodes': 126, 'stepsPerEpisode': 58, 'rewardPerEpisode': 41.40614419824548
'totalSteps': 20480, 'rewardStep': 0.9336619778421824, 'errorList': [0.1462279071126509, 0.14430038463503775, 0.17265996666065178, 0.19802675171902132, 0.1431515921028572, 0.14816371712547513, 0.14082015715742952, 0.1606923147080068, 0.1443566817820346, 0.14171499541170549, 0.22015354053495184, 0.15452867572138823, 0.14187259834904015, 0.35983210571283636, 0.16327593824934436, 0.142846925101843, 0.14482481548836063, 0.14615124019255119, 0.14206795603934086, 0.23174763154610484, 0.39382847609537197, 0.14012432892033466, 0.14003530010127643, 0.1383382749265632, 0.14423047951305415, 0.2181996938840689, 0.17417986911013822, 0.15149188345900055, 0.14615252791323277, 0.1447144525231278, 0.14126280722386306, 0.14200398760179986, 0.15122042088919105, 0.14643785423713812, 0.13948185471489905, 0.14782901552048783, 0.19266688052795528, 0.13888879421315065, 0.18336868254764307, 0.21037683607633248, 0.16221641320282504, 0.14238098366167165, 0.25238048057788104, 0.1410215042936474, 0.2868082329319506, 0.14493909496870394, 0.18422093397440437, 0.19944604345718528, 0.14827995920139547, 0.16482702337999755], 'lossList': [0.0, -1.235045120716095, 0.0, 3.1322953782975675, 0.0, 0.0, 0.0], 'rewardMean': 0.6899728118506567, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.9805510129856, 'successfulTests': 42
'totalSteps': 21760, 'rewardStep': 0.924638440257535, 'errorList': [], 'lossList': [0.0, -1.2391903018951416, 0.0, 2.3938902281224728, 0.0, 0.0, 0.0], 'rewardMean': 0.6983798673030935, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1079.6320889375168
'totalSteps': 23040, 'rewardStep': 0.7673547611877553, 'errorList': [], 'lossList': [0.0, -1.2216479218006133, 0.0, 1.3135955925285816, 0.0, 0.0, 0.0], 'rewardMean': 0.7211039698956314, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1103.7956918678242
'totalSteps': 24320, 'rewardStep': 0.73965079381341, 'errorList': [], 'lossList': [0.0, -1.1840047490596772, 0.0, 1.445726933479309, 0.0, 0.0, 0.0], 'rewardMean': 0.7121685209207879, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1156.2113971784977
'totalSteps': 25600, 'rewardStep': 0.9338008592975775, 'errorList': [0.2233531984642683, 0.12037566661964814, 0.15807808379438767, 0.09883647484571123, 0.14860802338350512, 0.20576538190943855, 0.10676108768287593, 0.15837171042935225, 0.10816359816636219, 0.1074729876516263, 0.16472069812983714, 0.1463420487937086, 0.16095256476573655, 0.08039440639049526, 0.06712224289219418, 0.0886412189375702, 0.15784341257229692, 0.08739291355780411, 0.15321171251853624, 0.12206700164673748, 0.11350858470227518, 0.09647978520533898, 0.1043999765558497, 0.10407339819617736, 0.24447461972202583, 0.101530552077987, 0.1535207277960239, 0.16065936702847577, 0.19595169806085153, 0.10430424450958098, 0.06812610594302254, 0.11849357145342909, 0.11010057894636767, 0.14334551376882892, 0.15785975005608213, 0.054732329522876974, 0.19018508998981154, 0.08814562765718759, 0.11701442648803849, 0.10182372034961619, 0.0829167447736831, 0.08463521222804402, 0.09912294336433171, 0.06188258372239696, 0.12223137681320007, 0.11439940338712036, 0.10576725006949722, 0.09788580949294608, 0.10321689003069963, 0.13428306156376144], 'lossList': [0.0, -1.1493046110868455, 0.0, 0.8040312610566616, 0.0, 0.0, 0.0], 'rewardMean': 0.7455239280287574, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.1524176947196, 'successfulTests': 47
'totalSteps': 26880, 'rewardStep': 0.965229724150847, 'errorList': [0.13003869923538222, 0.055213617543547824, 0.05753714276215668, 0.05685128110064189, 0.043652031539592184, 0.07012017215093479, 0.09716393495456624, 0.07030614941364836, 0.09273261662060057, 0.15147777764945292, 0.03512500616790425, 0.026756888969529113, 0.08306910721750496, 0.02683691520779125, 0.029179072694475488, 0.061759349494789936, 0.04887970047136797, 0.07774439710511667, 0.027332340615595405, 0.05285108397946553, 0.09333505690521841, 0.04177892698787979, 0.0357610241637024, 0.0283406600040923, 0.06157770313318837, 0.040809737615351234, 0.03258182186024127, 0.04950782352008822, 0.041344289209980074, 0.05381608559617776, 0.03697197763204407, 0.0381857356322656, 0.04654428508329133, 0.021233162413290322, 0.0746411011786904, 0.031139443346376278, 0.025493467907993908, 0.06730721253736179, 0.07264710830545043, 0.07837627742838477, 0.02976240516079945, 0.05260997994527776, 0.07182335379144807, 0.039692964545633924, 0.04535042787405193, 0.08587306724389179, 0.06408825848814804, 0.07777084109349007, 0.06090583990405217, 0.02933209601317045], 'lossList': [0.0, -1.122970736026764, 0.0, 0.608193929195404, 0.0, 0.0, 0.0], 'rewardMean': 0.8135415750091862, 'totalEpisodes': 126, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1160.5694058597658, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=87.53
