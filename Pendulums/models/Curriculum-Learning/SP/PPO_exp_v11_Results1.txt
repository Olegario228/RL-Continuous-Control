#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.01, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7007366092183429, 'errorList': [], 'lossList': [0.0, -1.4100516611337661, 0.0, 61.63966194152832, 0.0, 0.0, 0.0], 'rewardMean': 0.7007366092183429, 'totalEpisodes': 13, 'stepsPerEpisode': 99, 'rewardPerEpisode': 81.36989336878334
'totalSteps': 2560, 'rewardStep': 0.8189499606986063, 'errorList': [], 'lossList': [0.0, -1.4074388474225998, 0.0, 33.681897764205935, 0.0, 0.0, 0.0], 'rewardMean': 0.7598432849584746, 'totalEpisodes': 32, 'stepsPerEpisode': 70, 'rewardPerEpisode': 52.3981140866757
'totalSteps': 3840, 'rewardStep': 0.7084929788521187, 'errorList': [], 'lossList': [0.0, -1.4038635331392288, 0.0, 33.010725040435794, 0.0, 0.0, 0.0], 'rewardMean': 0.742726516256356, 'totalEpisodes': 46, 'stepsPerEpisode': 57, 'rewardPerEpisode': 36.41410200485821
'totalSteps': 5120, 'rewardStep': 0.8120982478934705, 'errorList': [], 'lossList': [0.0, -1.3885850751399993, 0.0, 26.891752076148986, 0.0, 0.0, 0.0], 'rewardMean': 0.7600694491656346, 'totalEpisodes': 55, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.74640920750402
'totalSteps': 6400, 'rewardStep': 0.49277071576304005, 'errorList': [], 'lossList': [0.0, -1.3779020142555236, 0.0, 25.63938404560089, 0.0, 0.0, 0.0], 'rewardMean': 0.7066097024851157, 'totalEpisodes': 58, 'stepsPerEpisode': 303, 'rewardPerEpisode': 184.11666977449474
'totalSteps': 7680, 'rewardStep': 0.6290802774993306, 'errorList': [], 'lossList': [0.0, -1.3671909308433532, 0.0, 11.63072975218296, 0.0, 0.0, 0.0], 'rewardMean': 0.6936881316541514, 'totalEpisodes': 58, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 927.8166597725125
'totalSteps': 8960, 'rewardStep': 0.5982955748561652, 'errorList': [], 'lossList': [0.0, -1.3475072407722473, 0.0, 161.62352474212648, 0.0, 0.0, 0.0], 'rewardMean': 0.6800606235401535, 'totalEpisodes': 75, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.35746593090129
'totalSteps': 10240, 'rewardStep': 0.5828087805474054, 'errorList': [], 'lossList': [0.0, -1.3349916088581084, 0.0, 136.2088112258911, 0.0, 0.0, 0.0], 'rewardMean': 0.66790414316606, 'totalEpisodes': 98, 'stepsPerEpisode': 63, 'rewardPerEpisode': 43.51032556210398
'totalSteps': 11520, 'rewardStep': 0.5704423829132309, 'errorList': [], 'lossList': [0.0, -1.3315475279092788, 0.0, 47.26324621200561, 0.0, 0.0, 0.0], 'rewardMean': 0.6570750586935233, 'totalEpisodes': 108, 'stepsPerEpisode': 155, 'rewardPerEpisode': 121.02791478169095
'totalSteps': 12800, 'rewardStep': 0.9458914100509719, 'errorList': [180.49151595484358, 24.52428235600589, 63.07819741883053, 161.58926381347968, 137.41925569298678, 56.555647224690865, 170.2037776321063, 124.60515047847349, 80.57655981424138, 93.93221125716829, 150.66616776034047, 85.40171006926217, 55.83409447505917, 3.5425773119390795, 166.85427084201802, 43.63397636161323, 102.02904056899877, 141.88849897432257, 179.06065332459863, 78.44471223992231, 151.2931521517522, 147.02649903350758, 6.87132903087433, 174.25334521333411, 145.72186927546758, 128.86355931431842, 79.32522663275559, 27.516795332107492, 32.97896651856578, 190.23444008965285, 136.08044610782235, 153.36586825756663, 152.9212380988548, 8.487771641501014, 124.98951776245885, 117.7581014399717, 117.09990715554514, 46.534650039404966, 128.96195814217938, 153.97819982083575, 117.10129576202904, 90.94990813034184, 144.0596675817981, 65.4612435029891, 105.67691605019631, 110.18241004888002, 127.8866421968258, 95.5006601637715, 15.596014157349586, 167.90818912894133], 'lossList': [0.0, -1.3181412315368652, 0.0, 55.65784296035767, 0.0, 0.0, 0.0], 'rewardMean': 0.6859566938292682, 'totalEpisodes': 125, 'stepsPerEpisode': 19, 'rewardPerEpisode': 17.43077899615393, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.7545311405052388, 'errorList': [], 'lossList': [0.0, -1.2959630626440048, 0.0, 10.930465767383575, 0.0, 0.0, 0.0], 'rewardMean': 0.6913361469579578, 'totalEpisodes': 134, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.756279717121533
'totalSteps': 15360, 'rewardStep': 0.647645672952673, 'errorList': [], 'lossList': [0.0, -1.286049764752388, 0.0, 11.89053650021553, 0.0, 0.0, 0.0], 'rewardMean': 0.6742057181833644, 'totalEpisodes': 138, 'stepsPerEpisode': 178, 'rewardPerEpisode': 144.94976809288514
'totalSteps': 16640, 'rewardStep': 0.8164557092680652, 'errorList': [], 'lossList': [0.0, -1.28262060880661, 0.0, 19.506397252082824, 0.0, 0.0, 0.0], 'rewardMean': 0.6850019912249591, 'totalEpisodes': 144, 'stepsPerEpisode': 71, 'rewardPerEpisode': 58.89133299645442
'totalSteps': 17920, 'rewardStep': 0.7493808486708373, 'errorList': [], 'lossList': [0.0, -1.2736043965816497, 0.0, 29.84521831035614, 0.0, 0.0, 0.0], 'rewardMean': 0.6787302513026958, 'totalEpisodes': 149, 'stepsPerEpisode': 34, 'rewardPerEpisode': 27.585545528730112
'totalSteps': 19200, 'rewardStep': 0.7079093895996058, 'errorList': [], 'lossList': [0.0, -1.2464147484302521, 0.0, 4.932339919805527, 0.0, 0.0, 0.0], 'rewardMean': 0.7002441186863524, 'totalEpisodes': 152, 'stepsPerEpisode': 382, 'rewardPerEpisode': 287.98849925581544
'totalSteps': 20480, 'rewardStep': 0.422238369751234, 'errorList': [], 'lossList': [0.0, -1.2120655077695845, 0.0, 4.071878219246864, 0.0, 0.0, 0.0], 'rewardMean': 0.6795599279115427, 'totalEpisodes': 154, 'stepsPerEpisode': 336, 'rewardPerEpisode': 249.44803783555054
'totalSteps': 21760, 'rewardStep': 0.6333365229219312, 'errorList': [], 'lossList': [0.0, -1.1939968520402908, 0.0, 4.33525936126709, 0.0, 0.0, 0.0], 'rewardMean': 0.6830640227181194, 'totalEpisodes': 156, 'stepsPerEpisode': 140, 'rewardPerEpisode': 122.38125782819745
'totalSteps': 23040, 'rewardStep': 0.527596112513144, 'errorList': [], 'lossList': [0.0, -1.184431260228157, 0.0, 2.2627639412879943, 0.0, 0.0, 0.0], 'rewardMean': 0.6775427559146932, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 854.6445708869969
'totalSteps': 24320, 'rewardStep': 0.518758086865847, 'errorList': [], 'lossList': [0.0, -1.1544112092256547, 0.0, 2.068892782330513, 0.0, 0.0, 0.0], 'rewardMean': 0.6723743263099549, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 724.4062096167913
'totalSteps': 25600, 'rewardStep': 0.7683626426695893, 'errorList': [], 'lossList': [0.0, -1.122038227915764, 0.0, 1.9119868242740632, 0.0, 0.0, 0.0], 'rewardMean': 0.6546214495718166, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 958.4651759608741
'totalSteps': 26880, 'rewardStep': 0.9385436992014042, 'errorList': [0.11234116237044428, 0.10103413167876382, 0.10139421068292236, 0.14599107269422945, 0.11637363383534985, 0.10100845555991894, 0.10109551376763971, 0.10688480085856496, 0.12947953228300033, 0.14564429969746626, 0.10097817212153369, 0.10361904288730084, 0.10147200923965505, 0.10095258889706295, 0.10170482890464058, 0.10063755460376611, 0.11301028716809461, 0.1017347440876224, 0.10077028211582746, 0.10113579087461325, 0.13087732635641489, 0.10095082233815504, 0.10418227230824903, 0.10135563771243618, 0.10241043682272667, 0.10115857739526231, 0.10095455135385431, 0.1008398867425675, 0.1006322914378914, 0.10361529675654021, 0.10544306206271971, 0.10065572082703862, 0.12103620742622315, 0.10144041535776833, 0.100640324170988, 0.10104001816405771, 0.1202785040420126, 0.10125903344625604, 0.10122698400301346, 0.10104638930085769, 0.10100633503592261, 0.10163016668586533, 0.1016726990769221, 0.10279004787279361, 0.1017284391242822, 0.1012476505915857, 0.10126102596138903, 0.10100464391818184, 0.10133429227787553, 0.10160923476650073], 'lossList': [0.0, -1.1021891433000564, 0.0, 1.2577657288312911, 0.0, 0.0, 0.0], 'rewardMean': 0.6730227054414331, 'totalEpisodes': 156, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1066.9622522614875, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=75.01
