#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.01, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8307793823522825, 'errorList': [], 'lossList': [0.0, -1.4134645575284959, 0.0, 56.15549911737442, 0.0, 0.0, 0.0], 'rewardMean': 0.8307793823522825, 'totalEpisodes': 13, 'stepsPerEpisode': 65, 'rewardPerEpisode': 51.051088777086996
'totalSteps': 2560, 'rewardStep': 0.49876515561920626, 'errorList': [], 'lossList': [0.0, -1.4152153873443603, 0.0, 31.159986953735352, 0.0, 0.0, 0.0], 'rewardMean': 0.6647722689857444, 'totalEpisodes': 38, 'stepsPerEpisode': 120, 'rewardPerEpisode': 77.6822214317707
'totalSteps': 3840, 'rewardStep': 0.7652550574072591, 'errorList': [], 'lossList': [0.0, -1.4078667467832566, 0.0, 20.313507990837095, 0.0, 0.0, 0.0], 'rewardMean': 0.6982665317929159, 'totalEpisodes': 54, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.79018325892151
'totalSteps': 5120, 'rewardStep': 0.4122684333169764, 'errorList': [], 'lossList': [0.0, -1.3983968609571458, 0.0, 23.291500322818756, 0.0, 0.0, 0.0], 'rewardMean': 0.6267670071739311, 'totalEpisodes': 65, 'stepsPerEpisode': 67, 'rewardPerEpisode': 40.80687183772658
'totalSteps': 6400, 'rewardStep': 0.15286913514477518, 'errorList': [], 'lossList': [0.0, -1.3887487995624541, 0.0, 37.99074848175049, 0.0, 0.0, 0.0], 'rewardMean': 0.5319874327680999, 'totalEpisodes': 69, 'stepsPerEpisode': 184, 'rewardPerEpisode': 135.29197865833376
'totalSteps': 7680, 'rewardStep': 0.734881827118186, 'errorList': [], 'lossList': [0.0, -1.3644353049993514, 0.0, 70.04630164146424, 0.0, 0.0, 0.0], 'rewardMean': 0.5658031651597809, 'totalEpisodes': 77, 'stepsPerEpisode': 56, 'rewardPerEpisode': 37.778471317566556
'totalSteps': 8960, 'rewardStep': 0.570763296808271, 'errorList': [], 'lossList': [0.0, -1.3438684523105622, 0.0, 126.28253154754638, 0.0, 0.0, 0.0], 'rewardMean': 0.5665117553952795, 'totalEpisodes': 91, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.135729927598556
'totalSteps': 10240, 'rewardStep': 0.7398477636298162, 'errorList': [], 'lossList': [0.0, -1.3333553487062455, 0.0, 76.96548274993897, 0.0, 0.0, 0.0], 'rewardMean': 0.5881787564245966, 'totalEpisodes': 108, 'stepsPerEpisode': 114, 'rewardPerEpisode': 100.20454052028765
'totalSteps': 11520, 'rewardStep': 0.5780681225184476, 'errorList': [], 'lossList': [0.0, -1.3368537145853043, 0.0, 29.435573840141295, 0.0, 0.0, 0.0], 'rewardMean': 0.5870553526572467, 'totalEpisodes': 117, 'stepsPerEpisode': 108, 'rewardPerEpisode': 79.90366498428541
'totalSteps': 12800, 'rewardStep': 0.4444363186348726, 'errorList': [], 'lossList': [0.0, -1.3349795508384705, 0.0, 41.31357026576996, 0.0, 0.0, 0.0], 'rewardMean': 0.5727934492550093, 'totalEpisodes': 124, 'stepsPerEpisode': 142, 'rewardPerEpisode': 92.796357967501
'totalSteps': 14080, 'rewardStep': 0.5930115979183446, 'errorList': [], 'lossList': [0.0, -1.3287195265293121, 0.0, 8.947496485710143, 0.0, 0.0, 0.0], 'rewardMean': 0.5490166708116154, 'totalEpisodes': 128, 'stepsPerEpisode': 161, 'rewardPerEpisode': 98.09738579961103
'totalSteps': 15360, 'rewardStep': 0.942419492879906, 'errorList': [1.2771888232519568, 0.9178656366775323, 1.9981899125542149, 1.1919884579805144, 0.9023497565236454, 1.6069028653685478, 3.0652020348236704, 1.8648079245260845, 1.488503189586894, 2.8927395306638766, 2.6674765468012662, 1.7167672805141907, 1.1924953283477016, 2.279884064199778, 0.612383618558038, 2.144286319275427, 1.0775878794808444, 2.554983032673459, 1.6083329812835019, 1.3053226550523274, 0.9777571841094971, 0.871052622723082, 1.1266919647991198, 1.9672608378402412, 0.9733405770354009, 1.6286985890819536, 3.4007166384868963, 1.0382465592791739, 1.9434997850458906, 4.102052928851567, 1.9309598150697935, 1.7908831050281562, 1.0987415588256029, 3.22060453049656, 2.019904067154566, 1.171272534502603, 1.438421442173415, 2.327005317935223, 1.3922424584737287, 2.8549293997564975, 1.8554386342588542, 0.9073912200131885, 1.5464122261777633, 2.111887310893207, 0.9965962882099125, 1.7906248981001192, 1.2557251713088864, 1.7765112754671462, 1.4889994097381198, 0.7094063661025386], 'lossList': [0.0, -1.3204896718263626, 0.0, 8.665088852643967, 0.0, 0.0, 0.0], 'rewardMean': 0.5933821045376855, 'totalEpisodes': 132, 'stepsPerEpisode': 342, 'rewardPerEpisode': 247.4862204721534, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.8283077860844802, 'errorList': [], 'lossList': [0.0, -1.3260517966747285, 0.0, 6.1313523238897325, 0.0, 0.0, 0.0], 'rewardMean': 0.5996873774054075, 'totalEpisodes': 133, 'stepsPerEpisode': 146, 'rewardPerEpisode': 123.31753193212778
'totalSteps': 17920, 'rewardStep': 0.77672716866161, 'errorList': [], 'lossList': [0.0, -1.310004667043686, 0.0, 4.3172446894645695, 0.0, 0.0, 0.0], 'rewardMean': 0.6361332509398709, 'totalEpisodes': 133, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 953.183000975975
'totalSteps': 19200, 'rewardStep': 0.9584362622582998, 'errorList': [0.09034586266080594, 0.08625306825886521, 0.11177613715794149, 0.0947613286913893, 0.08684866869266165, 0.08753837380769494, 0.11067070225557965, 0.10033754490771171, 0.09287736137703867, 0.08149771399554184, 0.09346591314280042, 0.08853055834992614, 0.08216822631855107, 0.08311620640580486, 0.08107741565579267, 0.08138092310285742, 0.11515238078608547, 0.10863254057254486, 0.08772055309962724, 0.08715948433764703, 0.0875411886968032, 0.08835889865673406, 0.1125177379760696, 0.08653452855752969, 0.08561722029102585, 0.07731691751252062, 0.08256781114254276, 0.08133927188984844, 0.08205443209488168, 0.08681002464818176, 0.07779823644080026, 0.10193989756871334, 0.07548666890156813, 0.09881651052833267, 0.09293507160534353, 0.09828844476484805, 0.07394101260649012, 0.1067040812055519, 0.0845219517096103, 0.08589395633525944, 0.08602385714881733, 0.08947239437798958, 0.0901217997978332, 0.10298064334285711, 0.09297246323751443, 0.08494734730460958, 0.08798106871962678, 0.08022980134666398, 0.07650216415680872, 0.08969658575965603], 'lossList': [0.0, -1.2661161839962005, 0.0, 14.202287009954453, 0.0, 0.0, 0.0], 'rewardMean': 0.7166899636512234, 'totalEpisodes': 134, 'stepsPerEpisode': 756, 'rewardPerEpisode': 654.5987364656177, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=64.69
