#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.01, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.3301610586060858, 'errorList': [], 'lossList': [0.0, -1.3985191583633423, 0.0, 60.5836457157135, 0.0, 0.0, 0.0], 'rewardMean': 0.3301610586060858, 'totalEpisodes': 11, 'stepsPerEpisode': 114, 'rewardPerEpisode': 76.68272768121885
'totalSteps': 2560, 'rewardStep': 0.7354915233137076, 'errorList': [], 'lossList': [0.0, -1.3750787246227265, 0.0, 30.16163613319397, 0.0, 0.0, 0.0], 'rewardMean': 0.5328262909598966, 'totalEpisodes': 27, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.207802869339897
'totalSteps': 3840, 'rewardStep': 0.8350231188689226, 'errorList': [], 'lossList': [0.0, -1.351456440091133, 0.0, 30.34837670326233, 0.0, 0.0, 0.0], 'rewardMean': 0.6335585669295719, 'totalEpisodes': 37, 'stepsPerEpisode': 226, 'rewardPerEpisode': 161.9884979178143
'totalSteps': 5120, 'rewardStep': 0.6412158460313513, 'errorList': [], 'lossList': [0.0, -1.326160967350006, 0.0, 21.055702986717225, 0.0, 0.0, 0.0], 'rewardMean': 0.6354728867050168, 'totalEpisodes': 38, 'stepsPerEpisode': 181, 'rewardPerEpisode': 143.3414747961123
'totalSteps': 6400, 'rewardStep': 0.56005818040052, 'errorList': [], 'lossList': [0.0, -1.3076259785890578, 0.0, 36.1259157037735, 0.0, 0.0, 0.0], 'rewardMean': 0.6203899454441174, 'totalEpisodes': 41, 'stepsPerEpisode': 742, 'rewardPerEpisode': 548.4827315315308
'totalSteps': 7680, 'rewardStep': 0.8813516171872533, 'errorList': [], 'lossList': [0.0, -1.296914974451065, 0.0, 37.336029899120334, 0.0, 0.0, 0.0], 'rewardMean': 0.6638835574013068, 'totalEpisodes': 44, 'stepsPerEpisode': 57, 'rewardPerEpisode': 41.29032489201683
'totalSteps': 8960, 'rewardStep': 0.8307478948447127, 'errorList': [], 'lossList': [0.0, -1.2880155795812607, 0.0, 183.8600507736206, 0.0, 0.0, 0.0], 'rewardMean': 0.6877213198932218, 'totalEpisodes': 64, 'stepsPerEpisode': 104, 'rewardPerEpisode': 81.72038435612762
'totalSteps': 10240, 'rewardStep': 0.8063809157388782, 'errorList': [], 'lossList': [0.0, -1.2864320373535156, 0.0, 138.68667493820192, 0.0, 0.0, 0.0], 'rewardMean': 0.7025537693739289, 'totalEpisodes': 92, 'stepsPerEpisode': 63, 'rewardPerEpisode': 51.60643175861336
'totalSteps': 11520, 'rewardStep': 0.8272487087524651, 'errorList': [], 'lossList': [0.0, -1.2898915457725524, 0.0, 46.25880429267883, 0.0, 0.0, 0.0], 'rewardMean': 0.7164087626382106, 'totalEpisodes': 107, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.99913288552809
'totalSteps': 12800, 'rewardStep': 0.5774584142990031, 'errorList': [], 'lossList': [0.0, -1.2908990645408631, 0.0, 20.58741468191147, 0.0, 0.0, 0.0], 'rewardMean': 0.7025137278042899, 'totalEpisodes': 114, 'stepsPerEpisode': 35, 'rewardPerEpisode': 26.342322647801147
'totalSteps': 14080, 'rewardStep': 0.815234348411406, 'errorList': [], 'lossList': [0.0, -1.275498520731926, 0.0, 17.62286595106125, 0.0, 0.0, 0.0], 'rewardMean': 0.751021056784822, 'totalEpisodes': 118, 'stepsPerEpisode': 177, 'rewardPerEpisode': 135.08801995653124
'totalSteps': 15360, 'rewardStep': 0.6531138847469649, 'errorList': [], 'lossList': [0.0, -1.265127745270729, 0.0, 22.06075041770935, 0.0, 0.0, 0.0], 'rewardMean': 0.7427832929281476, 'totalEpisodes': 120, 'stepsPerEpisode': 510, 'rewardPerEpisode': 409.97101165674513
'totalSteps': 16640, 'rewardStep': 0.766827357210556, 'errorList': [], 'lossList': [0.0, -1.2547728204727173, 0.0, 6.441845856308937, 0.0, 0.0, 0.0], 'rewardMean': 0.735963716762311, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.0730807933392
'totalSteps': 17920, 'rewardStep': 0.8495474950837407, 'errorList': [], 'lossList': [0.0, -1.237988536953926, 0.0, 5.766957921236753, 0.0, 0.0, 0.0], 'rewardMean': 0.7567968816675499, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1086.0673073123914
'totalSteps': 19200, 'rewardStep': 0.9078572946345252, 'errorList': [], 'lossList': [0.0, -1.2132200181484223, 0.0, 5.351384821981192, 0.0, 0.0, 0.0], 'rewardMean': 0.7915767930909505, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.9742128962566
'totalSteps': 20480, 'rewardStep': 0.9798170550192639, 'errorList': [0.05345290299296652, 0.04425105871546109, 0.015102530869328985, 0.04252216983657862, 0.016378926004979835, 0.03650153616533018, 0.04008362130693837, 0.06459072405769216, 0.06606725750987208, 0.012690399533311351, 0.06682198697418965, 0.048463024349527, 0.04696760998202469, 0.03482702499730182, 0.03413221192104392, 0.021012363299285186, 0.07695098074723167, 0.031251459607604046, 0.01082334046432366, 0.06671318240420313, 0.024184073068397583, 0.05101921394748273, 0.04130059915660285, 0.035232875467000116, 0.07595352960014551, 0.03944445380619334, 0.025085693079203126, 0.06257448439597299, 0.04890661844299788, 0.04014067029096922, 0.01309028790880185, 0.025879585490825363, 0.039139638379150195, 0.023036426882466425, 0.058041089771824005, 0.030083257397986497, 0.035175119755896246, 0.02415923381331378, 0.0552441634913671, 0.026270296781671768, 0.014387996652365271, 0.05705318761516722, 0.021638669183975695, 0.04901546034147698, 0.02532962594093543, 0.05948267707475962, 0.02629567750934528, 0.07077919855842817, 0.024870296043416274, 0.04225414186444521], 'lossList': [0.0, -1.1810816675424576, 0.0, 3.8941027978807687, 0.0, 0.0, 0.0], 'rewardMean': 0.8014233368741517, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.6153688653608, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=50.46
