#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v11_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.01, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.752852757982735, 'errorList': [], 'lossList': [0.0, -1.4099466925859452, 0.0, 59.88306597709656, 0.0, 0.0, 0.0], 'rewardMean': 0.752852757982735, 'totalEpisodes': 15, 'stepsPerEpisode': 145, 'rewardPerEpisode': 109.94264150889477
'totalSteps': 2560, 'rewardStep': 0.854345955473827, 'errorList': [], 'lossList': [0.0, -1.3997123795747757, 0.0, 26.301577515602112, 0.0, 0.0, 0.0], 'rewardMean': 0.803599356728281, 'totalEpisodes': 35, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.704152081466884
'totalSteps': 3840, 'rewardStep': 0.6833147377988361, 'errorList': [], 'lossList': [0.0, -1.385900889635086, 0.0, 34.821974754333496, 0.0, 0.0, 0.0], 'rewardMean': 0.7635044837517994, 'totalEpisodes': 54, 'stepsPerEpisode': 28, 'rewardPerEpisode': 21.939331748102042
'totalSteps': 5120, 'rewardStep': 0.5600995550388363, 'errorList': [], 'lossList': [0.0, -1.3598346090316773, 0.0, 20.955151052474974, 0.0, 0.0, 0.0], 'rewardMean': 0.7126532515735586, 'totalEpisodes': 58, 'stepsPerEpisode': 237, 'rewardPerEpisode': 187.80277698498085
'totalSteps': 6400, 'rewardStep': 0.6293783760886538, 'errorList': [], 'lossList': [0.0, -1.3455849254131318, 0.0, 26.611456294059753, 0.0, 0.0, 0.0], 'rewardMean': 0.6959982764765776, 'totalEpisodes': 60, 'stepsPerEpisode': 867, 'rewardPerEpisode': 641.060209976313
'totalSteps': 7680, 'rewardStep': 0.6956271423539152, 'errorList': [], 'lossList': [0.0, -1.3319185918569565, 0.0, 20.42855618238449, 0.0, 0.0, 0.0], 'rewardMean': 0.6959364207894673, 'totalEpisodes': 61, 'stepsPerEpisode': 428, 'rewardPerEpisode': 294.29650616165213
'totalSteps': 8960, 'rewardStep': 0.6860417322568118, 'errorList': [], 'lossList': [0.0, -1.305187047123909, 0.0, 152.88010948181153, 0.0, 0.0, 0.0], 'rewardMean': 0.6945228938562308, 'totalEpisodes': 78, 'stepsPerEpisode': 32, 'rewardPerEpisode': 26.16181724343632
'totalSteps': 10240, 'rewardStep': 0.780259195508187, 'errorList': [], 'lossList': [0.0, -1.299800164103508, 0.0, 114.46857170104981, 0.0, 0.0, 0.0], 'rewardMean': 0.7052399315627252, 'totalEpisodes': 100, 'stepsPerEpisode': 57, 'rewardPerEpisode': 48.74014180701272
'totalSteps': 11520, 'rewardStep': 0.9121788010140893, 'errorList': [], 'lossList': [0.0, -1.3009100401401519, 0.0, 68.60250276565552, 0.0, 0.0, 0.0], 'rewardMean': 0.7282331392795435, 'totalEpisodes': 124, 'stepsPerEpisode': 118, 'rewardPerEpisode': 100.42042905777117
'totalSteps': 12800, 'rewardStep': 0.6432321631094526, 'errorList': [], 'lossList': [0.0, -1.2976024639606476, 0.0, 20.98265187263489, 0.0, 0.0, 0.0], 'rewardMean': 0.7197330416625344, 'totalEpisodes': 133, 'stepsPerEpisode': 45, 'rewardPerEpisode': 28.22391750619385
'totalSteps': 14080, 'rewardStep': 0.16008387450642758, 'errorList': [], 'lossList': [0.0, -1.278346140384674, 0.0, 10.326780896186829, 0.0, 0.0, 0.0], 'rewardMean': 0.6604561533149036, 'totalEpisodes': 142, 'stepsPerEpisode': 144, 'rewardPerEpisode': 91.10137813309771
'totalSteps': 15360, 'rewardStep': 0.8471138395948287, 'errorList': [], 'lossList': [0.0, -1.2642252135276795, 0.0, 15.467749450206757, 0.0, 0.0, 0.0], 'rewardMean': 0.6597329417270037, 'totalEpisodes': 147, 'stepsPerEpisode': 464, 'rewardPerEpisode': 407.6136031291326
'totalSteps': 16640, 'rewardStep': 0.6608709904969862, 'errorList': [], 'lossList': [0.0, -1.257603566646576, 0.0, 8.869571452140809, 0.0, 0.0, 0.0], 'rewardMean': 0.6574885669968188, 'totalEpisodes': 151, 'stepsPerEpisode': 39, 'rewardPerEpisode': 31.49414027767791
'totalSteps': 17920, 'rewardStep': 0.6970916629916644, 'errorList': [], 'lossList': [0.0, -1.2437636423110963, 0.0, 12.359223185777664, 0.0, 0.0, 0.0], 'rewardMean': 0.6711877777921016, 'totalEpisodes': 155, 'stepsPerEpisode': 95, 'rewardPerEpisode': 67.67644167039079
'totalSteps': 19200, 'rewardStep': 0.8851914915722613, 'errorList': [], 'lossList': [0.0, -1.2247086095809936, 0.0, 4.779877058267593, 0.0, 0.0, 0.0], 'rewardMean': 0.6967690893404623, 'totalEpisodes': 155, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1010.6191853471831
'totalSteps': 20480, 'rewardStep': 0.9594225000912616, 'errorList': [0.05262275955912863, 0.05270522577816696, 0.05266251586281139, 0.05266134708408781, 0.05261617909171663, 0.05274108919000784, 0.07761940831945477, 0.1018446848829657, 0.052692640242484774, 0.05260165319449589, 0.06196614186033149, 0.08819051600596996, 0.0527501490513885, 0.0679588647068026, 0.052674086640221995, 0.0771348113062621, 0.06988185491060477, 0.05270799570059536, 0.08225505693516658, 0.052725041612098664, 0.052691736604509175, 0.05277919470714376, 0.05268303357924385, 0.052703639836536594, 0.05263482272909707, 0.05265494047465796, 0.05929441197936517, 0.0527522972432395, 0.07228880025615783, 0.09350342614209206, 0.052670695377060546, 0.05274581294548929, 0.07215643592737393, 0.05268887117290694, 0.05266730766924964, 0.05266882834432892, 0.052673924351464806, 0.05301025009876756, 0.052591478425598, 0.0527345984724335, 0.05263114080385276, 0.052728309294867236, 0.05267997203971951, 0.058669480209084066, 0.10193437053414323, 0.052645129954691165, 0.05266955256199928, 0.052715873839583725, 0.08722282794996505, 0.05267401906616222], 'lossList': [0.0, -1.1919906163215637, 0.0, 4.1472978512942795, 0.0, 0.0, 0.0], 'rewardMean': 0.723148625114197, 'totalEpisodes': 155, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.567809587687, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=50.88
