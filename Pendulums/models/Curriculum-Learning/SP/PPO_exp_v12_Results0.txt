#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v12_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v12_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9221407132001338, 'errorList': [], 'lossList': [0.0, -1.4099008005857467, 0.0, 60.66497577190399, 0.0, 0.0, 0.0], 'rewardMean': 0.9221407132001338, 'totalEpisodes': 13, 'stepsPerEpisode': 157, 'rewardPerEpisode': 119.09374324228895
'totalSteps': 2560, 'rewardStep': 0.8305064941756306, 'errorList': [], 'lossList': [0.0, -1.396156912446022, 0.0, 34.088755502700806, 0.0, 0.0, 0.0], 'rewardMean': 0.8763236036878822, 'totalEpisodes': 37, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.74241862280199
'totalSteps': 3840, 'rewardStep': 0.3449903973038709, 'errorList': [], 'lossList': [0.0, -1.3760164600610734, 0.0, 29.331429958343506, 0.0, 0.0, 0.0], 'rewardMean': 0.6992125348932118, 'totalEpisodes': 52, 'stepsPerEpisode': 48, 'rewardPerEpisode': 31.911132455091604
'totalSteps': 5120, 'rewardStep': 0.6825573406521979, 'errorList': [], 'lossList': [0.0, -1.365416680574417, 0.0, 20.241888525485994, 0.0, 0.0, 0.0], 'rewardMean': 0.6950487363329583, 'totalEpisodes': 59, 'stepsPerEpisode': 272, 'rewardPerEpisode': 170.49797790235837
'totalSteps': 6400, 'rewardStep': 0.8554589843870405, 'errorList': [], 'lossList': [0.0, -1.3677111250162124, 0.0, 23.753720024824144, 0.0, 0.0, 0.0], 'rewardMean': 0.7271307859437747, 'totalEpisodes': 61, 'stepsPerEpisode': 208, 'rewardPerEpisode': 176.0010928096695
'totalSteps': 7680, 'rewardStep': 0.8327213648402912, 'errorList': [], 'lossList': [0.0, -1.3574334383010864, 0.0, 27.575879459381103, 0.0, 0.0, 0.0], 'rewardMean': 0.7447292157598607, 'totalEpisodes': 63, 'stepsPerEpisode': 295, 'rewardPerEpisode': 227.98301380913747
'totalSteps': 8960, 'rewardStep': 0.8543710734667816, 'errorList': [], 'lossList': [0.0, -1.3241741609573365, 0.0, 154.63407669067382, 0.0, 0.0, 0.0], 'rewardMean': 0.7603923382894209, 'totalEpisodes': 77, 'stepsPerEpisode': 96, 'rewardPerEpisode': 80.10230851204697
'totalSteps': 10240, 'rewardStep': 0.5039242381230619, 'errorList': [], 'lossList': [0.0, -1.3111369502544403, 0.0, 164.39432693481444, 0.0, 0.0, 0.0], 'rewardMean': 0.728333825768626, 'totalEpisodes': 103, 'stepsPerEpisode': 36, 'rewardPerEpisode': 26.122448535778958
'totalSteps': 11520, 'rewardStep': 0.465052256634704, 'errorList': [], 'lossList': [0.0, -1.2983982080221177, 0.0, 79.00812280654907, 0.0, 0.0, 0.0], 'rewardMean': 0.6990803180870792, 'totalEpisodes': 121, 'stepsPerEpisode': 68, 'rewardPerEpisode': 39.99688376367309
'totalSteps': 12800, 'rewardStep': 0.4660647070206981, 'errorList': [], 'lossList': [0.0, -1.2729199784994125, 0.0, 35.48084679603576, 0.0, 0.0, 0.0], 'rewardMean': 0.675778756980441, 'totalEpisodes': 132, 'stepsPerEpisode': 122, 'rewardPerEpisode': 81.34933312573769
'totalSteps': 14080, 'rewardStep': 0.16642503710828227, 'errorList': [], 'lossList': [0.0, -1.2572631275653838, 0.0, 11.993501663208008, 0.0, 0.0, 0.0], 'rewardMean': 0.600207189371256, 'totalEpisodes': 136, 'stepsPerEpisode': 566, 'rewardPerEpisode': 392.91867455428564
'totalSteps': 15360, 'rewardStep': 0.7180413943880825, 'errorList': [], 'lossList': [0.0, -1.2492512136697769, 0.0, 31.56888316631317, 0.0, 0.0, 0.0], 'rewardMean': 0.5889606793925011, 'totalEpisodes': 143, 'stepsPerEpisode': 68, 'rewardPerEpisode': 56.91443231458987
'totalSteps': 16640, 'rewardStep': 0.9071361840625777, 'errorList': [], 'lossList': [0.0, -1.2596186381578445, 0.0, 6.594392876029015, 0.0, 0.0, 0.0], 'rewardMean': 0.6451752580683718, 'totalEpisodes': 144, 'stepsPerEpisode': 335, 'rewardPerEpisode': 264.407432124874
'totalSteps': 17920, 'rewardStep': 0.891196118397442, 'errorList': [], 'lossList': [0.0, -1.2547672235965728, 0.0, 5.2665879529714585, 0.0, 0.0, 0.0], 'rewardMean': 0.6660391358428962, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1029.2638343743747
'totalSteps': 19200, 'rewardStep': 0.8946344161745508, 'errorList': [], 'lossList': [0.0, -1.2135406452417374, 0.0, 4.938302851319313, 0.0, 0.0, 0.0], 'rewardMean': 0.6699566790216472, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1108.6565852025587
'totalSteps': 20480, 'rewardStep': 0.9408594890556914, 'errorList': [0.097727662397334, 0.09645943885438069, 0.11223526250984105, 0.13762502269231155, 0.12593293923956111, 0.09467910670959556, 0.1293761381596113, 0.10832492955511079, 0.09076464416462834, 0.13115545315283494, 0.14882081875271896, 0.10332312938532932, 0.11867419348660915, 0.21608369822361692, 0.1487467171617063, 0.13715830233177997, 0.06680491239836141, 0.08605781699461168, 0.1143427673065714, 0.10456566437990804, 0.2189452940497497, 0.14102196204939788, 0.11022626943106252, 0.12083312515733646, 0.07799119148455175, 0.1590450255195453, 0.1402582809593206, 0.09350572223430721, 0.09902649489079399, 0.11172844145232112, 0.1472945034669324, 0.13714157066459756, 0.1599552784034248, 0.09972073993143478, 0.1037990056014651, 0.10964705026938958, 0.131873269770009, 0.12477720220194417, 0.1776541331100291, 0.10597469079163543, 0.1164634179034793, 0.08964559121960423, 0.13955774188277897, 0.1457578970577254, 0.18810263264678578, 0.12851352030288032, 0.10885834633110518, 0.11011140794345252, 0.12162360229561209, 0.08701602820571777], 'lossList': [0.0, -1.2033790469169616, 0.0, 3.780396386012435, 0.0, 0.0, 0.0], 'rewardMean': 0.6807704914431871, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1142.5836381402669, 'successfulTests': 48
'totalSteps': 21760, 'rewardStep': 0.9126185128876685, 'errorList': [], 'lossList': [0.0, -1.1898047149181366, 0.0, 2.739425711557269, 0.0, 0.0, 0.0], 'rewardMean': 0.6865952353852759, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.3223293288452
'totalSteps': 23040, 'rewardStep': 0.8899553066922448, 'errorList': [], 'lossList': [0.0, -1.1736771816015243, 0.0, 1.9265895804949105, 0.0, 0.0, 0.0], 'rewardMean': 0.7251983422421941, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.5464447764484
'totalSteps': 24320, 'rewardStep': 0.8025644878686, 'errorList': [], 'lossList': [0.0, -1.1309132981300354, 0.0, 1.2061468365229666, 0.0, 0.0, 0.0], 'rewardMean': 0.7589495653655838, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.9338182774281
'totalSteps': 25600, 'rewardStep': 0.9559815012695345, 'errorList': [0.07511082115263733, 0.04457855109939547, 0.026539328406422587, 0.025769134663188874, 0.03339336973828052, 0.08889874992606163, 0.06004716877132596, 0.0709726052668752, 0.027276381103288037, 0.03342794818356014, 0.028483631804616768, 0.07280073564229389, 0.03171019862915101, 0.02817357548435224, 0.027026269129614842, 0.03000127543162961, 0.06828864193989227, 0.022030061074763446, 0.06103491680660957, 0.015809385435544596, 0.023407619409241704, 0.012276121957377569, 0.011029206588566712, 0.02496379519861215, 0.07428439638386328, 0.045420956389400996, 0.060692247262667094, 0.032855404267242684, 0.061672608837447566, 0.012990012639525797, 0.037516481035315435, 0.01528349114191058, 0.03240807513141462, 0.04438184737028393, 0.040477496983882096, 0.024282571663916338, 0.08273607589642977, 0.03554220261070858, 0.012647493119450444, 0.02755715289498115, 0.03507902708272258, 0.017602640108643694, 0.036969719186466785, 0.030805054368170156, 0.021754056192435353, 0.008474560516586656, 0.0229863618273931, 0.0424155962115552, 0.03763340159542678, 0.03944249007168631], 'lossList': [0.0, -1.0964346218109131, 0.0, 1.0508560610935092, 0.0, 0.0, 0.0], 'rewardMean': 0.8079412447904675, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1198.7796279442011, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=76.75
