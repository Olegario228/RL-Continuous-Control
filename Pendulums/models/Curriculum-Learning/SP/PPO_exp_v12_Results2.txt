#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v12_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v12_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7768147489361216, 'errorList': [], 'lossList': [0.0, -1.4041537922620773, 0.0, 59.33716954231262, 0.0, 0.0, 0.0], 'rewardMean': 0.7768147489361216, 'totalEpisodes': 18, 'stepsPerEpisode': 67, 'rewardPerEpisode': 57.355280730858674
'totalSteps': 2560, 'rewardStep': 0.5180554558881527, 'errorList': [], 'lossList': [0.0, -1.3836256736516952, 0.0, 31.5879660320282, 0.0, 0.0, 0.0], 'rewardMean': 0.6474351024121372, 'totalEpisodes': 42, 'stepsPerEpisode': 121, 'rewardPerEpisode': 74.53251731639033
'totalSteps': 3840, 'rewardStep': 0.9601477165796507, 'errorList': [], 'lossList': [0.0, -1.3711935710906982, 0.0, 30.525410337448122, 0.0, 0.0, 0.0], 'rewardMean': 0.7516726404679751, 'totalEpisodes': 58, 'stepsPerEpisode': 32, 'rewardPerEpisode': 22.962344884207965
'totalSteps': 5120, 'rewardStep': 0.4258609846505328, 'errorList': [], 'lossList': [0.0, -1.3568704760074615, 0.0, 18.272914204597473, 0.0, 0.0, 0.0], 'rewardMean': 0.6702197265136145, 'totalEpisodes': 67, 'stepsPerEpisode': 257, 'rewardPerEpisode': 153.3192628566809
'totalSteps': 6400, 'rewardStep': 0.23433168541268917, 'errorList': [], 'lossList': [0.0, -1.3455981397628785, 0.0, 38.981056156158445, 0.0, 0.0, 0.0], 'rewardMean': 0.5830421182934294, 'totalEpisodes': 74, 'stepsPerEpisode': 251, 'rewardPerEpisode': 173.65506415687972
'totalSteps': 7680, 'rewardStep': 0.6621053740429714, 'errorList': [], 'lossList': [0.0, -1.3187185060977935, 0.0, 88.97588665008544, 0.0, 0.0, 0.0], 'rewardMean': 0.5962193275850197, 'totalEpisodes': 85, 'stepsPerEpisode': 57, 'rewardPerEpisode': 44.75756133767132
'totalSteps': 8960, 'rewardStep': 0.8063698120738355, 'errorList': [], 'lossList': [0.0, -1.3018601113557815, 0.0, 126.16511959075928, 0.0, 0.0, 0.0], 'rewardMean': 0.6262408253691364, 'totalEpisodes': 101, 'stepsPerEpisode': 14, 'rewardPerEpisode': 11.70808154161772
'totalSteps': 10240, 'rewardStep': 0.7261562896777013, 'errorList': [], 'lossList': [0.0, -1.2986740070581435, 0.0, 61.25808738708496, 0.0, 0.0, 0.0], 'rewardMean': 0.6387302584077069, 'totalEpisodes': 111, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.033263882000432
'totalSteps': 11520, 'rewardStep': 0.47914146902047755, 'errorList': [], 'lossList': [0.0, -1.2985239309072494, 0.0, 46.57216825008393, 0.0, 0.0, 0.0], 'rewardMean': 0.6209981706980148, 'totalEpisodes': 123, 'stepsPerEpisode': 106, 'rewardPerEpisode': 71.40012966306506
'totalSteps': 12800, 'rewardStep': 0.7843687559605448, 'errorList': [], 'lossList': [0.0, -1.2956726551055908, 0.0, 22.261243164539337, 0.0, 0.0, 0.0], 'rewardMean': 0.6373352292242678, 'totalEpisodes': 129, 'stepsPerEpisode': 143, 'rewardPerEpisode': 106.28791582824418
'totalSteps': 14080, 'rewardStep': 0.6230822317363826, 'errorList': [], 'lossList': [0.0, -1.31021621465683, 0.0, 6.151316499710083, 0.0, 0.0, 0.0], 'rewardMean': 0.6219619775042938, 'totalEpisodes': 135, 'stepsPerEpisode': 95, 'rewardPerEpisode': 72.21121966220831
'totalSteps': 15360, 'rewardStep': 0.7412690209563454, 'errorList': [], 'lossList': [0.0, -1.3222293877601623, 0.0, 19.270095431804656, 0.0, 0.0, 0.0], 'rewardMean': 0.644283334011113, 'totalEpisodes': 142, 'stepsPerEpisode': 118, 'rewardPerEpisode': 88.57535186846631
'totalSteps': 16640, 'rewardStep': 0.9019930646828551, 'errorList': [], 'lossList': [0.0, -1.3081762653589248, 0.0, 5.3735782122612, 0.0, 0.0, 0.0], 'rewardMean': 0.6384678688214336, 'totalEpisodes': 147, 'stepsPerEpisode': 270, 'rewardPerEpisode': 226.7417434885827
'totalSteps': 17920, 'rewardStep': 0.4017908685965164, 'errorList': [], 'lossList': [0.0, -1.2927493864297868, 0.0, 5.939606603384018, 0.0, 0.0, 0.0], 'rewardMean': 0.6360608572160319, 'totalEpisodes': 149, 'stepsPerEpisode': 277, 'rewardPerEpisode': 199.88896649366274
'totalSteps': 19200, 'rewardStep': 0.9195001823378923, 'errorList': [], 'lossList': [0.0, -1.2731887048482895, 0.0, 30.18499518275261, 0.0, 0.0, 0.0], 'rewardMean': 0.7045777069085523, 'totalEpisodes': 152, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.6355562993288135
'totalSteps': 20480, 'rewardStep': 0.8055825213592888, 'errorList': [], 'lossList': [0.0, -1.2480188697576522, 0.0, 3.430131804347038, 0.0, 0.0, 0.0], 'rewardMean': 0.7189254216401839, 'totalEpisodes': 154, 'stepsPerEpisode': 380, 'rewardPerEpisode': 330.61636438272114
'totalSteps': 21760, 'rewardStep': 0.7977813804740418, 'errorList': [], 'lossList': [0.0, -1.2038293981552124, 0.0, 1.2473053647577763, 0.0, 0.0, 0.0], 'rewardMean': 0.7180665784802046, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1089.1875535575684
'totalSteps': 23040, 'rewardStep': 0.9359794440447838, 'errorList': [0.03454050087209299, 0.037230528923166625, 0.041941821601068865, 0.08884395769003688, 0.06576118175533407, 0.08400103193761826, 0.054511496226265346, 0.11352959272169007, 0.06322286422083397, 0.04250536917196456, 0.0677962202717768, 0.07700885086469199, 0.05306490553626624, 0.051732974286766625, 0.06760480719582904, 0.06682250818234077, 0.05605085066343101, 0.07888368014560886, 0.03422910321750036, 0.03979671952810876, 0.07145317540955927, 0.04897708948625349, 0.10861761131525668, 0.07619652559231108, 0.036934564664126666, 0.035195055846371184, 0.07145226561742157, 0.054852465789788384, 0.06951101238461699, 0.057687670959359304, 0.07424804930756536, 0.06294564819861632, 0.06701875205081648, 0.07604539270414538, 0.035151907773067786, 0.0784094963581579, 0.044308218837205926, 0.046212934514826054, 0.0705438696917502, 0.049712982083377134, 0.04456500560252494, 0.036464230980584426, 0.054813602831286654, 0.07606844183046829, 0.06208837514663965, 0.0716751332050977, 0.09789114860302556, 0.11038197059699424, 0.03670350951638729, 0.04791247358680865], 'lossList': [0.0, -1.165318002104759, 0.0, 1.143537363037467, 0.0, 0.0, 0.0], 'rewardMean': 0.7390488939169129, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1146.8102243003102, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=56.51
