#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v12_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v12_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9730208541167058, 'errorList': [], 'lossList': [0.0, -1.410806321501732, 0.0, 60.35728541374206, 0.0, 0.0, 0.0], 'rewardMean': 0.9730208541167058, 'totalEpisodes': 18, 'stepsPerEpisode': 71, 'rewardPerEpisode': 60.04842867581878
'totalSteps': 2560, 'rewardStep': 0.6333001781711488, 'errorList': [], 'lossList': [0.0, -1.3995739763975144, 0.0, 28.854497346878052, 0.0, 0.0, 0.0], 'rewardMean': 0.8031605161439272, 'totalEpisodes': 37, 'stepsPerEpisode': 15, 'rewardPerEpisode': 13.358660330157806
'totalSteps': 3840, 'rewardStep': 0.6951345984359647, 'errorList': [], 'lossList': [0.0, -1.3842171984910965, 0.0, 28.192330911159516, 0.0, 0.0, 0.0], 'rewardMean': 0.7671518769079397, 'totalEpisodes': 45, 'stepsPerEpisode': 289, 'rewardPerEpisode': 208.87471559062078
'totalSteps': 5120, 'rewardStep': 0.65966831290923, 'errorList': [], 'lossList': [0.0, -1.3654415422677995, 0.0, 25.028962168693543, 0.0, 0.0, 0.0], 'rewardMean': 0.7402809859082623, 'totalEpisodes': 47, 'stepsPerEpisode': 181, 'rewardPerEpisode': 143.02667647157233
'totalSteps': 6400, 'rewardStep': 0.4560428100848866, 'errorList': [], 'lossList': [0.0, -1.346854858994484, 0.0, 39.04169296741485, 0.0, 0.0, 0.0], 'rewardMean': 0.6834333507435872, 'totalEpisodes': 52, 'stepsPerEpisode': 128, 'rewardPerEpisode': 96.9329103346153
'totalSteps': 7680, 'rewardStep': 0.939188689909863, 'errorList': [], 'lossList': [0.0, -1.332170250415802, 0.0, 67.04654916763306, 0.0, 0.0, 0.0], 'rewardMean': 0.7260592406046332, 'totalEpisodes': 60, 'stepsPerEpisode': 55, 'rewardPerEpisode': 45.56199170628874
'totalSteps': 8960, 'rewardStep': 0.8750500232336174, 'errorList': [], 'lossList': [0.0, -1.3229440313577652, 0.0, 132.8013049507141, 0.0, 0.0, 0.0], 'rewardMean': 0.7473436381230595, 'totalEpisodes': 74, 'stepsPerEpisode': 109, 'rewardPerEpisode': 89.72825269995862
'totalSteps': 10240, 'rewardStep': 0.7924446018932224, 'errorList': [], 'lossList': [0.0, -1.323910235762596, 0.0, 141.44380805969237, 0.0, 0.0, 0.0], 'rewardMean': 0.7529812585943298, 'totalEpisodes': 99, 'stepsPerEpisode': 42, 'rewardPerEpisode': 35.08791463926246
'totalSteps': 11520, 'rewardStep': 0.5130561582055317, 'errorList': [], 'lossList': [0.0, -1.3356745755672454, 0.0, 24.479988989830016, 0.0, 0.0, 0.0], 'rewardMean': 0.7263229141066856, 'totalEpisodes': 109, 'stepsPerEpisode': 59, 'rewardPerEpisode': 48.270317570723826
'totalSteps': 12800, 'rewardStep': 0.6874985096580262, 'errorList': [], 'lossList': [0.0, -1.3389231014251708, 0.0, 26.64698417663574, 0.0, 0.0, 0.0], 'rewardMean': 0.7224404736618197, 'totalEpisodes': 116, 'stepsPerEpisode': 100, 'rewardPerEpisode': 82.78460071547585
'totalSteps': 14080, 'rewardStep': 0.9764637830230324, 'errorList': [0.1553146670768518, 0.17505606160387369, 0.11975482053579187, 0.14005242760865144, 0.1370044569100967, 0.14906359279762935, 0.24586835595805084, 0.14903149949166056, 0.18153003906117943, 0.170133845546353, 0.14055719915908946, 0.13812908931326998, 0.16662681202877272, 0.14244732011908057, 0.12315599904767129, 0.17063273081557032, 0.17902920883426887, 0.13081983643042708, 0.1596709088983689, 0.16164697299373057, 0.2718898777099976, 0.1437499962421092, 0.3105854311865191, 0.17190262149563282, 0.1142459685636017, 0.1289786645752005, 0.14842417944027742, 0.15019397777762591, 0.1275915544385655, 0.17325097735827988, 0.1582808160031112, 0.12695884195286522, 0.17550034654586424, 0.12179272261219226, 0.12848623756737512, 0.14074907329368064, 0.12111100865668024, 0.16626413433679976, 0.1559283734977047, 0.1346514284632907, 0.11108564923104143, 0.13361992445460746, 0.156451575410475, 0.2255769683382175, 0.17553684305642558, 0.14827998600766404, 0.12297240706351174, 0.1348559647977195, 0.14547423527231113, 0.17638266754395562], 'lossList': [0.0, -1.3246290272474288, 0.0, 12.700684980154037, 0.0, 0.0, 0.0], 'rewardMean': 0.7227847665524522, 'totalEpisodes': 118, 'stepsPerEpisode': 177, 'rewardPerEpisode': 143.14281937882544, 'successfulTests': 46
'totalSteps': 15360, 'rewardStep': 0.7724377023006228, 'errorList': [], 'lossList': [0.0, -1.299048859477043, 0.0, 8.399184278249741, 0.0, 0.0, 0.0], 'rewardMean': 0.7366985189653997, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1013.0000942355143
'totalSteps': 16640, 'rewardStep': 0.7045866088661441, 'errorList': [], 'lossList': [0.0, -1.277443641424179, 0.0, 6.513975450396538, 0.0, 0.0, 0.0], 'rewardMean': 0.7376437200084177, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.5133921282866
'totalSteps': 17920, 'rewardStep': 0.8958411824717215, 'errorList': [], 'lossList': [0.0, -1.2681266522407533, 0.0, 5.894705455824733, 0.0, 0.0, 0.0], 'rewardMean': 0.7612610069646667, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.0166574382959
'totalSteps': 19200, 'rewardStep': 0.9331351001577181, 'errorList': [0.03411178112742845, 0.03115290268668066, 0.06522770851798296, 0.06866064077118532, 0.03180359475959241, 0.03936604016484489, 0.030071915099858005, 0.03608863828111032, 0.07065658083359785, 0.035709659881661686, 0.033004379359749626, 0.03546379528068155, 0.030083795601693167, 0.04192880748239667, 0.03595011255167555, 0.03172070583330471, 0.03099229658620152, 0.03427265846026022, 0.042046868375554235, 0.037545556187894846, 0.05398795910747167, 0.035660014642682814, 0.03028926596239973, 0.034840589428242606, 0.032016320087242714, 0.04294967488305635, 0.031229208110865478, 0.047393135142384205, 0.03507123051679053, 0.030831444718933194, 0.02937297475183296, 0.05641892960714483, 0.053224189613013805, 0.041726824973363164, 0.03068631169321486, 0.030602010757397904, 0.03876945425018582, 0.032449412772534485, 0.03782960984178579, 0.04517554336448752, 0.03891925923919915, 0.029081185408349197, 0.03348856307986644, 0.030392467507385404, 0.043546979449872285, 0.048885503033822664, 0.028372804395907433, 0.05439434963012726, 0.028545513835471874, 0.03191301972418139], 'lossList': [0.0, -1.2340281230211259, 0.0, 3.1014261063933373, 0.0, 0.0, 0.0], 'rewardMean': 0.80897023597195, 'totalEpisodes': 118, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1122.7725327143526, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19200, timeSpent=67.99
