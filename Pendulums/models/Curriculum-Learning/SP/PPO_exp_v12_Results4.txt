#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v12_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v12_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6185648686861225, 'errorList': [], 'lossList': [0.0, -1.414069944024086, 0.0, 50.409586215019225, 0.0, 0.0, 0.0], 'rewardMean': 0.6185648686861225, 'totalEpisodes': 21, 'stepsPerEpisode': 123, 'rewardPerEpisode': 82.08415101615468
'totalSteps': 2560, 'rewardStep': 0.790588307879739, 'errorList': [], 'lossList': [0.0, -1.4034714108705522, 0.0, 28.84112169742584, 0.0, 0.0, 0.0], 'rewardMean': 0.7045765882829307, 'totalEpisodes': 45, 'stepsPerEpisode': 21, 'rewardPerEpisode': 13.75195521886597
'totalSteps': 3840, 'rewardStep': 0.5185025672071708, 'errorList': [], 'lossList': [0.0, -1.3828193420171737, 0.0, 27.876374311447144, 0.0, 0.0, 0.0], 'rewardMean': 0.6425519145910107, 'totalEpisodes': 62, 'stepsPerEpisode': 28, 'rewardPerEpisode': 18.638354754011544
'totalSteps': 5120, 'rewardStep': 0.5346394074832637, 'errorList': [], 'lossList': [0.0, -1.3532753640413284, 0.0, 21.913672814369203, 0.0, 0.0, 0.0], 'rewardMean': 0.6155737878140739, 'totalEpisodes': 69, 'stepsPerEpisode': 105, 'rewardPerEpisode': 80.79700882614434
'totalSteps': 6400, 'rewardStep': 0.8874956943824196, 'errorList': [], 'lossList': [0.0, -1.3371370327472687, 0.0, 24.070208458900453, 0.0, 0.0, 0.0], 'rewardMean': 0.669958169127743, 'totalEpisodes': 74, 'stepsPerEpisode': 34, 'rewardPerEpisode': 29.87870285512977
'totalSteps': 7680, 'rewardStep': 0.9547953824331571, 'errorList': [], 'lossList': [0.0, -1.3228168648481369, 0.0, 35.62870657444, 0.0, 0.0, 0.0], 'rewardMean': 0.7174310380119787, 'totalEpisodes': 78, 'stepsPerEpisode': 35, 'rewardPerEpisode': 30.83254546196482
'totalSteps': 8960, 'rewardStep': 0.7305762561048473, 'errorList': [], 'lossList': [0.0, -1.3083825892210006, 0.0, 149.21523300170898, 0.0, 0.0, 0.0], 'rewardMean': 0.71930892631096, 'totalEpisodes': 100, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.4571122455167638
'totalSteps': 10240, 'rewardStep': 0.9238437028361842, 'errorList': [], 'lossList': [0.0, -1.3060037964582443, 0.0, 111.27856924057006, 0.0, 0.0, 0.0], 'rewardMean': 0.7448757733766129, 'totalEpisodes': 121, 'stepsPerEpisode': 57, 'rewardPerEpisode': 51.95254720146646
'totalSteps': 11520, 'rewardStep': 0.6926895267319125, 'errorList': [], 'lossList': [0.0, -1.30683968603611, 0.0, 35.58806136131287, 0.0, 0.0, 0.0], 'rewardMean': 0.7390773015272019, 'totalEpisodes': 132, 'stepsPerEpisode': 82, 'rewardPerEpisode': 64.09210733253236
'totalSteps': 12800, 'rewardStep': 0.7895391660060309, 'errorList': [], 'lossList': [0.0, -1.2883257192373276, 0.0, 14.841619539260865, 0.0, 0.0, 0.0], 'rewardMean': 0.7441234879750847, 'totalEpisodes': 139, 'stepsPerEpisode': 64, 'rewardPerEpisode': 57.63086810161703
'totalSteps': 14080, 'rewardStep': 0.6768126612158807, 'errorList': [], 'lossList': [0.0, -1.2635378032922744, 0.0, 8.156555743217469, 0.0, 0.0, 0.0], 'rewardMean': 0.7499482672280606, 'totalEpisodes': 144, 'stepsPerEpisode': 360, 'rewardPerEpisode': 298.33646399039304
'totalSteps': 15360, 'rewardStep': 0.6125031411358879, 'errorList': [], 'lossList': [0.0, -1.2687260615825653, 0.0, 20.1439732003212, 0.0, 0.0, 0.0], 'rewardMean': 0.7321397505536754, 'totalEpisodes': 151, 'stepsPerEpisode': 317, 'rewardPerEpisode': 262.2620685822079
'totalSteps': 16640, 'rewardStep': 0.05134110119215074, 'errorList': [], 'lossList': [0.0, -1.2681259518861772, 0.0, 11.321352561712265, 0.0, 0.0, 0.0], 'rewardMean': 0.6854236039521735, 'totalEpisodes': 157, 'stepsPerEpisode': 521, 'rewardPerEpisode': 385.0039528053093
'totalSteps': 17920, 'rewardStep': 0.717211502678264, 'errorList': [], 'lossList': [0.0, -1.2567467749118806, 0.0, 4.608133971691132, 0.0, 0.0, 0.0], 'rewardMean': 0.7036808134716734, 'totalEpisodes': 159, 'stepsPerEpisode': 569, 'rewardPerEpisode': 448.16987572999324
'totalSteps': 19200, 'rewardStep': 0.6585420626509513, 'errorList': [], 'lossList': [0.0, -1.2379676228761674, 0.0, 3.94910549223423, 0.0, 0.0, 0.0], 'rewardMean': 0.6807854502985267, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 957.747123814635
'totalSteps': 20480, 'rewardStep': 0.9414134951079424, 'errorList': [0.14431861391800913, 0.14974933735574272, 0.1416149366094192, 0.20271786410663184, 0.23710974152757308, 0.14868851877598857, 0.14449260502403874, 0.14769614196290043, 0.14732462563789014, 0.14925295885933726, 0.14419409087742385, 0.26239397784754676, 0.153962397981513, 0.14904328835747416, 0.1483998051724063, 0.15342374058252928, 0.1509122945699134, 0.19738810251317565, 0.1522698170443777, 0.1454692533685198, 0.14971988601629443, 0.1571301758565679, 0.1518651393268099, 0.14128224649322316, 0.1488018242137688, 0.15216279458796456, 0.1465470965557156, 0.15186656644239777, 0.15336656764658363, 0.14729217223988667, 0.1739875736030955, 0.16103572664633858, 0.17420137232680374, 0.1794325198398454, 0.23850375902430324, 0.15525616849967913, 0.14483276953609409, 0.27186034626884076, 0.150007768344156, 0.15188656550685198, 0.15234600140490032, 0.1469216416374457, 0.14449693388109566, 0.15071596526432643, 0.1489192094809854, 0.15241311080662254, 0.1484060155285694, 0.15060005777214483, 0.15128093580792626, 0.14747814660303435], 'lossList': [0.0, -1.2172341841459273, 0.0, 3.850828697681427, 0.0, 0.0, 0.0], 'rewardMean': 0.6794472615660051, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1042.8484015931979, 'successfulTests': 45
'totalSteps': 21760, 'rewardStep': 0.7602729051171674, 'errorList': [], 'lossList': [0.0, -1.2075127732753754, 0.0, 1.5291798281669617, 0.0, 0.0, 0.0], 'rewardMean': 0.6824169264672372, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.3090422501864
'totalSteps': 23040, 'rewardStep': 0.8558110403485387, 'errorList': [], 'lossList': [0.0, -1.1982533073425292, 0.0, 0.832883412502706, 0.0, 0.0, 0.0], 'rewardMean': 0.6756136602184727, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.0896033716695
'totalSteps': 24320, 'rewardStep': 0.9127831442485879, 'errorList': [], 'lossList': [0.0, -1.180659034848213, 0.0, 1.05805168222636, 0.0, 0.0, 0.0], 'rewardMean': 0.6976230219701403, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1170.7717504925313
'totalSteps': 25600, 'rewardStep': 0.9670100811273709, 'errorList': [0.05311776066405443, 0.08747447162589593, 0.05266006446789524, 0.05234780435884713, 0.048168716225299064, 0.08486507217301455, 0.04840389625269554, 0.04728342973816197, 0.08391585363121264, 0.05241330347635235, 0.04777791723555916, 0.05099237374299469, 0.09447439966585121, 0.08648608547862906, 0.07239054942214167, 0.06973414099528148, 0.06517909287017565, 0.08087958569894431, 0.10958567070452481, 0.05353091264693941, 0.12036928074559024, 0.04887442803150355, 0.047142065087436916, 0.05130052148687104, 0.05283848651231965, 0.047992318993208286, 0.08925085661125405, 0.08110067074176368, 0.049270297193058726, 0.0663054456294051, 0.047818449336378986, 0.06164086707445129, 0.049342314781567774, 0.05003490860148526, 0.053742480582856286, 0.12342983776269274, 0.04735922409945429, 0.04718797955104909, 0.0509724222388298, 0.04901111912820031, 0.09551644834658916, 0.0538300441149398, 0.051142006197530364, 0.04977631124132056, 0.05031211296911793, 0.04464901330783607, 0.04446176159395006, 0.047492024842300244, 0.04975892884282864, 0.05006460629699206], 'lossList': [0.0, -1.1535346633195878, 0.0, 0.9567320219799876, 0.0, 0.0, 0.0], 'rewardMean': 0.7153701134822742, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1198.2713423128182, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=75.31
