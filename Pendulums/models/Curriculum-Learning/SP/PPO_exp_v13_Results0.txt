#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v13_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v13_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.49514308324319073, 'errorList': [], 'lossList': [0.0, -1.4140458709001542, 0.0, 48.21066846370697, 0.0, 0.0, 0.0], 'rewardMean': 0.49514308324319073, 'totalEpisodes': 24, 'stepsPerEpisode': 59, 'rewardPerEpisode': 37.32829579835474
'totalSteps': 2560, 'rewardStep': 0.8345659529714261, 'errorList': [], 'lossList': [0.0, -1.4043660706281662, 0.0, 31.951553964614867, 0.0, 0.0, 0.0], 'rewardMean': 0.6648545181073084, 'totalEpisodes': 49, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.608719721677613
'totalSteps': 3840, 'rewardStep': 0.7248434750922045, 'errorList': [], 'lossList': [0.0, -1.398698823451996, 0.0, 23.527807698249816, 0.0, 0.0, 0.0], 'rewardMean': 0.6848508371022738, 'totalEpisodes': 69, 'stepsPerEpisode': 14, 'rewardPerEpisode': 9.855724712260601
'totalSteps': 5120, 'rewardStep': 0.909722383119561, 'errorList': [], 'lossList': [0.0, -1.394997946023941, 0.0, 21.103545289039612, 0.0, 0.0, 0.0], 'rewardMean': 0.7410687236065956, 'totalEpisodes': 81, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.59820090840477
'totalSteps': 6400, 'rewardStep': 0.88535917477643, 'errorList': [], 'lossList': [0.0, -1.3871245396137237, 0.0, 14.482215974330902, 0.0, 0.0, 0.0], 'rewardMean': 0.7699268138405625, 'totalEpisodes': 89, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.465322008939768
'totalSteps': 7680, 'rewardStep': 0.8562365128112331, 'errorList': [], 'lossList': [0.0, -1.3698511928319932, 0.0, 38.232122049331664, 0.0, 0.0, 0.0], 'rewardMean': 0.7843117636690077, 'totalEpisodes': 93, 'stepsPerEpisode': 296, 'rewardPerEpisode': 215.38366674749065
'totalSteps': 8960, 'rewardStep': 0.8375196392345141, 'errorList': [], 'lossList': [0.0, -1.346011875271797, 0.0, 147.82474338531495, 0.0, 0.0, 0.0], 'rewardMean': 0.7919128887497943, 'totalEpisodes': 110, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.457174267015166
'totalSteps': 10240, 'rewardStep': 0.5622450250081052, 'errorList': [], 'lossList': [0.0, -1.3416282272338866, 0.0, 165.34805030822753, 0.0, 0.0, 0.0], 'rewardMean': 0.763204405782083, 'totalEpisodes': 144, 'stepsPerEpisode': 63, 'rewardPerEpisode': 47.815023376137354
'totalSteps': 11520, 'rewardStep': 0.7204000622457036, 'errorList': [], 'lossList': [0.0, -1.3295085257291794, 0.0, 73.76771865844727, 0.0, 0.0, 0.0], 'rewardMean': 0.7584483676113742, 'totalEpisodes': 167, 'stepsPerEpisode': 47, 'rewardPerEpisode': 33.13315082544995
'totalSteps': 12800, 'rewardStep': 0.7818310971110936, 'errorList': [], 'lossList': [0.0, -1.3115338253974915, 0.0, 26.44324556827545, 0.0, 0.0, 0.0], 'rewardMean': 0.7607866405613462, 'totalEpisodes': 177, 'stepsPerEpisode': 123, 'rewardPerEpisode': 91.5126907697246
'totalSteps': 14080, 'rewardStep': 0.2804733502784899, 'errorList': [], 'lossList': [0.0, -1.306843068599701, 0.0, 9.869145603179932, 0.0, 0.0, 0.0], 'rewardMean': 0.7393196672648761, 'totalEpisodes': 183, 'stepsPerEpisode': 129, 'rewardPerEpisode': 87.52390264879782
'totalSteps': 15360, 'rewardStep': 0.7915631703391263, 'errorList': [], 'lossList': [0.0, -1.29943617105484, 0.0, 47.904708971977236, 0.0, 0.0, 0.0], 'rewardMean': 0.7350193890016461, 'totalEpisodes': 193, 'stepsPerEpisode': 69, 'rewardPerEpisode': 63.11925730659006
'totalSteps': 16640, 'rewardStep': 0.7973638277592989, 'errorList': [], 'lossList': [0.0, -1.2957240438461304, 0.0, 13.20176602602005, 0.0, 0.0, 0.0], 'rewardMean': 0.7422714242683555, 'totalEpisodes': 198, 'stepsPerEpisode': 98, 'rewardPerEpisode': 79.39727784988992
'totalSteps': 17920, 'rewardStep': 0.42538429346980666, 'errorList': [], 'lossList': [0.0, -1.3040690886974335, 0.0, 6.197795247137546, 0.0, 0.0, 0.0], 'rewardMean': 0.6938376153033803, 'totalEpisodes': 201, 'stepsPerEpisode': 427, 'rewardPerEpisode': 284.774219597287
'totalSteps': 19200, 'rewardStep': 0.8586933683934327, 'errorList': [], 'lossList': [0.0, -1.3036011463403703, 0.0, 33.492479991912845, 0.0, 0.0, 0.0], 'rewardMean': 0.6911710346650805, 'totalEpisodes': 205, 'stepsPerEpisode': 208, 'rewardPerEpisode': 182.86074873583001
'totalSteps': 20480, 'rewardStep': 0.5442116064103368, 'errorList': [], 'lossList': [0.0, -1.3096460437774657, 0.0, 5.978699669241905, 0.0, 0.0, 0.0], 'rewardMean': 0.6599685440249907, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 901.4799903919
'totalSteps': 21760, 'rewardStep': 0.7826507999385937, 'errorList': [], 'lossList': [0.0, -1.3187256848812103, 0.0, 3.628638349175453, 0.0, 0.0, 0.0], 'rewardMean': 0.6544816600953987, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 959.2212646665444
'totalSteps': 23040, 'rewardStep': 0.5966965131708921, 'errorList': [], 'lossList': [0.0, -1.3080450975894928, 0.0, 2.585203268826008, 0.0, 0.0, 0.0], 'rewardMean': 0.6579268089116775, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 964.5364588351217
'totalSteps': 24320, 'rewardStep': 0.7168788694945126, 'errorList': [], 'lossList': [0.0, -1.2721745949983596, 0.0, 1.5481904804706574, 0.0, 0.0, 0.0], 'rewardMean': 0.6575746896365582, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.7147578149732
'totalSteps': 25600, 'rewardStep': 0.8986975596795826, 'errorList': [], 'lossList': [0.0, -1.232589882016182, 0.0, 1.6078758026659488, 0.0, 0.0, 0.0], 'rewardMean': 0.6692613358934072, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.3089653690656
'totalSteps': 26880, 'rewardStep': 0.9602217991353084, 'errorList': [0.03969940715917457, 0.058446049531774596, 0.026832646750923173, 0.02998094729648465, 0.039297389703850626, 0.026301622341222453, 0.026631939562748407, 0.04371381333150105, 0.03873396007705546, 0.04833620010203728, 0.036099994876585835, 0.035916875560788174, 0.04682792548286382, 0.033915797082303537, 0.05434152464459973, 0.028279570098130986, 0.04620399416469711, 0.02888721267625249, 0.04657006615762294, 0.028781200190099457, 0.040521302405790896, 0.027746367190368063, 0.03299959523339731, 0.04808728158490908, 0.03020804995781995, 0.04861857123555577, 0.029116105165111515, 0.03581623143554526, 0.051282269046318885, 0.03195005726771449, 0.0317829209018087, 0.03143942445943332, 0.026369678917038848, 0.06257477265193542, 0.038180077220800794, 0.03578447395534271, 0.04470501520004255, 0.0396392497858087, 0.027702210578775456, 0.038433409244272146, 0.03192105850032364, 0.05308298126621687, 0.036231164566674114, 0.029254192054428786, 0.054157604720379574, 0.04492604290878567, 0.03820697858149002, 0.04202065866805489, 0.03276966399255697, 0.02882564008214886], 'lossList': [0.0, -1.1993614023923873, 0.0, 1.149287077151239, 0.0, 0.0, 0.0], 'rewardMean': 0.737236180779089, 'totalEpisodes': 205, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.120671399421, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=59.46
