#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v13_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v13_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.36710913471577034, 'errorList': [], 'lossList': [0.0, -1.4118486976623534, 0.0, 51.28366688251495, 0.0, 0.0, 0.0], 'rewardMean': 0.36710913471577034, 'totalEpisodes': 25, 'stepsPerEpisode': 123, 'rewardPerEpisode': 79.14550706623413
'totalSteps': 2560, 'rewardStep': 0.9141780318802801, 'errorList': [], 'lossList': [0.0, -1.40122223675251, 0.0, 26.15008463382721, 0.0, 0.0, 0.0], 'rewardMean': 0.6406435832980253, 'totalEpisodes': 44, 'stepsPerEpisode': 33, 'rewardPerEpisode': 26.55893477586374
'totalSteps': 3840, 'rewardStep': 0.5608113688948246, 'errorList': [], 'lossList': [0.0, -1.3931880885362624, 0.0, 30.22028022289276, 0.0, 0.0, 0.0], 'rewardMean': 0.6140328451636251, 'totalEpisodes': 58, 'stepsPerEpisode': 71, 'rewardPerEpisode': 46.80023234482126
'totalSteps': 5120, 'rewardStep': 0.4614930603628957, 'errorList': [], 'lossList': [0.0, -1.3780719542503357, 0.0, 20.573487179279326, 0.0, 0.0, 0.0], 'rewardMean': 0.5758978989634427, 'totalEpisodes': 62, 'stepsPerEpisode': 180, 'rewardPerEpisode': 115.94696958644808
'totalSteps': 6400, 'rewardStep': 0.45983289391492854, 'errorList': [], 'lossList': [0.0, -1.3635965287685394, 0.0, 31.579710960388184, 0.0, 0.0, 0.0], 'rewardMean': 0.5526848979537398, 'totalEpisodes': 67, 'stepsPerEpisode': 143, 'rewardPerEpisode': 111.61909974630277
'totalSteps': 7680, 'rewardStep': 0.8649982515279849, 'errorList': [], 'lossList': [0.0, -1.3537333303689956, 0.0, 50.59545533180237, 0.0, 0.0, 0.0], 'rewardMean': 0.6047371235494473, 'totalEpisodes': 74, 'stepsPerEpisode': 66, 'rewardPerEpisode': 47.754973925087754
'totalSteps': 8960, 'rewardStep': 0.8863051279665471, 'errorList': [], 'lossList': [0.0, -1.3410492557287217, 0.0, 182.42544284820556, 0.0, 0.0, 0.0], 'rewardMean': 0.6449611241804616, 'totalEpisodes': 97, 'stepsPerEpisode': 80, 'rewardPerEpisode': 63.12553158314817
'totalSteps': 10240, 'rewardStep': 0.6119277187234942, 'errorList': [], 'lossList': [0.0, -1.333934757709503, 0.0, 139.26969009399414, 0.0, 0.0, 0.0], 'rewardMean': 0.6408319484983407, 'totalEpisodes': 121, 'stepsPerEpisode': 47, 'rewardPerEpisode': 30.283825552962814
'totalSteps': 11520, 'rewardStep': 0.9053638653205356, 'errorList': [], 'lossList': [0.0, -1.3422478467226029, 0.0, 36.94013418197632, 0.0, 0.0, 0.0], 'rewardMean': 0.6702243837008068, 'totalEpisodes': 131, 'stepsPerEpisode': 11, 'rewardPerEpisode': 10.042200177579534
'totalSteps': 12800, 'rewardStep': 0.6834188716485834, 'errorList': [], 'lossList': [0.0, -1.3396832883358, 0.0, 21.90071212530136, 0.0, 0.0, 0.0], 'rewardMean': 0.6715438324955845, 'totalEpisodes': 135, 'stepsPerEpisode': 142, 'rewardPerEpisode': 117.19257647723863
'totalSteps': 14080, 'rewardStep': 0.7314093113536262, 'errorList': [], 'lossList': [0.0, -1.3118256253004075, 0.0, 14.40109375, 0.0, 0.0, 0.0], 'rewardMean': 0.7079738501593701, 'totalEpisodes': 139, 'stepsPerEpisode': 352, 'rewardPerEpisode': 242.9256988580583
'totalSteps': 15360, 'rewardStep': 0.7428225681643739, 'errorList': [], 'lossList': [0.0, -1.3004055047035217, 0.0, 9.021640492677689, 0.0, 0.0, 0.0], 'rewardMean': 0.6908383037877793, 'totalEpisodes': 142, 'stepsPerEpisode': 112, 'rewardPerEpisode': 91.98998606868471
'totalSteps': 16640, 'rewardStep': 0.6382441740566803, 'errorList': [], 'lossList': [0.0, -1.2868449079990387, 0.0, 6.522482607364655, 0.0, 0.0, 0.0], 'rewardMean': 0.6985815843039649, 'totalEpisodes': 145, 'stepsPerEpisode': 206, 'rewardPerEpisode': 150.53018935593434
'totalSteps': 17920, 'rewardStep': 0.9159916316703195, 'errorList': [], 'lossList': [0.0, -1.2768541353940963, 0.0, 4.742755971550942, 0.0, 0.0, 0.0], 'rewardMean': 0.7440314414347073, 'totalEpisodes': 147, 'stepsPerEpisode': 420, 'rewardPerEpisode': 362.8467995781851
'totalSteps': 19200, 'rewardStep': 0.8653458408009266, 'errorList': [], 'lossList': [0.0, -1.270324399471283, 0.0, 2.577739753425121, 0.0, 0.0, 0.0], 'rewardMean': 0.7845827361233072, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 943.8593634265802
'totalSteps': 20480, 'rewardStep': 0.8964657523228673, 'errorList': [], 'lossList': [0.0, -1.2492500758171081, 0.0, 2.5395418879389764, 0.0, 0.0, 0.0], 'rewardMean': 0.7877294862027954, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.2124882041956
'totalSteps': 21760, 'rewardStep': 0.8898288512695782, 'errorList': [], 'lossList': [0.0, -1.2300558346509933, 0.0, 1.353190475925803, 0.0, 0.0, 0.0], 'rewardMean': 0.7880818585330984, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1073.1692231469772
'totalSteps': 23040, 'rewardStep': 0.7593054957980012, 'errorList': [], 'lossList': [0.0, -1.225399012565613, 0.0, 0.880815541036427, 0.0, 0.0, 0.0], 'rewardMean': 0.8028196362405492, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1104.1799993395134
'totalSteps': 24320, 'rewardStep': 0.8735125890488412, 'errorList': [], 'lossList': [0.0, -1.2078498709201813, 0.0, 1.3407311820983887, 0.0, 0.0, 0.0], 'rewardMean': 0.7996345086133798, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.953324175728
'totalSteps': 25600, 'rewardStep': 0.9544076486769211, 'errorList': [0.08412038955876393, 0.07118564165247442, 0.08680388066411854, 0.06490270014530947, 0.03375536695534666, 0.03621230578529176, 0.09092717533068424, 0.027494724366790083, 0.10571631329609353, 0.024557471004152263, 0.1285635051378643, 0.0593243384795904, 0.08657497284012135, 0.08083667226242142, 0.048110294778150486, 0.1045438873485801, 0.0655980421145866, 0.0756488420197192, 0.0862440612381693, 0.08291229896972409, 0.06380041342295502, 0.05775514481593844, 0.06910935510662841, 0.11834695609809245, 0.027225056656791573, 0.031145740584275522, 0.05538933364251036, 0.09137003364104655, 0.03051421918143195, 0.030902311811888315, 0.04455526463883817, 0.09406451069939857, 0.040062600958875164, 0.02648161153351753, 0.03991184201524173, 0.03437883858165698, 0.05790958451603639, 0.06523436128921546, 0.09175279284245141, 0.0992761598641477, 0.06275187004990078, 0.04683426789841729, 0.030815192276687185, 0.049086549047871256, 0.10470460668119824, 0.0646487487478498, 0.07163845121835946, 0.051343064990031266, 0.051535194341339075, 0.02348144052700444], 'lossList': [0.0, -1.183653511404991, 0.0, 0.8269209518656134, 0.0, 0.0, 0.0], 'rewardMean': 0.8267333863162136, 'totalEpisodes': 147, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.6194198172443, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=57.63
