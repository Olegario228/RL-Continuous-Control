#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v13_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v13_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6149510272207517, 'errorList': [], 'lossList': [0.0, -1.4183752566576004, 0.0, 50.31794131755829, 0.0, 0.0, 0.0], 'rewardMean': 0.6149510272207517, 'totalEpisodes': 27, 'stepsPerEpisode': 131, 'rewardPerEpisode': 99.67532641769097
'totalSteps': 2560, 'rewardStep': 0.9207464448533835, 'errorList': [], 'lossList': [0.0, -1.4149222922325135, 0.0, 34.24084337234497, 0.0, 0.0, 0.0], 'rewardMean': 0.7678487360370676, 'totalEpisodes': 52, 'stepsPerEpisode': 22, 'rewardPerEpisode': 15.489687884902368
'totalSteps': 3840, 'rewardStep': 0.6860338213906263, 'errorList': [], 'lossList': [0.0, -1.3985872501134873, 0.0, 33.4828691148758, 0.0, 0.0, 0.0], 'rewardMean': 0.7405770978215872, 'totalEpisodes': 67, 'stepsPerEpisode': 36, 'rewardPerEpisode': 25.163788684750276
'totalSteps': 5120, 'rewardStep': 0.5202187708910979, 'errorList': [], 'lossList': [0.0, -1.3705024498701095, 0.0, 34.92712720870972, 0.0, 0.0, 0.0], 'rewardMean': 0.6854875160889649, 'totalEpisodes': 75, 'stepsPerEpisode': 238, 'rewardPerEpisode': 191.03535111398875
'totalSteps': 6400, 'rewardStep': 0.606328337768588, 'errorList': [], 'lossList': [0.0, -1.3484323006868362, 0.0, 24.064065554141997, 0.0, 0.0, 0.0], 'rewardMean': 0.6696556804248894, 'totalEpisodes': 77, 'stepsPerEpisode': 867, 'rewardPerEpisode': 621.8163917303049
'totalSteps': 7680, 'rewardStep': 0.6559638118167915, 'errorList': [], 'lossList': [0.0, -1.3275024962425233, 0.0, 20.210277562141417, 0.0, 0.0, 0.0], 'rewardMean': 0.6673737023235398, 'totalEpisodes': 78, 'stepsPerEpisode': 428, 'rewardPerEpisode': 281.99692551150764
'totalSteps': 8960, 'rewardStep': 0.8189235224994965, 'errorList': [], 'lossList': [0.0, -1.3051236987113952, 0.0, 148.38102703094484, 0.0, 0.0, 0.0], 'rewardMean': 0.6890236766343908, 'totalEpisodes': 94, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.404013321528993
'totalSteps': 10240, 'rewardStep': 0.7296635193013984, 'errorList': [], 'lossList': [0.0, -1.2969281589984893, 0.0, 165.9471997833252, 0.0, 0.0, 0.0], 'rewardMean': 0.6941036569677668, 'totalEpisodes': 128, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.090303772006386
'totalSteps': 11520, 'rewardStep': 0.5341955637347204, 'errorList': [], 'lossList': [0.0, -1.3021627843379975, 0.0, 29.945132851600647, 0.0, 0.0, 0.0], 'rewardMean': 0.6763360910529839, 'totalEpisodes': 144, 'stepsPerEpisode': 248, 'rewardPerEpisode': 207.0074065586137
'totalSteps': 12800, 'rewardStep': 0.9197371360146518, 'errorList': [], 'lossList': [0.0, -1.3046290689706803, 0.0, 13.231938259601593, 0.0, 0.0, 0.0], 'rewardMean': 0.7006761955491506, 'totalEpisodes': 156, 'stepsPerEpisode': 63, 'rewardPerEpisode': 59.26433342407092
'totalSteps': 14080, 'rewardStep': 0.9751128596888579, 'errorList': [161.911991562236, 148.39774617504946, 121.09862823452342, 112.00277923562662, 116.1795144133934, 74.8214879184591, 14.702511115600238, 135.41319363186935, 18.932728791204543, 74.5548566560151, 93.51778534369242, 126.63001528738573, 117.18272607161688, 4.800781093669399, 40.4433314558571, 173.03845563701913, 87.41875814312003, 19.09850683585224, 113.52779325915387, 90.81469781392107, 146.9894930423822, 79.76927529370988, 67.00785293530137, 145.46869053453446, 153.63488738835125, 59.96517789608262, 1.06352126756621, 142.85300599299876, 89.73520947855818, 144.55626811996646, 40.486603776208284, 88.92951501726189, 150.8109598317798, 132.93466120488642, 137.84209857491635, 140.51074174217246, 80.10317423033402, 140.85960104730597, 139.12128705169144, 49.90357709694183, 57.50829785887994, 31.67520357368302, 126.57051393640987, 136.9009166086261, 22.049448713742603, 92.57978766140634, 89.9724174198992, 150.27360822508842, 10.586081685367928, 68.53317303006258], 'lossList': [0.0, -1.279715161919594, 0.0, 13.029539632797242, 0.0, 0.0, 0.0], 'rewardMean': 0.7366923787959612, 'totalEpisodes': 162, 'stepsPerEpisode': 92, 'rewardPerEpisode': 83.27362758802636, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.48777641687856876, 'errorList': [], 'lossList': [0.0, -1.2643917888402938, 0.0, 10.246049853563308, 0.0, 0.0, 0.0], 'rewardMean': 0.6933953759984798, 'totalEpisodes': 168, 'stepsPerEpisode': 211, 'rewardPerEpisode': 173.7251917837755
'totalSteps': 16640, 'rewardStep': 0.6739134477681027, 'errorList': [], 'lossList': [0.0, -1.2642580699920654, 0.0, 19.69904922246933, 0.0, 0.0, 0.0], 'rewardMean': 0.6921833386362273, 'totalEpisodes': 176, 'stepsPerEpisode': 60, 'rewardPerEpisode': 51.1302833910999
'totalSteps': 17920, 'rewardStep': 0.8423259046907743, 'errorList': [], 'lossList': [0.0, -1.2641935515403748, 0.0, 7.588142249584198, 0.0, 0.0, 0.0], 'rewardMean': 0.724394052016195, 'totalEpisodes': 181, 'stepsPerEpisode': 83, 'rewardPerEpisode': 69.21208062520776
'totalSteps': 19200, 'rewardStep': 0.8431338630196435, 'errorList': [], 'lossList': [0.0, -1.2670671939849854, 0.0, 5.111902149915696, 0.0, 0.0, 0.0], 'rewardMean': 0.7480746045413006, 'totalEpisodes': 183, 'stepsPerEpisode': 539, 'rewardPerEpisode': 463.10597687717063
'totalSteps': 20480, 'rewardStep': 0.9113815133207379, 'errorList': [], 'lossList': [0.0, -1.282774892449379, 0.0, 4.181001687645912, 0.0, 0.0, 0.0], 'rewardMean': 0.7736163746916951, 'totalEpisodes': 186, 'stepsPerEpisode': 27, 'rewardPerEpisode': 20.238261010534647
'totalSteps': 21760, 'rewardStep': 0.7865400646408779, 'errorList': [], 'lossList': [0.0, -1.2830837202072143, 0.0, 3.418206515610218, 0.0, 0.0, 0.0], 'rewardMean': 0.7703780289058333, 'totalEpisodes': 187, 'stepsPerEpisode': 809, 'rewardPerEpisode': 701.8047685234482
'totalSteps': 23040, 'rewardStep': 0.8824109783839932, 'errorList': [], 'lossList': [0.0, -1.2569487226009368, 0.0, 3.7185104793310164, 0.0, 0.0, 0.0], 'rewardMean': 0.7856527748140929, 'totalEpisodes': 188, 'stepsPerEpisode': 129, 'rewardPerEpisode': 115.27573975988547
'totalSteps': 24320, 'rewardStep': 0.8511330793778932, 'errorList': [], 'lossList': [0.0, -1.2590211480855942, 0.0, 2.313835486471653, 0.0, 0.0, 0.0], 'rewardMean': 0.8173465263784101, 'totalEpisodes': 189, 'stepsPerEpisode': 192, 'rewardPerEpisode': 175.96395904073802
'totalSteps': 25600, 'rewardStep': 0.9782222094902507, 'errorList': [0.017382950846682697, 0.00790429273293786, 0.0009595813109220117, 0.0033795490148864887, 0.008351055178902935, 0.03911158848645607, 0.02507924261017228, 0.012994709361038038, 0.013965058843463125, 0.011932043873234058, 0.0060657648938032055, 0.01718592216382786, 0.021630537235879405, 0.01023623658981575, 0.0035148941684598357, 0.008064272834476726, 0.03418040975876573, 0.003453945872749552, 0.025400455630858288, 0.05554520300127513, 0.017915011060729416, 0.021432571133401128, 0.015589801574773162, 0.014187710376259278, 0.011079251080756549, 0.006007258633673603, 0.0007372994841435596, 0.01284124258393981, 0.003921979920737673, 0.01727761761401097, 0.025176081619982377, 0.031345563919742594, 0.05681088504314241, 0.016948198058878646, 0.030961003440467844, 0.02248340484412609, 0.02168537734372439, 0.03336093621682836, 0.01595501785257638, 0.006148988903948545, 0.012273232087707397, 0.02325382291067951, 0.007488065992756349, 0.03251722844355974, 0.04475704157322974, 0.001035923622833597, 0.008175071386743624, 0.013093723084934073, 0.03420083564538047, 0.009975935887228893], 'lossList': [0.0, -1.2399100703001023, 0.0, 1.1227142312377691, 0.0, 0.0, 0.0], 'rewardMean': 0.82319503372597, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.2861996415602, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=73.04
