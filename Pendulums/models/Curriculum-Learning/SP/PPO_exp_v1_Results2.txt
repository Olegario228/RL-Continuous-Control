#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 512, 'episodeStepsMax': 640, 'totalLearningSteps': 50000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v1_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v1_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 6], [0, 3], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 640, 'rewardStep': 0.644018087385529, 'errorList': [], 'lossList': [0.0, -1.418024890422821, 0.0, 97.0514833831787, 0.0, 0.0, 0.0], 'rewardMean': 0.644018087385529, 'totalEpisodes': 3, 'stepsPerEpisode': 12, 'rewardPerEpisode': 9.818462339175642
'totalSteps': 1280, 'rewardStep': 0.8193149709709542, 'errorList': [], 'lossList': [0.0, -1.4091921329498291, 0.0, 57.25136009216308, 0.0, 0.0, 0.0], 'rewardMean': 0.7316665291782416, 'totalEpisodes': 7, 'stepsPerEpisode': 201, 'rewardPerEpisode': 156.00662754320467
'totalSteps': 1920, 'rewardStep': 0.7018691006663056, 'errorList': [], 'lossList': [0.0, -1.4061331272125244, 0.0, 38.08132102966309, 0.0, 0.0, 0.0], 'rewardMean': 0.7217340530075962, 'totalEpisodes': 13, 'stepsPerEpisode': 76, 'rewardPerEpisode': 54.26477225874603
'totalSteps': 2560, 'rewardStep': 0.6705872121761723, 'errorList': [], 'lossList': [0.0, -1.4235522747039795, 0.0, 41.9201895904541, 0.0, 0.0, 0.0], 'rewardMean': 0.7089473427997403, 'totalEpisodes': 19, 'stepsPerEpisode': 118, 'rewardPerEpisode': 89.91641549299693
'totalSteps': 3200, 'rewardStep': 0.9568900126300408, 'errorList': [], 'lossList': [0.0, -1.4114930593967439, 0.0, 22.73252562522888, 0.0, 0.0, 0.0], 'rewardMean': 0.7585358767658004, 'totalEpisodes': 22, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.529979544889802
'totalSteps': 3840, 'rewardStep': 0.7317031083783296, 'errorList': [], 'lossList': [0.0, -1.3913503658771516, 0.0, 35.98768083572388, 0.0, 0.0, 0.0], 'rewardMean': 0.754063748701222, 'totalEpisodes': 23, 'stepsPerEpisode': 77, 'rewardPerEpisode': 58.93454719415733
'totalSteps': 4480, 'rewardStep': 0.4603694329982089, 'errorList': [], 'lossList': [0.0, -1.3900570571422577, 0.0, 38.36560289382935, 0.0, 0.0, 0.0], 'rewardMean': 0.7121074178865058, 'totalEpisodes': 25, 'stepsPerEpisode': 88, 'rewardPerEpisode': 58.34124454000877
'totalSteps': 5120, 'rewardStep': 0.8808947898758752, 'errorList': [], 'lossList': [0.0, -1.3975043261051179, 0.0, 70.34979789733887, 0.0, 0.0, 0.0], 'rewardMean': 0.733205839385177, 'totalEpisodes': 31, 'stepsPerEpisode': 36, 'rewardPerEpisode': 32.29773820459417
'totalSteps': 5760, 'rewardStep': 0.982153429437476, 'errorList': [], 'lossList': [0.0, -1.3922929465770721, 0.0, 41.69271760940552, 0.0, 0.0, 0.0], 'rewardMean': 0.7608666827243213, 'totalEpisodes': 32, 'stepsPerEpisode': 464, 'rewardPerEpisode': 348.09863612609615
'totalSteps': 6400, 'rewardStep': 0.7031741986064007, 'errorList': [], 'lossList': [0.0, -1.3866760325431824, 0.0, 67.48861505508422, 0.0, 0.0, 0.0], 'rewardMean': 0.7550974343125293, 'totalEpisodes': 36, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.300594696975619
'totalSteps': 7040, 'rewardStep': 0.604964592823846, 'errorList': [], 'lossList': [0.0, -1.3778298604488373, 0.0, 64.5117261314392, 0.0, 0.0, 0.0], 'rewardMean': 0.7511920848563609, 'totalEpisodes': 39, 'stepsPerEpisode': 210, 'rewardPerEpisode': 137.75501102452233
'totalSteps': 7680, 'rewardStep': 0.8280875774361908, 'errorList': [], 'lossList': [0.0, -1.3800900673866272, 0.0, 64.05307577133179, 0.0, 0.0, 0.0], 'rewardMean': 0.7520693455028845, 'totalEpisodes': 41, 'stepsPerEpisode': 278, 'rewardPerEpisode': 233.7250120755054
'totalSteps': 8320, 'rewardStep': 0.9362064785877311, 'errorList': [], 'lossList': [0.0, -1.392304779291153, 0.0, 193.53673820495607, 0.0, 0.0, 0.0], 'rewardMean': 0.775503083295027, 'totalEpisodes': 50, 'stepsPerEpisode': 78, 'rewardPerEpisode': 70.13373265063747
'totalSteps': 8960, 'rewardStep': 0.7598535198923438, 'errorList': [], 'lossList': [0.0, -1.3864899682998657, 0.0, 102.66811412811279, 0.0, 0.0, 0.0], 'rewardMean': 0.7844297140666443, 'totalEpisodes': 59, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.873812393264628
'totalSteps': 9600, 'rewardStep': 0.631370610331271, 'errorList': [], 'lossList': [0.0, -1.3774514365196229, 0.0, 68.14084028244018, 0.0, 0.0, 0.0], 'rewardMean': 0.7518777738367673, 'totalEpisodes': 69, 'stepsPerEpisode': 154, 'rewardPerEpisode': 131.43058831955295
'totalSteps': 10240, 'rewardStep': 0.6903835168497958, 'errorList': [], 'lossList': [0.0, -1.3729871249198913, 0.0, 111.19973915100098, 0.0, 0.0, 0.0], 'rewardMean': 0.747745814683914, 'totalEpisodes': 82, 'stepsPerEpisode': 76, 'rewardPerEpisode': 53.88911407382648
'totalSteps': 10880, 'rewardStep': 0.7955922866537327, 'errorList': [], 'lossList': [0.0, -1.361892626285553, 0.0, 62.26023687362671, 0.0, 0.0, 0.0], 'rewardMean': 0.7812681000494663, 'totalEpisodes': 90, 'stepsPerEpisode': 68, 'rewardPerEpisode': 60.599389378979595
'totalSteps': 11520, 'rewardStep': 0.7892667132085047, 'errorList': [], 'lossList': [0.0, -1.3599457395076753, 0.0, 47.23261514663696, 0.0, 0.0, 0.0], 'rewardMean': 0.7721052923827294, 'totalEpisodes': 98, 'stepsPerEpisode': 70, 'rewardPerEpisode': 60.43294931906756
'totalSteps': 12160, 'rewardStep': 0.8717259875034111, 'errorList': [], 'lossList': [0.0, -1.3566506242752074, 0.0, 47.714347953796384, 0.0, 0.0, 0.0], 'rewardMean': 0.7610625481893227, 'totalEpisodes': 102, 'stepsPerEpisode': 53, 'rewardPerEpisode': 43.474946130371706
'totalSteps': 12800, 'rewardStep': 0.7777592953306637, 'errorList': [], 'lossList': [0.0, -1.3498839986324311, 0.0, 65.07036470413207, 0.0, 0.0, 0.0], 'rewardMean': 0.7685210578617492, 'totalEpisodes': 106, 'stepsPerEpisode': 143, 'rewardPerEpisode': 106.87583366596759
'totalSteps': 13440, 'rewardStep': 0.9595976957312847, 'errorList': [11.026730393907538, 13.163022322719739, 4.896638796543076, 4.132643239322911, 4.118897135607148, 2.4652447245352884, 3.978567976215257, 5.822702585308593, 8.99746819647059, 1.2355738697649226, 4.4320943660653125, 3.382393456736064, 1.3205061078739952, 0.11966636008209754, 3.9913516079372866, 2.578873633519461, 5.8327057778398785, 4.223275145920999, 0.7696625244466092, 3.136465169916712, 9.602334427480306, 0.7887844186570729, 2.3650752787566685, 1.531714333559166, 2.2602163646866416, 8.069637589398926, 0.7669331927303235, 3.713842747632448, 3.460832287695566, 6.464540538329994, 8.183587732574555, 0.40621534987215585, 1.5884454855154146, 1.49710748215839, 1.034140543771113, 12.63900431682582, 0.4418355335256232, 5.560297032347184, 1.3487257490210345, 0.716583398162048, 4.985160219582625, 3.181417431632775, 14.1831838024196, 10.739031643717588, 3.5144297757238814, 5.144168404422125, 15.339971345648422, 12.05899217957666, 1.3540720624043259, 1.0488882193596758], 'lossList': [0.0, -1.3381243014335633, 0.0, 19.2916237616539, 0.0, 0.0, 0.0], 'rewardMean': 0.803984368152493, 'totalEpisodes': 106, 'stepsPerEpisode': 640, 'rewardPerEpisode': 426.34056478158544, 'successfulTests': 1
'totalSteps': 14080, 'rewardStep': 0.43147353804224015, 'errorList': [], 'lossList': [0.0, -1.323329163789749, 0.0, 34.23926968574524, 0.0, 0.0, 0.0], 'rewardMean': 0.764322964213098, 'totalEpisodes': 107, 'stepsPerEpisode': 322, 'rewardPerEpisode': 234.33133520370353
'totalSteps': 14720, 'rewardStep': 0.5312873530435993, 'errorList': [], 'lossList': [0.0, -1.3241140472888946, 0.0, 34.71863922119141, 0.0, 0.0, 0.0], 'rewardMean': 0.7238310516586848, 'totalEpisodes': 109, 'stepsPerEpisode': 466, 'rewardPerEpisode': 305.8781218943332
