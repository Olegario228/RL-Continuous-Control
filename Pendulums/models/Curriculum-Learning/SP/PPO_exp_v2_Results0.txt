#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 256, 'episodeStepsMax': 320, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v2_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v2_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[1, 6], [0, 3], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 320, 'rewardStep': 0.6720075809172104, 'errorList': [], 'lossList': [0.0, -1.412278392314911, 0.0, 135.86974609375, 0.0, 0.0, 0.0], 'rewardMean': 0.6720075809172104, 'totalEpisodes': 2, 'stepsPerEpisode': 187, 'rewardPerEpisode': 157.1475997900463
'totalSteps': 640, 'rewardStep': 0.944238113784023, 'errorList': [], 'lossList': [0.0, -1.407355604171753, 0.0, 122.30580017089844, 0.0, 0.0, 0.0], 'rewardMean': 0.8081228473506167, 'totalEpisodes': 4, 'stepsPerEpisode': 92, 'rewardPerEpisode': 73.06998977028736
'totalSteps': 960, 'rewardStep': 0.8749725400644417, 'errorList': [], 'lossList': [0.0, -1.4146649408340455, 0.0, 94.64452491760254, 0.0, 0.0, 0.0], 'rewardMean': 0.830406078255225, 'totalEpisodes': 4, 'stepsPerEpisode': 320, 'rewardPerEpisode': 228.75067659193252
'totalSteps': 1280, 'rewardStep': 0.843159727353632, 'errorList': [], 'lossList': [0.0, -1.4195534181594849, 0.0, 102.64369873046876, 0.0, 0.0, 0.0], 'rewardMean': 0.8335944905298268, 'totalEpisodes': 4, 'stepsPerEpisode': 320, 'rewardPerEpisode': 250.35551311117297
'totalSteps': 1600, 'rewardStep': 0.5005609327655471, 'errorList': [], 'lossList': [0.0, -1.4222427105903626, 0.0, 69.89715049743653, 0.0, 0.0, 0.0], 'rewardMean': 0.7669877789769709, 'totalEpisodes': 4, 'stepsPerEpisode': 320, 'rewardPerEpisode': 229.94891539600482
'totalSteps': 1920, 'rewardStep': 0.5018145711477094, 'errorList': [], 'lossList': [0.0, -1.4284472513198851, 0.0, 57.617498626708986, 0.0, 0.0, 0.0], 'rewardMean': 0.7227922443387605, 'totalEpisodes': 7, 'stepsPerEpisode': 56, 'rewardPerEpisode': 37.206488227773924
'totalSteps': 2240, 'rewardStep': 0.9305389520278191, 'errorList': [], 'lossList': [0.0, -1.441420295238495, 0.0, 61.77714408874512, 0.0, 0.0, 0.0], 'rewardMean': 0.7524703454371975, 'totalEpisodes': 11, 'stepsPerEpisode': 36, 'rewardPerEpisode': 33.014978874315894
'totalSteps': 2560, 'rewardStep': 0.6391202761732839, 'errorList': [], 'lossList': [0.0, -1.4533438611030578, 0.0, 63.73336044311523, 0.0, 0.0, 0.0], 'rewardMean': 0.7383015867792083, 'totalEpisodes': 13, 'stepsPerEpisode': 93, 'rewardPerEpisode': 70.68210422465289
'totalSteps': 2880, 'rewardStep': 0.7330688754858815, 'errorList': [], 'lossList': [0.0, -1.44620187997818, 0.0, 82.37703300476075, 0.0, 0.0, 0.0], 'rewardMean': 0.7377201744132831, 'totalEpisodes': 15, 'stepsPerEpisode': 71, 'rewardPerEpisode': 55.09152600619994
'totalSteps': 3200, 'rewardStep': 0.7744116219119951, 'errorList': [], 'lossList': [0.0, -1.4319267320632934, 0.0, 66.3003458404541, 0.0, 0.0, 0.0], 'rewardMean': 0.7413893191631543, 'totalEpisodes': 17, 'stepsPerEpisode': 196, 'rewardPerEpisode': 144.283800731936
'totalSteps': 3520, 'rewardStep': 0.7605152748217852, 'errorList': [], 'lossList': [0.0, -1.4274401140213013, 0.0, 41.9813007736206, 0.0, 0.0, 0.0], 'rewardMean': 0.7502400885536119, 'totalEpisodes': 18, 'stepsPerEpisode': 26, 'rewardPerEpisode': 19.616491416894107
'totalSteps': 3840, 'rewardStep': 0.5723247401790628, 'errorList': [], 'lossList': [0.0, -1.4370355796813965, 0.0, 83.92988006591797, 0.0, 0.0, 0.0], 'rewardMean': 0.7130487511931157, 'totalEpisodes': 21, 'stepsPerEpisode': 23, 'rewardPerEpisode': 16.518815750463833
'totalSteps': 4160, 'rewardStep': 0.5039717549822491, 'errorList': [], 'lossList': [0.0, -1.456707170009613, 0.0, 42.78404022216797, 0.0, 0.0, 0.0], 'rewardMean': 0.6759486726848966, 'totalEpisodes': 22, 'stepsPerEpisode': 316, 'rewardPerEpisode': 215.19266732534163
'totalSteps': 4480, 'rewardStep': 0.7129846832821825, 'errorList': [], 'lossList': [0.0, -1.4515428638458252, 0.0, 24.00973731994629, 0.0, 0.0, 0.0], 'rewardMean': 0.6629311682777516, 'totalEpisodes': 23, 'stepsPerEpisode': 29, 'rewardPerEpisode': 20.579723103120326
'totalSteps': 4800, 'rewardStep': 0.4877034767059413, 'errorList': [], 'lossList': [0.0, -1.4460970401763915, 0.0, 39.86820213317871, 0.0, 0.0, 0.0], 'rewardMean': 0.6616454226717909, 'totalEpisodes': 24, 'stepsPerEpisode': 147, 'rewardPerEpisode': 86.64313388129723
'totalSteps': 5120, 'rewardStep': 0.8323324591667056, 'errorList': [], 'lossList': [0.0, -1.4521688985824586, 0.0, 20.749658336639403, 0.0, 0.0, 0.0], 'rewardMean': 0.6946972114736906, 'totalEpisodes': 24, 'stepsPerEpisode': 320, 'rewardPerEpisode': 211.70743727855324
'totalSteps': 5440, 'rewardStep': 0.9333708086193722, 'errorList': [], 'lossList': [0.0, -1.4552736067771912, 0.0, 42.43279193878174, 0.0, 0.0, 0.0], 'rewardMean': 0.6949803971328459, 'totalEpisodes': 24, 'stepsPerEpisode': 320, 'rewardPerEpisode': 261.0517036163079
'totalSteps': 5760, 'rewardStep': 0.7368697344715834, 'errorList': [], 'lossList': [0.0, -1.4519675016403197, 0.0, 31.463675994873046, 0.0, 0.0, 0.0], 'rewardMean': 0.7047553429626758, 'totalEpisodes': 24, 'stepsPerEpisode': 320, 'rewardPerEpisode': 249.70254272852605
'totalSteps': 6080, 'rewardStep': 0.5680370169517746, 'errorList': [], 'lossList': [0.0, -1.4487631297111512, 0.0, 24.33025653839111, 0.0, 0.0, 0.0], 'rewardMean': 0.6882521571092652, 'totalEpisodes': 24, 'stepsPerEpisode': 320, 'rewardPerEpisode': 237.04586534198404
'totalSteps': 6400, 'rewardStep': 0.8908233583464846, 'errorList': [], 'lossList': [0.0, -1.4487769389152527, 0.0, 25.55907917022705, 0.0, 0.0, 0.0], 'rewardMean': 0.6998933307527141, 'totalEpisodes': 24, 'stepsPerEpisode': 320, 'rewardPerEpisode': 240.0392628774722
'totalSteps': 6720, 'rewardStep': 0.7218958379023774, 'errorList': [], 'lossList': [0.0, -1.4456597208976745, 0.0, 36.736283226013185, 0.0, 0.0, 0.0], 'rewardMean': 0.6960313870607734, 'totalEpisodes': 24, 'stepsPerEpisode': 320, 'rewardPerEpisode': 275.2693586992983
'totalSteps': 7040, 'rewardStep': 0.8269095822370527, 'errorList': [], 'lossList': [0.0, -1.4454501938819886, 0.0, 28.264073638916017, 0.0, 0.0, 0.0], 'rewardMean': 0.7214898712665724, 'totalEpisodes': 24, 'stepsPerEpisode': 320, 'rewardPerEpisode': 263.4234193398947
'totalSteps': 7360, 'rewardStep': 0.865057538167835, 'errorList': [], 'lossList': [0.0, -1.4360555601119995, 0.0, 70.48725944519043, 0.0, 0.0, 0.0], 'rewardMean': 0.7575984495851309, 'totalEpisodes': 25, 'stepsPerEpisode': 79, 'rewardPerEpisode': 64.9335684033785
'totalSteps': 7680, 'rewardStep': 0.6971545609364584, 'errorList': [], 'lossList': [0.0, -1.4157487487792968, 0.0, 122.72201629638671, 0.0, 0.0, 0.0], 'rewardMean': 0.7560154373505584, 'totalEpisodes': 27, 'stepsPerEpisode': 9, 'rewardPerEpisode': 5.66788930912326
'totalSteps': 8000, 'rewardStep': 0.7972810753950352, 'errorList': [], 'lossList': [0.0, -1.4083198046684264, 0.0, 172.5465054321289, 0.0, 0.0, 0.0], 'rewardMean': 0.7869731972194679, 'totalEpisodes': 30, 'stepsPerEpisode': 41, 'rewardPerEpisode': 36.244391519404154
'totalSteps': 8320, 'rewardStep': 0.9695606950141162, 'errorList': [], 'lossList': [0.0, -1.4033811831474303, 0.0, 274.30104461669924, 0.0, 0.0, 0.0], 'rewardMean': 0.800696020804209, 'totalEpisodes': 36, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.842494171304257
'totalSteps': 8640, 'rewardStep': 0.5924305679251476, 'errorList': [], 'lossList': [0.0, -1.4011901664733886, 0.0, 292.73201354980466, 0.0, 0.0, 0.0], 'rewardMean': 0.7666019967347865, 'totalEpisodes': 43, 'stepsPerEpisode': 65, 'rewardPerEpisode': 49.14435609260637
'totalSteps': 8960, 'rewardStep': 0.6873633313948331, 'errorList': [], 'lossList': [0.0, -1.3994039916992187, 0.0, 217.61471771240235, 0.0, 0.0, 0.0], 'rewardMean': 0.7616513564271115, 'totalEpisodes': 48, 'stepsPerEpisode': 25, 'rewardPerEpisode': 17.384118077371497
'totalSteps': 9280, 'rewardStep': 0.9023115193450519, 'errorList': [], 'lossList': [0.0, -1.3906457662582397, 0.0, 187.68407623291014, 0.0, 0.0, 0.0], 'rewardMean': 0.7950788066664393, 'totalEpisodes': 53, 'stepsPerEpisode': 43, 'rewardPerEpisode': 39.9049634232052
'totalSteps': 9600, 'rewardStep': 0.1755778928718028, 'errorList': [], 'lossList': [0.0, -1.3724989652633668, 0.0, 93.31062446594238, 0.0, 0.0, 0.0], 'rewardMean': 0.723554260118971, 'totalEpisodes': 55, 'stepsPerEpisode': 133, 'rewardPerEpisode': 85.57605803522613
'totalSteps': 9920, 'rewardStep': 0.7324446175858771, 'errorList': [], 'lossList': [0.0, -1.3598229146003724, 0.0, 10.795722942352295, 0.0, 0.0, 0.0], 'rewardMean': 0.724609138087321, 'totalEpisodes': 58, 'stepsPerEpisode': 58, 'rewardPerEpisode': 37.326898135325735
'totalSteps': 10240, 'rewardStep': 0.635505894826502, 'errorList': [], 'lossList': [0.0, -1.359844355583191, 0.0, 87.44259567260742, 0.0, 0.0, 0.0], 'rewardMean': 0.7054687693462658, 'totalEpisodes': 61, 'stepsPerEpisode': 151, 'rewardPerEpisode': 127.63364654831054
'totalSteps': 10560, 'rewardStep': 0.8129563847908661, 'errorList': [], 'lossList': [0.0, -1.358651328086853, 0.0, 65.20083358764649, 0.0, 0.0, 0.0], 'rewardMean': 0.7002586540085691, 'totalEpisodes': 62, 'stepsPerEpisode': 224, 'rewardPerEpisode': 178.1580738427322
'totalSteps': 10880, 'rewardStep': 0.5082964299658858, 'errorList': [], 'lossList': [0.0, -1.3595545744895936, 0.0, 23.25094814300537, 0.0, 0.0, 0.0], 'rewardMean': 0.6813728409115117, 'totalEpisodes': 63, 'stepsPerEpisode': 65, 'rewardPerEpisode': 42.97396417007325
'totalSteps': 11200, 'rewardStep': 0.2720089452087978, 'errorList': [], 'lossList': [0.0, -1.359560055732727, 0.0, 127.59716369628906, 0.0, 0.0, 0.0], 'rewardMean': 0.6288456278928881, 'totalEpisodes': 66, 'stepsPerEpisode': 169, 'rewardPerEpisode': 108.24217904784017
'totalSteps': 11520, 'rewardStep': 0.5127345300802855, 'errorList': [], 'lossList': [0.0, -1.356111192703247, 0.0, 64.62208690643311, 0.0, 0.0, 0.0], 'rewardMean': 0.583163011399505, 'totalEpisodes': 68, 'stepsPerEpisode': 73, 'rewardPerEpisode': 57.274132489219504
'totalSteps': 11840, 'rewardStep': 0.39880855942772575, 'errorList': [], 'lossList': [0.0, -1.3491078066825866, 0.0, 139.71911613464354, 0.0, 0.0, 0.0], 'rewardMean': 0.5638008105497627, 'totalEpisodes': 72, 'stepsPerEpisode': 112, 'rewardPerEpisode': 75.03821226888175
'totalSteps': 12160, 'rewardStep': 0.48311877335070763, 'errorList': [], 'lossList': [0.0, -1.3504807353019714, 0.0, 33.09051628112793, 0.0, 0.0, 0.0], 'rewardMean': 0.5433763547453502, 'totalEpisodes': 73, 'stepsPerEpisode': 182, 'rewardPerEpisode': 110.0391436710329
'totalSteps': 12480, 'rewardStep': 0.46044892264818693, 'errorList': [], 'lossList': [0.0, -1.3462510132789611, 0.0, 118.03428466796875, 0.0, 0.0, 0.0], 'rewardMean': 0.4991900950756637, 'totalEpisodes': 75, 'stepsPerEpisode': 151, 'rewardPerEpisode': 118.09720298753494
'totalSteps': 12800, 'rewardStep': 0.45760457491207107, 'errorList': [], 'lossList': [0.0, -1.3495225358009337, 0.0, 112.8187387084961, 0.0, 0.0, 0.0], 'rewardMean': 0.5273927632796905, 'totalEpisodes': 78, 'stepsPerEpisode': 48, 'rewardPerEpisode': 26.673403465325702
'totalSteps': 13120, 'rewardStep': 0.2101968198138076, 'errorList': [], 'lossList': [0.0, -1.3506137108802796, 0.0, 134.79585601806642, 0.0, 0.0, 0.0], 'rewardMean': 0.4751679835024836, 'totalEpisodes': 81, 'stepsPerEpisode': 78, 'rewardPerEpisode': 51.34602394185309
'totalSteps': 13440, 'rewardStep': 0.9169896599742158, 'errorList': [], 'lossList': [0.0, -1.3539760041236877, 0.0, 74.46324569702148, 0.0, 0.0, 0.0], 'rewardMean': 0.503316360017255, 'totalEpisodes': 84, 'stepsPerEpisode': 74, 'rewardPerEpisode': 68.39438320664497
'totalSteps': 13760, 'rewardStep': 0.9821515739250124, 'errorList': [242.27482313232912, 197.41956375164142, 126.53670351981121, 202.11915759419307, 232.37165808061604, 197.59622535092518, 179.51485662947576, 222.1004781504832, 205.42105273632063, 261.42688149374766, 115.07284437753962, 118.70815067186547, 10.136422091674758, 200.57121292401547, 239.31765208220517, 244.72543291958402, 245.4545552520134, 229.98293681196213, 236.79920207228614, 242.6976979386615, 248.22476355798887, 253.7178042127456, 147.40619927255838, 249.07205302226356, 83.03282086274854, 171.56977739371717, 221.89740484734583, 172.55548003856163, 68.08606723209337, 284.11168785400287, 226.39683677165897, 175.95835573923645, 243.76690438682678, 4.51321194702119, 252.83311646255018, 189.88644870966584, 24.034205554648814, 166.725315576908, 28.285947687860435, 131.81027260639695, 24.729493412150994, 92.46673539459829, 195.11892500633806, 157.85857954086558, 237.9887148853394, 209.15722809014036, 223.11181767838454, 217.6815997660681, 169.7972392540396, 254.40144579445905], 'lossList': [0.0, -1.356382269859314, 0.0, 140.21496673583985, 0.0, 0.0, 0.0], 'rewardMean': 0.5202358789306697, 'totalEpisodes': 87, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.610089504983474, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.5315441659986144, 'errorList': [], 'lossList': [0.0, -1.3541886925697326, 0.0, 98.03689155578613, 0.0, 0.0, 0.0], 'rewardMean': 0.5225606525339425, 'totalEpisodes': 89, 'stepsPerEpisode': 161, 'rewardPerEpisode': 121.15780973493774
'totalSteps': 14400, 'rewardStep': 0.9097256169127503, 'errorList': [], 'lossList': [0.0, -1.3562609696388244, 0.0, 153.06810806274413, 0.0, 0.0, 0.0], 'rewardMean': 0.5863323197043376, 'totalEpisodes': 92, 'stepsPerEpisode': 90, 'rewardPerEpisode': 80.0603888113096
'totalSteps': 14720, 'rewardStep': 0.5054423262639668, 'errorList': [], 'lossList': [0.0, -1.3630451893806457, 0.0, 68.62020851135254, 0.0, 0.0, 0.0], 'rewardMean': 0.5856030993227057, 'totalEpisodes': 93, 'stepsPerEpisode': 269, 'rewardPerEpisode': 206.722377302488
'totalSteps': 15040, 'rewardStep': 0.8080020516917309, 'errorList': [], 'lossList': [0.0, -1.3634824109077455, 0.0, 77.039104347229, 0.0, 0.0, 0.0], 'rewardMean': 0.6265224485491064, 'totalEpisodes': 95, 'stepsPerEpisode': 74, 'rewardPerEpisode': 56.75726267783415
'totalSteps': 15360, 'rewardStep': 0.6576633619367206, 'errorList': [], 'lossList': [0.0, -1.3651874685287475, 0.0, 23.51921106338501, 0.0, 0.0, 0.0], 'rewardMean': 0.6439769074077077, 'totalEpisodes': 97, 'stepsPerEpisode': 70, 'rewardPerEpisode': 56.341934389607765
'totalSteps': 15680, 'rewardStep': 0.6610932507814185, 'errorList': [], 'lossList': [0.0, -1.3639750981330871, 0.0, 25.98091983795166, 0.0, 0.0, 0.0], 'rewardMean': 0.6640413402210308, 'totalEpisodes': 97, 'stepsPerEpisode': 320, 'rewardPerEpisode': 254.5123026504838
'totalSteps': 16000, 'rewardStep': 0.7536263495162424, 'errorList': [], 'lossList': [0.0, -1.36776638507843, 0.0, 14.650710010528565, 0.0, 0.0, 0.0], 'rewardMean': 0.6936435176814479, 'totalEpisodes': 98, 'stepsPerEpisode': 229, 'rewardPerEpisode': 175.46562570718686
'totalSteps': 16320, 'rewardStep': 0.796343876850668, 'errorList': [], 'lossList': [0.0, -1.3698256874084473, 0.0, 61.59982208251953, 0.0, 0.0, 0.0], 'rewardMean': 0.7522582233851339, 'totalEpisodes': 100, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.786026241177822
'totalSteps': 16640, 'rewardStep': 0.479225646575063, 'errorList': [], 'lossList': [0.0, -1.3663727068901061, 0.0, 32.03805221557617, 0.0, 0.0, 0.0], 'rewardMean': 0.7084818220452187, 'totalEpisodes': 100, 'stepsPerEpisode': 320, 'rewardPerEpisode': 256.5679989852427
'totalSteps': 16960, 'rewardStep': 0.7687451722063862, 'errorList': [], 'lossList': [0.0, -1.367760398387909, 0.0, 174.34424987792968, 0.0, 0.0, 0.0], 'rewardMean': 0.6871411818733562, 'totalEpisodes': 104, 'stepsPerEpisode': 38, 'rewardPerEpisode': 25.69906168455712
'totalSteps': 17280, 'rewardStep': 0.8356173562965554, 'errorList': [], 'lossList': [0.0, -1.3708035373687744, 0.0, 117.35214599609375, 0.0, 0.0, 0.0], 'rewardMean': 0.7175485009031501, 'totalEpisodes': 106, 'stepsPerEpisode': 76, 'rewardPerEpisode': 67.0078463560893
'totalSteps': 17600, 'rewardStep': 0.8688009101233075, 'errorList': [], 'lossList': [0.0, -1.3670187449455262, 0.0, 118.86407020568848, 0.0, 0.0, 0.0], 'rewardMean': 0.7134560302242059, 'totalEpisodes': 108, 'stepsPerEpisode': 21, 'rewardPerEpisode': 18.622366841102103
'totalSteps': 17920, 'rewardStep': 0.866205552328321, 'errorList': [], 'lossList': [0.0, -1.3622787475585938, 0.0, 87.44920249938964, 0.0, 0.0, 0.0], 'rewardMean': 0.7495323528306412, 'totalEpisodes': 109, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.293613966717107
'totalSteps': 18240, 'rewardStep': 0.8041082953226477, 'errorList': [], 'lossList': [0.0, -1.366019194126129, 0.0, 22.277170658111572, 0.0, 0.0, 0.0], 'rewardMean': 0.7491429771937329, 'totalEpisodes': 109, 'stepsPerEpisode': 320, 'rewardPerEpisode': 247.03419876175974
'totalSteps': 18560, 'rewardStep': 0.435272140023642, 'errorList': [], 'lossList': [0.0, -1.36358873128891, 0.0, 66.12094848632813, 0.0, 0.0, 0.0], 'rewardMean': 0.726903855002425, 'totalEpisodes': 110, 'stepsPerEpisode': 182, 'rewardPerEpisode': 120.93660053331276
'totalSteps': 18880, 'rewardStep': 0.9024627957307169, 'errorList': [], 'lossList': [0.0, -1.3694744873046876, 0.0, 93.1551774597168, 0.0, 0.0, 0.0], 'rewardMean': 0.751040809497355, 'totalEpisodes': 111, 'stepsPerEpisode': 208, 'rewardPerEpisode': 179.36788734732812
'totalSteps': 19200, 'rewardStep': 0.7221014347869031, 'errorList': [], 'lossList': [0.0, -1.367175738811493, 0.0, 78.75073455810546, 0.0, 0.0, 0.0], 'rewardMean': 0.7478883180244209, 'totalEpisodes': 112, 'stepsPerEpisode': 156, 'rewardPerEpisode': 118.17201281943679
'totalSteps': 19520, 'rewardStep': 0.4955772902933144, 'errorList': [], 'lossList': [0.0, -1.3746790742874146, 0.0, 17.774326095581056, 0.0, 0.0, 0.0], 'rewardMean': 0.7178116593686857, 'totalEpisodes': 112, 'stepsPerEpisode': 320, 'rewardPerEpisode': 244.5004515532306
'totalSteps': 19840, 'rewardStep': 0.7538616656877651, 'errorList': [], 'lossList': [0.0, -1.3754768514633178, 0.0, 70.9100372505188, 0.0, 0.0, 0.0], 'rewardMean': 0.745275261279956, 'totalEpisodes': 113, 'stepsPerEpisode': 221, 'rewardPerEpisode': 135.50545563920255
'totalSteps': 20160, 'rewardStep': 0.6579310208369731, 'errorList': [], 'lossList': [0.0, -1.3768786334991454, 0.0, 77.3847052192688, 0.0, 0.0, 0.0], 'rewardMean': 0.7341938461430146, 'totalEpisodes': 115, 'stepsPerEpisode': 27, 'rewardPerEpisode': 20.685200731808177
'totalSteps': 20480, 'rewardStep': 0.6297704635788773, 'errorList': [], 'lossList': [0.0, -1.3713516807556152, 0.0, 140.53382026672364, 0.0, 0.0, 0.0], 'rewardMean': 0.7136091568712468, 'totalEpisodes': 117, 'stepsPerEpisode': 68, 'rewardPerEpisode': 46.39403936230021
'totalSteps': 20800, 'rewardStep': 0.6548442195253544, 'errorList': [], 'lossList': [0.0, -1.3636263608932495, 0.0, 73.55490131378174, 0.0, 0.0, 0.0], 'rewardMean': 0.6922134878114515, 'totalEpisodes': 118, 'stepsPerEpisode': 148, 'rewardPerEpisode': 105.09881234620778
'totalSteps': 21120, 'rewardStep': 0.943009122086763, 'errorList': [11.395301291142355, 11.904653438297736, 10.057759483211854, 13.10117723364956, 7.743283771267051, 12.103495767181165, 0.39195400546755604, 6.359757872467746, 1.433787363591391, 16.074803051350884, 9.362324675917629, 16.746747125975652, 15.786158021659041, 8.119601786582715, 6.173742976669031, 4.955730816149482, 17.37379028853537, 7.6813378540799775, 14.242248916612008, 16.78087261419319, 4.2734912969032814, 6.642225406151275, 9.469407086026457, 13.849908010410783, 10.14484733910498, 12.958591257309207, 12.325516106015874, 7.0339537100755996, 1.9913825819934035, 12.352150851682117, 9.896020782371, 12.629636311473568, 15.525766586667213, 12.988962282401204, 5.276534123533971, 16.481014799010502, 5.155130463844812, 13.412789283589548, 14.259713092296847, 15.487860317840271, 5.223887748111739, 16.86622581145494, 14.150315278550075, 5.037696624978171, 10.544370235932064, 10.637067012747265, 11.502120580863762, 16.264872301023484, 13.339351899126736, 12.773625881823532], 'lossList': [0.0, -1.3706939792633057, 0.0, 8.393024339675904, 0.0, 0.0, 0.0], 'rewardMean': 0.6998938447872957, 'totalEpisodes': 119, 'stepsPerEpisode': 29, 'rewardPerEpisode': 27.180405957617857, 'successfulTests': 0
'totalSteps': 21440, 'rewardStep': 0.6120822299914228, 'errorList': [], 'lossList': [0.0, -1.3787045097351074, 0.0, 82.82932765960693, 0.0, 0.0, 0.0], 'rewardMean': 0.6806912382541732, 'totalEpisodes': 120, 'stepsPerEpisode': 72, 'rewardPerEpisode': 57.07007266065602
'totalSteps': 21760, 'rewardStep': 0.8953314603723613, 'errorList': [], 'lossList': [0.0, -1.3939016723632813, 0.0, 84.1981861114502, 0.0, 0.0, 0.0], 'rewardMean': 0.7266971702890451, 'totalEpisodes': 121, 'stepsPerEpisode': 196, 'rewardPerEpisode': 168.1752649326492
'totalSteps': 22080, 'rewardStep': 0.830211461294349, 'errorList': [], 'lossList': [0.0, -1.4002538657188415, 0.0, 22.898865909576415, 0.0, 0.0, 0.0], 'rewardMean': 0.7194720368454084, 'totalEpisodes': 121, 'stepsPerEpisode': 320, 'rewardPerEpisode': 266.4607135460514
'totalSteps': 22400, 'rewardStep': 0.678995816477781, 'errorList': [], 'lossList': [0.0, -1.402239956855774, 0.0, 85.29413452148438, 0.0, 0.0, 0.0], 'rewardMean': 0.7151614750144961, 'totalEpisodes': 122, 'stepsPerEpisode': 192, 'rewardPerEpisode': 144.29299515823604
'totalSteps': 22720, 'rewardStep': 0.8621509255207259, 'errorList': [], 'lossList': [0.0, -1.4017469215393066, 0.0, 17.30950532913208, 0.0, 0.0, 0.0], 'rewardMean': 0.7518188385372373, 'totalEpisodes': 123, 'stepsPerEpisode': 206, 'rewardPerEpisode': 167.0501382602489
'totalSteps': 23040, 'rewardStep': 0.6103815523163386, 'errorList': [], 'lossList': [0.0, -1.401575255393982, 0.0, 93.06924629211426, 0.0, 0.0, 0.0], 'rewardMean': 0.7374708272000946, 'totalEpisodes': 124, 'stepsPerEpisode': 119, 'rewardPerEpisode': 92.48705748318281
'totalSteps': 23360, 'rewardStep': 0.8633366895927288, 'errorList': [], 'lossList': [0.0, -1.4062287783622742, 0.0, 7.84391230583191, 0.0, 0.0, 0.0], 'rewardMean': 0.7580113940756703, 'totalEpisodes': 125, 'stepsPerEpisode': 88, 'rewardPerEpisode': 68.5883975299015
'totalSteps': 23680, 'rewardStep': 0.7765947902013319, 'errorList': [], 'lossList': [0.0, -1.4050698018074035, 0.0, 16.342601642608642, 0.0, 0.0, 0.0], 'rewardMean': 0.7726938267379158, 'totalEpisodes': 125, 'stepsPerEpisode': 320, 'rewardPerEpisode': 260.69839844061255
'totalSteps': 24000, 'rewardStep': 0.8203529351723416, 'errorList': [], 'lossList': [0.0, -1.3982468914985657, 0.0, 104.11528129577637, 0.0, 0.0, 0.0], 'rewardMean': 0.7892446983026145, 'totalEpisodes': 126, 'stepsPerEpisode': 189, 'rewardPerEpisode': 164.3925956223451
'totalSteps': 24320, 'rewardStep': 0.7607651590061475, 'errorList': [], 'lossList': [0.0, -1.3866110587120055, 0.0, 15.081456298828124, 0.0, 0.0, 0.0], 'rewardMean': 0.771020301994553, 'totalEpisodes': 127, 'stepsPerEpisode': 85, 'rewardPerEpisode': 63.25594633781173
'totalSteps': 24640, 'rewardStep': 0.6313653228003338, 'errorList': [], 'lossList': [0.0, -1.3868199014663696, 0.0, 8.79981584072113, 0.0, 0.0, 0.0], 'rewardMean': 0.7729486112754439, 'totalEpisodes': 128, 'stepsPerEpisode': 237, 'rewardPerEpisode': 164.85896410563618
'totalSteps': 24960, 'rewardStep': 0.7727788820145526, 'errorList': [], 'lossList': [0.0, -1.3870647692680358, 0.0, 1.602366052865982, 0.0, 0.0, 0.0], 'rewardMean': 0.760693353439663, 'totalEpisodes': 128, 'stepsPerEpisode': 320, 'rewardPerEpisode': 205.32728269527388
