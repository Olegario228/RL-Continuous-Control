#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 512, 'episodeStepsMax': 640, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v3_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v3_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000, 14000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 640, 'rewardStep': 0.9691915654732828, 'errorList': [], 'lossList': [0.0, -1.4229459869861603, 0.0, 120.92727294921875, 0.0, 0.0, 0.0], 'rewardMean': 0.9691915654732828, 'totalEpisodes': 3, 'stepsPerEpisode': 91, 'rewardPerEpisode': 67.50772086281113
'totalSteps': 1280, 'rewardStep': 0.9225367910610006, 'errorList': [], 'lossList': [0.0, -1.435627635717392, 0.0, 82.72586112976074, 0.0, 0.0, 0.0], 'rewardMean': 0.9458641782671418, 'totalEpisodes': 4, 'stepsPerEpisode': 591, 'rewardPerEpisode': 439.08631285990435
'totalSteps': 1920, 'rewardStep': 0.6320647027529609, 'errorList': [], 'lossList': [0.0, -1.4355569767951966, 0.0, 50.34507365226746, 0.0, 0.0, 0.0], 'rewardMean': 0.8412643530957481, 'totalEpisodes': 4, 'stepsPerEpisode': 640, 'rewardPerEpisode': 469.51701776629113
'totalSteps': 2560, 'rewardStep': 0.6505350016843607, 'errorList': [], 'lossList': [0.0, -1.4236077773571014, 0.0, 38.412275371551516, 0.0, 0.0, 0.0], 'rewardMean': 0.7935820152429013, 'totalEpisodes': 7, 'stepsPerEpisode': 90, 'rewardPerEpisode': 68.7489677028808
'totalSteps': 3200, 'rewardStep': 0.6635913275382515, 'errorList': [], 'lossList': [0.0, -1.4206316101551055, 0.0, 32.06141621589661, 0.0, 0.0, 0.0], 'rewardMean': 0.7675838777019713, 'totalEpisodes': 8, 'stepsPerEpisode': 218, 'rewardPerEpisode': 154.63670875320096
'totalSteps': 3840, 'rewardStep': 0.39415409216050956, 'errorList': [], 'lossList': [0.0, -1.4075989294052125, 0.0, 50.34122131347656, 0.0, 0.0, 0.0], 'rewardMean': 0.7053455801117278, 'totalEpisodes': 11, 'stepsPerEpisode': 153, 'rewardPerEpisode': 105.09984025919114
'totalSteps': 4480, 'rewardStep': 0.4987279672505356, 'errorList': [], 'lossList': [0.0, -1.4073328614234923, 0.0, 34.286966953277584, 0.0, 0.0, 0.0], 'rewardMean': 0.6758287782744146, 'totalEpisodes': 12, 'stepsPerEpisode': 635, 'rewardPerEpisode': 483.36849662877887
'totalSteps': 5120, 'rewardStep': 0.8197812726869432, 'errorList': [], 'lossList': [0.0, -1.4020106732845306, 0.0, 29.249950828552247, 0.0, 0.0, 0.0], 'rewardMean': 0.6938228400759807, 'totalEpisodes': 12, 'stepsPerEpisode': 640, 'rewardPerEpisode': 487.1788492652107
'totalSteps': 5760, 'rewardStep': 0.8097464261791008, 'errorList': [], 'lossList': [0.0, -1.4025432240962983, 0.0, 30.542036371231077, 0.0, 0.0, 0.0], 'rewardMean': 0.7067032385318829, 'totalEpisodes': 12, 'stepsPerEpisode': 640, 'rewardPerEpisode': 521.6813867614727
'totalSteps': 6400, 'rewardStep': 0.7780836199421288, 'errorList': [], 'lossList': [0.0, -1.4039935541152955, 0.0, 18.04064197063446, 0.0, 0.0, 0.0], 'rewardMean': 0.7138412766729075, 'totalEpisodes': 12, 'stepsPerEpisode': 640, 'rewardPerEpisode': 489.51325821494993
'totalSteps': 7040, 'rewardStep': 0.7483106506705806, 'errorList': [], 'lossList': [0.0, -1.401037414073944, 0.0, 19.077071685791015, 0.0, 0.0, 0.0], 'rewardMean': 0.6917531851926372, 'totalEpisodes': 12, 'stepsPerEpisode': 640, 'rewardPerEpisode': 518.9775498888683
'totalSteps': 7680, 'rewardStep': 0.8458645790086663, 'errorList': [], 'lossList': [0.0, -1.3878318929672242, 0.0, 19.656403584480287, 0.0, 0.0, 0.0], 'rewardMean': 0.6840859639874038, 'totalEpisodes': 12, 'stepsPerEpisode': 640, 'rewardPerEpisode': 543.3334575736375
'totalSteps': 8320, 'rewardStep': 0.5849237959392662, 'errorList': [], 'lossList': [0.0, -1.3722860288619996, 0.0, 11.579029178619384, 0.0, 0.0, 0.0], 'rewardMean': 0.6793718733060344, 'totalEpisodes': 12, 'stepsPerEpisode': 640, 'rewardPerEpisode': 506.7373085159766
'totalSteps': 8960, 'rewardStep': 0.930061644242598, 'errorList': [], 'lossList': [0.0, -1.3641306483745574, 0.0, 47.04509885787964, 0.0, 0.0, 0.0], 'rewardMean': 0.7073245375618581, 'totalEpisodes': 13, 'stepsPerEpisode': 502, 'rewardPerEpisode': 361.1045115692844
'totalSteps': 9600, 'rewardStep': 0.7123578261389684, 'errorList': [], 'lossList': [0.0, -1.3525067615509032, 0.0, 240.6659130859375, 0.0, 0.0, 0.0], 'rewardMean': 0.7122011874219298, 'totalEpisodes': 19, 'stepsPerEpisode': 229, 'rewardPerEpisode': 171.3637037326598
'totalSteps': 10240, 'rewardStep': 0.6082542710723005, 'errorList': [], 'lossList': [0.0, -1.3459681570529938, 0.0, 314.61856170654295, 0.0, 0.0, 0.0], 'rewardMean': 0.7336112053131087, 'totalEpisodes': 29, 'stepsPerEpisode': 57, 'rewardPerEpisode': 39.37807870171138
'totalSteps': 10880, 'rewardStep': 0.7631216484570592, 'errorList': [], 'lossList': [0.0, -1.345021494626999, 0.0, 418.9765802001953, 0.0, 0.0, 0.0], 'rewardMean': 0.7600505734337613, 'totalEpisodes': 45, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.7631216484570592
'totalSteps': 11520, 'rewardStep': 0.8178686651482625, 'errorList': [], 'lossList': [0.0, -1.3409162294864654, 0.0, 291.88832092285156, 0.0, 0.0, 0.0], 'rewardMean': 0.7598593126798932, 'totalEpisodes': 58, 'stepsPerEpisode': 48, 'rewardPerEpisode': 40.99411633080728
'totalSteps': 12160, 'rewardStep': 0.679344117993403, 'errorList': [], 'lossList': [0.0, -1.3318922650814056, 0.0, 202.32376861572266, 0.0, 0.0, 0.0], 'rewardMean': 0.7468190818613233, 'totalEpisodes': 69, 'stepsPerEpisode': 54, 'rewardPerEpisode': 39.10395150389621
'totalSteps': 12800, 'rewardStep': 0.8108864508112936, 'errorList': [], 'lossList': [0.0, -1.3240051221847535, 0.0, 114.97784664154052, 0.0, 0.0, 0.0], 'rewardMean': 0.7500993649482398, 'totalEpisodes': 77, 'stepsPerEpisode': 122, 'rewardPerEpisode': 93.88688819492215
'totalSteps': 13440, 'rewardStep': 0.7446795489674389, 'errorList': [], 'lossList': [0.0, -1.3135331225395204, 0.0, 73.97760257720947, 0.0, 0.0, 0.0], 'rewardMean': 0.7497362547779256, 'totalEpisodes': 83, 'stepsPerEpisode': 87, 'rewardPerEpisode': 71.26946731680731
'totalSteps': 14080, 'rewardStep': 0.6823004217470816, 'errorList': [], 'lossList': [0.0, -1.313479824066162, 0.0, 36.99372364997864, 0.0, 0.0, 0.0], 'rewardMean': 0.7333798390517672, 'totalEpisodes': 86, 'stepsPerEpisode': 61, 'rewardPerEpisode': 41.36373164380382
'totalSteps': 14720, 'rewardStep': 0.6120956920635903, 'errorList': [], 'lossList': [0.0, -1.3055766928195953, 0.0, 95.85844944000245, 0.0, 0.0, 0.0], 'rewardMean': 0.7360970286641996, 'totalEpisodes': 92, 'stepsPerEpisode': 16, 'rewardPerEpisode': 8.470319583176169
'totalSteps': 15360, 'rewardStep': 0.5617238017285962, 'errorList': [], 'lossList': [0.0, -1.2971198546886444, 0.0, 43.804069671630856, 0.0, 0.0, 0.0], 'rewardMean': 0.6992632444127994, 'totalEpisodes': 96, 'stepsPerEpisode': 36, 'rewardPerEpisode': 19.423676054803185
'totalSteps': 16000, 'rewardStep': 0.7332588543926062, 'errorList': [], 'lossList': [0.0, -1.2834266340732574, 0.0, 13.49869140148163, 0.0, 0.0, 0.0], 'rewardMean': 0.7013533472381631, 'totalEpisodes': 97, 'stepsPerEpisode': 66, 'rewardPerEpisode': 52.878541973006826
'totalSteps': 16640, 'rewardStep': 0.7850373126231809, 'errorList': [], 'lossList': [0.0, -1.267351986169815, 0.0, 35.53406943321228, 0.0, 0.0, 0.0], 'rewardMean': 0.7190316513932513, 'totalEpisodes': 101, 'stepsPerEpisode': 54, 'rewardPerEpisode': 41.68752543623562
'totalSteps': 17280, 'rewardStep': 0.3332046116012857, 'errorList': [], 'lossList': [0.0, -1.2775382530689239, 0.0, 11.055550289154052, 0.0, 0.0, 0.0], 'rewardMean': 0.6760399477076737, 'totalEpisodes': 104, 'stepsPerEpisode': 93, 'rewardPerEpisode': 53.56724732502663
'totalSteps': 17920, 'rewardStep': 0.6539994030763521, 'errorList': [], 'lossList': [0.0, -1.2862431252002715, 0.0, 14.522695813179016, 0.0, 0.0, 0.0], 'rewardMean': 0.6596530215004828, 'totalEpisodes': 106, 'stepsPerEpisode': 31, 'rewardPerEpisode': 20.092009999675494
'totalSteps': 18560, 'rewardStep': 0.702225716806261, 'errorList': [], 'lossList': [0.0, -1.2805186331272125, 0.0, 42.440759100914, 0.0, 0.0, 0.0], 'rewardMean': 0.6619411813817686, 'totalEpisodes': 107, 'stepsPerEpisode': 182, 'rewardPerEpisode': 142.68624992453184
'totalSteps': 19200, 'rewardStep': 0.5716661788004531, 'errorList': [], 'lossList': [0.0, -1.287813731431961, 0.0, 9.4792036318779, 0.0, 0.0, 0.0], 'rewardMean': 0.6380191541806846, 'totalEpisodes': 107, 'stepsPerEpisode': 640, 'rewardPerEpisode': 490.9682401840158
'totalSteps': 19840, 'rewardStep': 0.9871005335522791, 'errorList': [0.09469826494003025, 0.09145391397354323, 0.0950646785810854, 0.10668242394013362, 0.09218780692802483, 0.09503501487345806, 0.09143181008476513, 0.09460850095095259, 0.11595847955790635, 0.1052340323594079, 0.09452730874312862, 0.1346329131156655, 0.09282099649693715, 0.09627433474059205, 0.0966075652274289, 0.11339168528660515, 0.09332872328170463, 0.13999406519157673, 0.09353434275210978, 0.0944886737220967, 0.0982403641304295, 0.09018855452775403, 0.09494429664979165, 0.14824206413398883, 0.09007578828577308, 0.09099405811194959, 0.12534031013989716, 0.08860093921300713, 0.09624036812955808, 0.09752464943490056, 0.1248668303636052, 0.09461789496348134, 0.09441454650408186, 0.09017680983596466, 0.1284206828366803, 0.09202028862779632, 0.09185408229103502, 0.10631972525434849, 0.09028198392613258, 0.11220230189453798, 0.09016677311895274, 0.09498238780617264, 0.09487149879626867, 0.0917936640594584, 0.09132588088370043, 0.09390862907575867, 0.09085790529375165, 0.0943967832406728, 0.0907717392095273, 0.09168572964647353], 'lossList': [0.0, -1.2806866574287414, 0.0, 82.10817108154296, 0.0, 0.0, 0.0], 'rewardMean': 0.6622612526391686, 'totalEpisodes': 109, 'stepsPerEpisode': 68, 'rewardPerEpisode': 63.998044728089816, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=19840, timeSpent=43.41
