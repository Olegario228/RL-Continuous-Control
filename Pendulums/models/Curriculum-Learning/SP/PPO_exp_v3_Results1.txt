#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 512, 'episodeStepsMax': 640, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v3_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v3_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000, 14000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 640, 'rewardStep': 0.4630855124575506, 'errorList': [], 'lossList': [0.0, -1.4105364000797271, 0.0, 104.60637565612792, 0.0, 0.0, 0.0], 'rewardMean': 0.4630855124575506, 'totalEpisodes': 4, 'stepsPerEpisode': 132, 'rewardPerEpisode': 90.56225012656787
'totalSteps': 1280, 'rewardStep': 0.6859624479858126, 'errorList': [], 'lossList': [0.0, -1.407593276500702, 0.0, 59.80135724067688, 0.0, 0.0, 0.0], 'rewardMean': 0.5745239802216816, 'totalEpisodes': 6, 'stepsPerEpisode': 111, 'rewardPerEpisode': 81.1720872414294
'totalSteps': 1920, 'rewardStep': 0.9566123635236072, 'errorList': [], 'lossList': [0.0, -1.4027867209911347, 0.0, 43.16782423019409, 0.0, 0.0, 0.0], 'rewardMean': 0.7018867746556569, 'totalEpisodes': 10, 'stepsPerEpisode': 24, 'rewardPerEpisode': 20.95423651894295
'totalSteps': 2560, 'rewardStep': 0.7158641165473328, 'errorList': [], 'lossList': [0.0, -1.3895700287818908, 0.0, 44.76870276451111, 0.0, 0.0, 0.0], 'rewardMean': 0.7053811101285759, 'totalEpisodes': 10, 'stepsPerEpisode': 640, 'rewardPerEpisode': 459.72851293193315
'totalSteps': 3200, 'rewardStep': 0.5435194797966044, 'errorList': [], 'lossList': [0.0, -1.3832914221286774, 0.0, 45.217540760040286, 0.0, 0.0, 0.0], 'rewardMean': 0.6730087840621816, 'totalEpisodes': 13, 'stepsPerEpisode': 479, 'rewardPerEpisode': 338.0404684945378
'totalSteps': 3840, 'rewardStep': 0.8187890285487988, 'errorList': [], 'lossList': [0.0, -1.3751055777072907, 0.0, 33.23685419082641, 0.0, 0.0, 0.0], 'rewardMean': 0.6973054914766178, 'totalEpisodes': 14, 'stepsPerEpisode': 553, 'rewardPerEpisode': 408.33129923233986
'totalSteps': 4480, 'rewardStep': 0.7423178900088644, 'errorList': [], 'lossList': [0.0, -1.3780208384990693, 0.0, 28.0702596950531, 0.0, 0.0, 0.0], 'rewardMean': 0.7037358341240817, 'totalEpisodes': 14, 'stepsPerEpisode': 640, 'rewardPerEpisode': 467.67580289418856
'totalSteps': 5120, 'rewardStep': 0.823937955017832, 'errorList': [], 'lossList': [0.0, -1.3895887184143065, 0.0, 34.04152976036072, 0.0, 0.0, 0.0], 'rewardMean': 0.7187610992358003, 'totalEpisodes': 15, 'stepsPerEpisode': 381, 'rewardPerEpisode': 266.5760001963677
'totalSteps': 5760, 'rewardStep': 0.7445096713843169, 'errorList': [], 'lossList': [0.0, -1.4226509273052215, 0.0, 29.404295759201048, 0.0, 0.0, 0.0], 'rewardMean': 0.7216220516967466, 'totalEpisodes': 15, 'stepsPerEpisode': 640, 'rewardPerEpisode': 514.9608613573704
'totalSteps': 6400, 'rewardStep': 0.7187264780845857, 'errorList': [], 'lossList': [0.0, -1.4165659439563751, 0.0, 39.20954254150391, 0.0, 0.0, 0.0], 'rewardMean': 0.7213324943355305, 'totalEpisodes': 16, 'stepsPerEpisode': 303, 'rewardPerEpisode': 215.48318107126457
'totalSteps': 7040, 'rewardStep': 0.749355461212607, 'errorList': [], 'lossList': [0.0, -1.407656934261322, 0.0, 20.11242949485779, 0.0, 0.0, 0.0], 'rewardMean': 0.7499594892110362, 'totalEpisodes': 16, 'stepsPerEpisode': 640, 'rewardPerEpisode': 526.6914349042961
'totalSteps': 7680, 'rewardStep': 0.7460625261729036, 'errorList': [], 'lossList': [0.0, -1.3983419620990754, 0.0, 16.139355130195618, 0.0, 0.0, 0.0], 'rewardMean': 0.7559694970297453, 'totalEpisodes': 16, 'stepsPerEpisode': 640, 'rewardPerEpisode': 532.0704462266198
'totalSteps': 8320, 'rewardStep': 0.7367839730320894, 'errorList': [], 'lossList': [0.0, -1.4010798192024232, 0.0, 6.455607128143311, 0.0, 0.0, 0.0], 'rewardMean': 0.7339866579805935, 'totalEpisodes': 16, 'stepsPerEpisode': 640, 'rewardPerEpisode': 429.7108653739018
'totalSteps': 8960, 'rewardStep': 0.4957697548137585, 'errorList': [], 'lossList': [0.0, -1.3905973970890044, 0.0, 259.79542846679686, 0.0, 0.0, 0.0], 'rewardMean': 0.711977221807236, 'totalEpisodes': 23, 'stepsPerEpisode': 108, 'rewardPerEpisode': 77.97919507573559
'totalSteps': 9600, 'rewardStep': 0.8121372663859256, 'errorList': [], 'lossList': [0.0, -1.3882230246067047, 0.0, 287.16759216308594, 0.0, 0.0, 0.0], 'rewardMean': 0.7388390004661682, 'totalEpisodes': 33, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.376901015238946
'totalSteps': 10240, 'rewardStep': 0.8927027365310626, 'errorList': [], 'lossList': [0.0, -1.3873070120811462, 0.0, 318.60022659301757, 0.0, 0.0, 0.0], 'rewardMean': 0.7462303712643946, 'totalEpisodes': 47, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.5472442985100274
'totalSteps': 10880, 'rewardStep': 0.9496641217863002, 'errorList': [], 'lossList': [0.0, -1.3830645155906678, 0.0, 265.84244522094724, 0.0, 0.0, 0.0], 'rewardMean': 0.7669649944421382, 'totalEpisodes': 61, 'stepsPerEpisode': 25, 'rewardPerEpisode': 22.557511124303456
'totalSteps': 11520, 'rewardStep': 0.6193019963439299, 'errorList': [], 'lossList': [0.0, -1.3806631696224212, 0.0, 209.3512776184082, 0.0, 0.0, 0.0], 'rewardMean': 0.746501398574748, 'totalEpisodes': 77, 'stepsPerEpisode': 32, 'rewardPerEpisode': 24.944982026269525
'totalSteps': 12160, 'rewardStep': 0.4758311040092332, 'errorList': [], 'lossList': [0.0, -1.3787029051780701, 0.0, 107.95604629516602, 0.0, 0.0, 0.0], 'rewardMean': 0.7196335418372397, 'totalEpisodes': 91, 'stepsPerEpisode': 68, 'rewardPerEpisode': 54.23148373975537
'totalSteps': 12800, 'rewardStep': 0.7389900731560124, 'errorList': [], 'lossList': [0.0, -1.371407618522644, 0.0, 102.4008108139038, 0.0, 0.0, 0.0], 'rewardMean': 0.7216599013443823, 'totalEpisodes': 102, 'stepsPerEpisode': 106, 'rewardPerEpisode': 80.00185836909917
'totalSteps': 13440, 'rewardStep': 0.9332590464239385, 'errorList': [], 'lossList': [0.0, -1.364531134366989, 0.0, 65.53386365890503, 0.0, 0.0, 0.0], 'rewardMean': 0.7400502598655153, 'totalEpisodes': 108, 'stepsPerEpisode': 23, 'rewardPerEpisode': 16.522190823315906
'totalSteps': 14080, 'rewardStep': 0.7005065363825589, 'errorList': [], 'lossList': [0.0, -1.3568164646625518, 0.0, 23.457631039619447, 0.0, 0.0, 0.0], 'rewardMean': 0.735494660886481, 'totalEpisodes': 110, 'stepsPerEpisode': 133, 'rewardPerEpisode': 93.5424622984762
'totalSteps': 14720, 'rewardStep': 0.8403430152835998, 'errorList': [], 'lossList': [0.0, -1.3396905422210694, 0.0, 17.12575284957886, 0.0, 0.0, 0.0], 'rewardMean': 0.745850565111632, 'totalEpisodes': 114, 'stepsPerEpisode': 92, 'rewardPerEpisode': 74.40605972902623
'totalSteps': 15360, 'rewardStep': 0.6142536977906012, 'errorList': [], 'lossList': [0.0, -1.3406176233291627, 0.0, 16.935081915855406, 0.0, 0.0, 0.0], 'rewardMean': 0.7576989594093162, 'totalEpisodes': 116, 'stepsPerEpisode': 121, 'rewardPerEpisode': 88.61436486348136
'totalSteps': 16000, 'rewardStep': 0.6784833265274481, 'errorList': [], 'lossList': [0.0, -1.3417576003074645, 0.0, 10.078793792724609, 0.0, 0.0, 0.0], 'rewardMean': 0.7443335654234685, 'totalEpisodes': 118, 'stepsPerEpisode': 60, 'rewardPerEpisode': 49.57994289266614
'totalSteps': 16640, 'rewardStep': 0.7472998231018979, 'errorList': [], 'lossList': [0.0, -1.331474347114563, 0.0, 43.802993869781496, 0.0, 0.0, 0.0], 'rewardMean': 0.729793274080552, 'totalEpisodes': 121, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.5519130553907345
'totalSteps': 17280, 'rewardStep': 0.7495435292384314, 'errorList': [], 'lossList': [0.0, -1.3294082117080688, 0.0, 10.748257608413697, 0.0, 0.0, 0.0], 'rewardMean': 0.7097812148257651, 'totalEpisodes': 123, 'stepsPerEpisode': 157, 'rewardPerEpisode': 117.14335184481064
'totalSteps': 17920, 'rewardStep': 0.8828824969622955, 'errorList': [], 'lossList': [0.0, -1.3395619630813598, 0.0, 7.616893827915192, 0.0, 0.0, 0.0], 'rewardMean': 0.7361392648876018, 'totalEpisodes': 125, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.94369116620805
'totalSteps': 18560, 'rewardStep': 0.6651391156997088, 'errorList': [], 'lossList': [0.0, -1.3389079439640046, 0.0, 9.31240927696228, 0.0, 0.0, 0.0], 'rewardMean': 0.7550700660566493, 'totalEpisodes': 126, 'stepsPerEpisode': 419, 'rewardPerEpisode': 340.90418393306663
'totalSteps': 19200, 'rewardStep': 0.8876657274643743, 'errorList': [], 'lossList': [0.0, -1.333330203294754, 0.0, 6.926575603485108, 0.0, 0.0, 0.0], 'rewardMean': 0.7699376314874855, 'totalEpisodes': 127, 'stepsPerEpisode': 500, 'rewardPerEpisode': 415.2296216021611
'totalSteps': 19840, 'rewardStep': 0.5382276977111271, 'errorList': [], 'lossList': [0.0, -1.3350341796875, 0.0, 9.987959656715393, 0.0, 0.0, 0.0], 'rewardMean': 0.7304344966162043, 'totalEpisodes': 129, 'stepsPerEpisode': 186, 'rewardPerEpisode': 128.63023921797256
'totalSteps': 20480, 'rewardStep': 0.7144882729772835, 'errorList': [], 'lossList': [0.0, -1.3337749826908112, 0.0, 14.10777506351471, 0.0, 0.0, 0.0], 'rewardMean': 0.7318326702756768, 'totalEpisodes': 132, 'stepsPerEpisode': 41, 'rewardPerEpisode': 37.37453975693417
'totalSteps': 21120, 'rewardStep': 0.7285354858397457, 'errorList': [], 'lossList': [0.0, -1.3400959646701813, 0.0, 9.043883752822875, 0.0, 0.0, 0.0], 'rewardMean': 0.7206519173312913, 'totalEpisodes': 134, 'stepsPerEpisode': 224, 'rewardPerEpisode': 184.31741335658486
'totalSteps': 21760, 'rewardStep': 0.6559529116222733, 'errorList': [], 'lossList': [0.0, -1.3405521821975708, 0.0, 8.144341421127319, 0.0, 0.0, 0.0], 'rewardMean': 0.7248218387144586, 'totalEpisodes': 135, 'stepsPerEpisode': 449, 'rewardPerEpisode': 349.1895009145908
'totalSteps': 22400, 'rewardStep': 0.5153186360316135, 'errorList': [], 'lossList': [0.0, -1.3247943997383118, 0.0, 6.379637081623077, 0.0, 0.0, 0.0], 'rewardMean': 0.7085053696648751, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 410.09359436287474
'totalSteps': 23040, 'rewardStep': 0.7836397143884284, 'errorList': [], 'lossList': [0.0, -1.310146653652191, 0.0, 4.85236468911171, 0.0, 0.0, 0.0], 'rewardMean': 0.7121393587935282, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 420.3568005090394
'totalSteps': 23680, 'rewardStep': 0.5341260148214952, 'errorList': [], 'lossList': [0.0, -1.3022333979606628, 0.0, 2.1712809509038924, 0.0, 0.0, 0.0], 'rewardMean': 0.6905976073518346, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 490.66430140591206
'totalSteps': 24320, 'rewardStep': 0.6885403634989133, 'errorList': [], 'lossList': [0.0, -1.3157544791698457, 0.0, 3.438463809490204, 0.0, 0.0, 0.0], 'rewardMean': 0.6711633940054964, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 424.99605756063414
'totalSteps': 24960, 'rewardStep': 0.9123526208424243, 'errorList': [], 'lossList': [0.0, -1.2951855981349945, 0.0, 1.2113773426413537, 0.0, 0.0, 0.0], 'rewardMean': 0.6958847445197678, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 541.904554910657
'totalSteps': 25600, 'rewardStep': 0.8702201453211608, 'errorList': [], 'lossList': [0.0, -1.244923380613327, 0.0, 1.411225266456604, 0.0, 0.0, 0.0], 'rewardMean': 0.6941401863054465, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 564.2470369975081
'totalSteps': 26240, 'rewardStep': 0.7896101427637112, 'errorList': [], 'lossList': [0.0, -1.1995833289623261, 0.0, 1.1907547852396965, 0.0, 0.0, 0.0], 'rewardMean': 0.7192784308107051, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 571.7019358829455
'totalSteps': 26880, 'rewardStep': 0.9291706666186128, 'errorList': [], 'lossList': [0.0, -1.172972034215927, 0.0, 0.3939715840667486, 0.0, 0.0, 0.0], 'rewardMean': 0.7407466701748379, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 553.593939007872
'totalSteps': 27520, 'rewardStep': 0.9873655341957107, 'errorList': [0.025100148865536225, 0.042044068166455784, 0.030119288549237042, 0.032686638144021334, 0.03155024642754842, 0.023694110853995077, 0.023000111498519035, 0.02748205699983923, 0.024535779806934638, 0.027499667460956834, 0.023383673699263682, 0.022886732025116884, 0.03450516807822944, 0.023375807028093564, 0.0235504724329909, 0.030926450659756386, 0.02391495216424605, 0.0232688853959458, 0.04346830104679554, 0.03225943529345385, 0.024160836356338283, 0.02280429670235698, 0.03196486127814212, 0.030193640980437707, 0.025823421575941957, 0.02262439295810017, 0.033970412035616945, 0.03592813080452479, 0.04040907533929009, 0.04503220463064278, 0.02600613549395414, 0.0233770794086752, 0.02477672073072197, 0.02602451684204513, 0.023465800123066295, 0.03227598824205575, 0.02279667616553647, 0.03560126919860998, 0.02275374178245026, 0.0409107099618709, 0.02607512749845325, 0.023432387329400006, 0.024226794027427688, 0.0259199909406046, 0.03031121292165904, 0.03337876486055594, 0.029264779595078616, 0.025744484088459422, 0.034239854012365324, 0.031241023526955757], 'lossList': [0.0, -1.1494519412517548, 0.0, 1.0180942432582378, 0.0, 0.0, 0.0], 'rewardMean': 0.7666296750104343, 'totalEpisodes': 135, 'stepsPerEpisode': 640, 'rewardPerEpisode': 596.5586837237659, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=27520, timeSpent=51.92
