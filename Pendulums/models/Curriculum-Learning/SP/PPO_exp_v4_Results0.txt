#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v4_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v4_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000, 14000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9736934473677203, 'errorList': [], 'lossList': [0.0, -1.4319616508483888, 0.0, 91.73895628929138, 0.0, 0.0, 0.0], 'rewardMean': 0.9736934473677203, 'totalEpisodes': 6, 'stepsPerEpisode': 156, 'rewardPerEpisode': 137.1307948606565
'totalSteps': 2560, 'rewardStep': 0.930999194451572, 'errorList': [], 'lossList': [0.0, -1.4378350245952607, 0.0, 26.745213581323625, 0.0, 0.0, 0.0], 'rewardMean': 0.9523463209096461, 'totalEpisodes': 9, 'stepsPerEpisode': 259, 'rewardPerEpisode': 199.38829073747664
'totalSteps': 3840, 'rewardStep': 0.5129210074482276, 'errorList': [], 'lossList': [0.0, -1.4305314230918884, 0.0, 32.48619882583618, 0.0, 0.0, 0.0], 'rewardMean': 0.8058712164225067, 'totalEpisodes': 15, 'stepsPerEpisode': 153, 'rewardPerEpisode': 119.18896717382849
'totalSteps': 5120, 'rewardStep': 0.7936720066933936, 'errorList': [], 'lossList': [0.0, -1.4298061841726304, 0.0, 20.29407395839691, 0.0, 0.0, 0.0], 'rewardMean': 0.8028214139902283, 'totalEpisodes': 16, 'stepsPerEpisode': 1266, 'rewardPerEpisode': 921.8884882166403
'totalSteps': 6400, 'rewardStep': 0.9446197273364012, 'errorList': [], 'lossList': [0.0, -1.422766199707985, 0.0, 21.546101879179478, 0.0, 0.0, 0.0], 'rewardMean': 0.831181076659463, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1039.8992714595026
'totalSteps': 7680, 'rewardStep': 0.7699141871196036, 'errorList': [], 'lossList': [0.0, -1.3975875449180604, 0.0, 18.420489843785763, 0.0, 0.0, 0.0], 'rewardMean': 0.8209699284028197, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1093.4207795780205
'totalSteps': 8960, 'rewardStep': 0.7722719995400477, 'errorList': [], 'lossList': [0.0, -1.380628205537796, 0.0, 6.882694716155529, 0.0, 0.0, 0.0], 'rewardMean': 0.8140130814224237, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 988.9298345067556
'totalSteps': 10240, 'rewardStep': 0.4850961344952831, 'errorList': [], 'lossList': [0.0, -1.3962126439809799, 0.0, 261.6809607696533, 0.0, 0.0, 0.0], 'rewardMean': 0.7728984630565311, 'totalEpisodes': 33, 'stepsPerEpisode': 11, 'rewardPerEpisode': 5.873540469978378
'totalSteps': 11520, 'rewardStep': 0.8041023243830099, 'errorList': [], 'lossList': [0.0, -1.3925281816720962, 0.0, 236.62354652404784, 0.0, 0.0, 0.0], 'rewardMean': 0.7763655587594731, 'totalEpisodes': 61, 'stepsPerEpisode': 49, 'rewardPerEpisode': 43.59290732223412
'totalSteps': 12800, 'rewardStep': 0.8784647224146152, 'errorList': [], 'lossList': [0.0, -1.3916203677654266, 0.0, 87.97302198410034, 0.0, 0.0, 0.0], 'rewardMean': 0.7865754751249874, 'totalEpisodes': 95, 'stepsPerEpisode': 17, 'rewardPerEpisode': 16.032846398498677
'totalSteps': 14080, 'rewardStep': 0.527586208612905, 'errorList': [], 'lossList': [0.0, -1.3903858822584152, 0.0, 43.813152904510495, 0.0, 0.0, 0.0], 'rewardMean': 0.7419647512495059, 'totalEpisodes': 118, 'stepsPerEpisode': 57, 'rewardPerEpisode': 37.373290719979515
'totalSteps': 15360, 'rewardStep': 0.8250980701633946, 'errorList': [], 'lossList': [0.0, -1.3860932606458665, 0.0, 46.30180883407593, 0.0, 0.0, 0.0], 'rewardMean': 0.7313746388206882, 'totalEpisodes': 131, 'stepsPerEpisode': 68, 'rewardPerEpisode': 63.58715819863596
'totalSteps': 16640, 'rewardStep': 0.11001708932706855, 'errorList': [], 'lossList': [0.0, -1.3767939901351929, 0.0, 15.259006979465484, 0.0, 0.0, 0.0], 'rewardMean': 0.6910842470085722, 'totalEpisodes': 142, 'stepsPerEpisode': 98, 'rewardPerEpisode': 53.09428377395759
'totalSteps': 17920, 'rewardStep': 0.715607054434585, 'errorList': [], 'lossList': [0.0, -1.3714321994781493, 0.0, 14.45398994922638, 0.0, 0.0, 0.0], 'rewardMean': 0.6832777517826913, 'totalEpisodes': 150, 'stepsPerEpisode': 78, 'rewardPerEpisode': 57.85816021488103
'totalSteps': 19200, 'rewardStep': 0.47518664020274937, 'errorList': [], 'lossList': [0.0, -1.3696569675207138, 0.0, 20.224698731899263, 0.0, 0.0, 0.0], 'rewardMean': 0.6363344430693262, 'totalEpisodes': 155, 'stepsPerEpisode': 155, 'rewardPerEpisode': 113.4563681209425
'totalSteps': 20480, 'rewardStep': 0.928817702295531, 'errorList': [], 'lossList': [0.0, -1.3675579571723937, 0.0, 9.119198231697082, 0.0, 0.0, 0.0], 'rewardMean': 0.652224794586919, 'totalEpisodes': 160, 'stepsPerEpisode': 27, 'rewardPerEpisode': 21.500250768642292
'totalSteps': 21760, 'rewardStep': 0.7758110515370509, 'errorList': [], 'lossList': [0.0, -1.3668752467632295, 0.0, 5.037317671775818, 0.0, 0.0, 0.0], 'rewardMean': 0.6525786997866192, 'totalEpisodes': 165, 'stepsPerEpisode': 88, 'rewardPerEpisode': 72.85729911643674
'totalSteps': 23040, 'rewardStep': 0.7258660561787117, 'errorList': [], 'lossList': [0.0, -1.364110488295555, 0.0, 4.98112048625946, 0.0, 0.0, 0.0], 'rewardMean': 0.6766556919549622, 'totalEpisodes': 167, 'stepsPerEpisode': 417, 'rewardPerEpisode': 360.0068820240213
'totalSteps': 24320, 'rewardStep': 0.5610591211258087, 'errorList': [], 'lossList': [0.0, -1.3330476742982864, 0.0, 5.685473161935806, 0.0, 0.0, 0.0], 'rewardMean': 0.6523513716292418, 'totalEpisodes': 167, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 959.1124987440155
'totalSteps': 25600, 'rewardStep': 0.5233188972038207, 'errorList': [], 'lossList': [0.0, -1.2957677716016769, 0.0, 5.468404006958008, 0.0, 0.0, 0.0], 'rewardMean': 0.6168367891081624, 'totalEpisodes': 168, 'stepsPerEpisode': 1210, 'rewardPerEpisode': 924.5414627069966
'totalSteps': 26880, 'rewardStep': 0.9555657393646492, 'errorList': [0.013159531199310553, 0.032302225700468555, 0.006742328187013869, 0.03567314120851087, 0.020118711663904845, 0.015644586837457033, 0.017940647395561202, 0.05523594622795612, 0.019893607042291983, 0.02001795266892318, 0.03321384412462348, 0.08305988750142365, 0.026133991436489073, 0.07472812902864436, 0.040700957680599115, 0.020985705475954103, 0.015102394067095366, 0.0297261761350873, 0.016002660831020903, 0.009909955354522897, 0.00435084420664847, 0.027379574842608518, 0.008470640202500492, 0.0162764189169024, 0.005069675157777617, 0.012157282143200172, 0.025525974556997445, 0.006406933885950703, 0.004417224326172782, 0.051648740825538116, 0.051034065699745126, 0.05626542154850437, 0.011748665540419407, 0.030982738122590222, 0.03441185409135347, 0.005929986483722543, 0.027487169906748973, 0.03840255080710773, 0.033047377643114884, 0.008166403577770234, 0.005829723018474519, 0.01971650027239961, 0.010704158939519396, 0.014979151509408104, 0.014005281569710027, 0.017543859473111323, 0.004242851411342771, 0.02031922372617469, 0.009354326075327676, 0.0333299414186706], 'lossList': [0.0, -1.2592030268907548, 0.0, 2.358769386932254, 0.0, 0.0, 0.0], 'rewardMean': 0.659634742183337, 'totalEpisodes': 168, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.911817417993, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=58.64
