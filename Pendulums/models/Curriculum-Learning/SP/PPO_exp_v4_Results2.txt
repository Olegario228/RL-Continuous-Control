#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v4_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v4_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000, 14000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8280920161904477, 'errorList': [], 'lossList': [0.0, -1.41705382168293, 0.0, 82.44616906642914, 0.0, 0.0, 0.0], 'rewardMean': 0.8280920161904477, 'totalEpisodes': 5, 'stepsPerEpisode': 241, 'rewardPerEpisode': 205.5822162166961
'totalSteps': 2560, 'rewardStep': 0.7055659774538258, 'errorList': [], 'lossList': [0.0, -1.417495858669281, 0.0, 27.777225496768953, 0.0, 0.0, 0.0], 'rewardMean': 0.7668289968221367, 'totalEpisodes': 13, 'stepsPerEpisode': 271, 'rewardPerEpisode': 193.7475707293494
'totalSteps': 3840, 'rewardStep': 0.7834608199650239, 'errorList': [], 'lossList': [0.0, -1.416220599412918, 0.0, 24.059616723060607, 0.0, 0.0, 0.0], 'rewardMean': 0.7723729378697657, 'totalEpisodes': 17, 'stepsPerEpisode': 471, 'rewardPerEpisode': 336.2454978554248
'totalSteps': 5120, 'rewardStep': 0.47937618283556876, 'errorList': [], 'lossList': [0.0, -1.4164248436689377, 0.0, 27.751172075271608, 0.0, 0.0, 0.0], 'rewardMean': 0.6991237491112164, 'totalEpisodes': 22, 'stepsPerEpisode': 66, 'rewardPerEpisode': 39.7976711465447
'totalSteps': 6400, 'rewardStep': 0.2286853859694324, 'errorList': [], 'lossList': [0.0, -1.4080487132072448, 0.0, 24.6325882434845, 0.0, 0.0, 0.0], 'rewardMean': 0.6050360764828596, 'totalEpisodes': 23, 'stepsPerEpisode': 575, 'rewardPerEpisode': 402.35769690502724
'totalSteps': 7680, 'rewardStep': 0.6385552138074897, 'errorList': [], 'lossList': [0.0, -1.3876803642511368, 0.0, 14.18588240981102, 0.0, 0.0, 0.0], 'rewardMean': 0.610622599370298, 'totalEpisodes': 23, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 981.7420754455324
'totalSteps': 8960, 'rewardStep': 0.9583902390430078, 'errorList': [], 'lossList': [0.0, -1.3812388271093368, 0.0, 24.87866995751858, 0.0, 0.0, 0.0], 'rewardMean': 0.6603036907521137, 'totalEpisodes': 24, 'stepsPerEpisode': 360, 'rewardPerEpisode': 270.64527539330237
'totalSteps': 10240, 'rewardStep': 0.8712047616917364, 'errorList': [], 'lossList': [0.0, -1.3513477861881256, 0.0, 110.91294893264771, 0.0, 0.0, 0.0], 'rewardMean': 0.6866663246195666, 'totalEpisodes': 33, 'stepsPerEpisode': 21, 'rewardPerEpisode': 17.327222670942646
'totalSteps': 11520, 'rewardStep': 0.8696735402565577, 'errorList': [], 'lossList': [0.0, -1.3469467288255692, 0.0, 294.43819389343264, 0.0, 0.0, 0.0], 'rewardMean': 0.7070004596903433, 'totalEpisodes': 65, 'stepsPerEpisode': 13, 'rewardPerEpisode': 11.201391603712143
'totalSteps': 12800, 'rewardStep': 0.7024827165475281, 'errorList': [], 'lossList': [0.0, -1.3392156088352203, 0.0, 76.10030714035034, 0.0, 0.0, 0.0], 'rewardMean': 0.7065486853760617, 'totalEpisodes': 93, 'stepsPerEpisode': 21, 'rewardPerEpisode': 14.789451151108612
'totalSteps': 14080, 'rewardStep': 0.5056967025132051, 'errorList': [], 'lossList': [0.0, -1.3340757524967193, 0.0, 33.08020233154297, 0.0, 0.0, 0.0], 'rewardMean': 0.6743091540083376, 'totalEpisodes': 113, 'stepsPerEpisode': 41, 'rewardPerEpisode': 32.68893249510178
'totalSteps': 15360, 'rewardStep': 0.6528059657554539, 'errorList': [], 'lossList': [0.0, -1.3333067429065704, 0.0, 32.313594450950625, 0.0, 0.0, 0.0], 'rewardMean': 0.6690331528385005, 'totalEpisodes': 125, 'stepsPerEpisode': 93, 'rewardPerEpisode': 69.89831999918827
'totalSteps': 16640, 'rewardStep': 0.9010970050770672, 'errorList': [], 'lossList': [0.0, -1.3258439427614213, 0.0, 23.76195387840271, 0.0, 0.0, 0.0], 'rewardMean': 0.6807967713497047, 'totalEpisodes': 132, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.72035843003843
'totalSteps': 17920, 'rewardStep': 0.7371986996389139, 'errorList': [], 'lossList': [0.0, -1.319838889837265, 0.0, 30.355368342399597, 0.0, 0.0, 0.0], 'rewardMean': 0.7065790230300393, 'totalEpisodes': 136, 'stepsPerEpisode': 218, 'rewardPerEpisode': 174.12907594023497
'totalSteps': 19200, 'rewardStep': 0.6649322590858948, 'errorList': [], 'lossList': [0.0, -1.3147871941328049, 0.0, 16.725551850795746, 0.0, 0.0, 0.0], 'rewardMean': 0.7502037103416855, 'totalEpisodes': 139, 'stepsPerEpisode': 354, 'rewardPerEpisode': 239.2719129600674
'totalSteps': 20480, 'rewardStep': 0.7342112518573796, 'errorList': [], 'lossList': [0.0, -1.2912620186805726, 0.0, 6.71914730399847, 0.0, 0.0, 0.0], 'rewardMean': 0.7597693141466746, 'totalEpisodes': 140, 'stepsPerEpisode': 1197, 'rewardPerEpisode': 986.7911221153595
'totalSteps': 21760, 'rewardStep': 0.7867640704975364, 'errorList': [], 'lossList': [0.0, -1.2557837891578674, 0.0, 20.236101378202438, 0.0, 0.0, 0.0], 'rewardMean': 0.7426066972921272, 'totalEpisodes': 143, 'stepsPerEpisode': 121, 'rewardPerEpisode': 96.1741568867562
'totalSteps': 23040, 'rewardStep': 0.7083824412478179, 'errorList': [], 'lossList': [0.0, -1.2378573203086853, 0.0, 2.4221068751066923, 0.0, 0.0, 0.0], 'rewardMean': 0.7263244652477355, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1007.8492825208865
'totalSteps': 24320, 'rewardStep': 0.6199811001609746, 'errorList': [], 'lossList': [0.0, -1.230531973838806, 0.0, 1.4012663918733597, 0.0, 0.0, 0.0], 'rewardMean': 0.7013552212381772, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 989.3105088670093
'totalSteps': 25600, 'rewardStep': 0.49637428571407144, 'errorList': [], 'lossList': [0.0, -1.2369937640428543, 0.0, 1.6154679681360722, 0.0, 0.0, 0.0], 'rewardMean': 0.6807443781548315, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1034.5339786255056
'totalSteps': 26880, 'rewardStep': 0.9876255415158094, 'errorList': [0.04711867310132968, 0.06121503949151628, 0.0413800877574097, 0.06247221121997241, 0.06749272304193145, 0.08565816225319473, 0.14746114505412913, 0.06594607652941703, 0.17140545958228146, 0.04181705343650276, 0.11888410333545685, 0.13339515038010372, 0.046463987960995594, 0.06328427008766338, 0.042304165301156674, 0.11631600936471709, 0.042429607556038675, 0.08739928367475236, 0.04237136904586292, 0.04057403172374123, 0.0431973178929918, 0.05962327419846633, 0.04601350321439665, 0.16340223295664483, 0.10575315955399968, 0.06738634970093858, 0.15482714941638226, 0.04302608521416618, 0.04030420760095899, 0.04015128269456039, 0.07381771985539863, 0.04963639602745589, 0.10810448756703568, 0.1070264474150598, 0.0613295008056001, 0.07431384098805738, 0.044434395457792195, 0.040897564904553485, 0.0741264141281718, 0.0415130978556948, 0.06183053723029037, 0.04154964185906805, 0.04217426323822555, 0.09150464714446535, 0.04241877218006515, 0.11984039412780072, 0.08852445097723471, 0.1954142233566351, 0.10289255831524831, 0.06984211784478946], 'lossList': [0.0, -1.2139721357822417, 0.0, 1.1030315587297082, 0.0, 0.0, 0.0], 'rewardMean': 0.7289372620550919, 'totalEpisodes': 143, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1092.751128568788, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=59.04
