#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v4_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v4_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000, 14000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.720468672746902, 'errorList': [], 'lossList': [0.0, -1.4278227895498277, 0.0, 82.66252764225005, 0.0, 0.0, 0.0], 'rewardMean': 0.720468672746902, 'totalEpisodes': 6, 'stepsPerEpisode': 204, 'rewardPerEpisode': 144.31493761881387
'totalSteps': 2560, 'rewardStep': 0.7096300476766275, 'errorList': [], 'lossList': [0.0, -1.4298783040046692, 0.0, 29.93927188873291, 0.0, 0.0, 0.0], 'rewardMean': 0.7150493602117647, 'totalEpisodes': 10, 'stepsPerEpisode': 29, 'rewardPerEpisode': 24.43150345758116
'totalSteps': 3840, 'rewardStep': 0.9408660940810958, 'errorList': [], 'lossList': [0.0, -1.4190494918823242, 0.0, 27.62391316652298, 0.0, 0.0, 0.0], 'rewardMean': 0.7903216048348751, 'totalEpisodes': 14, 'stepsPerEpisode': 495, 'rewardPerEpisode': 313.46338332043064
'totalSteps': 5120, 'rewardStep': 0.7976647037490069, 'errorList': [], 'lossList': [0.0, -1.40831700861454, 0.0, 31.37659512758255, 0.0, 0.0, 0.0], 'rewardMean': 0.792157379563408, 'totalEpisodes': 15, 'stepsPerEpisode': 181, 'rewardPerEpisode': 153.78857578707803
'totalSteps': 6400, 'rewardStep': 0.40080788688884184, 'errorList': [], 'lossList': [0.0, -1.4145697414875031, 0.0, 18.815698095560073, 0.0, 0.0, 0.0], 'rewardMean': 0.7138874810284948, 'totalEpisodes': 16, 'stepsPerEpisode': 180, 'rewardPerEpisode': 112.28754308972805
'totalSteps': 7680, 'rewardStep': 0.8191860275277166, 'errorList': [], 'lossList': [0.0, -1.3954857289791107, 0.0, 21.535279519557953, 0.0, 0.0, 0.0], 'rewardMean': 0.7314372387783651, 'totalEpisodes': 17, 'stepsPerEpisode': 948, 'rewardPerEpisode': 698.6384534648882
'totalSteps': 8960, 'rewardStep': 0.870476617514018, 'errorList': [], 'lossList': [0.0, -1.39228202521801, 0.0, 77.90407802581787, 0.0, 0.0, 0.0], 'rewardMean': 0.7513000071691726, 'totalEpisodes': 23, 'stepsPerEpisode': 92, 'rewardPerEpisode': 64.8570269019428
'totalSteps': 10240, 'rewardStep': 0.8409165358417052, 'errorList': [], 'lossList': [0.0, -1.3950990879535674, 0.0, 201.43543830871582, 0.0, 0.0, 0.0], 'rewardMean': 0.7625020732532393, 'totalEpisodes': 40, 'stepsPerEpisode': 109, 'rewardPerEpisode': 93.31386322279849
'totalSteps': 11520, 'rewardStep': 0.5149142408037957, 'errorList': [], 'lossList': [0.0, -1.392067723274231, 0.0, 241.98822048187256, 0.0, 0.0, 0.0], 'rewardMean': 0.73499231409219, 'totalEpisodes': 72, 'stepsPerEpisode': 150, 'rewardPerEpisode': 123.81810291290826
'totalSteps': 12800, 'rewardStep': 0.9523844464908154, 'errorList': [], 'lossList': [0.0, -1.393157365322113, 0.0, 84.9391901397705, 0.0, 0.0, 0.0], 'rewardMean': 0.7567315273320525, 'totalEpisodes': 86, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.371019378943751
'totalSteps': 14080, 'rewardStep': 0.8539742223418024, 'errorList': [], 'lossList': [0.0, -1.3969334387779235, 0.0, 49.80745237350464, 0.0, 0.0, 0.0], 'rewardMean': 0.7700820822915426, 'totalEpisodes': 99, 'stepsPerEpisode': 66, 'rewardPerEpisode': 54.256117951875055
'totalSteps': 15360, 'rewardStep': 0.5290104923371424, 'errorList': [], 'lossList': [0.0, -1.388438937664032, 0.0, 41.124755635261536, 0.0, 0.0, 0.0], 'rewardMean': 0.752020126757594, 'totalEpisodes': 108, 'stepsPerEpisode': 223, 'rewardPerEpisode': 181.33003691604026
'totalSteps': 16640, 'rewardStep': 0.7975562366419703, 'errorList': [], 'lossList': [0.0, -1.3797850358486174, 0.0, 12.58027551651001, 0.0, 0.0, 0.0], 'rewardMean': 0.7376891410136814, 'totalEpisodes': 111, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.181911451541355
'totalSteps': 17920, 'rewardStep': 0.8344464241726189, 'errorList': [], 'lossList': [0.0, -1.3841272443532944, 0.0, 17.777775588035585, 0.0, 0.0, 0.0], 'rewardMean': 0.7413673130560426, 'totalEpisodes': 114, 'stepsPerEpisode': 225, 'rewardPerEpisode': 180.1105949450577
'totalSteps': 19200, 'rewardStep': 0.8422156047706061, 'errorList': [], 'lossList': [0.0, -1.377785177230835, 0.0, 31.85792172908783, 0.0, 0.0, 0.0], 'rewardMean': 0.7855080848442191, 'totalEpisodes': 118, 'stepsPerEpisode': 75, 'rewardPerEpisode': 55.62403060246662
'totalSteps': 20480, 'rewardStep': 0.9416194783121759, 'errorList': [0.28988697457989915, 0.3235528344033188, 0.12677298465205133, 0.11838027993072031, 0.2950919414323651, 0.15776388038298828, 0.10126216372508545, 0.3366616751073359, 0.19187193971014113, 0.1571634159800343, 0.09722791657961355, 0.19325372806234195, 0.46084537171240475, 0.20435769179575783, 0.15678426099003245, 0.14951376037454933, 0.15015122059341232, 0.4524645299897738, 0.13514012684729315, 0.1908027715726297, 0.20606600797758706, 0.2245204989998389, 0.3197049094163296, 0.10067237898793967, 0.23632024579950534, 0.16116005129824262, 0.22516496492799898, 0.3839643906495394, 0.14820336145303054, 0.2942317306741382, 0.11397300935928299, 0.16546265204989097, 0.12539231425240707, 0.1706403625735719, 0.30300303242459176, 0.4387840248218765, 0.3073276771068681, 0.23705219201744757, 0.19690825003103057, 0.3586076597070961, 0.2592591197750376, 0.13207017940858753, 0.5114267065592669, 0.3787582754176269, 0.13848743850807052, 0.10123986431516893, 0.19980834667296807, 0.39872844760327275, 0.12465100476071744, 0.4426597012940051], 'lossList': [0.0, -1.37323016166687, 0.0, 7.514454437494278, 0.0, 0.0, 0.0], 'rewardMean': 0.7977514299226651, 'totalEpisodes': 120, 'stepsPerEpisode': 196, 'rewardPerEpisode': 162.08873296184524, 'successfulTests': 26
'totalSteps': 21760, 'rewardStep': 0.9111179207214892, 'errorList': [], 'lossList': [0.0, -1.3513954710960387, 0.0, 6.438683638572693, 0.0, 0.0, 0.0], 'rewardMean': 0.801815560243412, 'totalEpisodes': 120, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 979.3172234123076
'totalSteps': 23040, 'rewardStep': 0.52043509489504, 'errorList': [], 'lossList': [0.0, -1.3038088470697402, 0.0, 6.848661786317825, 0.0, 0.0, 0.0], 'rewardMean': 0.7697674161487457, 'totalEpisodes': 122, 'stepsPerEpisode': 425, 'rewardPerEpisode': 293.7619988190517
'totalSteps': 24320, 'rewardStep': 0.41647723801632824, 'errorList': [], 'lossList': [0.0, -1.2692202240228654, 0.0, 5.512541276216507, 0.0, 0.0, 0.0], 'rewardMean': 0.7599237158699989, 'totalEpisodes': 122, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 832.1625365697673
'totalSteps': 25600, 'rewardStep': 0.9572662558719554, 'errorList': [0.5287561800206056, 0.47422936384391656, 0.45236068550426783, 0.4809917163715466, 0.5173325655295985, 0.465119376459172, 0.4275211028789874, 0.47242357599886736, 0.5051842474225486, 0.4949115889632666, 0.5104282240586424, 0.47833335879748057, 0.49183952319057095, 0.5159376923916779, 0.4202057693013259, 0.4937919741183037, 0.5275762954846674, 0.49455449523978734, 0.4478472404125271, 0.5206287332672253, 0.4786856310910597, 0.52745394839814, 0.48586738768584187, 0.46290086045522394, 0.4774219715243954, 0.45889420227561034, 0.4637041377328175, 0.43356027622619986, 0.4809930856832677, 0.423511813368444, 0.4315477714410579, 0.4810272119977419, 0.5149683121431978, 0.47606163124363143, 0.5143064017491384, 0.48421339475150704, 0.5149097542181512, 0.49900349223065266, 0.4360990054288623, 0.4200634394656514, 0.5042471908121415, 0.4977985504768852, 0.5040755431631477, 0.4597442725337662, 0.47554745962764433, 0.4854252791362071, 0.43032591761977484, 0.5281837269704163, 0.5387315322076495, 0.4434238064342675], 'lossList': [0.0, -1.2590229260921477, 0.0, 8.43987207531929, 0.0, 0.0, 0.0], 'rewardMean': 0.7604118968081128, 'totalEpisodes': 125, 'stepsPerEpisode': 43, 'rewardPerEpisode': 34.839321477281196, 'successfulTests': 0
'totalSteps': 26880, 'rewardStep': 0.7658852550997444, 'errorList': [], 'lossList': [0.0, -1.253086695075035, 0.0, 3.1905886006355284, 0.0, 0.0, 0.0], 'rewardMean': 0.7516030000839071, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.6440096552228
'totalSteps': 28160, 'rewardStep': 0.700533592157465, 'errorList': [], 'lossList': [0.0, -1.2379878485202789, 0.0, 1.8486132526397705, 0.0, 0.0, 0.0], 'rewardMean': 0.7687553100659394, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1057.6737627748375
'totalSteps': 29440, 'rewardStep': 0.8312510284728766, 'errorList': [], 'lossList': [0.0, -1.2208433192968369, 0.0, 0.8604007575660944, 0.0, 0.0, 0.0], 'rewardMean': 0.77212478924903, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1119.5239994713447
'totalSteps': 30720, 'rewardStep': 0.8587093475311898, 'errorList': [], 'lossList': [0.0, -1.207945789694786, 0.0, 0.7955369440466166, 0.0, 0.0, 0.0], 'rewardMean': 0.774551081584887, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.8399970937373
'totalSteps': 32000, 'rewardStep': 0.917312297218489, 'errorList': [], 'lossList': [0.0, -1.18748515188694, 0.0, 0.5999614576622844, 0.0, 0.0, 0.0], 'rewardMean': 0.7820607508296753, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.9193481413013
'totalSteps': 33280, 'rewardStep': 0.9452260846728765, 'errorList': [0.030919096283846, 0.018253083548480827, 0.03434254819930017, 0.02143579740731543, 0.05564961962008687, 0.02628370599582284, 0.021365903628100272, 0.021925499391057184, 0.028830228604063438, 0.02127414902820017, 0.02474914355270091, 0.02269542477653379, 0.021052019036679734, 0.0273845794997535, 0.021575696529633547, 0.032977264914750344, 0.02222370491045815, 0.020563314277210996, 0.0357474466832346, 0.02270424467639175, 0.04558735317419984, 0.021232737540866527, 0.019406641272927568, 0.0185386369413326, 0.035800592916223194, 0.01982363087993529, 0.019208122578542593, 0.03023755000030177, 0.021120294554104056, 0.040658192469717816, 0.02167013360295902, 0.020161870188605227, 0.023663816417596607, 0.03342805649889319, 0.03010196204138255, 0.033026061017397246, 0.0445434993319869, 0.020677980050961945, 0.019076888692291395, 0.023474145812239697, 0.030304168578154912, 0.0352718974834185, 0.024366788821213035, 0.022254524577381637, 0.0252086601505581, 0.019928983703829596, 0.019999980137012038, 0.022226208437675404, 0.020700787426841626, 0.03683627325125143], 'lossList': [0.0, -1.1344190657138824, 0.0, 0.49770754324272276, 0.0, 0.0, 0.0], 'rewardMean': 0.7824214114657454, 'totalEpisodes': 125, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1208.835170950213, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=33280, timeSpent=93.83
