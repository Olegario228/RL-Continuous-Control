#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v4_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v4_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 7000, 14000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5371426615508459, 'errorList': [], 'lossList': [0.0, -1.4251431292295456, 0.0, 84.48239258766175, 0.0, 0.0, 0.0], 'rewardMean': 0.5371426615508459, 'totalEpisodes': 6, 'stepsPerEpisode': 168, 'rewardPerEpisode': 119.63663339514541
'totalSteps': 2560, 'rewardStep': 0.9230491441921866, 'errorList': [], 'lossList': [0.0, -1.4140464401245116, 0.0, 30.342431704998017, 0.0, 0.0, 0.0], 'rewardMean': 0.7300959028715163, 'totalEpisodes': 12, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.474831055592304
'totalSteps': 3840, 'rewardStep': 0.5981979918941253, 'errorList': [], 'lossList': [0.0, -1.4012728238105774, 0.0, 22.97545709967613, 0.0, 0.0, 0.0], 'rewardMean': 0.6861299325457192, 'totalEpisodes': 15, 'stepsPerEpisode': 186, 'rewardPerEpisode': 130.90566709755484
'totalSteps': 5120, 'rewardStep': 0.6201933140605085, 'errorList': [], 'lossList': [0.0, -1.3974952042102813, 0.0, 17.83756891965866, 0.0, 0.0, 0.0], 'rewardMean': 0.6696457779244165, 'totalEpisodes': 15, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 922.4212031632029
'totalSteps': 6400, 'rewardStep': 0.8014707960579361, 'errorList': [], 'lossList': [0.0, -1.3800172233581542, 0.0, 38.27224908828735, 0.0, 0.0, 0.0], 'rewardMean': 0.6960107815511204, 'totalEpisodes': 17, 'stepsPerEpisode': 865, 'rewardPerEpisode': 710.2695618132585
'totalSteps': 7680, 'rewardStep': 0.8602478154658919, 'errorList': [], 'lossList': [0.0, -1.3623620545864106, 0.0, 15.64489092350006, 0.0, 0.0, 0.0], 'rewardMean': 0.7233836205369156, 'totalEpisodes': 17, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.504217634861
'totalSteps': 8960, 'rewardStep': 0.8377927591775779, 'errorList': [], 'lossList': [0.0, -1.3496259534358979, 0.0, 36.00972026348114, 0.0, 0.0, 0.0], 'rewardMean': 0.7397277831998673, 'totalEpisodes': 19, 'stepsPerEpisode': 311, 'rewardPerEpisode': 245.99866909059324
'totalSteps': 10240, 'rewardStep': 0.9029153098843596, 'errorList': [], 'lossList': [0.0, -1.3419334644079208, 0.0, 196.5365161895752, 0.0, 0.0, 0.0], 'rewardMean': 0.760126224035429, 'totalEpisodes': 32, 'stepsPerEpisode': 131, 'rewardPerEpisode': 108.85907161401595
'totalSteps': 11520, 'rewardStep': 0.6630672785421639, 'errorList': [], 'lossList': [0.0, -1.3357631117105484, 0.0, 213.7039236831665, 0.0, 0.0, 0.0], 'rewardMean': 0.7493418967583995, 'totalEpisodes': 58, 'stepsPerEpisode': 48, 'rewardPerEpisode': 36.31250940765769
'totalSteps': 12800, 'rewardStep': 0.6246565616948014, 'errorList': [], 'lossList': [0.0, -1.3358256208896637, 0.0, 83.63051701545716, 0.0, 0.0, 0.0], 'rewardMean': 0.7368733632520397, 'totalEpisodes': 92, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6246565616948014
'totalSteps': 14080, 'rewardStep': 0.6268112317483802, 'errorList': [], 'lossList': [0.0, -1.3359331780672072, 0.0, 36.688920679092405, 0.0, 0.0, 0.0], 'rewardMean': 0.7458402202717932, 'totalEpisodes': 114, 'stepsPerEpisode': 78, 'rewardPerEpisode': 64.31413157040367
'totalSteps': 15360, 'rewardStep': 0.7533777364637405, 'errorList': [], 'lossList': [0.0, -1.3320645195245744, 0.0, 43.863841953277586, 0.0, 0.0, 0.0], 'rewardMean': 0.7288730794989485, 'totalEpisodes': 135, 'stepsPerEpisode': 122, 'rewardPerEpisode': 108.24070306605667
'totalSteps': 16640, 'rewardStep': 0.6870498236575229, 'errorList': [], 'lossList': [0.0, -1.3283050352334975, 0.0, 29.458078985214232, 0.0, 0.0, 0.0], 'rewardMean': 0.7377582626752883, 'totalEpisodes': 150, 'stepsPerEpisode': 203, 'rewardPerEpisode': 165.81070243014003
'totalSteps': 17920, 'rewardStep': 0.8192431363840262, 'errorList': [], 'lossList': [0.0, -1.3260231977701187, 0.0, 18.584937479496002, 0.0, 0.0, 0.0], 'rewardMean': 0.75766324490764, 'totalEpisodes': 156, 'stepsPerEpisode': 53, 'rewardPerEpisode': 46.43973395455411
'totalSteps': 19200, 'rewardStep': 0.7985855128558873, 'errorList': [], 'lossList': [0.0, -1.3117101442813874, 0.0, 13.689833929538727, 0.0, 0.0, 0.0], 'rewardMean': 0.7573747165874353, 'totalEpisodes': 162, 'stepsPerEpisode': 56, 'rewardPerEpisode': 47.67084684235607
'totalSteps': 20480, 'rewardStep': 0.5866318651044815, 'errorList': [], 'lossList': [0.0, -1.2786809605360032, 0.0, 11.910741698741912, 0.0, 0.0, 0.0], 'rewardMean': 0.7300131215512943, 'totalEpisodes': 163, 'stepsPerEpisode': 445, 'rewardPerEpisode': 329.0086477098085
'totalSteps': 21760, 'rewardStep': 0.7394574288511686, 'errorList': [], 'lossList': [0.0, -1.2467241328954697, 0.0, 7.713107698559761, 0.0, 0.0, 0.0], 'rewardMean': 0.7201795885186532, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.9062139855331
'totalSteps': 23040, 'rewardStep': 0.885845078189762, 'errorList': [], 'lossList': [0.0, -1.2176093405485153, 0.0, 4.56000626027584, 0.0, 0.0, 0.0], 'rewardMean': 0.7184725653491933, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1070.3007666055541
'totalSteps': 24320, 'rewardStep': 0.7803818405264799, 'errorList': [], 'lossList': [0.0, -1.1847861868143081, 0.0, 3.133533546924591, 0.0, 0.0, 0.0], 'rewardMean': 0.7302040215476251, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1130.3163171065364
'totalSteps': 25600, 'rewardStep': 0.9672493270570384, 'errorList': [0.01564768067497736, 0.018030564738549786, 0.011995778427875664, 0.013047774478080329, 0.012985559215478344, 0.014867255593557803, 0.028098700524463633, 0.012025192720952186, 0.026387538122506247, 0.024861408296500766, 0.020174310328321406, 0.0177152534864164, 0.023456637420139155, 0.019179613436041878, 0.030857718029450074, 0.021332886292579494, 0.01735007226889734, 0.029854738708605617, 0.012043645433891373, 0.017087043671270973, 0.01715959272738595, 0.014843909203410848, 0.015319940216456641, 0.021572671451850513, 0.01647017620491619, 0.014689172332435167, 0.01259911953134753, 0.012343641001032793, 0.013971119537030941, 0.013128448093383838, 0.029329760062600317, 0.016373410285304178, 0.012807895543670003, 0.01388251265845136, 0.014632279115860587, 0.018943797061996608, 0.029884967036134944, 0.016100105491026914, 0.013907597347845595, 0.014228374157137656, 0.01661102945246462, 0.0141536612229542, 0.01973012924564338, 0.019491327956963816, 0.01439266436922008, 0.028683950946160237, 0.01798503640983085, 0.021184449414072146, 0.028643805158941233, 0.013790595660490942], 'lossList': [0.0, -1.158912057876587, 0.0, 2.715751207657158, 0.0, 0.0, 0.0], 'rewardMean': 0.7644632980838486, 'totalEpisodes': 163, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1161.0987909658663, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=56.86
