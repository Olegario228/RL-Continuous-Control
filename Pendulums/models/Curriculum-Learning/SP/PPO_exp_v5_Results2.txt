#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000, 16000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8280920161904477, 'errorList': [], 'lossList': [0.0, -1.41705382168293, 0.0, 82.44616906642914, 0.0, 0.0, 0.0], 'rewardMean': 0.8280920161904477, 'totalEpisodes': 5, 'stepsPerEpisode': 241, 'rewardPerEpisode': 205.5822162166961
'totalSteps': 2560, 'rewardStep': 0.6823505079940079, 'errorList': [], 'lossList': [0.0, -1.431431660056114, 0.0, 24.357220385074616, 0.0, 0.0, 0.0], 'rewardMean': 0.7552212620922278, 'totalEpisodes': 11, 'stepsPerEpisode': 549, 'rewardPerEpisode': 349.0606707163415
'totalSteps': 3840, 'rewardStep': 0.8158332148716162, 'errorList': [], 'lossList': [0.0, -1.4293188971281052, 0.0, 28.39004186153412, 0.0, 0.0, 0.0], 'rewardMean': 0.7754252463520239, 'totalEpisodes': 16, 'stepsPerEpisode': 76, 'rewardPerEpisode': 49.97748698008227
'totalSteps': 5120, 'rewardStep': 0.40155281806570026, 'errorList': [], 'lossList': [0.0, -1.396449589729309, 0.0, 19.168232998847962, 0.0, 0.0, 0.0], 'rewardMean': 0.681957139280443, 'totalEpisodes': 20, 'stepsPerEpisode': 276, 'rewardPerEpisode': 174.48358229710985
'totalSteps': 6400, 'rewardStep': 0.2135294710264689, 'errorList': [], 'lossList': [0.0, -1.381508018374443, 0.0, 31.884376814365385, 0.0, 0.0, 0.0], 'rewardMean': 0.5882716056296482, 'totalEpisodes': 22, 'stepsPerEpisode': 559, 'rewardPerEpisode': 385.9631120826061
'totalSteps': 7680, 'rewardStep': 0.6061493869126569, 'errorList': [], 'lossList': [0.0, -1.3615487200021743, 0.0, 15.79511111319065, 0.0, 0.0, 0.0], 'rewardMean': 0.5912512358434829, 'totalEpisodes': 23, 'stepsPerEpisode': 821, 'rewardPerEpisode': 651.5573989195788
'totalSteps': 8960, 'rewardStep': 0.9620350945462016, 'errorList': [], 'lossList': [0.0, -1.3495024502277375, 0.0, 11.264566361308098, 0.0, 0.0, 0.0], 'rewardMean': 0.6442203585152999, 'totalEpisodes': 23, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.0262482475398
'totalSteps': 10240, 'rewardStep': 0.8065397722790213, 'errorList': [], 'lossList': [0.0, -1.3307751685380935, 0.0, 8.335464549660683, 0.0, 0.0, 0.0], 'rewardMean': 0.6645102852357652, 'totalEpisodes': 23, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1067.2709194077622
'totalSteps': 11520, 'rewardStep': 0.7656337262189039, 'errorList': [], 'lossList': [0.0, -1.31920749604702, 0.0, 132.77406436920165, 0.0, 0.0, 0.0], 'rewardMean': 0.6757462231227805, 'totalEpisodes': 30, 'stepsPerEpisode': 65, 'rewardPerEpisode': 54.904821618649905
'totalSteps': 12800, 'rewardStep': 0.9793345876708447, 'errorList': [], 'lossList': [0.0, -1.314812769293785, 0.0, 294.55728912353516, 0.0, 0.0, 0.0], 'rewardMean': 0.7061050595775871, 'totalEpisodes': 58, 'stepsPerEpisode': 23, 'rewardPerEpisode': 21.416852987825955
'totalSteps': 14080, 'rewardStep': 0.645394422508724, 'errorList': [], 'lossList': [0.0, -1.3129497128725052, 0.0, 87.37554450988769, 0.0, 0.0, 0.0], 'rewardMean': 0.6878353002094146, 'totalEpisodes': 85, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.645394422508724
'totalSteps': 15360, 'rewardStep': 0.6485383571277534, 'errorList': [], 'lossList': [0.0, -1.3072455632686615, 0.0, 51.92456653594971, 0.0, 0.0, 0.0], 'rewardMean': 0.6844540851227892, 'totalEpisodes': 109, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.9410662336875
'totalSteps': 16640, 'rewardStep': 0.8360753884048726, 'errorList': [], 'lossList': [0.0, -1.2992565608024598, 0.0, 28.26651327133179, 0.0, 0.0, 0.0], 'rewardMean': 0.6864783024761147, 'totalEpisodes': 129, 'stepsPerEpisode': 159, 'rewardPerEpisode': 141.49205645011875
'totalSteps': 17920, 'rewardStep': 0.7060769696192796, 'errorList': [], 'lossList': [0.0, -1.2933018416166306, 0.0, 21.54809413909912, 0.0, 0.0, 0.0], 'rewardMean': 0.7169307176314726, 'totalEpisodes': 140, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.359167983333233
'totalSteps': 19200, 'rewardStep': 0.6253641102443672, 'errorList': [], 'lossList': [0.0, -1.2908258473873138, 0.0, 14.76419629573822, 0.0, 0.0, 0.0], 'rewardMean': 0.7581141815532625, 'totalEpisodes': 146, 'stepsPerEpisode': 221, 'rewardPerEpisode': 181.41631605601975
'totalSteps': 20480, 'rewardStep': 0.5219521492923846, 'errorList': [], 'lossList': [0.0, -1.2854078298807143, 0.0, 9.570104669332505, 0.0, 0.0, 0.0], 'rewardMean': 0.7496944577912352, 'totalEpisodes': 149, 'stepsPerEpisode': 113, 'rewardPerEpisode': 82.6146418970818
'totalSteps': 21760, 'rewardStep': 0.5901550399662776, 'errorList': [], 'lossList': [0.0, -1.2743490982055663, 0.0, 6.975274035930633, 0.0, 0.0, 0.0], 'rewardMean': 0.7125064523332428, 'totalEpisodes': 150, 'stepsPerEpisode': 711, 'rewardPerEpisode': 531.6078618800221
'totalSteps': 23040, 'rewardStep': 0.9439727576322119, 'errorList': [0.29241599458929074, 0.2689806304056585, 0.15186592631275414, 0.3031119268118998, 0.49381075188464707, 0.4169689181972461, 0.29190032040619474, 0.6488773674395535, 0.3531679507419324, 0.23684099249674134, 0.18426074243384782, 0.5197741181544655, 0.1841938552346282, 0.32599638022561434, 0.22679188911413864, 0.3098146468285289, 0.19638609932050766, 0.17460095730284192, 0.20896221474432844, 0.1552747545077295, 0.6304461294302103, 0.30870821445215807, 0.3091438563020088, 0.31627987353920145, 0.5198071620767597, 0.35718415539172116, 0.28948389896981697, 0.2640323232043977, 0.21801037349455946, 0.6559156662785737, 0.328039507557319, 0.584838487158631, 0.6879266295491155, 0.45979272498395546, 0.3916827168198375, 0.5893242894180583, 0.24919698842166338, 0.24420402399406876, 0.5867476485165188, 0.14941244898376543, 0.2744265044468139, 0.23032997881729023, 0.4841656513521455, 0.34248444480761087, 0.4399157581185553, 0.34862930102246825, 0.25856356429960226, 0.4284783552741748, 0.24489256415211344, 0.23276171099870066], 'lossList': [0.0, -1.2531692761182784, 0.0, 6.833248335123062, 0.0, 0.0, 0.0], 'rewardMean': 0.7262497508685619, 'totalEpisodes': 154, 'stepsPerEpisode': 112, 'rewardPerEpisode': 99.73807270102337, 'successfulTests': 7
'totalSteps': 24320, 'rewardStep': 0.7361841582110377, 'errorList': [], 'lossList': [0.0, -1.234728499650955, 0.0, 4.532222735881805, 0.0, 0.0, 0.0], 'rewardMean': 0.7233047940677754, 'totalEpisodes': 154, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.8263574536934
'totalSteps': 25600, 'rewardStep': 0.6467446265609227, 'errorList': [], 'lossList': [0.0, -1.240847430229187, 0.0, 6.22641157746315, 0.0, 0.0, 0.0], 'rewardMean': 0.6900457979567832, 'totalEpisodes': 157, 'stepsPerEpisode': 85, 'rewardPerEpisode': 69.62063592641647
'totalSteps': 26880, 'rewardStep': 0.8098703869011767, 'errorList': [], 'lossList': [0.0, -1.2501299560070038, 0.0, 5.116681144237519, 0.0, 0.0, 0.0], 'rewardMean': 0.7064933943960283, 'totalEpisodes': 159, 'stepsPerEpisode': 672, 'rewardPerEpisode': 547.2417157094997
'totalSteps': 28160, 'rewardStep': 0.669392255704395, 'errorList': [], 'lossList': [0.0, -1.2074790674448013, 0.0, 1.4691419273614883, 0.0, 0.0, 0.0], 'rewardMean': 0.7085787842536926, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.9449200102242
'totalSteps': 29440, 'rewardStep': 0.9095151748793145, 'errorList': [], 'lossList': [0.0, -1.1465961199998855, 0.0, 1.3160683024674653, 0.0, 0.0, 0.0], 'rewardMean': 0.7159227629011367, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.2923954542882
'totalSteps': 30720, 'rewardStep': 0.9775911697851986, 'errorList': [0.03880295605705014, 0.0815225630646277, 0.050743198454084246, 0.029850231843112217, 0.025507943975309765, 0.06294917551564992, 0.05916647290343428, 0.026375853004561235, 0.02505155557138358, 0.04625803145570114, 0.050391038219944916, 0.023666274546727383, 0.029296197603008264, 0.055343020057734955, 0.025801732464942054, 0.030993728915393898, 0.04895311204861933, 0.02951617055096557, 0.05822126245488398, 0.052848473701267246, 0.05036256953437322, 0.04028277797552032, 0.05438628925339971, 0.06612663780508386, 0.07480582069930691, 0.03146452168348094, 0.0335791451595504, 0.031479794695256996, 0.02322189005334586, 0.044788156613264324, 0.03056617527673756, 0.03360817045176317, 0.026987517740065844, 0.021784077859008762, 0.06455554495813219, 0.05571841389765289, 0.03354110798978612, 0.026462508232311728, 0.026041844905523862, 0.05947879463052178, 0.024309330399844876, 0.026310388138981577, 0.09371186138396455, 0.051382896983855765, 0.025178789078357362, 0.059744996350338316, 0.053486863137353464, 0.06651106465949733, 0.036468096471400674, 0.038799516502870904], 'lossList': [0.0, -1.0924365848302842, 0.0, 0.9925003966316581, 0.0, 0.0, 0.0], 'rewardMean': 0.7430741829177286, 'totalEpisodes': 159, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1183.5772461584102, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=30720, timeSpent=85.83
