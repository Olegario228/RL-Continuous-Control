#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000, 16000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.720468672746902, 'errorList': [], 'lossList': [0.0, -1.4278227895498277, 0.0, 82.66252764225005, 0.0, 0.0, 0.0], 'rewardMean': 0.720468672746902, 'totalEpisodes': 6, 'stepsPerEpisode': 204, 'rewardPerEpisode': 144.31493761881387
'totalSteps': 2560, 'rewardStep': 0.7394789512825355, 'errorList': [], 'lossList': [0.0, -1.4371810925006867, 0.0, 31.950340665578842, 0.0, 0.0, 0.0], 'rewardMean': 0.7299738120147188, 'totalEpisodes': 10, 'stepsPerEpisode': 25, 'rewardPerEpisode': 21.973846592178777
'totalSteps': 3840, 'rewardStep': 0.8823149229771323, 'errorList': [], 'lossList': [0.0, -1.4388823515176774, 0.0, 28.539806969165802, 0.0, 0.0, 0.0], 'rewardMean': 0.7807541823355232, 'totalEpisodes': 12, 'stepsPerEpisode': 529, 'rewardPerEpisode': 384.85659110778033
'totalSteps': 5120, 'rewardStep': 0.7872913545079037, 'errorList': [], 'lossList': [0.0, -1.435578734278679, 0.0, 32.2244726139307, 0.0, 0.0, 0.0], 'rewardMean': 0.7823884753786183, 'totalEpisodes': 13, 'stepsPerEpisode': 181, 'rewardPerEpisode': 153.6402592072173
'totalSteps': 6400, 'rewardStep': 0.5380093408671988, 'errorList': [], 'lossList': [0.0, -1.4276250809431077, 0.0, 14.810354318022728, 0.0, 0.0, 0.0], 'rewardMean': 0.7335126484763344, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 953.8010380940152
'totalSteps': 7680, 'rewardStep': 0.8937244742095476, 'errorList': [], 'lossList': [0.0, -1.3941685616970063, 0.0, 16.54304382622242, 0.0, 0.0, 0.0], 'rewardMean': 0.7602146194318699, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.4513694620528
'totalSteps': 8960, 'rewardStep': 0.7378016013599683, 'errorList': [], 'lossList': [0.0, -1.3797235506772996, 0.0, 10.328394825458526, 0.0, 0.0, 0.0], 'rewardMean': 0.7570127597073125, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1043.583256759886
'totalSteps': 10240, 'rewardStep': 0.8094368117038058, 'errorList': [], 'lossList': [0.0, -1.36972751557827, 0.0, 4.699409461021423, 0.0, 0.0, 0.0], 'rewardMean': 0.7635657662068742, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.5617223093255
'totalSteps': 11520, 'rewardStep': 0.5560961140693175, 'errorList': [], 'lossList': [0.0, -1.3673747968673706, 0.0, 103.4932969903946, 0.0, 0.0, 0.0], 'rewardMean': 0.7405135826360346, 'totalEpisodes': 20, 'stepsPerEpisode': 16, 'rewardPerEpisode': 9.31330911150658
'totalSteps': 12800, 'rewardStep': 0.9021696074156041, 'errorList': [], 'lossList': [0.0, -1.363551122546196, 0.0, 321.15770538330077, 0.0, 0.0, 0.0], 'rewardMean': 0.7566791851139916, 'totalEpisodes': 42, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.873769491484072
'totalSteps': 14080, 'rewardStep': 0.6025358636643923, 'errorList': [], 'lossList': [0.0, -1.360497196316719, 0.0, 155.01997032165528, 0.0, 0.0, 0.0], 'rewardMean': 0.7448859042057405, 'totalEpisodes': 59, 'stepsPerEpisode': 95, 'rewardPerEpisode': 71.58689377725062
'totalSteps': 15360, 'rewardStep': 0.7955153691000804, 'errorList': [], 'lossList': [0.0, -1.3569695085287095, 0.0, 97.82190654754639, 0.0, 0.0, 0.0], 'rewardMean': 0.7504895459874951, 'totalEpisodes': 77, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.191061761372323
'totalSteps': 16640, 'rewardStep': 0.4884215223062777, 'errorList': [], 'lossList': [0.0, -1.3543022173643111, 0.0, 33.385239934921266, 0.0, 0.0, 0.0], 'rewardMean': 0.7111002059204096, 'totalEpisodes': 92, 'stepsPerEpisode': 37, 'rewardPerEpisode': 22.436198679974012
'totalSteps': 17920, 'rewardStep': 0.7204190618929519, 'errorList': [], 'lossList': [0.0, -1.3491162830591201, 0.0, 23.425612874031067, 0.0, 0.0, 0.0], 'rewardMean': 0.7044129766589144, 'totalEpisodes': 106, 'stepsPerEpisode': 51, 'rewardPerEpisode': 40.184498463686324
'totalSteps': 19200, 'rewardStep': 0.6987695145464398, 'errorList': [], 'lossList': [0.0, -1.346512874364853, 0.0, 21.447789573669432, 0.0, 0.0, 0.0], 'rewardMean': 0.7204889940268384, 'totalEpisodes': 112, 'stepsPerEpisode': 74, 'rewardPerEpisode': 59.97815336931322
'totalSteps': 20480, 'rewardStep': 0.8359665984047211, 'errorList': [], 'lossList': [0.0, -1.3453105348348617, 0.0, 17.55645651102066, 0.0, 0.0, 0.0], 'rewardMean': 0.7147132064463558, 'totalEpisodes': 116, 'stepsPerEpisode': 85, 'rewardPerEpisode': 68.81040293635601
'totalSteps': 21760, 'rewardStep': 0.9008457282512453, 'errorList': [], 'lossList': [0.0, -1.3360869592428208, 0.0, 6.799434292316437, 0.0, 0.0, 0.0], 'rewardMean': 0.7310176191354836, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 847.780992942311
'totalSteps': 23040, 'rewardStep': 0.7166736460909815, 'errorList': [], 'lossList': [0.0, -1.3060466402769089, 0.0, 5.180157236754894, 0.0, 0.0, 0.0], 'rewardMean': 0.7217413025742012, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1055.0511824800797
'totalSteps': 24320, 'rewardStep': 0.6666083090435423, 'errorList': [], 'lossList': [0.0, -1.2779532724618912, 0.0, 4.224990657866001, 0.0, 0.0, 0.0], 'rewardMean': 0.7327925220716236, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1076.394150784007
'totalSteps': 25600, 'rewardStep': 0.7164110912362602, 'errorList': [], 'lossList': [0.0, -1.2746164429187774, 0.0, 1.9659667076915504, 0.0, 0.0, 0.0], 'rewardMean': 0.7142166704536892, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1005.287613522719
'totalSteps': 26880, 'rewardStep': 0.597445779117032, 'errorList': [], 'lossList': [0.0, -1.2604008185863496, 0.0, 0.8251716025918722, 0.0, 0.0, 0.0], 'rewardMean': 0.7137076619989531, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 898.6806830030979
'totalSteps': 28160, 'rewardStep': 0.950451955283629, 'errorList': [0.7736026280816404, 0.5387342427304996, 0.6830641467447505, 0.4489561499103387, 0.3551049546519968, 0.7093574849648708, 0.6781642964820994, 0.41064387780946143, 0.6499349413945702, 0.521488773805056, 0.7837361359924567, 0.5428729858092949, 0.2986472827108879, 0.5450149599284829, 0.24667645103400912, 0.5400706399283328, 0.5394241243971692, 0.27231454387136494, 0.33490627175706905, 0.42556101614423036, 0.8272180584714445, 0.28364410099357057, 1.0563673749324092, 0.5195535095500703, 0.3653914977291982, 0.5762157192371068, 0.2375082814528051, 0.7477818039733007, 0.3390076900492976, 0.3937370578873178, 0.6648445898434991, 0.2997116247485273, 0.25883617079997717, 0.3295259553779832, 0.8033191128113799, 0.4127374120496958, 0.35431183295281504, 0.3108744267252412, 0.4583265148184935, 0.5737154330588754, 0.655275183600092, 0.2476666441008378, 0.23190843060642438, 0.6788579239067211, 0.4309961547812886, 0.487375715959882, 0.5886007179857161, 0.2972381336335642, 0.19574043192304677, 0.5261774110966377], 'lossList': [0.0, -1.2438628423213958, 0.0, 4.267049043178559, 0.0, 0.0, 0.0], 'rewardMean': 0.7292013206173081, 'totalEpisodes': 118, 'stepsPerEpisode': 74, 'rewardPerEpisode': 65.7155216572508, 'successfulTests': 1
'totalSteps': 29440, 'rewardStep': 0.8027412303154542, 'errorList': [], 'lossList': [0.0, -1.2500103205442428, 0.0, 3.727441679239273, 0.0, 0.0, 0.0], 'rewardMean': 0.7606332914182257, 'totalEpisodes': 119, 'stepsPerEpisode': 258, 'rewardPerEpisode': 215.52012207987858
'totalSteps': 30720, 'rewardStep': 0.6728927416927234, 'errorList': [], 'lossList': [0.0, -1.2579210954904556, 0.0, 0.9628178682923317, 0.0, 0.0, 0.0], 'rewardMean': 0.7558806593982028, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1024.688622629942
'totalSteps': 32000, 'rewardStep': 0.869129930834087, 'errorList': [], 'lossList': [0.0, -1.2379941499233247, 0.0, 0.8563275279849768, 0.0, 0.0, 0.0], 'rewardMean': 0.7729167010269675, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1138.3273001270043
'totalSteps': 33280, 'rewardStep': 0.9510505779511008, 'errorList': [0.19667102993861282, 0.18673135908014177, 0.19105281633656337, 0.17771057282172653, 0.1970379645573759, 0.13093695260998703, 0.14392405400719, 0.15157081480931336, 0.3160065474582678, 0.17502566028927585, 0.20125450237486534, 0.19084540676023662, 0.19669890717255095, 0.14732103821732262, 0.16700924819982368, 0.22366310198267322, 0.1747920402470963, 0.1881052375084225, 0.357978623184904, 0.19769860146662674, 0.3122887794083, 0.17720935748675934, 0.13528068918673158, 0.23321529790494472, 0.23380858043731106, 0.19921428315981252, 0.18461331328704841, 0.14167793641723536, 0.14495471647684383, 0.18636480146219922, 0.19634202488388003, 0.1998432612839322, 0.18594516952855783, 0.14973965821224297, 0.2844776764745648, 0.18973295754857739, 0.17844773917895138, 0.14967331799555017, 0.20291108431428628, 0.20491271354559532, 0.16451060739148823, 0.27772599513053065, 0.1301757091105449, 0.18076301445210208, 0.32144876071313516, 0.1509813537768621, 0.1399102856295416, 0.326755776834516, 0.1467237718181662, 0.1722276612539339], 'lossList': [0.0, -1.1990755051374435, 0.0, 0.5589522026479244, 0.0, 0.0, 0.0], 'rewardMean': 0.7844250989816056, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1176.6074083667445, 'successfulTests': 37
'totalSteps': 34560, 'rewardStep': 0.8754458556074977, 'errorList': [], 'lossList': [0.0, -1.190692857503891, 0.0, 0.2869149457104504, 0.0, 0.0, 0.0], 'rewardMean': 0.7818851117172307, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1150.0278427382223
'totalSteps': 35840, 'rewardStep': 0.9782814981327623, 'errorList': [1.114472762029356, 0.4730905518138763, 0.1500576550709964, 0.15598234683230866, 0.07408323597358935, 0.3128727106972115, 0.2550099622125125, 0.20097816649154307, 0.08082469794668343, 0.5060084958010842, 0.3203722684062588, 0.2682224581775445, 0.09775877210876233, 0.12287706090172618, 0.10432645675269372, 0.23941907064543363, 0.22822613176838621, 0.5633619312848052, 0.28659043790811345, 0.9393298114370621, 0.13567315403591812, 0.6850413614574228, 0.26934390814208736, 0.4514573529367709, 0.3313769317714028, 0.9678610800624956, 0.1463898301373142, 0.3536668447756616, 0.757676280678345, 0.3199175640178667, 0.4000709825455272, 0.4979978260112483, 0.8283712058099517, 0.6005309278354681, 1.1520476315671637, 0.49833860123434837, 0.5149070558889487, 0.308643397825988, 0.2257916483525299, 0.11310089819502028, 0.30219616195744414, 0.2543803062758955, 0.1406350127794542, 0.055766739236645504, 0.3220222212192644, 0.32782703662422985, 0.07595452730784572, 0.07901426446957721, 0.06708099191858005, 0.15216627360533358], 'lossList': [0.0, -1.1612476819753648, 0.0, 0.43373413203284145, 0.0, 0.0, 0.0], 'rewardMean': 0.8080458969214088, 'totalEpisodes': 119, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1216.2042723487252, 'successfulTests': 16
#maxSuccessfulTests=37, maxSuccessfulTestsAtStep=33280, timeSpent=107.92
