#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v5_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v5_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 8000, 16000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5371426615508459, 'errorList': [], 'lossList': [0.0, -1.4251431292295456, 0.0, 84.48239258766175, 0.0, 0.0, 0.0], 'rewardMean': 0.5371426615508459, 'totalEpisodes': 6, 'stepsPerEpisode': 168, 'rewardPerEpisode': 119.63663339514541
'totalSteps': 2560, 'rewardStep': 0.9199629991349041, 'errorList': [], 'lossList': [0.0, -1.413911634683609, 0.0, 22.251304124593734, 0.0, 0.0, 0.0], 'rewardMean': 0.728552830342875, 'totalEpisodes': 9, 'stepsPerEpisode': 22, 'rewardPerEpisode': 20.236658458634903
'totalSteps': 3840, 'rewardStep': 0.45237869175010653, 'errorList': [], 'lossList': [0.0, -1.3973867231607437, 0.0, 24.023374457359314, 0.0, 0.0, 0.0], 'rewardMean': 0.6364947841452855, 'totalEpisodes': 13, 'stepsPerEpisode': 171, 'rewardPerEpisode': 112.36503086702844
'totalSteps': 5120, 'rewardStep': 0.570448535850639, 'errorList': [], 'lossList': [0.0, -1.4050697028636931, 0.0, 19.062010474205017, 0.0, 0.0, 0.0], 'rewardMean': 0.6199832220716239, 'totalEpisodes': 14, 'stepsPerEpisode': 723, 'rewardPerEpisode': 513.8643159991191
'totalSteps': 6400, 'rewardStep': 0.7509431737789033, 'errorList': [], 'lossList': [0.0, -1.4024966162443162, 0.0, 29.583812053203584, 0.0, 0.0, 0.0], 'rewardMean': 0.6461752124130797, 'totalEpisodes': 16, 'stepsPerEpisode': 866, 'rewardPerEpisode': 641.6390889304671
'totalSteps': 7680, 'rewardStep': 0.965312418936639, 'errorList': [], 'lossList': [0.0, -1.413150371313095, 0.0, 24.22711897969246, 0.0, 0.0, 0.0], 'rewardMean': 0.6993647468336729, 'totalEpisodes': 17, 'stepsPerEpisode': 309, 'rewardPerEpisode': 217.64922843832247
'totalSteps': 8960, 'rewardStep': 0.6141761135341155, 'errorList': [], 'lossList': [0.0, -1.427928831577301, 0.0, 23.71869718313217, 0.0, 0.0, 0.0], 'rewardMean': 0.6871949420765934, 'totalEpisodes': 18, 'stepsPerEpisode': 1123, 'rewardPerEpisode': 788.9259396647969
'totalSteps': 10240, 'rewardStep': 0.7575837921947923, 'errorList': [], 'lossList': [0.0, -1.4094780629873276, 0.0, 35.72907754659653, 0.0, 0.0, 0.0], 'rewardMean': 0.6959935483413682, 'totalEpisodes': 20, 'stepsPerEpisode': 705, 'rewardPerEpisode': 456.3676513134902
'totalSteps': 11520, 'rewardStep': 0.5729875526294477, 'errorList': [], 'lossList': [0.0, -1.4099618178606033, 0.0, 154.55059745788574, 0.0, 0.0, 0.0], 'rewardMean': 0.6823262154844881, 'totalEpisodes': 30, 'stepsPerEpisode': 82, 'rewardPerEpisode': 62.58002925890435
'totalSteps': 12800, 'rewardStep': 0.8066651527931039, 'errorList': [], 'lossList': [0.0, -1.4125205844640731, 0.0, 195.31925987243653, 0.0, 0.0, 0.0], 'rewardMean': 0.6947601092153497, 'totalEpisodes': 51, 'stepsPerEpisode': 54, 'rewardPerEpisode': 48.52540114977259
'totalSteps': 14080, 'rewardStep': 0.6401368378079916, 'errorList': [], 'lossList': [0.0, -1.4059597408771516, 0.0, 132.99098411560058, 0.0, 0.0, 0.0], 'rewardMean': 0.7050595268410643, 'totalEpisodes': 69, 'stepsPerEpisode': 121, 'rewardPerEpisode': 95.98222628696215
'totalSteps': 15360, 'rewardStep': 0.41793595727636024, 'errorList': [], 'lossList': [0.0, -1.399288769364357, 0.0, 71.22903248786926, 0.0, 0.0, 0.0], 'rewardMean': 0.65485682265521, 'totalEpisodes': 78, 'stepsPerEpisode': 255, 'rewardPerEpisode': 211.14154829624135
'totalSteps': 16640, 'rewardStep': 0.29729489036440504, 'errorList': [], 'lossList': [0.0, -1.3955923753976822, 0.0, 56.59985447406769, 0.0, 0.0, 0.0], 'rewardMean': 0.6393484425166398, 'totalEpisodes': 88, 'stepsPerEpisode': 99, 'rewardPerEpisode': 60.91584580696672
'totalSteps': 17920, 'rewardStep': 0.728880983862449, 'errorList': [], 'lossList': [0.0, -1.3911931014060974, 0.0, 27.977165701389314, 0.0, 0.0, 0.0], 'rewardMean': 0.6551916873178207, 'totalEpisodes': 96, 'stepsPerEpisode': 54, 'rewardPerEpisode': 48.11903534098294
'totalSteps': 19200, 'rewardStep': 0.4129340797668064, 'errorList': [], 'lossList': [0.0, -1.3845383483171463, 0.0, 15.010395963191986, 0.0, 0.0, 0.0], 'rewardMean': 0.621390777916611, 'totalEpisodes': 102, 'stepsPerEpisode': 142, 'rewardPerEpisode': 92.95243603749412
'totalSteps': 20480, 'rewardStep': 0.9445071260951116, 'errorList': [3.231758660145979, 7.1156370288084005, 15.101826335854895, 27.843740707397142, 8.754132935515765, 7.027666506668416, 0.18437755713962628, 10.431574579356003, 3.355013788823434, 12.637870544255518, 1.3992296546498189, 18.69698266181727, 9.95317306383767, 2.306387434092943, 10.410293101410252, 1.5920908858299463, 11.660014848984279, 2.940679801833752, 19.291473646048537, 2.4310775890739316, 10.056858518231568, 1.398066024410251, 2.9647964863390444, 1.9506535093521808, 9.687272790952456, 6.219665453389095, 22.91598563930432, 15.87457463601239, 3.363801299199565, 8.33267368516899, 5.781216722239797, 9.482146350778082, 5.803695487585634, 8.643313375049424, 0.8801116836803976, 2.13975597988597, 0.3972937160204018, 6.300933752066952, 9.440591466820539, 9.230514077361937, 6.330229484506766, 0.9126925290064083, 7.986658424942724, 25.61189156186403, 5.177266968856752, 12.443264848730143, 6.67187244527717, 9.38467238928143, 16.987655966162283, 2.8130221965183675], 'lossList': [0.0, -1.3672016388177872, 0.0, 7.909429688453674, 0.0, 0.0, 0.0], 'rewardMean': 0.6193102486324584, 'totalEpisodes': 110, 'stepsPerEpisode': 33, 'rewardPerEpisode': 27.763665394697064, 'successfulTests': 1
'totalSteps': 21760, 'rewardStep': 0.6642061717743022, 'errorList': [], 'lossList': [0.0, -1.356836338043213, 0.0, 5.72153249502182, 0.0, 0.0, 0.0], 'rewardMean': 0.624313254456477, 'totalEpisodes': 115, 'stepsPerEpisode': 104, 'rewardPerEpisode': 89.0088663437289
'totalSteps': 23040, 'rewardStep': 0.8327892692876898, 'errorList': [], 'lossList': [0.0, -1.3673554068803788, 0.0, 19.16365395784378, 0.0, 0.0, 0.0], 'rewardMean': 0.6318338021657668, 'totalEpisodes': 119, 'stepsPerEpisode': 306, 'rewardPerEpisode': 247.35791602806088
'totalSteps': 24320, 'rewardStep': 0.815295030479029, 'errorList': [], 'lossList': [0.0, -1.3738851046562195, 0.0, 6.347338936924935, 0.0, 0.0, 0.0], 'rewardMean': 0.6560645499507249, 'totalEpisodes': 121, 'stepsPerEpisode': 228, 'rewardPerEpisode': 207.02889586211037
'totalSteps': 25600, 'rewardStep': 0.6396959645720965, 'errorList': [], 'lossList': [0.0, -1.3671720933914184, 0.0, 4.056136102676391, 0.0, 0.0, 0.0], 'rewardMean': 0.6393676311286242, 'totalEpisodes': 121, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 961.8536389587317
'totalSteps': 26880, 'rewardStep': 0.815974740970806, 'errorList': [], 'lossList': [0.0, -1.3521550869941712, 0.0, 4.093219032287598, 0.0, 0.0, 0.0], 'rewardMean': 0.6569514214449057, 'totalEpisodes': 123, 'stepsPerEpisode': 35, 'rewardPerEpisode': 29.09643013151874
'totalSteps': 28160, 'rewardStep': 0.6820417049130746, 'errorList': [], 'lossList': [0.0, -1.3453829097747803, 0.0, 4.535304750204086, 0.0, 0.0, 0.0], 'rewardMean': 0.683361996208577, 'totalEpisodes': 123, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 832.1059463426498
'totalSteps': 29440, 'rewardStep': 0.8107737399760817, 'errorList': [], 'lossList': [0.0, -1.3208649402856827, 0.0, 13.304462655186652, 0.0, 0.0, 0.0], 'rewardMean': 0.7347098811697447, 'totalEpisodes': 124, 'stepsPerEpisode': 1188, 'rewardPerEpisode': 976.3347455569882
'totalSteps': 30720, 'rewardStep': 0.7509660351775616, 'errorList': [], 'lossList': [0.0, -1.2877309691905976, 0.0, 0.9067195395380259, 0.0, 0.0, 0.0], 'rewardMean': 0.7369183863012558, 'totalEpisodes': 124, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1155.2588406892899
'totalSteps': 32000, 'rewardStep': 0.9473846535937368, 'errorList': [0.05941053213916813, 0.060974614666233606, 0.06071623113613285, 0.062137941405342234, 0.06012849894595274, 0.08082063371102018, 0.05910398586187557, 0.05874381273954755, 0.06121538792626715, 0.06013197115003361, 0.07820372627552484, 0.05975702568321555, 0.06352994973988353, 0.07302955639578503, 0.059234232457921816, 0.06307753583663736, 0.061571036557320845, 0.05862708843277431, 0.060125253498069, 0.06199902043966471, 0.10428221750025078, 0.06087255429348317, 0.058507062184094995, 0.06145347195630824, 0.058914164519287486, 0.058971140165861476, 0.083693062781405, 0.061260393278567804, 0.06050229351836361, 0.0598803745764694, 0.058848330006590656, 0.061358797235348296, 0.08824104930142813, 0.07998730833594371, 0.0594129020836898, 0.05950010342531071, 0.062306387566407946, 0.06015728714899887, 0.06290926334887952, 0.0625614347689006, 0.05869401305636851, 0.059756121073248465, 0.059454507668392324, 0.060555215163488524, 0.06039977605592505, 0.06035684845303162, 0.05991639226881449, 0.06097651641468576, 0.07558899767133469, 0.060695280070439284], 'lossList': [0.0, -1.2678047162294388, 0.0, 0.6771776435524225, 0.0, 0.0, 0.0], 'rewardMean': 0.7903634436839491, 'totalEpisodes': 124, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1132.0982218517847, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=32000, timeSpent=88.31
