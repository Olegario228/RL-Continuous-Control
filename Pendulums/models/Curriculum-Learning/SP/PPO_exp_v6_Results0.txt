#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9736934473677203, 'errorList': [], 'lossList': [0.0, -1.4319616508483888, 0.0, 91.73895628929138, 0.0, 0.0, 0.0], 'rewardMean': 0.9736934473677203, 'totalEpisodes': 6, 'stepsPerEpisode': 156, 'rewardPerEpisode': 137.1307948606565
'totalSteps': 2560, 'rewardStep': 0.8721541379205013, 'errorList': [], 'lossList': [0.0, -1.4360503625869752, 0.0, 26.528868389129638, 0.0, 0.0, 0.0], 'rewardMean': 0.9229237926441108, 'totalEpisodes': 10, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.649423887931107
'totalSteps': 3840, 'rewardStep': 0.48661737556853185, 'errorList': [], 'lossList': [0.0, -1.4290150624513627, 0.0, 25.895906875133516, 0.0, 0.0, 0.0], 'rewardMean': 0.7774883202855846, 'totalEpisodes': 15, 'stepsPerEpisode': 257, 'rewardPerEpisode': 182.52468144423213
'totalSteps': 5120, 'rewardStep': 0.7428431527982012, 'errorList': [], 'lossList': [0.0, -1.4321898812055587, 0.0, 26.097147505283356, 0.0, 0.0, 0.0], 'rewardMean': 0.7688270284137387, 'totalEpisodes': 17, 'stepsPerEpisode': 470, 'rewardPerEpisode': 328.8309203568927
'totalSteps': 6400, 'rewardStep': 0.9518324092631003, 'errorList': [], 'lossList': [0.0, -1.4280042052268982, 0.0, 21.0372944560647, 0.0, 0.0, 0.0], 'rewardMean': 0.8054281045836109, 'totalEpisodes': 17, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1035.639043373389
'totalSteps': 7680, 'rewardStep': 0.8064726295791328, 'errorList': [], 'lossList': [0.0, -1.3910250836610794, 0.0, 28.395940707325934, 0.0, 0.0, 0.0], 'rewardMean': 0.8056021920828647, 'totalEpisodes': 18, 'stepsPerEpisode': 418, 'rewardPerEpisode': 327.38897441298656
'totalSteps': 8960, 'rewardStep': 0.7524352293625116, 'errorList': [], 'lossList': [0.0, -1.3695242428779602, 0.0, 205.30644638061523, 0.0, 0.0, 0.0], 'rewardMean': 0.7980069116942428, 'totalEpisodes': 34, 'stepsPerEpisode': 97, 'rewardPerEpisode': 74.90777084983715
'totalSteps': 10240, 'rewardStep': 0.6385970733481553, 'errorList': [], 'lossList': [0.0, -1.367686161994934, 0.0, 212.95543605804443, 0.0, 0.0, 0.0], 'rewardMean': 0.7780806819009818, 'totalEpisodes': 61, 'stepsPerEpisode': 56, 'rewardPerEpisode': 45.83912862349282
'totalSteps': 11520, 'rewardStep': 0.8780047473073581, 'errorList': [], 'lossList': [0.0, -1.3594855511188506, 0.0, 77.31762321472168, 0.0, 0.0, 0.0], 'rewardMean': 0.7891833558350236, 'totalEpisodes': 89, 'stepsPerEpisode': 39, 'rewardPerEpisode': 32.94823430909572
'totalSteps': 12800, 'rewardStep': 0.6868020739359052, 'errorList': [], 'lossList': [0.0, -1.3500615578889847, 0.0, 42.775560455322264, 0.0, 0.0, 0.0], 'rewardMean': 0.7789452276451118, 'totalEpisodes': 112, 'stepsPerEpisode': 95, 'rewardPerEpisode': 81.30616919815695
'totalSteps': 14080, 'rewardStep': 0.5316487046452436, 'errorList': [], 'lossList': [0.0, -1.335846220254898, 0.0, 15.88595073223114, 0.0, 0.0, 0.0], 'rewardMean': 0.7347407533728643, 'totalEpisodes': 125, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.794591401985018
'totalSteps': 15360, 'rewardStep': 0.2746872941886978, 'errorList': [], 'lossList': [0.0, -1.3224533998966217, 0.0, 16.784817500114443, 0.0, 0.0, 0.0], 'rewardMean': 0.6749940689996838, 'totalEpisodes': 134, 'stepsPerEpisode': 163, 'rewardPerEpisode': 116.34502146890678
'totalSteps': 16640, 'rewardStep': 0.9270507412472099, 'errorList': [], 'lossList': [0.0, -1.3123737508058548, 0.0, 9.948344188928605, 0.0, 0.0, 0.0], 'rewardMean': 0.7190374055675516, 'totalEpisodes': 138, 'stepsPerEpisode': 157, 'rewardPerEpisode': 142.076169969912
'totalSteps': 17920, 'rewardStep': 0.9072601050619667, 'errorList': [], 'lossList': [0.0, -1.296952679157257, 0.0, 10.090640323162079, 0.0, 0.0, 0.0], 'rewardMean': 0.7354791007939281, 'totalEpisodes': 139, 'stepsPerEpisode': 78, 'rewardPerEpisode': 66.62066051754238
'totalSteps': 19200, 'rewardStep': 0.7340824673126777, 'errorList': [], 'lossList': [0.0, -1.2765438169240952, 0.0, 23.18457947254181, 0.0, 0.0, 0.0], 'rewardMean': 0.7137041065988858, 'totalEpisodes': 142, 'stepsPerEpisode': 176, 'rewardPerEpisode': 151.04067870738757
'totalSteps': 20480, 'rewardStep': 0.9559226891332182, 'errorList': [0.11992733200183142, 0.11111548906082928, 0.11145344962546543, 0.14099398936269852, 0.12172701063788649, 0.12406100415852642, 0.10816313189747814, 0.1406900832719487, 0.10362323800108089, 0.12821428189816844, 0.1399608771483348, 0.11828029684078348, 0.14082889384110872, 0.12042417013396593, 0.12851415756515816, 0.14274770881506468, 0.12551604948923867, 0.14233147363909737, 0.1094203744421866, 0.13511217606422515, 0.13760983038952668, 0.09967309736515126, 0.1326829355342643, 0.12017459971913523, 0.11180189971814516, 0.11888904236449598, 0.11738605102159054, 0.13465242788515885, 0.14353953358249413, 0.12911611242097604, 0.11458165705080348, 0.13077197107571292, 0.13637324660466651, 0.12482109461462446, 0.10893403117786174, 0.13074066795227296, 0.13146349111843333, 0.13650182118934912, 0.13989568392052465, 0.1218406044545526, 0.12548991336539073, 0.1387410373565312, 0.12216485778789632, 0.12873757211093811, 0.1336062699023604, 0.13143060198037845, 0.11214959887853572, 0.14386514623635216, 0.15133032306144426, 0.13508563855994477], 'lossList': [0.0, -1.2690829914808273, 0.0, 5.291350329518318, 0.0, 0.0, 0.0], 'rewardMean': 0.7286491125542943, 'totalEpisodes': 142, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1006.3285879561344, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=52.91
