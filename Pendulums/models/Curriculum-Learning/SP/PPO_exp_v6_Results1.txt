#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.834403614878477, 'errorList': [], 'lossList': [0.0, -1.4132035720348357, 0.0, 82.28911556720733, 0.0, 0.0, 0.0], 'rewardMean': 0.834403614878477, 'totalEpisodes': 6, 'stepsPerEpisode': 116, 'rewardPerEpisode': 100.87355602208763
'totalSteps': 2560, 'rewardStep': 0.6001710402654932, 'errorList': [], 'lossList': [0.0, -1.4054425275325775, 0.0, 31.04784006357193, 0.0, 0.0, 0.0], 'rewardMean': 0.7172873275719851, 'totalEpisodes': 14, 'stepsPerEpisode': 105, 'rewardPerEpisode': 80.48396825997047
'totalSteps': 3840, 'rewardStep': 0.6062171639386785, 'errorList': [], 'lossList': [0.0, -1.3888576155900956, 0.0, 18.80247653245926, 0.0, 0.0, 0.0], 'rewardMean': 0.6802639396942163, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 824.8065299425045
'totalSteps': 5120, 'rewardStep': 0.8087286824443356, 'errorList': [], 'lossList': [0.0, -1.3659832876920701, 0.0, 63.98055892944336, 0.0, 0.0, 0.0], 'rewardMean': 0.7123801253817461, 'totalEpisodes': 21, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.771757598552735
'totalSteps': 6400, 'rewardStep': 0.4123297492747196, 'errorList': [], 'lossList': [0.0, -1.375011357665062, 0.0, 31.16104217529297, 0.0, 0.0, 0.0], 'rewardMean': 0.6523700501603409, 'totalEpisodes': 23, 'stepsPerEpisode': 303, 'rewardPerEpisode': 212.71937314420694
'totalSteps': 7680, 'rewardStep': 0.8146920397120424, 'errorList': [], 'lossList': [0.0, -1.3692321890592576, 0.0, 16.373034401237966, 0.0, 0.0, 0.0], 'rewardMean': 0.6794237150856244, 'totalEpisodes': 23, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1017.7534846552737
'totalSteps': 8960, 'rewardStep': 0.8248349426306076, 'errorList': [], 'lossList': [0.0, -1.35570443212986, 0.0, 259.33635452270505, 0.0, 0.0, 0.0], 'rewardMean': 0.7001967475920505, 'totalEpisodes': 50, 'stepsPerEpisode': 67, 'rewardPerEpisode': 50.62311889174262
'totalSteps': 10240, 'rewardStep': 0.7867687494415506, 'errorList': [], 'lossList': [0.0, -1.3450244271755218, 0.0, 217.3705290222168, 0.0, 0.0, 0.0], 'rewardMean': 0.7110182478232381, 'totalEpisodes': 85, 'stepsPerEpisode': 22, 'rewardPerEpisode': 19.238600185816573
'totalSteps': 11520, 'rewardStep': 0.4632597734945213, 'errorList': [], 'lossList': [0.0, -1.3444002020359038, 0.0, 113.1984443283081, 0.0, 0.0, 0.0], 'rewardMean': 0.6834895284533807, 'totalEpisodes': 113, 'stepsPerEpisode': 82, 'rewardPerEpisode': 62.55573307937784
'totalSteps': 12800, 'rewardStep': 0.7144149119481668, 'errorList': [], 'lossList': [0.0, -1.339059671163559, 0.0, 73.22830862045288, 0.0, 0.0, 0.0], 'rewardMean': 0.6865820668028593, 'totalEpisodes': 143, 'stepsPerEpisode': 49, 'rewardPerEpisode': 38.83867881051421
'totalSteps': 14080, 'rewardStep': 0.9291816787172087, 'errorList': [], 'lossList': [0.0, -1.3255638164281844, 0.0, 51.92709589958191, 0.0, 0.0, 0.0], 'rewardMean': 0.6960598731867323, 'totalEpisodes': 161, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.164125921541565
'totalSteps': 15360, 'rewardStep': 0.8953530780938522, 'errorList': [], 'lossList': [0.0, -1.3199084562063217, 0.0, 22.06534402370453, 0.0, 0.0, 0.0], 'rewardMean': 0.7255780769695683, 'totalEpisodes': 169, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.6692528875134895
'totalSteps': 16640, 'rewardStep': 0.8379341548925132, 'errorList': [], 'lossList': [0.0, -1.3096608000993728, 0.0, 23.28794373035431, 0.0, 0.0, 0.0], 'rewardMean': 0.7487497760649517, 'totalEpisodes': 177, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.736889212452134
'totalSteps': 17920, 'rewardStep': 0.7825671239168299, 'errorList': [], 'lossList': [0.0, -1.296520652770996, 0.0, 22.76867374300957, 0.0, 0.0, 0.0], 'rewardMean': 0.7461336202122012, 'totalEpisodes': 181, 'stepsPerEpisode': 127, 'rewardPerEpisode': 103.40307084328832
'totalSteps': 19200, 'rewardStep': 0.7618587380284839, 'errorList': [], 'lossList': [0.0, -1.2959812277555465, 0.0, 9.595930701494217, 0.0, 0.0, 0.0], 'rewardMean': 0.7810865190875776, 'totalEpisodes': 182, 'stepsPerEpisode': 1030, 'rewardPerEpisode': 849.6843899086108
'totalSteps': 20480, 'rewardStep': 0.7533374899064675, 'errorList': [], 'lossList': [0.0, -1.2921068328619003, 0.0, 5.419533504247665, 0.0, 0.0, 0.0], 'rewardMean': 0.7749510641070201, 'totalEpisodes': 186, 'stepsPerEpisode': 79, 'rewardPerEpisode': 68.39272801824417
'totalSteps': 21760, 'rewardStep': 0.7303584329197936, 'errorList': [], 'lossList': [0.0, -1.277467769384384, 0.0, 5.822030390799045, 0.0, 0.0, 0.0], 'rewardMean': 0.7655034131359388, 'totalEpisodes': 186, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1111.3837195497542
'totalSteps': 23040, 'rewardStep': 0.8573173842637632, 'errorList': [], 'lossList': [0.0, -1.2502984756231308, 0.0, 4.703607103824615, 0.0, 0.0, 0.0], 'rewardMean': 0.77255827661816, 'totalEpisodes': 188, 'stepsPerEpisode': 407, 'rewardPerEpisode': 369.77441556196345
'totalSteps': 24320, 'rewardStep': 0.7002480039792103, 'errorList': [], 'lossList': [0.0, -1.2454244005680084, 0.0, 4.079332257509232, 0.0, 0.0, 0.0], 'rewardMean': 0.7962570996666289, 'totalEpisodes': 189, 'stepsPerEpisode': 743, 'rewardPerEpisode': 613.3302125678688
'totalSteps': 25600, 'rewardStep': 0.5955513257485242, 'errorList': [], 'lossList': [0.0, -1.2183937579393387, 0.0, 1.7035678565502166, 0.0, 0.0, 0.0], 'rewardMean': 0.7843707410466647, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 904.0894969449206
'totalSteps': 26880, 'rewardStep': 0.9078833386563745, 'errorList': [], 'lossList': [0.0, -1.1814081501960754, 0.0, 1.398594712242484, 0.0, 0.0, 0.0], 'rewardMean': 0.7822409070405814, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1015.1109034329249
'totalSteps': 28160, 'rewardStep': 0.8548482567144905, 'errorList': [], 'lossList': [0.0, -1.1493841511011125, 0.0, 0.9615070862323045, 0.0, 0.0, 0.0], 'rewardMean': 0.778190424902645, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1135.6998560346747
'totalSteps': 29440, 'rewardStep': 0.9042150682100213, 'errorList': [], 'lossList': [0.0, -1.1065249592065811, 0.0, 0.6685347301140427, 0.0, 0.0, 0.0], 'rewardMean': 0.7848185162343959, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1177.44339461507
'totalSteps': 30720, 'rewardStep': 0.969692846797664, 'errorList': [0.11105350309897559, 0.09441670469197816, 0.07731999438258959, 0.07082412834599716, 0.08173261319186258, 0.06475195147947571, 0.06359246983447647, 0.08420579837503608, 0.053731299455877596, 0.08683293209850895, 0.07754649912974204, 0.0832125564644407, 0.05920146782775926, 0.09846010483298293, 0.07612416106872019, 0.08823913266056174, 0.0829037001560298, 0.09920953610076508, 0.1073178167154194, 0.08376402408228244, 0.07217637948892004, 0.09688571041813733, 0.09760069086183908, 0.06901676143644363, 0.06238163476818905, 0.08533341651584582, 0.09186415087325445, 0.06003944638284698, 0.07810445045945753, 0.054640161713721436, 0.06474043246503963, 0.07585682185618882, 0.06534328644471775, 0.04985955444183562, 0.06691450911397218, 0.07851579962224382, 0.11402019138543047, 0.07934027386912007, 0.08149666595168832, 0.07455026582195912, 0.07060157207635955, 0.05620819050747166, 0.05590126435490352, 0.06939619115210041, 0.06979303660848234, 0.062366982309841154, 0.05300551499204116, 0.10620736959439779, 0.05409941072392807, 0.05282588686235922], 'lossList': [0.0, -1.0578971087932587, 0.0, 0.5504591331072152, 0.0, 0.0, 0.0], 'rewardMean': 0.8035310885224792, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1207.2756363286458, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=30720, timeSpent=67.36
