#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8280920161904477, 'errorList': [], 'lossList': [0.0, -1.41705382168293, 0.0, 82.44616906642914, 0.0, 0.0, 0.0], 'rewardMean': 0.8280920161904477, 'totalEpisodes': 5, 'stepsPerEpisode': 241, 'rewardPerEpisode': 205.5822162166961
'totalSteps': 2560, 'rewardStep': 0.7507861750265638, 'errorList': [], 'lossList': [0.0, -1.4237725883722305, 0.0, 27.646035459041595, 0.0, 0.0, 0.0], 'rewardMean': 0.7894390956085058, 'totalEpisodes': 13, 'stepsPerEpisode': 222, 'rewardPerEpisode': 166.45583849598495
'totalSteps': 3840, 'rewardStep': 0.9266248265853667, 'errorList': [], 'lossList': [0.0, -1.423655862212181, 0.0, 28.71441924095154, 0.0, 0.0, 0.0], 'rewardMean': 0.8351676726007927, 'totalEpisodes': 19, 'stepsPerEpisode': 76, 'rewardPerEpisode': 57.16870293190257
'totalSteps': 5120, 'rewardStep': 0.4504547479625854, 'errorList': [], 'lossList': [0.0, -1.4033960556983949, 0.0, 24.47244997024536, 0.0, 0.0, 0.0], 'rewardMean': 0.7389894414412409, 'totalEpisodes': 25, 'stepsPerEpisode': 66, 'rewardPerEpisode': 41.39851712386193
'totalSteps': 6400, 'rewardStep': 0.12558401672692654, 'errorList': [], 'lossList': [0.0, -1.3827847450971604, 0.0, 29.96060767173767, 0.0, 0.0, 0.0], 'rewardMean': 0.616308356498378, 'totalEpisodes': 27, 'stepsPerEpisode': 559, 'rewardPerEpisode': 374.94155787187236
'totalSteps': 7680, 'rewardStep': 0.7260749997549203, 'errorList': [], 'lossList': [0.0, -1.3684038084745407, 0.0, 70.00208757400513, 0.0, 0.0, 0.0], 'rewardMean': 0.6346027970411351, 'totalEpisodes': 34, 'stepsPerEpisode': 55, 'rewardPerEpisode': 45.11554206957837
'totalSteps': 8960, 'rewardStep': 0.46137620603026763, 'errorList': [], 'lossList': [0.0, -1.3642477303743363, 0.0, 150.17664085388185, 0.0, 0.0, 0.0], 'rewardMean': 0.6098561411824397, 'totalEpisodes': 50, 'stepsPerEpisode': 17, 'rewardPerEpisode': 12.065524609525301
'totalSteps': 10240, 'rewardStep': 0.5226966436308691, 'errorList': [], 'lossList': [0.0, -1.357355723977089, 0.0, 182.7684920501709, 0.0, 0.0, 0.0], 'rewardMean': 0.5989612039884934, 'totalEpisodes': 84, 'stepsPerEpisode': 88, 'rewardPerEpisode': 65.14651260766368
'totalSteps': 11520, 'rewardStep': 0.567046092699947, 'errorList': [], 'lossList': [0.0, -1.3604458224773408, 0.0, 52.53894093513489, 0.0, 0.0, 0.0], 'rewardMean': 0.5954150805119882, 'totalEpisodes': 106, 'stepsPerEpisode': 107, 'rewardPerEpisode': 82.04030518895942
'totalSteps': 12800, 'rewardStep': 0.44411957522294654, 'errorList': [], 'lossList': [0.0, -1.3543880826234818, 0.0, 50.98517385482788, 0.0, 0.0, 0.0], 'rewardMean': 0.580285529983084, 'totalEpisodes': 126, 'stepsPerEpisode': 143, 'rewardPerEpisode': 105.85079037290215
'totalSteps': 14080, 'rewardStep': 0.818344441964674, 'errorList': [], 'lossList': [0.0, -1.3548848247528076, 0.0, 17.613299210071563, 0.0, 0.0, 0.0], 'rewardMean': 0.5793107725605068, 'totalEpisodes': 136, 'stepsPerEpisode': 105, 'rewardPerEpisode': 88.159444687002
'totalSteps': 15360, 'rewardStep': 0.8678632372314352, 'errorList': [], 'lossList': [0.0, -1.3519592380523682, 0.0, 21.06760882139206, 0.0, 0.0, 0.0], 'rewardMean': 0.5910184787809938, 'totalEpisodes': 140, 'stepsPerEpisode': 91, 'rewardPerEpisode': 73.96752022090119
'totalSteps': 16640, 'rewardStep': 0.8979031415460186, 'errorList': [], 'lossList': [0.0, -1.34513077378273, 0.0, 8.392090364098548, 0.0, 0.0, 0.0], 'rewardMean': 0.588146310277059, 'totalEpisodes': 144, 'stepsPerEpisode': 58, 'rewardPerEpisode': 49.78221595801517
'totalSteps': 17920, 'rewardStep': 0.8711121298635859, 'errorList': [], 'lossList': [0.0, -1.3310763317346572, 0.0, 9.529220728874206, 0.0, 0.0, 0.0], 'rewardMean': 0.6302120484671592, 'totalEpisodes': 147, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.311438682705944
'totalSteps': 19200, 'rewardStep': 0.9122882274051942, 'errorList': [], 'lossList': [0.0, -1.3229522919654846, 0.0, 10.650285880565644, 0.0, 0.0, 0.0], 'rewardMean': 0.7088824695349858, 'totalEpisodes': 150, 'stepsPerEpisode': 32, 'rewardPerEpisode': 29.352054589079934
'totalSteps': 20480, 'rewardStep': 0.8653358823434, 'errorList': [], 'lossList': [0.0, -1.300855268239975, 0.0, 5.937195496559143, 0.0, 0.0, 0.0], 'rewardMean': 0.7228085577938338, 'totalEpisodes': 151, 'stepsPerEpisode': 239, 'rewardPerEpisode': 215.78360782165134
'totalSteps': 21760, 'rewardStep': 0.7910075381928858, 'errorList': [], 'lossList': [0.0, -1.2802658951282502, 0.0, 6.950586395263672, 0.0, 0.0, 0.0], 'rewardMean': 0.7557716910100958, 'totalEpisodes': 152, 'stepsPerEpisode': 188, 'rewardPerEpisode': 148.20220168616154
'totalSteps': 23040, 'rewardStep': 0.5100426671170231, 'errorList': [], 'lossList': [0.0, -1.257117749452591, 0.0, 2.298889145851135, 0.0, 0.0, 0.0], 'rewardMean': 0.7545062933587111, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 902.6713624462529
'totalSteps': 24320, 'rewardStep': 0.8230962982453699, 'errorList': [], 'lossList': [0.0, -1.2186011397838592, 0.0, 1.583158078789711, 0.0, 0.0, 0.0], 'rewardMean': 0.7801113139132535, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1016.0640894566955
'totalSteps': 25600, 'rewardStep': 0.8100483021024772, 'errorList': [], 'lossList': [0.0, -1.1949939620494843, 0.0, 1.1109918539226056, 0.0, 0.0, 0.0], 'rewardMean': 0.8167041866012065, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.378648353023
'totalSteps': 26880, 'rewardStep': 0.9540187125043983, 'errorList': [0.046574582989477795, 0.06490994695578961, 0.04663819981800864, 0.04785691477728776, 0.04877830849112884, 0.04903925821097264, 0.04935220702203833, 0.07356508621751623, 0.0496009215380939, 0.05178360564558452, 0.04937848843189294, 0.04958715270321612, 0.04834853530037813, 0.04824110443549757, 0.04658100225755916, 0.048434425242899065, 0.053410836442591286, 0.048218488569386646, 0.056596096328352975, 0.049760399633217274, 0.06257348565446667, 0.0638464505450047, 0.04667527912376588, 0.05012009570104281, 0.0490166630445296, 0.04858064188788551, 0.049428222275766695, 0.05651741398302732, 0.047565000537982315, 0.04817001527686813, 0.06940469991708641, 0.04666020513544648, 0.04849573984097589, 0.047751871107428796, 0.06716007704501355, 0.048257707067864086, 0.06261378299781713, 0.04765953847818897, 0.04807279310753965, 0.04943302549931216, 0.0472121657598932, 0.049179967981585845, 0.05068686320070982, 0.04889127369524038, 0.0503596660055955, 0.049051135703841425, 0.04868537556679897, 0.05003661632436464, 0.07763152366565824, 0.04876452528166839], 'lossList': [0.0, -1.1566895991563797, 0.0, 0.9331422389671207, 0.0, 0.0, 0.0], 'rewardMean': 0.8302716136551789, 'totalEpisodes': 152, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.822322710093, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=61.98
