#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.720468672746902, 'errorList': [], 'lossList': [0.0, -1.4278227895498277, 0.0, 82.66252764225005, 0.0, 0.0, 0.0], 'rewardMean': 0.720468672746902, 'totalEpisodes': 6, 'stepsPerEpisode': 204, 'rewardPerEpisode': 144.31493761881387
'totalSteps': 2560, 'rewardStep': 0.708283617518427, 'errorList': [], 'lossList': [0.0, -1.429812199473381, 0.0, 29.883386815786363, 0.0, 0.0, 0.0], 'rewardMean': 0.7143761451326645, 'totalEpisodes': 10, 'stepsPerEpisode': 27, 'rewardPerEpisode': 23.004590202275654
'totalSteps': 3840, 'rewardStep': 0.8526906831179181, 'errorList': [], 'lossList': [0.0, -1.4185941159725188, 0.0, 29.65625648856163, 0.0, 0.0, 0.0], 'rewardMean': 0.760480991127749, 'totalEpisodes': 12, 'stepsPerEpisode': 527, 'rewardPerEpisode': 383.6203207558943
'totalSteps': 5120, 'rewardStep': 0.7864931815637127, 'errorList': [], 'lossList': [0.0, -1.4185672533512115, 0.0, 33.56672460079193, 0.0, 0.0, 0.0], 'rewardMean': 0.7669840387367399, 'totalEpisodes': 13, 'stepsPerEpisode': 181, 'rewardPerEpisode': 153.52840304020916
'totalSteps': 6400, 'rewardStep': 0.5575090713668804, 'errorList': [], 'lossList': [0.0, -1.4168326091766357, 0.0, 15.090018570423126, 0.0, 0.0, 0.0], 'rewardMean': 0.7250890452627681, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 949.9290076101062
'totalSteps': 7680, 'rewardStep': 0.8772546673361907, 'errorList': [], 'lossList': [0.0, -1.3818873542547225, 0.0, 13.272741163372993, 0.0, 0.0, 0.0], 'rewardMean': 0.7504499822750051, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 992.5894268131445
'totalSteps': 8960, 'rewardStep': 0.8781694724803297, 'errorList': [], 'lossList': [0.0, -1.3695027178525925, 0.0, 263.72500076293943, 0.0, 0.0, 0.0], 'rewardMean': 0.7686956237329087, 'totalEpisodes': 36, 'stepsPerEpisode': 38, 'rewardPerEpisode': 32.98265671709747
'totalSteps': 10240, 'rewardStep': 0.5936897197669351, 'errorList': [], 'lossList': [0.0, -1.369223559498787, 0.0, 207.68104034423828, 0.0, 0.0, 0.0], 'rewardMean': 0.746819885737162, 'totalEpisodes': 61, 'stepsPerEpisode': 10, 'rewardPerEpisode': 6.624315394696389
'totalSteps': 11520, 'rewardStep': 0.7721532049819518, 'errorList': [], 'lossList': [0.0, -1.3695249825716018, 0.0, 101.72377002716064, 0.0, 0.0, 0.0], 'rewardMean': 0.7496346989865831, 'totalEpisodes': 86, 'stepsPerEpisode': 55, 'rewardPerEpisode': 44.49329953404668
'totalSteps': 12800, 'rewardStep': 0.6762082688446388, 'errorList': [], 'lossList': [0.0, -1.3709219306707383, 0.0, 53.30207329750061, 0.0, 0.0, 0.0], 'rewardMean': 0.7422920559723887, 'totalEpisodes': 101, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.8424755291708035
'totalSteps': 14080, 'rewardStep': 0.9152933462358714, 'errorList': [], 'lossList': [0.0, -1.3704944223165512, 0.0, 61.643119201660156, 0.0, 0.0, 0.0], 'rewardMean': 0.7617745233212856, 'totalEpisodes': 112, 'stepsPerEpisode': 51, 'rewardPerEpisode': 41.25854047662318
'totalSteps': 15360, 'rewardStep': 0.6154218981105702, 'errorList': [], 'lossList': [0.0, -1.3686416631937026, 0.0, 38.46875702381134, 0.0, 0.0, 0.0], 'rewardMean': 0.7524883513804999, 'totalEpisodes': 122, 'stepsPerEpisode': 82, 'rewardPerEpisode': 70.6137723037245
'totalSteps': 16640, 'rewardStep': 0.9195026627192309, 'errorList': [], 'lossList': [0.0, -1.3799454122781754, 0.0, 9.977227016687394, 0.0, 0.0, 0.0], 'rewardMean': 0.7591695493406312, 'totalEpisodes': 127, 'stepsPerEpisode': 149, 'rewardPerEpisode': 123.35477207353499
'totalSteps': 17920, 'rewardStep': 0.7241527971218102, 'errorList': [], 'lossList': [0.0, -1.3844389879703523, 0.0, 6.429461728334427, 0.0, 0.0, 0.0], 'rewardMean': 0.752935510896441, 'totalEpisodes': 132, 'stepsPerEpisode': 87, 'rewardPerEpisode': 71.14061850859277
'totalSteps': 19200, 'rewardStep': 0.7317487245731301, 'errorList': [], 'lossList': [0.0, -1.362976514697075, 0.0, 5.740210449695587, 0.0, 0.0, 0.0], 'rewardMean': 0.770359476217066, 'totalEpisodes': 136, 'stepsPerEpisode': 282, 'rewardPerEpisode': 227.36936538810278
'totalSteps': 20480, 'rewardStep': 0.9400486616323505, 'errorList': [0.7407601984908866, 0.5493155984393273, 6.017049003477881, 5.795355333083082, 1.1807683613520639, 3.264586025476417, 5.188662719206738, 0.10663224463950428, 2.516515157603432, 3.1590821309226, 5.123889795410945, 2.5883243824857307, 1.7142362192939828, 8.007772241114484, 3.2929200026812273, 6.681569288232637, 3.18124824184502, 1.6512752117485325, 3.6452628635174547, 2.5237826488376247, 2.1500401573473646, 2.1613710304960767, 0.24157735292963659, 5.357590818868727, 1.9171269912872142, 6.949225094229119, 8.435548671683256, 0.6070531933323239, 6.2023862667102, 1.100825298032631, 5.08399950582803, 6.262353000414961, 4.303578262836958, 6.863376929610357, 0.5210994691779538, 1.4325749991284185, 0.839938380613889, 1.8011949624508812, 7.6009391453890895, 0.14657485753472774, 1.694223397984104, 4.413934146582723, 2.8937863475715018, 0.3959308831784555, 4.1100719347030825, 5.144842892079331, 7.6856692015423524, 0.6382526980363744, 4.081164043394851, 1.462606466686471], 'lossList': [0.0, -1.35094466984272, 0.0, 4.195228809118271, 0.0, 0.0, 0.0], 'rewardMean': 0.7766388756466818, 'totalEpisodes': 141, 'stepsPerEpisode': 102, 'rewardPerEpisode': 84.49260588542218, 'successfulTests': 2
'totalSteps': 21760, 'rewardStep': 0.8396639880551122, 'errorList': [], 'lossList': [0.0, -1.3415083813667297, 0.0, 14.88102083325386, 0.0, 0.0, 0.0], 'rewardMean': 0.7727883272041602, 'totalEpisodes': 144, 'stepsPerEpisode': 290, 'rewardPerEpisode': 259.1310227214282
'totalSteps': 23040, 'rewardStep': 0.8922189355619254, 'errorList': [], 'lossList': [0.0, -1.349777069091797, 0.0, 6.173995661139489, 0.0, 0.0, 0.0], 'rewardMean': 0.8026412487836592, 'totalEpisodes': 148, 'stepsPerEpisode': 38, 'rewardPerEpisode': 31.219785648109955
'totalSteps': 24320, 'rewardStep': 0.1851750204109509, 'errorList': [], 'lossList': [0.0, -1.366731463074684, 0.0, 34.338223860263824, 0.0, 0.0, 0.0], 'rewardMean': 0.7439434303265591, 'totalEpisodes': 151, 'stepsPerEpisode': 458, 'rewardPerEpisode': 291.0395946399727
'totalSteps': 25600, 'rewardStep': 0.8085300372742269, 'errorList': [], 'lossList': [0.0, -1.3781305849552155, 0.0, 6.194990721344948, 0.0, 0.0, 0.0], 'rewardMean': 0.7571756071695178, 'totalEpisodes': 153, 'stepsPerEpisode': 207, 'rewardPerEpisode': 161.04750856750994
'totalSteps': 26880, 'rewardStep': 0.8688466190426979, 'errorList': [], 'lossList': [0.0, -1.3756684005260467, 0.0, 1.1911424134671689, 0.0, 0.0, 0.0], 'rewardMean': 0.7525309344502005, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1041.2695637408492
'totalSteps': 28160, 'rewardStep': 0.9189765569799058, 'errorList': [], 'lossList': [0.0, -1.3319564020633698, 0.0, 0.8887596642971038, 0.0, 0.0, 0.0], 'rewardMean': 0.7828864003371341, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1049.3027186460988
'totalSteps': 29440, 'rewardStep': 0.86958625635451, 'errorList': [], 'lossList': [0.0, -1.2876337552070618, 0.0, 0.9196069436892867, 0.0, 0.0, 0.0], 'rewardMean': 0.777894759700662, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.8050749044867
'totalSteps': 30720, 'rewardStep': 0.8489487577970851, 'errorList': [], 'lossList': [0.0, -1.2544142186641694, 0.0, 0.8624439699947835, 0.0, 0.0, 0.0], 'rewardMean': 0.7903743557681894, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1169.052833044514
'totalSteps': 32000, 'rewardStep': 0.9274070089303335, 'errorList': [], 'lossList': [0.0, -1.2157177597284317, 0.0, 0.5796250098571182, 0.0, 0.0, 0.0], 'rewardMean': 0.8099401842039097, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.0272689853507
'totalSteps': 33280, 'rewardStep': 0.9505278260894106, 'errorList': [0.06376748688855574, 0.04726353064610753, 0.07161523919943888, 0.04562118616034731, 0.10367725611263662, 0.05788671804137821, 0.049873784943347335, 0.04998721194454382, 0.06300712248166387, 0.0444520720628443, 0.05606783044189164, 0.06387406766788221, 0.044757260214571624, 0.06006761377796841, 0.04555219432916936, 0.0651926947417656, 0.052666326944600224, 0.04305955160048125, 0.06927372663441296, 0.04822877182136477, 0.08298767555193856, 0.05276993905891723, 0.044778159265279276, 0.049699416071660535, 0.0803213128469888, 0.041632935049762364, 0.0668001639033772, 0.06450677421643511, 0.04750595395262165, 0.07614396949556523, 0.047735469751291164, 0.04239596236777063, 0.05174227148431424, 0.0712489763324369, 0.06776114811040261, 0.06697554253025662, 0.08381696435171795, 0.05696868073401184, 0.06461652783647714, 0.05349520142069716, 0.06255116907397991, 0.06983678223164766, 0.0533558974720412, 0.047311890462357474, 0.05800508847101308, 0.06961629917147717, 0.044848212667711204, 0.05082166684660931, 0.0437982031450207, 0.07096320513338611], 'lossList': [0.0, -1.1552769833803176, 0.0, 0.4614857016503811, 0.0, 0.0, 0.0], 'rewardMean': 0.8109881006496158, 'totalEpisodes': 153, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1204.1470715373193, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=33280, timeSpent=84.24
