#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v6_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5371426615508459, 'errorList': [], 'lossList': [0.0, -1.4251431292295456, 0.0, 84.48239258766175, 0.0, 0.0, 0.0], 'rewardMean': 0.5371426615508459, 'totalEpisodes': 6, 'stepsPerEpisode': 168, 'rewardPerEpisode': 119.63663339514541
'totalSteps': 2560, 'rewardStep': 0.9049820117839624, 'errorList': [], 'lossList': [0.0, -1.4166743564605713, 0.0, 27.88579257965088, 0.0, 0.0, 0.0], 'rewardMean': 0.7210623366674042, 'totalEpisodes': 12, 'stepsPerEpisode': 23, 'rewardPerEpisode': 19.679174122436137
'totalSteps': 3840, 'rewardStep': 0.5511653522941389, 'errorList': [], 'lossList': [0.0, -1.411307326555252, 0.0, 27.674237489700317, 0.0, 0.0, 0.0], 'rewardMean': 0.6644300085429824, 'totalEpisodes': 16, 'stepsPerEpisode': 151, 'rewardPerEpisode': 104.75860243948603
'totalSteps': 5120, 'rewardStep': 0.5848888327902355, 'errorList': [], 'lossList': [0.0, -1.4146560937166215, 0.0, 15.313681038618087, 0.0, 0.0, 0.0], 'rewardMean': 0.6445447146047957, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 869.8106292874901
'totalSteps': 6400, 'rewardStep': 0.7508371081150662, 'errorList': [], 'lossList': [0.0, -1.4007644772529602, 0.0, 34.70546609640122, 0.0, 0.0, 0.0], 'rewardMean': 0.6658031933068498, 'totalEpisodes': 18, 'stepsPerEpisode': 866, 'rewardPerEpisode': 693.8694293562221
'totalSteps': 7680, 'rewardStep': 0.8187678227013964, 'errorList': [], 'lossList': [0.0, -1.3894254767894745, 0.0, 24.725468633174895, 0.0, 0.0, 0.0], 'rewardMean': 0.6912972982059409, 'totalEpisodes': 19, 'stepsPerEpisode': 428, 'rewardPerEpisode': 322.5005662430342
'totalSteps': 8960, 'rewardStep': 0.8282139573339213, 'errorList': [], 'lossList': [0.0, -1.3615942257642746, 0.0, 219.23658897399903, 0.0, 0.0, 0.0], 'rewardMean': 0.7108568209385095, 'totalEpisodes': 38, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.482046780022343
'totalSteps': 10240, 'rewardStep': 0.9035763072592213, 'errorList': [], 'lossList': [0.0, -1.358720326423645, 0.0, 149.6588492012024, 0.0, 0.0, 0.0], 'rewardMean': 0.7349467567285985, 'totalEpisodes': 63, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.892963827669965
'totalSteps': 11520, 'rewardStep': 0.7815878451703994, 'errorList': [], 'lossList': [0.0, -1.359167588353157, 0.0, 69.13557374954223, 0.0, 0.0, 0.0], 'rewardMean': 0.7401290998887986, 'totalEpisodes': 87, 'stepsPerEpisode': 93, 'rewardPerEpisode': 78.5221323119522
'totalSteps': 12800, 'rewardStep': 0.7105460335355167, 'errorList': [], 'lossList': [0.0, -1.3587789940834045, 0.0, 39.7423925113678, 0.0, 0.0, 0.0], 'rewardMean': 0.7371707932534705, 'totalEpisodes': 107, 'stepsPerEpisode': 7, 'rewardPerEpisode': 4.859242246034064
'totalSteps': 14080, 'rewardStep': 0.868437498915272, 'errorList': [], 'lossList': [0.0, -1.353145082592964, 0.0, 16.187752170562746, 0.0, 0.0, 0.0], 'rewardMean': 0.770300276989913, 'totalEpisodes': 116, 'stepsPerEpisode': 45, 'rewardPerEpisode': 40.19486655870162
'totalSteps': 15360, 'rewardStep': 0.9411954384389807, 'errorList': [107.20622817200683, 153.09891063888105, 85.43227118753938, 19.453220868195753, 10.63454835229913, 66.18901337321518, 12.256751595850433, 94.49040835872897, 75.91142350682341, 28.11585191044179, 120.961104063898, 36.709463812079, 79.20922200889734, 128.40521864045795, 109.58997278939717, 113.1969950483231, 139.98949262067197, 105.56180916133324, 123.81989431206397, 52.23073696693214, 41.856036336252465, 102.69885998182576, 29.189258247062877, 115.23279041561065, 54.65360864563935, 70.24404308417786, 79.97577472770209, 83.31590994188461, 39.53729955148968, 118.27394045686418, 34.70326899434047, 147.5456187230988, 25.378336699444414, 109.53696898744539, 52.38250727616094, 87.1134275260303, 105.62254561348209, 85.08499918780917, 97.28387116834342, 82.86891485222027, 8.358424759648232, 64.6223429947685, 3.3845548921641053, 111.6584045168568, 154.79136781185233, 88.52926317282534, 151.06774319051857, 66.62303926221038, 71.35104254243595, 101.61603769555643], 'lossList': [0.0, -1.3413125967979431, 0.0, 32.32674144268036, 0.0, 0.0, 0.0], 'rewardMean': 0.7739216196554148, 'totalEpisodes': 125, 'stepsPerEpisode': 34, 'rewardPerEpisode': 30.216441470463824, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7917780744573916, 'errorList': [], 'lossList': [0.0, -1.3297154599428176, 0.0, 19.344341821670533, 0.0, 0.0, 0.0], 'rewardMean': 0.7979828918717401, 'totalEpisodes': 133, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.4865635090644393
'totalSteps': 17920, 'rewardStep': 0.765760704719969, 'errorList': [], 'lossList': [0.0, -1.3127651119232178, 0.0, 11.039081131219865, 0.0, 0.0, 0.0], 'rewardMean': 0.8160700790647134, 'totalEpisodes': 139, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.11113698876157
'totalSteps': 19200, 'rewardStep': 0.8246207980399635, 'errorList': [], 'lossList': [0.0, -1.2888042277097702, 0.0, 7.533872516155243, 0.0, 0.0, 0.0], 'rewardMean': 0.8234484480572032, 'totalEpisodes': 142, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.99980341576828
'totalSteps': 20480, 'rewardStep': 0.8099415036843395, 'errorList': [], 'lossList': [0.0, -1.2579506713151931, 0.0, 7.907558373212814, 0.0, 0.0, 0.0], 'rewardMean': 0.8225658161554975, 'totalEpisodes': 144, 'stepsPerEpisode': 125, 'rewardPerEpisode': 88.94541810712118
'totalSteps': 21760, 'rewardStep': 0.6263253752109674, 'errorList': [], 'lossList': [0.0, -1.2355670791864395, 0.0, 4.64227433681488, 0.0, 0.0, 0.0], 'rewardMean': 0.8023769579432022, 'totalEpisodes': 146, 'stepsPerEpisode': 480, 'rewardPerEpisode': 404.07741220499224
'totalSteps': 23040, 'rewardStep': 0.5177970517931345, 'errorList': [], 'lossList': [0.0, -1.237887242436409, 0.0, 3.0144181883335115, 0.0, 0.0, 0.0], 'rewardMean': 0.7637990323965933, 'totalEpisodes': 147, 'stepsPerEpisode': 1123, 'rewardPerEpisode': 877.2035496523125
'totalSteps': 24320, 'rewardStep': 0.8021806053903788, 'errorList': [], 'lossList': [0.0, -1.2448873299360275, 0.0, 2.5686910246312618, 0.0, 0.0, 0.0], 'rewardMean': 0.7658583084185915, 'totalEpisodes': 148, 'stepsPerEpisode': 1158, 'rewardPerEpisode': 1029.6864145859583
'totalSteps': 25600, 'rewardStep': 0.9660762715084461, 'errorList': [0.010575548448113424, 0.010917536641466259, 0.010651802935432648, 0.014042701566348645, 0.012251149665884871, 0.01083600586761333, 0.010901910143338838, 0.010305020897382864, 0.011136222736133815, 0.01382929674944834, 0.01255983260380786, 0.010088997923840258, 0.009669720211519432, 0.012975860179550535, 0.009574417737842716, 0.01310852970299996, 0.009551628299788607, 0.01058062816232106, 0.013565563255711075, 0.015510650172860223, 0.018790469826749936, 0.015942893706883877, 0.014589825691084664, 0.00969072143772586, 0.012919751679485196, 0.011633973066827796, 0.01650439226625236, 0.013412891591325448, 0.013350042360437865, 0.014662138630009602, 0.01172907772329388, 0.015676791902020232, 0.014856666270476769, 0.015414691350525258, 0.01768163064172829, 0.014494285291210152, 0.012899847045582257, 0.01418089769399317, 0.009769666861379865, 0.010331750556289189, 0.013678991201453957, 0.016282079573012552, 0.011492666645472927, 0.014467902125187363, 0.017374486771214344, 0.018180086900788442, 0.009574094763258541, 0.009611171878922567, 0.017742624798385762, 0.016266991791145144], 'lossList': [0.0, -1.2293145942687989, 0.0, 1.4034528666734696, 0.0, 0.0, 0.0], 'rewardMean': 0.7914113322158842, 'totalEpisodes': 148, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1145.3046774207294, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=75.42
