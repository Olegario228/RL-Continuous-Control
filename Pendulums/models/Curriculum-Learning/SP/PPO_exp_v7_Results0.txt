#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000, 10000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9736934473677203, 'errorList': [], 'lossList': [0.0, -1.4319616508483888, 0.0, 91.73895628929138, 0.0, 0.0, 0.0], 'rewardMean': 0.9736934473677203, 'totalEpisodes': 6, 'stepsPerEpisode': 156, 'rewardPerEpisode': 137.1307948606565
'totalSteps': 2560, 'rewardStep': 0.8578428374349295, 'errorList': [], 'lossList': [0.0, -1.441227690577507, 0.0, 25.426279010772706, 0.0, 0.0, 0.0], 'rewardMean': 0.9157681424013249, 'totalEpisodes': 10, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.565970910885248
'totalSteps': 3840, 'rewardStep': 0.46653799930658396, 'errorList': [], 'lossList': [0.0, -1.4372283554077148, 0.0, 27.61333806991577, 0.0, 0.0, 0.0], 'rewardMean': 0.7660247613697445, 'totalEpisodes': 16, 'stepsPerEpisode': 152, 'rewardPerEpisode': 117.51127870338067
'totalSteps': 5120, 'rewardStep': 0.7288181889210381, 'errorList': [], 'lossList': [0.0, -1.4304623091220856, 0.0, 19.317062760591508, 0.0, 0.0, 0.0], 'rewardMean': 0.756723118257568, 'totalEpisodes': 17, 'stepsPerEpisode': 1269, 'rewardPerEpisode': 897.9366791839252
'totalSteps': 6400, 'rewardStep': 0.9080190405169092, 'errorList': [], 'lossList': [0.0, -1.42556543469429, 0.0, 18.12821074694395, 0.0, 0.0, 0.0], 'rewardMean': 0.7869823027094363, 'totalEpisodes': 17, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1009.6135768647557
'totalSteps': 7680, 'rewardStep': 0.9789083125623473, 'errorList': [], 'lossList': [0.0, -1.4052419179677964, 0.0, 193.62662441253661, 0.0, 0.0, 0.0], 'rewardMean': 0.8189699710182547, 'totalEpisodes': 40, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.393027506776441
'totalSteps': 8960, 'rewardStep': 0.8708213940400443, 'errorList': [], 'lossList': [0.0, -1.4012517708539962, 0.0, 221.5584203338623, 0.0, 0.0, 0.0], 'rewardMean': 0.8263773171642247, 'totalEpisodes': 91, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.446224201030866
'totalSteps': 10240, 'rewardStep': 0.5262437147597624, 'errorList': [], 'lossList': [0.0, -1.4013191652297974, 0.0, 65.94347122192383, 0.0, 0.0, 0.0], 'rewardMean': 0.7888606168636669, 'totalEpisodes': 124, 'stepsPerEpisode': 44, 'rewardPerEpisode': 28.54569176325127
'totalSteps': 11520, 'rewardStep': 0.9459944976607808, 'errorList': [211.80735565618383, 102.01670187389567, 156.35354477877803, 182.75653462836505, 156.61530982201148, 136.37504831654695, 70.9365910434013, 72.77909659140012, 22.13293435938733, 199.93961327214484, 159.88994292270226, 199.41014186159873, 103.08369143691971, 163.0446872604733, 151.75300685144444, 151.2532605574597, 155.1748605405752, 94.9684917813812, 158.44941972629925, 61.27901294792169, 175.85129954216637, 190.31421409323178, 83.88874224723318, 192.49436665270642, 103.48951005684334, 190.7427712852993, 189.509964737725, 126.28107936852129, 136.9030977909105, 160.83416413325307, 207.6352340430821, 135.67387951246272, 144.09118262273375, 71.96094969477903, 160.38764836701017, 108.38108606510062, 204.25518339956048, 52.002490038470434, 141.9493441063934, 44.0657075687776, 159.78139394757275, 174.18138416881666, 162.39005440644797, 6.267906422707042, 201.57524519016843, 9.076337955219705, 139.60702913990397, 90.84278184887637, 154.3893730391554, 182.45490624985945], 'lossList': [0.0, -1.3961971986293793, 0.0, 34.445795531272886, 0.0, 0.0, 0.0], 'rewardMean': 0.8063199369522351, 'totalEpisodes': 144, 'stepsPerEpisode': 49, 'rewardPerEpisode': 41.68626184557479, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.4157629215335803, 'errorList': [], 'lossList': [0.0, -1.3857824414968491, 0.0, 32.51129432678223, 0.0, 0.0, 0.0], 'rewardMean': 0.7672642354103696, 'totalEpisodes': 154, 'stepsPerEpisode': 122, 'rewardPerEpisode': 82.69454008897088
'totalSteps': 14080, 'rewardStep': 0.5759207162471979, 'errorList': [], 'lossList': [0.0, -1.3808551520109176, 0.0, 22.19988641500473, 0.0, 0.0, 0.0], 'rewardMean': 0.7274869622983173, 'totalEpisodes': 162, 'stepsPerEpisode': 215, 'rewardPerEpisode': 171.63067885448265
'totalSteps': 15360, 'rewardStep': 0.680461932064865, 'errorList': [], 'lossList': [0.0, -1.3709290629625321, 0.0, 21.177532002925872, 0.0, 0.0, 0.0], 'rewardMean': 0.7097488717613109, 'totalEpisodes': 168, 'stepsPerEpisode': 70, 'rewardPerEpisode': 60.733507567494094
'totalSteps': 16640, 'rewardStep': 0.9451442403022355, 'errorList': [0.6769445750146463, 0.78729239144389, 0.20712903890424206, 0.21640221043910648, 0.7254486995950923, 0.15719306553908338, 0.14417577425114114, 0.44148303637653197, 1.4510882328291121, 0.21085793394277974, 0.6363758386800866, 0.5213056170599428, 0.4452387063239357, 0.2761563512877536, 0.6577794919050032, 0.4809778435898108, 0.4081461941546521, 0.4430253918097082, 0.5106426526705213, 0.37358705920065305, 0.6061497417091767, 0.7030561661665288, 0.5018927989296625, 0.3669480764039615, 0.3675822967022951, 0.3471564339830159, 0.3724237571506528, 0.26458205673370583, 0.4779376553360064, 0.5149237278156367, 0.312478282803314, 0.3392015310707878, 0.6042731351562253, 0.5680058874174562, 0.6874520639273789, 0.13436854723414163, 0.19980184741779589, 0.28330578914688664, 0.16292392660350358, 0.1432393257655114, 0.3758744581914004, 0.30663986517608355, 0.4651093620090825, 0.21965049086013672, 0.24308663713604306, 0.9392236456348256, 0.8991875185485072, 0.5977357260552526, 0.5932579284590137, 0.14818969236350463], 'lossList': [0.0, -1.3653332537412644, 0.0, 8.719701602458954, 0.0, 0.0, 0.0], 'rewardMean': 0.7576094958608761, 'totalEpisodes': 173, 'stepsPerEpisode': 97, 'rewardPerEpisode': 85.87584755538475, 'successfulTests': 7
'totalSteps': 17920, 'rewardStep': 0.8212619402764987, 'errorList': [], 'lossList': [0.0, -1.377390968799591, 0.0, 9.62077056646347, 0.0, 0.0, 0.0], 'rewardMean': 0.7668538709964222, 'totalEpisodes': 175, 'stepsPerEpisode': 63, 'rewardPerEpisode': 51.25442650122804
'totalSteps': 19200, 'rewardStep': 0.7858724140991182, 'errorList': [], 'lossList': [0.0, -1.3803280681371688, 0.0, 24.934873976707458, 0.0, 0.0, 0.0], 'rewardMean': 0.754639208354643, 'totalEpisodes': 176, 'stepsPerEpisode': 819, 'rewardPerEpisode': 665.516882678447
'totalSteps': 20480, 'rewardStep': 0.7007357356446493, 'errorList': [], 'lossList': [0.0, -1.3697198563814164, 0.0, 4.652458938658238, 0.0, 0.0, 0.0], 'rewardMean': 0.7268219506628733, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 971.1817207141348
'totalSteps': 21760, 'rewardStep': 0.8761059734500382, 'errorList': [], 'lossList': [0.0, -1.3564826327562332, 0.0, 4.1801412576437, 0.0, 0.0, 0.0], 'rewardMean': 0.7273504086038726, 'totalEpisodes': 176, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1019.5922707570709
'totalSteps': 23040, 'rewardStep': 0.6285695995678345, 'errorList': [], 'lossList': [0.0, -1.3359679061174392, 0.0, 6.680203802585602, 0.0, 0.0, 0.0], 'rewardMean': 0.7375829970846798, 'totalEpisodes': 177, 'stepsPerEpisode': 234, 'rewardPerEpisode': 194.7442838590242
'totalSteps': 24320, 'rewardStep': 0.857012429776607, 'errorList': [], 'lossList': [0.0, -1.3186260491609574, 0.0, 1.763076402693987, 0.0, 0.0, 0.0], 'rewardMean': 0.7286847902962624, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.8979009288596
'totalSteps': 25600, 'rewardStep': 0.8199062879300548, 'errorList': [], 'lossList': [0.0, -1.3229935997724533, 0.0, 0.8584458301961422, 0.0, 0.0, 0.0], 'rewardMean': 0.7690991269359099, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.6488506303285
'totalSteps': 26880, 'rewardStep': 0.9527072548819951, 'errorList': [0.04719786655435906, 0.047435145480059504, 0.0801234359275182, 0.06293975026358545, 0.06318275411769819, 0.08111929277362818, 0.10902949934787755, 0.07471787725127305, 0.07553062431494731, 0.04981348147763628, 0.08246134562185654, 0.05853784717229452, 0.10270503558969035, 0.062116097486491606, 0.06551431709616368, 0.04878003382798826, 0.07340338624997758, 0.08597862335719647, 0.061259479175431585, 0.06313705629316474, 0.07381924096848495, 0.06400811408813249, 0.09540597631554742, 0.08068829463391101, 0.0573800505235539, 0.07072184692966275, 0.06136516250544574, 0.07184436894291914, 0.08048260899896313, 0.07737513164698546, 0.06797670299264705, 0.07100861252201161, 0.06679662479700793, 0.0937057279992089, 0.08281642316432325, 0.08189258332802532, 0.07694486128401809, 0.0528504125886893, 0.05712194841547658, 0.09177614022349138, 0.06342321246630539, 0.10053091179011296, 0.0821069034368559, 0.06064156067968132, 0.10015754311469112, 0.07370130871960545, 0.08456472307998476, 0.09065962989366802, 0.06849915782551741, 0.08736154495604952], 'lossList': [0.0, -1.296746111512184, 0.0, 0.8671185294538737, 0.0, 0.0, 0.0], 'rewardMean': 0.8067777807993896, 'totalEpisodes': 177, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.155932604687, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=98.75
