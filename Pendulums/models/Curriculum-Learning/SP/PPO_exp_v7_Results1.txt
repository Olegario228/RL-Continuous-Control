#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000, 10000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.834403614878477, 'errorList': [], 'lossList': [0.0, -1.4132035720348357, 0.0, 82.28911556720733, 0.0, 0.0, 0.0], 'rewardMean': 0.834403614878477, 'totalEpisodes': 6, 'stepsPerEpisode': 116, 'rewardPerEpisode': 100.87355602208763
'totalSteps': 2560, 'rewardStep': 0.5286656473230745, 'errorList': [], 'lossList': [0.0, -1.4107400369644165, 0.0, 29.968337857723235, 0.0, 0.0, 0.0], 'rewardMean': 0.6815346311007757, 'totalEpisodes': 14, 'stepsPerEpisode': 93, 'rewardPerEpisode': 64.42048787490168
'totalSteps': 3840, 'rewardStep': 0.8187149517589281, 'errorList': [], 'lossList': [0.0, -1.4151323574781418, 0.0, 31.52760503292084, 0.0, 0.0, 0.0], 'rewardMean': 0.7272614046534932, 'totalEpisodes': 18, 'stepsPerEpisode': 30, 'rewardPerEpisode': 25.98380485820449
'totalSteps': 5120, 'rewardStep': 0.6946836272148915, 'errorList': [], 'lossList': [0.0, -1.4375445812940597, 0.0, 48.08797842979431, 0.0, 0.0, 0.0], 'rewardMean': 0.7191169602938428, 'totalEpisodes': 27, 'stepsPerEpisode': 75, 'rewardPerEpisode': 52.313199461299256
'totalSteps': 6400, 'rewardStep': 0.47850057470781576, 'errorList': [], 'lossList': [0.0, -1.4474675643444062, 0.0, 34.542420613765714, 0.0, 0.0, 0.0], 'rewardMean': 0.6709936831766374, 'totalEpisodes': 31, 'stepsPerEpisode': 83, 'rewardPerEpisode': 60.88010481065486
'totalSteps': 7680, 'rewardStep': 0.5775078651294088, 'errorList': [], 'lossList': [0.0, -1.4097370511293412, 0.0, 145.94737064361573, 0.0, 0.0, 0.0], 'rewardMean': 0.6554127135020993, 'totalEpisodes': 51, 'stepsPerEpisode': 25, 'rewardPerEpisode': 19.201043500782347
'totalSteps': 8960, 'rewardStep': 0.7567755279498328, 'errorList': [], 'lossList': [0.0, -1.4027507758140565, 0.0, 151.62852359771728, 0.0, 0.0, 0.0], 'rewardMean': 0.6698931155660611, 'totalEpisodes': 89, 'stepsPerEpisode': 16, 'rewardPerEpisode': 14.117369502462017
'totalSteps': 10240, 'rewardStep': 0.603441752205464, 'errorList': [], 'lossList': [0.0, -1.3998457396030426, 0.0, 82.99238380432129, 0.0, 0.0, 0.0], 'rewardMean': 0.6615866951459866, 'totalEpisodes': 118, 'stepsPerEpisode': 42, 'rewardPerEpisode': 30.648028711195394
'totalSteps': 11520, 'rewardStep': 0.5513634578173112, 'errorList': [], 'lossList': [0.0, -1.3807405602931977, 0.0, 50.47670099258423, 0.0, 0.0, 0.0], 'rewardMean': 0.6493396687761338, 'totalEpisodes': 132, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.088173049838508
'totalSteps': 12800, 'rewardStep': 0.5761604311662825, 'errorList': [], 'lossList': [0.0, -1.337932315468788, 0.0, 48.24587334156036, 0.0, 0.0, 0.0], 'rewardMean': 0.6420217450151486, 'totalEpisodes': 144, 'stepsPerEpisode': 116, 'rewardPerEpisode': 73.80234934318439
'totalSteps': 14080, 'rewardStep': 0.7097507132681082, 'errorList': [], 'lossList': [0.0, -1.315567263364792, 0.0, 27.499932677745818, 0.0, 0.0, 0.0], 'rewardMean': 0.6295564548541118, 'totalEpisodes': 150, 'stepsPerEpisode': 385, 'rewardPerEpisode': 302.9633370057719
'totalSteps': 15360, 'rewardStep': 0.9395897694656771, 'errorList': [4.077692212786949, 15.810402984046592, 11.411558629358051, 8.253047015002224, 5.637967464646351, 12.906578268906605, 5.542691856447905, 16.183956694939493, 13.81605553429626, 11.239066210093732, 10.96630132272974, 5.048793957295577, 16.71343814773551, 13.962150527986859, 11.890572923430227, 10.934715062146601, 19.26018122923826, 10.633400622218387, 2.6916328675845635, 9.361993952304049, 8.162957013603657, 16.43807426612135, 13.894608468719099, 17.109159685507443, 14.691193267427407, 11.26011788152857, 14.661791905726227, 13.323974212990867, 7.635730147291539, 5.514396393952471, 9.095302270004677, 10.204558758819202, 14.227322338227962, 10.869416707914539, 9.460881954602723, 13.906970581482199, 12.249519645808126, 10.342801764356613, 8.038560706100938, 5.076075461792959, 11.465104732256853, 11.12140950991203, 15.578914613503033, 14.417610574250093, 12.837591825385479, 15.348407262662537, 18.015049912804898, 7.663432126407892, 12.29362733317916, 12.64807881059473], 'lossList': [0.0, -1.3157357895374298, 0.0, 13.180572246313096, 0.0, 0.0, 0.0], 'rewardMean': 0.6706488670683719, 'totalEpisodes': 156, 'stepsPerEpisode': 221, 'rewardPerEpisode': 190.22091577832393, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.6034962745570163, 'errorList': [], 'lossList': [0.0, -1.3182815110683441, 0.0, 9.438017251491546, 0.0, 0.0, 0.0], 'rewardMean': 0.6491269993481807, 'totalEpisodes': 161, 'stepsPerEpisode': 238, 'rewardPerEpisode': 177.1873983911988
'totalSteps': 17920, 'rewardStep': 0.393762994511021, 'errorList': [], 'lossList': [0.0, -1.302343413233757, 0.0, 19.11507397174835, 0.0, 0.0, 0.0], 'rewardMean': 0.6190349360777938, 'totalEpisodes': 167, 'stepsPerEpisode': 150, 'rewardPerEpisode': 95.4621841067152
'totalSteps': 19200, 'rewardStep': 0.5647414525697382, 'errorList': [], 'lossList': [0.0, -1.3073702043294906, 0.0, 4.719494683742523, 0.0, 0.0, 0.0], 'rewardMean': 0.627659023863986, 'totalEpisodes': 171, 'stepsPerEpisode': 337, 'rewardPerEpisode': 235.2352078945503
'totalSteps': 20480, 'rewardStep': 0.7119863757526074, 'errorList': [], 'lossList': [0.0, -1.3060285550355912, 0.0, 3.8086519491672517, 0.0, 0.0, 0.0], 'rewardMean': 0.6411068749263058, 'totalEpisodes': 174, 'stepsPerEpisode': 117, 'rewardPerEpisode': 101.2292572229923
'totalSteps': 21760, 'rewardStep': 0.34299076671309675, 'errorList': [], 'lossList': [0.0, -1.2840476459264756, 0.0, 8.597177716493606, 0.0, 0.0, 0.0], 'rewardMean': 0.5997283988026323, 'totalEpisodes': 175, 'stepsPerEpisode': 820, 'rewardPerEpisode': 604.1742054058865
'totalSteps': 23040, 'rewardStep': 0.8184626870386658, 'errorList': [], 'lossList': [0.0, -1.2521757185459137, 0.0, 2.6024839025735855, 0.0, 0.0, 0.0], 'rewardMean': 0.6212304922859525, 'totalEpisodes': 175, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 831.9854669180069
'totalSteps': 24320, 'rewardStep': 0.6488635480991105, 'errorList': [], 'lossList': [0.0, -1.21984168112278, 0.0, 3.8817288690805434, 0.0, 0.0, 0.0], 'rewardMean': 0.6309805013141324, 'totalEpisodes': 179, 'stepsPerEpisode': 151, 'rewardPerEpisode': 117.27017076308852
'totalSteps': 25600, 'rewardStep': 0.9251009029793676, 'errorList': [], 'lossList': [0.0, -1.1895295798778533, 0.0, 2.5820218008756637, 0.0, 0.0, 0.0], 'rewardMean': 0.6658745484954409, 'totalEpisodes': 182, 'stepsPerEpisode': 12, 'rewardPerEpisode': 11.123690475565137
'totalSteps': 26880, 'rewardStep': 0.7423072566511331, 'errorList': [], 'lossList': [0.0, -1.187306119799614, 0.0, 1.902901317179203, 0.0, 0.0, 0.0], 'rewardMean': 0.6691302028337434, 'totalEpisodes': 183, 'stepsPerEpisode': 412, 'rewardPerEpisode': 339.5950376877313
'totalSteps': 28160, 'rewardStep': 0.838661090754724, 'errorList': [], 'lossList': [0.0, -1.161076347231865, 0.0, 1.5506620118021965, 0.0, 0.0, 0.0], 'rewardMean': 0.6590373349626482, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1002.2007480999962
'totalSteps': 29440, 'rewardStep': 0.8808332703727064, 'errorList': [], 'lossList': [0.0, -1.1079696500301361, 0.0, 0.5184773183614015, 0.0, 0.0, 0.0], 'rewardMean': 0.6867710345442171, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1060.0781496843304
'totalSteps': 30720, 'rewardStep': 0.9064920918500532, 'errorList': [], 'lossList': [0.0, -1.0779308915138244, 0.0, 0.4548653497546911, 0.0, 0.0, 0.0], 'rewardMean': 0.7380439442781203, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.5194873031476
'totalSteps': 32000, 'rewardStep': 0.9435357153617403, 'errorList': [0.08629578650515293, 0.11806474331174506, 0.08331761538031549, 0.13354878707752654, 0.06919612158468522, 0.10151195254653156, 0.10444648205725904, 0.05799481818293461, 0.08858001094384062, 0.08015612726221778, 0.06314351312626144, 0.06431243474016106, 0.09726462963030807, 0.1336451131329845, 0.06695504662667558, 0.127001573420136, 0.08892156514272734, 0.06575635060806505, 0.12612960942799523, 0.12319806987165033, 0.12799490574449338, 0.06253157556060823, 0.08921498153109546, 0.13679409032671816, 0.1247549250602491, 0.1081637055344388, 0.07050203874588416, 0.12112675012580996, 0.108767259287672, 0.07503229755035275, 0.08771478456600695, 0.1161669001884883, 0.05477520994648308, 0.0616115796919157, 0.09946386573942027, 0.10898528661586772, 0.12220218225252054, 0.059407593049583825, 0.11108124541628692, 0.12275430057602257, 0.1189622936475526, 0.08892433857194104, 0.1322776086746805, 0.07995836421129614, 0.055366497537626534, 0.09777300224251262, 0.09390160787619929, 0.15416046585504475, 0.08039289583593774, 0.12349554221378714], 'lossList': [0.0, -1.054963988661766, 0.0, 0.4698922984069213, 0.0, 0.0, 0.0], 'rewardMean': 0.7759233705573205, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1188.44316096893, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=32000, timeSpent=90.86
