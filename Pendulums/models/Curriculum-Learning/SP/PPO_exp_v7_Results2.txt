#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000, 10000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8280920161904477, 'errorList': [], 'lossList': [0.0, -1.41705382168293, 0.0, 82.44616906642914, 0.0, 0.0, 0.0], 'rewardMean': 0.8280920161904477, 'totalEpisodes': 5, 'stepsPerEpisode': 241, 'rewardPerEpisode': 205.5822162166961
'totalSteps': 2560, 'rewardStep': 0.6405074743428226, 'errorList': [], 'lossList': [0.0, -1.4144787830114365, 0.0, 28.773785285949707, 0.0, 0.0, 0.0], 'rewardMean': 0.7342997452666351, 'totalEpisodes': 14, 'stepsPerEpisode': 119, 'rewardPerEpisode': 89.61588615210077
'totalSteps': 3840, 'rewardStep': 0.8405232560686556, 'errorList': [], 'lossList': [0.0, -1.399182243347168, 0.0, 30.25256422281265, 0.0, 0.0, 0.0], 'rewardMean': 0.769707582200642, 'totalEpisodes': 20, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.517780786992834
'totalSteps': 5120, 'rewardStep': 0.5087591855408902, 'errorList': [], 'lossList': [0.0, -1.3671398705244064, 0.0, 28.39423264026642, 0.0, 0.0, 0.0], 'rewardMean': 0.704470483035704, 'totalEpisodes': 26, 'stepsPerEpisode': 67, 'rewardPerEpisode': 42.74719106346588
'totalSteps': 6400, 'rewardStep': 0.028972860418325475, 'errorList': [], 'lossList': [0.0, -1.358095588684082, 0.0, 33.66952144145966, 0.0, 0.0, 0.0], 'rewardMean': 0.5693709585122283, 'totalEpisodes': 29, 'stepsPerEpisode': 186, 'rewardPerEpisode': 134.3634726760865
'totalSteps': 7680, 'rewardStep': 0.9626538750596798, 'errorList': [], 'lossList': [0.0, -1.3511519277095794, 0.0, 209.1596652984619, 0.0, 0.0, 0.0], 'rewardMean': 0.6349181112701369, 'totalEpisodes': 66, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.373238903889137
'totalSteps': 8960, 'rewardStep': 0.43416485101630853, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.5847297962066798, 'totalEpisodes': 109, 'stepsPerEpisode': 89, 'rewardPerEpisode': 67.20980485605722
'totalSteps': 10240, 'rewardStep': 0.8335704656491575, 'errorList': [], 'lossList': [0.0, -1.3528657370805741, 0.0, 119.5013271331787, 0.0, 0.0, 0.0], 'rewardMean': 0.6123787594780662, 'totalEpisodes': 148, 'stepsPerEpisode': 40, 'rewardPerEpisode': 33.71354002517783
'totalSteps': 11520, 'rewardStep': 0.9490078741055259, 'errorList': [174.62981130150735, 6.336580067824035, 160.36181222892787, 188.43551730982597, 247.79456920170836, 184.39734657270026, 242.25686227803558, 204.95419581245585, 207.83408613976795, 198.38002139064807, 187.03078008811107, 215.29055763141753, 209.17109703329933, 88.24201442376994, 129.30908424729387, 234.811671151753, 197.83341238599706, 171.31025033729122, 154.11472601178014, 112.57270881429756, 220.77027600439916, 113.68122199955323, 202.79512081404607, 137.08359162389473, 58.350703039175485, 221.87668679951105, 160.96820249362747, 227.58765037148837, 179.3210029240328, 91.7206357332581, 50.58460802922313, 197.89733364790627, 224.66716942469415, 243.20988649184483, 172.6865034225747, 220.0227993576945, 146.96501547308767, 152.5229712579423, 196.05729059073897, 162.82886365737355, 147.1120804881267, 124.96468211816968, 156.63831253910092, 206.47622771562268, 174.3856127318261, 234.4241844015357, 187.22491732717512, 169.9601871988976, 123.36968121556394, 212.7469042778289], 'lossList': [0.0, -1.3641732960939408, 0.0, 64.90066415786742, 0.0, 0.0, 0.0], 'rewardMean': 0.6460416709408121, 'totalEpisodes': 176, 'stepsPerEpisode': 18, 'rewardPerEpisode': 16.937015299289264, 'successfulTests': 0
'totalSteps': 12800, 'rewardStep': 0.647958150427865, 'errorList': [], 'lossList': [0.0, -1.3735128980875015, 0.0, 37.78453828811646, 0.0, 0.0, 0.0], 'rewardMean': 0.6280282843645539, 'totalEpisodes': 192, 'stepsPerEpisode': 12, 'rewardPerEpisode': 6.811036583978626
'totalSteps': 14080, 'rewardStep': 0.8262695870402875, 'errorList': [], 'lossList': [0.0, -1.3703556311130525, 0.0, 14.421440911293029, 0.0, 0.0, 0.0], 'rewardMean': 0.6466044956343004, 'totalEpisodes': 199, 'stepsPerEpisode': 44, 'rewardPerEpisode': 35.871843803936464
'totalSteps': 15360, 'rewardStep': 0.45356988196031645, 'errorList': [], 'lossList': [0.0, -1.364889405965805, 0.0, 22.889051513671873, 0.0, 0.0, 0.0], 'rewardMean': 0.6079091582234665, 'totalEpisodes': 207, 'stepsPerEpisode': 92, 'rewardPerEpisode': 58.91330678444038
'totalSteps': 16640, 'rewardStep': 0.7718691507217857, 'errorList': [], 'lossList': [0.0, -1.367347116470337, 0.0, 16.48118236541748, 0.0, 0.0, 0.0], 'rewardMean': 0.6342201547415561, 'totalEpisodes': 216, 'stepsPerEpisode': 125, 'rewardPerEpisode': 102.65446172591763
'totalSteps': 17920, 'rewardStep': 0.8393272986543433, 'errorList': [], 'lossList': [0.0, -1.3587965524196626, 0.0, 14.884020092487335, 0.0, 0.0, 0.0], 'rewardMean': 0.7152555985651577, 'totalEpisodes': 222, 'stepsPerEpisode': 72, 'rewardPerEpisode': 63.904101161196536
'totalSteps': 19200, 'rewardStep': 0.6471788098772907, 'errorList': [], 'lossList': [0.0, -1.3383933126926422, 0.0, 18.63231610417366, 0.0, 0.0, 0.0], 'rewardMean': 0.683708092046919, 'totalEpisodes': 225, 'stepsPerEpisode': 139, 'rewardPerEpisode': 103.16571786833434
'totalSteps': 20480, 'rewardStep': 0.8808040647356641, 'errorList': [], 'lossList': [0.0, -1.317540985941887, 0.0, 7.082213858366012, 0.0, 0.0, 0.0], 'rewardMean': 0.7283720134188545, 'totalEpisodes': 230, 'stepsPerEpisode': 65, 'rewardPerEpisode': 53.152028540749185
'totalSteps': 21760, 'rewardStep': 0.8342562285521588, 'errorList': [], 'lossList': [0.0, -1.2932425475120544, 0.0, 4.510041902065277, 0.0, 0.0, 0.0], 'rewardMean': 0.7683811511724394, 'totalEpisodes': 234, 'stepsPerEpisode': 96, 'rewardPerEpisode': 84.20371158552429
'totalSteps': 23040, 'rewardStep': 0.8144828210467162, 'errorList': [], 'lossList': [0.0, -1.2806421214342116, 0.0, 2.3496943587064743, 0.0, 0.0, 0.0], 'rewardMean': 0.7664723867121953, 'totalEpisodes': 237, 'stepsPerEpisode': 244, 'rewardPerEpisode': 204.28177185434959
'totalSteps': 24320, 'rewardStep': 0.9268811433927141, 'errorList': [], 'lossList': [0.0, -1.2592371219396592, 0.0, 2.4179396814107896, 0.0, 0.0, 0.0], 'rewardMean': 0.7642597136409142, 'totalEpisodes': 238, 'stepsPerEpisode': 1008, 'rewardPerEpisode': 875.7079735524467
'totalSteps': 25600, 'rewardStep': 0.6836186429852001, 'errorList': [], 'lossList': [0.0, -1.2286701953411103, 0.0, 2.474077293872833, 0.0, 0.0, 0.0], 'rewardMean': 0.7678257628966477, 'totalEpisodes': 239, 'stepsPerEpisode': 248, 'rewardPerEpisode': 191.08453400164677
'totalSteps': 26880, 'rewardStep': 0.8290375626915512, 'errorList': [], 'lossList': [0.0, -1.212710456252098, 0.0, 1.4167054702341557, 0.0, 0.0, 0.0], 'rewardMean': 0.768102560461774, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1080.9198411260488
'totalSteps': 28160, 'rewardStep': 0.8939165148758919, 'errorList': [], 'lossList': [0.0, -1.1802608835697175, 0.0, 0.9395927339792252, 0.0, 0.0, 0.0], 'rewardMean': 0.8121372237533316, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1128.9693271428748
'totalSteps': 29440, 'rewardStep': 0.937069123476246, 'errorList': [0.037263495372295785, 0.039494386895632005, 0.058403303872775314, 0.04390314904249151, 0.03153379045184308, 0.03581284529335277, 0.045831787758501275, 0.06105566997576357, 0.03612986043477051, 0.033535507998829914, 0.044217101315851534, 0.03518027890655409, 0.04235069518846552, 0.03789918036021425, 0.03486183989569947, 0.030380809202657472, 0.034519823003353024, 0.035448998487837266, 0.04002066244893655, 0.03499475235484723, 0.033490033168610295, 0.036338691825983205, 0.04191183861425195, 0.03935036059688331, 0.03411012267441822, 0.037065427482254595, 0.03268313090079843, 0.03749225281184636, 0.04170689475219296, 0.03632230830551177, 0.04535088031052994, 0.04857639654544777, 0.041457532438511586, 0.03750829962652952, 0.03655617943948833, 0.06356592193325829, 0.039173358497900046, 0.03346305211287503, 0.03571959555149421, 0.04308213950304341, 0.03285540972761777, 0.05266195172154554, 0.047455844490971504, 0.040683481913126845, 0.03090807320292907, 0.033893122738475216, 0.03570916635127972, 0.03205150701930163, 0.0405800818302592, 0.04207914283380897], 'lossList': [0.0, -1.1341386705636978, 0.0, 0.599002612233162, 0.0, 0.0, 0.0], 'rewardMean': 0.8286572210287776, 'totalEpisodes': 239, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1175.5895973648057, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=29440, timeSpent=87.44
