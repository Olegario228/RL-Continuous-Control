#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000, 10000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.720468672746902, 'errorList': [], 'lossList': [0.0, -1.4278227895498277, 0.0, 82.66252764225005, 0.0, 0.0, 0.0], 'rewardMean': 0.720468672746902, 'totalEpisodes': 6, 'stepsPerEpisode': 204, 'rewardPerEpisode': 144.31493761881387
'totalSteps': 2560, 'rewardStep': 0.7507429405893908, 'errorList': [], 'lossList': [0.0, -1.433533914089203, 0.0, 27.691174790859222, 0.0, 0.0, 0.0], 'rewardMean': 0.7356058066681463, 'totalEpisodes': 10, 'stepsPerEpisode': 22, 'rewardPerEpisode': 18.764362443060104
'totalSteps': 3840, 'rewardStep': 0.8637562853032059, 'errorList': [], 'lossList': [0.0, -1.42561077773571, 0.0, 33.0931499838829, 0.0, 0.0, 0.0], 'rewardMean': 0.7783226328798328, 'totalEpisodes': 13, 'stepsPerEpisode': 361, 'rewardPerEpisode': 274.9671405001057
'totalSteps': 5120, 'rewardStep': 0.7738541445728554, 'errorList': [], 'lossList': [0.0, -1.4340560299158096, 0.0, 35.793261506557464, 0.0, 0.0, 0.0], 'rewardMean': 0.7772055108030885, 'totalEpisodes': 14, 'stepsPerEpisode': 181, 'rewardPerEpisode': 152.4311441318912
'totalSteps': 6400, 'rewardStep': 0.48105735089768686, 'errorList': [], 'lossList': [0.0, -1.436170136332512, 0.0, 11.338880128860474, 0.0, 0.0, 0.0], 'rewardMean': 0.7179758788220082, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 839.4424776684691
'totalSteps': 7680, 'rewardStep': 0.9702066325853771, 'errorList': [], 'lossList': [0.0, -1.4110330086946488, 0.0, 199.9795106124878, 0.0, 0.0, 0.0], 'rewardMean': 0.7600143377825695, 'totalEpisodes': 40, 'stepsPerEpisode': 52, 'rewardPerEpisode': 43.879237051955954
'totalSteps': 8960, 'rewardStep': 0.8782631230727869, 'errorList': [], 'lossList': [0.0, -1.407234445810318, 0.0, 183.7304779815674, 0.0, 0.0, 0.0], 'rewardMean': 0.7769070213954578, 'totalEpisodes': 79, 'stepsPerEpisode': 15, 'rewardPerEpisode': 14.231699863304598
'totalSteps': 10240, 'rewardStep': 0.5727454279261908, 'errorList': [], 'lossList': [0.0, -1.4009675008058549, 0.0, 118.28572067260743, 0.0, 0.0, 0.0], 'rewardMean': 0.7513868222117994, 'totalEpisodes': 117, 'stepsPerEpisode': 18, 'rewardPerEpisode': 10.882726896420937
'totalSteps': 11520, 'rewardStep': 0.7418567187979516, 'errorList': [], 'lossList': [0.0, -1.4012998425960541, 0.0, 65.93868213653565, 0.0, 0.0, 0.0], 'rewardMean': 0.750327921832483, 'totalEpisodes': 133, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.266217299282587
'totalSteps': 12800, 'rewardStep': 0.507060229805167, 'errorList': [], 'lossList': [0.0, -1.3985621976852416, 0.0, 64.18601502418518, 0.0, 0.0, 0.0], 'rewardMean': 0.7260011526297514, 'totalEpisodes': 146, 'stepsPerEpisode': 252, 'rewardPerEpisode': 179.00924960652424
'totalSteps': 14080, 'rewardStep': 0.9207346955197941, 'errorList': [], 'lossList': [0.0, -1.3949753904342652, 0.0, 53.183509578704836, 0.0, 0.0, 0.0], 'rewardMean': 0.7460277549070407, 'totalEpisodes': 156, 'stepsPerEpisode': 66, 'rewardPerEpisode': 51.18014498737311
'totalSteps': 15360, 'rewardStep': 0.9275526779458532, 'errorList': [], 'lossList': [0.0, -1.388305931687355, 0.0, 74.5499564743042, 0.0, 0.0, 0.0], 'rewardMean': 0.7637087286426869, 'totalEpisodes': 166, 'stepsPerEpisode': 7, 'rewardPerEpisode': 6.562102202177922
'totalSteps': 16640, 'rewardStep': 0.6709749254368875, 'errorList': [], 'lossList': [0.0, -1.3944262260198592, 0.0, 42.57726923465729, 0.0, 0.0, 0.0], 'rewardMean': 0.7444305926560549, 'totalEpisodes': 169, 'stepsPerEpisode': 218, 'rewardPerEpisode': 161.29947180289236
'totalSteps': 17920, 'rewardStep': 0.6331445533199282, 'errorList': [], 'lossList': [0.0, -1.393047814965248, 0.0, 36.195812335014345, 0.0, 0.0, 0.0], 'rewardMean': 0.7303596335307623, 'totalEpisodes': 172, 'stepsPerEpisode': 414, 'rewardPerEpisode': 329.7552949544488
'totalSteps': 19200, 'rewardStep': 0.8575716362683533, 'errorList': [], 'lossList': [0.0, -1.368946103453636, 0.0, 50.428098769187926, 0.0, 0.0, 0.0], 'rewardMean': 0.768011062067829, 'totalEpisodes': 177, 'stepsPerEpisode': 75, 'rewardPerEpisode': 58.141998685153695
'totalSteps': 20480, 'rewardStep': 0.7424877763565979, 'errorList': [], 'lossList': [0.0, -1.3446567505598068, 0.0, 10.177145614624024, 0.0, 0.0, 0.0], 'rewardMean': 0.7452391764449511, 'totalEpisodes': 179, 'stepsPerEpisode': 225, 'rewardPerEpisode': 166.20412625451112
'totalSteps': 21760, 'rewardStep': 0.9510896255605055, 'errorList': [0.7479390535112773, 0.7026414798942159, 0.7317201198857067, 0.7953149664848994, 0.7118320157203444, 0.711021510234978, 0.7373072562445881, 0.7330274301088749, 0.7175354399860627, 0.7107221166717916, 0.7133360242863338, 0.7350441353332284, 0.7111160936916978, 0.733088799092348, 0.7141377961629947, 0.7505927147424964, 0.7122367311850594, 0.7340695677459285, 0.7195965392856358, 0.735817367000166, 0.7059350591796019, 0.7178969732775172, 0.7203869949920427, 0.7713990251528338, 0.7072451095166612, 0.7206816342023419, 0.7697311683213225, 0.7099030876914303, 0.7274265128992757, 0.7237386397515336, 0.7257456939219443, 0.7443262548085278, 0.7408724443042335, 0.7497206651944625, 0.7255675047570869, 0.7226116521377123, 0.7221575635195112, 0.7039084910241129, 0.7224980940913064, 0.7216689594601756, 0.744537659131667, 0.7285348654043096, 0.6965656213495353, 0.7167155699967048, 0.7055388837687089, 0.7463556581863002, 0.7133034363636155, 0.7201059496336515, 0.7395819228779172, 0.7192243296085664], 'lossList': [0.0, -1.3245544785261154, 0.0, 10.798844769001008, 0.0, 0.0, 0.0], 'rewardMean': 0.7525218266937228, 'totalEpisodes': 180, 'stepsPerEpisode': 861, 'rewardPerEpisode': 636.2037675103486, 'successfulTests': 0
'totalSteps': 23040, 'rewardStep': 0.4430895596184699, 'errorList': [], 'lossList': [0.0, -1.322243371605873, 0.0, 6.058890529870987, 0.0, 0.0, 0.0], 'rewardMean': 0.7395562398629508, 'totalEpisodes': 181, 'stepsPerEpisode': 839, 'rewardPerEpisode': 567.0595986811501
'totalSteps': 24320, 'rewardStep': 0.5903232148164458, 'errorList': [], 'lossList': [0.0, -1.3185847878456116, 0.0, 6.218162156939506, 0.0, 0.0, 0.0], 'rewardMean': 0.7244028894648004, 'totalEpisodes': 182, 'stepsPerEpisode': 1240, 'rewardPerEpisode': 892.5132448019793
'totalSteps': 25600, 'rewardStep': 0.8526414328065188, 'errorList': [], 'lossList': [0.0, -1.3086500674486161, 0.0, 3.251582436710596, 0.0, 0.0, 0.0], 'rewardMean': 0.7589610097649355, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1075.8445447231757
'totalSteps': 26880, 'rewardStep': 0.9206400494025422, 'errorList': [], 'lossList': [0.0, -1.3030335479974746, 0.0, 2.8192141362279655, 0.0, 0.0, 0.0], 'rewardMean': 0.7589515451532102, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1131.644081682542
'totalSteps': 28160, 'rewardStep': 0.7890645934812014, 'errorList': [], 'lossList': [0.0, -1.2924782884120942, 0.0, 1.4767377189919353, 0.0, 0.0, 0.0], 'rewardMean': 0.7451027367067451, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1090.080828285811
'totalSteps': 29440, 'rewardStep': 0.8466008168617033, 'errorList': [], 'lossList': [0.0, -1.3033983516693115, 0.0, 0.9542403860390186, 0.0, 0.0, 0.0], 'rewardMean': 0.7626653258492266, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1129.7701044935848
'totalSteps': 30720, 'rewardStep': 0.8261173000268539, 'errorList': [], 'lossList': [0.0, -1.2922396564483642, 0.0, 1.0712233186885713, 0.0, 0.0, 0.0], 'rewardMean': 0.7819626005199192, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1181.0097618418738
'totalSteps': 32000, 'rewardStep': 0.9035453308728548, 'errorList': [], 'lossList': [0.0, -1.2514285606145858, 0.0, 0.6342882541008293, 0.0, 0.0, 0.0], 'rewardMean': 0.7865599699803694, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1189.3571666876019
'totalSteps': 33280, 'rewardStep': 0.95289181504858, 'errorList': [0.05642348490431008, 0.07151062471985026, 0.050693563261672465, 0.052793689606887656, 0.04986347843355693, 0.08655662885675058, 0.09910237601500574, 0.06358734896745681, 0.09771133979862824, 0.09035574962246691, 0.046129891373115114, 0.09096461141908388, 0.04520572475228979, 0.06916873691836843, 0.08039269402481453, 0.06577118540603105, 0.0626100582229173, 0.06501475408532402, 0.0759449358002796, 0.05431995092724393, 0.06385463287626297, 0.05447302668707873, 0.09201955304388452, 0.05891700683353153, 0.07264493767711998, 0.09510669612963232, 0.058556113547824876, 0.10192207035919643, 0.08616682035906263, 0.07630194251718045, 0.08369284304121784, 0.1012291808613529, 0.08946058978540063, 0.08818998139219643, 0.10979056823106356, 0.06215181292954338, 0.08605080268116856, 0.07375627549238142, 0.05131527623813243, 0.08536385556717492, 0.07949606980620084, 0.08489358465435203, 0.09184166879603711, 0.05239298619605123, 0.07638012777653774, 0.06680561078806792, 0.07963622848476805, 0.08969471279426507, 0.07324746934873815, 0.0498618247184043], 'lossList': [0.0, -1.200536150932312, 0.0, 0.4785282956715673, 0.0, 0.0, 0.0], 'rewardMean': 0.8076003738495675, 'totalEpisodes': 182, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1208.0801494844316, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=33280, timeSpent=91.95
