#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v7_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 5000, 10000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5371426615508459, 'errorList': [], 'lossList': [0.0, -1.4251431292295456, 0.0, 84.48239258766175, 0.0, 0.0, 0.0], 'rewardMean': 0.5371426615508459, 'totalEpisodes': 6, 'stepsPerEpisode': 168, 'rewardPerEpisode': 119.63663339514541
'totalSteps': 2560, 'rewardStep': 0.8197129461256928, 'errorList': [], 'lossList': [0.0, -1.4182184273004532, 0.0, 27.567556908130644, 0.0, 0.0, 0.0], 'rewardMean': 0.6784278038382694, 'totalEpisodes': 13, 'stepsPerEpisode': 36, 'rewardPerEpisode': 27.90156233449731
'totalSteps': 3840, 'rewardStep': 0.7594309432089698, 'errorList': [], 'lossList': [0.0, -1.4116627120971679, 0.0, 25.10745061159134, 0.0, 0.0, 0.0], 'rewardMean': 0.7054288502951694, 'totalEpisodes': 18, 'stepsPerEpisode': 202, 'rewardPerEpisode': 130.16173147117257
'totalSteps': 5120, 'rewardStep': 0.5943374042097745, 'errorList': [], 'lossList': [0.0, -1.4082597678899764, 0.0, 16.084340302944184, 0.0, 0.0, 0.0], 'rewardMean': 0.6776559887738207, 'totalEpisodes': 18, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 890.0387443094486
'totalSteps': 6400, 'rewardStep': 0.7710839001903871, 'errorList': [], 'lossList': [0.0, -1.3959667301177978, 0.0, 34.761945410966874, 0.0, 0.0, 0.0], 'rewardMean': 0.696341571057134, 'totalEpisodes': 20, 'stepsPerEpisode': 866, 'rewardPerEpisode': 697.1874689703135
'totalSteps': 7680, 'rewardStep': 0.4839986864811432, 'errorList': [], 'lossList': [0.0, -1.39605386197567, 0.0, 131.51983108520508, 0.0, 0.0, 0.0], 'rewardMean': 0.6609510902944689, 'totalEpisodes': 33, 'stepsPerEpisode': 54, 'rewardPerEpisode': 37.46775276969893
'totalSteps': 8960, 'rewardStep': 0.6932496866968489, 'errorList': [], 'lossList': [0.0, -1.3985722190141678, 0.0, 165.09929313659669, 0.0, 0.0, 0.0], 'rewardMean': 0.6655651754948089, 'totalEpisodes': 68, 'stepsPerEpisode': 58, 'rewardPerEpisode': 43.30135512880849
'totalSteps': 10240, 'rewardStep': 0.49503726748258586, 'errorList': [], 'lossList': [0.0, -1.3940784972906113, 0.0, 83.1666365814209, 0.0, 0.0, 0.0], 'rewardMean': 0.6442491869932809, 'totalEpisodes': 93, 'stepsPerEpisode': 29, 'rewardPerEpisode': 16.893157994572356
'totalSteps': 11520, 'rewardStep': 0.579742812100153, 'errorList': [], 'lossList': [0.0, -1.3849562448263169, 0.0, 36.98255365371704, 0.0, 0.0, 0.0], 'rewardMean': 0.6370818120051556, 'totalEpisodes': 112, 'stepsPerEpisode': 124, 'rewardPerEpisode': 98.03889895422428
'totalSteps': 12800, 'rewardStep': 0.4572072427673946, 'errorList': [], 'lossList': [0.0, -1.3622278690338134, 0.0, 30.476763548851014, 0.0, 0.0, 0.0], 'rewardMean': 0.6190943550813796, 'totalEpisodes': 124, 'stepsPerEpisode': 66, 'rewardPerEpisode': 47.81325251633491
'totalSteps': 14080, 'rewardStep': 0.7399158995818986, 'errorList': [], 'lossList': [0.0, -1.3330621755123138, 0.0, 16.678020780086516, 0.0, 0.0, 0.0], 'rewardMean': 0.639371678884485, 'totalEpisodes': 132, 'stepsPerEpisode': 38, 'rewardPerEpisode': 26.176496409381773
'totalSteps': 15360, 'rewardStep': 0.8697579588572514, 'errorList': [], 'lossList': [0.0, -1.3121827095746994, 0.0, 42.745574316978455, 0.0, 0.0, 0.0], 'rewardMean': 0.6443761801576408, 'totalEpisodes': 139, 'stepsPerEpisode': 75, 'rewardPerEpisode': 64.9218678730444
'totalSteps': 16640, 'rewardStep': 0.21598235665434973, 'errorList': [], 'lossList': [0.0, -1.2892704844474792, 0.0, 8.547369223833083, 0.0, 0.0, 0.0], 'rewardMean': 0.5900313215021786, 'totalEpisodes': 141, 'stepsPerEpisode': 502, 'rewardPerEpisode': 359.35704675698275
'totalSteps': 17920, 'rewardStep': 0.7794880562942874, 'errorList': [], 'lossList': [0.0, -1.2597600656747818, 0.0, 7.403466756343842, 0.0, 0.0, 0.0], 'rewardMean': 0.60854638671063, 'totalEpisodes': 142, 'stepsPerEpisode': 1239, 'rewardPerEpisode': 902.9812512362039
'totalSteps': 19200, 'rewardStep': 0.8612787133413642, 'errorList': [], 'lossList': [0.0, -1.243157775402069, 0.0, 11.095721554756164, 0.0, 0.0, 0.0], 'rewardMean': 0.6175658680257278, 'totalEpisodes': 144, 'stepsPerEpisode': 37, 'rewardPerEpisode': 32.95025525823476
'totalSteps': 20480, 'rewardStep': 0.8237210499500268, 'errorList': [], 'lossList': [0.0, -1.22344535946846, 0.0, 3.069442326128483, 0.0, 0.0, 0.0], 'rewardMean': 0.6515381043726161, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 978.6214879213788
'totalSteps': 21760, 'rewardStep': 0.8335607122212985, 'errorList': [], 'lossList': [0.0, -1.1989077317714691, 0.0, 2.752204016149044, 0.0, 0.0, 0.0], 'rewardMean': 0.6655692069250609, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.8902909221677
'totalSteps': 23040, 'rewardStep': 0.8537673630664222, 'errorList': [], 'lossList': [0.0, -1.185057933330536, 0.0, 1.7379074563086032, 0.0, 0.0, 0.0], 'rewardMean': 0.7014422164834446, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1098.3722536604637
'totalSteps': 24320, 'rewardStep': 0.9167483405453546, 'errorList': [], 'lossList': [0.0, -1.1669842207431793, 0.0, 1.621172504425049, 0.0, 0.0, 0.0], 'rewardMean': 0.7351427693279649, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1171.0626103019626
'totalSteps': 25600, 'rewardStep': 0.9730297528804215, 'errorList': [0.05038873538498441, 0.05421655757184253, 0.043941207791864986, 0.04661783776313344, 0.046102798908458795, 0.045876788301316015, 0.08522389898066009, 0.05005596778970484, 0.0768729485482051, 0.07458506256334373, 0.06525640418563214, 0.06346393882818428, 0.07286000361561842, 0.05522445100382934, 0.08534145964271593, 0.06653416808152667, 0.05543971620649223, 0.09546978774373943, 0.04242343156684359, 0.052105867110278925, 0.06444124966744277, 0.048933831558139955, 0.05627635579012863, 0.07192553519657664, 0.05122145871985538, 0.048759323974877686, 0.0446755319932327, 0.044693499133616435, 0.04764172339358405, 0.04314770638366871, 0.08290777842676954, 0.05135056076078898, 0.04678891573518991, 0.0473251293633142, 0.048663284356469765, 0.05534378006861144, 0.08822058464998409, 0.05179194485118702, 0.045118998970017914, 0.04812642829014241, 0.05227359054152014, 0.04806712961169535, 0.057189582809102114, 0.05610411500894751, 0.04506389816085231, 0.09245968028692747, 0.053005402489251445, 0.0680006676788191, 0.08848921431346878, 0.05508639758689056], 'lossList': [0.0, -1.141569092273712, 0.0, 1.393082933165133, 0.0, 0.0, 0.0], 'rewardMean': 0.7867250203392675, 'totalEpisodes': 144, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.3540719367759, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=65.28
