#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.1, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8429282860660212, 'errorList': [], 'lossList': [0.0, -1.4285613173246383, 0.0, 95.01976095199585, 0.0, 0.0, 0.0], 'rewardMean': 0.8429282860660212, 'totalEpisodes': 6, 'stepsPerEpisode': 162, 'rewardPerEpisode': 139.66137887053395
'totalSteps': 2560, 'rewardStep': 0.8739700910980726, 'errorList': [], 'lossList': [0.0, -1.4404729419946671, 0.0, 27.004051829576493, 0.0, 0.0, 0.0], 'rewardMean': 0.8584491885820469, 'totalEpisodes': 9, 'stepsPerEpisode': 223, 'rewardPerEpisode': 162.15480734782744
'totalSteps': 3840, 'rewardStep': 0.42432544738699574, 'errorList': [], 'lossList': [0.0, -1.4413786739110948, 0.0, 31.989982235431672, 0.0, 0.0, 0.0], 'rewardMean': 0.7137412748503632, 'totalEpisodes': 13, 'stepsPerEpisode': 263, 'rewardPerEpisode': 193.5614490091544
'totalSteps': 5120, 'rewardStep': 0.8701240774733104, 'errorList': [], 'lossList': [0.0, -1.440006108880043, 0.0, 25.263187160491942, 0.0, 0.0, 0.0], 'rewardMean': 0.7528369755061001, 'totalEpisodes': 14, 'stepsPerEpisode': 1274, 'rewardPerEpisode': 941.2248208129458
'totalSteps': 6400, 'rewardStep': 0.9848640898442724, 'errorList': [], 'lossList': [0.0, -1.4322613835334779, 0.0, 26.79175545454025, 0.0, 0.0, 0.0], 'rewardMean': 0.7992423983737346, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1072.311484076374
'totalSteps': 7680, 'rewardStep': 0.7440540536746454, 'errorList': [], 'lossList': [0.0, -1.4239571303129197, 0.0, 9.915406733155251, 0.0, 0.0, 0.0], 'rewardMean': 0.7900443409238864, 'totalEpisodes': 14, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 972.7257481139736
'totalSteps': 8960, 'rewardStep': 0.46118439128876765, 'errorList': [], 'lossList': [0.0, -1.396061017513275, 0.0, 213.25106563568116, 0.0, 0.0, 0.0], 'rewardMean': 0.7430643481188695, 'totalEpisodes': 31, 'stepsPerEpisode': 100, 'rewardPerEpisode': 70.5295839023017
'totalSteps': 10240, 'rewardStep': 0.8449365091371848, 'errorList': [], 'lossList': [0.0, -1.389907134771347, 0.0, 307.20372634887696, 0.0, 0.0, 0.0], 'rewardMean': 0.7557983682461588, 'totalEpisodes': 67, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.6662610254191765
'totalSteps': 11520, 'rewardStep': 0.5199591389069219, 'errorList': [], 'lossList': [0.0, -1.3906700658798217, 0.0, 219.30822395324708, 0.0, 0.0, 0.0], 'rewardMean': 0.729594009430688, 'totalEpisodes': 105, 'stepsPerEpisode': 12, 'rewardPerEpisode': 8.152182101878529
'totalSteps': 12800, 'rewardStep': 0.6540965014869112, 'errorList': [], 'lossList': [0.0, -1.3865449279546738, 0.0, 126.62596309661865, 0.0, 0.0, 0.0], 'rewardMean': 0.7220442586363103, 'totalEpisodes': 143, 'stepsPerEpisode': 14, 'rewardPerEpisode': 8.023537410693772
'totalSteps': 14080, 'rewardStep': 0.5851015209311161, 'errorList': [], 'lossList': [0.0, -1.3779349386692048, 0.0, 85.52031715393066, 0.0, 0.0, 0.0], 'rewardMean': 0.6962615821228197, 'totalEpisodes': 172, 'stepsPerEpisode': 5, 'rewardPerEpisode': 2.7859405719641988
'totalSteps': 15360, 'rewardStep': 0.8268358832607053, 'errorList': [], 'lossList': [0.0, -1.3794352000951766, 0.0, 84.2908151435852, 0.0, 0.0, 0.0], 'rewardMean': 0.6915481613390831, 'totalEpisodes': 191, 'stepsPerEpisode': 34, 'rewardPerEpisode': 31.935009779904806
'totalSteps': 16640, 'rewardStep': 0.854681844470512, 'errorList': [], 'lossList': [0.0, -1.3881032001972198, 0.0, 62.72127870559692, 0.0, 0.0, 0.0], 'rewardMean': 0.7345838010474347, 'totalEpisodes': 204, 'stepsPerEpisode': 71, 'rewardPerEpisode': 61.605425583019844
'totalSteps': 17920, 'rewardStep': 0.9132498152964144, 'errorList': [], 'lossList': [0.0, -1.4026191705465316, 0.0, 90.21233341217041, 0.0, 0.0, 0.0], 'rewardMean': 0.7388963748297451, 'totalEpisodes': 216, 'stepsPerEpisode': 33, 'rewardPerEpisode': 30.049124205696575
'totalSteps': 19200, 'rewardStep': 0.9264985570579599, 'errorList': [], 'lossList': [0.0, -1.4079729670286179, 0.0, 78.06122440338135, 0.0, 0.0, 0.0], 'rewardMean': 0.7330598215511139, 'totalEpisodes': 224, 'stepsPerEpisode': 16, 'rewardPerEpisode': 13.858692968625853
'totalSteps': 20480, 'rewardStep': 0.343948424345536, 'errorList': [], 'lossList': [0.0, -1.3998669904470444, 0.0, 37.42441344022751, 0.0, 0.0, 0.0], 'rewardMean': 0.6930492586182029, 'totalEpisodes': 229, 'stepsPerEpisode': 116, 'rewardPerEpisode': 70.33960456958653
'totalSteps': 21760, 'rewardStep': 0.6043485622806262, 'errorList': [], 'lossList': [0.0, -1.4027743172645568, 0.0, 23.35907037973404, 0.0, 0.0, 0.0], 'rewardMean': 0.7073656757173887, 'totalEpisodes': 235, 'stepsPerEpisode': 168, 'rewardPerEpisode': 111.14542273687933
'totalSteps': 23040, 'rewardStep': 0.5321310142271881, 'errorList': [], 'lossList': [0.0, -1.4171007478237152, 0.0, 25.331764664649963, 0.0, 0.0, 0.0], 'rewardMean': 0.6760851262263892, 'totalEpisodes': 241, 'stepsPerEpisode': 83, 'rewardPerEpisode': 64.08717211162399
'totalSteps': 24320, 'rewardStep': 0.7883973103790842, 'errorList': [], 'lossList': [0.0, -1.4172250348329545, 0.0, 12.95381979227066, 0.0, 0.0, 0.0], 'rewardMean': 0.7029289433736053, 'totalEpisodes': 244, 'stepsPerEpisode': 122, 'rewardPerEpisode': 96.11883190919185
'totalSteps': 25600, 'rewardStep': 0.43439866406859634, 'errorList': [], 'lossList': [0.0, -1.4163270181417464, 0.0, 6.954076684713364, 0.0, 0.0, 0.0], 'rewardMean': 0.6809591596317739, 'totalEpisodes': 245, 'stepsPerEpisode': 377, 'rewardPerEpisode': 292.501342263843
'totalSteps': 26880, 'rewardStep': 0.6761604229838878, 'errorList': [], 'lossList': [0.0, -1.4141167557239533, 0.0, 4.135637003183365, 0.0, 0.0, 0.0], 'rewardMean': 0.690065049837051, 'totalEpisodes': 245, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 781.33969741358
'totalSteps': 28160, 'rewardStep': 0.672141689220818, 'errorList': [], 'lossList': [0.0, -1.4241281473636627, 0.0, 6.279738428592682, 0.0, 0.0, 0.0], 'rewardMean': 0.6745956304330624, 'totalEpisodes': 247, 'stepsPerEpisode': 132, 'rewardPerEpisode': 109.59940410341218
'totalSteps': 29440, 'rewardStep': 0.7194965080772552, 'errorList': [], 'lossList': [0.0, -1.4235693550109862, 0.0, 2.423588973283768, 0.0, 0.0, 0.0], 'rewardMean': 0.6610770967937366, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 996.0342662748127
'totalSteps': 30720, 'rewardStep': 0.9456009000161526, 'errorList': [0.11872741538868521, 0.1351747297970563, 0.1656570939429182, 0.21828571175525535, 0.2811050769709415, 0.16611634088361732, 0.29045511985040307, 0.1360971999008976, 0.1149101540468746, 0.24111534233288975, 0.2538202610495865, 0.22442074757165223, 0.12664639757800286, 0.14514215204019645, 0.1076900151527346, 0.2267288909144252, 0.1690099111602938, 0.1343494672779659, 0.1419819420768187, 0.1242346992022278, 0.16817196350270758, 0.1266521674215607, 0.12038481554927741, 0.16719807063587008, 0.13193872827422162, 0.12004753726290276, 0.15005365627733555, 0.1354861766583192, 0.1273967289195489, 0.13698818822081296, 0.15974398787613323, 0.12046368216704015, 0.115351391335579, 0.13758279269778717, 0.11087378418672862, 0.13979846162128332, 0.15205742413201298, 0.1258606186777872, 0.09917194140051443, 0.15860533825083203, 0.10978711763356275, 0.20652054223912816, 0.2789796388134776, 0.1647772141568545, 0.1368668922783876, 0.11426691628988626, 0.14212498690036007, 0.10362309115941706, 0.13724986211882736, 0.12624036599135638], 'lossList': [0.0, -1.3816528737545013, 0.0, 1.561383834183216, 0.0, 0.0, 0.0], 'rewardMean': 0.6643122052657106, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.6920380890085, 'successfulTests': 41
'totalSteps': 32000, 'rewardStep': 0.8899757679749859, 'errorList': [], 'lossList': [0.0, -1.3477507936954498, 0.0, 1.144266217984259, 0.0, 0.0, 0.0], 'rewardMean': 0.660659926357413, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.0982191875262
'totalSteps': 33280, 'rewardStep': 0.9537715708165835, 'errorList': [0.060624045455340794, 0.09581540522215028, 0.15465923348237304, 0.10801330936340085, 0.17241139209964607, 0.2034198636762227, 0.11608520188477152, 0.07553458406493793, 0.1761037019793441, 0.07001816604973403, 0.08558567476588581, 0.03982176070370633, 0.03543097656044661, 0.04738708636498851, 0.14620683756741215, 0.06832980884105529, 0.02734026721279045, 0.07314076194855829, 0.11499507459420079, 0.11334544570162026, 0.17771179378116353, 0.11515406458192724, 0.09243340687571996, 0.16832740172053656, 0.18257311196517484, 0.0944555392675338, 0.024371116292233087, 0.04675566972466537, 0.03486623094486024, 0.028286798299475075, 0.23936325198058878, 0.06043173691623151, 0.134282688176855, 0.07483919745343857, 0.16119037571448694, 0.07253222973376505, 0.21631088997529954, 0.09771638661634997, 0.053869259652913865, 0.11752991920974243, 0.166650258257132, 0.16330434262818086, 0.10177695644370341, 0.09267426762036284, 0.16237876517146038, 0.023560398407807177, 0.14316523489824687, 0.04400180540506748, 0.12057848247069751, 0.04199179747566925], 'lossList': [0.0, -1.3043733495473862, 0.0, 1.260613507181406, 0.0, 0.0, 0.0], 'rewardMean': 0.7216422410045178, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1162.295976789541, 'successfulTests': 47
'totalSteps': 34560, 'rewardStep': 0.9693681305047077, 'errorList': [0.09085208953895575, 0.0660095655632295, 0.04099777027742953, 0.04119583942695319, 0.05733036887169907, 0.06035790654664037, 0.07114583007754391, 0.036971407975396615, 0.02220641512109501, 0.12193101221738814, 0.03734177613543798, 0.04701533016422578, 0.019559039247921104, 0.07655835747623237, 0.02202729070001232, 0.08711353718493658, 0.034086079195069514, 0.023267632425034292, 0.0791281553984243, 0.04320844582514359, 0.14326371784661626, 0.03599582820378851, 0.021611882685604572, 0.02336084612065532, 0.027024582337861987, 0.07757816501578349, 0.032490712301040635, 0.0823105299995616, 0.09137130264789427, 0.03385629513969941, 0.02766524130427042, 0.0729857479123287, 0.08399617367212354, 0.03545038436915205, 0.01716287521411174, 0.058675081911027396, 0.038114064286818565, 0.07236108039334883, 0.03761054366077265, 0.09877475172174272, 0.03545059136365013, 0.09171259387484083, 0.04634017709936671, 0.05235269419267736, 0.06488660181783693, 0.08507535643207033, 0.05100188521258921, 0.09703008078771526, 0.053560176611231185, 0.04656314163704161], 'lossList': [0.0, -1.286722104549408, 0.0, 0.6712887555733323, 0.0, 0.0, 0.0], 'rewardMean': 0.758144197826926, 'totalEpisodes': 247, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1157.7835168565732, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=34560, timeSpent=102.34
