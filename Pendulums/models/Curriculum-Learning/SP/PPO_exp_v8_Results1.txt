#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.1, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8547283221549538, 'errorList': [], 'lossList': [0.0, -1.4215758085250854, 0.0, 84.47800897598266, 0.0, 0.0, 0.0], 'rewardMean': 0.8547283221549538, 'totalEpisodes': 5, 'stepsPerEpisode': 113, 'rewardPerEpisode': 96.08251935003153
'totalSteps': 2560, 'rewardStep': 0.7920734909843146, 'errorList': [], 'lossList': [0.0, -1.4374132996797562, 0.0, 31.761003717184067, 0.0, 0.0, 0.0], 'rewardMean': 0.8234009065696342, 'totalEpisodes': 8, 'stepsPerEpisode': 61, 'rewardPerEpisode': 52.79219335677483
'totalSteps': 3840, 'rewardStep': 0.7490727849173608, 'errorList': [], 'lossList': [0.0, -1.4458949589729309, 0.0, 26.776501106023787, 0.0, 0.0, 0.0], 'rewardMean': 0.7986248660188764, 'totalEpisodes': 10, 'stepsPerEpisode': 546, 'rewardPerEpisode': 352.33598575932194
'totalSteps': 5120, 'rewardStep': 0.8223476585308873, 'errorList': [], 'lossList': [0.0, -1.444164354801178, 0.0, 40.35998079776764, 0.0, 0.0, 0.0], 'rewardMean': 0.8045555641468791, 'totalEpisodes': 15, 'stepsPerEpisode': 52, 'rewardPerEpisode': 39.54138065223168
'totalSteps': 6400, 'rewardStep': 0.5545504596751397, 'errorList': [], 'lossList': [0.0, -1.4460889023542405, 0.0, 18.925491478443146, 0.0, 0.0, 0.0], 'rewardMean': 0.7545545432525312, 'totalEpisodes': 16, 'stepsPerEpisode': 383, 'rewardPerEpisode': 277.1265395483473
'totalSteps': 7680, 'rewardStep': 0.7620383691643244, 'errorList': [], 'lossList': [0.0, -1.430043671131134, 0.0, 17.115057026743887, 0.0, 0.0, 0.0], 'rewardMean': 0.7558018475711634, 'totalEpisodes': 16, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1078.3884854165692
'totalSteps': 8960, 'rewardStep': 0.7132533910334395, 'errorList': [], 'lossList': [0.0, -1.398321075439453, 0.0, 229.61070449829103, 0.0, 0.0, 0.0], 'rewardMean': 0.7497234966372028, 'totalEpisodes': 38, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.1262862076590636
'totalSteps': 10240, 'rewardStep': 0.9133553031534658, 'errorList': [], 'lossList': [0.0, -1.3947019332647324, 0.0, 197.23354866027833, 0.0, 0.0, 0.0], 'rewardMean': 0.7701774724517357, 'totalEpisodes': 68, 'stepsPerEpisode': 56, 'rewardPerEpisode': 49.96346901614531
'totalSteps': 11520, 'rewardStep': 0.7501773134021507, 'errorList': [], 'lossList': [0.0, -1.3943002015352248, 0.0, 148.6079423522949, 0.0, 0.0, 0.0], 'rewardMean': 0.7679552325573373, 'totalEpisodes': 105, 'stepsPerEpisode': 15, 'rewardPerEpisode': 8.621715106175365
'totalSteps': 12800, 'rewardStep': 0.7975112157176274, 'errorList': [], 'lossList': [0.0, -1.388041426539421, 0.0, 76.29064918518067, 0.0, 0.0, 0.0], 'rewardMean': 0.7709108308733663, 'totalEpisodes': 135, 'stepsPerEpisode': 50, 'rewardPerEpisode': 40.210942288312985
'totalSteps': 14080, 'rewardStep': 0.7911760721649235, 'errorList': [], 'lossList': [0.0, -1.3797382354736327, 0.0, 42.305356187820436, 0.0, 0.0, 0.0], 'rewardMean': 0.7645556058743633, 'totalEpisodes': 150, 'stepsPerEpisode': 13, 'rewardPerEpisode': 8.852793812849292
'totalSteps': 15360, 'rewardStep': 0.4695453203363587, 'errorList': [], 'lossList': [0.0, -1.3766599017381669, 0.0, 17.879992158412932, 0.0, 0.0, 0.0], 'rewardMean': 0.7323027888095677, 'totalEpisodes': 157, 'stepsPerEpisode': 111, 'rewardPerEpisode': 74.59379833511386
'totalSteps': 16640, 'rewardStep': 0.9459839878392569, 'errorList': [57.60723285461968, 48.02334651699526, 3.460686947763193, 16.935036229106352, 1.3186473761343909, 3.850907987637933, 8.917752981221494, 21.917843684276285, 27.206638888419036, 26.09065641344222, 21.765468139146098, 47.3750113250192, 33.91633927150007, 65.13366299933689, 36.99765100202569, 9.832134858831761, 10.844370480385964, 57.88911895442424, 15.094875838341922, 45.492314552882995, 3.9001809931984575, 19.52599218301184, 9.954352703302256, 31.796490621983033, 42.25383027209867, 25.71794619481334, 19.475870778212954, 23.488007653285315, 34.396791655378664, 34.65338353855852, 4.1530331710328445, 19.934809524782725, 39.49357584302635, 42.493719414708885, 4.6807349780875285, 29.311289694975297, 54.193598352584836, 40.255200137952606, 36.5784349034927, 63.24810999172298, 22.87383848313186, 27.639598158255787, 52.3753799865714, 1.4873985216936865, 16.461629258085715, 17.069931865146618, 41.64926174382923, 3.0190906190412976, 28.923572303833595, 45.19809946746416], 'lossList': [0.0, -1.3674180084466934, 0.0, 29.018815050125124, 0.0, 0.0, 0.0], 'rewardMean': 0.7519939091017573, 'totalEpisodes': 166, 'stepsPerEpisode': 71, 'rewardPerEpisode': 65.77594295925486, 'successfulTests': 0
'totalSteps': 17920, 'rewardStep': 0.4273728755890113, 'errorList': [], 'lossList': [0.0, -1.3444521605968476, 0.0, 20.387865023612974, 0.0, 0.0, 0.0], 'rewardMean': 0.7124964308075697, 'totalEpisodes': 172, 'stepsPerEpisode': 151, 'rewardPerEpisode': 106.24078324676532
'totalSteps': 19200, 'rewardStep': 0.8244682497973367, 'errorList': [], 'lossList': [0.0, -1.3271643155813218, 0.0, 6.9412005686759946, 0.0, 0.0, 0.0], 'rewardMean': 0.7394882098197895, 'totalEpisodes': 177, 'stepsPerEpisode': 83, 'rewardPerEpisode': 62.31161926902063
'totalSteps': 20480, 'rewardStep': 0.9145401556954599, 'errorList': [], 'lossList': [0.0, -1.313083820939064, 0.0, 4.5213789308071135, 0.0, 0.0, 0.0], 'rewardMean': 0.7547383884729031, 'totalEpisodes': 181, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.43812040180499
'totalSteps': 21760, 'rewardStep': 0.29277185021749297, 'errorList': [], 'lossList': [0.0, -1.2992959588766098, 0.0, 23.61257058262825, 0.0, 0.0, 0.0], 'rewardMean': 0.7126902343913083, 'totalEpisodes': 182, 'stepsPerEpisode': 822, 'rewardPerEpisode': 611.4459527340722
'totalSteps': 23040, 'rewardStep': 0.8257627025105698, 'errorList': [], 'lossList': [0.0, -1.2723838526010514, 0.0, 4.649125292301178, 0.0, 0.0, 0.0], 'rewardMean': 0.7039309743270188, 'totalEpisodes': 183, 'stepsPerEpisode': 689, 'rewardPerEpisode': 591.4646901428058
'totalSteps': 24320, 'rewardStep': 0.7044102697522205, 'errorList': [], 'lossList': [0.0, -1.24692063331604, 0.0, 3.0004811757802963, 0.0, 0.0, 0.0], 'rewardMean': 0.6993542699620258, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1044.3511795078964
'totalSteps': 25600, 'rewardStep': 0.9538900258457237, 'errorList': [0.07220773613959591, 0.07624989061398112, 0.07737168503260702, 0.08156918790939918, 0.06227146326565492, 0.06878154928810404, 0.06546194428663765, 0.0635233334781365, 0.06075784914500197, 0.0735425083796194, 0.06540517320741313, 0.06555713875175329, 0.06466553016698907, 0.0828485094842992, 0.06259268800217625, 0.06164447830954894, 0.059628930751348605, 0.07019616923630612, 0.06668337096848041, 0.09153864570375708, 0.07055843440273125, 0.06400599024968529, 0.08874868456209822, 0.06973766231025966, 0.06210681204493592, 0.07354421592908125, 0.06399573414567876, 0.07016452096666156, 0.0728470570764001, 0.07472279477648293, 0.06814562465488792, 0.06498699333792492, 0.08103852492533839, 0.06844784245779094, 0.06216777828573314, 0.06548179147173949, 0.0653311668532465, 0.05913107976383492, 0.07724766146710291, 0.09799437348474112, 0.0704126483789654, 0.06599648577450554, 0.0705848997229642, 0.06118840710147135, 0.06229949506429178, 0.06316962902406566, 0.059276913681130694, 0.06646138274587796, 0.06827745244963236, 0.06766781392272915], 'lossList': [0.0, -1.2038500130176544, 0.0, 1.9055388075113298, 0.0, 0.0, 0.0], 'rewardMean': 0.7149921509748355, 'totalEpisodes': 183, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1121.350000480295, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=76.23
