#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.1, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8895777764077926, 'errorList': [], 'lossList': [0.0, -1.4293801921606064, 0.0, 89.97683355808257, 0.0, 0.0, 0.0], 'rewardMean': 0.8895777764077926, 'totalEpisodes': 5, 'stepsPerEpisode': 254, 'rewardPerEpisode': 216.92817778130882
'totalSteps': 2560, 'rewardStep': 0.6022098824757339, 'errorList': [], 'lossList': [0.0, -1.4440815454721452, 0.0, 28.55058839440346, 0.0, 0.0, 0.0], 'rewardMean': 0.7458938294417632, 'totalEpisodes': 9, 'stepsPerEpisode': 277, 'rewardPerEpisode': 218.87314479132505
'totalSteps': 3840, 'rewardStep': 0.7147785543390233, 'errorList': [], 'lossList': [0.0, -1.4488423955440521, 0.0, 33.86644857168198, 0.0, 0.0, 0.0], 'rewardMean': 0.7355220710741831, 'totalEpisodes': 12, 'stepsPerEpisode': 494, 'rewardPerEpisode': 384.7618819340059
'totalSteps': 5120, 'rewardStep': 0.5046008414359395, 'errorList': [], 'lossList': [0.0, -1.4306613683700562, 0.0, 13.979012662768364, 0.0, 0.0, 0.0], 'rewardMean': 0.6777917636646222, 'totalEpisodes': 12, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 872.9857197270127
'totalSteps': 6400, 'rewardStep': 0.42615841163806384, 'errorList': [], 'lossList': [0.0, -1.435302963256836, 0.0, 28.942129940986632, 0.0, 0.0, 0.0], 'rewardMean': 0.6274650932593105, 'totalEpisodes': 13, 'stepsPerEpisode': 558, 'rewardPerEpisode': 427.56122594487584
'totalSteps': 7680, 'rewardStep': 0.7461249993054462, 'errorList': [], 'lossList': [0.0, -1.4226037329435348, 0.0, 20.687570090293885, 0.0, 0.0, 0.0], 'rewardMean': 0.6472417442669998, 'totalEpisodes': 14, 'stepsPerEpisode': 811, 'rewardPerEpisode': 655.8624116863009
'totalSteps': 8960, 'rewardStep': 0.8018218521946336, 'errorList': [], 'lossList': [0.0, -1.4141579180955888, 0.0, 211.16822914123534, 0.0, 0.0, 0.0], 'rewardMean': 0.6693246168280903, 'totalEpisodes': 32, 'stepsPerEpisode': 18, 'rewardPerEpisode': 15.9850039249033
'totalSteps': 10240, 'rewardStep': 0.6268911302332959, 'errorList': [], 'lossList': [0.0, -1.4133329164981843, 0.0, 212.88578567504882, 0.0, 0.0, 0.0], 'rewardMean': 0.6640204310037411, 'totalEpisodes': 69, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.6268911302332959
'totalSteps': 11520, 'rewardStep': 0.6310193293593226, 'errorList': [], 'lossList': [0.0, -1.4079005056619645, 0.0, 56.67433783531189, 0.0, 0.0, 0.0], 'rewardMean': 0.660353641932139, 'totalEpisodes': 107, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.0706649425157484
'totalSteps': 12800, 'rewardStep': 0.9476503280655779, 'errorList': [292.55224007190884, 293.2919993426154, 273.6388085263956, 268.0350011141877, 229.98768027497047, 279.98982813134614, 250.73086413385334, 248.8955952203065, 270.07263829821727, 241.86189218073798, 263.84905664261345, 307.3871707871574, 270.1326218943537, 288.580486926493, 232.58204689276135, 244.57963698654055, 282.63501067391667, 299.57966316958573, 211.89461628799708, 303.56442316154056, 213.7583229502212, 270.08458528984517, 286.25470645028673, 277.0515924864161, 269.1477334077287, 276.08100434627346, 291.49826429521937, 222.65352769482084, 268.4541415766221, 216.1168921025812, 284.5389302915993, 259.33788295013073, 292.6385748362424, 219.74375428990177, 249.08476763020238, 290.7676632660303, 241.6337601416014, 287.74905980291993, 283.4927531522256, 276.8424831376236, 249.44244425622705, 252.82764028381655, 229.92122723935591, 274.6761577256667, 216.2671519464983, 247.42174857993425, 267.2169706714224, 271.74555604397864, 285.40728077213925, 214.63872358306563], 'lossList': [0.0, -1.398685666322708, 0.0, 31.70148723125458, 0.0, 0.0, 0.0], 'rewardMean': 0.6890833105454829, 'totalEpisodes': 141, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.879691479677313, 'successfulTests': 0
'totalSteps': 14080, 'rewardStep': 0.9664646344875099, 'errorList': [195.76280203502992, 230.94404406467405, 237.16382993026198, 219.29068332517133, 201.1293720504172, 189.9593408256766, 189.64913909478983, 208.65179789526192, 259.55279933476834, 206.13446202806142, 223.53528417740614, 232.4600740565387, 224.61668952931393, 231.39016606515492, 202.66112581193022, 232.85978721906844, 245.698652642891, 231.8671771694547, 233.210218959919, 278.4080194930811, 268.023023908613, 214.4813306576942, 259.1202475196059, 177.12283964267792, 265.4421735845192, 199.32554510701925, 266.9145901542365, 219.11356486481264, 242.00049849660078, 255.4810366562283, 231.89961192041372, 179.92561371985474, 231.24566224744723, 224.41264182723228, 231.35424660977463, 149.54782196052022, 210.62621514406922, 249.30282467376966, 213.1434476456884, 222.63379110624928, 226.32851805739082, 236.80786088914874, 109.06461682647435, 237.63089854633944, 187.74239068793813, 156.13650727528827, 246.48895045244743, 182.3286721453274, 216.24587310597767, 278.14233237095976], 'lossList': [0.0, -1.3839854717254638, 0.0, 21.000467586517335, 0.0, 0.0, 0.0], 'rewardMean': 0.6967719963534547, 'totalEpisodes': 162, 'stepsPerEpisode': 31, 'rewardPerEpisode': 26.03938441330014, 'successfulTests': 0
'totalSteps': 15360, 'rewardStep': 0.9070654149594396, 'errorList': [], 'lossList': [0.0, -1.3758544170856475, 0.0, 23.90516816139221, 0.0, 0.0, 0.0], 'rewardMean': 0.7272575496018252, 'totalEpisodes': 176, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.536973371155383
'totalSteps': 16640, 'rewardStep': 0.9124032172684964, 'errorList': [], 'lossList': [0.0, -1.3701522850990295, 0.0, 10.526425412893296, 0.0, 0.0, 0.0], 'rewardMean': 0.7470200158947726, 'totalEpisodes': 184, 'stepsPerEpisode': 17, 'rewardPerEpisode': 16.15038925932708
'totalSteps': 17920, 'rewardStep': 0.7605808680734498, 'errorList': [], 'lossList': [0.0, -1.3388168758153915, 0.0, 9.374213583469391, 0.0, 0.0, 0.0], 'rewardMean': 0.7726180185585234, 'totalEpisodes': 190, 'stepsPerEpisode': 54, 'rewardPerEpisode': 44.49592384872907
'totalSteps': 19200, 'rewardStep': 0.8436025142530013, 'errorList': [], 'lossList': [0.0, -1.3319103717803955, 0.0, 9.865535368919373, 0.0, 0.0, 0.0], 'rewardMean': 0.8143624288200172, 'totalEpisodes': 195, 'stepsPerEpisode': 98, 'rewardPerEpisode': 79.91357222811922
'totalSteps': 20480, 'rewardStep': 0.5953984175923648, 'errorList': [], 'lossList': [0.0, -1.3409255015850068, 0.0, 5.765843344926834, 0.0, 0.0, 0.0], 'rewardMean': 0.7992897706487091, 'totalEpisodes': 200, 'stepsPerEpisode': 170, 'rewardPerEpisode': 131.89027158486093
'totalSteps': 21760, 'rewardStep': 0.7122140616322123, 'errorList': [], 'lossList': [0.0, -1.3237334406375885, 0.0, 4.882679409980774, 0.0, 0.0, 0.0], 'rewardMean': 0.7903289915924671, 'totalEpisodes': 206, 'stepsPerEpisode': 73, 'rewardPerEpisode': 53.52519076274392
'totalSteps': 23040, 'rewardStep': 0.8769955695881139, 'errorList': [], 'lossList': [0.0, -1.3067122054100038, 0.0, 4.147116019725799, 0.0, 0.0, 0.0], 'rewardMean': 0.8153394355279489, 'totalEpisodes': 209, 'stepsPerEpisode': 325, 'rewardPerEpisode': 278.3288754950524
'totalSteps': 24320, 'rewardStep': 0.43732824967893436, 'errorList': [], 'lossList': [0.0, -1.3126398146152496, 0.0, 3.930632325410843, 0.0, 0.0, 0.0], 'rewardMean': 0.79597032755991, 'totalEpisodes': 210, 'stepsPerEpisode': 972, 'rewardPerEpisode': 803.8534031658467
'totalSteps': 25600, 'rewardStep': 0.7427809959162195, 'errorList': [], 'lossList': [0.0, -1.2883406901359558, 0.0, 2.0872705575823782, 0.0, 0.0, 0.0], 'rewardMean': 0.7754833943449742, 'totalEpisodes': 211, 'stepsPerEpisode': 1254, 'rewardPerEpisode': 1055.1209763461347
'totalSteps': 26880, 'rewardStep': 0.9334850737629706, 'errorList': [0.046362897875774474, 0.04803855984851316, 0.05158433929251513, 0.053329474747123747, 0.048897037266914085, 0.05354555006179029, 0.05191087393852389, 0.04400164188829545, 0.04414824975548991, 0.053875025285878, 0.04761357287698076, 0.04903872775674945, 0.04487050638919501, 0.05017760020935133, 0.053027678718815204, 0.04520871149173218, 0.04834102245829352, 0.04396850601416693, 0.04794019574449555, 0.04419859417719561, 0.04880432940753888, 0.044533862757736783, 0.050039599293732015, 0.0440611153215504, 0.04582294169703146, 0.04511962176730214, 0.05076516959232878, 0.043904234019608676, 0.04927231164638946, 0.055361163876968954, 0.044126102861203796, 0.048469213907776865, 0.049480640174171905, 0.05387109606794051, 0.04738144044674939, 0.05260743680758172, 0.04424899014546197, 0.04711953049045306, 0.04687849909026169, 0.044164768081329644, 0.04404603235176136, 0.04868821099803234, 0.043951833376915536, 0.04407358805668349, 0.0455334591468424, 0.04427583083547026, 0.04593225765511833, 0.05278430440717185, 0.044835376346368715, 0.05259987671760018], 'lossList': [0.0, -1.2283160817623138, 0.0, 1.7133694518357516, 0.0, 0.0, 0.0], 'rewardMean': 0.7721854382725203, 'totalEpisodes': 211, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1147.0915116130573, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=26880, timeSpent=93.41
