#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.1, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6692756425376761, 'errorList': [], 'lossList': [0.0, -1.4342697912454605, 0.0, 90.17581271648407, 0.0, 0.0, 0.0], 'rewardMean': 0.6692756425376761, 'totalEpisodes': 6, 'stepsPerEpisode': 198, 'rewardPerEpisode': 145.58395154240813
'totalSteps': 2560, 'rewardStep': 0.7543439765532742, 'errorList': [], 'lossList': [0.0, -1.4375670439004897, 0.0, 31.762581284046174, 0.0, 0.0, 0.0], 'rewardMean': 0.7118098095454752, 'totalEpisodes': 9, 'stepsPerEpisode': 34, 'rewardPerEpisode': 27.70561730713184
'totalSteps': 3840, 'rewardStep': 0.7214258028451401, 'errorList': [], 'lossList': [0.0, -1.4219777566194534, 0.0, 25.05196123480797, 0.0, 0.0, 0.0], 'rewardMean': 0.7150151406453635, 'totalEpisodes': 13, 'stepsPerEpisode': 175, 'rewardPerEpisode': 124.89787131641609
'totalSteps': 5120, 'rewardStep': 0.8370906883107004, 'errorList': [], 'lossList': [0.0, -1.4250906419754028, 0.0, 18.99498005390167, 0.0, 0.0, 0.0], 'rewardMean': 0.7455340275616977, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 947.5298239343921
'totalSteps': 6400, 'rewardStep': 0.6605136524376436, 'errorList': [], 'lossList': [0.0, -1.4038800722360611, 0.0, 17.490183926820755, 0.0, 0.0, 0.0], 'rewardMean': 0.7285299525368869, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 992.334721415371
'totalSteps': 7680, 'rewardStep': 0.8302594619733568, 'errorList': [], 'lossList': [0.0, -1.3924477702379228, 0.0, 13.93420553892851, 0.0, 0.0, 0.0], 'rewardMean': 0.7454848707762984, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1039.8249338982555
'totalSteps': 8960, 'rewardStep': 0.7724554084826324, 'errorList': [], 'lossList': [0.0, -1.3747379845380783, 0.0, 245.6469501876831, 0.0, 0.0, 0.0], 'rewardMean': 0.7493378047343462, 'totalEpisodes': 32, 'stepsPerEpisode': 38, 'rewardPerEpisode': 27.2067665557323
'totalSteps': 10240, 'rewardStep': 0.6418498950893825, 'errorList': [], 'lossList': [0.0, -1.3700979483127593, 0.0, 209.10883724212647, 0.0, 0.0, 0.0], 'rewardMean': 0.7359018160287257, 'totalEpisodes': 69, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.254955366881815
'totalSteps': 11520, 'rewardStep': 0.5330323218082548, 'errorList': [], 'lossList': [0.0, -1.36520565867424, 0.0, 79.61561489105225, 0.0, 0.0, 0.0], 'rewardMean': 0.7133607611153401, 'totalEpisodes': 100, 'stepsPerEpisode': 10, 'rewardPerEpisode': 5.330114019078979
'totalSteps': 12800, 'rewardStep': 0.7598668419416167, 'errorList': [], 'lossList': [0.0, -1.36138489484787, 0.0, 35.85612463951111, 0.0, 0.0, 0.0], 'rewardMean': 0.7180113691979677, 'totalEpisodes': 127, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.063831952570824
'totalSteps': 14080, 'rewardStep': 0.658348373715421, 'errorList': [], 'lossList': [0.0, -1.357825640439987, 0.0, 32.37836801052094, 0.0, 0.0, 0.0], 'rewardMean': 0.7169186423157422, 'totalEpisodes': 148, 'stepsPerEpisode': 29, 'rewardPerEpisode': 19.9771623977593
'totalSteps': 15360, 'rewardStep': 0.5749334125188575, 'errorList': [], 'lossList': [0.0, -1.3531569415330886, 0.0, 25.549169359207152, 0.0, 0.0, 0.0], 'rewardMean': 0.6989775859123005, 'totalEpisodes': 156, 'stepsPerEpisode': 11, 'rewardPerEpisode': 6.287437635408475
'totalSteps': 16640, 'rewardStep': 0.7358981770720012, 'errorList': [], 'lossList': [0.0, -1.3502612048387528, 0.0, 15.036390233039857, 0.0, 0.0, 0.0], 'rewardMean': 0.7004248233349867, 'totalEpisodes': 158, 'stepsPerEpisode': 369, 'rewardPerEpisode': 239.58949330534725
'totalSteps': 17920, 'rewardStep': 0.890022805364483, 'errorList': [], 'lossList': [0.0, -1.3242845618724823, 0.0, 9.004473505020142, 0.0, 0.0, 0.0], 'rewardMean': 0.7057180350403649, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 983.4525199792384
'totalSteps': 19200, 'rewardStep': 0.7969348357011962, 'errorList': [], 'lossList': [0.0, -1.2789072650671005, 0.0, 7.023437717854977, 0.0, 0.0, 0.0], 'rewardMean': 0.7193601533667202, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1018.807190907234
'totalSteps': 20480, 'rewardStep': 0.9652054618677031, 'errorList': [0.48744176113294413, 0.4927901965033012, 0.48739623122179176, 0.4951984610681454, 0.4912786938297509, 0.4882948123398466, 0.48792708747402913, 0.4951594043482463, 0.49628803214401324, 0.4968107211092251, 0.487839666233348, 0.5063330681274915, 0.5320818883469904, 0.48701532891825144, 0.5176307567450608, 0.49375871897190093, 0.4985607325211448, 0.4891004834830859, 0.48828622424541784, 0.4831824897128515, 0.49385440711222955, 0.4859121532375866, 0.48586295397559953, 0.5032178554212161, 0.4945286939206621, 0.4819449652984, 0.48386848651081477, 0.500565504383909, 0.4945481046349485, 0.4834088643036345, 0.49781045609582225, 0.48918693128391666, 0.497507061611, 0.49462746576140987, 0.4864033069586784, 0.5050866134305836, 0.489484866869405, 0.5097280079851559, 0.4884699786475725, 0.48764922299876207, 0.48721423847729817, 0.48919481562909567, 0.49221139157702204, 0.4875462425021477, 0.4903585973263494, 0.49230564004923316, 0.4996020127373556, 0.4880249954072298, 0.48716496632899814, 0.5056884209460969], 'lossList': [0.0, -1.2458164763450623, 0.0, 6.954139328151942, 0.0, 0.0, 0.0], 'rewardMean': 0.7328547533561548, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1091.0668028604305, 'successfulTests': 0
'totalSteps': 21760, 'rewardStep': 0.784995121414984, 'errorList': [], 'lossList': [0.0, -1.2175628346204759, 0.0, 1.9479528218507767, 0.0, 0.0, 0.0], 'rewardMean': 0.73410872464939, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 921.0057624449107
'totalSteps': 23040, 'rewardStep': 0.7052547682325789, 'errorList': [], 'lossList': [0.0, -1.2123848474025727, 0.0, 1.3496188992261886, 0.0, 0.0, 0.0], 'rewardMean': 0.7404492119637097, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1021.3837700582623
'totalSteps': 24320, 'rewardStep': 0.7899282214433025, 'errorList': [], 'lossList': [0.0, -1.1731666749715806, 0.0, 2.6908357647806405, 0.0, 0.0, 0.0], 'rewardMean': 0.7661388019272144, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1117.6525344999088
'totalSteps': 25600, 'rewardStep': 0.9477746553032973, 'errorList': [0.04099330955530772, 0.03846283142937734, 0.048709734879264215, 0.027838277984116987, 0.0782811213784717, 0.0443113419938318, 0.0905042281247545, 0.036361193600245256, 0.046912057285885385, 0.028828048730134073, 0.10442867862797983, 0.034932304180166705, 0.03658487827181238, 0.04605438037878448, 0.09413564808550846, 0.016297437230929294, 0.04771044850261098, 0.01958966813957534, 0.07072093680377835, 0.0972129551733869, 0.036108586552540005, 0.13498981153282152, 0.030081380052721682, 0.04511350326635403, 0.08668563789353251, 0.04861855710048004, 0.0513422641560985, 0.08455068613949664, 0.054387179276819304, 0.0881134085858213, 0.08862739254100804, 0.03605182287821393, 0.053984029963502204, 0.019399227492442987, 0.02855552543298122, 0.060987062500346495, 0.10433770858939762, 0.01873164366147566, 0.04820747339100629, 0.0881196382981888, 0.03219315038747532, 0.045761957029926276, 0.0942197230053989, 0.04879584146115176, 0.02647080798604277, 0.016069232912035457, 0.07754934721298788, 0.0434226604178948, 0.06617478761145491, 0.05286286547891672], 'lossList': [0.0, -1.1486012881994248, 0.0, 2.0461331046000124, 0.0, 0.0, 0.0], 'rewardMean': 0.7849295832633825, 'totalEpisodes': 158, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1159.7761894438636, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=76.48
