#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v8_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.1, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.6108126788388039, 'errorList': [], 'lossList': [0.0, -1.4297511976957322, 0.0, 91.60471320152283, 0.0, 0.0, 0.0], 'rewardMean': 0.6108126788388039, 'totalEpisodes': 6, 'stepsPerEpisode': 162, 'rewardPerEpisode': 124.08033664013776
'totalSteps': 2560, 'rewardStep': 0.7615100213187631, 'errorList': [], 'lossList': [0.0, -1.431296461224556, 0.0, 25.03360767841339, 0.0, 0.0, 0.0], 'rewardMean': 0.6861613500787835, 'totalEpisodes': 8, 'stepsPerEpisode': 1079, 'rewardPerEpisode': 721.8838649678354
'totalSteps': 3840, 'rewardStep': 0.686000489330449, 'errorList': [], 'lossList': [0.0, -1.4217183089256287, 0.0, 29.911885565519334, 0.0, 0.0, 0.0], 'rewardMean': 0.6861077298293387, 'totalEpisodes': 11, 'stepsPerEpisode': 157, 'rewardPerEpisode': 113.12815618510284
'totalSteps': 5120, 'rewardStep': 0.7005559223713718, 'errorList': [], 'lossList': [0.0, -1.4225225889682769, 0.0, 26.570095643401146, 0.0, 0.0, 0.0], 'rewardMean': 0.689719777964847, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.0999430211816
'totalSteps': 6400, 'rewardStep': 0.827490154963226, 'errorList': [], 'lossList': [0.0, -1.407121897339821, 0.0, 20.04038020670414, 0.0, 0.0, 0.0], 'rewardMean': 0.7172738533645228, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1034.6921787886602
'totalSteps': 7680, 'rewardStep': 0.840524213249037, 'errorList': [], 'lossList': [0.0, -1.4084030336141586, 0.0, 10.53748271226883, 0.0, 0.0, 0.0], 'rewardMean': 0.7378155800119418, 'totalEpisodes': 11, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 974.1928716684947
'totalSteps': 8960, 'rewardStep': 0.5027462252912593, 'errorList': [], 'lossList': [0.0, -1.3930916112661362, 0.0, 299.30556999206544, 0.0, 0.0, 0.0], 'rewardMean': 0.7042342436232729, 'totalEpisodes': 37, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.0967385259915075
'totalSteps': 10240, 'rewardStep': 0.6293331808637669, 'errorList': [], 'lossList': [0.0, -1.3911105805635453, 0.0, 253.80759750366212, 0.0, 0.0, 0.0], 'rewardMean': 0.6948716107783346, 'totalEpisodes': 82, 'stepsPerEpisode': 25, 'rewardPerEpisode': 18.484913213779073
'totalSteps': 11520, 'rewardStep': 0.8419929363056647, 'errorList': [], 'lossList': [0.0, -1.389432224035263, 0.0, 88.81594383239747, 0.0, 0.0, 0.0], 'rewardMean': 0.7112184247258156, 'totalEpisodes': 121, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.376673858314028
'totalSteps': 12800, 'rewardStep': 0.4313946370879318, 'errorList': [], 'lossList': [0.0, -1.383099946975708, 0.0, 66.8800400352478, 0.0, 0.0, 0.0], 'rewardMean': 0.6832360459620272, 'totalEpisodes': 157, 'stepsPerEpisode': 54, 'rewardPerEpisode': 35.1705046181224
'totalSteps': 14080, 'rewardStep': 0.46609976533750785, 'errorList': [], 'lossList': [0.0, -1.3779928088188171, 0.0, 37.63454953193664, 0.0, 0.0, 0.0], 'rewardMean': 0.6687647546118977, 'totalEpisodes': 182, 'stepsPerEpisode': 44, 'rewardPerEpisode': 28.414061664805175
'totalSteps': 15360, 'rewardStep': 0.9677177168552318, 'errorList': [193.3018983672801, 198.47526949477967, 162.81403029357804, 123.24627819281712, 143.2359706029603, 178.58364633261996, 188.93417516137956, 192.96937277960748, 206.4140731171722, 191.80054224996616, 191.5095000034584, 96.72578686815199, 97.47483166835234, 161.69316975834658, 204.26997706731802, 109.36485900230349, 204.87572366394934, 121.36763291728364, 171.59285819841747, 146.45528841847562, 172.0106889971048, 107.9508397002947, 86.4725836192809, 202.98046621598633, 159.4259118114289, 61.26571485746705, 114.69304033534402, 178.97864523800695, 71.67907042159457, 197.8028233210841, 162.1370987776131, 212.70536143166362, 112.34757976900927, 191.5186592648699, 212.16248538195012, 209.427174419918, 150.92335592445497, 85.67791642386784, 188.1355843940102, 128.81259838538398, 144.65853721249948, 144.58774554406378, 134.83258168797929, 171.90166241605507, 133.6680728784901, 169.41330191476538, 189.8133965015656, 219.22401436982878, 103.50523598204317, 163.35137951558556], 'lossList': [0.0, -1.378121668100357, 0.0, 36.345763053894046, 0.0, 0.0, 0.0], 'rewardMean': 0.6893855241655445, 'totalEpisodes': 200, 'stepsPerEpisode': 69, 'rewardPerEpisode': 63.63601079611843, 'successfulTests': 0
'totalSteps': 16640, 'rewardStep': 0.7118388645912105, 'errorList': [], 'lossList': [0.0, -1.3755832368135452, 0.0, 25.509045629501344, 0.0, 0.0, 0.0], 'rewardMean': 0.6919693616916207, 'totalEpisodes': 211, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.008316094210726
'totalSteps': 17920, 'rewardStep': 0.33294231091779375, 'errorList': [], 'lossList': [0.0, -1.3718911451101303, 0.0, 13.85958091020584, 0.0, 0.0, 0.0], 'rewardMean': 0.6552080005462629, 'totalEpisodes': 217, 'stepsPerEpisode': 125, 'rewardPerEpisode': 89.60550304734718
'totalSteps': 19200, 'rewardStep': 0.9059768909266124, 'errorList': [], 'lossList': [0.0, -1.3685103803873062, 0.0, 18.612576229572298, 0.0, 0.0, 0.0], 'rewardMean': 0.6630566741426016, 'totalEpisodes': 225, 'stepsPerEpisode': 64, 'rewardPerEpisode': 55.55307363207407
'totalSteps': 20480, 'rewardStep': 0.923995777708824, 'errorList': [], 'lossList': [0.0, -1.3547890359163284, 0.0, 25.748873497247697, 0.0, 0.0, 0.0], 'rewardMean': 0.6714038305885802, 'totalEpisodes': 229, 'stepsPerEpisode': 163, 'rewardPerEpisode': 129.88541641035414
'totalSteps': 21760, 'rewardStep': 0.5571065672207111, 'errorList': [], 'lossList': [0.0, -1.3625354838371277, 0.0, 6.435385029315949, 0.0, 0.0, 0.0], 'rewardMean': 0.6768398647815255, 'totalEpisodes': 233, 'stepsPerEpisode': 284, 'rewardPerEpisode': 238.2990993409361
'totalSteps': 23040, 'rewardStep': 0.8089652440773345, 'errorList': [], 'lossList': [0.0, -1.3727654778957368, 0.0, 25.579383107423784, 0.0, 0.0, 0.0], 'rewardMean': 0.6948030711028823, 'totalEpisodes': 238, 'stepsPerEpisode': 57, 'rewardPerEpisode': 47.731840392523274
'totalSteps': 24320, 'rewardStep': 0.6884362964558076, 'errorList': [], 'lossList': [0.0, -1.3668277156352997, 0.0, 3.9608636552095415, 0.0, 0.0, 0.0], 'rewardMean': 0.6794474071178965, 'totalEpisodes': 240, 'stepsPerEpisode': 137, 'rewardPerEpisode': 115.19773672055341
'totalSteps': 25600, 'rewardStep': 0.8326516492999347, 'errorList': [], 'lossList': [0.0, -1.3541910618543624, 0.0, 3.781525073647499, 0.0, 0.0, 0.0], 'rewardMean': 0.7195731083390968, 'totalEpisodes': 241, 'stepsPerEpisode': 603, 'rewardPerEpisode': 521.8963024402664
'totalSteps': 26880, 'rewardStep': 0.7888628241526814, 'errorList': [], 'lossList': [0.0, -1.3381539964675904, 0.0, 2.943981050848961, 0.0, 0.0, 0.0], 'rewardMean': 0.7518494142206141, 'totalEpisodes': 241, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1082.3786867969022
'totalSteps': 28160, 'rewardStep': 0.778556107943634, 'errorList': [], 'lossList': [0.0, -1.313991087079048, 0.0, 2.009758992344141, 0.0, 0.0, 0.0], 'rewardMean': 0.7329332533294544, 'totalEpisodes': 241, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1112.6560053473606
'totalSteps': 29440, 'rewardStep': 0.9856385726438441, 'errorList': [0.059341304696886955, 0.049750862212852125, 0.049696641158609486, 0.04236084681949463, 0.04320377430932213, 0.043017022263452366, 0.043330208278207785, 0.04939265922104078, 0.049255758012338724, 0.042197830606519646, 0.043302125345077706, 0.051789077743791295, 0.05825477624475442, 0.044848075443815236, 0.04495796592011523, 0.05697709780043363, 0.042111713614035505, 0.049341884165286544, 0.05272027936941633, 0.05616309301961787, 0.0504121117576773, 0.04197518623261319, 0.05325313480690865, 0.052488555231641046, 0.054012358423663163, 0.04691936205048918, 0.04972988238392597, 0.04234291972893038, 0.06065316015016523, 0.04822455686508451, 0.05163829281381589, 0.05485858913097023, 0.04182019481496192, 0.04795368792908578, 0.054371393170756684, 0.05143283087618468, 0.049038567874011896, 0.0511805931516568, 0.0445904304907387, 0.05016971169634069, 0.04418695280240018, 0.05169249551190709, 0.058179910601019756, 0.04695210747554317, 0.043216963391513756, 0.04893588110246285, 0.051943860778212864, 0.042696134897757296, 0.04323990642804256, 0.059697241751669836], 'lossList': [0.0, -1.2765867364406587, 0.0, 1.4158170659095048, 0.0, 0.0, 0.0], 'rewardMean': 0.7603132241347177, 'totalEpisodes': 241, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1141.23046331888, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=29440, timeSpent=81.19
