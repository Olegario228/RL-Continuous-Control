#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v9_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v9_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.025, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7760301006985493, 'errorList': [], 'lossList': [0.0, -1.4143394386768342, 0.0, 73.07291454792022, 0.0, 0.0, 0.0], 'rewardMean': 0.7760301006985493, 'totalEpisodes': 7, 'stepsPerEpisode': 235, 'rewardPerEpisode': 188.9836084357245
'totalSteps': 2560, 'rewardStep': 0.7303141484873269, 'errorList': [], 'lossList': [0.0, -1.4133539217710496, 0.0, 31.389601359367372, 0.0, 0.0, 0.0], 'rewardMean': 0.7531721245929381, 'totalEpisodes': 26, 'stepsPerEpisode': 119, 'rewardPerEpisode': 86.74383189274256
'totalSteps': 3840, 'rewardStep': 0.7414539288012914, 'errorList': [], 'lossList': [0.0, -1.4069097566604614, 0.0, 33.8563277721405, 0.0, 0.0, 0.0], 'rewardMean': 0.7492660593290559, 'totalEpisodes': 37, 'stepsPerEpisode': 17, 'rewardPerEpisode': 14.03799199959877
'totalSteps': 5120, 'rewardStep': 0.6076223501730477, 'errorList': [], 'lossList': [0.0, -1.3971196830272674, 0.0, 30.704017806053162, 0.0, 0.0, 0.0], 'rewardMean': 0.7138551320400538, 'totalEpisodes': 44, 'stepsPerEpisode': 258, 'rewardPerEpisode': 156.65801542553345
'totalSteps': 6400, 'rewardStep': 0.6172758544314559, 'errorList': [], 'lossList': [0.0, -1.3966998356580733, 0.0, 33.55022863864899, 0.0, 0.0, 0.0], 'rewardMean': 0.6945392765183342, 'totalEpisodes': 49, 'stepsPerEpisode': 16, 'rewardPerEpisode': 12.79067497594754
'totalSteps': 7680, 'rewardStep': 0.7551078911831792, 'errorList': [], 'lossList': [0.0, -1.3903225791454314, 0.0, 86.93167840957642, 0.0, 0.0, 0.0], 'rewardMean': 0.7046340456291418, 'totalEpisodes': 59, 'stepsPerEpisode': 56, 'rewardPerEpisode': 43.46174460702494
'totalSteps': 8960, 'rewardStep': 0.6509865295644381, 'errorList': [], 'lossList': [0.0, -1.3740272754430771, 0.0, 138.55525722503663, 0.0, 0.0, 0.0], 'rewardMean': 0.6969701147627555, 'totalEpisodes': 77, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2732602836615086
'totalSteps': 10240, 'rewardStep': 0.7280779591464677, 'errorList': [], 'lossList': [0.0, -1.3733954513072968, 0.0, 104.25097881317139, 0.0, 0.0, 0.0], 'rewardMean': 0.7008585953107196, 'totalEpisodes': 97, 'stepsPerEpisode': 34, 'rewardPerEpisode': 26.710085792023715
'totalSteps': 11520, 'rewardStep': 0.7373874892030079, 'errorList': [], 'lossList': [0.0, -1.3713543152809142, 0.0, 53.77302721977234, 0.0, 0.0, 0.0], 'rewardMean': 0.7049173612987516, 'totalEpisodes': 108, 'stepsPerEpisode': 226, 'rewardPerEpisode': 164.5321245887328
'totalSteps': 12800, 'rewardStep': 0.9268091245110931, 'errorList': [], 'lossList': [0.0, -1.358596412539482, 0.0, 40.36482852935791, 0.0, 0.0, 0.0], 'rewardMean': 0.7271065376199858, 'totalEpisodes': 113, 'stepsPerEpisode': 143, 'rewardPerEpisode': 117.4956000042576
'totalSteps': 14080, 'rewardStep': 0.7795028338878295, 'errorList': [], 'lossList': [0.0, -1.347650584578514, 0.0, 9.594948375225067, 0.0, 0.0, 0.0], 'rewardMean': 0.7274538109389138, 'totalEpisodes': 115, 'stepsPerEpisode': 74, 'rewardPerEpisode': 59.177359855967964
'totalSteps': 15360, 'rewardStep': 0.6644634783022675, 'errorList': [], 'lossList': [0.0, -1.352615583539009, 0.0, 8.766737389564515, 0.0, 0.0, 0.0], 'rewardMean': 0.7208687439204078, 'totalEpisodes': 116, 'stepsPerEpisode': 1027, 'rewardPerEpisode': 716.0135655630057
'totalSteps': 16640, 'rewardStep': 0.8188403790743358, 'errorList': [], 'lossList': [0.0, -1.3382008159160614, 0.0, 4.125635663866997, 0.0, 0.0, 0.0], 'rewardMean': 0.7286073889477123, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 942.8648935336649
'totalSteps': 17920, 'rewardStep': 0.8765615124138544, 'errorList': [], 'lossList': [0.0, -1.3064540886878968, 0.0, 4.992820102572441, 0.0, 0.0, 0.0], 'rewardMean': 0.7555013051717929, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1110.8854724019811
'totalSteps': 19200, 'rewardStep': 0.9244187987617825, 'errorList': [], 'lossList': [0.0, -1.2812653964757919, 0.0, 3.8312402079999446, 0.0, 0.0, 0.0], 'rewardMean': 0.7862155996048256, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1124.5634424050634
'totalSteps': 20480, 'rewardStep': 0.9956519381860383, 'errorList': [0.07258303216683017, 0.06391968409572309, 0.0781888190439509, 0.06592216195526546, 0.08247208198392149, 0.03688118202932086, 0.07053719547499912, 0.06330288247343342, 0.049012320238556464, 0.050806029500204776, 0.06528959840956934, 0.06364755687512651, 0.07280696954443495, 0.0368315867496302, 0.07428661961965242, 0.06864417070916348, 0.05765848205984526, 0.051967888851692255, 0.0654980334808201, 0.04493362058591343, 0.043591191105327325, 0.07669603262546788, 0.038987555039444684, 0.058162144205019535, 0.07854851046534955, 0.06423334505306445, 0.036296662631520316, 0.0863332053679483, 0.07389998658744831, 0.052298670571755836, 0.038279304167778944, 0.04306638302521985, 0.08969242854157237, 0.07663961619188499, 0.06368492124708076, 0.06187389243810329, 0.043765527187963685, 0.03893664123590135, 0.0653765603499691, 0.06864851942358259, 0.065555406041392, 0.048559872836128226, 0.06603281380648329, 0.04775208173917281, 0.07683139235676509, 0.038838752812435, 0.037723268583028755, 0.03746113025405957, 0.04503836603732464, 0.06337849418355608], 'lossList': [0.0, -1.2366322994232177, 0.0, 3.3274090330675246, 0.0, 0.0, 0.0], 'rewardMean': 0.8102700043051115, 'totalEpisodes': 116, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.5110557247792, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=54.34
