#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v9_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v9_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.025, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8129037665292386, 'errorList': [], 'lossList': [0.0, -1.4277315598726272, 0.0, 67.32247997283936, 0.0, 0.0, 0.0], 'rewardMean': 0.8129037665292386, 'totalEpisodes': 8, 'stepsPerEpisode': 230, 'rewardPerEpisode': 159.5923150595545
'totalSteps': 2560, 'rewardStep': 0.8150293380451613, 'errorList': [], 'lossList': [0.0, -1.4349777179956436, 0.0, 29.13724765777588, 0.0, 0.0, 0.0], 'rewardMean': 0.8139665522871999, 'totalEpisodes': 19, 'stepsPerEpisode': 33, 'rewardPerEpisode': 24.988321946887798
'totalSteps': 3840, 'rewardStep': 0.9397103015164279, 'errorList': [], 'lossList': [0.0, -1.4236400765180588, 0.0, 26.343409419059753, 0.0, 0.0, 0.0], 'rewardMean': 0.8558811353636093, 'totalEpisodes': 23, 'stepsPerEpisode': 379, 'rewardPerEpisode': 224.19533599129042
'totalSteps': 5120, 'rewardStep': 0.8281510046784096, 'errorList': [], 'lossList': [0.0, -1.4212653040885925, 0.0, 27.38193420410156, 0.0, 0.0, 0.0], 'rewardMean': 0.8489486026923094, 'totalEpisodes': 25, 'stepsPerEpisode': 180, 'rewardPerEpisode': 139.75614313600792
'totalSteps': 6400, 'rewardStep': 0.563227055522085, 'errorList': [], 'lossList': [0.0, -1.4130002892017364, 0.0, 15.385670313835144, 0.0, 0.0, 0.0], 'rewardMean': 0.7918042932582645, 'totalEpisodes': 26, 'stepsPerEpisode': 146, 'rewardPerEpisode': 116.80668911372784
'totalSteps': 7680, 'rewardStep': 0.9104994368030294, 'errorList': [], 'lossList': [0.0, -1.3935900127887726, 0.0, 58.48883284568787, 0.0, 0.0, 0.0], 'rewardMean': 0.811586817182392, 'totalEpisodes': 33, 'stepsPerEpisode': 65, 'rewardPerEpisode': 46.729217097753704
'totalSteps': 8960, 'rewardStep': 0.9949630169716391, 'errorList': [], 'lossList': [0.0, -1.3871470147371292, 0.0, 189.42270877838135, 0.0, 0.0, 0.0], 'rewardMean': 0.8377834171522844, 'totalEpisodes': 54, 'stepsPerEpisode': 105, 'rewardPerEpisode': 83.77783399982505
'totalSteps': 10240, 'rewardStep': 0.8337171384671623, 'errorList': [], 'lossList': [0.0, -1.3854632610082627, 0.0, 162.543995552063, 0.0, 0.0, 0.0], 'rewardMean': 0.8372751323166441, 'totalEpisodes': 85, 'stepsPerEpisode': 51, 'rewardPerEpisode': 45.758334137933474
'totalSteps': 11520, 'rewardStep': 0.7399657130782104, 'errorList': [], 'lossList': [0.0, -1.3793024694919587, 0.0, 55.69093517303467, 0.0, 0.0, 0.0], 'rewardMean': 0.8264629746234848, 'totalEpisodes': 102, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.570229956421186
'totalSteps': 12800, 'rewardStep': 0.36425410411577236, 'errorList': [], 'lossList': [0.0, -1.3770197558403015, 0.0, 25.4626628780365, 0.0, 0.0, 0.0], 'rewardMean': 0.7802420875727136, 'totalEpisodes': 110, 'stepsPerEpisode': 234, 'rewardPerEpisode': 174.34210390822844
'totalSteps': 14080, 'rewardStep': 0.6541953127154405, 'errorList': [], 'lossList': [0.0, -1.3833839452266694, 0.0, 9.897678413391112, 0.0, 0.0, 0.0], 'rewardMean': 0.7643712421913339, 'totalEpisodes': 114, 'stepsPerEpisode': 536, 'rewardPerEpisode': 357.666027067187
'totalSteps': 15360, 'rewardStep': 0.4736667705719877, 'errorList': [], 'lossList': [0.0, -1.3892270588874818, 0.0, 11.562142853736878, 0.0, 0.0, 0.0], 'rewardMean': 0.7302349854440164, 'totalEpisodes': 117, 'stepsPerEpisode': 541, 'rewardPerEpisode': 399.51984051359256
'totalSteps': 16640, 'rewardStep': 0.8386902423144562, 'errorList': [], 'lossList': [0.0, -1.3762285703420638, 0.0, 5.084098033905029, 0.0, 0.0, 0.0], 'rewardMean': 0.7201329795238193, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 871.1790728908229
'totalSteps': 17920, 'rewardStep': 0.7314690179447877, 'errorList': [], 'lossList': [0.0, -1.3434304070472718, 0.0, 6.460359859466553, 0.0, 0.0, 0.0], 'rewardMean': 0.7104647808504571, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 997.9234632634863
'totalSteps': 19200, 'rewardStep': 0.7311235333129675, 'errorList': [], 'lossList': [0.0, -1.3190786588191985, 0.0, 3.5106802874803544, 0.0, 0.0, 0.0], 'rewardMean': 0.7272544286295453, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 999.0769402730965
'totalSteps': 20480, 'rewardStep': 0.9651456175498565, 'errorList': [0.06518588131055146, 0.06560870514622973, 0.056569708169253254, 0.06437260733236246, 0.06006401044081361, 0.06287016303193993, 0.06583126532101118, 0.06905708790859166, 0.0693413942373558, 0.05688533624889203, 0.06841620236167817, 0.06329517663789062, 0.06359186857716956, 0.06283267522621984, 0.06373887967902075, 0.058269606193850666, 0.06981110073383828, 0.06269617092712418, 0.0577614118593001, 0.06859099263337509, 0.06139406766002958, 0.06541502430005652, 0.061053003132432886, 0.06285314044743512, 0.06805534577235417, 0.0636345518306215, 0.06025052076817403, 0.06834976932331362, 0.06534328081152786, 0.06375194183059313, 0.05666786949161085, 0.06164827560691942, 0.06644484415236224, 0.05847597973926089, 0.06677393915741463, 0.06249801898153327, 0.06310628861094504, 0.06430653289789286, 0.06470967856760215, 0.06145056471496866, 0.060752303921964866, 0.06639055211777001, 0.06047869399002634, 0.06465602425047741, 0.061304973126806155, 0.0655935386027637, 0.06341414031992448, 0.06725031946154275, 0.060548982925505715, 0.06391622813642255], 'lossList': [0.0, -1.2936304634809495, 0.0, 2.6361562809348107, 0.0, 0.0, 0.0], 'rewardMean': 0.7327190467042279, 'totalEpisodes': 117, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1053.379264714673, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=20480, timeSpent=54.3
