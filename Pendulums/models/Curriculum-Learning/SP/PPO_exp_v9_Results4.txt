#parameter variation file for learning
#varied parameters:
#case = 5
#computationIndex = 4
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_exp_v9_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_exp_v9_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6000, 12000], 'controlValues': [[2, 8], [0, 4], [0, 0]], 'dFactor': 0.025, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.7379634867381379, 'errorList': [], 'lossList': [0.0, -1.4121723181009294, 0.0, 78.3379943227768, 0.0, 0.0, 0.0], 'rewardMean': 0.7379634867381379, 'totalEpisodes': 8, 'stepsPerEpisode': 145, 'rewardPerEpisode': 117.25212369143394
'totalSteps': 2560, 'rewardStep': 0.8983981655778015, 'errorList': [], 'lossList': [0.0, -1.401323580145836, 0.0, 27.754631419181823, 0.0, 0.0, 0.0], 'rewardMean': 0.8181808261579697, 'totalEpisodes': 18, 'stepsPerEpisode': 39, 'rewardPerEpisode': 30.6965724776231
'totalSteps': 3840, 'rewardStep': 0.7292937815993633, 'errorList': [], 'lossList': [0.0, -1.3986024898290634, 0.0, 22.75932638168335, 0.0, 0.0, 0.0], 'rewardMean': 0.7885518113051009, 'totalEpisodes': 27, 'stepsPerEpisode': 25, 'rewardPerEpisode': 20.85164918992872
'totalSteps': 5120, 'rewardStep': 0.6457118401400859, 'errorList': [], 'lossList': [0.0, -1.3919462579488755, 0.0, 32.84077982664108, 0.0, 0.0, 0.0], 'rewardMean': 0.7528418185138471, 'totalEpisodes': 31, 'stepsPerEpisode': 237, 'rewardPerEpisode': 191.89052005780553
'totalSteps': 6400, 'rewardStep': 0.6173912683191827, 'errorList': [], 'lossList': [0.0, -1.3715169888734817, 0.0, 31.30202038049698, 0.0, 0.0, 0.0], 'rewardMean': 0.7257517084749143, 'totalEpisodes': 33, 'stepsPerEpisode': 867, 'rewardPerEpisode': 656.7052964530735
'totalSteps': 7680, 'rewardStep': 0.7123198656875578, 'errorList': [], 'lossList': [0.0, -1.3587413817644118, 0.0, 22.492714879512786, 0.0, 0.0, 0.0], 'rewardMean': 0.7235130680103549, 'totalEpisodes': 34, 'stepsPerEpisode': 428, 'rewardPerEpisode': 299.04003260144316
'totalSteps': 8960, 'rewardStep': 0.7885725537298075, 'errorList': [], 'lossList': [0.0, -1.335479616522789, 0.0, 211.83796478271483, 0.0, 0.0, 0.0], 'rewardMean': 0.732807280255991, 'totalEpisodes': 56, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.413979220866617
'totalSteps': 10240, 'rewardStep': 0.8736183368015324, 'errorList': [], 'lossList': [0.0, -1.3309226697683334, 0.0, 153.048589553833, 0.0, 0.0, 0.0], 'rewardMean': 0.7504086623241837, 'totalEpisodes': 81, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.492852558316512
'totalSteps': 11520, 'rewardStep': 0.8876184972039046, 'errorList': [], 'lossList': [0.0, -1.3248483222723006, 0.0, 51.85436964035034, 0.0, 0.0, 0.0], 'rewardMean': 0.7656541995330416, 'totalEpisodes': 99, 'stepsPerEpisode': 61, 'rewardPerEpisode': 55.753698528189915
'totalSteps': 12800, 'rewardStep': 0.7942136557354778, 'errorList': [], 'lossList': [0.0, -1.3228437936306, 0.0, 24.68408057689667, 0.0, 0.0, 0.0], 'rewardMean': 0.7685101451532852, 'totalEpisodes': 110, 'stepsPerEpisode': 90, 'rewardPerEpisode': 57.129956305705754
'totalSteps': 14080, 'rewardStep': 0.8514551579863414, 'errorList': [], 'lossList': [0.0, -1.3160864222049713, 0.0, 21.34819992542267, 0.0, 0.0, 0.0], 'rewardMean': 0.7798593122781055, 'totalEpisodes': 119, 'stepsPerEpisode': 98, 'rewardPerEpisode': 73.33520087188528
'totalSteps': 15360, 'rewardStep': 0.4834223574363912, 'errorList': [], 'lossList': [0.0, -1.3096581816673278, 0.0, 39.55623070240021, 0.0, 0.0, 0.0], 'rewardMean': 0.7383617314639646, 'totalEpisodes': 125, 'stepsPerEpisode': 136, 'rewardPerEpisode': 99.3058182846655
'totalSteps': 16640, 'rewardStep': 0.5153199168984994, 'errorList': [], 'lossList': [0.0, -1.3077062088251115, 0.0, 19.651555020809173, 0.0, 0.0, 0.0], 'rewardMean': 0.7169643449938781, 'totalEpisodes': 130, 'stepsPerEpisode': 206, 'rewardPerEpisode': 170.3990175205465
'totalSteps': 17920, 'rewardStep': 0.6986334726966477, 'errorList': [], 'lossList': [0.0, -1.3071350133419037, 0.0, 8.956778635978699, 0.0, 0.0, 0.0], 'rewardMean': 0.7222565082495344, 'totalEpisodes': 132, 'stepsPerEpisode': 16, 'rewardPerEpisode': 11.326360342959013
'totalSteps': 19200, 'rewardStep': 0.3707484493633286, 'errorList': [], 'lossList': [0.0, -1.3082586389780044, 0.0, 7.0929245865345, 0.0, 0.0, 0.0], 'rewardMean': 0.6975922263539489, 'totalEpisodes': 132, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 900.4479161071577
'totalSteps': 20480, 'rewardStep': 0.808496157066255, 'errorList': [], 'lossList': [0.0, -1.3016451293230056, 0.0, 7.930092434883118, 0.0, 0.0, 0.0], 'rewardMean': 0.7072098554918186, 'totalEpisodes': 135, 'stepsPerEpisode': 108, 'rewardPerEpisode': 76.0507911343708
'totalSteps': 21760, 'rewardStep': 0.8303317615146957, 'errorList': [], 'lossList': [0.0, -1.289500778913498, 0.0, 3.055471191108227, 0.0, 0.0, 0.0], 'rewardMean': 0.7113857762703073, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1012.4825766597976
'totalSteps': 23040, 'rewardStep': 0.9069006414852978, 'errorList': [], 'lossList': [0.0, -1.2771660369634628, 0.0, 2.4955748504400255, 0.0, 0.0, 0.0], 'rewardMean': 0.714714006738684, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1068.8227505104976
'totalSteps': 24320, 'rewardStep': 0.8302686144223613, 'errorList': [], 'lossList': [0.0, -1.2615240103006362, 0.0, 1.6775663477554916, 0.0, 0.0, 0.0], 'rewardMean': 0.7089790184605296, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1154.0832808759376
'totalSteps': 25600, 'rewardStep': 0.9903938193286844, 'errorList': [0.048015525614310564, 0.049390004541301616, 0.04492261710623887, 0.045645376603498364, 0.04565006645449063, 0.04644197266944102, 0.06608987751501863, 0.04782477013715498, 0.06048788282220339, 0.05853141244458023, 0.05588466109276264, 0.05520876933397283, 0.05996162193414865, 0.050229302939129736, 0.06500595881027556, 0.05711297114676352, 0.049055416042207166, 0.06985421469017265, 0.04447526483533209, 0.04890017138540673, 0.053964307677960656, 0.047334074289096285, 0.05095580361531974, 0.05992485144130466, 0.04847289507514637, 0.04726430925717479, 0.046393696334842534, 0.045119487536562496, 0.04693152302948384, 0.04660871910822156, 0.06436000638262668, 0.04829857762091349, 0.0460339604714301, 0.046658235040053, 0.04701999860378093, 0.050099568560995046, 0.066512055422969, 0.048096282717469206, 0.04803481749566253, 0.04688912519420522, 0.04859013075071261, 0.04705471706412451, 0.05062019035072015, 0.05032097355561039, 0.04727861862222314, 0.06669452054924975, 0.04945075213255547, 0.05792223819329032, 0.06398211755537468, 0.05043550605056542], 'lossList': [0.0, -1.226905981898308, 0.0, 1.5653537764772774, 0.0, 0.0, 0.0], 'rewardMean': 0.7285970348198503, 'totalEpisodes': 135, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1185.0655635110572, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=61.15
