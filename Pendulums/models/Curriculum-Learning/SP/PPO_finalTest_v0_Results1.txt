#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_finalTest_v0_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_finalTest_v0_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': (0, 6500, 11000), 'controlValues': [[2, 4], [0, 1], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8930538770161992, 'errorList': [], 'lossList': [0.0, -1.4154496312141418, 0.0, 37.97710075378418, 0.0, 0.0, 0.0], 'rewardMean': 0.8930538770161992, 'totalEpisodes': 38, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.810999421126475
'totalSteps': 2560, 'rewardStep': 0.6696567460131431, 'errorList': [], 'lossList': [0.0, -1.3981279337406158, 0.0, 31.40701463699341, 0.0, 0.0, 0.0], 'rewardMean': 0.7813553115146712, 'totalEpisodes': 81, 'stepsPerEpisode': 40, 'rewardPerEpisode': 29.109222683363566
'totalSteps': 3840, 'rewardStep': 0.5674913331782706, 'errorList': [], 'lossList': [0.0, -1.3663812255859376, 0.0, 39.25942735671997, 0.0, 0.0, 0.0], 'rewardMean': 0.710067318735871, 'totalEpisodes': 121, 'stepsPerEpisode': 26, 'rewardPerEpisode': 21.040121764566504
'totalSteps': 5120, 'rewardStep': 0.8609495485721437, 'errorList': [], 'lossList': [0.0, -1.3342629438638687, 0.0, 47.182689037323, 0.0, 0.0, 0.0], 'rewardMean': 0.7477878761949392, 'totalEpisodes': 144, 'stepsPerEpisode': 6, 'rewardPerEpisode': 4.51246695318524
'totalSteps': 6400, 'rewardStep': 0.8813671742730798, 'errorList': [], 'lossList': [0.0, -1.3207066875696183, 0.0, 53.16013655662537, 0.0, 0.0, 0.0], 'rewardMean': 0.7745037358105673, 'totalEpisodes': 155, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.715075806178713
'totalSteps': 7680, 'rewardStep': 0.783874918813346, 'errorList': [], 'lossList': [0.0, -1.315006467103958, 0.0, 22.758118005990983, 0.0, 0.0, 0.0], 'rewardMean': 0.7760655996443638, 'totalEpisodes': 161, 'stepsPerEpisode': 78, 'rewardPerEpisode': 63.44652603706243
'totalSteps': 8960, 'rewardStep': 0.5480048070806358, 'errorList': [], 'lossList': [0.0, -1.3077131807804108, 0.0, 72.32340070724487, 0.0, 0.0, 0.0], 'rewardMean': 0.7434854864209741, 'totalEpisodes': 173, 'stepsPerEpisode': 63, 'rewardPerEpisode': 49.68897720896967
'totalSteps': 10240, 'rewardStep': 0.867556542628371, 'errorList': [], 'lossList': [0.0, -1.3050459420681, 0.0, 37.870011315345764, 0.0, 0.0, 0.0], 'rewardMean': 0.7589943684468987, 'totalEpisodes': 183, 'stepsPerEpisode': 36, 'rewardPerEpisode': 31.202488671437475
'totalSteps': 11520, 'rewardStep': 0.6959562315992299, 'errorList': [], 'lossList': [0.0, -1.3029069221019745, 0.0, 19.417146322727202, 0.0, 0.0, 0.0], 'rewardMean': 0.75199013101938, 'totalEpisodes': 188, 'stepsPerEpisode': 101, 'rewardPerEpisode': 86.04610808390568
'totalSteps': 12800, 'rewardStep': 0.6675290642304215, 'errorList': [], 'lossList': [0.0, -1.2891288489103316, 0.0, 11.760322176218033, 0.0, 0.0, 0.0], 'rewardMean': 0.743544024340484, 'totalEpisodes': 191, 'stepsPerEpisode': 590, 'rewardPerEpisode': 480.2303365903703
'totalSteps': 14080, 'rewardStep': 0.6916650538027371, 'errorList': [], 'lossList': [0.0, -1.277435285449028, 0.0, 7.7698886573314665, 0.0, 0.0, 0.0], 'rewardMean': 0.7234051420191377, 'totalEpisodes': 194, 'stepsPerEpisode': 125, 'rewardPerEpisode': 102.10597748452524
'totalSteps': 15360, 'rewardStep': 0.6958797478741281, 'errorList': [], 'lossList': [0.0, -1.271608414053917, 0.0, 3.1989501506090163, 0.0, 0.0, 0.0], 'rewardMean': 0.7260274422052364, 'totalEpisodes': 194, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 878.7075944661451
'totalSteps': 16640, 'rewardStep': 0.711223381569759, 'errorList': [], 'lossList': [0.0, -1.2382415771484374, 0.0, 4.806019988656044, 0.0, 0.0, 0.0], 'rewardMean': 0.7404006470443851, 'totalEpisodes': 197, 'stepsPerEpisode': 142, 'rewardPerEpisode': 114.88084311928932
'totalSteps': 17920, 'rewardStep': 0.6544779949998896, 'errorList': [], 'lossList': [0.0, -1.2102549296617509, 0.0, 4.040616434216499, 0.0, 0.0, 0.0], 'rewardMean': 0.7197534916871599, 'totalEpisodes': 198, 'stepsPerEpisode': 957, 'rewardPerEpisode': 767.0488317275692
'totalSteps': 19200, 'rewardStep': 0.6513418983019177, 'errorList': [], 'lossList': [0.0, -1.1963241916894913, 0.0, 2.8524278011918067, 0.0, 0.0, 0.0], 'rewardMean': 0.6967509640900436, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 928.0461426613333
'totalSteps': 20480, 'rewardStep': 0.8399678405022417, 'errorList': [], 'lossList': [0.0, -1.1735745871067047, 0.0, 1.570242832005024, 0.0, 0.0, 0.0], 'rewardMean': 0.7023602562589331, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1077.1105639751188
'totalSteps': 21760, 'rewardStep': 0.8476282922393938, 'errorList': [], 'lossList': [0.0, -1.1438182973861695, 0.0, 1.4337841718643904, 0.0, 0.0, 0.0], 'rewardMean': 0.7323226047748089, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1143.0216847468503
'totalSteps': 23040, 'rewardStep': 0.956452833625758, 'errorList': [0.14050042801868004, 0.10277691478913283, 0.10436405371947124, 0.08802413873255807, 0.08383975234147768, 0.08963434238891953, 0.07758481309561185, 0.07993139213843654, 0.10921045872937764, 0.08015230286133682, 0.07187553292216034, 0.07866733137665638, 0.08067256081519134, 0.09840033989618668, 0.08871369566808887, 0.07859504342101035, 0.08647371856908036, 0.07595274920327444, 0.09780020870920927, 0.10981575091603654, 0.09230034122578128, 0.13052575110730524, 0.0776570622840802, 0.07741020833140877, 0.0885881641772628, 0.0761988258795441, 0.09298841121355764, 0.0879827436278368, 0.10685400216912948, 0.07667970749313467, 0.10993580552342162, 0.12263947464541938, 0.07302956257180021, 0.11183251878870736, 0.10156311948535907, 0.11201605690476253, 0.11946438659984807, 0.08176329831575208, 0.09505622481157813, 0.1341886513765426, 0.07927794627055923, 0.07304447684347365, 0.10639256703279551, 0.07281883124576703, 0.07574011266908279, 0.12894555258831908, 0.1064539172487754, 0.10365995573636531, 0.11409861373885079, 0.1269185773487759], 'lossList': [0.0, -1.1205812162160873, 0.0, 0.6836010219529272, 0.0, 0.0, 0.0], 'rewardMean': 0.7412122338745477, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.2620976586027, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=27.99
