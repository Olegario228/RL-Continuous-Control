#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_finalTest_v1_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_finalTest_v1_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': [0, 6500, 11000], 'controlValues': [[2, 4], [0, 1], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.9543837330523498, 'errorList': [], 'lossList': [0.0, -1.3992200100421905, 0.0, 38.18412993431091, 0.0, 0.0, 0.0], 'rewardMean': 0.9543837330523498, 'totalEpisodes': 40, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.8454518244399503
'totalSteps': 2560, 'rewardStep': 0.8727581029976245, 'errorList': [], 'lossList': [0.0, -1.3682298731803895, 0.0, 25.687027835845946, 0.0, 0.0, 0.0], 'rewardMean': 0.9135709180249871, 'totalEpisodes': 96, 'stepsPerEpisode': 8, 'rewardPerEpisode': 6.372564213636796
'totalSteps': 3840, 'rewardStep': 0.5270273275882168, 'errorList': [], 'lossList': [0.0, -1.3516790986061096, 0.0, 41.56531579971313, 0.0, 0.0, 0.0], 'rewardMean': 0.7847230545460637, 'totalEpisodes': 133, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.512183091720655
'totalSteps': 5120, 'rewardStep': 0.6355963274080916, 'errorList': [], 'lossList': [0.0, -1.3340712887048722, 0.0, 44.49765604019165, 0.0, 0.0, 0.0], 'rewardMean': 0.7474413727615707, 'totalEpisodes': 155, 'stepsPerEpisode': 105, 'rewardPerEpisode': 74.60425439761988
'totalSteps': 6400, 'rewardStep': 0.8367277511808423, 'errorList': [], 'lossList': [0.0, -1.3246904796361922, 0.0, 19.294466848373414, 0.0, 0.0, 0.0], 'rewardMean': 0.7652986484454251, 'totalEpisodes': 162, 'stepsPerEpisode': 15, 'rewardPerEpisode': 11.184564956830926
'totalSteps': 7680, 'rewardStep': 0.726120919011121, 'errorList': [], 'lossList': [0.0, -1.3117644155025483, 0.0, 24.954922935962678, 0.0, 0.0, 0.0], 'rewardMean': 0.758769026873041, 'totalEpisodes': 167, 'stepsPerEpisode': 142, 'rewardPerEpisode': 101.8417473478121
'totalSteps': 8960, 'rewardStep': 0.8956942905052111, 'errorList': [], 'lossList': [0.0, -1.2856878584623337, 0.0, 73.87400939941406, 0.0, 0.0, 0.0], 'rewardMean': 0.7783297788204938, 'totalEpisodes': 177, 'stepsPerEpisode': 98, 'rewardPerEpisode': 83.15779818394816
'totalSteps': 10240, 'rewardStep': 0.4486434814979543, 'errorList': [], 'lossList': [0.0, -1.2889255040884018, 0.0, 71.31495050430298, 0.0, 0.0, 0.0], 'rewardMean': 0.7371189916551764, 'totalEpisodes': 185, 'stepsPerEpisode': 108, 'rewardPerEpisode': 82.55653166961413
'totalSteps': 11520, 'rewardStep': 0.6549100752125614, 'errorList': [], 'lossList': [0.0, -1.2826699149608611, 0.0, 71.6889665031433, 0.0, 0.0, 0.0], 'rewardMean': 0.7279846676059969, 'totalEpisodes': 194, 'stepsPerEpisode': 53, 'rewardPerEpisode': 45.525248758484494
'totalSteps': 12800, 'rewardStep': 0.7940293887934095, 'errorList': [], 'lossList': [0.0, -1.2734136188030243, 0.0, 28.309666336774825, 0.0, 0.0, 0.0], 'rewardMean': 0.7345891397247382, 'totalEpisodes': 198, 'stepsPerEpisode': 162, 'rewardPerEpisode': 132.74947279679898
'totalSteps': 14080, 'rewardStep': 0.7940446100004354, 'errorList': [], 'lossList': [0.0, -1.256539825797081, 0.0, 45.1038805270195, 0.0, 0.0, 0.0], 'rewardMean': 0.7185552274195468, 'totalEpisodes': 203, 'stepsPerEpisode': 76, 'rewardPerEpisode': 67.20131353614137
'totalSteps': 15360, 'rewardStep': 0.7718454957443037, 'errorList': [], 'lossList': [0.0, -1.268081620335579, 0.0, 47.63237442493439, 0.0, 0.0, 0.0], 'rewardMean': 0.7084639666942147, 'totalEpisodes': 211, 'stepsPerEpisode': 69, 'rewardPerEpisode': 57.46438154268256
'totalSteps': 16640, 'rewardStep': 0.977441726245475, 'errorList': [2.7010800448355474, 6.632063654573955, 3.758347210046158, 1.3758603281694086, 0.14221399884716612, 1.3344034702500491, 1.3065874259294985, 1.6641148022191876, 2.1697395641789474, 2.5585084603582735, 1.2333977416479178, 3.1999969949748395, 1.552672752959195, 5.764529115529306, 5.3246713446717, 5.5260166413239125, 7.7859153519808535, 7.984123206012379, 6.611271838953123, 8.163400746573483, 8.123299185572558, 1.8243660852612893, 5.804288488637921, 1.190935842247848, 5.845457120210189, 5.911004170348408, 9.23974641510834, 8.155438630439537, 6.839572072432993, 5.908656234802579, 7.322155884591587, 5.870492106474918, 3.133851265529515, 5.706062886573194, 3.539805006955205, 5.433617538206959, 0.29543539436177424, 9.631178589977592, 5.815154740157739, 3.974177962180567, 6.471973827727385, 6.670375034032342, 5.854564521688202, 8.934441877518099, 7.442531108445127, 8.82392782988962, 7.267407466909773, 7.469710695627109, 9.806388713259755, 7.985229966528974], 'lossList': [0.0, -1.2822921913862229, 0.0, 47.0198800611496, 0.0, 0.0, 0.0], 'rewardMean': 0.7535054065599406, 'totalEpisodes': 215, 'stepsPerEpisode': 253, 'rewardPerEpisode': 219.2153220813365, 'successfulTests': 1
'totalSteps': 17920, 'rewardStep': 0.9266495257017914, 'errorList': [], 'lossList': [0.0, -1.289367271065712, 0.0, 51.800476272106174, 0.0, 0.0, 0.0], 'rewardMean': 0.7826107263893106, 'totalEpisodes': 220, 'stepsPerEpisode': 123, 'rewardPerEpisode': 100.42826616697924
'totalSteps': 19200, 'rewardStep': 0.7479292310145716, 'errorList': [], 'lossList': [0.0, -1.2986487114429475, 0.0, 28.05238792896271, 0.0, 0.0, 0.0], 'rewardMean': 0.7737308743726835, 'totalEpisodes': 222, 'stepsPerEpisode': 456, 'rewardPerEpisode': 382.8842624732329
'totalSteps': 20480, 'rewardStep': 0.6058075274158479, 'errorList': [], 'lossList': [0.0, -1.3007348918914794, 0.0, 29.851490609645843, 0.0, 0.0, 0.0], 'rewardMean': 0.7616995352131563, 'totalEpisodes': 223, 'stepsPerEpisode': 506, 'rewardPerEpisode': 323.2716195640051
'totalSteps': 21760, 'rewardStep': 0.6738157308592648, 'errorList': [], 'lossList': [0.0, -1.3095604985952378, 0.0, 2.91857970893383, 0.0, 0.0, 0.0], 'rewardMean': 0.7395116792485615, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 846.0995050569585
'totalSteps': 23040, 'rewardStep': 0.5984707938614384, 'errorList': [], 'lossList': [0.0, -1.3224543452262878, 0.0, 1.8511030164361, 0.0, 0.0, 0.0], 'rewardMean': 0.75449441048491, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 832.5866490466344
'totalSteps': 24320, 'rewardStep': 0.8197316381182189, 'errorList': [], 'lossList': [0.0, -1.3252253490686416, 0.0, 43.21937059044838, 0.0, 0.0, 0.0], 'rewardMean': 0.7709765667754758, 'totalEpisodes': 225, 'stepsPerEpisode': 62, 'rewardPerEpisode': 48.22858442800971
'totalSteps': 25600, 'rewardStep': 0.7091743813850917, 'errorList': [], 'lossList': [0.0, -1.2988759690523148, 0.0, 1.1066996985673905, 0.0, 0.0, 0.0], 'rewardMean': 0.7624910660346439, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 905.3381514901746
'totalSteps': 26880, 'rewardStep': 0.8429580869486173, 'errorList': [], 'lossList': [0.0, -1.2602937752008438, 0.0, 0.8087150791287422, 0.0, 0.0, 0.0], 'rewardMean': 0.767382413729462, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 962.902327269647
'totalSteps': 28160, 'rewardStep': 0.601257509016258, 'errorList': [], 'lossList': [0.0, -1.241351103782654, 0.0, 0.3356489391624928, 0.0, 0.0, 0.0], 'rewardMean': 0.7503236150566576, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 943.7678905051944
'totalSteps': 29440, 'rewardStep': 0.818792383703399, 'errorList': [], 'lossList': [0.0, -1.2210983586311341, 0.0, 1.0105089612305165, 0.0, 0.0, 0.0], 'rewardMean': 0.7344586808024499, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1020.9035283924543
'totalSteps': 30720, 'rewardStep': 0.7401334427269664, 'errorList': [], 'lossList': [0.0, -1.1923454612493516, 0.0, 0.5089543277025222, 0.0, 0.0, 0.0], 'rewardMean': 0.7158070725049673, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1046.40943466918
'totalSteps': 32000, 'rewardStep': 0.9120448429062661, 'errorList': [], 'lossList': [0.0, -1.189652391076088, 0.0, 0.8737250210344791, 0.0, 0.0, 0.0], 'rewardMean': 0.7322186336941369, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1083.724291291812
'totalSteps': 33280, 'rewardStep': 0.9096179673629636, 'errorList': [], 'lossList': [0.0, -1.1841222763061523, 0.0, 1.0841663447022438, 0.0, 0.0, 0.0], 'rewardMean': 0.7625996776888485, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1182.5239524576418
'totalSteps': 34560, 'rewardStep': 0.9514630181817024, 'errorList': [0.32737868599009373, 0.9070045126348875, 0.5125636076303784, 0.5845992093244295, 0.4836466684375979, 1.0057027550817312, 0.3501323359046583, 0.25061662419017927, 0.12026658000971094, 0.3228923364570767, 0.4579735524711229, 0.11961845139132568, 0.2873530566196308, 0.19581856569782832, 0.18110101106919646, 0.6856230561013509, 0.08421050007035329, 0.6299193209328012, 0.21485508482156374, 0.33723226327617223, 0.07179402230581602, 0.3772138321418738, 0.11009508222672262, 0.2935858043392718, 0.23820650666146054, 0.23099676295350202, 0.2639783028242907, 0.09382621963288007, 0.5473155798975443, 0.10661270682463726, 0.32425855159607214, 0.25319687268573754, 0.33227592418589763, 0.11705066478000778, 0.28595230944687733, 0.4602439287743286, 0.044174867053510224, 0.7989079095456473, 0.0667346178423627, 0.34935428651492884, 0.43862667012840045, 0.20901554267945496, 0.09561057300897884, 0.3320705601639378, 0.869702582430683, 0.20207455104446714, 0.08317770228904171, 0.1337890518554887, 0.42297849097609413, 0.28294101029668867], 'lossList': [0.0, -1.1597338092327119, 0.0, 0.5634939595125615, 0.0, 0.0, 0.0], 'rewardMean': 0.7903644064210922, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1184.8523702907514, 'successfulTests': 15
'totalSteps': 35840, 'rewardStep': 0.7919040956683597, 'errorList': [], 'lossList': [0.0, -1.1490651434659958, 0.0, 0.5059707760810852, 0.0, 0.0, 0.0], 'rewardMean': 0.8097077366017844, 'totalEpisodes': 225, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1199.3882985966718
#maxSuccessfulTests=15, maxSuccessfulTestsAtStep=34560, timeSpent=45.29
