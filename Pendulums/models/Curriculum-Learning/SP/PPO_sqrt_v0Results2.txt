#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 5000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 50000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_sqrt_v0Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': False, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_sqrt_v0Results', 'verbose': True, 'curicculumLearning': {'decayType': 'sqrt', 'decaySteps': [0, 7000, 14000], 'controlValues': [[2, 10], [0, 5], [0, 0]], 'dFactor': 0.05, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8571311275834804, 'errorList': [], 'lossList': [0.0, -1.4173127537965775, 0.0, 87.8449548959732, 0.0, 0.0, 0.0], 'rewardMean': 0.8571311275834804, 'totalEpisodes': 5, 'stepsPerEpisode': 241, 'rewardPerEpisode': 210.29344693213
'totalSteps': 2560, 'rewardStep': 0.7898139616254887, 'errorList': [], 'lossList': [0.0, -1.4275869476795195, 0.0, 28.562128094434737, 0.0, 0.0, 0.0], 'rewardMean': 0.8234725446044846, 'totalEpisodes': 8, 'stepsPerEpisode': 415, 'rewardPerEpisode': 310.43305569431953
'totalSteps': 3840, 'rewardStep': 0.8644072872125681, 'errorList': [], 'lossList': [0.0, -1.4337931549549103, 0.0, 35.5527476143837, 0.0, 0.0, 0.0], 'rewardMean': 0.837117458807179, 'totalEpisodes': 11, 'stepsPerEpisode': 488, 'rewardPerEpisode': 393.9815925965769
'totalSteps': 5120, 'rewardStep': 0.5801631675468806, 'errorList': [], 'lossList': [0.0, -1.4213754159212113, 0.0, 18.95071546435356, 0.0, 0.0, 0.0], 'rewardMean': 0.7728788859921044, 'totalEpisodes': 12, 'stepsPerEpisode': 254, 'rewardPerEpisode': 183.32356175014394
'totalSteps': 6400, 'rewardStep': 0.4664437198362644, 'errorList': [], 'lossList': [0.0, -1.4227371501922608, 0.0, 27.807801748514176, 0.0, 0.0, 0.0], 'rewardMean': 0.7115918527609364, 'totalEpisodes': 13, 'stepsPerEpisode': 575, 'rewardPerEpisode': 433.5090894741076
'totalSteps': 7680, 'rewardStep': 0.7485254355491454, 'errorList': [], 'lossList': [0.0, -1.4054852372407913, 0.0, 14.295722932219505, 0.0, 0.0, 0.0], 'rewardMean': 0.7177474498923045, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1033.2543628703856
'totalSteps': 8960, 'rewardStep': 0.8706746688598762, 'errorList': [], 'lossList': [0.0, -1.4038251781463622, 0.0, 10.192800270020962, 0.0, 0.0, 0.0], 'rewardMean': 0.7395941954591005, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1065.442206702305
'totalSteps': 10240, 'rewardStep': 0.7837882429734935, 'errorList': [], 'lossList': [0.0, -1.372012352347374, 0.0, 8.452308779060841, 0.0, 0.0, 0.0], 'rewardMean': 0.7451184513983997, 'totalEpisodes': 13, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1100.0717669857152
'totalSteps': 11520, 'rewardStep': 0.7770870396581179, 'errorList': [], 'lossList': [0.0, -1.35781174659729, 0.0, 31.014189465641977, 0.0, 0.0, 0.0], 'rewardMean': 0.7486705167605906, 'totalEpisodes': 14, 'stepsPerEpisode': 1222, 'rewardPerEpisode': 1027.240124488509
'totalSteps': 12800, 'rewardStep': 0.9586133139626485, 'errorList': [], 'lossList': [0.0, -1.3503509896993637, 0.0, 86.96968161106109, 0.0, 0.0, 0.0], 'rewardMean': 0.7696647964807963, 'totalEpisodes': 17, 'stepsPerEpisode': 107, 'rewardPerEpisode': 91.06286599031385
'totalSteps': 14080, 'rewardStep': 0.8528537164861313, 'errorList': [], 'lossList': [0.0, -1.343087215423584, 0.0, 171.17001945495605, 0.0, 0.0, 0.0], 'rewardMean': 0.7692370553710615, 'totalEpisodes': 24, 'stepsPerEpisode': 30, 'rewardPerEpisode': 20.42253551165716
'totalSteps': 15360, 'rewardStep': 0.7111282535213523, 'errorList': [], 'lossList': [0.0, -1.3414534360170365, 0.0, 304.4087098312378, 0.0, 0.0, 0.0], 'rewardMean': 0.7613684845606479, 'totalEpisodes': 59, 'stepsPerEpisode': 30, 'rewardPerEpisode': 24.619917655084134
'totalSteps': 16640, 'rewardStep': 0.766093243473, 'errorList': [], 'lossList': [0.0, -1.3414740830659866, 0.0, 58.48166273117065, 0.0, 0.0, 0.0], 'rewardMean': 0.751537080186691, 'totalEpisodes': 86, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.483512255303647
'totalSteps': 17920, 'rewardStep': 0.5857785481880964, 'errorList': [], 'lossList': [0.0, -1.3428660595417024, 0.0, 29.710272965431212, 0.0, 0.0, 0.0], 'rewardMean': 0.7520986182508126, 'totalEpisodes': 110, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.5857785481880964
'totalSteps': 19200, 'rewardStep': 0.6594432544874061, 'errorList': [], 'lossList': [0.0, -1.3405960339307785, 0.0, 25.55327751159668, 0.0, 0.0, 0.0], 'rewardMean': 0.7713985717159267, 'totalEpisodes': 130, 'stepsPerEpisode': 76, 'rewardPerEpisode': 65.84789618687444
'totalSteps': 20480, 'rewardStep': 0.6493032252232026, 'errorList': [], 'lossList': [0.0, -1.3326138466596604, 0.0, 19.165570974349976, 0.0, 0.0, 0.0], 'rewardMean': 0.7614763506833324, 'totalEpisodes': 149, 'stepsPerEpisode': 11, 'rewardPerEpisode': 7.018044496138739
'totalSteps': 21760, 'rewardStep': 0.8699341052999419, 'errorList': [], 'lossList': [0.0, -1.3251981925964356, 0.0, 18.51438738822937, 0.0, 0.0, 0.0], 'rewardMean': 0.7614022943273391, 'totalEpisodes': 164, 'stepsPerEpisode': 106, 'rewardPerEpisode': 90.76118768180302
'totalSteps': 23040, 'rewardStep': 0.8847944052623364, 'errorList': [], 'lossList': [0.0, -1.3196946275234223, 0.0, 14.593441603183747, 0.0, 0.0, 0.0], 'rewardMean': 0.7715029105562234, 'totalEpisodes': 173, 'stepsPerEpisode': 36, 'rewardPerEpisode': 28.84548106152731
'totalSteps': 24320, 'rewardStep': 0.10820298790549487, 'errorList': [], 'lossList': [0.0, -1.3093070101737976, 0.0, 14.701400938034057, 0.0, 0.0, 0.0], 'rewardMean': 0.704614505380961, 'totalEpisodes': 180, 'stepsPerEpisode': 151, 'rewardPerEpisode': 103.88492896128483
'totalSteps': 25600, 'rewardStep': 0.5381536096409087, 'errorList': [], 'lossList': [0.0, -1.291964846253395, 0.0, 16.53557808637619, 0.0, 0.0, 0.0], 'rewardMean': 0.6625685349487871, 'totalEpisodes': 185, 'stepsPerEpisode': 345, 'rewardPerEpisode': 282.4001403729706
'totalSteps': 26880, 'rewardStep': 0.9052798933881396, 'errorList': [], 'lossList': [0.0, -1.275187925696373, 0.0, 7.856619963645935, 0.0, 0.0, 0.0], 'rewardMean': 0.6678111526389878, 'totalEpisodes': 189, 'stepsPerEpisode': 299, 'rewardPerEpisode': 247.54684133401577
'totalSteps': 28160, 'rewardStep': 0.7228984140684647, 'errorList': [], 'lossList': [0.0, -1.2422944957017898, 0.0, 3.507339400947094, 0.0, 0.0, 0.0], 'rewardMean': 0.6689881686936991, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1106.3873871281273
'totalSteps': 29440, 'rewardStep': 0.9364374789380946, 'errorList': [], 'lossList': [0.0, -1.1863188654184342, 0.0, 2.823317193314433, 0.0, 0.0, 0.0], 'rewardMean': 0.6860225922402086, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1151.204169084939
'totalSteps': 30720, 'rewardStep': 0.9894614397342459, 'errorList': [0.031052944244038334, 0.04721622245139578, 0.025728872986695697, 0.035184195959252054, 0.04090615313540749, 0.0312803985141629, 0.02456633889557655, 0.03492821401314188, 0.03361761943224503, 0.033972069299712956, 0.044574197012238, 0.02798978371607196, 0.030693981142017983, 0.03322035157720079, 0.035904915740284635, 0.041679583549119444, 0.026561224714250414, 0.024956861329286175, 0.03140143528335235, 0.03392448146772275, 0.03712739861950773, 0.04232729365976326, 0.029805691902205837, 0.025610010404205182, 0.03808219696289943, 0.04521412240088097, 0.025880023525207053, 0.02509524163081865, 0.028724487040539967, 0.032375885696187816, 0.03440407245968567, 0.0381729391086061, 0.024444301687451007, 0.05318436477633865, 0.032196370610356816, 0.05754051365462507, 0.02861170708385286, 0.024590922622708018, 0.03266144951589371, 0.03294584980499259, 0.024560301588745892, 0.025332182036690465, 0.0325852883405415, 0.03024756163228858, 0.04947949525609073, 0.037217030408750816, 0.03619007360231041, 0.03841589505564156, 0.029286771252926953, 0.0307958717471479], 'lossList': [0.0, -1.1212770277261734, 0.0, 2.713697746619582, 0.0, 0.0, 0.0], 'rewardMean': 0.7263908813948234, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1209.9974882738948, 'successfulTests': 50
'totalSteps': 32000, 'rewardStep': 0.9576944504671765, 'errorList': [], 'lossList': [0.0, -1.0786157697439194, 0.0, 1.8597547342255711, 0.0, 0.0, 0.0], 'rewardMean': 0.7562160009928005, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1204.9997353541512
'totalSteps': 33280, 'rewardStep': 0.976881816441146, 'errorList': [], 'lossList': [0.0, -1.0622722041606902, 0.0, 1.0267463862150907, 0.0, 0.0, 0.0], 'rewardMean': 0.788973860114595, 'totalEpisodes': 189, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.0916905662336
'totalSteps': 34560, 'rewardStep': 0.9514968331788928, 'errorList': [], 'lossList': [0.0, -1.064325920343399, 0.0, 3.179520370066166, 0.0, 0.0, 0.0], 'rewardMean': 0.7971301329024901, 'totalEpisodes': 190, 'stepsPerEpisode': 113, 'rewardPerEpisode': 104.61303694816975
'totalSteps': 35840, 'rewardStep': 0.8714962801303437, 'errorList': [], 'lossList': [0.0, -1.0728317773342133, 0.0, 3.2004524295032026, 0.0, 0.0, 0.0], 'rewardMean': 0.7958003203892907, 'totalEpisodes': 191, 'stepsPerEpisode': 240, 'rewardPerEpisode': 221.38196525671242
'totalSteps': 37120, 'rewardStep': 0.9179587774340293, 'errorList': [], 'lossList': [0.0, -1.069304999113083, 0.0, 0.12598807205446064, 0.0, 0.0, 0.0], 'rewardMean': 0.8767758993421442, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1094.1880731157107
'totalSteps': 38400, 'rewardStep': 0.8929653012092579, 'errorList': [], 'lossList': [0.0, -1.039883677959442, 0.0, 0.2984238135721535, 0.0, 0.0, 0.0], 'rewardMean': 0.912257068498979, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1195.467021296398
'totalSteps': 39680, 'rewardStep': 0.8923365111300736, 'errorList': [], 'lossList': [0.0, -1.0215438908338548, 0.0, 0.18815314992330967, 0.0, 0.0, 0.0], 'rewardMean': 0.9109627302731725, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1201.002114050901
'totalSteps': 40960, 'rewardStep': 0.9377435040168063, 'errorList': [], 'lossList': [0.0, -1.0054847550392152, 0.0, 0.18815264586359262, 0.0, 0.0, 0.0], 'rewardMean': 0.9324472392680067, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1220.5543605731964
'totalSteps': 42240, 'rewardStep': 0.9150181349288505, 'errorList': [], 'lossList': [0.0, -0.9721235609054566, 0.0, 0.13450259638018905, 0.0, 0.0, 0.0], 'rewardMean': 0.9303053048670822, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1206.9295406543483
'totalSteps': 43520, 'rewardStep': 0.8736493827612329, 'errorList': [], 'lossList': [0.0, -0.9347252035140992, 0.0, 0.04776426125317812, 0.0, 0.0, 0.0], 'rewardMean': 0.9187240991697809, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1173.41694975432
'totalSteps': 44800, 'rewardStep': 0.9476624475531835, 'errorList': [], 'lossList': [0.0, -0.9004943972826004, 0.0, 0.0740870838984847, 0.0, 0.0, 0.0], 'rewardMean': 0.9177208988783816, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1227.5137471680928
'totalSteps': 46080, 'rewardStep': 0.9904183113001371, 'errorList': [0.1542879387171055, 0.3019309682630659, 0.2006513004116014, 0.9704419195364474, 0.20920000431331642, 0.7867277988863802, 0.1989528039975906, 0.8711380495906156, 1.9751161676849152, 0.09218029255445995, 0.2786402803487142, 0.5362458333033788, 1.5559635239092682, 0.10092513702084253, 0.4130033245069481, 2.372781447842927, 0.34969249776146044, 0.42421376488781887, 0.2968750597076913, 0.10089590967524598, 0.16323509376834697, 0.48631506226573057, 0.28074114161634556, 3.5514663298313995, 0.1645011235219141, 0.5648911616723532, 1.0879343444343383, 0.2541042046942017, 0.4577631965406261, 0.29861244047262187, 1.7788599870705104, 0.39777017860118724, 0.16916301572677528, 2.46102589254489, 1.4344151228878772, 0.30598354397172095, 0.5920501973816319, 1.8531913119698566, 0.3968930820311668, 0.0533458490928512, 0.10881717580086324, 0.25972071191934903, 0.2437477356802632, 0.6942946828154666, 2.449396715039071, 4.3866269139109, 0.42426475335150116, 0.7759978825069372, 0.48669581628129616, 0.3301258029048635], 'lossList': [0.0, -0.8733778861165047, 0.0, 0.06428606636822223, 0.0, 0.0, 0.0], 'rewardMean': 0.9190745483642807, 'totalEpisodes': 191, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1237.7302667065724, 'successfulTests': 10
'totalSteps': 47360, 'rewardStep': 0.9775664085898652, 'errorList': [], 'lossList': [0.0, -0.8566189044713974, 0.0, 1.6639630737900735, 0.0, 0.0, 0.0], 'rewardMean': 0.9216815059053781, 'totalEpisodes': 193, 'stepsPerEpisode': 107, 'rewardPerEpisode': 104.48662131704292
'totalSteps': 48640, 'rewardStep': 0.9935129418760799, 'errorList': [], 'lossList': [0.0, -0.8576232758164406, 0.0, 1.3333522750437259, 0.0, 0.0, 0.0], 'rewardMean': 0.9338831720799516, 'totalEpisodes': 195, 'stepsPerEpisode': 79, 'rewardPerEpisode': 74.8022640659199
'totalSteps': 49920, 'rewardStep': 0.9170856121706513, 'errorList': [], 'lossList': [0.0, -0.8587243288755417, 0.0, 0.6576131528802216, 0.0, 0.0, 0.0], 'rewardMean': 0.9337958555536139, 'totalEpisodes': 196, 'stepsPerEpisode': 434, 'rewardPerEpisode': 411.54633825875544
'totalSteps': 51200, 'rewardStep': 0.8722278875316947, 'errorList': [], 'lossList': [0.0, -0.8591490426659584, 0.0, 0.013572981914039701, 0.0, 0.0, 0.0], 'rewardMean': 0.9317221141858576, 'totalEpisodes': 196, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1200.2401923974667
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=30720, timeSpent=101.57
