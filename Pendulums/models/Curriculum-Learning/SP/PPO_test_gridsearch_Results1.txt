#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_test_gridsearch_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_test_gridsearch_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': (0, 6000, 10000), 'controlValues': [[2, 4], [0, 1], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.8930538770161992, 'errorList': [], 'lossList': [0.0, -1.4154496312141418, 0.0, 37.97710075378418, 0.0, 0.0, 0.0], 'rewardMean': 0.8930538770161992, 'totalEpisodes': 38, 'stepsPerEpisode': 35, 'rewardPerEpisode': 28.810999421126475
'totalSteps': 2560, 'rewardStep': 0.8739191791975769, 'errorList': [], 'lossList': [0.0, -1.389553853869438, 0.0, 25.89778744697571, 0.0, 0.0, 0.0], 'rewardMean': 0.883486528106888, 'totalEpisodes': 92, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.1728795089126
'totalSteps': 3840, 'rewardStep': 0.5955898318274238, 'errorList': [], 'lossList': [0.0, -1.3598167884349823, 0.0, 33.342536516189575, 0.0, 0.0, 0.0], 'rewardMean': 0.7875209626804, 'totalEpisodes': 133, 'stepsPerEpisode': 23, 'rewardPerEpisode': 14.604189192530459
'totalSteps': 5120, 'rewardStep': 0.7342270407833005, 'errorList': [], 'lossList': [0.0, -1.337787560224533, 0.0, 46.142229347229005, 0.0, 0.0, 0.0], 'rewardMean': 0.774197482206125, 'totalEpisodes': 153, 'stepsPerEpisode': 9, 'rewardPerEpisode': 7.652768385067384
'totalSteps': 6400, 'rewardStep': 0.9396624722674725, 'errorList': [], 'lossList': [0.0, -1.3126577812433242, 0.0, 55.53948185920715, 0.0, 0.0, 0.0], 'rewardMean': 0.8072904802183946, 'totalEpisodes': 167, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9396624722674725
'totalSteps': 7680, 'rewardStep': 0.5286428762710538, 'errorList': [], 'lossList': [0.0, -1.297998961210251, 0.0, 17.016767508983612, 0.0, 0.0, 0.0], 'rewardMean': 0.7608492128938377, 'totalEpisodes': 168, 'stepsPerEpisode': 849, 'rewardPerEpisode': 586.8935496462036
'totalSteps': 8960, 'rewardStep': 0.27122226114488557, 'errorList': [], 'lossList': [0.0, -1.286347945332527, 0.0, 80.59917254447937, 0.0, 0.0, 0.0], 'rewardMean': 0.6909025055011302, 'totalEpisodes': 181, 'stepsPerEpisode': 96, 'rewardPerEpisode': 53.47190657269014
'totalSteps': 10240, 'rewardStep': 0.8479545700981255, 'errorList': [], 'lossList': [0.0, -1.2882914984226226, 0.0, 10.244339547157288, 0.0, 0.0, 0.0], 'rewardMean': 0.7105340135757547, 'totalEpisodes': 189, 'stepsPerEpisode': 50, 'rewardPerEpisode': 34.82008158183499
'totalSteps': 11520, 'rewardStep': 0.49644195784701567, 'errorList': [], 'lossList': [0.0, -1.2813443744182587, 0.0, 23.842026691436768, 0.0, 0.0, 0.0], 'rewardMean': 0.6867460073836726, 'totalEpisodes': 194, 'stepsPerEpisode': 428, 'rewardPerEpisode': 334.2529640035784
'totalSteps': 12800, 'rewardStep': 0.6234798788991722, 'errorList': [], 'lossList': [0.0, -1.2690965390205384, 0.0, 11.83157753109932, 0.0, 0.0, 0.0], 'rewardMean': 0.6804193945352226, 'totalEpisodes': 198, 'stepsPerEpisode': 155, 'rewardPerEpisode': 123.7527990085501
'totalSteps': 14080, 'rewardStep': 0.7364238824787241, 'errorList': [], 'lossList': [0.0, -1.2328839081525802, 0.0, 6.728034600019455, 0.0, 0.0, 0.0], 'rewardMean': 0.664756395081475, 'totalEpisodes': 198, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 913.3150862747784
'totalSteps': 15360, 'rewardStep': 0.8023209850715629, 'errorList': [], 'lossList': [0.0, -1.2125802862644195, 0.0, 5.3693459808826445, 0.0, 0.0, 0.0], 'rewardMean': 0.6575965756688736, 'totalEpisodes': 199, 'stepsPerEpisode': 228, 'rewardPerEpisode': 198.64001358320388
'totalSteps': 16640, 'rewardStep': 0.8762413183911277, 'errorList': [], 'lossList': [0.0, -1.196361758708954, 0.0, 3.579805111885071, 0.0, 0.0, 0.0], 'rewardMean': 0.6856617243252441, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1052.2616025291156
'totalSteps': 17920, 'rewardStep': 0.8585438058423823, 'errorList': [], 'lossList': [0.0, -1.150255257487297, 0.0, 2.419918381124735, 0.0, 0.0, 0.0], 'rewardMean': 0.6980934008311522, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1097.076807961366
'totalSteps': 19200, 'rewardStep': 0.9110161034330413, 'errorList': [], 'lossList': [0.0, -1.1297844278812408, 0.0, 1.9990029828250409, 0.0, 0.0, 0.0], 'rewardMean': 0.695228763947709, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1133.005654510688
'totalSteps': 20480, 'rewardStep': 0.9024975458545278, 'errorList': [], 'lossList': [0.0, -1.0938618803024291, 0.0, 1.4075431215390564, 0.0, 0.0, 0.0], 'rewardMean': 0.7326142309060566, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1172.4405069574173
'totalSteps': 21760, 'rewardStep': 0.8700875627552279, 'errorList': [], 'lossList': [0.0, -1.0271669852733611, 0.0, 1.1215385297313332, 0.0, 0.0, 0.0], 'rewardMean': 0.7925007610670908, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1197.9819309188076
'totalSteps': 23040, 'rewardStep': 0.9962232994614423, 'errorList': [0.026007687638837956, 0.026489538543497518, 0.03430879581558976, 0.022309631734587828, 0.03584335449656502, 0.04708018307351423, 0.0200271313008554, 0.03818533791030145, 0.03096673025943046, 0.03932817770696828, 0.046458718338515705, 0.03565783473267516, 0.024604554779238348, 0.05798846412728932, 0.03174415735415933, 0.020154965212164123, 0.0348440572435726, 0.02014228370322481, 0.023326677036428902, 0.05412538823470678, 0.051711485968553045, 0.035128768939643264, 0.07364111920075789, 0.050432281518253935, 0.036217952840150516, 0.04810757592403638, 0.0329078082939837, 0.02141763291730653, 0.049394380231551904, 0.04930864681832597, 0.05443543694402485, 0.05301077461847086, 0.029785455135540918, 0.03715270423219007, 0.0527621022895798, 0.05050653973866605, 0.03642507840626991, 0.02766446793051753, 0.036081194512906124, 0.04546366920803682, 0.019791061636623548, 0.02986097847535939, 0.06868257644204905, 0.05903136835949095, 0.03824432820230421, 0.01904447057876374, 0.03739042141005154, 0.01779870576055082, 0.016849259055279328, 0.03346264824858759], 'lossList': [0.0, -0.9888882860541344, 0.0, 0.8173919283784926, 0.0, 0.0, 0.0], 'rewardMean': 0.8073276340034224, 'totalEpisodes': 199, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1207.477004112112, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=23040, timeSpent=35.23
