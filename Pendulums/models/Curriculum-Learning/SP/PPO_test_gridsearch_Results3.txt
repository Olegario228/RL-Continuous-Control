#parameter variation file for learning
#varied parameters:
#case = 4
#computationIndex = 3
#functionData = {'nArms': 1, 'evaluationSteps': 1000, 'episodeSteps': 1024, 'episodeStepsMax': 1280, 'totalLearningSteps': 35000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.93, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'forceFactor': 1, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models\\Curriculum-Learning\\SP\\PPO_test_gridsearch_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models\\Curriculum-Learning\\SP\\PPO_test_gridsearch_Results', 'verbose': True, 'curicculumLearning': {'decayType': 'exp', 'decaySteps': (0, 6000, 10000), 'controlValues': [[2, 4], [0, 1], [0, 0]], 'dFactor': 0.0005, 'nStepsLastChange': 0, 'iDecay': 0}}
#
'totalSteps': 1280, 'rewardStep': 0.5809953713846492, 'errorList': [], 'lossList': [0.0, -1.4171791988611222, 0.0, 34.73336408615112, 0.0, 0.0, 0.0], 'rewardMean': 0.5809953713846492, 'totalEpisodes': 44, 'stepsPerEpisode': 71, 'rewardPerEpisode': 52.210451438359044
'totalSteps': 2560, 'rewardStep': 0.8245743565779902, 'errorList': [], 'lossList': [0.0, -1.410081239938736, 0.0, 30.05238286972046, 0.0, 0.0, 0.0], 'rewardMean': 0.7027848639813197, 'totalEpisodes': 87, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8245743565779902
'totalSteps': 3840, 'rewardStep': 0.7380288070305707, 'errorList': [], 'lossList': [0.0, -1.4023247581720353, 0.0, 40.26002892494202, 0.0, 0.0, 0.0], 'rewardMean': 0.7145328449977367, 'totalEpisodes': 129, 'stepsPerEpisode': 20, 'rewardPerEpisode': 14.0827720047209
'totalSteps': 5120, 'rewardStep': 0.8963549969670039, 'errorList': [], 'lossList': [0.0, -1.3872741013765335, 0.0, 45.665366086959835, 0.0, 0.0, 0.0], 'rewardMean': 0.7599883829900536, 'totalEpisodes': 148, 'stepsPerEpisode': 77, 'rewardPerEpisode': 64.8816481265448
'totalSteps': 6400, 'rewardStep': 0.7607522983036091, 'errorList': [], 'lossList': [0.0, -1.3676900666952134, 0.0, 42.731220169067385, 0.0, 0.0, 0.0], 'rewardMean': 0.7601411660527646, 'totalEpisodes': 159, 'stepsPerEpisode': 20, 'rewardPerEpisode': 17.844306654916704
'totalSteps': 7680, 'rewardStep': 0.9731550873881765, 'errorList': [], 'lossList': [0.0, -1.3494258576631546, 0.0, 54.50215022087097, 0.0, 0.0, 0.0], 'rewardMean': 0.7956434862753333, 'totalEpisodes': 167, 'stepsPerEpisode': 83, 'rewardPerEpisode': 65.63820001994361
'totalSteps': 8960, 'rewardStep': 0.41439892202135675, 'errorList': [], 'lossList': [0.0, -1.347651480436325, 0.0, 82.76651525497437, 0.0, 0.0, 0.0], 'rewardMean': 0.7411799770961938, 'totalEpisodes': 182, 'stepsPerEpisode': 87, 'rewardPerEpisode': 60.859622049384676
'totalSteps': 10240, 'rewardStep': 0.5162477154400921, 'errorList': [], 'lossList': [0.0, -1.3553948992490767, 0.0, 59.33730298995972, 0.0, 0.0, 0.0], 'rewardMean': 0.713063444389181, 'totalEpisodes': 193, 'stepsPerEpisode': 161, 'rewardPerEpisode': 118.24751734776338
'totalSteps': 11520, 'rewardStep': 0.7981197160692935, 'errorList': [], 'lossList': [0.0, -1.3692529642581939, 0.0, 37.265438475608825, 0.0, 0.0, 0.0], 'rewardMean': 0.7225141412425269, 'totalEpisodes': 203, 'stepsPerEpisode': 150, 'rewardPerEpisode': 135.27608558526723
'totalSteps': 12800, 'rewardStep': 0.5199786067126317, 'errorList': [], 'lossList': [0.0, -1.382176171541214, 0.0, 49.6063078212738, 0.0, 0.0, 0.0], 'rewardMean': 0.7022605877895373, 'totalEpisodes': 212, 'stepsPerEpisode': 117, 'rewardPerEpisode': 88.20950773569082
'totalSteps': 14080, 'rewardStep': 0.588768018862082, 'errorList': [], 'lossList': [0.0, -1.3788232952356339, 0.0, 8.232705763578416, 0.0, 0.0, 0.0], 'rewardMean': 0.7030378525372806, 'totalEpisodes': 217, 'stepsPerEpisode': 152, 'rewardPerEpisode': 118.64565934588104
'totalSteps': 15360, 'rewardStep': 0.5150936618774251, 'errorList': [], 'lossList': [0.0, -1.3725698870420455, 0.0, 9.084793559312821, 0.0, 0.0, 0.0], 'rewardMean': 0.6720897830672241, 'totalEpisodes': 220, 'stepsPerEpisode': 539, 'rewardPerEpisode': 369.07069999272466
'totalSteps': 16640, 'rewardStep': 0.5813762681899313, 'errorList': [], 'lossList': [0.0, -1.3707004475593567, 0.0, 4.556597719788551, 0.0, 0.0, 0.0], 'rewardMean': 0.6564245291831602, 'totalEpisodes': 220, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 842.653575209574
'totalSteps': 17920, 'rewardStep': 0.5348760484528048, 'errorList': [], 'lossList': [0.0, -1.3659332364797592, 0.0, 5.6567373341321945, 0.0, 0.0, 0.0], 'rewardMean': 0.6202766343317403, 'totalEpisodes': 222, 'stepsPerEpisode': 366, 'rewardPerEpisode': 264.3101651940686
'totalSteps': 19200, 'rewardStep': 0.8199600289785276, 'errorList': [], 'lossList': [0.0, -1.3730018705129623, 0.0, 6.407617312073707, 0.0, 0.0, 0.0], 'rewardMean': 0.6261974073992322, 'totalEpisodes': 223, 'stepsPerEpisode': 232, 'rewardPerEpisode': 182.1464242580094
'totalSteps': 20480, 'rewardStep': 0.7648882273785909, 'errorList': [], 'lossList': [0.0, -1.3473576045036315, 0.0, 2.271462295651436, 0.0, 0.0, 0.0], 'rewardMean': 0.6053707213982735, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 939.1508899963795
'totalSteps': 21760, 'rewardStep': 0.8544200165162952, 'errorList': [], 'lossList': [0.0, -1.3235098367929459, 0.0, 1.1566767895966767, 0.0, 0.0, 0.0], 'rewardMean': 0.6493728308477673, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1016.360884178653
'totalSteps': 23040, 'rewardStep': 0.7427912590536552, 'errorList': [], 'lossList': [0.0, -1.3008556115627288, 0.0, 1.005966298468411, 0.0, 0.0, 0.0], 'rewardMean': 0.6720271852091237, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1109.04419983015
'totalSteps': 24320, 'rewardStep': 0.8660461449821344, 'errorList': [], 'lossList': [0.0, -1.2511480343341828, 0.0, 1.0471154156327247, 0.0, 0.0, 0.0], 'rewardMean': 0.6788198281004079, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1139.1104049701198
'totalSteps': 25600, 'rewardStep': 0.9390111674079724, 'errorList': [0.0433498460675002, 0.05784589061633474, 0.012976286882611002, 0.012916669008352293, 0.02041731785797747, 0.057730793767611664, 0.05383610753091574, 0.01663506357227025, 0.021619293409319482, 0.004743178541915169, 0.01717048619192332, 0.035961854227343035, 0.04687657572361478, 0.09185406570105768, 0.019409684324097683, 0.024618699491950082, 0.013257087782556095, 0.03962988546871993, 0.0634533052521232, 0.07973951551110607, 0.09705922515689218, 0.04632270471549925, 0.04068411130993518, 0.004602516628611594, 0.10281116586750033, 0.02054292725454485, 0.07011463242189521, 0.06625429902717085, 0.019417002975563945, 0.047085453236456444, 0.04187316245203865, 0.08112442260740872, 0.044015488979090446, 0.026144697662965743, 0.014565436158373237, 0.03824886301609209, 0.09824064822728507, 0.017925185857645284, 0.07464474489245079, 0.0229537603980538, 0.035516799587330214, 0.04293353040463188, 0.043497446191068496, 0.07589782158091064, 0.09151244265000813, 0.038333300690489146, 0.03160744334738249, 0.008916400623508864, 0.013107228586430465, 0.02723714468073471], 'lossList': [0.0, -1.2285528141260147, 0.0, 0.8890226441062987, 0.0, 0.0, 0.0], 'rewardMean': 0.7207230841699419, 'totalEpisodes': 223, 'stepsPerEpisode': 1280, 'rewardPerEpisode': 1180.996344538814, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=25600, timeSpent=37.14
