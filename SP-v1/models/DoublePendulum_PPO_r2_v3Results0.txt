#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 2, 'evaluationSteps': 5000, 'episodeSteps': 1000, 'episodeStepsMax': 1250, 'totalLearningSteps': 30000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 4, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 40, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models/DoublePendulum_PPO_r2_v3Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': False, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models/DoublePendulum_PPO_r2_v3Results', 'verbose': True}
#
'totalSteps': 1250, 'rewardStep': 0.8725047489570668, 'errorList': [], 'lossList': [0.0, -1.3793825513124467, 0.0, 3.860846892595291, 0.0, 0.0, 0.0], 'rewardMean': 0.8725047489570668, 'totalEpisodes': 317, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.79749920016233
'totalSteps': 2500, 'rewardStep': 0.9292757436269389, 'errorList': [], 'lossList': [0.0, -1.3023039370775222, 0.0, 7.444335888624192, 0.0, 0.0, 0.0], 'rewardMean': 0.9008902462920028, 'totalEpisodes': 573, 'stepsPerEpisode': 8, 'rewardPerEpisode': 7.429181366235642
'totalSteps': 3750, 'rewardStep': 0.6873511760035129, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.7941207111477577, 'totalEpisodes': 770, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.1317201571782958
'totalSteps': 5000, 'rewardStep': 0.8213351246843034, 'errorList': [], 'lossList': [0.0, -1.2442991495132447, 0.0, 10.356476728916169, 0.0, 0.0, 0.0], 'rewardMean': 0.7995635938550668, 'totalEpisodes': 976, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.668897786145851
'totalSteps': 6250, 'rewardStep': 0.8919611022580999, 'errorList': [], 'lossList': [0.0, -1.207974436879158, 0.0, 18.157536249160767, 0.0, 0.0, 0.0], 'rewardMean': 0.8149631785889057, 'totalEpisodes': 1116, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.8919611022580999
'totalSteps': 7500, 'rewardStep': 0.9045360159842815, 'errorList': [], 'lossList': [0.0, -1.1763502794504166, 0.0, 24.971228189468384, 0.0, 0.0, 0.0], 'rewardMean': 0.8277592982168166, 'totalEpisodes': 1220, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.738103448366779
'totalSteps': 8750, 'rewardStep': 0.8305327659845722, 'errorList': [], 'lossList': [0.0, -1.1446052032709122, 0.0, 34.61876009941101, 0.0, 0.0, 0.0], 'rewardMean': 0.828105981687786, 'totalEpisodes': 1290, 'stepsPerEpisode': 11, 'rewardPerEpisode': 9.137116760531654
'totalSteps': 10000, 'rewardStep': 0.8634251887436402, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.8351698230989569, 'totalEpisodes': 1364, 'stepsPerEpisode': 10, 'rewardPerEpisode': 8.462955208700782
'totalSteps': 11250, 'rewardStep': 0.8182666776657055, 'errorList': [], 'lossList': [0.0, -1.1220573151111604, 0.0, 37.83129016876221, 0.0, 0.0, 0.0], 'rewardMean': 0.8297460159698208, 'totalEpisodes': 1442, 'stepsPerEpisode': 33, 'rewardPerEpisode': 28.481688427121366
'totalSteps': 12500, 'rewardStep': 0.9422987326292438, 'errorList': [], 'lossList': [0.0, -1.1044413936138153, 0.0, 42.03220543861389, 0.0, 0.0, 0.0], 'rewardMean': 0.8310483148700513, 'totalEpisodes': 1504, 'stepsPerEpisode': 1, 'rewardPerEpisode': 0.9422987326292438
'totalSteps': 13750, 'rewardStep': 0.8216728927483973, 'errorList': [], 'lossList': [0.0, -1.0859168845415115, 0.0, 46.49568892478943, 0.0, 0.0, 0.0], 'rewardMean': 0.8444804865445397, 'totalEpisodes': 1553, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.438126404662758
'totalSteps': 15000, 'rewardStep': 0.8389451539177425, 'errorList': [], 'lossList': [0.0, -1.063809221982956, 0.0, 51.11532163619995, 0.0, 0.0, 0.0], 'rewardMean': 0.8596398843359626, 'totalEpisodes': 1598, 'stepsPerEpisode': 34, 'rewardPerEpisode': 30.770511917642587
'totalSteps': 16250, 'rewardStep': 0.7529688923003831, 'errorList': [], 'lossList': [0.0, -1.0455908477306366, 0.0, 57.81680606842041, 0.0, 0.0, 0.0], 'rewardMean': 0.8528032610975705, 'totalEpisodes': 1655, 'stepsPerEpisode': 10, 'rewardPerEpisode': 7.786131297628552
'totalSteps': 17500, 'rewardStep': 0.8963216231614245, 'errorList': [], 'lossList': [0.0, -1.0287850153446199, 0.0, 51.48872373580932, 0.0, 0.0, 0.0], 'rewardMean': 0.8532393131879031, 'totalEpisodes': 1707, 'stepsPerEpisode': 51, 'rewardPerEpisode': 44.78048762267279
'totalSteps': 18750, 'rewardStep': 0.7150217653187048, 'errorList': [], 'lossList': [0.0, -1.0174241733551026, 0.0, 43.58644494056702, 0.0, 0.0, 0.0], 'rewardMean': 0.8342878881213454, 'totalEpisodes': 1754, 'stepsPerEpisode': 49, 'rewardPerEpisode': 41.076510304035594
'totalSteps': 20000, 'rewardStep': 0.9475060350196705, 'errorList': [], 'lossList': [0.0, -1.0145374757051469, 0.0, 28.510515918731688, 0.0, 0.0, 0.0], 'rewardMean': 0.8459852150248551, 'totalEpisodes': 1797, 'stepsPerEpisode': 46, 'rewardPerEpisode': 40.98934543345344
'totalSteps': 21250, 'rewardStep': 0.8185137500266242, 'errorList': [], 'lossList': [0.0, -1.0047095102071761, 0.0, 23.98813953399658, 0.0, 0.0, 0.0], 'rewardMean': 0.8414940711531536, 'totalEpisodes': 1845, 'stepsPerEpisode': 34, 'rewardPerEpisode': 29.322034221429895
'totalSteps': 22500, 'rewardStep': 0.6592212581887669, 'errorList': [], 'lossList': [0.0, -0.9858382198214531, 0.0, 16.316153659820557, 0.0, 0.0, 0.0], 'rewardMean': 0.8210736780976664, 'totalEpisodes': 1894, 'stepsPerEpisode': 30, 'rewardPerEpisode': 23.029185412579245
'totalSteps': 23750, 'rewardStep': 0.8432390847224225, 'errorList': [], 'lossList': [0.0, -0.9754500004649163, 0.0, 19.99422010421753, 0.0, 0.0, 0.0], 'rewardMean': 0.8235709188033379, 'totalEpisodes': 1938, 'stepsPerEpisode': 15, 'rewardPerEpisode': 12.681661862816732
'totalSteps': 25000, 'rewardStep': 0.8393083843133311, 'errorList': [], 'lossList': [0.0, -0.9623840379714966, 0.0, 22.633498239517213, 0.0, 0.0, 0.0], 'rewardMean': 0.8132718839717468, 'totalEpisodes': 1979, 'stepsPerEpisode': 26, 'rewardPerEpisode': 23.194210944984047
'totalSteps': 26250, 'rewardStep': 0.738998434996199, 'errorList': [], 'lossList': [0.0, -0.9515255054831505, 0.0, 13.080641798973083, 0.0, 0.0, 0.0], 'rewardMean': 0.805004438196527, 'totalEpisodes': 2024, 'stepsPerEpisode': 12, 'rewardPerEpisode': 10.084460069953236
'totalSteps': 27500, 'rewardStep': 0.9168587209499162, 'errorList': [], 'lossList': [0.0, -0.940661928653717, 0.0, 16.01072475910187, 0.0, 0.0, 0.0], 'rewardMean': 0.8127957948997443, 'totalEpisodes': 2062, 'stepsPerEpisode': 24, 'rewardPerEpisode': 22.135905214487067
'totalSteps': 28750, 'rewardStep': 0.7395118288297923, 'errorList': [], 'lossList': [0.0, -0.9259021526575089, 0.0, 12.836956677436829, 0.0, 0.0, 0.0], 'rewardMean': 0.8114500885526852, 'totalEpisodes': 2103, 'stepsPerEpisode': 29, 'rewardPerEpisode': 23.240981929864493
'totalSteps': 30000, 'rewardStep': 0.8925255926723186, 'errorList': [], 'lossList': [0.0, -0.913113523721695, 0.0, 11.875675232410432, 0.0, 0.0, 0.0], 'rewardMean': 0.8110704855037746, 'totalEpisodes': 2144, 'stepsPerEpisode': 51, 'rewardPerEpisode': 43.63024858669312
'totalSteps': 31250, 'rewardStep': 0.6812846390658669, 'errorList': [], 'lossList': [0.0, -0.9011310249567032, 0.0, 18.165171637535096, 0.0, 0.0, 0.0], 'rewardMean': 0.8076967728784907, 'totalEpisodes': 2171, 'stepsPerEpisode': 63, 'rewardPerEpisode': 50.46407460534521
#less than 4 successful tests, storing failed model
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=19.19
