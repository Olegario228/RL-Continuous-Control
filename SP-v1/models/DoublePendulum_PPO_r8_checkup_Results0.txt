#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 2, 'evaluationSteps': 5000, 'episodeSteps': 1000, 'episodeStepsMax': 1250, 'totalLearningSteps': 30000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'PPO', 'rewardMode': 8, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 40, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models/DoublePendulum_PPO_r8_checkup_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': False, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models/DoublePendulum_PPO_r8_checkup_Results', 'verbose': True}
#
'totalSteps': 1250, 'rewardStep': 0.6434619691094217, 'errorList': [], 'lossList': [0.0, -1.3767597329616548, 0.0, 2.7865394124388696, 0.0, 0.0, 0.0], 'rewardMean': 0.6434619691094217, 'totalEpisodes': 317, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.3544876140389817
'totalSteps': 2500, 'rewardStep': 0.6878853025510163, 'errorList': [], 'lossList': [0.0, -1.2955310678482055, 0.0, 4.670158071517944, 0.0, 0.0, 0.0], 'rewardMean': 0.665673635830219, 'totalEpisodes': 580, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.004370823384322
'totalSteps': 3750, 'rewardStep': 0.6824156182000055, 'errorList': [], 'lossList': [0.0, -1.2388389670848847, 0.0, 7.651588861942291, 0.0, 0.0, 0.0], 'rewardMean': 0.6712542966201478, 'totalEpisodes': 772, 'stepsPerEpisode': 3, 'rewardPerEpisode': 2.140100273361145
'totalSteps': 5000, 'rewardStep': 0.6153910409836385, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6489089943655441, 'totalEpisodes': 940, 'stepsPerEpisode': 4, 'rewardPerEpisode': 3.235394965173923
'totalSteps': 6250, 'rewardStep': 0.6069969438046151, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6369341227767072, 'totalEpisodes': 1094, 'stepsPerEpisode': 2, 'rewardPerEpisode': 1.2968789690729796
'totalSteps': 7500, 'rewardStep': 0.6823036447830807, 'errorList': [], 'lossList': [0.0, -1.2043167757987976, 0.0, 14.315797972679139, 0.0, 0.0, 0.0], 'rewardMean': 0.642605313027504, 'totalEpisodes': 1223, 'stepsPerEpisode': 14, 'rewardPerEpisode': 10.750988799129384
'totalSteps': 8750, 'rewardStep': 0.8802511728189554, 'errorList': [], 'lossList': [0.0, -1.1714101344347, 0.0, 17.6157110786438, 0.0, 0.0, 0.0], 'rewardMean': 0.6690104085598875, 'totalEpisodes': 1319, 'stepsPerEpisode': 11, 'rewardPerEpisode': 8.560333864705285
'totalSteps': 10000, 'rewardStep': 0.6276040861058246, 'errorList': [], 'lossList': [0.0, -1.1385897755622865, 0.0, 20.19604593753815, 0.0, 0.0, 0.0], 'rewardMean': 0.6648697763144812, 'totalEpisodes': 1417, 'stepsPerEpisode': 7, 'rewardPerEpisode': 5.358628790701492
'totalSteps': 11250, 'rewardStep': 0.7015447010207274, 'errorList': [], 'lossList': [0.0, -1.1072936105728148, 0.0, 23.104126167297363, 0.0, 0.0, 0.0], 'rewardMean': 0.6706780495056117, 'totalEpisodes': 1496, 'stepsPerEpisode': 21, 'rewardPerEpisode': 16.307093773391816
'totalSteps': 12500, 'rewardStep': 0.7376067048098865, 'errorList': [], 'lossList': [0.0, -1.084935269355774, 0.0, 30.489331665039064, 0.0, 0.0, 0.0], 'rewardMean': 0.6756501897314987, 'totalEpisodes': 1550, 'stepsPerEpisode': 19, 'rewardPerEpisode': 14.708148175474
'totalSteps': 13750, 'rewardStep': 0.6129935858311522, 'errorList': [], 'lossList': [0.0, -1.063617644906044, 0.0, 36.98971777915955, 0.0, 0.0, 0.0], 'rewardMean': 0.6687079864946133, 'totalEpisodes': 1594, 'stepsPerEpisode': 24, 'rewardPerEpisode': 17.08098098869029
'totalSteps': 15000, 'rewardStep': 0.6316921692520565, 'errorList': [], 'lossList': [0.0, -1.0484618771076202, 0.0, 36.16350433349609, 0.0, 0.0, 0.0], 'rewardMean': 0.6703380993214552, 'totalEpisodes': 1646, 'stepsPerEpisode': 37, 'rewardPerEpisode': 27.964474791244037
'totalSteps': 16250, 'rewardStep': 0.7841157158446656, 'errorList': [], 'lossList': [0.0, -1.0382543689012527, 0.0, 28.54865132331848, 0.0, 0.0, 0.0], 'rewardMean': 0.6872105668075579, 'totalEpisodes': 1691, 'stepsPerEpisode': 49, 'rewardPerEpisode': 33.45993816122281
'totalSteps': 17500, 'rewardStep': 0.651922283942527, 'errorList': [], 'lossList': [0.0, -1.0314793395996094, 0.0, 32.98885648727417, 0.0, 0.0, 0.0], 'rewardMean': 0.691703100821349, 'totalEpisodes': 1737, 'stepsPerEpisode': 26, 'rewardPerEpisode': 19.53142202338877
'totalSteps': 18750, 'rewardStep': 0.7137990371617653, 'errorList': [], 'lossList': [0.0, -1.02472183406353, 0.0, 19.719687361717224, 0.0, 0.0, 0.0], 'rewardMean': 0.7023833101570641, 'totalEpisodes': 1776, 'stepsPerEpisode': 5, 'rewardPerEpisode': 4.0558477830979545
'totalSteps': 20000, 'rewardStep': 0.8643173000887985, 'errorList': [], 'lossList': [0.0, -1.014263809323311, 0.0, 19.36812340259552, 0.0, 0.0, 0.0], 'rewardMean': 0.7205846756876358, 'totalEpisodes': 1819, 'stepsPerEpisode': 21, 'rewardPerEpisode': 15.276593378365286
'totalSteps': 21250, 'rewardStep': 0.6549939142590875, 'errorList': [], 'lossList': [0.0, -0.9994797801971436, 0.0, 18.1833397769928, 0.0, 0.0, 0.0], 'rewardMean': 0.6980589498316491, 'totalEpisodes': 1858, 'stepsPerEpisode': 37, 'rewardPerEpisode': 28.759995361943357
'totalSteps': 22500, 'rewardStep': 0.6633298135912377, 'errorList': [], 'lossList': [0.0, -0.9862455180287362, 0.0, 11.41811995267868, 0.0, 0.0, 0.0], 'rewardMean': 0.7016315225801903, 'totalEpisodes': 1895, 'stepsPerEpisode': 51, 'rewardPerEpisode': 37.22829483018393
'totalSteps': 23750, 'rewardStep': 0.7836789712655494, 'errorList': [], 'lossList': [0.0, -0.977047430574894, 0.0, 14.65219419002533, 0.0, 0.0, 0.0], 'rewardMean': 0.7098449496046728, 'totalEpisodes': 1941, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.0301265644278725
'totalSteps': 25000, 'rewardStep': 0.6157676021927851, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6979384409791258, 'totalEpisodes': 1980, 'stepsPerEpisode': 22, 'rewardPerEpisode': 16.62852729863407
'totalSteps': 26250, 'rewardStep': 0.7029202661849655, 'errorList': [], 'lossList': [0.0, -0.972437354028225, 0.0, 13.512422001361847, 0.0, 0.0, 0.0], 'rewardMean': 0.7050612506724167, 'totalEpisodes': 2018, 'stepsPerEpisode': 9, 'rewardPerEpisode': 6.86339432085971
'totalSteps': 27500, 'rewardStep': 0.6725402806124687, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.6959655068161912, 'totalEpisodes': 2055, 'stepsPerEpisode': 40, 'rewardPerEpisode': 31.19660885249498
'totalSteps': 28750, 'rewardStep': 0.8453812770291833, 'errorList': [], 'lossList': [0.0, -0.9630582001805306, 0.0, 7.5922626423835755, 0.0, 0.0, 0.0], 'rewardMean': 0.709123730802933, 'totalEpisodes': 2096, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.915205868860075
'totalSteps': 30000, 'rewardStep': 0.8328741351183548, 'errorList': [], 'lossList': [0.0, -0.9494765099883079, 0.0, 12.073055894374848, 0.0, 0.0, 0.0], 'rewardMean': 0.7059794143058886, 'totalEpisodes': 2129, 'stepsPerEpisode': 57, 'rewardPerEpisode': 43.50021002659965
'totalSteps': 31250, 'rewardStep': 0.7265993558341106, 'errorList': [], 'lossList': [0.0, -0.941344385445118, 0.0, 13.82300250530243, 0.0, 0.0, 0.0], 'rewardMean': 0.7131399584633911, 'totalEpisodes': 2169, 'stepsPerEpisode': 36, 'rewardPerEpisode': 26.26250200708553
#less than 4 successful tests, storing failed model
#maxSuccessfulTests=0, maxSuccessfulTestsAtStep=-1, timeSpent=20.62
