#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 5000, 'episodeSteps': 1000, 'episodeStepsMax': 1250, 'totalLearningSteps': 60000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'A2C', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models/SP_A2C_r2_v4_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': False, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models/SP_A2C_r2_v4_Results', 'verbose': True}
#
'totalSteps': 1052, 'rewardStep': 0.3284889557403515, 'errorList': [], 'lossList': [0.0, -1.384339451789856, 1.716070532798767, 1.3879998922348022, 0.0, 0.0, 0.0], 'rewardMean': 0.3284889557403515, 'totalEpisodes': 47, 'stepsPerEpisode': 90, 'rewardPerEpisode': 68.05377573262285
'totalSteps': 2090, 'rewardStep': 0.3937662046711087, 'errorList': [], 'lossList': [0.0, -1.3854541778564453, 1.158376932144165, 0.8455918431282043, 0.0, 0.0, 0.0], 'rewardMean': 0.3611275802057301, 'totalEpisodes': 66, 'stepsPerEpisode': 88, 'rewardPerEpisode': 73.95304386031074
'totalSteps': 3137, 'rewardStep': 0.31479375877277127, 'errorList': [], 'lossList': [0.0, -1.388951063156128, 0.4883878827095032, 0.21342697739601135, 0.0, 0.0, 0.0], 'rewardMean': 0.34568297306141044, 'totalEpisodes': 82, 'stepsPerEpisode': 68, 'rewardPerEpisode': 47.148250167495235
'totalSteps': 4173, 'rewardStep': 0.2830127425166715, 'errorList': [], 'lossList': [0.0, -1.3871058225631714, -0.3049503266811371, 0.041891105473041534, 0.0, 0.0, 0.0], 'rewardMean': 0.3300154154252257, 'totalEpisodes': 89, 'stepsPerEpisode': 129, 'rewardPerEpisode': 103.35176468080226
'totalSteps': 5191, 'rewardStep': 0.30481265276895364, 'errorList': [], 'lossList': [0.0, -1.3907535076141357, 0.05031811445951462, 0.005539822392165661, 0.0, 0.0, 0.0], 'rewardMean': 0.3249748628939713, 'totalEpisodes': 97, 'stepsPerEpisode': 290, 'rewardPerEpisode': 206.61024771434745
'totalSteps': 6441, 'rewardStep': 0.8951979182733689, 'errorList': [], 'lossList': [0.0, -1.3553657531738281, 1.9259401559829712, 1.1105191707611084, 0.0, 0.0, 0.0], 'rewardMean': 0.38164575914727306, 'totalEpisodes': 103, 'stepsPerEpisode': 367, 'rewardPerEpisode': 309.71621853117693
'totalSteps': 7691, 'rewardStep': 0.6481832601541726, 'errorList': [], 'lossList': [0.0, -1.3334070444107056, 0.004754642955958843, 0.0004374893323983997, 0.0, 0.0, 0.0], 'rewardMean': 0.4136151895886552, 'totalEpisodes': 104, 'stepsPerEpisode': 302, 'rewardPerEpisode': 231.30167207222837
'totalSteps': 8783, 'rewardStep': 0.3508979120941408, 'errorList': [], 'lossList': [0.0, -1.3159551620483398, -3.8635833263397217, 16.616975784301758, 0.0, 0.0, 0.0], 'rewardMean': 0.4050415310732616, 'totalEpisodes': 108, 'stepsPerEpisode': 299, 'rewardPerEpisode': 217.19964336019416
'totalSteps': 9875, 'rewardStep': 0.31834817144862904, 'errorList': [], 'lossList': [0.0, -1.3039453029632568, 4.488714694976807, 5.896407127380371, 0.0, 0.0, 0.0], 'rewardMean': 0.4057524136084331, 'totalEpisodes': 112, 'stepsPerEpisode': 293, 'rewardPerEpisode': 223.40373548815904
'totalSteps': 10967, 'rewardStep': 0.21320306922287846, 'errorList': [], 'lossList': [0.0, -1.2950332164764404, -2.1921353340148926, 2.7289481163024902, 0.0, 0.0, 0.0], 'rewardMean': 0.39179047894967456, 'totalEpisodes': 116, 'stepsPerEpisode': 376, 'rewardPerEpisode': 292.9000179237203
'totalSteps': 12097, 'rewardStep': 0.4719488892349841, 'errorList': [], 'lossList': [0.0, -1.28846275806427, 2.7938055992126465, 10.100578308105469, 0.0, 0.0, 0.0], 'rewardMean': 0.4252177262428806, 'totalEpisodes': 121, 'stepsPerEpisode': 343, 'rewardPerEpisode': 262.79514027157035
'totalSteps': 13347, 'rewardStep': 0.7735038219479579, 'errorList': [], 'lossList': [0.0, -1.272827386856079, -2.424574375152588, 3.200241804122925, 0.0, 0.0, 0.0], 'rewardMean': 0.41304831661033947, 'totalEpisodes': 125, 'stepsPerEpisode': 325, 'rewardPerEpisode': 288.15365617886664
'totalSteps': 14597, 'rewardStep': 0.9112465504635413, 'errorList': [], 'lossList': [0.0, -1.2737005949020386, 0.16353824734687805, 0.022456970065832138, 0.0, 0.0, 0.0], 'rewardMean': 0.4393546456412764, 'totalEpisodes': 128, 'stepsPerEpisode': 357, 'rewardPerEpisode': 317.4370438463674
'totalSteps': 15788, 'rewardStep': 0.3917493416778485, 'errorList': [], 'lossList': [0.0, -1.264522910118103, 2.9284839630126953, 4.399756908416748, 0.0, 0.0, 0.0], 'rewardMean': 0.447524931558018, 'totalEpisodes': 132, 'stepsPerEpisode': 371, 'rewardPerEpisode': 281.27402002709795
'totalSteps': 16920, 'rewardStep': 0.36621607183459914, 'errorList': [], 'lossList': [0.0, -1.2632540464401245, 0.6918529868125916, 0.734540581703186, 0.0, 0.0, 0.0], 'rewardMean': 0.45709851163521203, 'totalEpisodes': 134, 'stepsPerEpisode': 695, 'rewardPerEpisode': 532.4989762198109
'totalSteps': 18170, 'rewardStep': 0.7678432753537917, 'errorList': [], 'lossList': [0.0, -1.2690951824188232, -0.43989795446395874, 0.14130668342113495, 0.0, 0.0, 0.0], 'rewardMean': 0.5125625322483033, 'totalEpisodes': 134, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1101.1694813865297
'totalSteps': 19420, 'rewardStep': 0.9411030618241764, 'errorList': [], 'lossList': [0.0, -1.2303651571273804, 0.12101602554321289, 0.006960754282772541, 0.0, 0.0, 0.0], 'rewardMean': 0.585352531508433, 'totalEpisodes': 134, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1150.902669477291
'totalSteps': 20670, 'rewardStep': 0.9376281228814967, 'errorList': [], 'lossList': [0.0, -1.1906362771987915, 0.012238672003149986, 0.0011082964483648539, 0.0, 0.0, 0.0], 'rewardMean': 0.6319204548730843, 'totalEpisodes': 134, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1152.1930066428374
'totalSteps': 21920, 'rewardStep': 0.7473688492931586, 'errorList': [], 'lossList': [0.0, -1.1872531175613403, 1.4309141635894775, 1.3108265399932861, 0.0, 0.0, 0.0], 'rewardMean': 0.6594624508789018, 'totalEpisodes': 134, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1104.589468159031
'totalSteps': 23170, 'rewardStep': 0.8603063042672526, 'errorList': [], 'lossList': [0.0, -1.196523666381836, -0.17780528962612152, 0.030760610476136208, 0.0, 0.0, 0.0], 'rewardMean': 0.6681426991108312, 'totalEpisodes': 135, 'stepsPerEpisode': 933, 'rewardPerEpisode': 767.3801413692635
'totalSteps': 24420, 'rewardStep': 0.8584752231460399, 'errorList': [], 'lossList': [0.0, -1.168535828590393, -0.01449816208332777, 0.00027258056798018515, 0.0, 0.0, 0.0], 'rewardMean': 0.6628655663790812, 'totalEpisodes': 135, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1075.3731578591567
