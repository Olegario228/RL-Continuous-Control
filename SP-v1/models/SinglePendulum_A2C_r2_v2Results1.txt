#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 5000, 'episodeSteps': 1000, 'episodeStepsMax': 1250, 'totalLearningSteps': 200000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'A2C', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models/SinglePendulum_A2C_r2_v2Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models/SinglePendulum_A2C_r2_v2Results', 'verbose': True}
#
'totalSteps': 1021, 'rewardStep': 0.43834985586539693, 'errorList': [], 'lossList': [0.0, -1.4089202880859375, 1.2720811367034912, 1.5618101358413696, 0.0, 0.0, 0.0], 'rewardMean': 0.43834985586539693, 'totalEpisodes': 53, 'stepsPerEpisode': 57, 'rewardPerEpisode': 44.27397806428217
'totalSteps': 2029, 'rewardStep': 0.4761212613312595, 'errorList': [], 'lossList': [0.0, -1.3856420516967773, 1.0651507377624512, 0.8596129417419434, 0.0, 0.0, 0.0], 'rewardMean': 0.4572355585983282, 'totalEpisodes': 74, 'stepsPerEpisode': 20, 'rewardPerEpisode': 12.604158825866223
'totalSteps': 3133, 'rewardStep': 0.09189847604477469, 'errorList': [], 'lossList': [0.0, -1.3878602981567383, -0.24648785591125488, 0.04492747411131859, 0.0, 0.0, 0.0], 'rewardMean': 0.33545653108047707, 'totalEpisodes': 84, 'stepsPerEpisode': 379, 'rewardPerEpisode': 255.60479327218076
'totalSteps': 4174, 'rewardStep': 0.419085403322903, 'errorList': [], 'lossList': [0.0, -1.3728492259979248, 0.1745365858078003, 0.03162802383303642, 0.0, 0.0, 0.0], 'rewardMean': 0.35636374914108354, 'totalEpisodes': 92, 'stepsPerEpisode': 195, 'rewardPerEpisode': 137.8924954914232
'totalSteps': 5310, 'rewardStep': 0.30716920399544523, 'errorList': [], 'lossList': [0.0, -1.359729290008545, -0.5979306697845459, 0.2976420223712921, 0.0, 0.0, 0.0], 'rewardMean': 0.3465248401119559, 'totalEpisodes': 99, 'stepsPerEpisode': 483, 'rewardPerEpisode': 300.6105737761503
'totalSteps': 6560, 'rewardStep': 0.5495353623202336, 'errorList': [], 'lossList': [0.0, -1.3567473888397217, -0.056545842438936234, 0.0028863227926194668, 0.0, 0.0, 0.0], 'rewardMean': 0.35764339075743956, 'totalEpisodes': 106, 'stepsPerEpisode': 280, 'rewardPerEpisode': 217.50960079591283
'totalSteps': 7810, 'rewardStep': 0.6717151954964752, 'errorList': [], 'lossList': [0.0, -1.3448747396469116, 0.05171225219964981, 0.001634165644645691, 0.0, 0.0, 0.0], 'rewardMean': 0.38097992472054737, 'totalEpisodes': 110, 'stepsPerEpisode': 319, 'rewardPerEpisode': 262.03384243898427
'totalSteps': 9060, 'rewardStep': 0.451513968169201, 'errorList': [], 'lossList': [0.0, -1.3174456357955933, -1.1509361267089844, 0.5654299855232239, 0.0, 0.0, 0.0], 'rewardMean': 0.37851919540434154, 'totalEpisodes': 112, 'stepsPerEpisode': 294, 'rewardPerEpisode': 203.12745522858785
'totalSteps': 10132, 'rewardStep': 0.42470758139642695, 'errorList': [], 'lossList': [0.0, -1.3045263290405273, -1.3111343383789062, 8.788146018981934, 0.0, 0.0, 0.0], 'rewardMean': 0.4066587379460234, 'totalEpisodes': 118, 'stepsPerEpisode': 130, 'rewardPerEpisode': 84.7933093331659
'totalSteps': 11294, 'rewardStep': 0.2879573807577053, 'errorList': [], 'lossList': [0.0, -1.2860184907913208, -2.2770371437072754, 2.623626232147217, 0.0, 0.0, 0.0], 'rewardMean': 0.4131518261607968, 'totalEpisodes': 124, 'stepsPerEpisode': 222, 'rewardPerEpisode': 150.19906264660153
'totalSteps': 12523, 'rewardStep': 0.07819590580894986, 'errorList': [], 'lossList': [0.0, -1.2593538761138916, -1.727452278137207, 3.914931058883667, 0.0, 0.0, 0.0], 'rewardMean': 0.3561655465907519, 'totalEpisodes': 126, 'stepsPerEpisode': 611, 'rewardPerEpisode': 480.6890288913736
'totalSteps': 13636, 'rewardStep': 0.4844884009276359, 'errorList': [], 'lossList': [0.0, -1.2430955171585083, -0.8717620968818665, 1.1163842678070068, 0.0, 0.0, 0.0], 'rewardMean': 0.36739277014471117, 'totalEpisodes': 133, 'stepsPerEpisode': 279, 'rewardPerEpisode': 219.2397609633144
'totalSteps': 14749, 'rewardStep': 0.16196341370683787, 'errorList': [], 'lossList': [0.0, -1.2385209798812866, -1.7980544567108154, 2.940599203109741, 0.0, 0.0, 0.0], 'rewardMean': 0.28746253651951115, 'totalEpisodes': 137, 'stepsPerEpisode': 218, 'rewardPerEpisode': 160.05782758757783
'totalSteps': 15823, 'rewardStep': 0.2776920573823817, 'errorList': [], 'lossList': [0.0, -1.256189227104187, -1.284523367881775, 2.147761344909668, 0.0, 0.0, 0.0], 'rewardMean': 0.2580594317167021, 'totalEpisodes': 142, 'stepsPerEpisode': 125, 'rewardPerEpisode': 75.28789483305538
'totalSteps': 16833, 'rewardStep': 0.109388126200201, 'errorList': [], 'lossList': [0.0, -1.2383553981781006, 1.6673448085784912, 4.6615705490112305, 0.0, 0.0, 0.0], 'rewardMean': 0.22234558080520123, 'totalEpisodes': 147, 'stepsPerEpisode': 225, 'rewardPerEpisode': 157.85244956778848
'totalSteps': 18034, 'rewardStep': 0.18862300574026114, 'errorList': [], 'lossList': [0.0, -1.2257364988327026, -1.680354356765747, 3.7813496589660645, 0.0, 0.0, 0.0], 'rewardMean': 0.24443100079146357, 'totalEpisodes': 151, 'stepsPerEpisode': 209, 'rewardPerEpisode': 149.2460429365681
'totalSteps': 19284, 'rewardStep': 0.5313981788528219, 'errorList': [], 'lossList': [0.0, -1.2132656574249268, -0.8002179861068726, 0.709703803062439, 0.0, 0.0, 0.0], 'rewardMean': 0.24912197858398213, 'totalEpisodes': 155, 'stepsPerEpisode': 351, 'rewardPerEpisode': 301.036944015709
'totalSteps': 20294, 'rewardStep': 0.36407865628296887, 'errorList': [], 'lossList': [0.0, -1.21671724319458, -1.282250165939331, 1.7529712915420532, 0.0, 0.0, 0.0], 'rewardMean': 0.2572925283771285, 'totalEpisodes': 161, 'stepsPerEpisode': 221, 'rewardPerEpisode': 167.30638954755509
'totalSteps': 21496, 'rewardStep': 0.4447253069042625, 'errorList': [], 'lossList': [0.0, -1.21457040309906, 0.7947254776954651, 2.3932900428771973, 0.0, 0.0, 0.0], 'rewardMean': 0.30227204264905905, 'totalEpisodes': 164, 'stepsPerEpisode': 459, 'rewardPerEpisode': 362.7321683499914
'totalSteps': 22746, 'rewardStep': 0.5639880245470733, 'errorList': [], 'lossList': [0.0, -1.2008532285690308, -0.5940573215484619, 0.6585467457771301, 0.0, 0.0, 0.0], 'rewardMean': 0.33090163936552824, 'totalEpisodes': 166, 'stepsPerEpisode': 301, 'rewardPerEpisode': 243.46839922264164
'totalSteps': 23996, 'rewardStep': 0.7745675994137406, 'errorList': [], 'lossList': [0.0, -1.168861985206604, -0.5699560642242432, 0.10283617675304413, 0.0, 0.0, 0.0], 'rewardMean': 0.3974195866868822, 'totalEpisodes': 168, 'stepsPerEpisode': 570, 'rewardPerEpisode': 521.7379806813428
'totalSteps': 25246, 'rewardStep': 0.9370477649058733, 'errorList': [], 'lossList': [0.0, -1.1602060794830322, 0.10741521418094635, 0.025944624096155167, 0.0, 0.0, 0.0], 'rewardMean': 0.4801855505574494, 'totalEpisodes': 170, 'stepsPerEpisode': 530, 'rewardPerEpisode': 490.32613377695594
'totalSteps': 26496, 'rewardStep': 0.8179520690702174, 'errorList': [], 'lossList': [0.0, -1.160441517829895, -0.11970637738704681, 0.032313454896211624, 0.0, 0.0, 0.0], 'rewardMean': 0.543118456890445, 'totalEpisodes': 172, 'stepsPerEpisode': 349, 'rewardPerEpisode': 289.27793184703546
'totalSteps': 27746, 'rewardStep': 0.9755515167168541, 'errorList': [0.1114196902561784, 0.1557765226863341, 0.12355304829339471, 0.1730877876021395, 0.10504309119820959, 0.1137053992428569, 0.12772934196046767, 0.13817241762811128, 0.11905713178015682, 0.13664227615666655, 0.12962775073997204, 0.12069103827580038, 0.1530866374521727, 0.15798757293588067, 0.13498392936975684, 0.1305780653149777, 0.13930070746610831, 0.1340039983678601, 0.15515920108683787, 0.12232931292025159, 0.11124676573854968, 0.1169406307722782, 0.13034430639699995, 0.1620063613841429, 0.11352963482230312, 0.1007339822334905, 0.12530483800291561, 0.16178896304140325, 0.13825872217902746, 0.11780662263581622, 0.1432607373739243, 0.15309454809658526, 0.1114676535087928, 0.09661934600744011, 0.1203021308017611, 0.1383243913339751, 0.13935356311619712, 0.1517716817048634, 0.15240827717783212, 0.17097205744776445, 0.1172674471373303, 0.10723448961189755, 0.1236143579973996, 0.11460069057050229, 0.13675405504322685, 0.1240502770712114, 0.11957618613829857, 0.12619121019535473, 0.1149508174939844, 0.18875713973400676], 'lossList': [0.0, -1.1683062314987183, 0.28163042664527893, 0.13313078880310059, 0.0, 0.0, 0.0], 'rewardMean': 0.6218113079881042, 'totalEpisodes': 172, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1009.6373710653249, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=27746, timeSpent=74.95
