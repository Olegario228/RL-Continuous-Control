#parameter variation file for learning
#varied parameters:
#case = 1
#computationIndex = 0
#functionData = {'nArms': 1, 'evaluationSteps': 5000, 'episodeSteps': 1000, 'episodeStepsMax': 1250, 'totalLearningSteps': 200000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'TD3', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models/SinglePendulum_TD3_r2_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models/SinglePendulum_TD3_r2_Results', 'verbose': True}
#
'totalSteps': 1000, 'rewardStep': 0.36789448614600806, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.36789448614600806, 'totalEpisodes': 79, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.435574214008849
'totalSteps': 2004, 'rewardStep': 0.4882852887984178, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.428089887472213, 'totalEpisodes': 172, 'stepsPerEpisode': 8, 'rewardPerEpisode': 5.803749077547948
'totalSteps': 3013, 'rewardStep': 0.4611312468538779, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4391036739327679, 'totalEpisodes': 260, 'stepsPerEpisode': 17, 'rewardPerEpisode': 11.988989786817308
'totalSteps': 4014, 'rewardStep': 0.3392161321016013, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4141317884749762, 'totalEpisodes': 345, 'stepsPerEpisode': 19, 'rewardPerEpisode': 12.453547092606302
'totalSteps': 5019, 'rewardStep': 0.20267946342846127, 'errorList': [], 'lossList': [0.686561955139041, 0.0, 0.0, 0.0, -0.739410400390625, 0.05284844525158405, 0.0], 'rewardMean': 0.3718413234656732, 'totalEpisodes': 436, 'stepsPerEpisode': 6, 'rewardPerEpisode': 3.616668498145159
'totalSteps': 6269, 'rewardStep': 0.9446477448879874, 'errorList': [], 'lossList': [5.607409931719303, 0.0, 0.0, 0.0, -5.710476875305176, 0.10306694358587265, 0.0], 'rewardMean': 0.4295166493398712, 'totalEpisodes': 482, 'stepsPerEpisode': 544, 'rewardPerEpisode': 497.5926355937724
'totalSteps': 7519, 'rewardStep': 0.4288435228480722, 'errorList': [], 'lossList': [8.782225012779236, 0.0, 0.0, 0.0, -9.899677276611328, 1.1174522638320923, 0.0], 'rewardMean': 0.43561155301007765, 'totalEpisodes': 483, 'stepsPerEpisode': 880, 'rewardPerEpisode': 701.2372784884434
'totalSteps': 8577, 'rewardStep': 0.4836519939886813, 'errorList': [], 'lossList': [13.411854267120361, 0.0, 0.0, 0.0, -13.908273696899414, 0.49641942977905273, 0.0], 'rewardMean': 0.4346848940481303, 'totalEpisodes': 487, 'stepsPerEpisode': 191, 'rewardPerEpisode': 126.48967935265688
'totalSteps': 9613, 'rewardStep': 0.491906756341156, 'errorList': [], 'lossList': [14.743447482585907, 0.0, 0.0, 0.0, -16.94768714904785, 2.2042396664619446, 0.0], 'rewardMean': 0.44083999594558587, 'totalEpisodes': 491, 'stepsPerEpisode': 289, 'rewardPerEpisode': 204.94815008740758
'totalSteps': 10703, 'rewardStep': 0.3661390319460418, 'errorList': [], 'lossList': [15.578723460435867, 0.0, 0.0, 0.0, -20.163969039916992, 4.585245579481125, 0.0], 'rewardMean': 0.446224575914474, 'totalEpisodes': 496, 'stepsPerEpisode': 153, 'rewardPerEpisode': 109.48496658964163
'totalSteps': 11953, 'rewardStep': 0.5963450439403337, 'errorList': [], 'lossList': [20.758706033229828, 0.0, 0.0, 0.0, -22.970674514770508, 2.21196848154068, 0.0], 'rewardMean': 0.4855911339656613, 'totalEpisodes': 501, 'stepsPerEpisode': 265, 'rewardPerEpisode': 218.07614549804717
'totalSteps': 13166, 'rewardStep': 0.14288508555988644, 'errorList': [], 'lossList': [22.895355701446533, 0.0, 0.0, 0.0, -24.87519073486328, 1.979835033416748, 0.0], 'rewardMean': 0.39943543024599365, 'totalEpisodes': 507, 'stepsPerEpisode': 298, 'rewardPerEpisode': 228.13532092056644
'totalSteps': 14333, 'rewardStep': 0.2627090121784891, 'errorList': [], 'lossList': [27.75963395833969, 0.0, 0.0, 0.0, -28.62958335876465, 0.8699494004249573, 0.0], 'rewardMean': 0.36072768099801616, 'totalEpisodes': 510, 'stepsPerEpisode': 558, 'rewardPerEpisode': 429.2462484894333
'totalSteps': 15583, 'rewardStep': 0.8108986334374175, 'errorList': [], 'lossList': [18.30294108390808, 0.0, 0.0, 0.0, -30.19762420654297, 11.894683122634888, 0.0], 'rewardMean': 0.39345234494288983, 'totalEpisodes': 512, 'stepsPerEpisode': 483, 'rewardPerEpisode': 402.6070140675448
'totalSteps': 16833, 'rewardStep': 0.7531001660139536, 'errorList': [], 'lossList': [31.512077122926712, 0.0, 0.0, 0.0, -32.72807693481445, 1.215999811887741, 0.0], 'rewardMean': 0.4195716859101696, 'totalEpisodes': 513, 'stepsPerEpisode': 1142, 'rewardPerEpisode': 815.154424022252
'totalSteps': 18083, 'rewardStep': 0.7209714976895637, 'errorList': [], 'lossList': [30.428601026535034, 0.0, 0.0, 0.0, -33.85737991333008, 3.428778886795044, 0.0], 'rewardMean': 0.44247816004501034, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 885.1739889903679
'totalSteps': 19333, 'rewardStep': 0.6914682677420412, 'errorList': [], 'lossList': [30.969574689865112, 0.0, 0.0, 0.0, -35.412845611572266, 4.443270921707153, 0.0], 'rewardMean': 0.4750110836246103, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 886.7057361017563
'totalSteps': 20583, 'rewardStep': 0.7894903396483377, 'errorList': [], 'lossList': [37.296061396598816, 0.0, 0.0, 0.0, -38.71320343017578, 1.4171420335769653, 0.0], 'rewardMean': 0.5173462143948399, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 942.0290767792314
'totalSteps': 21833, 'rewardStep': 0.7959640397474204, 'errorList': [], 'lossList': [39.64380592107773, 0.0, 0.0, 0.0, -41.70808410644531, 2.0642781853675842, 0.0], 'rewardMean': 0.5373081139755485, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 946.5353323363573
'totalSteps': 23083, 'rewardStep': 0.765847293074424, 'errorList': [], 'lossList': [35.7774600982666, 0.0, 0.0, 0.0, -43.031837463378906, 7.254377365112305, 0.0], 'rewardMean': 0.5996043347270023, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1043.146509618921
'totalSteps': 24333, 'rewardStep': 0.8108729013878379, 'errorList': [], 'lossList': [43.81227684020996, 0.0, 0.0, 0.0, -45.23406219482422, 1.4217853546142578, 0.0], 'rewardMean': 0.6664031163097974, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1125.0525509566498
'totalSteps': 25583, 'rewardStep': 0.9415385280822178, 'errorList': [], 'lossList': [45.3458514213562, 0.0, 0.0, 0.0, -47.096412658691406, 1.750561237335205, 0.0], 'rewardMean': 0.7342860679001703, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1137.6339771758373
'totalSteps': 26833, 'rewardStep': 0.9068424028810135, 'errorList': [], 'lossList': [47.94105143845081, 0.0, 0.0, 0.0, -50.47629928588867, 2.5352478474378586, 0.0], 'rewardMean': 0.7986994069704226, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1131.400835353087
'totalSteps': 28083, 'rewardStep': 0.9586903813831742, 'errorList': [0.08432850157506375, 0.08569678384824243, 0.0850388552915229, 0.08414022701767722, 0.08468743551462354, 0.0843160668982578, 0.08438799670068187, 0.0839403518320836, 0.08366698552079477, 0.08399356967892037, 0.08430538602049073, 0.08598722120094875, 0.08384224928504368, 0.08419231201088104, 0.08416732972825226, 0.08428315615596671, 0.08427530119576142, 0.08433449345332898, 0.08418204098214781, 0.08345762621405242, 0.0839251605816416, 0.08932558180830075, 0.08582622290703203, 0.0838981179142998, 0.08414217875753537, 0.08422653581292422, 0.08434873462600635, 0.08583238836222178, 0.08626749349574432, 0.08616714583480609, 0.08649842641619793, 0.08432078879156366, 0.08424450750841181, 0.08438347750258453, 0.08417006433691801, 0.08407740725805117, 0.08436104929374273, 0.08414303404453805, 0.08595309228411518, 0.08432158343170122, 0.08383093973976516, 0.08421026113191907, 0.08433254481176941, 0.08435445716068868, 0.08539053081245546, 0.08506694519397427, 0.08488722732657061, 0.08589805555592753, 0.08419275906420674, 0.08461548183916245], 'lossList': [48.06584107875824, 0.0, 0.0, 0.0, -52.01972579956055, 3.953884720802307, 0.0], 'rewardMean': 0.8134785817649984, 'totalEpisodes': 513, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1130.8140489543564, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=28083, timeSpent=609.41
