#parameter variation file for learning
#varied parameters:
#case = 2
#computationIndex = 1
#functionData = {'nArms': 1, 'evaluationSteps': 5000, 'episodeSteps': 1000, 'episodeStepsMax': 1250, 'totalLearningSteps': 200000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'TD3', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models/SinglePendulum_TD3_r2_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models/SinglePendulum_TD3_r2_Results', 'verbose': True}
#
'totalSteps': 1005, 'rewardStep': 0.4178812137730543, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4178812137730543, 'totalEpisodes': 87, 'stepsPerEpisode': 29, 'rewardPerEpisode': 22.688980043537068
'totalSteps': 2025, 'rewardStep': 0.46983028061767207, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.44385574719536325, 'totalEpisodes': 172, 'stepsPerEpisode': 36, 'rewardPerEpisode': 30.409853549824717
'totalSteps': 3027, 'rewardStep': 0.42914370038318406, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4389517315913036, 'totalEpisodes': 264, 'stepsPerEpisode': 3, 'rewardPerEpisode': 1.4865677172023868
'totalSteps': 4038, 'rewardStep': 0.4409407516828172, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.4394489866141819, 'totalEpisodes': 367, 'stepsPerEpisode': 13, 'rewardPerEpisode': 10.60925213255522
'totalSteps': 5042, 'rewardStep': 0.38964189739744853, 'errorList': [], 'lossList': [0.7826478630304337, 0.0, 0.0, 0.0, -0.854546070098877, 0.0718982070684433, 0.0], 'rewardMean': 0.42948756877083527, 'totalEpisodes': 465, 'stepsPerEpisode': 5, 'rewardPerEpisode': 3.5877830120304273
'totalSteps': 6155, 'rewardStep': 0.3899040644475228, 'errorList': [], 'lossList': [3.9103330969810486, 0.0, 0.0, 0.0, -4.00416374206543, 0.0938306450843811, 0.0], 'rewardMean': 0.4238921389057289, 'totalEpisodes': 693, 'stepsPerEpisode': 139, 'rewardPerEpisode': 92.53786811508373
'totalSteps': 7222, 'rewardStep': 0.43835490630783014, 'errorList': [], 'lossList': [7.635124087333679, 0.0, 0.0, 0.0, -8.22824478149414, 0.5931206941604614, 0.0], 'rewardMean': 0.41759706404376057, 'totalEpisodes': 696, 'stepsPerEpisode': 311, 'rewardPerEpisode': 199.76896926777627
'totalSteps': 8278, 'rewardStep': 0.49720677927941215, 'errorList': [], 'lossList': [11.14291825890541, 0.0, 0.0, 0.0, -12.03897476196289, 0.8960565030574799, 0.0], 'rewardMean': 0.4312096798230061, 'totalEpisodes': 700, 'stepsPerEpisode': 326, 'rewardPerEpisode': 256.79863456088606
'totalSteps': 9528, 'rewardStep': 0.8452898520176876, 'errorList': [], 'lossList': [16.185204297304153, 0.0, 0.0, 0.0, -16.834470748901367, 0.6492664515972137, 0.0], 'rewardMean': 0.4716445898564932, 'totalEpisodes': 701, 'stepsPerEpisode': 486, 'rewardPerEpisode': 428.5773856680981
'totalSteps': 10539, 'rewardStep': 0.15465918117070365, 'errorList': [], 'lossList': [18.80867111682892, 0.0, 0.0, 0.0, -19.473421096801758, 0.6647499799728394, 0.0], 'rewardMean': 0.41951816118260743, 'totalEpisodes': 702, 'stepsPerEpisode': 1011, 'rewardPerEpisode': 799.3795986157967
'totalSteps': 11789, 'rewardStep': 0.9206651517161437, 'errorList': [], 'lossList': [19.62355202436447, 0.0, 0.0, 0.0, -22.623096466064453, 2.9995444416999817, 0.0], 'rewardMean': 0.4726204866144769, 'totalEpisodes': 704, 'stepsPerEpisode': 410, 'rewardPerEpisode': 367.50786273575375
'totalSteps': 12908, 'rewardStep': 0.2916233404242409, 'errorList': [], 'lossList': [22.859751224517822, 0.0, 0.0, 0.0, -27.064599990844727, 4.204848766326904, 0.0], 'rewardMean': 0.4529643418098206, 'totalEpisodes': 708, 'stepsPerEpisode': 187, 'rewardPerEpisode': 124.03212633883092
'totalSteps': 14158, 'rewardStep': 0.8738411347162898, 'errorList': [], 'lossList': [28.699160009622574, 0.0, 0.0, 0.0, -29.695960998535156, 0.9968009889125824, 0.0], 'rewardMean': 0.4965129646506664, 'totalEpisodes': 711, 'stepsPerEpisode': 276, 'rewardPerEpisode': 233.3653181551399
'totalSteps': 15408, 'rewardStep': 0.748748674483765, 'errorList': [], 'lossList': [29.959965229034424, 0.0, 0.0, 0.0, -32.5369987487793, 2.577033519744873, 0.0], 'rewardMean': 0.5275523414682599, 'totalEpisodes': 711, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1040.805829400062
'totalSteps': 16658, 'rewardStep': 0.7453960236336943, 'errorList': [], 'lossList': [31.226068943738937, 0.0, 0.0, 0.0, -34.603515625, 3.3774466812610626, 0.0], 'rewardMean': 0.5523712659036881, 'totalEpisodes': 711, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 944.1319891442035
'totalSteps': 17814, 'rewardStep': 0.12369888084234415, 'errorList': [], 'lossList': [28.903315722942352, 0.0, 0.0, 0.0, -37.676815032958984, 8.773499310016632, 0.0], 'rewardMean': 0.4428613789424469, 'totalEpisodes': 717, 'stepsPerEpisode': 196, 'rewardPerEpisode': 138.8009748851295
'totalSteps': 18955, 'rewardStep': 0.05589818759956189, 'errorList': [], 'lossList': [35.509910970926285, 0.0, 0.0, 0.0, -36.50954818725586, 0.9996372163295746, 0.0], 'rewardMean': 0.4231091802282186, 'totalEpisodes': 721, 'stepsPerEpisode': 540, 'rewardPerEpisode': 435.0792237850978
'totalSteps': 20205, 'rewardStep': 0.7976084747996494, 'errorList': [], 'lossList': [35.7728910446167, 0.0, 0.0, 0.0, -39.470645904541016, 3.6977548599243164, 0.0], 'rewardMean': 0.4108035125365692, 'totalEpisodes': 724, 'stepsPerEpisode': 461, 'rewardPerEpisode': 383.2984774716929
'totalSteps': 21455, 'rewardStep': 0.8327389300476168, 'errorList': [], 'lossList': [42.08322048187256, 0.0, 0.0, 0.0, -44.74830627441406, 2.665085792541504, 0.0], 'rewardMean': 0.4649150714989069, 'totalEpisodes': 724, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1027.9546388847764
'totalSteps': 22705, 'rewardStep': 0.9045811962920709, 'errorList': [], 'lossList': [37.96811430156231, 0.0, 0.0, 0.0, -44.54970169067383, 6.581587389111519, 0.0], 'rewardMean': 0.5262108570856898, 'totalEpisodes': 724, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 952.7895920539794
'totalSteps': 23955, 'rewardStep': 0.8715438101016053, 'errorList': [], 'lossList': [46.13667857646942, 0.0, 0.0, 0.0, -48.09816360473633, 1.9614850282669067, 0.0], 'rewardMean': 0.5259811246242214, 'totalEpisodes': 724, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1056.952145643864
'totalSteps': 25205, 'rewardStep': 0.7853180660754131, 'errorList': [], 'lossList': [43.94700467586517, 0.0, 0.0, 0.0, -48.34500503540039, 4.398000359535217, 0.0], 'rewardMean': 0.5296380637833862, 'totalEpisodes': 725, 'stepsPerEpisode': 282, 'rewardPerEpisode': 256.2496570995482
'totalSteps': 26455, 'rewardStep': 0.8454769097292537, 'errorList': [], 'lossList': [50.623501658439636, 0.0, 0.0, 0.0, -51.256839752197266, 0.6333380937576294, 0.0], 'rewardMean': 0.5396461523929421, 'totalEpisodes': 725, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 1027.4350761590802
'totalSteps': 27617, 'rewardStep': 0.3322557179174144, 'errorList': [], 'lossList': [48.91936522722244, 0.0, 0.0, 0.0, -50.543067932128906, 1.6237027049064636, 0.0], 'rewardMean': 0.5813575198079561, 'totalEpisodes': 726, 'stepsPerEpisode': 1162, 'rewardPerEpisode': 904.6859122543231
'totalSteps': 28867, 'rewardStep': 0.7279663972739008, 'errorList': [], 'lossList': [50.58849394321442, 0.0, 0.0, 0.0, -52.6587028503418, 2.0702089071273804, 0.0], 'rewardMean': 0.6485643407753902, 'totalEpisodes': 729, 'stepsPerEpisode': 415, 'rewardPerEpisode': 322.7052053664181
'totalSteps': 30117, 'rewardStep': 0.7026100312122838, 'errorList': [], 'lossList': [52.55344367027283, 0.0, 0.0, 0.0, -53.74274826049805, 1.1893045902252197, 0.0], 'rewardMean': 0.7132355251366624, 'totalEpisodes': 729, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 944.294543890014
