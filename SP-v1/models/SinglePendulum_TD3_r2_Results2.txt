#parameter variation file for learning
#varied parameters:
#case = 3
#computationIndex = 2
#functionData = {'nArms': 1, 'evaluationSteps': 5000, 'episodeSteps': 1000, 'episodeStepsMax': 1250, 'totalLearningSteps': 200000, 'logLevel': 3, 'lossThreshold': 0.01, 'rewardThreshold': 0.95, 'meanSampleSize': 10, 'RLalgorithm': 'TD3', 'rewardMode': 2, 'rewardPositionFactor': 0.5, 'stepUpdateTime': 0.02, 'thresholdFactor': 0.5, 'cartForce': 12, 'numberOfTests': 50, 'relativeFriction': 0.0, 'storeBestModel': 'models/SinglePendulum_TD3_r2_Model', 'netarchLayerSize': 64, 'netarchNumberOfLayers': 2, 'stopWhenAllTestsSuccess': True, 'breakTestsWhenFailed': False, 'nThreadsTraining': 1, 'resultsFile': 'models/SinglePendulum_TD3_r2_Results', 'verbose': True}
#
'totalSteps': 1004, 'rewardStep': 0.45436921714507655, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.45436921714507655, 'totalEpisodes': 84, 'stepsPerEpisode': 25, 'rewardPerEpisode': 14.950914769055103
'totalSteps': 2005, 'rewardStep': 0.4446071392930207, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.44948817821904863, 'totalEpisodes': 170, 'stepsPerEpisode': 18, 'rewardPerEpisode': 14.999564275782806
'totalSteps': 3026, 'rewardStep': 0.3977022086317117, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.432226188356603, 'totalEpisodes': 261, 'stepsPerEpisode': 24, 'rewardPerEpisode': 18.68033146925287
'totalSteps': 4032, 'rewardStep': 0.4468603195451384, 'errorList': [], 'lossList': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'rewardMean': 0.43588472115373683, 'totalEpisodes': 341, 'stepsPerEpisode': 21, 'rewardPerEpisode': 14.94032942815429
'totalSteps': 5033, 'rewardStep': 0.3574642162641037, 'errorList': [], 'lossList': [0.8140121847391129, 0.0, 0.0, 0.0, -0.8695008158683777, 0.05548863112926483, 0.0], 'rewardMean': 0.42020062017581017, 'totalEpisodes': 437, 'stepsPerEpisode': 4, 'rewardPerEpisode': 2.61366940958073
'totalSteps': 6136, 'rewardStep': 0.4763109588760335, 'errorList': [], 'lossList': [4.128152996301651, 0.0, 0.0, 0.0, -4.628290176391602, 0.5001371800899506, 0.0], 'rewardMean': 0.42458896852200156, 'totalEpisodes': 525, 'stepsPerEpisode': 329, 'rewardPerEpisode': 259.7938743312356
'totalSteps': 7386, 'rewardStep': 0.5529934693325347, 'errorList': [], 'lossList': [8.65715965628624, 0.0, 0.0, 0.0, -9.383561134338379, 0.7264014780521393, 0.0], 'rewardMean': 0.435427601525953, 'totalEpisodes': 527, 'stepsPerEpisode': 281, 'rewardPerEpisode': 240.37552325846335
'totalSteps': 8636, 'rewardStep': 0.6488537505283039, 'errorList': [], 'lossList': [13.179392218589783, 0.0, 0.0, 0.0, -14.152115821838379, 0.9727236032485962, 0.0], 'rewardMean': 0.4558522626494813, 'totalEpisodes': 528, 'stepsPerEpisode': 351, 'rewardPerEpisode': 285.21399477207507
'totalSteps': 9869, 'rewardStep': 0.357250021341027, 'errorList': [], 'lossList': [16.880001068115234, 0.0, 0.0, 0.0, -17.784879684448242, 0.9048786163330078, 0.0], 'rewardMean': 0.44776182519134444, 'totalEpisodes': 530, 'stepsPerEpisode': 898, 'rewardPerEpisode': 622.4820973523643
'totalSteps': 11004, 'rewardStep': 0.30617580939802447, 'errorList': [], 'lossList': [17.912445068359375, 0.0, 0.0, 0.0, -20.484474182128906, 2.5720291137695312, 0.0], 'rewardMean': 0.4196249231619216, 'totalEpisodes': 534, 'stepsPerEpisode': 199, 'rewardPerEpisode': 143.47916553374503
'totalSteps': 12137, 'rewardStep': 0.4924578763194857, 'errorList': [], 'lossList': [22.491839051246643, 0.0, 0.0, 0.0, -23.914966583251953, 1.42312753200531, 0.0], 'rewardMean': 0.446623655172998, 'totalEpisodes': 539, 'stepsPerEpisode': 198, 'rewardPerEpisode': 152.50753032880206
'totalSteps': 13301, 'rewardStep': 0.25194933514448026, 'errorList': [], 'lossList': [25.943325996398926, 0.0, 0.0, 0.0, -27.53142547607422, 1.588099479675293, 0.0], 'rewardMean': 0.4017513304266873, 'totalEpisodes': 545, 'stepsPerEpisode': 179, 'rewardPerEpisode': 123.51233300848905
'totalSteps': 14421, 'rewardStep': 0.41055446284889063, 'errorList': [], 'lossList': [23.838586688041687, 0.0, 0.0, 0.0, -27.704381942749023, 3.8657952547073364, 0.0], 'rewardMean': 0.3636775010103816, 'totalEpisodes': 550, 'stepsPerEpisode': 248, 'rewardPerEpisode': 182.71184709089303
'totalSteps': 15432, 'rewardStep': 0.39237639893130316, 'errorList': [], 'lossList': [22.28856098651886, 0.0, 0.0, 0.0, -29.551860809326172, 7.263299822807312, 0.0], 'rewardMean': 0.37070277652843686, 'totalEpisodes': 554, 'stepsPerEpisode': 315, 'rewardPerEpisode': 230.43771003030398
'totalSteps': 16553, 'rewardStep': 0.3005781834039529, 'errorList': [], 'lossList': [30.47405970096588, 0.0, 0.0, 0.0, -32.12141418457031, 1.6473544836044312, 0.0], 'rewardMean': 0.36958325132962255, 'totalEpisodes': 557, 'stepsPerEpisode': 424, 'rewardPerEpisode': 309.44480082446046
'totalSteps': 17772, 'rewardStep': 0.4316569634672579, 'errorList': [], 'lossList': [32.551969945430756, 0.0, 0.0, 0.0, -33.16283416748047, 0.6108642220497131, 0.0], 'rewardMean': 0.35742306875917695, 'totalEpisodes': 559, 'stepsPerEpisode': 468, 'rewardPerEpisode': 340.6063720103297
'totalSteps': 19016, 'rewardStep': 0.43795748870686924, 'errorList': [], 'lossList': [32.124879002571106, 0.0, 0.0, 0.0, -34.11689758300781, 1.9920185804367065, 0.0], 'rewardMean': 0.3946246994716548, 'totalEpisodes': 561, 'stepsPerEpisode': 248, 'rewardPerEpisode': 168.93794405307608
'totalSteps': 20266, 'rewardStep': 0.6261639619201744, 'errorList': [], 'lossList': [25.336263179779053, 0.0, 0.0, 0.0, -35.761539459228516, 10.425276279449463, 0.0], 'rewardMean': 0.4161856493787831, 'totalEpisodes': 562, 'stepsPerEpisode': 677, 'rewardPerEpisode': 482.512560083456
'totalSteps': 21516, 'rewardStep': 0.6050242255269445, 'errorList': [], 'lossList': [35.46052688360214, 0.0, 0.0, 0.0, -37.7713737487793, 2.3108468651771545, 0.0], 'rewardMean': 0.4356326256465886, 'totalEpisodes': 562, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 958.0446723864386
'totalSteps': 22766, 'rewardStep': 0.844779265791734, 'errorList': [], 'lossList': [35.80641329288483, 0.0, 0.0, 0.0, -41.10382843017578, 5.297415137290955, 0.0], 'rewardMean': 0.4808729123326316, 'totalEpisodes': 562, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 790.1147465240115
'totalSteps': 24016, 'rewardStep': 0.9713134277024826, 'errorList': [0.0419743781745352, 0.040461801312694216, 0.0404766761441399, 0.04186764947423861, 0.040501686910030026, 0.04130530482740777, 0.040520130225460194, 0.0404897553690954, 0.04048421568513459, 0.040843650607093385, 0.04051102458236459, 0.04051141280728106, 0.04046485079079423, 0.040322546070499364, 0.040493915335868505, 0.04049654756751239, 0.040508116104189945, 0.04048506248175679, 0.040506965937384, 0.040493412640493685, 0.04051424458256552, 0.040498209585572945, 0.04048048903171959, 0.040504410633566565, 0.0404873543740439, 0.042802416203305285, 0.04046865531796532, 0.040520428245280384, 0.04050178037272127, 0.040508782954048826, 0.04098005255945767, 0.04048422959506021, 0.04050533253162767, 0.040498082873857935, 0.04048599637938954, 0.04049909407041007, 0.040507784916388984, 0.04050186012975674, 0.040644891203290213, 0.04049512820676894, 0.04050909774261935, 0.045305042988279816, 0.04289984191501773, 0.04048445751799135, 0.04051750223306202, 0.045440891940321616, 0.04035668044016589, 0.04555899597401935, 0.04049512789776726, 0.040472744779125006], 'lossList': [42.21582370996475, 0.0, 0.0, 0.0, -43.31422805786133, 1.098404347896576, 0.0], 'rewardMean': 0.5387666152097496, 'totalEpisodes': 562, 'stepsPerEpisode': 1250, 'rewardPerEpisode': 946.7123620330613, 'successfulTests': 50
#maxSuccessfulTests=50, maxSuccessfulTestsAtStep=24016, timeSpent=497.04
